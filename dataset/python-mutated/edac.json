[
    {
        "func_name": "_init_learn",
        "original": "def _init_learn(self) -> None:\n    \"\"\"\n        Overview:\n            Learn mode init method. Called by ``self.__init__``.\n            Init q, value and policy's optimizers, algorithm config, main and target models.\n        \"\"\"\n    super()._init_learn()\n    self._eta = self._cfg.learn.eta\n    self._with_q_entropy = self._cfg.learn.with_q_entropy\n    self._forward_learn_cnt = 0",
        "mutated": [
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n    \"\\n        Overview:\\n            Learn mode init method. Called by ``self.__init__``.\\n            Init q, value and policy's optimizers, algorithm config, main and target models.\\n        \"\n    super()._init_learn()\n    self._eta = self._cfg.learn.eta\n    self._with_q_entropy = self._cfg.learn.with_q_entropy\n    self._forward_learn_cnt = 0",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Overview:\\n            Learn mode init method. Called by ``self.__init__``.\\n            Init q, value and policy's optimizers, algorithm config, main and target models.\\n        \"\n    super()._init_learn()\n    self._eta = self._cfg.learn.eta\n    self._with_q_entropy = self._cfg.learn.with_q_entropy\n    self._forward_learn_cnt = 0",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Overview:\\n            Learn mode init method. Called by ``self.__init__``.\\n            Init q, value and policy's optimizers, algorithm config, main and target models.\\n        \"\n    super()._init_learn()\n    self._eta = self._cfg.learn.eta\n    self._with_q_entropy = self._cfg.learn.with_q_entropy\n    self._forward_learn_cnt = 0",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Overview:\\n            Learn mode init method. Called by ``self.__init__``.\\n            Init q, value and policy's optimizers, algorithm config, main and target models.\\n        \"\n    super()._init_learn()\n    self._eta = self._cfg.learn.eta\n    self._with_q_entropy = self._cfg.learn.with_q_entropy\n    self._forward_learn_cnt = 0",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Overview:\\n            Learn mode init method. Called by ``self.__init__``.\\n            Init q, value and policy's optimizers, algorithm config, main and target models.\\n        \"\n    super()._init_learn()\n    self._eta = self._cfg.learn.eta\n    self._with_q_entropy = self._cfg.learn.with_q_entropy\n    self._forward_learn_cnt = 0"
        ]
    },
    {
        "func_name": "_forward_learn",
        "original": "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    loss_dict = {}\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if len(data.get('action').shape) == 1:\n        data['action'] = data['action'].reshape(-1, 1)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    obs = data['obs']\n    next_obs = data['next_obs']\n    reward = data['reward']\n    done = data['done']\n    acs = data['action']\n    q_value = self._learn_model.forward(data, mode='compute_critic')['q_value']\n    with torch.no_grad():\n        (mu, sigma) = self._learn_model.forward(next_obs, mode='compute_actor')['logit']\n        dist = Independent(Normal(mu, sigma), 1)\n        pred = dist.rsample()\n        next_action = torch.tanh(pred)\n        y = 1 - next_action.pow(2) + 1e-06\n        next_log_prob = dist.log_prob(pred).unsqueeze(-1)\n        next_log_prob = next_log_prob - torch.log(y).sum(-1, keepdim=True)\n        next_data = {'obs': next_obs, 'action': next_action}\n        target_q_value = self._target_model.forward(next_data, mode='compute_critic')['q_value']\n        (target_q_value, _) = torch.min(target_q_value, dim=0)\n        if self._with_q_entropy:\n            target_q_value -= self._alpha * next_log_prob.squeeze(-1)\n        target_q_value = self._gamma * (1 - done) * target_q_value + reward\n    weight = data['weight']\n    if weight is None:\n        weight = torch.ones_like(q_value)\n    td_error_per_sample = nn.MSELoss(reduction='none')(q_value, target_q_value).mean(dim=1).sum()\n    loss_dict['critic_loss'] = (td_error_per_sample * weight).mean()\n    if self._eta > 0:\n        pre_obs = obs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0)\n        pre_acs = acs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0).requires_grad_(True)\n        q_pred_tile = self._learn_model.forward({'obs': pre_obs, 'action': pre_acs}, mode='compute_critic')['q_value'].requires_grad_(True)\n        q_pred_grads = torch.autograd.grad(q_pred_tile.sum(), pre_acs, retain_graph=True, create_graph=True)[0]\n        q_pred_grads = q_pred_grads / (torch.norm(q_pred_grads, p=2, dim=2).unsqueeze(-1) + 1e-10)\n        q_pred_grads = q_pred_grads.transpose(0, 1)\n        q_pred_grads = q_pred_grads @ q_pred_grads.permute(0, 2, 1)\n        masks = torch.eye(self._cfg.model.ensemble_num, device=obs.device).unsqueeze(dim=0).repeat(q_pred_grads.size(0), 1, 1)\n        q_pred_grads = (1 - masks) * q_pred_grads\n        grad_loss = torch.mean(torch.sum(q_pred_grads, dim=(1, 2))) / (self._cfg.model.ensemble_num - 1)\n        loss_dict['critic_loss'] += grad_loss * self._eta\n    self._optimizer_q.zero_grad()\n    loss_dict['critic_loss'].backward()\n    self._optimizer_q.step()\n    (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']\n    dist = Independent(Normal(mu, sigma), 1)\n    pred = dist.rsample()\n    action = torch.tanh(pred)\n    y = 1 - action.pow(2) + 1e-06\n    log_prob = dist.log_prob(pred).unsqueeze(-1)\n    log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)\n    eval_data = {'obs': obs, 'action': action}\n    new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']\n    (new_q_value, _) = torch.min(new_q_value, dim=0)\n    policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()\n    loss_dict['policy_loss'] = policy_loss\n    self._optimizer_policy.zero_grad()\n    loss_dict['policy_loss'].backward()\n    self._optimizer_policy.step()\n    if self._auto_alpha:\n        if self._log_space:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = self._log_alpha.detach().exp()\n        else:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = max(0, self._alpha)\n    loss_dict['total_loss'] = sum(loss_dict.values())\n    self._forward_learn_cnt += 1\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr_q': self._optimizer_q.defaults['lr'], 'cur_lr_p': self._optimizer_policy.defaults['lr'], 'priority': td_error_per_sample.abs().tolist(), 'td_error': td_error_per_sample.detach().mean().item(), 'alpha': self._alpha.item(), 'target_q_value': target_q_value.detach().mean().item(), **loss_dict}",
        "mutated": [
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n    loss_dict = {}\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if len(data.get('action').shape) == 1:\n        data['action'] = data['action'].reshape(-1, 1)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    obs = data['obs']\n    next_obs = data['next_obs']\n    reward = data['reward']\n    done = data['done']\n    acs = data['action']\n    q_value = self._learn_model.forward(data, mode='compute_critic')['q_value']\n    with torch.no_grad():\n        (mu, sigma) = self._learn_model.forward(next_obs, mode='compute_actor')['logit']\n        dist = Independent(Normal(mu, sigma), 1)\n        pred = dist.rsample()\n        next_action = torch.tanh(pred)\n        y = 1 - next_action.pow(2) + 1e-06\n        next_log_prob = dist.log_prob(pred).unsqueeze(-1)\n        next_log_prob = next_log_prob - torch.log(y).sum(-1, keepdim=True)\n        next_data = {'obs': next_obs, 'action': next_action}\n        target_q_value = self._target_model.forward(next_data, mode='compute_critic')['q_value']\n        (target_q_value, _) = torch.min(target_q_value, dim=0)\n        if self._with_q_entropy:\n            target_q_value -= self._alpha * next_log_prob.squeeze(-1)\n        target_q_value = self._gamma * (1 - done) * target_q_value + reward\n    weight = data['weight']\n    if weight is None:\n        weight = torch.ones_like(q_value)\n    td_error_per_sample = nn.MSELoss(reduction='none')(q_value, target_q_value).mean(dim=1).sum()\n    loss_dict['critic_loss'] = (td_error_per_sample * weight).mean()\n    if self._eta > 0:\n        pre_obs = obs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0)\n        pre_acs = acs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0).requires_grad_(True)\n        q_pred_tile = self._learn_model.forward({'obs': pre_obs, 'action': pre_acs}, mode='compute_critic')['q_value'].requires_grad_(True)\n        q_pred_grads = torch.autograd.grad(q_pred_tile.sum(), pre_acs, retain_graph=True, create_graph=True)[0]\n        q_pred_grads = q_pred_grads / (torch.norm(q_pred_grads, p=2, dim=2).unsqueeze(-1) + 1e-10)\n        q_pred_grads = q_pred_grads.transpose(0, 1)\n        q_pred_grads = q_pred_grads @ q_pred_grads.permute(0, 2, 1)\n        masks = torch.eye(self._cfg.model.ensemble_num, device=obs.device).unsqueeze(dim=0).repeat(q_pred_grads.size(0), 1, 1)\n        q_pred_grads = (1 - masks) * q_pred_grads\n        grad_loss = torch.mean(torch.sum(q_pred_grads, dim=(1, 2))) / (self._cfg.model.ensemble_num - 1)\n        loss_dict['critic_loss'] += grad_loss * self._eta\n    self._optimizer_q.zero_grad()\n    loss_dict['critic_loss'].backward()\n    self._optimizer_q.step()\n    (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']\n    dist = Independent(Normal(mu, sigma), 1)\n    pred = dist.rsample()\n    action = torch.tanh(pred)\n    y = 1 - action.pow(2) + 1e-06\n    log_prob = dist.log_prob(pred).unsqueeze(-1)\n    log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)\n    eval_data = {'obs': obs, 'action': action}\n    new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']\n    (new_q_value, _) = torch.min(new_q_value, dim=0)\n    policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()\n    loss_dict['policy_loss'] = policy_loss\n    self._optimizer_policy.zero_grad()\n    loss_dict['policy_loss'].backward()\n    self._optimizer_policy.step()\n    if self._auto_alpha:\n        if self._log_space:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = self._log_alpha.detach().exp()\n        else:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = max(0, self._alpha)\n    loss_dict['total_loss'] = sum(loss_dict.values())\n    self._forward_learn_cnt += 1\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr_q': self._optimizer_q.defaults['lr'], 'cur_lr_p': self._optimizer_policy.defaults['lr'], 'priority': td_error_per_sample.abs().tolist(), 'td_error': td_error_per_sample.detach().mean().item(), 'alpha': self._alpha.item(), 'target_q_value': target_q_value.detach().mean().item(), **loss_dict}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    loss_dict = {}\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if len(data.get('action').shape) == 1:\n        data['action'] = data['action'].reshape(-1, 1)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    obs = data['obs']\n    next_obs = data['next_obs']\n    reward = data['reward']\n    done = data['done']\n    acs = data['action']\n    q_value = self._learn_model.forward(data, mode='compute_critic')['q_value']\n    with torch.no_grad():\n        (mu, sigma) = self._learn_model.forward(next_obs, mode='compute_actor')['logit']\n        dist = Independent(Normal(mu, sigma), 1)\n        pred = dist.rsample()\n        next_action = torch.tanh(pred)\n        y = 1 - next_action.pow(2) + 1e-06\n        next_log_prob = dist.log_prob(pred).unsqueeze(-1)\n        next_log_prob = next_log_prob - torch.log(y).sum(-1, keepdim=True)\n        next_data = {'obs': next_obs, 'action': next_action}\n        target_q_value = self._target_model.forward(next_data, mode='compute_critic')['q_value']\n        (target_q_value, _) = torch.min(target_q_value, dim=0)\n        if self._with_q_entropy:\n            target_q_value -= self._alpha * next_log_prob.squeeze(-1)\n        target_q_value = self._gamma * (1 - done) * target_q_value + reward\n    weight = data['weight']\n    if weight is None:\n        weight = torch.ones_like(q_value)\n    td_error_per_sample = nn.MSELoss(reduction='none')(q_value, target_q_value).mean(dim=1).sum()\n    loss_dict['critic_loss'] = (td_error_per_sample * weight).mean()\n    if self._eta > 0:\n        pre_obs = obs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0)\n        pre_acs = acs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0).requires_grad_(True)\n        q_pred_tile = self._learn_model.forward({'obs': pre_obs, 'action': pre_acs}, mode='compute_critic')['q_value'].requires_grad_(True)\n        q_pred_grads = torch.autograd.grad(q_pred_tile.sum(), pre_acs, retain_graph=True, create_graph=True)[0]\n        q_pred_grads = q_pred_grads / (torch.norm(q_pred_grads, p=2, dim=2).unsqueeze(-1) + 1e-10)\n        q_pred_grads = q_pred_grads.transpose(0, 1)\n        q_pred_grads = q_pred_grads @ q_pred_grads.permute(0, 2, 1)\n        masks = torch.eye(self._cfg.model.ensemble_num, device=obs.device).unsqueeze(dim=0).repeat(q_pred_grads.size(0), 1, 1)\n        q_pred_grads = (1 - masks) * q_pred_grads\n        grad_loss = torch.mean(torch.sum(q_pred_grads, dim=(1, 2))) / (self._cfg.model.ensemble_num - 1)\n        loss_dict['critic_loss'] += grad_loss * self._eta\n    self._optimizer_q.zero_grad()\n    loss_dict['critic_loss'].backward()\n    self._optimizer_q.step()\n    (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']\n    dist = Independent(Normal(mu, sigma), 1)\n    pred = dist.rsample()\n    action = torch.tanh(pred)\n    y = 1 - action.pow(2) + 1e-06\n    log_prob = dist.log_prob(pred).unsqueeze(-1)\n    log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)\n    eval_data = {'obs': obs, 'action': action}\n    new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']\n    (new_q_value, _) = torch.min(new_q_value, dim=0)\n    policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()\n    loss_dict['policy_loss'] = policy_loss\n    self._optimizer_policy.zero_grad()\n    loss_dict['policy_loss'].backward()\n    self._optimizer_policy.step()\n    if self._auto_alpha:\n        if self._log_space:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = self._log_alpha.detach().exp()\n        else:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = max(0, self._alpha)\n    loss_dict['total_loss'] = sum(loss_dict.values())\n    self._forward_learn_cnt += 1\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr_q': self._optimizer_q.defaults['lr'], 'cur_lr_p': self._optimizer_policy.defaults['lr'], 'priority': td_error_per_sample.abs().tolist(), 'td_error': td_error_per_sample.detach().mean().item(), 'alpha': self._alpha.item(), 'target_q_value': target_q_value.detach().mean().item(), **loss_dict}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    loss_dict = {}\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if len(data.get('action').shape) == 1:\n        data['action'] = data['action'].reshape(-1, 1)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    obs = data['obs']\n    next_obs = data['next_obs']\n    reward = data['reward']\n    done = data['done']\n    acs = data['action']\n    q_value = self._learn_model.forward(data, mode='compute_critic')['q_value']\n    with torch.no_grad():\n        (mu, sigma) = self._learn_model.forward(next_obs, mode='compute_actor')['logit']\n        dist = Independent(Normal(mu, sigma), 1)\n        pred = dist.rsample()\n        next_action = torch.tanh(pred)\n        y = 1 - next_action.pow(2) + 1e-06\n        next_log_prob = dist.log_prob(pred).unsqueeze(-1)\n        next_log_prob = next_log_prob - torch.log(y).sum(-1, keepdim=True)\n        next_data = {'obs': next_obs, 'action': next_action}\n        target_q_value = self._target_model.forward(next_data, mode='compute_critic')['q_value']\n        (target_q_value, _) = torch.min(target_q_value, dim=0)\n        if self._with_q_entropy:\n            target_q_value -= self._alpha * next_log_prob.squeeze(-1)\n        target_q_value = self._gamma * (1 - done) * target_q_value + reward\n    weight = data['weight']\n    if weight is None:\n        weight = torch.ones_like(q_value)\n    td_error_per_sample = nn.MSELoss(reduction='none')(q_value, target_q_value).mean(dim=1).sum()\n    loss_dict['critic_loss'] = (td_error_per_sample * weight).mean()\n    if self._eta > 0:\n        pre_obs = obs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0)\n        pre_acs = acs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0).requires_grad_(True)\n        q_pred_tile = self._learn_model.forward({'obs': pre_obs, 'action': pre_acs}, mode='compute_critic')['q_value'].requires_grad_(True)\n        q_pred_grads = torch.autograd.grad(q_pred_tile.sum(), pre_acs, retain_graph=True, create_graph=True)[0]\n        q_pred_grads = q_pred_grads / (torch.norm(q_pred_grads, p=2, dim=2).unsqueeze(-1) + 1e-10)\n        q_pred_grads = q_pred_grads.transpose(0, 1)\n        q_pred_grads = q_pred_grads @ q_pred_grads.permute(0, 2, 1)\n        masks = torch.eye(self._cfg.model.ensemble_num, device=obs.device).unsqueeze(dim=0).repeat(q_pred_grads.size(0), 1, 1)\n        q_pred_grads = (1 - masks) * q_pred_grads\n        grad_loss = torch.mean(torch.sum(q_pred_grads, dim=(1, 2))) / (self._cfg.model.ensemble_num - 1)\n        loss_dict['critic_loss'] += grad_loss * self._eta\n    self._optimizer_q.zero_grad()\n    loss_dict['critic_loss'].backward()\n    self._optimizer_q.step()\n    (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']\n    dist = Independent(Normal(mu, sigma), 1)\n    pred = dist.rsample()\n    action = torch.tanh(pred)\n    y = 1 - action.pow(2) + 1e-06\n    log_prob = dist.log_prob(pred).unsqueeze(-1)\n    log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)\n    eval_data = {'obs': obs, 'action': action}\n    new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']\n    (new_q_value, _) = torch.min(new_q_value, dim=0)\n    policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()\n    loss_dict['policy_loss'] = policy_loss\n    self._optimizer_policy.zero_grad()\n    loss_dict['policy_loss'].backward()\n    self._optimizer_policy.step()\n    if self._auto_alpha:\n        if self._log_space:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = self._log_alpha.detach().exp()\n        else:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = max(0, self._alpha)\n    loss_dict['total_loss'] = sum(loss_dict.values())\n    self._forward_learn_cnt += 1\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr_q': self._optimizer_q.defaults['lr'], 'cur_lr_p': self._optimizer_policy.defaults['lr'], 'priority': td_error_per_sample.abs().tolist(), 'td_error': td_error_per_sample.detach().mean().item(), 'alpha': self._alpha.item(), 'target_q_value': target_q_value.detach().mean().item(), **loss_dict}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    loss_dict = {}\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if len(data.get('action').shape) == 1:\n        data['action'] = data['action'].reshape(-1, 1)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    obs = data['obs']\n    next_obs = data['next_obs']\n    reward = data['reward']\n    done = data['done']\n    acs = data['action']\n    q_value = self._learn_model.forward(data, mode='compute_critic')['q_value']\n    with torch.no_grad():\n        (mu, sigma) = self._learn_model.forward(next_obs, mode='compute_actor')['logit']\n        dist = Independent(Normal(mu, sigma), 1)\n        pred = dist.rsample()\n        next_action = torch.tanh(pred)\n        y = 1 - next_action.pow(2) + 1e-06\n        next_log_prob = dist.log_prob(pred).unsqueeze(-1)\n        next_log_prob = next_log_prob - torch.log(y).sum(-1, keepdim=True)\n        next_data = {'obs': next_obs, 'action': next_action}\n        target_q_value = self._target_model.forward(next_data, mode='compute_critic')['q_value']\n        (target_q_value, _) = torch.min(target_q_value, dim=0)\n        if self._with_q_entropy:\n            target_q_value -= self._alpha * next_log_prob.squeeze(-1)\n        target_q_value = self._gamma * (1 - done) * target_q_value + reward\n    weight = data['weight']\n    if weight is None:\n        weight = torch.ones_like(q_value)\n    td_error_per_sample = nn.MSELoss(reduction='none')(q_value, target_q_value).mean(dim=1).sum()\n    loss_dict['critic_loss'] = (td_error_per_sample * weight).mean()\n    if self._eta > 0:\n        pre_obs = obs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0)\n        pre_acs = acs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0).requires_grad_(True)\n        q_pred_tile = self._learn_model.forward({'obs': pre_obs, 'action': pre_acs}, mode='compute_critic')['q_value'].requires_grad_(True)\n        q_pred_grads = torch.autograd.grad(q_pred_tile.sum(), pre_acs, retain_graph=True, create_graph=True)[0]\n        q_pred_grads = q_pred_grads / (torch.norm(q_pred_grads, p=2, dim=2).unsqueeze(-1) + 1e-10)\n        q_pred_grads = q_pred_grads.transpose(0, 1)\n        q_pred_grads = q_pred_grads @ q_pred_grads.permute(0, 2, 1)\n        masks = torch.eye(self._cfg.model.ensemble_num, device=obs.device).unsqueeze(dim=0).repeat(q_pred_grads.size(0), 1, 1)\n        q_pred_grads = (1 - masks) * q_pred_grads\n        grad_loss = torch.mean(torch.sum(q_pred_grads, dim=(1, 2))) / (self._cfg.model.ensemble_num - 1)\n        loss_dict['critic_loss'] += grad_loss * self._eta\n    self._optimizer_q.zero_grad()\n    loss_dict['critic_loss'].backward()\n    self._optimizer_q.step()\n    (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']\n    dist = Independent(Normal(mu, sigma), 1)\n    pred = dist.rsample()\n    action = torch.tanh(pred)\n    y = 1 - action.pow(2) + 1e-06\n    log_prob = dist.log_prob(pred).unsqueeze(-1)\n    log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)\n    eval_data = {'obs': obs, 'action': action}\n    new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']\n    (new_q_value, _) = torch.min(new_q_value, dim=0)\n    policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()\n    loss_dict['policy_loss'] = policy_loss\n    self._optimizer_policy.zero_grad()\n    loss_dict['policy_loss'].backward()\n    self._optimizer_policy.step()\n    if self._auto_alpha:\n        if self._log_space:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = self._log_alpha.detach().exp()\n        else:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = max(0, self._alpha)\n    loss_dict['total_loss'] = sum(loss_dict.values())\n    self._forward_learn_cnt += 1\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr_q': self._optimizer_q.defaults['lr'], 'cur_lr_p': self._optimizer_policy.defaults['lr'], 'priority': td_error_per_sample.abs().tolist(), 'td_error': td_error_per_sample.detach().mean().item(), 'alpha': self._alpha.item(), 'target_q_value': target_q_value.detach().mean().item(), **loss_dict}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    loss_dict = {}\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if len(data.get('action').shape) == 1:\n        data['action'] = data['action'].reshape(-1, 1)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    obs = data['obs']\n    next_obs = data['next_obs']\n    reward = data['reward']\n    done = data['done']\n    acs = data['action']\n    q_value = self._learn_model.forward(data, mode='compute_critic')['q_value']\n    with torch.no_grad():\n        (mu, sigma) = self._learn_model.forward(next_obs, mode='compute_actor')['logit']\n        dist = Independent(Normal(mu, sigma), 1)\n        pred = dist.rsample()\n        next_action = torch.tanh(pred)\n        y = 1 - next_action.pow(2) + 1e-06\n        next_log_prob = dist.log_prob(pred).unsqueeze(-1)\n        next_log_prob = next_log_prob - torch.log(y).sum(-1, keepdim=True)\n        next_data = {'obs': next_obs, 'action': next_action}\n        target_q_value = self._target_model.forward(next_data, mode='compute_critic')['q_value']\n        (target_q_value, _) = torch.min(target_q_value, dim=0)\n        if self._with_q_entropy:\n            target_q_value -= self._alpha * next_log_prob.squeeze(-1)\n        target_q_value = self._gamma * (1 - done) * target_q_value + reward\n    weight = data['weight']\n    if weight is None:\n        weight = torch.ones_like(q_value)\n    td_error_per_sample = nn.MSELoss(reduction='none')(q_value, target_q_value).mean(dim=1).sum()\n    loss_dict['critic_loss'] = (td_error_per_sample * weight).mean()\n    if self._eta > 0:\n        pre_obs = obs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0)\n        pre_acs = acs.unsqueeze(0).repeat_interleave(self._cfg.model.ensemble_num, dim=0).requires_grad_(True)\n        q_pred_tile = self._learn_model.forward({'obs': pre_obs, 'action': pre_acs}, mode='compute_critic')['q_value'].requires_grad_(True)\n        q_pred_grads = torch.autograd.grad(q_pred_tile.sum(), pre_acs, retain_graph=True, create_graph=True)[0]\n        q_pred_grads = q_pred_grads / (torch.norm(q_pred_grads, p=2, dim=2).unsqueeze(-1) + 1e-10)\n        q_pred_grads = q_pred_grads.transpose(0, 1)\n        q_pred_grads = q_pred_grads @ q_pred_grads.permute(0, 2, 1)\n        masks = torch.eye(self._cfg.model.ensemble_num, device=obs.device).unsqueeze(dim=0).repeat(q_pred_grads.size(0), 1, 1)\n        q_pred_grads = (1 - masks) * q_pred_grads\n        grad_loss = torch.mean(torch.sum(q_pred_grads, dim=(1, 2))) / (self._cfg.model.ensemble_num - 1)\n        loss_dict['critic_loss'] += grad_loss * self._eta\n    self._optimizer_q.zero_grad()\n    loss_dict['critic_loss'].backward()\n    self._optimizer_q.step()\n    (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']\n    dist = Independent(Normal(mu, sigma), 1)\n    pred = dist.rsample()\n    action = torch.tanh(pred)\n    y = 1 - action.pow(2) + 1e-06\n    log_prob = dist.log_prob(pred).unsqueeze(-1)\n    log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)\n    eval_data = {'obs': obs, 'action': action}\n    new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']\n    (new_q_value, _) = torch.min(new_q_value, dim=0)\n    policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()\n    loss_dict['policy_loss'] = policy_loss\n    self._optimizer_policy.zero_grad()\n    loss_dict['policy_loss'].backward()\n    self._optimizer_policy.step()\n    if self._auto_alpha:\n        if self._log_space:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = self._log_alpha.detach().exp()\n        else:\n            log_prob = log_prob + self._target_entropy\n            loss_dict['alpha_loss'] = -(self._alpha * log_prob.detach()).mean()\n            self._alpha_optim.zero_grad()\n            loss_dict['alpha_loss'].backward()\n            self._alpha_optim.step()\n            self._alpha = max(0, self._alpha)\n    loss_dict['total_loss'] = sum(loss_dict.values())\n    self._forward_learn_cnt += 1\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr_q': self._optimizer_q.defaults['lr'], 'cur_lr_p': self._optimizer_policy.defaults['lr'], 'priority': td_error_per_sample.abs().tolist(), 'td_error': td_error_per_sample.detach().mean().item(), 'alpha': self._alpha.item(), 'target_q_value': target_q_value.detach().mean().item(), **loss_dict}"
        ]
    }
]