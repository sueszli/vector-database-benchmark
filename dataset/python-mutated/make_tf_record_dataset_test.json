[
    {
        "func_name": "testRead",
        "original": "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], file_index=[None, 1], num_parallel_reads=[1, 8], drop_final_batch=[False, True], parser_fn=[True, False])))\ndef testRead(self, batch_size, num_epochs, file_index, num_parallel_reads, drop_final_batch, parser_fn):\n    if file_index is None:\n        file_pattern = self._filenames\n    else:\n        file_pattern = self._filenames[file_index]\n    if parser_fn:\n        fn = lambda x: string_ops.substr(x, 1, 999)\n    else:\n        fn = None\n    outputs = self.getNext(readers.make_tf_record_dataset(file_pattern=file_pattern, num_epochs=num_epochs, batch_size=batch_size, parser_fn=fn, num_parallel_reads=num_parallel_reads, drop_final_batch=drop_final_batch, shuffle=False))\n    self._verify_records(outputs, batch_size, file_index, num_epochs=num_epochs, interleave_cycle_length=num_parallel_reads, drop_final_batch=drop_final_batch, use_parser_fn=parser_fn)\n    with self.assertRaises(errors.OutOfRangeError):\n        self.evaluate(outputs())",
        "mutated": [
            "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], file_index=[None, 1], num_parallel_reads=[1, 8], drop_final_batch=[False, True], parser_fn=[True, False])))\ndef testRead(self, batch_size, num_epochs, file_index, num_parallel_reads, drop_final_batch, parser_fn):\n    if False:\n        i = 10\n    if file_index is None:\n        file_pattern = self._filenames\n    else:\n        file_pattern = self._filenames[file_index]\n    if parser_fn:\n        fn = lambda x: string_ops.substr(x, 1, 999)\n    else:\n        fn = None\n    outputs = self.getNext(readers.make_tf_record_dataset(file_pattern=file_pattern, num_epochs=num_epochs, batch_size=batch_size, parser_fn=fn, num_parallel_reads=num_parallel_reads, drop_final_batch=drop_final_batch, shuffle=False))\n    self._verify_records(outputs, batch_size, file_index, num_epochs=num_epochs, interleave_cycle_length=num_parallel_reads, drop_final_batch=drop_final_batch, use_parser_fn=parser_fn)\n    with self.assertRaises(errors.OutOfRangeError):\n        self.evaluate(outputs())",
            "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], file_index=[None, 1], num_parallel_reads=[1, 8], drop_final_batch=[False, True], parser_fn=[True, False])))\ndef testRead(self, batch_size, num_epochs, file_index, num_parallel_reads, drop_final_batch, parser_fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if file_index is None:\n        file_pattern = self._filenames\n    else:\n        file_pattern = self._filenames[file_index]\n    if parser_fn:\n        fn = lambda x: string_ops.substr(x, 1, 999)\n    else:\n        fn = None\n    outputs = self.getNext(readers.make_tf_record_dataset(file_pattern=file_pattern, num_epochs=num_epochs, batch_size=batch_size, parser_fn=fn, num_parallel_reads=num_parallel_reads, drop_final_batch=drop_final_batch, shuffle=False))\n    self._verify_records(outputs, batch_size, file_index, num_epochs=num_epochs, interleave_cycle_length=num_parallel_reads, drop_final_batch=drop_final_batch, use_parser_fn=parser_fn)\n    with self.assertRaises(errors.OutOfRangeError):\n        self.evaluate(outputs())",
            "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], file_index=[None, 1], num_parallel_reads=[1, 8], drop_final_batch=[False, True], parser_fn=[True, False])))\ndef testRead(self, batch_size, num_epochs, file_index, num_parallel_reads, drop_final_batch, parser_fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if file_index is None:\n        file_pattern = self._filenames\n    else:\n        file_pattern = self._filenames[file_index]\n    if parser_fn:\n        fn = lambda x: string_ops.substr(x, 1, 999)\n    else:\n        fn = None\n    outputs = self.getNext(readers.make_tf_record_dataset(file_pattern=file_pattern, num_epochs=num_epochs, batch_size=batch_size, parser_fn=fn, num_parallel_reads=num_parallel_reads, drop_final_batch=drop_final_batch, shuffle=False))\n    self._verify_records(outputs, batch_size, file_index, num_epochs=num_epochs, interleave_cycle_length=num_parallel_reads, drop_final_batch=drop_final_batch, use_parser_fn=parser_fn)\n    with self.assertRaises(errors.OutOfRangeError):\n        self.evaluate(outputs())",
            "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], file_index=[None, 1], num_parallel_reads=[1, 8], drop_final_batch=[False, True], parser_fn=[True, False])))\ndef testRead(self, batch_size, num_epochs, file_index, num_parallel_reads, drop_final_batch, parser_fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if file_index is None:\n        file_pattern = self._filenames\n    else:\n        file_pattern = self._filenames[file_index]\n    if parser_fn:\n        fn = lambda x: string_ops.substr(x, 1, 999)\n    else:\n        fn = None\n    outputs = self.getNext(readers.make_tf_record_dataset(file_pattern=file_pattern, num_epochs=num_epochs, batch_size=batch_size, parser_fn=fn, num_parallel_reads=num_parallel_reads, drop_final_batch=drop_final_batch, shuffle=False))\n    self._verify_records(outputs, batch_size, file_index, num_epochs=num_epochs, interleave_cycle_length=num_parallel_reads, drop_final_batch=drop_final_batch, use_parser_fn=parser_fn)\n    with self.assertRaises(errors.OutOfRangeError):\n        self.evaluate(outputs())",
            "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], file_index=[None, 1], num_parallel_reads=[1, 8], drop_final_batch=[False, True], parser_fn=[True, False])))\ndef testRead(self, batch_size, num_epochs, file_index, num_parallel_reads, drop_final_batch, parser_fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if file_index is None:\n        file_pattern = self._filenames\n    else:\n        file_pattern = self._filenames[file_index]\n    if parser_fn:\n        fn = lambda x: string_ops.substr(x, 1, 999)\n    else:\n        fn = None\n    outputs = self.getNext(readers.make_tf_record_dataset(file_pattern=file_pattern, num_epochs=num_epochs, batch_size=batch_size, parser_fn=fn, num_parallel_reads=num_parallel_reads, drop_final_batch=drop_final_batch, shuffle=False))\n    self._verify_records(outputs, batch_size, file_index, num_epochs=num_epochs, interleave_cycle_length=num_parallel_reads, drop_final_batch=drop_final_batch, use_parser_fn=parser_fn)\n    with self.assertRaises(errors.OutOfRangeError):\n        self.evaluate(outputs())"
        ]
    },
    {
        "func_name": "dataset_fn",
        "original": "def dataset_fn():\n    return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)",
        "mutated": [
            "def dataset_fn():\n    if False:\n        i = 10\n    return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)",
            "def dataset_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)",
            "def dataset_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)",
            "def dataset_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)",
            "def dataset_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)"
        ]
    },
    {
        "func_name": "testShuffle",
        "original": "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], num_parallel_reads=[1, 2], seed=[None, 21345])))\ndef testShuffle(self, batch_size, num_epochs, num_parallel_reads, seed):\n\n    def dataset_fn():\n        return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)\n    next_element = self.getNext(dataset_fn())\n    first_batches = []\n    try:\n        while True:\n            first_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    next_element = self.getNext(dataset_fn())\n    second_batches = []\n    try:\n        while True:\n            second_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    self.assertEqual(len(first_batches), len(second_batches))\n    if seed is not None:\n        for i in range(len(first_batches)):\n            self.assertAllEqual(first_batches[i], second_batches[i])\n    expected = []\n    for f in range(self._num_files):\n        for r in range(self._num_records):\n            expected.extend([self._record(f, r)] * num_epochs)\n    for batches in (first_batches, second_batches):\n        actual = []\n        for b in batches:\n            actual.extend(b)\n        self.assertAllEqual(sorted(expected), sorted(actual))",
        "mutated": [
            "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], num_parallel_reads=[1, 2], seed=[None, 21345])))\ndef testShuffle(self, batch_size, num_epochs, num_parallel_reads, seed):\n    if False:\n        i = 10\n\n    def dataset_fn():\n        return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)\n    next_element = self.getNext(dataset_fn())\n    first_batches = []\n    try:\n        while True:\n            first_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    next_element = self.getNext(dataset_fn())\n    second_batches = []\n    try:\n        while True:\n            second_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    self.assertEqual(len(first_batches), len(second_batches))\n    if seed is not None:\n        for i in range(len(first_batches)):\n            self.assertAllEqual(first_batches[i], second_batches[i])\n    expected = []\n    for f in range(self._num_files):\n        for r in range(self._num_records):\n            expected.extend([self._record(f, r)] * num_epochs)\n    for batches in (first_batches, second_batches):\n        actual = []\n        for b in batches:\n            actual.extend(b)\n        self.assertAllEqual(sorted(expected), sorted(actual))",
            "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], num_parallel_reads=[1, 2], seed=[None, 21345])))\ndef testShuffle(self, batch_size, num_epochs, num_parallel_reads, seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def dataset_fn():\n        return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)\n    next_element = self.getNext(dataset_fn())\n    first_batches = []\n    try:\n        while True:\n            first_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    next_element = self.getNext(dataset_fn())\n    second_batches = []\n    try:\n        while True:\n            second_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    self.assertEqual(len(first_batches), len(second_batches))\n    if seed is not None:\n        for i in range(len(first_batches)):\n            self.assertAllEqual(first_batches[i], second_batches[i])\n    expected = []\n    for f in range(self._num_files):\n        for r in range(self._num_records):\n            expected.extend([self._record(f, r)] * num_epochs)\n    for batches in (first_batches, second_batches):\n        actual = []\n        for b in batches:\n            actual.extend(b)\n        self.assertAllEqual(sorted(expected), sorted(actual))",
            "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], num_parallel_reads=[1, 2], seed=[None, 21345])))\ndef testShuffle(self, batch_size, num_epochs, num_parallel_reads, seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def dataset_fn():\n        return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)\n    next_element = self.getNext(dataset_fn())\n    first_batches = []\n    try:\n        while True:\n            first_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    next_element = self.getNext(dataset_fn())\n    second_batches = []\n    try:\n        while True:\n            second_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    self.assertEqual(len(first_batches), len(second_batches))\n    if seed is not None:\n        for i in range(len(first_batches)):\n            self.assertAllEqual(first_batches[i], second_batches[i])\n    expected = []\n    for f in range(self._num_files):\n        for r in range(self._num_records):\n            expected.extend([self._record(f, r)] * num_epochs)\n    for batches in (first_batches, second_batches):\n        actual = []\n        for b in batches:\n            actual.extend(b)\n        self.assertAllEqual(sorted(expected), sorted(actual))",
            "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], num_parallel_reads=[1, 2], seed=[None, 21345])))\ndef testShuffle(self, batch_size, num_epochs, num_parallel_reads, seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def dataset_fn():\n        return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)\n    next_element = self.getNext(dataset_fn())\n    first_batches = []\n    try:\n        while True:\n            first_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    next_element = self.getNext(dataset_fn())\n    second_batches = []\n    try:\n        while True:\n            second_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    self.assertEqual(len(first_batches), len(second_batches))\n    if seed is not None:\n        for i in range(len(first_batches)):\n            self.assertAllEqual(first_batches[i], second_batches[i])\n    expected = []\n    for f in range(self._num_files):\n        for r in range(self._num_records):\n            expected.extend([self._record(f, r)] * num_epochs)\n    for batches in (first_batches, second_batches):\n        actual = []\n        for b in batches:\n            actual.extend(b)\n        self.assertAllEqual(sorted(expected), sorted(actual))",
            "@combinations.generate(combinations.times(test_base.default_test_combinations(), combinations.combine(batch_size=[1, 2], num_epochs=[1, 3], num_parallel_reads=[1, 2], seed=[None, 21345])))\ndef testShuffle(self, batch_size, num_epochs, num_parallel_reads, seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def dataset_fn():\n        return readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=num_epochs, batch_size=batch_size, num_parallel_reads=num_parallel_reads, shuffle=True, shuffle_seed=seed)\n    next_element = self.getNext(dataset_fn())\n    first_batches = []\n    try:\n        while True:\n            first_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    next_element = self.getNext(dataset_fn())\n    second_batches = []\n    try:\n        while True:\n            second_batches.append(self.evaluate(next_element()))\n    except errors.OutOfRangeError:\n        pass\n    self.assertEqual(len(first_batches), len(second_batches))\n    if seed is not None:\n        for i in range(len(first_batches)):\n            self.assertAllEqual(first_batches[i], second_batches[i])\n    expected = []\n    for f in range(self._num_files):\n        for r in range(self._num_records):\n            expected.extend([self._record(f, r)] * num_epochs)\n    for batches in (first_batches, second_batches):\n        actual = []\n        for b in batches:\n            actual.extend(b)\n        self.assertAllEqual(sorted(expected), sorted(actual))"
        ]
    },
    {
        "func_name": "testIndefiniteRepeatShapeInference",
        "original": "@combinations.generate(test_base.default_test_combinations())\ndef testIndefiniteRepeatShapeInference(self):\n    dataset = readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=None, batch_size=32)\n    for shape in nest.flatten(dataset_ops.get_legacy_output_shapes(dataset)):\n        self.assertEqual(32, shape[0])",
        "mutated": [
            "@combinations.generate(test_base.default_test_combinations())\ndef testIndefiniteRepeatShapeInference(self):\n    if False:\n        i = 10\n    dataset = readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=None, batch_size=32)\n    for shape in nest.flatten(dataset_ops.get_legacy_output_shapes(dataset)):\n        self.assertEqual(32, shape[0])",
            "@combinations.generate(test_base.default_test_combinations())\ndef testIndefiniteRepeatShapeInference(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dataset = readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=None, batch_size=32)\n    for shape in nest.flatten(dataset_ops.get_legacy_output_shapes(dataset)):\n        self.assertEqual(32, shape[0])",
            "@combinations.generate(test_base.default_test_combinations())\ndef testIndefiniteRepeatShapeInference(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dataset = readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=None, batch_size=32)\n    for shape in nest.flatten(dataset_ops.get_legacy_output_shapes(dataset)):\n        self.assertEqual(32, shape[0])",
            "@combinations.generate(test_base.default_test_combinations())\ndef testIndefiniteRepeatShapeInference(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dataset = readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=None, batch_size=32)\n    for shape in nest.flatten(dataset_ops.get_legacy_output_shapes(dataset)):\n        self.assertEqual(32, shape[0])",
            "@combinations.generate(test_base.default_test_combinations())\ndef testIndefiniteRepeatShapeInference(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dataset = readers.make_tf_record_dataset(file_pattern=self._filenames, num_epochs=None, batch_size=32)\n    for shape in nest.flatten(dataset_ops.get_legacy_output_shapes(dataset)):\n        self.assertEqual(32, shape[0])"
        ]
    }
]