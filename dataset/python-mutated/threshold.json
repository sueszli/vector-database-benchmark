[
    {
        "func_name": "__init__",
        "original": "def __init__(self, estimator, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, **kwargs):\n    if not force_model and (not is_classifier(estimator) or not is_probabilistic(estimator)):\n        raise YellowbrickTypeError('{} requires a probabilistic binary classifier'.format(self.__class__.__name__))\n    self._check_quantiles(quantiles)\n    self._check_cv(cv)\n    self._check_exclude(exclude)\n    self._check_argmax(argmax, exclude)\n    super(DiscriminationThreshold, self).__init__(estimator, ax=ax, is_fitted=is_fitted, **kwargs)\n    self.n_trials = n_trials\n    self.cv = cv\n    self.fbeta = fbeta\n    self.argmax = argmax\n    self.exclude = exclude\n    self.quantiles = quantiles\n    self.random_state = random_state",
        "mutated": [
            "def __init__(self, estimator, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, **kwargs):\n    if False:\n        i = 10\n    if not force_model and (not is_classifier(estimator) or not is_probabilistic(estimator)):\n        raise YellowbrickTypeError('{} requires a probabilistic binary classifier'.format(self.__class__.__name__))\n    self._check_quantiles(quantiles)\n    self._check_cv(cv)\n    self._check_exclude(exclude)\n    self._check_argmax(argmax, exclude)\n    super(DiscriminationThreshold, self).__init__(estimator, ax=ax, is_fitted=is_fitted, **kwargs)\n    self.n_trials = n_trials\n    self.cv = cv\n    self.fbeta = fbeta\n    self.argmax = argmax\n    self.exclude = exclude\n    self.quantiles = quantiles\n    self.random_state = random_state",
            "def __init__(self, estimator, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not force_model and (not is_classifier(estimator) or not is_probabilistic(estimator)):\n        raise YellowbrickTypeError('{} requires a probabilistic binary classifier'.format(self.__class__.__name__))\n    self._check_quantiles(quantiles)\n    self._check_cv(cv)\n    self._check_exclude(exclude)\n    self._check_argmax(argmax, exclude)\n    super(DiscriminationThreshold, self).__init__(estimator, ax=ax, is_fitted=is_fitted, **kwargs)\n    self.n_trials = n_trials\n    self.cv = cv\n    self.fbeta = fbeta\n    self.argmax = argmax\n    self.exclude = exclude\n    self.quantiles = quantiles\n    self.random_state = random_state",
            "def __init__(self, estimator, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not force_model and (not is_classifier(estimator) or not is_probabilistic(estimator)):\n        raise YellowbrickTypeError('{} requires a probabilistic binary classifier'.format(self.__class__.__name__))\n    self._check_quantiles(quantiles)\n    self._check_cv(cv)\n    self._check_exclude(exclude)\n    self._check_argmax(argmax, exclude)\n    super(DiscriminationThreshold, self).__init__(estimator, ax=ax, is_fitted=is_fitted, **kwargs)\n    self.n_trials = n_trials\n    self.cv = cv\n    self.fbeta = fbeta\n    self.argmax = argmax\n    self.exclude = exclude\n    self.quantiles = quantiles\n    self.random_state = random_state",
            "def __init__(self, estimator, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not force_model and (not is_classifier(estimator) or not is_probabilistic(estimator)):\n        raise YellowbrickTypeError('{} requires a probabilistic binary classifier'.format(self.__class__.__name__))\n    self._check_quantiles(quantiles)\n    self._check_cv(cv)\n    self._check_exclude(exclude)\n    self._check_argmax(argmax, exclude)\n    super(DiscriminationThreshold, self).__init__(estimator, ax=ax, is_fitted=is_fitted, **kwargs)\n    self.n_trials = n_trials\n    self.cv = cv\n    self.fbeta = fbeta\n    self.argmax = argmax\n    self.exclude = exclude\n    self.quantiles = quantiles\n    self.random_state = random_state",
            "def __init__(self, estimator, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not force_model and (not is_classifier(estimator) or not is_probabilistic(estimator)):\n        raise YellowbrickTypeError('{} requires a probabilistic binary classifier'.format(self.__class__.__name__))\n    self._check_quantiles(quantiles)\n    self._check_cv(cv)\n    self._check_exclude(exclude)\n    self._check_argmax(argmax, exclude)\n    super(DiscriminationThreshold, self).__init__(estimator, ax=ax, is_fitted=is_fitted, **kwargs)\n    self.n_trials = n_trials\n    self.cv = cv\n    self.fbeta = fbeta\n    self.argmax = argmax\n    self.exclude = exclude\n    self.quantiles = quantiles\n    self.random_state = random_state"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, X, y, **kwargs):\n    \"\"\"\n        Fit is the entry point for the visualizer. Given instances described\n        by X and binary classes described in the target y, fit performs n\n        trials by shuffling and splitting the dataset then computing the\n        precision, recall, f1, and queue rate scores for each trial. The\n        scores are aggregated by the quantiles expressed then drawn.\n\n        Parameters\n        ----------\n        X : ndarray or DataFrame of shape n x m\n            A matrix of n instances with m features\n\n        y : ndarray or Series of length n\n            An array or series of target or class values. The target y must\n            be a binary classification target.\n\n        kwargs: dict\n            keyword arguments passed to Scikit-Learn API.\n\n        Returns\n        -------\n        self : instance\n            Returns the instance of the visualizer\n\n        raises: YellowbrickValueError\n            If the target y is not a binary classification target.\n        \"\"\"\n    if type_of_target(y) != 'binary':\n        raise YellowbrickValueError('multiclass format is not supported')\n    (X, y) = indexable(X, y)\n    trials = [metric for idx in range(self.n_trials) for metric in self._split_fit_score_trial(X, y, idx)]\n    n_thresholds = np.array([len(t['thresholds']) for t in trials]).min()\n    self.thresholds_ = np.linspace(0.0, 1.0, num=n_thresholds)\n    metrics = frozenset(METRICS) - self._check_exclude(self.exclude)\n    uniform_metrics = defaultdict(list)\n    for trial in trials:\n        rows = defaultdict(list)\n        for t in self.thresholds_:\n            idx = bisect.bisect_left(trial['thresholds'], t)\n            for metric in metrics:\n                rows[metric].append(trial[metric][idx])\n        for (metric, row) in rows.items():\n            uniform_metrics[metric].append(row)\n    uniform_metrics = {metric: np.array(values) for (metric, values) in uniform_metrics.items()}\n    quantiles = self._check_quantiles(self.quantiles)\n    self.cv_scores_ = {}\n    for (metric, values) in uniform_metrics.items():\n        (lower, median, upper) = mstats.mquantiles(values, prob=quantiles, axis=0)\n        self.cv_scores_[metric] = median\n        self.cv_scores_['{}_lower'.format(metric)] = lower\n        self.cv_scores_['{}_upper'.format(metric)] = upper\n    super(DiscriminationThreshold, self).fit(X, y)\n    self.draw()\n    return self",
        "mutated": [
            "def fit(self, X, y, **kwargs):\n    if False:\n        i = 10\n    '\\n        Fit is the entry point for the visualizer. Given instances described\\n        by X and binary classes described in the target y, fit performs n\\n        trials by shuffling and splitting the dataset then computing the\\n        precision, recall, f1, and queue rate scores for each trial. The\\n        scores are aggregated by the quantiles expressed then drawn.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features\\n\\n        y : ndarray or Series of length n\\n            An array or series of target or class values. The target y must\\n            be a binary classification target.\\n\\n        kwargs: dict\\n            keyword arguments passed to Scikit-Learn API.\\n\\n        Returns\\n        -------\\n        self : instance\\n            Returns the instance of the visualizer\\n\\n        raises: YellowbrickValueError\\n            If the target y is not a binary classification target.\\n        '\n    if type_of_target(y) != 'binary':\n        raise YellowbrickValueError('multiclass format is not supported')\n    (X, y) = indexable(X, y)\n    trials = [metric for idx in range(self.n_trials) for metric in self._split_fit_score_trial(X, y, idx)]\n    n_thresholds = np.array([len(t['thresholds']) for t in trials]).min()\n    self.thresholds_ = np.linspace(0.0, 1.0, num=n_thresholds)\n    metrics = frozenset(METRICS) - self._check_exclude(self.exclude)\n    uniform_metrics = defaultdict(list)\n    for trial in trials:\n        rows = defaultdict(list)\n        for t in self.thresholds_:\n            idx = bisect.bisect_left(trial['thresholds'], t)\n            for metric in metrics:\n                rows[metric].append(trial[metric][idx])\n        for (metric, row) in rows.items():\n            uniform_metrics[metric].append(row)\n    uniform_metrics = {metric: np.array(values) for (metric, values) in uniform_metrics.items()}\n    quantiles = self._check_quantiles(self.quantiles)\n    self.cv_scores_ = {}\n    for (metric, values) in uniform_metrics.items():\n        (lower, median, upper) = mstats.mquantiles(values, prob=quantiles, axis=0)\n        self.cv_scores_[metric] = median\n        self.cv_scores_['{}_lower'.format(metric)] = lower\n        self.cv_scores_['{}_upper'.format(metric)] = upper\n    super(DiscriminationThreshold, self).fit(X, y)\n    self.draw()\n    return self",
            "def fit(self, X, y, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fit is the entry point for the visualizer. Given instances described\\n        by X and binary classes described in the target y, fit performs n\\n        trials by shuffling and splitting the dataset then computing the\\n        precision, recall, f1, and queue rate scores for each trial. The\\n        scores are aggregated by the quantiles expressed then drawn.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features\\n\\n        y : ndarray or Series of length n\\n            An array or series of target or class values. The target y must\\n            be a binary classification target.\\n\\n        kwargs: dict\\n            keyword arguments passed to Scikit-Learn API.\\n\\n        Returns\\n        -------\\n        self : instance\\n            Returns the instance of the visualizer\\n\\n        raises: YellowbrickValueError\\n            If the target y is not a binary classification target.\\n        '\n    if type_of_target(y) != 'binary':\n        raise YellowbrickValueError('multiclass format is not supported')\n    (X, y) = indexable(X, y)\n    trials = [metric for idx in range(self.n_trials) for metric in self._split_fit_score_trial(X, y, idx)]\n    n_thresholds = np.array([len(t['thresholds']) for t in trials]).min()\n    self.thresholds_ = np.linspace(0.0, 1.0, num=n_thresholds)\n    metrics = frozenset(METRICS) - self._check_exclude(self.exclude)\n    uniform_metrics = defaultdict(list)\n    for trial in trials:\n        rows = defaultdict(list)\n        for t in self.thresholds_:\n            idx = bisect.bisect_left(trial['thresholds'], t)\n            for metric in metrics:\n                rows[metric].append(trial[metric][idx])\n        for (metric, row) in rows.items():\n            uniform_metrics[metric].append(row)\n    uniform_metrics = {metric: np.array(values) for (metric, values) in uniform_metrics.items()}\n    quantiles = self._check_quantiles(self.quantiles)\n    self.cv_scores_ = {}\n    for (metric, values) in uniform_metrics.items():\n        (lower, median, upper) = mstats.mquantiles(values, prob=quantiles, axis=0)\n        self.cv_scores_[metric] = median\n        self.cv_scores_['{}_lower'.format(metric)] = lower\n        self.cv_scores_['{}_upper'.format(metric)] = upper\n    super(DiscriminationThreshold, self).fit(X, y)\n    self.draw()\n    return self",
            "def fit(self, X, y, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fit is the entry point for the visualizer. Given instances described\\n        by X and binary classes described in the target y, fit performs n\\n        trials by shuffling and splitting the dataset then computing the\\n        precision, recall, f1, and queue rate scores for each trial. The\\n        scores are aggregated by the quantiles expressed then drawn.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features\\n\\n        y : ndarray or Series of length n\\n            An array or series of target or class values. The target y must\\n            be a binary classification target.\\n\\n        kwargs: dict\\n            keyword arguments passed to Scikit-Learn API.\\n\\n        Returns\\n        -------\\n        self : instance\\n            Returns the instance of the visualizer\\n\\n        raises: YellowbrickValueError\\n            If the target y is not a binary classification target.\\n        '\n    if type_of_target(y) != 'binary':\n        raise YellowbrickValueError('multiclass format is not supported')\n    (X, y) = indexable(X, y)\n    trials = [metric for idx in range(self.n_trials) for metric in self._split_fit_score_trial(X, y, idx)]\n    n_thresholds = np.array([len(t['thresholds']) for t in trials]).min()\n    self.thresholds_ = np.linspace(0.0, 1.0, num=n_thresholds)\n    metrics = frozenset(METRICS) - self._check_exclude(self.exclude)\n    uniform_metrics = defaultdict(list)\n    for trial in trials:\n        rows = defaultdict(list)\n        for t in self.thresholds_:\n            idx = bisect.bisect_left(trial['thresholds'], t)\n            for metric in metrics:\n                rows[metric].append(trial[metric][idx])\n        for (metric, row) in rows.items():\n            uniform_metrics[metric].append(row)\n    uniform_metrics = {metric: np.array(values) for (metric, values) in uniform_metrics.items()}\n    quantiles = self._check_quantiles(self.quantiles)\n    self.cv_scores_ = {}\n    for (metric, values) in uniform_metrics.items():\n        (lower, median, upper) = mstats.mquantiles(values, prob=quantiles, axis=0)\n        self.cv_scores_[metric] = median\n        self.cv_scores_['{}_lower'.format(metric)] = lower\n        self.cv_scores_['{}_upper'.format(metric)] = upper\n    super(DiscriminationThreshold, self).fit(X, y)\n    self.draw()\n    return self",
            "def fit(self, X, y, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fit is the entry point for the visualizer. Given instances described\\n        by X and binary classes described in the target y, fit performs n\\n        trials by shuffling and splitting the dataset then computing the\\n        precision, recall, f1, and queue rate scores for each trial. The\\n        scores are aggregated by the quantiles expressed then drawn.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features\\n\\n        y : ndarray or Series of length n\\n            An array or series of target or class values. The target y must\\n            be a binary classification target.\\n\\n        kwargs: dict\\n            keyword arguments passed to Scikit-Learn API.\\n\\n        Returns\\n        -------\\n        self : instance\\n            Returns the instance of the visualizer\\n\\n        raises: YellowbrickValueError\\n            If the target y is not a binary classification target.\\n        '\n    if type_of_target(y) != 'binary':\n        raise YellowbrickValueError('multiclass format is not supported')\n    (X, y) = indexable(X, y)\n    trials = [metric for idx in range(self.n_trials) for metric in self._split_fit_score_trial(X, y, idx)]\n    n_thresholds = np.array([len(t['thresholds']) for t in trials]).min()\n    self.thresholds_ = np.linspace(0.0, 1.0, num=n_thresholds)\n    metrics = frozenset(METRICS) - self._check_exclude(self.exclude)\n    uniform_metrics = defaultdict(list)\n    for trial in trials:\n        rows = defaultdict(list)\n        for t in self.thresholds_:\n            idx = bisect.bisect_left(trial['thresholds'], t)\n            for metric in metrics:\n                rows[metric].append(trial[metric][idx])\n        for (metric, row) in rows.items():\n            uniform_metrics[metric].append(row)\n    uniform_metrics = {metric: np.array(values) for (metric, values) in uniform_metrics.items()}\n    quantiles = self._check_quantiles(self.quantiles)\n    self.cv_scores_ = {}\n    for (metric, values) in uniform_metrics.items():\n        (lower, median, upper) = mstats.mquantiles(values, prob=quantiles, axis=0)\n        self.cv_scores_[metric] = median\n        self.cv_scores_['{}_lower'.format(metric)] = lower\n        self.cv_scores_['{}_upper'.format(metric)] = upper\n    super(DiscriminationThreshold, self).fit(X, y)\n    self.draw()\n    return self",
            "def fit(self, X, y, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fit is the entry point for the visualizer. Given instances described\\n        by X and binary classes described in the target y, fit performs n\\n        trials by shuffling and splitting the dataset then computing the\\n        precision, recall, f1, and queue rate scores for each trial. The\\n        scores are aggregated by the quantiles expressed then drawn.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features\\n\\n        y : ndarray or Series of length n\\n            An array or series of target or class values. The target y must\\n            be a binary classification target.\\n\\n        kwargs: dict\\n            keyword arguments passed to Scikit-Learn API.\\n\\n        Returns\\n        -------\\n        self : instance\\n            Returns the instance of the visualizer\\n\\n        raises: YellowbrickValueError\\n            If the target y is not a binary classification target.\\n        '\n    if type_of_target(y) != 'binary':\n        raise YellowbrickValueError('multiclass format is not supported')\n    (X, y) = indexable(X, y)\n    trials = [metric for idx in range(self.n_trials) for metric in self._split_fit_score_trial(X, y, idx)]\n    n_thresholds = np.array([len(t['thresholds']) for t in trials]).min()\n    self.thresholds_ = np.linspace(0.0, 1.0, num=n_thresholds)\n    metrics = frozenset(METRICS) - self._check_exclude(self.exclude)\n    uniform_metrics = defaultdict(list)\n    for trial in trials:\n        rows = defaultdict(list)\n        for t in self.thresholds_:\n            idx = bisect.bisect_left(trial['thresholds'], t)\n            for metric in metrics:\n                rows[metric].append(trial[metric][idx])\n        for (metric, row) in rows.items():\n            uniform_metrics[metric].append(row)\n    uniform_metrics = {metric: np.array(values) for (metric, values) in uniform_metrics.items()}\n    quantiles = self._check_quantiles(self.quantiles)\n    self.cv_scores_ = {}\n    for (metric, values) in uniform_metrics.items():\n        (lower, median, upper) = mstats.mquantiles(values, prob=quantiles, axis=0)\n        self.cv_scores_[metric] = median\n        self.cv_scores_['{}_lower'.format(metric)] = lower\n        self.cv_scores_['{}_upper'.format(metric)] = upper\n    super(DiscriminationThreshold, self).fit(X, y)\n    self.draw()\n    return self"
        ]
    },
    {
        "func_name": "_split_fit_score_trial",
        "original": "def _split_fit_score_trial(self, X, y, idx=0):\n    \"\"\"\n        Splits the dataset, fits a clone of the estimator, then scores it\n        according to the required metrics.\n\n        The index of the split is added to the random_state if the\n        random_state is not None; this ensures that every split is shuffled\n        differently but in a deterministic fashion for testing purposes.\n        \"\"\"\n    random_state = self.random_state\n    if random_state is not None:\n        random_state += idx\n    splitter = self._check_cv(self.cv, random_state)\n    for (train_index, test_index) in splitter.split(X, y):\n        X_train = _safe_indexing(X, train_index)\n        y_train = _safe_indexing(y, train_index)\n        X_test = _safe_indexing(X, test_index)\n        y_test = _safe_indexing(y, test_index)\n        model = clone(self.estimator)\n        model.fit(X_train, y_train)\n        if hasattr(model, 'predict_proba'):\n            y_scores = model.predict_proba(X_test)[:, 1]\n        else:\n            y_scores = model.decision_function(X_test)\n        curve_metrics = precision_recall_curve(y_test, y_scores)\n        (precision, recall, thresholds) = curve_metrics\n        with np.errstate(divide='ignore', invalid='ignore'):\n            beta = self.fbeta ** 2\n            f_score = (1 + beta) * precision * recall / (beta * precision + recall)\n        thresholds = np.append(thresholds, 1)\n        queue_rate = np.array([(y_scores >= threshold).mean() for threshold in thresholds])\n        yield {'thresholds': thresholds, 'precision': precision, 'recall': recall, 'fscore': f_score, 'queue_rate': queue_rate}",
        "mutated": [
            "def _split_fit_score_trial(self, X, y, idx=0):\n    if False:\n        i = 10\n    '\\n        Splits the dataset, fits a clone of the estimator, then scores it\\n        according to the required metrics.\\n\\n        The index of the split is added to the random_state if the\\n        random_state is not None; this ensures that every split is shuffled\\n        differently but in a deterministic fashion for testing purposes.\\n        '\n    random_state = self.random_state\n    if random_state is not None:\n        random_state += idx\n    splitter = self._check_cv(self.cv, random_state)\n    for (train_index, test_index) in splitter.split(X, y):\n        X_train = _safe_indexing(X, train_index)\n        y_train = _safe_indexing(y, train_index)\n        X_test = _safe_indexing(X, test_index)\n        y_test = _safe_indexing(y, test_index)\n        model = clone(self.estimator)\n        model.fit(X_train, y_train)\n        if hasattr(model, 'predict_proba'):\n            y_scores = model.predict_proba(X_test)[:, 1]\n        else:\n            y_scores = model.decision_function(X_test)\n        curve_metrics = precision_recall_curve(y_test, y_scores)\n        (precision, recall, thresholds) = curve_metrics\n        with np.errstate(divide='ignore', invalid='ignore'):\n            beta = self.fbeta ** 2\n            f_score = (1 + beta) * precision * recall / (beta * precision + recall)\n        thresholds = np.append(thresholds, 1)\n        queue_rate = np.array([(y_scores >= threshold).mean() for threshold in thresholds])\n        yield {'thresholds': thresholds, 'precision': precision, 'recall': recall, 'fscore': f_score, 'queue_rate': queue_rate}",
            "def _split_fit_score_trial(self, X, y, idx=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Splits the dataset, fits a clone of the estimator, then scores it\\n        according to the required metrics.\\n\\n        The index of the split is added to the random_state if the\\n        random_state is not None; this ensures that every split is shuffled\\n        differently but in a deterministic fashion for testing purposes.\\n        '\n    random_state = self.random_state\n    if random_state is not None:\n        random_state += idx\n    splitter = self._check_cv(self.cv, random_state)\n    for (train_index, test_index) in splitter.split(X, y):\n        X_train = _safe_indexing(X, train_index)\n        y_train = _safe_indexing(y, train_index)\n        X_test = _safe_indexing(X, test_index)\n        y_test = _safe_indexing(y, test_index)\n        model = clone(self.estimator)\n        model.fit(X_train, y_train)\n        if hasattr(model, 'predict_proba'):\n            y_scores = model.predict_proba(X_test)[:, 1]\n        else:\n            y_scores = model.decision_function(X_test)\n        curve_metrics = precision_recall_curve(y_test, y_scores)\n        (precision, recall, thresholds) = curve_metrics\n        with np.errstate(divide='ignore', invalid='ignore'):\n            beta = self.fbeta ** 2\n            f_score = (1 + beta) * precision * recall / (beta * precision + recall)\n        thresholds = np.append(thresholds, 1)\n        queue_rate = np.array([(y_scores >= threshold).mean() for threshold in thresholds])\n        yield {'thresholds': thresholds, 'precision': precision, 'recall': recall, 'fscore': f_score, 'queue_rate': queue_rate}",
            "def _split_fit_score_trial(self, X, y, idx=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Splits the dataset, fits a clone of the estimator, then scores it\\n        according to the required metrics.\\n\\n        The index of the split is added to the random_state if the\\n        random_state is not None; this ensures that every split is shuffled\\n        differently but in a deterministic fashion for testing purposes.\\n        '\n    random_state = self.random_state\n    if random_state is not None:\n        random_state += idx\n    splitter = self._check_cv(self.cv, random_state)\n    for (train_index, test_index) in splitter.split(X, y):\n        X_train = _safe_indexing(X, train_index)\n        y_train = _safe_indexing(y, train_index)\n        X_test = _safe_indexing(X, test_index)\n        y_test = _safe_indexing(y, test_index)\n        model = clone(self.estimator)\n        model.fit(X_train, y_train)\n        if hasattr(model, 'predict_proba'):\n            y_scores = model.predict_proba(X_test)[:, 1]\n        else:\n            y_scores = model.decision_function(X_test)\n        curve_metrics = precision_recall_curve(y_test, y_scores)\n        (precision, recall, thresholds) = curve_metrics\n        with np.errstate(divide='ignore', invalid='ignore'):\n            beta = self.fbeta ** 2\n            f_score = (1 + beta) * precision * recall / (beta * precision + recall)\n        thresholds = np.append(thresholds, 1)\n        queue_rate = np.array([(y_scores >= threshold).mean() for threshold in thresholds])\n        yield {'thresholds': thresholds, 'precision': precision, 'recall': recall, 'fscore': f_score, 'queue_rate': queue_rate}",
            "def _split_fit_score_trial(self, X, y, idx=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Splits the dataset, fits a clone of the estimator, then scores it\\n        according to the required metrics.\\n\\n        The index of the split is added to the random_state if the\\n        random_state is not None; this ensures that every split is shuffled\\n        differently but in a deterministic fashion for testing purposes.\\n        '\n    random_state = self.random_state\n    if random_state is not None:\n        random_state += idx\n    splitter = self._check_cv(self.cv, random_state)\n    for (train_index, test_index) in splitter.split(X, y):\n        X_train = _safe_indexing(X, train_index)\n        y_train = _safe_indexing(y, train_index)\n        X_test = _safe_indexing(X, test_index)\n        y_test = _safe_indexing(y, test_index)\n        model = clone(self.estimator)\n        model.fit(X_train, y_train)\n        if hasattr(model, 'predict_proba'):\n            y_scores = model.predict_proba(X_test)[:, 1]\n        else:\n            y_scores = model.decision_function(X_test)\n        curve_metrics = precision_recall_curve(y_test, y_scores)\n        (precision, recall, thresholds) = curve_metrics\n        with np.errstate(divide='ignore', invalid='ignore'):\n            beta = self.fbeta ** 2\n            f_score = (1 + beta) * precision * recall / (beta * precision + recall)\n        thresholds = np.append(thresholds, 1)\n        queue_rate = np.array([(y_scores >= threshold).mean() for threshold in thresholds])\n        yield {'thresholds': thresholds, 'precision': precision, 'recall': recall, 'fscore': f_score, 'queue_rate': queue_rate}",
            "def _split_fit_score_trial(self, X, y, idx=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Splits the dataset, fits a clone of the estimator, then scores it\\n        according to the required metrics.\\n\\n        The index of the split is added to the random_state if the\\n        random_state is not None; this ensures that every split is shuffled\\n        differently but in a deterministic fashion for testing purposes.\\n        '\n    random_state = self.random_state\n    if random_state is not None:\n        random_state += idx\n    splitter = self._check_cv(self.cv, random_state)\n    for (train_index, test_index) in splitter.split(X, y):\n        X_train = _safe_indexing(X, train_index)\n        y_train = _safe_indexing(y, train_index)\n        X_test = _safe_indexing(X, test_index)\n        y_test = _safe_indexing(y, test_index)\n        model = clone(self.estimator)\n        model.fit(X_train, y_train)\n        if hasattr(model, 'predict_proba'):\n            y_scores = model.predict_proba(X_test)[:, 1]\n        else:\n            y_scores = model.decision_function(X_test)\n        curve_metrics = precision_recall_curve(y_test, y_scores)\n        (precision, recall, thresholds) = curve_metrics\n        with np.errstate(divide='ignore', invalid='ignore'):\n            beta = self.fbeta ** 2\n            f_score = (1 + beta) * precision * recall / (beta * precision + recall)\n        thresholds = np.append(thresholds, 1)\n        queue_rate = np.array([(y_scores >= threshold).mean() for threshold in thresholds])\n        yield {'thresholds': thresholds, 'precision': precision, 'recall': recall, 'fscore': f_score, 'queue_rate': queue_rate}"
        ]
    },
    {
        "func_name": "draw",
        "original": "def draw(self):\n    \"\"\"\n        Draws the cv scores as a line chart on the current axes.\n        \"\"\"\n    color_values = resolve_colors(n_colors=4, colors=self.color)\n    argmax = self._check_argmax(self.argmax, self.exclude)\n    for (idx, metric) in enumerate(METRICS):\n        if metric not in self.cv_scores_:\n            continue\n        color = color_values[idx]\n        if metric == 'fscore':\n            if self.fbeta == 1.0:\n                label = '$f_1$'\n            else:\n                label = '$f_{{\\x08eta={:0.1f}}}'.format(self.fbeta)\n        else:\n            label = metric.replace('_', ' ')\n        self.ax.plot(self.thresholds_, self.cv_scores_[metric], color=color, label=label)\n        lower = self.cv_scores_['{}_lower'.format(metric)]\n        upper = self.cv_scores_['{}_upper'.format(metric)]\n        self.ax.fill_between(self.thresholds_, upper, lower, alpha=0.35, linewidth=0, color=color)\n        if argmax and argmax == metric:\n            argmax = self.cv_scores_[metric].argmax()\n            threshold = self.thresholds_[argmax]\n            self.ax.axvline(threshold, ls='--', c='k', lw=1, label='$t_{}={:0.2f}$'.format(metric[0], threshold))\n    return self.ax",
        "mutated": [
            "def draw(self):\n    if False:\n        i = 10\n    '\\n        Draws the cv scores as a line chart on the current axes.\\n        '\n    color_values = resolve_colors(n_colors=4, colors=self.color)\n    argmax = self._check_argmax(self.argmax, self.exclude)\n    for (idx, metric) in enumerate(METRICS):\n        if metric not in self.cv_scores_:\n            continue\n        color = color_values[idx]\n        if metric == 'fscore':\n            if self.fbeta == 1.0:\n                label = '$f_1$'\n            else:\n                label = '$f_{{\\x08eta={:0.1f}}}'.format(self.fbeta)\n        else:\n            label = metric.replace('_', ' ')\n        self.ax.plot(self.thresholds_, self.cv_scores_[metric], color=color, label=label)\n        lower = self.cv_scores_['{}_lower'.format(metric)]\n        upper = self.cv_scores_['{}_upper'.format(metric)]\n        self.ax.fill_between(self.thresholds_, upper, lower, alpha=0.35, linewidth=0, color=color)\n        if argmax and argmax == metric:\n            argmax = self.cv_scores_[metric].argmax()\n            threshold = self.thresholds_[argmax]\n            self.ax.axvline(threshold, ls='--', c='k', lw=1, label='$t_{}={:0.2f}$'.format(metric[0], threshold))\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Draws the cv scores as a line chart on the current axes.\\n        '\n    color_values = resolve_colors(n_colors=4, colors=self.color)\n    argmax = self._check_argmax(self.argmax, self.exclude)\n    for (idx, metric) in enumerate(METRICS):\n        if metric not in self.cv_scores_:\n            continue\n        color = color_values[idx]\n        if metric == 'fscore':\n            if self.fbeta == 1.0:\n                label = '$f_1$'\n            else:\n                label = '$f_{{\\x08eta={:0.1f}}}'.format(self.fbeta)\n        else:\n            label = metric.replace('_', ' ')\n        self.ax.plot(self.thresholds_, self.cv_scores_[metric], color=color, label=label)\n        lower = self.cv_scores_['{}_lower'.format(metric)]\n        upper = self.cv_scores_['{}_upper'.format(metric)]\n        self.ax.fill_between(self.thresholds_, upper, lower, alpha=0.35, linewidth=0, color=color)\n        if argmax and argmax == metric:\n            argmax = self.cv_scores_[metric].argmax()\n            threshold = self.thresholds_[argmax]\n            self.ax.axvline(threshold, ls='--', c='k', lw=1, label='$t_{}={:0.2f}$'.format(metric[0], threshold))\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Draws the cv scores as a line chart on the current axes.\\n        '\n    color_values = resolve_colors(n_colors=4, colors=self.color)\n    argmax = self._check_argmax(self.argmax, self.exclude)\n    for (idx, metric) in enumerate(METRICS):\n        if metric not in self.cv_scores_:\n            continue\n        color = color_values[idx]\n        if metric == 'fscore':\n            if self.fbeta == 1.0:\n                label = '$f_1$'\n            else:\n                label = '$f_{{\\x08eta={:0.1f}}}'.format(self.fbeta)\n        else:\n            label = metric.replace('_', ' ')\n        self.ax.plot(self.thresholds_, self.cv_scores_[metric], color=color, label=label)\n        lower = self.cv_scores_['{}_lower'.format(metric)]\n        upper = self.cv_scores_['{}_upper'.format(metric)]\n        self.ax.fill_between(self.thresholds_, upper, lower, alpha=0.35, linewidth=0, color=color)\n        if argmax and argmax == metric:\n            argmax = self.cv_scores_[metric].argmax()\n            threshold = self.thresholds_[argmax]\n            self.ax.axvline(threshold, ls='--', c='k', lw=1, label='$t_{}={:0.2f}$'.format(metric[0], threshold))\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Draws the cv scores as a line chart on the current axes.\\n        '\n    color_values = resolve_colors(n_colors=4, colors=self.color)\n    argmax = self._check_argmax(self.argmax, self.exclude)\n    for (idx, metric) in enumerate(METRICS):\n        if metric not in self.cv_scores_:\n            continue\n        color = color_values[idx]\n        if metric == 'fscore':\n            if self.fbeta == 1.0:\n                label = '$f_1$'\n            else:\n                label = '$f_{{\\x08eta={:0.1f}}}'.format(self.fbeta)\n        else:\n            label = metric.replace('_', ' ')\n        self.ax.plot(self.thresholds_, self.cv_scores_[metric], color=color, label=label)\n        lower = self.cv_scores_['{}_lower'.format(metric)]\n        upper = self.cv_scores_['{}_upper'.format(metric)]\n        self.ax.fill_between(self.thresholds_, upper, lower, alpha=0.35, linewidth=0, color=color)\n        if argmax and argmax == metric:\n            argmax = self.cv_scores_[metric].argmax()\n            threshold = self.thresholds_[argmax]\n            self.ax.axvline(threshold, ls='--', c='k', lw=1, label='$t_{}={:0.2f}$'.format(metric[0], threshold))\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Draws the cv scores as a line chart on the current axes.\\n        '\n    color_values = resolve_colors(n_colors=4, colors=self.color)\n    argmax = self._check_argmax(self.argmax, self.exclude)\n    for (idx, metric) in enumerate(METRICS):\n        if metric not in self.cv_scores_:\n            continue\n        color = color_values[idx]\n        if metric == 'fscore':\n            if self.fbeta == 1.0:\n                label = '$f_1$'\n            else:\n                label = '$f_{{\\x08eta={:0.1f}}}'.format(self.fbeta)\n        else:\n            label = metric.replace('_', ' ')\n        self.ax.plot(self.thresholds_, self.cv_scores_[metric], color=color, label=label)\n        lower = self.cv_scores_['{}_lower'.format(metric)]\n        upper = self.cv_scores_['{}_upper'.format(metric)]\n        self.ax.fill_between(self.thresholds_, upper, lower, alpha=0.35, linewidth=0, color=color)\n        if argmax and argmax == metric:\n            argmax = self.cv_scores_[metric].argmax()\n            threshold = self.thresholds_[argmax]\n            self.ax.axvline(threshold, ls='--', c='k', lw=1, label='$t_{}={:0.2f}$'.format(metric[0], threshold))\n    return self.ax"
        ]
    },
    {
        "func_name": "finalize",
        "original": "def finalize(self, **kwargs):\n    \"\"\"\n        Sets a title and axis labels on the visualizer and ensures that the\n        axis limits are scaled to valid threshold values.\n\n        Parameters\n        ----------\n        kwargs: generic keyword arguments.\n\n        Notes\n        -----\n        Generally this method is called from show and not directly by the user.\n        \"\"\"\n    super(DiscriminationThreshold, self).finalize(**kwargs)\n    self.set_title('Threshold Plot for {}'.format(self.name))\n    self.ax.legend(frameon=True, loc='best')\n    self.ax.set_xlabel('discrimination threshold')\n    self.ax.set_ylabel('score')\n    self.ax.set_xlim(0.0, 1.0)\n    self.ax.set_ylim(0.0, 1.0)",
        "mutated": [
            "def finalize(self, **kwargs):\n    if False:\n        i = 10\n    '\\n        Sets a title and axis labels on the visualizer and ensures that the\\n        axis limits are scaled to valid threshold values.\\n\\n        Parameters\\n        ----------\\n        kwargs: generic keyword arguments.\\n\\n        Notes\\n        -----\\n        Generally this method is called from show and not directly by the user.\\n        '\n    super(DiscriminationThreshold, self).finalize(**kwargs)\n    self.set_title('Threshold Plot for {}'.format(self.name))\n    self.ax.legend(frameon=True, loc='best')\n    self.ax.set_xlabel('discrimination threshold')\n    self.ax.set_ylabel('score')\n    self.ax.set_xlim(0.0, 1.0)\n    self.ax.set_ylim(0.0, 1.0)",
            "def finalize(self, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Sets a title and axis labels on the visualizer and ensures that the\\n        axis limits are scaled to valid threshold values.\\n\\n        Parameters\\n        ----------\\n        kwargs: generic keyword arguments.\\n\\n        Notes\\n        -----\\n        Generally this method is called from show and not directly by the user.\\n        '\n    super(DiscriminationThreshold, self).finalize(**kwargs)\n    self.set_title('Threshold Plot for {}'.format(self.name))\n    self.ax.legend(frameon=True, loc='best')\n    self.ax.set_xlabel('discrimination threshold')\n    self.ax.set_ylabel('score')\n    self.ax.set_xlim(0.0, 1.0)\n    self.ax.set_ylim(0.0, 1.0)",
            "def finalize(self, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Sets a title and axis labels on the visualizer and ensures that the\\n        axis limits are scaled to valid threshold values.\\n\\n        Parameters\\n        ----------\\n        kwargs: generic keyword arguments.\\n\\n        Notes\\n        -----\\n        Generally this method is called from show and not directly by the user.\\n        '\n    super(DiscriminationThreshold, self).finalize(**kwargs)\n    self.set_title('Threshold Plot for {}'.format(self.name))\n    self.ax.legend(frameon=True, loc='best')\n    self.ax.set_xlabel('discrimination threshold')\n    self.ax.set_ylabel('score')\n    self.ax.set_xlim(0.0, 1.0)\n    self.ax.set_ylim(0.0, 1.0)",
            "def finalize(self, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Sets a title and axis labels on the visualizer and ensures that the\\n        axis limits are scaled to valid threshold values.\\n\\n        Parameters\\n        ----------\\n        kwargs: generic keyword arguments.\\n\\n        Notes\\n        -----\\n        Generally this method is called from show and not directly by the user.\\n        '\n    super(DiscriminationThreshold, self).finalize(**kwargs)\n    self.set_title('Threshold Plot for {}'.format(self.name))\n    self.ax.legend(frameon=True, loc='best')\n    self.ax.set_xlabel('discrimination threshold')\n    self.ax.set_ylabel('score')\n    self.ax.set_xlim(0.0, 1.0)\n    self.ax.set_ylim(0.0, 1.0)",
            "def finalize(self, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Sets a title and axis labels on the visualizer and ensures that the\\n        axis limits are scaled to valid threshold values.\\n\\n        Parameters\\n        ----------\\n        kwargs: generic keyword arguments.\\n\\n        Notes\\n        -----\\n        Generally this method is called from show and not directly by the user.\\n        '\n    super(DiscriminationThreshold, self).finalize(**kwargs)\n    self.set_title('Threshold Plot for {}'.format(self.name))\n    self.ax.legend(frameon=True, loc='best')\n    self.ax.set_xlabel('discrimination threshold')\n    self.ax.set_ylabel('score')\n    self.ax.set_xlim(0.0, 1.0)\n    self.ax.set_ylim(0.0, 1.0)"
        ]
    },
    {
        "func_name": "_check_quantiles",
        "original": "def _check_quantiles(self, val):\n    \"\"\"\n        Validate the quantiles passed in. Returns the np array if valid.\n        \"\"\"\n    if len(val) != 3 or not is_monotonic(val) or (not np.all(val < 1)):\n        raise YellowbrickValueError('quantiles must be a sequence of three monotonically increasing values less than 1')\n    return np.asarray(val)",
        "mutated": [
            "def _check_quantiles(self, val):\n    if False:\n        i = 10\n    '\\n        Validate the quantiles passed in. Returns the np array if valid.\\n        '\n    if len(val) != 3 or not is_monotonic(val) or (not np.all(val < 1)):\n        raise YellowbrickValueError('quantiles must be a sequence of three monotonically increasing values less than 1')\n    return np.asarray(val)",
            "def _check_quantiles(self, val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Validate the quantiles passed in. Returns the np array if valid.\\n        '\n    if len(val) != 3 or not is_monotonic(val) or (not np.all(val < 1)):\n        raise YellowbrickValueError('quantiles must be a sequence of three monotonically increasing values less than 1')\n    return np.asarray(val)",
            "def _check_quantiles(self, val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Validate the quantiles passed in. Returns the np array if valid.\\n        '\n    if len(val) != 3 or not is_monotonic(val) or (not np.all(val < 1)):\n        raise YellowbrickValueError('quantiles must be a sequence of three monotonically increasing values less than 1')\n    return np.asarray(val)",
            "def _check_quantiles(self, val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Validate the quantiles passed in. Returns the np array if valid.\\n        '\n    if len(val) != 3 or not is_monotonic(val) or (not np.all(val < 1)):\n        raise YellowbrickValueError('quantiles must be a sequence of three monotonically increasing values less than 1')\n    return np.asarray(val)",
            "def _check_quantiles(self, val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Validate the quantiles passed in. Returns the np array if valid.\\n        '\n    if len(val) != 3 or not is_monotonic(val) or (not np.all(val < 1)):\n        raise YellowbrickValueError('quantiles must be a sequence of three monotonically increasing values less than 1')\n    return np.asarray(val)"
        ]
    },
    {
        "func_name": "_check_cv",
        "original": "def _check_cv(self, val, random_state=None):\n    \"\"\"\n        Validate the cv method passed in. Returns the split strategy if no\n        validation exception is raised.\n        \"\"\"\n    if val is None:\n        val = 0.1\n    if isinstance(val, float) and val <= 1.0:\n        return ShuffleSplit(n_splits=1, test_size=val, random_state=random_state)\n    if hasattr(val, 'split') and hasattr(val, 'get_n_splits'):\n        if random_state is not None and hasattr(val, 'random_state'):\n            val.random_state = random_state\n        return val\n    raise YellowbrickValueError(\"'{}' is not a valid cv splitter\".format(type(val)))",
        "mutated": [
            "def _check_cv(self, val, random_state=None):\n    if False:\n        i = 10\n    '\\n        Validate the cv method passed in. Returns the split strategy if no\\n        validation exception is raised.\\n        '\n    if val is None:\n        val = 0.1\n    if isinstance(val, float) and val <= 1.0:\n        return ShuffleSplit(n_splits=1, test_size=val, random_state=random_state)\n    if hasattr(val, 'split') and hasattr(val, 'get_n_splits'):\n        if random_state is not None and hasattr(val, 'random_state'):\n            val.random_state = random_state\n        return val\n    raise YellowbrickValueError(\"'{}' is not a valid cv splitter\".format(type(val)))",
            "def _check_cv(self, val, random_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Validate the cv method passed in. Returns the split strategy if no\\n        validation exception is raised.\\n        '\n    if val is None:\n        val = 0.1\n    if isinstance(val, float) and val <= 1.0:\n        return ShuffleSplit(n_splits=1, test_size=val, random_state=random_state)\n    if hasattr(val, 'split') and hasattr(val, 'get_n_splits'):\n        if random_state is not None and hasattr(val, 'random_state'):\n            val.random_state = random_state\n        return val\n    raise YellowbrickValueError(\"'{}' is not a valid cv splitter\".format(type(val)))",
            "def _check_cv(self, val, random_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Validate the cv method passed in. Returns the split strategy if no\\n        validation exception is raised.\\n        '\n    if val is None:\n        val = 0.1\n    if isinstance(val, float) and val <= 1.0:\n        return ShuffleSplit(n_splits=1, test_size=val, random_state=random_state)\n    if hasattr(val, 'split') and hasattr(val, 'get_n_splits'):\n        if random_state is not None and hasattr(val, 'random_state'):\n            val.random_state = random_state\n        return val\n    raise YellowbrickValueError(\"'{}' is not a valid cv splitter\".format(type(val)))",
            "def _check_cv(self, val, random_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Validate the cv method passed in. Returns the split strategy if no\\n        validation exception is raised.\\n        '\n    if val is None:\n        val = 0.1\n    if isinstance(val, float) and val <= 1.0:\n        return ShuffleSplit(n_splits=1, test_size=val, random_state=random_state)\n    if hasattr(val, 'split') and hasattr(val, 'get_n_splits'):\n        if random_state is not None and hasattr(val, 'random_state'):\n            val.random_state = random_state\n        return val\n    raise YellowbrickValueError(\"'{}' is not a valid cv splitter\".format(type(val)))",
            "def _check_cv(self, val, random_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Validate the cv method passed in. Returns the split strategy if no\\n        validation exception is raised.\\n        '\n    if val is None:\n        val = 0.1\n    if isinstance(val, float) and val <= 1.0:\n        return ShuffleSplit(n_splits=1, test_size=val, random_state=random_state)\n    if hasattr(val, 'split') and hasattr(val, 'get_n_splits'):\n        if random_state is not None and hasattr(val, 'random_state'):\n            val.random_state = random_state\n        return val\n    raise YellowbrickValueError(\"'{}' is not a valid cv splitter\".format(type(val)))"
        ]
    },
    {
        "func_name": "_check_exclude",
        "original": "def _check_exclude(self, val):\n    \"\"\"\n        Validate the excluded metrics. Returns the set of excluded params.\n        \"\"\"\n    if val is None:\n        exclude = frozenset()\n    elif isinstance(val, str):\n        exclude = frozenset([val.lower()])\n    else:\n        exclude = frozenset(map(lambda s: s.lower(), val))\n    if len(exclude - frozenset(METRICS)) > 0:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to exclude\".format(repr(val)))\n    return exclude",
        "mutated": [
            "def _check_exclude(self, val):\n    if False:\n        i = 10\n    '\\n        Validate the excluded metrics. Returns the set of excluded params.\\n        '\n    if val is None:\n        exclude = frozenset()\n    elif isinstance(val, str):\n        exclude = frozenset([val.lower()])\n    else:\n        exclude = frozenset(map(lambda s: s.lower(), val))\n    if len(exclude - frozenset(METRICS)) > 0:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to exclude\".format(repr(val)))\n    return exclude",
            "def _check_exclude(self, val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Validate the excluded metrics. Returns the set of excluded params.\\n        '\n    if val is None:\n        exclude = frozenset()\n    elif isinstance(val, str):\n        exclude = frozenset([val.lower()])\n    else:\n        exclude = frozenset(map(lambda s: s.lower(), val))\n    if len(exclude - frozenset(METRICS)) > 0:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to exclude\".format(repr(val)))\n    return exclude",
            "def _check_exclude(self, val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Validate the excluded metrics. Returns the set of excluded params.\\n        '\n    if val is None:\n        exclude = frozenset()\n    elif isinstance(val, str):\n        exclude = frozenset([val.lower()])\n    else:\n        exclude = frozenset(map(lambda s: s.lower(), val))\n    if len(exclude - frozenset(METRICS)) > 0:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to exclude\".format(repr(val)))\n    return exclude",
            "def _check_exclude(self, val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Validate the excluded metrics. Returns the set of excluded params.\\n        '\n    if val is None:\n        exclude = frozenset()\n    elif isinstance(val, str):\n        exclude = frozenset([val.lower()])\n    else:\n        exclude = frozenset(map(lambda s: s.lower(), val))\n    if len(exclude - frozenset(METRICS)) > 0:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to exclude\".format(repr(val)))\n    return exclude",
            "def _check_exclude(self, val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Validate the excluded metrics. Returns the set of excluded params.\\n        '\n    if val is None:\n        exclude = frozenset()\n    elif isinstance(val, str):\n        exclude = frozenset([val.lower()])\n    else:\n        exclude = frozenset(map(lambda s: s.lower(), val))\n    if len(exclude - frozenset(METRICS)) > 0:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to exclude\".format(repr(val)))\n    return exclude"
        ]
    },
    {
        "func_name": "_check_argmax",
        "original": "def _check_argmax(self, val, exclude=None):\n    \"\"\"\n        Validate the argmax metric. Returns the metric used to annotate the graph.\n        \"\"\"\n    if val is None:\n        return None\n    argmax = val.lower()\n    if argmax not in METRICS:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to use\".format(repr(val)))\n    exclude = self._check_exclude(exclude)\n    if argmax in exclude:\n        argmax = None\n    return argmax",
        "mutated": [
            "def _check_argmax(self, val, exclude=None):\n    if False:\n        i = 10\n    '\\n        Validate the argmax metric. Returns the metric used to annotate the graph.\\n        '\n    if val is None:\n        return None\n    argmax = val.lower()\n    if argmax not in METRICS:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to use\".format(repr(val)))\n    exclude = self._check_exclude(exclude)\n    if argmax in exclude:\n        argmax = None\n    return argmax",
            "def _check_argmax(self, val, exclude=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Validate the argmax metric. Returns the metric used to annotate the graph.\\n        '\n    if val is None:\n        return None\n    argmax = val.lower()\n    if argmax not in METRICS:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to use\".format(repr(val)))\n    exclude = self._check_exclude(exclude)\n    if argmax in exclude:\n        argmax = None\n    return argmax",
            "def _check_argmax(self, val, exclude=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Validate the argmax metric. Returns the metric used to annotate the graph.\\n        '\n    if val is None:\n        return None\n    argmax = val.lower()\n    if argmax not in METRICS:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to use\".format(repr(val)))\n    exclude = self._check_exclude(exclude)\n    if argmax in exclude:\n        argmax = None\n    return argmax",
            "def _check_argmax(self, val, exclude=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Validate the argmax metric. Returns the metric used to annotate the graph.\\n        '\n    if val is None:\n        return None\n    argmax = val.lower()\n    if argmax not in METRICS:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to use\".format(repr(val)))\n    exclude = self._check_exclude(exclude)\n    if argmax in exclude:\n        argmax = None\n    return argmax",
            "def _check_argmax(self, val, exclude=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Validate the argmax metric. Returns the metric used to annotate the graph.\\n        '\n    if val is None:\n        return None\n    argmax = val.lower()\n    if argmax not in METRICS:\n        raise YellowbrickValueError(\"'{}' is not a valid metric to use\".format(repr(val)))\n    exclude = self._check_exclude(exclude)\n    if argmax in exclude:\n        argmax = None\n    return argmax"
        ]
    },
    {
        "func_name": "discrimination_threshold",
        "original": "def discrimination_threshold(estimator, X, y, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    \"\"\"Discrimination Threshold\n\n    Visualizes how precision, recall, f1 score, and queue rate change as the\n    discrimination threshold increases. For probabilistic, binary classifiers,\n    the discrimination threshold is the probability at which you choose the\n    positive class over the negative. Generally this is set to 50%, but\n    adjusting the discrimination threshold will adjust sensitivity to false\n    positives which is described by the inverse relationship of precision and\n    recall with respect to the threshold.\n\n    .. seealso:: See DiscriminationThreshold for more details.\n\n    Parameters\n    ----------\n    estimator : estimator\n        A scikit-learn estimator that should be a classifier. If the model is\n        not a classifier, an exception is raised. If the internal model is not\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\n        by ``is_fitted``.\n\n    X : ndarray or DataFrame of shape n x m\n        A matrix of n instances with m features\n\n    y : ndarray or Series of length n\n        An array or series of target or class values. The target y must\n        be a binary classification target.\n\n    ax : matplotlib Axes, default: None\n        The axes to plot the figure on. If not specified the current axes will be\n        used (or generated if required).\n\n    n_trials : integer, default: 50\n        Number of times to shuffle and split the dataset to account for noise\n        in the threshold metrics curves. Note if cv provides > 1 splits,\n        the number of trials will be n_trials * cv.get_n_splits()\n\n    cv : float or cross-validation generator, default: 0.1\n        Determines the splitting strategy for each trial. Possible inputs are:\n\n        - float, to specify the percent of the test split\n        - object to be used as cross-validation generator\n\n        This attribute is meant to give flexibility with stratified splitting\n        but if a splitter is provided, it should only return one split and\n        have shuffle set to True.\n\n    fbeta : float, 1.0 by default\n        The strength of recall versus precision in the F-score.\n\n    argmax : str or None, default: 'fscore'\n        Annotate the threshold maximized by the supplied metric (see exclude\n        for the possible metrics to use). If None or passed to exclude,\n        will not annotate the graph.\n\n    exclude : str or list, optional\n        Specify metrics to omit from the graph, can include:\n\n        - ``\"precision\"``\n        - ``\"recall\"``\n        - ``\"queue_rate\"``\n        - ``\"fscore\"``\n\n        Excluded metrics will not be displayed in the graph, nor will they\n        be available in ``thresholds_``; however, they will be computed on fit.\n\n    quantiles : sequence, default: np.array([0.1, 0.5, 0.9])\n        Specify the quantiles to view model variability across a number of\n        trials. Must be monotonic and have three elements such that the first\n        element is the lower bound, the second is the drawn curve, and the\n        third is the upper bound. By default the curve is drawn at the median,\n        and the bounds from the 10th percentile to the 90th percentile.\n\n    random_state : int, optional\n        Used to seed the random state for shuffling the data while composing\n        different train and test splits. If supplied, the random state is\n        incremented in a deterministic fashion for each split.\n\n        Note that if a splitter is provided, it's random state will also be\n        updated with this random state, even if it was previously set.\n\n    is_fitted : bool or str, default=\"auto\"\n        Specify if the wrapped estimator is already fitted. If False, the estimator\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\n        modified. If \"auto\" (default), a helper method will check if the estimator\n        is fitted before fitting it again.\n\n    force_model : bool, default: False\n        Do not check to ensure that the underlying estimator is a classifier. This\n        will prevent an exception when the visualizer is initialized but may result\n        in unexpected or unintended behavior.\n\n    show : bool, default: True\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\n        calls ``finalize()``\n\n    kwargs : dict\n        Keyword arguments passed to the visualizer base classes.\n\n    Notes\n    -----\n    The term \"discrimination threshold\" is rare in the literature. Here, we\n    use it to mean the probability at which the positive class is selected\n    over the negative class in binary classification.\n\n    Classification models must implement either a ``decision_function`` or\n    ``predict_proba`` method in order to be used with this class. A\n    ``YellowbrickTypeError`` is raised otherwise.\n\n    .. seealso::\n        For a thorough explanation of discrimination thresholds, see:\n        `Visualizing Machine Learning Thresholds to Make Better Business\n        Decisions\n        <http://blog.insightdatalabs.com/visualizing-classifier-thresholds/>`_\n        by Insight Data.\n\n    Examples\n    --------\n    >>> from yellowbrick.classifier.threshold import discrimination_threshold\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from yellowbrick.datasets import load_occupancy\n    >>> X, y = load_occupancy()\n    >>> model = LogisticRegression(multi_class=\"auto\", solver=\"liblinear\")\n    >>> discrimination_threshold(model, X, y)\n\n    Returns\n    -------\n    viz : DiscriminationThreshold\n        Returns the fitted and finalized visualizer object.\n    \"\"\"\n    visualizer = DiscriminationThreshold(estimator, ax=ax, n_trials=n_trials, cv=cv, fbeta=fbeta, argmax=argmax, exclude=exclude, quantiles=quantiles, random_state=random_state, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X, y)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer",
        "mutated": [
            "def discrimination_threshold(estimator, X, y, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    if False:\n        i = 10\n    'Discrimination Threshold\\n\\n    Visualizes how precision, recall, f1 score, and queue rate change as the\\n    discrimination threshold increases. For probabilistic, binary classifiers,\\n    the discrimination threshold is the probability at which you choose the\\n    positive class over the negative. Generally this is set to 50%, but\\n    adjusting the discrimination threshold will adjust sensitivity to false\\n    positives which is described by the inverse relationship of precision and\\n    recall with respect to the threshold.\\n\\n    .. seealso:: See DiscriminationThreshold for more details.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator\\n        A scikit-learn estimator that should be a classifier. If the model is\\n        not a classifier, an exception is raised. If the internal model is not\\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\\n        by ``is_fitted``.\\n\\n    X : ndarray or DataFrame of shape n x m\\n        A matrix of n instances with m features\\n\\n    y : ndarray or Series of length n\\n        An array or series of target or class values. The target y must\\n        be a binary classification target.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If not specified the current axes will be\\n        used (or generated if required).\\n\\n    n_trials : integer, default: 50\\n        Number of times to shuffle and split the dataset to account for noise\\n        in the threshold metrics curves. Note if cv provides > 1 splits,\\n        the number of trials will be n_trials * cv.get_n_splits()\\n\\n    cv : float or cross-validation generator, default: 0.1\\n        Determines the splitting strategy for each trial. Possible inputs are:\\n\\n        - float, to specify the percent of the test split\\n        - object to be used as cross-validation generator\\n\\n        This attribute is meant to give flexibility with stratified splitting\\n        but if a splitter is provided, it should only return one split and\\n        have shuffle set to True.\\n\\n    fbeta : float, 1.0 by default\\n        The strength of recall versus precision in the F-score.\\n\\n    argmax : str or None, default: \\'fscore\\'\\n        Annotate the threshold maximized by the supplied metric (see exclude\\n        for the possible metrics to use). If None or passed to exclude,\\n        will not annotate the graph.\\n\\n    exclude : str or list, optional\\n        Specify metrics to omit from the graph, can include:\\n\\n        - ``\"precision\"``\\n        - ``\"recall\"``\\n        - ``\"queue_rate\"``\\n        - ``\"fscore\"``\\n\\n        Excluded metrics will not be displayed in the graph, nor will they\\n        be available in ``thresholds_``; however, they will be computed on fit.\\n\\n    quantiles : sequence, default: np.array([0.1, 0.5, 0.9])\\n        Specify the quantiles to view model variability across a number of\\n        trials. Must be monotonic and have three elements such that the first\\n        element is the lower bound, the second is the drawn curve, and the\\n        third is the upper bound. By default the curve is drawn at the median,\\n        and the bounds from the 10th percentile to the 90th percentile.\\n\\n    random_state : int, optional\\n        Used to seed the random state for shuffling the data while composing\\n        different train and test splits. If supplied, the random state is\\n        incremented in a deterministic fashion for each split.\\n\\n        Note that if a splitter is provided, it\\'s random state will also be\\n        updated with this random state, even if it was previously set.\\n\\n    is_fitted : bool or str, default=\"auto\"\\n        Specify if the wrapped estimator is already fitted. If False, the estimator\\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\\n        modified. If \"auto\" (default), a helper method will check if the estimator\\n        is fitted before fitting it again.\\n\\n    force_model : bool, default: False\\n        Do not check to ensure that the underlying estimator is a classifier. This\\n        will prevent an exception when the visualizer is initialized but may result\\n        in unexpected or unintended behavior.\\n\\n    show : bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\\n        calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments passed to the visualizer base classes.\\n\\n    Notes\\n    -----\\n    The term \"discrimination threshold\" is rare in the literature. Here, we\\n    use it to mean the probability at which the positive class is selected\\n    over the negative class in binary classification.\\n\\n    Classification models must implement either a ``decision_function`` or\\n    ``predict_proba`` method in order to be used with this class. A\\n    ``YellowbrickTypeError`` is raised otherwise.\\n\\n    .. seealso::\\n        For a thorough explanation of discrimination thresholds, see:\\n        `Visualizing Machine Learning Thresholds to Make Better Business\\n        Decisions\\n        <http://blog.insightdatalabs.com/visualizing-classifier-thresholds/>`_\\n        by Insight Data.\\n\\n    Examples\\n    --------\\n    >>> from yellowbrick.classifier.threshold import discrimination_threshold\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> from yellowbrick.datasets import load_occupancy\\n    >>> X, y = load_occupancy()\\n    >>> model = LogisticRegression(multi_class=\"auto\", solver=\"liblinear\")\\n    >>> discrimination_threshold(model, X, y)\\n\\n    Returns\\n    -------\\n    viz : DiscriminationThreshold\\n        Returns the fitted and finalized visualizer object.\\n    '\n    visualizer = DiscriminationThreshold(estimator, ax=ax, n_trials=n_trials, cv=cv, fbeta=fbeta, argmax=argmax, exclude=exclude, quantiles=quantiles, random_state=random_state, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X, y)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer",
            "def discrimination_threshold(estimator, X, y, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Discrimination Threshold\\n\\n    Visualizes how precision, recall, f1 score, and queue rate change as the\\n    discrimination threshold increases. For probabilistic, binary classifiers,\\n    the discrimination threshold is the probability at which you choose the\\n    positive class over the negative. Generally this is set to 50%, but\\n    adjusting the discrimination threshold will adjust sensitivity to false\\n    positives which is described by the inverse relationship of precision and\\n    recall with respect to the threshold.\\n\\n    .. seealso:: See DiscriminationThreshold for more details.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator\\n        A scikit-learn estimator that should be a classifier. If the model is\\n        not a classifier, an exception is raised. If the internal model is not\\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\\n        by ``is_fitted``.\\n\\n    X : ndarray or DataFrame of shape n x m\\n        A matrix of n instances with m features\\n\\n    y : ndarray or Series of length n\\n        An array or series of target or class values. The target y must\\n        be a binary classification target.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If not specified the current axes will be\\n        used (or generated if required).\\n\\n    n_trials : integer, default: 50\\n        Number of times to shuffle and split the dataset to account for noise\\n        in the threshold metrics curves. Note if cv provides > 1 splits,\\n        the number of trials will be n_trials * cv.get_n_splits()\\n\\n    cv : float or cross-validation generator, default: 0.1\\n        Determines the splitting strategy for each trial. Possible inputs are:\\n\\n        - float, to specify the percent of the test split\\n        - object to be used as cross-validation generator\\n\\n        This attribute is meant to give flexibility with stratified splitting\\n        but if a splitter is provided, it should only return one split and\\n        have shuffle set to True.\\n\\n    fbeta : float, 1.0 by default\\n        The strength of recall versus precision in the F-score.\\n\\n    argmax : str or None, default: \\'fscore\\'\\n        Annotate the threshold maximized by the supplied metric (see exclude\\n        for the possible metrics to use). If None or passed to exclude,\\n        will not annotate the graph.\\n\\n    exclude : str or list, optional\\n        Specify metrics to omit from the graph, can include:\\n\\n        - ``\"precision\"``\\n        - ``\"recall\"``\\n        - ``\"queue_rate\"``\\n        - ``\"fscore\"``\\n\\n        Excluded metrics will not be displayed in the graph, nor will they\\n        be available in ``thresholds_``; however, they will be computed on fit.\\n\\n    quantiles : sequence, default: np.array([0.1, 0.5, 0.9])\\n        Specify the quantiles to view model variability across a number of\\n        trials. Must be monotonic and have three elements such that the first\\n        element is the lower bound, the second is the drawn curve, and the\\n        third is the upper bound. By default the curve is drawn at the median,\\n        and the bounds from the 10th percentile to the 90th percentile.\\n\\n    random_state : int, optional\\n        Used to seed the random state for shuffling the data while composing\\n        different train and test splits. If supplied, the random state is\\n        incremented in a deterministic fashion for each split.\\n\\n        Note that if a splitter is provided, it\\'s random state will also be\\n        updated with this random state, even if it was previously set.\\n\\n    is_fitted : bool or str, default=\"auto\"\\n        Specify if the wrapped estimator is already fitted. If False, the estimator\\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\\n        modified. If \"auto\" (default), a helper method will check if the estimator\\n        is fitted before fitting it again.\\n\\n    force_model : bool, default: False\\n        Do not check to ensure that the underlying estimator is a classifier. This\\n        will prevent an exception when the visualizer is initialized but may result\\n        in unexpected or unintended behavior.\\n\\n    show : bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\\n        calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments passed to the visualizer base classes.\\n\\n    Notes\\n    -----\\n    The term \"discrimination threshold\" is rare in the literature. Here, we\\n    use it to mean the probability at which the positive class is selected\\n    over the negative class in binary classification.\\n\\n    Classification models must implement either a ``decision_function`` or\\n    ``predict_proba`` method in order to be used with this class. A\\n    ``YellowbrickTypeError`` is raised otherwise.\\n\\n    .. seealso::\\n        For a thorough explanation of discrimination thresholds, see:\\n        `Visualizing Machine Learning Thresholds to Make Better Business\\n        Decisions\\n        <http://blog.insightdatalabs.com/visualizing-classifier-thresholds/>`_\\n        by Insight Data.\\n\\n    Examples\\n    --------\\n    >>> from yellowbrick.classifier.threshold import discrimination_threshold\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> from yellowbrick.datasets import load_occupancy\\n    >>> X, y = load_occupancy()\\n    >>> model = LogisticRegression(multi_class=\"auto\", solver=\"liblinear\")\\n    >>> discrimination_threshold(model, X, y)\\n\\n    Returns\\n    -------\\n    viz : DiscriminationThreshold\\n        Returns the fitted and finalized visualizer object.\\n    '\n    visualizer = DiscriminationThreshold(estimator, ax=ax, n_trials=n_trials, cv=cv, fbeta=fbeta, argmax=argmax, exclude=exclude, quantiles=quantiles, random_state=random_state, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X, y)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer",
            "def discrimination_threshold(estimator, X, y, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Discrimination Threshold\\n\\n    Visualizes how precision, recall, f1 score, and queue rate change as the\\n    discrimination threshold increases. For probabilistic, binary classifiers,\\n    the discrimination threshold is the probability at which you choose the\\n    positive class over the negative. Generally this is set to 50%, but\\n    adjusting the discrimination threshold will adjust sensitivity to false\\n    positives which is described by the inverse relationship of precision and\\n    recall with respect to the threshold.\\n\\n    .. seealso:: See DiscriminationThreshold for more details.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator\\n        A scikit-learn estimator that should be a classifier. If the model is\\n        not a classifier, an exception is raised. If the internal model is not\\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\\n        by ``is_fitted``.\\n\\n    X : ndarray or DataFrame of shape n x m\\n        A matrix of n instances with m features\\n\\n    y : ndarray or Series of length n\\n        An array or series of target or class values. The target y must\\n        be a binary classification target.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If not specified the current axes will be\\n        used (or generated if required).\\n\\n    n_trials : integer, default: 50\\n        Number of times to shuffle and split the dataset to account for noise\\n        in the threshold metrics curves. Note if cv provides > 1 splits,\\n        the number of trials will be n_trials * cv.get_n_splits()\\n\\n    cv : float or cross-validation generator, default: 0.1\\n        Determines the splitting strategy for each trial. Possible inputs are:\\n\\n        - float, to specify the percent of the test split\\n        - object to be used as cross-validation generator\\n\\n        This attribute is meant to give flexibility with stratified splitting\\n        but if a splitter is provided, it should only return one split and\\n        have shuffle set to True.\\n\\n    fbeta : float, 1.0 by default\\n        The strength of recall versus precision in the F-score.\\n\\n    argmax : str or None, default: \\'fscore\\'\\n        Annotate the threshold maximized by the supplied metric (see exclude\\n        for the possible metrics to use). If None or passed to exclude,\\n        will not annotate the graph.\\n\\n    exclude : str or list, optional\\n        Specify metrics to omit from the graph, can include:\\n\\n        - ``\"precision\"``\\n        - ``\"recall\"``\\n        - ``\"queue_rate\"``\\n        - ``\"fscore\"``\\n\\n        Excluded metrics will not be displayed in the graph, nor will they\\n        be available in ``thresholds_``; however, they will be computed on fit.\\n\\n    quantiles : sequence, default: np.array([0.1, 0.5, 0.9])\\n        Specify the quantiles to view model variability across a number of\\n        trials. Must be monotonic and have three elements such that the first\\n        element is the lower bound, the second is the drawn curve, and the\\n        third is the upper bound. By default the curve is drawn at the median,\\n        and the bounds from the 10th percentile to the 90th percentile.\\n\\n    random_state : int, optional\\n        Used to seed the random state for shuffling the data while composing\\n        different train and test splits. If supplied, the random state is\\n        incremented in a deterministic fashion for each split.\\n\\n        Note that if a splitter is provided, it\\'s random state will also be\\n        updated with this random state, even if it was previously set.\\n\\n    is_fitted : bool or str, default=\"auto\"\\n        Specify if the wrapped estimator is already fitted. If False, the estimator\\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\\n        modified. If \"auto\" (default), a helper method will check if the estimator\\n        is fitted before fitting it again.\\n\\n    force_model : bool, default: False\\n        Do not check to ensure that the underlying estimator is a classifier. This\\n        will prevent an exception when the visualizer is initialized but may result\\n        in unexpected or unintended behavior.\\n\\n    show : bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\\n        calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments passed to the visualizer base classes.\\n\\n    Notes\\n    -----\\n    The term \"discrimination threshold\" is rare in the literature. Here, we\\n    use it to mean the probability at which the positive class is selected\\n    over the negative class in binary classification.\\n\\n    Classification models must implement either a ``decision_function`` or\\n    ``predict_proba`` method in order to be used with this class. A\\n    ``YellowbrickTypeError`` is raised otherwise.\\n\\n    .. seealso::\\n        For a thorough explanation of discrimination thresholds, see:\\n        `Visualizing Machine Learning Thresholds to Make Better Business\\n        Decisions\\n        <http://blog.insightdatalabs.com/visualizing-classifier-thresholds/>`_\\n        by Insight Data.\\n\\n    Examples\\n    --------\\n    >>> from yellowbrick.classifier.threshold import discrimination_threshold\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> from yellowbrick.datasets import load_occupancy\\n    >>> X, y = load_occupancy()\\n    >>> model = LogisticRegression(multi_class=\"auto\", solver=\"liblinear\")\\n    >>> discrimination_threshold(model, X, y)\\n\\n    Returns\\n    -------\\n    viz : DiscriminationThreshold\\n        Returns the fitted and finalized visualizer object.\\n    '\n    visualizer = DiscriminationThreshold(estimator, ax=ax, n_trials=n_trials, cv=cv, fbeta=fbeta, argmax=argmax, exclude=exclude, quantiles=quantiles, random_state=random_state, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X, y)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer",
            "def discrimination_threshold(estimator, X, y, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Discrimination Threshold\\n\\n    Visualizes how precision, recall, f1 score, and queue rate change as the\\n    discrimination threshold increases. For probabilistic, binary classifiers,\\n    the discrimination threshold is the probability at which you choose the\\n    positive class over the negative. Generally this is set to 50%, but\\n    adjusting the discrimination threshold will adjust sensitivity to false\\n    positives which is described by the inverse relationship of precision and\\n    recall with respect to the threshold.\\n\\n    .. seealso:: See DiscriminationThreshold for more details.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator\\n        A scikit-learn estimator that should be a classifier. If the model is\\n        not a classifier, an exception is raised. If the internal model is not\\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\\n        by ``is_fitted``.\\n\\n    X : ndarray or DataFrame of shape n x m\\n        A matrix of n instances with m features\\n\\n    y : ndarray or Series of length n\\n        An array or series of target or class values. The target y must\\n        be a binary classification target.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If not specified the current axes will be\\n        used (or generated if required).\\n\\n    n_trials : integer, default: 50\\n        Number of times to shuffle and split the dataset to account for noise\\n        in the threshold metrics curves. Note if cv provides > 1 splits,\\n        the number of trials will be n_trials * cv.get_n_splits()\\n\\n    cv : float or cross-validation generator, default: 0.1\\n        Determines the splitting strategy for each trial. Possible inputs are:\\n\\n        - float, to specify the percent of the test split\\n        - object to be used as cross-validation generator\\n\\n        This attribute is meant to give flexibility with stratified splitting\\n        but if a splitter is provided, it should only return one split and\\n        have shuffle set to True.\\n\\n    fbeta : float, 1.0 by default\\n        The strength of recall versus precision in the F-score.\\n\\n    argmax : str or None, default: \\'fscore\\'\\n        Annotate the threshold maximized by the supplied metric (see exclude\\n        for the possible metrics to use). If None or passed to exclude,\\n        will not annotate the graph.\\n\\n    exclude : str or list, optional\\n        Specify metrics to omit from the graph, can include:\\n\\n        - ``\"precision\"``\\n        - ``\"recall\"``\\n        - ``\"queue_rate\"``\\n        - ``\"fscore\"``\\n\\n        Excluded metrics will not be displayed in the graph, nor will they\\n        be available in ``thresholds_``; however, they will be computed on fit.\\n\\n    quantiles : sequence, default: np.array([0.1, 0.5, 0.9])\\n        Specify the quantiles to view model variability across a number of\\n        trials. Must be monotonic and have three elements such that the first\\n        element is the lower bound, the second is the drawn curve, and the\\n        third is the upper bound. By default the curve is drawn at the median,\\n        and the bounds from the 10th percentile to the 90th percentile.\\n\\n    random_state : int, optional\\n        Used to seed the random state for shuffling the data while composing\\n        different train and test splits. If supplied, the random state is\\n        incremented in a deterministic fashion for each split.\\n\\n        Note that if a splitter is provided, it\\'s random state will also be\\n        updated with this random state, even if it was previously set.\\n\\n    is_fitted : bool or str, default=\"auto\"\\n        Specify if the wrapped estimator is already fitted. If False, the estimator\\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\\n        modified. If \"auto\" (default), a helper method will check if the estimator\\n        is fitted before fitting it again.\\n\\n    force_model : bool, default: False\\n        Do not check to ensure that the underlying estimator is a classifier. This\\n        will prevent an exception when the visualizer is initialized but may result\\n        in unexpected or unintended behavior.\\n\\n    show : bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\\n        calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments passed to the visualizer base classes.\\n\\n    Notes\\n    -----\\n    The term \"discrimination threshold\" is rare in the literature. Here, we\\n    use it to mean the probability at which the positive class is selected\\n    over the negative class in binary classification.\\n\\n    Classification models must implement either a ``decision_function`` or\\n    ``predict_proba`` method in order to be used with this class. A\\n    ``YellowbrickTypeError`` is raised otherwise.\\n\\n    .. seealso::\\n        For a thorough explanation of discrimination thresholds, see:\\n        `Visualizing Machine Learning Thresholds to Make Better Business\\n        Decisions\\n        <http://blog.insightdatalabs.com/visualizing-classifier-thresholds/>`_\\n        by Insight Data.\\n\\n    Examples\\n    --------\\n    >>> from yellowbrick.classifier.threshold import discrimination_threshold\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> from yellowbrick.datasets import load_occupancy\\n    >>> X, y = load_occupancy()\\n    >>> model = LogisticRegression(multi_class=\"auto\", solver=\"liblinear\")\\n    >>> discrimination_threshold(model, X, y)\\n\\n    Returns\\n    -------\\n    viz : DiscriminationThreshold\\n        Returns the fitted and finalized visualizer object.\\n    '\n    visualizer = DiscriminationThreshold(estimator, ax=ax, n_trials=n_trials, cv=cv, fbeta=fbeta, argmax=argmax, exclude=exclude, quantiles=quantiles, random_state=random_state, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X, y)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer",
            "def discrimination_threshold(estimator, X, y, ax=None, n_trials=50, cv=0.1, fbeta=1.0, argmax='fscore', exclude=None, quantiles=QUANTILES_MEDIAN_80, random_state=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Discrimination Threshold\\n\\n    Visualizes how precision, recall, f1 score, and queue rate change as the\\n    discrimination threshold increases. For probabilistic, binary classifiers,\\n    the discrimination threshold is the probability at which you choose the\\n    positive class over the negative. Generally this is set to 50%, but\\n    adjusting the discrimination threshold will adjust sensitivity to false\\n    positives which is described by the inverse relationship of precision and\\n    recall with respect to the threshold.\\n\\n    .. seealso:: See DiscriminationThreshold for more details.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator\\n        A scikit-learn estimator that should be a classifier. If the model is\\n        not a classifier, an exception is raised. If the internal model is not\\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\\n        by ``is_fitted``.\\n\\n    X : ndarray or DataFrame of shape n x m\\n        A matrix of n instances with m features\\n\\n    y : ndarray or Series of length n\\n        An array or series of target or class values. The target y must\\n        be a binary classification target.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If not specified the current axes will be\\n        used (or generated if required).\\n\\n    n_trials : integer, default: 50\\n        Number of times to shuffle and split the dataset to account for noise\\n        in the threshold metrics curves. Note if cv provides > 1 splits,\\n        the number of trials will be n_trials * cv.get_n_splits()\\n\\n    cv : float or cross-validation generator, default: 0.1\\n        Determines the splitting strategy for each trial. Possible inputs are:\\n\\n        - float, to specify the percent of the test split\\n        - object to be used as cross-validation generator\\n\\n        This attribute is meant to give flexibility with stratified splitting\\n        but if a splitter is provided, it should only return one split and\\n        have shuffle set to True.\\n\\n    fbeta : float, 1.0 by default\\n        The strength of recall versus precision in the F-score.\\n\\n    argmax : str or None, default: \\'fscore\\'\\n        Annotate the threshold maximized by the supplied metric (see exclude\\n        for the possible metrics to use). If None or passed to exclude,\\n        will not annotate the graph.\\n\\n    exclude : str or list, optional\\n        Specify metrics to omit from the graph, can include:\\n\\n        - ``\"precision\"``\\n        - ``\"recall\"``\\n        - ``\"queue_rate\"``\\n        - ``\"fscore\"``\\n\\n        Excluded metrics will not be displayed in the graph, nor will they\\n        be available in ``thresholds_``; however, they will be computed on fit.\\n\\n    quantiles : sequence, default: np.array([0.1, 0.5, 0.9])\\n        Specify the quantiles to view model variability across a number of\\n        trials. Must be monotonic and have three elements such that the first\\n        element is the lower bound, the second is the drawn curve, and the\\n        third is the upper bound. By default the curve is drawn at the median,\\n        and the bounds from the 10th percentile to the 90th percentile.\\n\\n    random_state : int, optional\\n        Used to seed the random state for shuffling the data while composing\\n        different train and test splits. If supplied, the random state is\\n        incremented in a deterministic fashion for each split.\\n\\n        Note that if a splitter is provided, it\\'s random state will also be\\n        updated with this random state, even if it was previously set.\\n\\n    is_fitted : bool or str, default=\"auto\"\\n        Specify if the wrapped estimator is already fitted. If False, the estimator\\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\\n        modified. If \"auto\" (default), a helper method will check if the estimator\\n        is fitted before fitting it again.\\n\\n    force_model : bool, default: False\\n        Do not check to ensure that the underlying estimator is a classifier. This\\n        will prevent an exception when the visualizer is initialized but may result\\n        in unexpected or unintended behavior.\\n\\n    show : bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\\n        calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments passed to the visualizer base classes.\\n\\n    Notes\\n    -----\\n    The term \"discrimination threshold\" is rare in the literature. Here, we\\n    use it to mean the probability at which the positive class is selected\\n    over the negative class in binary classification.\\n\\n    Classification models must implement either a ``decision_function`` or\\n    ``predict_proba`` method in order to be used with this class. A\\n    ``YellowbrickTypeError`` is raised otherwise.\\n\\n    .. seealso::\\n        For a thorough explanation of discrimination thresholds, see:\\n        `Visualizing Machine Learning Thresholds to Make Better Business\\n        Decisions\\n        <http://blog.insightdatalabs.com/visualizing-classifier-thresholds/>`_\\n        by Insight Data.\\n\\n    Examples\\n    --------\\n    >>> from yellowbrick.classifier.threshold import discrimination_threshold\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> from yellowbrick.datasets import load_occupancy\\n    >>> X, y = load_occupancy()\\n    >>> model = LogisticRegression(multi_class=\"auto\", solver=\"liblinear\")\\n    >>> discrimination_threshold(model, X, y)\\n\\n    Returns\\n    -------\\n    viz : DiscriminationThreshold\\n        Returns the fitted and finalized visualizer object.\\n    '\n    visualizer = DiscriminationThreshold(estimator, ax=ax, n_trials=n_trials, cv=cv, fbeta=fbeta, argmax=argmax, exclude=exclude, quantiles=quantiles, random_state=random_state, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X, y)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer"
        ]
    }
]