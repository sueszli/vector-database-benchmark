[
    {
        "func_name": "convert_bertabs_checkpoints",
        "original": "def convert_bertabs_checkpoints(path_to_checkpoints, dump_path):\n    \"\"\"Copy/paste and tweak the pre-trained weights provided by the creators\n    of BertAbs for the internal architecture.\n    \"\"\"\n    config = BertAbsConfig(temp_dir='.', finetune_bert=False, large=False, share_emb=True, use_bert_emb=False, encoder='bert', max_pos=512, enc_layers=6, enc_hidden_size=512, enc_heads=8, enc_ff_size=512, enc_dropout=0.2, dec_layers=6, dec_hidden_size=768, dec_heads=8, dec_ff_size=2048, dec_dropout=0.2)\n    checkpoints = torch.load(path_to_checkpoints, lambda storage, loc: storage)\n    original = AbsSummarizer(config, torch.device('cpu'), checkpoints)\n    original.eval()\n    new_model = BertAbsSummarizer(config, torch.device('cpu'))\n    new_model.eval()\n    logging.info('convert the model')\n    new_model.bert.load_state_dict(original.bert.state_dict())\n    new_model.decoder.load_state_dict(original.decoder.state_dict())\n    new_model.generator.load_state_dict(original.generator.state_dict())\n    logging.info(\"Make sure that the models' outputs are identical\")\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    encoder_input_ids = tokenizer.encode(\"This is sample \u00e9\u00e0alj'-.\")\n    encoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(encoder_input_ids)))\n    encoder_input_ids = torch.tensor(encoder_input_ids).unsqueeze(0)\n    decoder_input_ids = tokenizer.encode(\"This is sample 3 \u00e9\u00e0alj'-.\")\n    decoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(decoder_input_ids)))\n    decoder_input_ids = torch.tensor(decoder_input_ids).unsqueeze(0)\n    assert torch.max(torch.abs(original.generator[0].weight - new_model.generator[0].weight)) == 0\n    src = encoder_input_ids\n    tgt = decoder_input_ids\n    segs = token_type_ids = None\n    clss = None\n    mask_src = encoder_attention_mask = None\n    mask_tgt = decoder_attention_mask = None\n    mask_cls = None\n    output_original_model = original(src, tgt, segs, clss, mask_src, mask_tgt, mask_cls)[0]\n    output_original_generator = original.generator(output_original_model)\n    output_converted_model = new_model(encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask)[0]\n    output_converted_generator = new_model.generator(output_converted_model)\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_model - output_original_model)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_generator - output_original_generator)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    are_identical = torch.allclose(output_converted_model, output_original_model, atol=0.001)\n    if are_identical:\n        logging.info('all weights are equal up to 1e-3')\n    else:\n        raise ValueError('the weights are different. The new model is likely different from the original one.')\n    logging.info(\"saving the model's state dictionary\")\n    torch.save(new_model.state_dict(), './bertabs-finetuned-cnndm-extractive-abstractive-summarization/pytorch_model.bin')",
        "mutated": [
            "def convert_bertabs_checkpoints(path_to_checkpoints, dump_path):\n    if False:\n        i = 10\n    'Copy/paste and tweak the pre-trained weights provided by the creators\\n    of BertAbs for the internal architecture.\\n    '\n    config = BertAbsConfig(temp_dir='.', finetune_bert=False, large=False, share_emb=True, use_bert_emb=False, encoder='bert', max_pos=512, enc_layers=6, enc_hidden_size=512, enc_heads=8, enc_ff_size=512, enc_dropout=0.2, dec_layers=6, dec_hidden_size=768, dec_heads=8, dec_ff_size=2048, dec_dropout=0.2)\n    checkpoints = torch.load(path_to_checkpoints, lambda storage, loc: storage)\n    original = AbsSummarizer(config, torch.device('cpu'), checkpoints)\n    original.eval()\n    new_model = BertAbsSummarizer(config, torch.device('cpu'))\n    new_model.eval()\n    logging.info('convert the model')\n    new_model.bert.load_state_dict(original.bert.state_dict())\n    new_model.decoder.load_state_dict(original.decoder.state_dict())\n    new_model.generator.load_state_dict(original.generator.state_dict())\n    logging.info(\"Make sure that the models' outputs are identical\")\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    encoder_input_ids = tokenizer.encode(\"This is sample \u00e9\u00e0alj'-.\")\n    encoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(encoder_input_ids)))\n    encoder_input_ids = torch.tensor(encoder_input_ids).unsqueeze(0)\n    decoder_input_ids = tokenizer.encode(\"This is sample 3 \u00e9\u00e0alj'-.\")\n    decoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(decoder_input_ids)))\n    decoder_input_ids = torch.tensor(decoder_input_ids).unsqueeze(0)\n    assert torch.max(torch.abs(original.generator[0].weight - new_model.generator[0].weight)) == 0\n    src = encoder_input_ids\n    tgt = decoder_input_ids\n    segs = token_type_ids = None\n    clss = None\n    mask_src = encoder_attention_mask = None\n    mask_tgt = decoder_attention_mask = None\n    mask_cls = None\n    output_original_model = original(src, tgt, segs, clss, mask_src, mask_tgt, mask_cls)[0]\n    output_original_generator = original.generator(output_original_model)\n    output_converted_model = new_model(encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask)[0]\n    output_converted_generator = new_model.generator(output_converted_model)\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_model - output_original_model)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_generator - output_original_generator)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    are_identical = torch.allclose(output_converted_model, output_original_model, atol=0.001)\n    if are_identical:\n        logging.info('all weights are equal up to 1e-3')\n    else:\n        raise ValueError('the weights are different. The new model is likely different from the original one.')\n    logging.info(\"saving the model's state dictionary\")\n    torch.save(new_model.state_dict(), './bertabs-finetuned-cnndm-extractive-abstractive-summarization/pytorch_model.bin')",
            "def convert_bertabs_checkpoints(path_to_checkpoints, dump_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Copy/paste and tweak the pre-trained weights provided by the creators\\n    of BertAbs for the internal architecture.\\n    '\n    config = BertAbsConfig(temp_dir='.', finetune_bert=False, large=False, share_emb=True, use_bert_emb=False, encoder='bert', max_pos=512, enc_layers=6, enc_hidden_size=512, enc_heads=8, enc_ff_size=512, enc_dropout=0.2, dec_layers=6, dec_hidden_size=768, dec_heads=8, dec_ff_size=2048, dec_dropout=0.2)\n    checkpoints = torch.load(path_to_checkpoints, lambda storage, loc: storage)\n    original = AbsSummarizer(config, torch.device('cpu'), checkpoints)\n    original.eval()\n    new_model = BertAbsSummarizer(config, torch.device('cpu'))\n    new_model.eval()\n    logging.info('convert the model')\n    new_model.bert.load_state_dict(original.bert.state_dict())\n    new_model.decoder.load_state_dict(original.decoder.state_dict())\n    new_model.generator.load_state_dict(original.generator.state_dict())\n    logging.info(\"Make sure that the models' outputs are identical\")\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    encoder_input_ids = tokenizer.encode(\"This is sample \u00e9\u00e0alj'-.\")\n    encoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(encoder_input_ids)))\n    encoder_input_ids = torch.tensor(encoder_input_ids).unsqueeze(0)\n    decoder_input_ids = tokenizer.encode(\"This is sample 3 \u00e9\u00e0alj'-.\")\n    decoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(decoder_input_ids)))\n    decoder_input_ids = torch.tensor(decoder_input_ids).unsqueeze(0)\n    assert torch.max(torch.abs(original.generator[0].weight - new_model.generator[0].weight)) == 0\n    src = encoder_input_ids\n    tgt = decoder_input_ids\n    segs = token_type_ids = None\n    clss = None\n    mask_src = encoder_attention_mask = None\n    mask_tgt = decoder_attention_mask = None\n    mask_cls = None\n    output_original_model = original(src, tgt, segs, clss, mask_src, mask_tgt, mask_cls)[0]\n    output_original_generator = original.generator(output_original_model)\n    output_converted_model = new_model(encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask)[0]\n    output_converted_generator = new_model.generator(output_converted_model)\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_model - output_original_model)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_generator - output_original_generator)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    are_identical = torch.allclose(output_converted_model, output_original_model, atol=0.001)\n    if are_identical:\n        logging.info('all weights are equal up to 1e-3')\n    else:\n        raise ValueError('the weights are different. The new model is likely different from the original one.')\n    logging.info(\"saving the model's state dictionary\")\n    torch.save(new_model.state_dict(), './bertabs-finetuned-cnndm-extractive-abstractive-summarization/pytorch_model.bin')",
            "def convert_bertabs_checkpoints(path_to_checkpoints, dump_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Copy/paste and tweak the pre-trained weights provided by the creators\\n    of BertAbs for the internal architecture.\\n    '\n    config = BertAbsConfig(temp_dir='.', finetune_bert=False, large=False, share_emb=True, use_bert_emb=False, encoder='bert', max_pos=512, enc_layers=6, enc_hidden_size=512, enc_heads=8, enc_ff_size=512, enc_dropout=0.2, dec_layers=6, dec_hidden_size=768, dec_heads=8, dec_ff_size=2048, dec_dropout=0.2)\n    checkpoints = torch.load(path_to_checkpoints, lambda storage, loc: storage)\n    original = AbsSummarizer(config, torch.device('cpu'), checkpoints)\n    original.eval()\n    new_model = BertAbsSummarizer(config, torch.device('cpu'))\n    new_model.eval()\n    logging.info('convert the model')\n    new_model.bert.load_state_dict(original.bert.state_dict())\n    new_model.decoder.load_state_dict(original.decoder.state_dict())\n    new_model.generator.load_state_dict(original.generator.state_dict())\n    logging.info(\"Make sure that the models' outputs are identical\")\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    encoder_input_ids = tokenizer.encode(\"This is sample \u00e9\u00e0alj'-.\")\n    encoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(encoder_input_ids)))\n    encoder_input_ids = torch.tensor(encoder_input_ids).unsqueeze(0)\n    decoder_input_ids = tokenizer.encode(\"This is sample 3 \u00e9\u00e0alj'-.\")\n    decoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(decoder_input_ids)))\n    decoder_input_ids = torch.tensor(decoder_input_ids).unsqueeze(0)\n    assert torch.max(torch.abs(original.generator[0].weight - new_model.generator[0].weight)) == 0\n    src = encoder_input_ids\n    tgt = decoder_input_ids\n    segs = token_type_ids = None\n    clss = None\n    mask_src = encoder_attention_mask = None\n    mask_tgt = decoder_attention_mask = None\n    mask_cls = None\n    output_original_model = original(src, tgt, segs, clss, mask_src, mask_tgt, mask_cls)[0]\n    output_original_generator = original.generator(output_original_model)\n    output_converted_model = new_model(encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask)[0]\n    output_converted_generator = new_model.generator(output_converted_model)\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_model - output_original_model)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_generator - output_original_generator)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    are_identical = torch.allclose(output_converted_model, output_original_model, atol=0.001)\n    if are_identical:\n        logging.info('all weights are equal up to 1e-3')\n    else:\n        raise ValueError('the weights are different. The new model is likely different from the original one.')\n    logging.info(\"saving the model's state dictionary\")\n    torch.save(new_model.state_dict(), './bertabs-finetuned-cnndm-extractive-abstractive-summarization/pytorch_model.bin')",
            "def convert_bertabs_checkpoints(path_to_checkpoints, dump_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Copy/paste and tweak the pre-trained weights provided by the creators\\n    of BertAbs for the internal architecture.\\n    '\n    config = BertAbsConfig(temp_dir='.', finetune_bert=False, large=False, share_emb=True, use_bert_emb=False, encoder='bert', max_pos=512, enc_layers=6, enc_hidden_size=512, enc_heads=8, enc_ff_size=512, enc_dropout=0.2, dec_layers=6, dec_hidden_size=768, dec_heads=8, dec_ff_size=2048, dec_dropout=0.2)\n    checkpoints = torch.load(path_to_checkpoints, lambda storage, loc: storage)\n    original = AbsSummarizer(config, torch.device('cpu'), checkpoints)\n    original.eval()\n    new_model = BertAbsSummarizer(config, torch.device('cpu'))\n    new_model.eval()\n    logging.info('convert the model')\n    new_model.bert.load_state_dict(original.bert.state_dict())\n    new_model.decoder.load_state_dict(original.decoder.state_dict())\n    new_model.generator.load_state_dict(original.generator.state_dict())\n    logging.info(\"Make sure that the models' outputs are identical\")\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    encoder_input_ids = tokenizer.encode(\"This is sample \u00e9\u00e0alj'-.\")\n    encoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(encoder_input_ids)))\n    encoder_input_ids = torch.tensor(encoder_input_ids).unsqueeze(0)\n    decoder_input_ids = tokenizer.encode(\"This is sample 3 \u00e9\u00e0alj'-.\")\n    decoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(decoder_input_ids)))\n    decoder_input_ids = torch.tensor(decoder_input_ids).unsqueeze(0)\n    assert torch.max(torch.abs(original.generator[0].weight - new_model.generator[0].weight)) == 0\n    src = encoder_input_ids\n    tgt = decoder_input_ids\n    segs = token_type_ids = None\n    clss = None\n    mask_src = encoder_attention_mask = None\n    mask_tgt = decoder_attention_mask = None\n    mask_cls = None\n    output_original_model = original(src, tgt, segs, clss, mask_src, mask_tgt, mask_cls)[0]\n    output_original_generator = original.generator(output_original_model)\n    output_converted_model = new_model(encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask)[0]\n    output_converted_generator = new_model.generator(output_converted_model)\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_model - output_original_model)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_generator - output_original_generator)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    are_identical = torch.allclose(output_converted_model, output_original_model, atol=0.001)\n    if are_identical:\n        logging.info('all weights are equal up to 1e-3')\n    else:\n        raise ValueError('the weights are different. The new model is likely different from the original one.')\n    logging.info(\"saving the model's state dictionary\")\n    torch.save(new_model.state_dict(), './bertabs-finetuned-cnndm-extractive-abstractive-summarization/pytorch_model.bin')",
            "def convert_bertabs_checkpoints(path_to_checkpoints, dump_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Copy/paste and tweak the pre-trained weights provided by the creators\\n    of BertAbs for the internal architecture.\\n    '\n    config = BertAbsConfig(temp_dir='.', finetune_bert=False, large=False, share_emb=True, use_bert_emb=False, encoder='bert', max_pos=512, enc_layers=6, enc_hidden_size=512, enc_heads=8, enc_ff_size=512, enc_dropout=0.2, dec_layers=6, dec_hidden_size=768, dec_heads=8, dec_ff_size=2048, dec_dropout=0.2)\n    checkpoints = torch.load(path_to_checkpoints, lambda storage, loc: storage)\n    original = AbsSummarizer(config, torch.device('cpu'), checkpoints)\n    original.eval()\n    new_model = BertAbsSummarizer(config, torch.device('cpu'))\n    new_model.eval()\n    logging.info('convert the model')\n    new_model.bert.load_state_dict(original.bert.state_dict())\n    new_model.decoder.load_state_dict(original.decoder.state_dict())\n    new_model.generator.load_state_dict(original.generator.state_dict())\n    logging.info(\"Make sure that the models' outputs are identical\")\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    encoder_input_ids = tokenizer.encode(\"This is sample \u00e9\u00e0alj'-.\")\n    encoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(encoder_input_ids)))\n    encoder_input_ids = torch.tensor(encoder_input_ids).unsqueeze(0)\n    decoder_input_ids = tokenizer.encode(\"This is sample 3 \u00e9\u00e0alj'-.\")\n    decoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(decoder_input_ids)))\n    decoder_input_ids = torch.tensor(decoder_input_ids).unsqueeze(0)\n    assert torch.max(torch.abs(original.generator[0].weight - new_model.generator[0].weight)) == 0\n    src = encoder_input_ids\n    tgt = decoder_input_ids\n    segs = token_type_ids = None\n    clss = None\n    mask_src = encoder_attention_mask = None\n    mask_tgt = decoder_attention_mask = None\n    mask_cls = None\n    output_original_model = original(src, tgt, segs, clss, mask_src, mask_tgt, mask_cls)[0]\n    output_original_generator = original.generator(output_original_model)\n    output_converted_model = new_model(encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask)[0]\n    output_converted_generator = new_model.generator(output_converted_model)\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_model - output_original_model)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    maximum_absolute_difference = torch.max(torch.abs(output_converted_generator - output_original_generator)).item()\n    print('Maximum absolute difference beween weights: {:.2f}'.format(maximum_absolute_difference))\n    are_identical = torch.allclose(output_converted_model, output_original_model, atol=0.001)\n    if are_identical:\n        logging.info('all weights are equal up to 1e-3')\n    else:\n        raise ValueError('the weights are different. The new model is likely different from the original one.')\n    logging.info(\"saving the model's state dictionary\")\n    torch.save(new_model.state_dict(), './bertabs-finetuned-cnndm-extractive-abstractive-summarization/pytorch_model.bin')"
        ]
    }
]