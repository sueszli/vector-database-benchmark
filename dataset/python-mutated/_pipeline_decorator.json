[
    {
        "func_name": "pipeline",
        "original": "@overload\ndef pipeline(func: None=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[[Callable[P, T]], Callable[P, PipelineJob]]:\n    ...",
        "mutated": [
            "@overload\ndef pipeline(func: None=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[[Callable[P, T]], Callable[P, PipelineJob]]:\n    if False:\n        i = 10\n    ...",
            "@overload\ndef pipeline(func: None=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[[Callable[P, T]], Callable[P, PipelineJob]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ...",
            "@overload\ndef pipeline(func: None=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[[Callable[P, T]], Callable[P, PipelineJob]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ...",
            "@overload\ndef pipeline(func: None=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[[Callable[P, T]], Callable[P, PipelineJob]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ...",
            "@overload\ndef pipeline(func: None=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[[Callable[P, T]], Callable[P, PipelineJob]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ..."
        ]
    },
    {
        "func_name": "pipeline",
        "original": "@overload\ndef pipeline(func: Callable[P, T]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[P, PipelineJob]:\n    ...",
        "mutated": [
            "@overload\ndef pipeline(func: Callable[P, T]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[P, PipelineJob]:\n    if False:\n        i = 10\n    ...",
            "@overload\ndef pipeline(func: Callable[P, T]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[P, PipelineJob]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ...",
            "@overload\ndef pipeline(func: Callable[P, T]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[P, PipelineJob]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ...",
            "@overload\ndef pipeline(func: Callable[P, T]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[P, PipelineJob]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ...",
            "@overload\ndef pipeline(func: Callable[P, T]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Callable[P, PipelineJob]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ..."
        ]
    },
    {
        "func_name": "wrapper",
        "original": "@wraps(func)\ndef wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n    _dsl_settings_stack.push()\n    try:\n        provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n        pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n        pipeline_builder._update_inputs(pipeline_parameters)\n        non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n        pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n    finally:\n        dsl_settings = _dsl_settings_stack.pop()\n    if dsl_settings.init_job_set:\n        job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n    if dsl_settings.finalize_job_set:\n        job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n    common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n    if _is_inside_dsl_pipeline_func() or get_component:\n        if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n            raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n        built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n        if job_settings:\n            module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n    else:\n        built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n    return built_pipeline",
        "mutated": [
            "@wraps(func)\ndef wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n    if False:\n        i = 10\n    _dsl_settings_stack.push()\n    try:\n        provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n        pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n        pipeline_builder._update_inputs(pipeline_parameters)\n        non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n        pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n    finally:\n        dsl_settings = _dsl_settings_stack.pop()\n    if dsl_settings.init_job_set:\n        job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n    if dsl_settings.finalize_job_set:\n        job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n    common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n    if _is_inside_dsl_pipeline_func() or get_component:\n        if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n            raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n        built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n        if job_settings:\n            module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n    else:\n        built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n    return built_pipeline",
            "@wraps(func)\ndef wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    _dsl_settings_stack.push()\n    try:\n        provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n        pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n        pipeline_builder._update_inputs(pipeline_parameters)\n        non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n        pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n    finally:\n        dsl_settings = _dsl_settings_stack.pop()\n    if dsl_settings.init_job_set:\n        job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n    if dsl_settings.finalize_job_set:\n        job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n    common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n    if _is_inside_dsl_pipeline_func() or get_component:\n        if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n            raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n        built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n        if job_settings:\n            module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n    else:\n        built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n    return built_pipeline",
            "@wraps(func)\ndef wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    _dsl_settings_stack.push()\n    try:\n        provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n        pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n        pipeline_builder._update_inputs(pipeline_parameters)\n        non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n        pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n    finally:\n        dsl_settings = _dsl_settings_stack.pop()\n    if dsl_settings.init_job_set:\n        job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n    if dsl_settings.finalize_job_set:\n        job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n    common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n    if _is_inside_dsl_pipeline_func() or get_component:\n        if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n            raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n        built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n        if job_settings:\n            module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n    else:\n        built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n    return built_pipeline",
            "@wraps(func)\ndef wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    _dsl_settings_stack.push()\n    try:\n        provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n        pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n        pipeline_builder._update_inputs(pipeline_parameters)\n        non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n        pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n    finally:\n        dsl_settings = _dsl_settings_stack.pop()\n    if dsl_settings.init_job_set:\n        job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n    if dsl_settings.finalize_job_set:\n        job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n    common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n    if _is_inside_dsl_pipeline_func() or get_component:\n        if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n            raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n        built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n        if job_settings:\n            module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n    else:\n        built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n    return built_pipeline",
            "@wraps(func)\ndef wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    _dsl_settings_stack.push()\n    try:\n        provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n        pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n        pipeline_builder._update_inputs(pipeline_parameters)\n        non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n        pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n    finally:\n        dsl_settings = _dsl_settings_stack.pop()\n    if dsl_settings.init_job_set:\n        job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n    if dsl_settings.finalize_job_set:\n        job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n    common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n    if _is_inside_dsl_pipeline_func() or get_component:\n        if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n            raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n        built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n        if job_settings:\n            module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n    else:\n        built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n    return built_pipeline"
        ]
    },
    {
        "func_name": "pipeline_decorator",
        "original": "def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n    if not isinstance(func, Callable):\n        raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n    non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n    compute = kwargs.get('compute', None)\n    default_compute_target = kwargs.get('default_compute_target', None)\n    default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n    continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n    on_init = kwargs.get('on_init', None)\n    on_finalize = kwargs.get('on_finalize', None)\n    default_datastore = kwargs.get('default_datastore', None)\n    force_rerun = kwargs.get('force_rerun', None)\n    job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n    func_entry_path = _resolve_source_file()\n    if not func_entry_path:\n        func_path = Path(inspect.getfile(func))\n        if func_path.exists():\n            func_entry_path = func_path.resolve().absolute()\n    job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n    pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n    @wraps(func)\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n        _dsl_settings_stack.push()\n        try:\n            provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n            pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n            pipeline_builder._update_inputs(pipeline_parameters)\n            non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n            pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n        finally:\n            dsl_settings = _dsl_settings_stack.pop()\n        if dsl_settings.init_job_set:\n            job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n        if dsl_settings.finalize_job_set:\n            job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n        common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n        if _is_inside_dsl_pipeline_func() or get_component:\n            if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n            built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n            if job_settings:\n                module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n        else:\n            built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n        return built_pipeline\n    wrapper._is_dsl_func = True\n    wrapper._job_settings = job_settings\n    wrapper._pipeline_builder = pipeline_builder\n    return wrapper",
        "mutated": [
            "def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n    if False:\n        i = 10\n    if not isinstance(func, Callable):\n        raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n    non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n    compute = kwargs.get('compute', None)\n    default_compute_target = kwargs.get('default_compute_target', None)\n    default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n    continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n    on_init = kwargs.get('on_init', None)\n    on_finalize = kwargs.get('on_finalize', None)\n    default_datastore = kwargs.get('default_datastore', None)\n    force_rerun = kwargs.get('force_rerun', None)\n    job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n    func_entry_path = _resolve_source_file()\n    if not func_entry_path:\n        func_path = Path(inspect.getfile(func))\n        if func_path.exists():\n            func_entry_path = func_path.resolve().absolute()\n    job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n    pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n    @wraps(func)\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n        _dsl_settings_stack.push()\n        try:\n            provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n            pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n            pipeline_builder._update_inputs(pipeline_parameters)\n            non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n            pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n        finally:\n            dsl_settings = _dsl_settings_stack.pop()\n        if dsl_settings.init_job_set:\n            job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n        if dsl_settings.finalize_job_set:\n            job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n        common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n        if _is_inside_dsl_pipeline_func() or get_component:\n            if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n            built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n            if job_settings:\n                module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n        else:\n            built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n        return built_pipeline\n    wrapper._is_dsl_func = True\n    wrapper._job_settings = job_settings\n    wrapper._pipeline_builder = pipeline_builder\n    return wrapper",
            "def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not isinstance(func, Callable):\n        raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n    non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n    compute = kwargs.get('compute', None)\n    default_compute_target = kwargs.get('default_compute_target', None)\n    default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n    continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n    on_init = kwargs.get('on_init', None)\n    on_finalize = kwargs.get('on_finalize', None)\n    default_datastore = kwargs.get('default_datastore', None)\n    force_rerun = kwargs.get('force_rerun', None)\n    job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n    func_entry_path = _resolve_source_file()\n    if not func_entry_path:\n        func_path = Path(inspect.getfile(func))\n        if func_path.exists():\n            func_entry_path = func_path.resolve().absolute()\n    job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n    pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n    @wraps(func)\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n        _dsl_settings_stack.push()\n        try:\n            provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n            pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n            pipeline_builder._update_inputs(pipeline_parameters)\n            non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n            pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n        finally:\n            dsl_settings = _dsl_settings_stack.pop()\n        if dsl_settings.init_job_set:\n            job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n        if dsl_settings.finalize_job_set:\n            job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n        common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n        if _is_inside_dsl_pipeline_func() or get_component:\n            if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n            built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n            if job_settings:\n                module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n        else:\n            built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n        return built_pipeline\n    wrapper._is_dsl_func = True\n    wrapper._job_settings = job_settings\n    wrapper._pipeline_builder = pipeline_builder\n    return wrapper",
            "def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not isinstance(func, Callable):\n        raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n    non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n    compute = kwargs.get('compute', None)\n    default_compute_target = kwargs.get('default_compute_target', None)\n    default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n    continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n    on_init = kwargs.get('on_init', None)\n    on_finalize = kwargs.get('on_finalize', None)\n    default_datastore = kwargs.get('default_datastore', None)\n    force_rerun = kwargs.get('force_rerun', None)\n    job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n    func_entry_path = _resolve_source_file()\n    if not func_entry_path:\n        func_path = Path(inspect.getfile(func))\n        if func_path.exists():\n            func_entry_path = func_path.resolve().absolute()\n    job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n    pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n    @wraps(func)\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n        _dsl_settings_stack.push()\n        try:\n            provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n            pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n            pipeline_builder._update_inputs(pipeline_parameters)\n            non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n            pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n        finally:\n            dsl_settings = _dsl_settings_stack.pop()\n        if dsl_settings.init_job_set:\n            job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n        if dsl_settings.finalize_job_set:\n            job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n        common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n        if _is_inside_dsl_pipeline_func() or get_component:\n            if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n            built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n            if job_settings:\n                module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n        else:\n            built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n        return built_pipeline\n    wrapper._is_dsl_func = True\n    wrapper._job_settings = job_settings\n    wrapper._pipeline_builder = pipeline_builder\n    return wrapper",
            "def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not isinstance(func, Callable):\n        raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n    non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n    compute = kwargs.get('compute', None)\n    default_compute_target = kwargs.get('default_compute_target', None)\n    default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n    continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n    on_init = kwargs.get('on_init', None)\n    on_finalize = kwargs.get('on_finalize', None)\n    default_datastore = kwargs.get('default_datastore', None)\n    force_rerun = kwargs.get('force_rerun', None)\n    job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n    func_entry_path = _resolve_source_file()\n    if not func_entry_path:\n        func_path = Path(inspect.getfile(func))\n        if func_path.exists():\n            func_entry_path = func_path.resolve().absolute()\n    job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n    pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n    @wraps(func)\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n        _dsl_settings_stack.push()\n        try:\n            provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n            pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n            pipeline_builder._update_inputs(pipeline_parameters)\n            non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n            pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n        finally:\n            dsl_settings = _dsl_settings_stack.pop()\n        if dsl_settings.init_job_set:\n            job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n        if dsl_settings.finalize_job_set:\n            job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n        common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n        if _is_inside_dsl_pipeline_func() or get_component:\n            if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n            built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n            if job_settings:\n                module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n        else:\n            built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n        return built_pipeline\n    wrapper._is_dsl_func = True\n    wrapper._job_settings = job_settings\n    wrapper._pipeline_builder = pipeline_builder\n    return wrapper",
            "def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not isinstance(func, Callable):\n        raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n    non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n    compute = kwargs.get('compute', None)\n    default_compute_target = kwargs.get('default_compute_target', None)\n    default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n    continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n    on_init = kwargs.get('on_init', None)\n    on_finalize = kwargs.get('on_finalize', None)\n    default_datastore = kwargs.get('default_datastore', None)\n    force_rerun = kwargs.get('force_rerun', None)\n    job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n    func_entry_path = _resolve_source_file()\n    if not func_entry_path:\n        func_path = Path(inspect.getfile(func))\n        if func_path.exists():\n            func_entry_path = func_path.resolve().absolute()\n    job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n    pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n    @wraps(func)\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n        _dsl_settings_stack.push()\n        try:\n            provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n            pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n            pipeline_builder._update_inputs(pipeline_parameters)\n            non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n            pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n        finally:\n            dsl_settings = _dsl_settings_stack.pop()\n        if dsl_settings.init_job_set:\n            job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n        if dsl_settings.finalize_job_set:\n            job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n        common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n        if _is_inside_dsl_pipeline_func() or get_component:\n            if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n            built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n            if job_settings:\n                module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n        else:\n            built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n        return built_pipeline\n    wrapper._is_dsl_func = True\n    wrapper._job_settings = job_settings\n    wrapper._pipeline_builder = pipeline_builder\n    return wrapper"
        ]
    },
    {
        "func_name": "pipeline",
        "original": "def pipeline(func: Optional[Callable[P, T]]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Union[Callable[[Callable[P, T]], Callable[P, PipelineJob]], Callable[P, PipelineJob]]:\n    \"\"\"Build a pipeline which contains all component nodes defined in this function.\n\n    :param func: The user pipeline function to be decorated.\n    :type func: types.FunctionType\n    :keyword name: The name of pipeline component, defaults to function name.\n    :paramtype name: str\n    :keyword version: The version of pipeline component, defaults to \"1\".\n    :paramtype version: str\n    :keyword display_name: The display name of pipeline component, defaults to function name.\n    :paramtype display_name: str\n    :keyword description: The description of the built pipeline.\n    :paramtype description: str\n    :keyword experiment_name: Name of the experiment the job will be created under,                 if None is provided, experiment will be set to current directory.\n    :paramtype experiment_name: str\n    :keyword tags: The tags of pipeline component.\n    :paramtype tags: dict[str, str]\n    :keyword kwargs: A dictionary of additional configuration parameters.\n    :paramtype kwargs: dict\n\n    .. admonition:: Example:\n\n        .. literalinclude:: ../../../../samples/ml_samples_pipeline_job_configurations.py\n            :start-after: [START configure_pipeline]\n            :end-before: [END configure_pipeline]\n            :language: python\n            :dedent: 8\n            :caption: Shows how to create a pipeline using this decorator.\n    :return: Either\n      * A decorator, if `func` is None\n      * The decorated `func`\n    :rtype: Union[\n        Callable[[Callable], Callable[..., PipelineJob]],\n        Callable[P, PipelineJob]\n      ]\n    \"\"\"\n    get_component = kwargs.get('get_component', False)\n\n    def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n        if not isinstance(func, Callable):\n            raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n        non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n        compute = kwargs.get('compute', None)\n        default_compute_target = kwargs.get('default_compute_target', None)\n        default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n        continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n        on_init = kwargs.get('on_init', None)\n        on_finalize = kwargs.get('on_finalize', None)\n        default_datastore = kwargs.get('default_datastore', None)\n        force_rerun = kwargs.get('force_rerun', None)\n        job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n        func_entry_path = _resolve_source_file()\n        if not func_entry_path:\n            func_path = Path(inspect.getfile(func))\n            if func_path.exists():\n                func_entry_path = func_path.resolve().absolute()\n        job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n        pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n        @wraps(func)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n            _dsl_settings_stack.push()\n            try:\n                provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n                pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n                pipeline_builder._update_inputs(pipeline_parameters)\n                non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n                pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n            finally:\n                dsl_settings = _dsl_settings_stack.pop()\n            if dsl_settings.init_job_set:\n                job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n            if dsl_settings.finalize_job_set:\n                job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n            common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n            if _is_inside_dsl_pipeline_func() or get_component:\n                if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                    raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n                built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n                if job_settings:\n                    module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n            else:\n                built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n            return built_pipeline\n        wrapper._is_dsl_func = True\n        wrapper._job_settings = job_settings\n        wrapper._pipeline_builder = pipeline_builder\n        return wrapper\n    if func is not None:\n        return pipeline_decorator(func)\n    return pipeline_decorator",
        "mutated": [
            "def pipeline(func: Optional[Callable[P, T]]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Union[Callable[[Callable[P, T]], Callable[P, PipelineJob]], Callable[P, PipelineJob]]:\n    if False:\n        i = 10\n    'Build a pipeline which contains all component nodes defined in this function.\\n\\n    :param func: The user pipeline function to be decorated.\\n    :type func: types.FunctionType\\n    :keyword name: The name of pipeline component, defaults to function name.\\n    :paramtype name: str\\n    :keyword version: The version of pipeline component, defaults to \"1\".\\n    :paramtype version: str\\n    :keyword display_name: The display name of pipeline component, defaults to function name.\\n    :paramtype display_name: str\\n    :keyword description: The description of the built pipeline.\\n    :paramtype description: str\\n    :keyword experiment_name: Name of the experiment the job will be created under,                 if None is provided, experiment will be set to current directory.\\n    :paramtype experiment_name: str\\n    :keyword tags: The tags of pipeline component.\\n    :paramtype tags: dict[str, str]\\n    :keyword kwargs: A dictionary of additional configuration parameters.\\n    :paramtype kwargs: dict\\n\\n    .. admonition:: Example:\\n\\n        .. literalinclude:: ../../../../samples/ml_samples_pipeline_job_configurations.py\\n            :start-after: [START configure_pipeline]\\n            :end-before: [END configure_pipeline]\\n            :language: python\\n            :dedent: 8\\n            :caption: Shows how to create a pipeline using this decorator.\\n    :return: Either\\n      * A decorator, if `func` is None\\n      * The decorated `func`\\n    :rtype: Union[\\n        Callable[[Callable], Callable[..., PipelineJob]],\\n        Callable[P, PipelineJob]\\n      ]\\n    '\n    get_component = kwargs.get('get_component', False)\n\n    def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n        if not isinstance(func, Callable):\n            raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n        non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n        compute = kwargs.get('compute', None)\n        default_compute_target = kwargs.get('default_compute_target', None)\n        default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n        continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n        on_init = kwargs.get('on_init', None)\n        on_finalize = kwargs.get('on_finalize', None)\n        default_datastore = kwargs.get('default_datastore', None)\n        force_rerun = kwargs.get('force_rerun', None)\n        job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n        func_entry_path = _resolve_source_file()\n        if not func_entry_path:\n            func_path = Path(inspect.getfile(func))\n            if func_path.exists():\n                func_entry_path = func_path.resolve().absolute()\n        job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n        pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n        @wraps(func)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n            _dsl_settings_stack.push()\n            try:\n                provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n                pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n                pipeline_builder._update_inputs(pipeline_parameters)\n                non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n                pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n            finally:\n                dsl_settings = _dsl_settings_stack.pop()\n            if dsl_settings.init_job_set:\n                job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n            if dsl_settings.finalize_job_set:\n                job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n            common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n            if _is_inside_dsl_pipeline_func() or get_component:\n                if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                    raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n                built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n                if job_settings:\n                    module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n            else:\n                built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n            return built_pipeline\n        wrapper._is_dsl_func = True\n        wrapper._job_settings = job_settings\n        wrapper._pipeline_builder = pipeline_builder\n        return wrapper\n    if func is not None:\n        return pipeline_decorator(func)\n    return pipeline_decorator",
            "def pipeline(func: Optional[Callable[P, T]]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Union[Callable[[Callable[P, T]], Callable[P, PipelineJob]], Callable[P, PipelineJob]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Build a pipeline which contains all component nodes defined in this function.\\n\\n    :param func: The user pipeline function to be decorated.\\n    :type func: types.FunctionType\\n    :keyword name: The name of pipeline component, defaults to function name.\\n    :paramtype name: str\\n    :keyword version: The version of pipeline component, defaults to \"1\".\\n    :paramtype version: str\\n    :keyword display_name: The display name of pipeline component, defaults to function name.\\n    :paramtype display_name: str\\n    :keyword description: The description of the built pipeline.\\n    :paramtype description: str\\n    :keyword experiment_name: Name of the experiment the job will be created under,                 if None is provided, experiment will be set to current directory.\\n    :paramtype experiment_name: str\\n    :keyword tags: The tags of pipeline component.\\n    :paramtype tags: dict[str, str]\\n    :keyword kwargs: A dictionary of additional configuration parameters.\\n    :paramtype kwargs: dict\\n\\n    .. admonition:: Example:\\n\\n        .. literalinclude:: ../../../../samples/ml_samples_pipeline_job_configurations.py\\n            :start-after: [START configure_pipeline]\\n            :end-before: [END configure_pipeline]\\n            :language: python\\n            :dedent: 8\\n            :caption: Shows how to create a pipeline using this decorator.\\n    :return: Either\\n      * A decorator, if `func` is None\\n      * The decorated `func`\\n    :rtype: Union[\\n        Callable[[Callable], Callable[..., PipelineJob]],\\n        Callable[P, PipelineJob]\\n      ]\\n    '\n    get_component = kwargs.get('get_component', False)\n\n    def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n        if not isinstance(func, Callable):\n            raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n        non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n        compute = kwargs.get('compute', None)\n        default_compute_target = kwargs.get('default_compute_target', None)\n        default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n        continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n        on_init = kwargs.get('on_init', None)\n        on_finalize = kwargs.get('on_finalize', None)\n        default_datastore = kwargs.get('default_datastore', None)\n        force_rerun = kwargs.get('force_rerun', None)\n        job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n        func_entry_path = _resolve_source_file()\n        if not func_entry_path:\n            func_path = Path(inspect.getfile(func))\n            if func_path.exists():\n                func_entry_path = func_path.resolve().absolute()\n        job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n        pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n        @wraps(func)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n            _dsl_settings_stack.push()\n            try:\n                provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n                pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n                pipeline_builder._update_inputs(pipeline_parameters)\n                non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n                pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n            finally:\n                dsl_settings = _dsl_settings_stack.pop()\n            if dsl_settings.init_job_set:\n                job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n            if dsl_settings.finalize_job_set:\n                job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n            common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n            if _is_inside_dsl_pipeline_func() or get_component:\n                if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                    raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n                built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n                if job_settings:\n                    module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n            else:\n                built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n            return built_pipeline\n        wrapper._is_dsl_func = True\n        wrapper._job_settings = job_settings\n        wrapper._pipeline_builder = pipeline_builder\n        return wrapper\n    if func is not None:\n        return pipeline_decorator(func)\n    return pipeline_decorator",
            "def pipeline(func: Optional[Callable[P, T]]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Union[Callable[[Callable[P, T]], Callable[P, PipelineJob]], Callable[P, PipelineJob]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Build a pipeline which contains all component nodes defined in this function.\\n\\n    :param func: The user pipeline function to be decorated.\\n    :type func: types.FunctionType\\n    :keyword name: The name of pipeline component, defaults to function name.\\n    :paramtype name: str\\n    :keyword version: The version of pipeline component, defaults to \"1\".\\n    :paramtype version: str\\n    :keyword display_name: The display name of pipeline component, defaults to function name.\\n    :paramtype display_name: str\\n    :keyword description: The description of the built pipeline.\\n    :paramtype description: str\\n    :keyword experiment_name: Name of the experiment the job will be created under,                 if None is provided, experiment will be set to current directory.\\n    :paramtype experiment_name: str\\n    :keyword tags: The tags of pipeline component.\\n    :paramtype tags: dict[str, str]\\n    :keyword kwargs: A dictionary of additional configuration parameters.\\n    :paramtype kwargs: dict\\n\\n    .. admonition:: Example:\\n\\n        .. literalinclude:: ../../../../samples/ml_samples_pipeline_job_configurations.py\\n            :start-after: [START configure_pipeline]\\n            :end-before: [END configure_pipeline]\\n            :language: python\\n            :dedent: 8\\n            :caption: Shows how to create a pipeline using this decorator.\\n    :return: Either\\n      * A decorator, if `func` is None\\n      * The decorated `func`\\n    :rtype: Union[\\n        Callable[[Callable], Callable[..., PipelineJob]],\\n        Callable[P, PipelineJob]\\n      ]\\n    '\n    get_component = kwargs.get('get_component', False)\n\n    def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n        if not isinstance(func, Callable):\n            raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n        non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n        compute = kwargs.get('compute', None)\n        default_compute_target = kwargs.get('default_compute_target', None)\n        default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n        continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n        on_init = kwargs.get('on_init', None)\n        on_finalize = kwargs.get('on_finalize', None)\n        default_datastore = kwargs.get('default_datastore', None)\n        force_rerun = kwargs.get('force_rerun', None)\n        job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n        func_entry_path = _resolve_source_file()\n        if not func_entry_path:\n            func_path = Path(inspect.getfile(func))\n            if func_path.exists():\n                func_entry_path = func_path.resolve().absolute()\n        job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n        pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n        @wraps(func)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n            _dsl_settings_stack.push()\n            try:\n                provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n                pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n                pipeline_builder._update_inputs(pipeline_parameters)\n                non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n                pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n            finally:\n                dsl_settings = _dsl_settings_stack.pop()\n            if dsl_settings.init_job_set:\n                job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n            if dsl_settings.finalize_job_set:\n                job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n            common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n            if _is_inside_dsl_pipeline_func() or get_component:\n                if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                    raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n                built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n                if job_settings:\n                    module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n            else:\n                built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n            return built_pipeline\n        wrapper._is_dsl_func = True\n        wrapper._job_settings = job_settings\n        wrapper._pipeline_builder = pipeline_builder\n        return wrapper\n    if func is not None:\n        return pipeline_decorator(func)\n    return pipeline_decorator",
            "def pipeline(func: Optional[Callable[P, T]]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Union[Callable[[Callable[P, T]], Callable[P, PipelineJob]], Callable[P, PipelineJob]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Build a pipeline which contains all component nodes defined in this function.\\n\\n    :param func: The user pipeline function to be decorated.\\n    :type func: types.FunctionType\\n    :keyword name: The name of pipeline component, defaults to function name.\\n    :paramtype name: str\\n    :keyword version: The version of pipeline component, defaults to \"1\".\\n    :paramtype version: str\\n    :keyword display_name: The display name of pipeline component, defaults to function name.\\n    :paramtype display_name: str\\n    :keyword description: The description of the built pipeline.\\n    :paramtype description: str\\n    :keyword experiment_name: Name of the experiment the job will be created under,                 if None is provided, experiment will be set to current directory.\\n    :paramtype experiment_name: str\\n    :keyword tags: The tags of pipeline component.\\n    :paramtype tags: dict[str, str]\\n    :keyword kwargs: A dictionary of additional configuration parameters.\\n    :paramtype kwargs: dict\\n\\n    .. admonition:: Example:\\n\\n        .. literalinclude:: ../../../../samples/ml_samples_pipeline_job_configurations.py\\n            :start-after: [START configure_pipeline]\\n            :end-before: [END configure_pipeline]\\n            :language: python\\n            :dedent: 8\\n            :caption: Shows how to create a pipeline using this decorator.\\n    :return: Either\\n      * A decorator, if `func` is None\\n      * The decorated `func`\\n    :rtype: Union[\\n        Callable[[Callable], Callable[..., PipelineJob]],\\n        Callable[P, PipelineJob]\\n      ]\\n    '\n    get_component = kwargs.get('get_component', False)\n\n    def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n        if not isinstance(func, Callable):\n            raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n        non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n        compute = kwargs.get('compute', None)\n        default_compute_target = kwargs.get('default_compute_target', None)\n        default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n        continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n        on_init = kwargs.get('on_init', None)\n        on_finalize = kwargs.get('on_finalize', None)\n        default_datastore = kwargs.get('default_datastore', None)\n        force_rerun = kwargs.get('force_rerun', None)\n        job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n        func_entry_path = _resolve_source_file()\n        if not func_entry_path:\n            func_path = Path(inspect.getfile(func))\n            if func_path.exists():\n                func_entry_path = func_path.resolve().absolute()\n        job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n        pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n        @wraps(func)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n            _dsl_settings_stack.push()\n            try:\n                provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n                pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n                pipeline_builder._update_inputs(pipeline_parameters)\n                non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n                pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n            finally:\n                dsl_settings = _dsl_settings_stack.pop()\n            if dsl_settings.init_job_set:\n                job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n            if dsl_settings.finalize_job_set:\n                job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n            common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n            if _is_inside_dsl_pipeline_func() or get_component:\n                if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                    raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n                built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n                if job_settings:\n                    module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n            else:\n                built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n            return built_pipeline\n        wrapper._is_dsl_func = True\n        wrapper._job_settings = job_settings\n        wrapper._pipeline_builder = pipeline_builder\n        return wrapper\n    if func is not None:\n        return pipeline_decorator(func)\n    return pipeline_decorator",
            "def pipeline(func: Optional[Callable[P, T]]=None, *, name: Optional[str]=None, version: Optional[str]=None, display_name: Optional[str]=None, description: Optional[str]=None, experiment_name: Optional[str]=None, tags: Optional[Dict[str, str]]=None, **kwargs) -> Union[Callable[[Callable[P, T]], Callable[P, PipelineJob]], Callable[P, PipelineJob]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Build a pipeline which contains all component nodes defined in this function.\\n\\n    :param func: The user pipeline function to be decorated.\\n    :type func: types.FunctionType\\n    :keyword name: The name of pipeline component, defaults to function name.\\n    :paramtype name: str\\n    :keyword version: The version of pipeline component, defaults to \"1\".\\n    :paramtype version: str\\n    :keyword display_name: The display name of pipeline component, defaults to function name.\\n    :paramtype display_name: str\\n    :keyword description: The description of the built pipeline.\\n    :paramtype description: str\\n    :keyword experiment_name: Name of the experiment the job will be created under,                 if None is provided, experiment will be set to current directory.\\n    :paramtype experiment_name: str\\n    :keyword tags: The tags of pipeline component.\\n    :paramtype tags: dict[str, str]\\n    :keyword kwargs: A dictionary of additional configuration parameters.\\n    :paramtype kwargs: dict\\n\\n    .. admonition:: Example:\\n\\n        .. literalinclude:: ../../../../samples/ml_samples_pipeline_job_configurations.py\\n            :start-after: [START configure_pipeline]\\n            :end-before: [END configure_pipeline]\\n            :language: python\\n            :dedent: 8\\n            :caption: Shows how to create a pipeline using this decorator.\\n    :return: Either\\n      * A decorator, if `func` is None\\n      * The decorated `func`\\n    :rtype: Union[\\n        Callable[[Callable], Callable[..., PipelineJob]],\\n        Callable[P, PipelineJob]\\n      ]\\n    '\n    get_component = kwargs.get('get_component', False)\n\n    def pipeline_decorator(func: Callable[P, T]) -> Callable[P, PipelineJob]:\n        if not isinstance(func, Callable):\n            raise UserErrorException(f'Dsl pipeline decorator accept only function type, got {type(func)}.')\n        non_pipeline_inputs = kwargs.get('non_pipeline_inputs', []) or kwargs.get('non_pipeline_parameters', [])\n        compute = kwargs.get('compute', None)\n        default_compute_target = kwargs.get('default_compute_target', None)\n        default_compute_target = kwargs.get('default_compute', None) or default_compute_target\n        continue_on_step_failure = kwargs.get('continue_on_step_failure', None)\n        on_init = kwargs.get('on_init', None)\n        on_finalize = kwargs.get('on_finalize', None)\n        default_datastore = kwargs.get('default_datastore', None)\n        force_rerun = kwargs.get('force_rerun', None)\n        job_settings = {'default_datastore': default_datastore, 'continue_on_step_failure': continue_on_step_failure, 'force_rerun': force_rerun, 'default_compute': default_compute_target, 'on_init': on_init, 'on_finalize': on_finalize}\n        func_entry_path = _resolve_source_file()\n        if not func_entry_path:\n            func_path = Path(inspect.getfile(func))\n            if func_path.exists():\n                func_entry_path = func_path.resolve().absolute()\n        job_settings = {k: v for (k, v) in job_settings.items() if v is not None}\n        pipeline_builder = PipelineComponentBuilder(func=func, name=name, version=version, display_name=display_name, description=description, default_datastore=default_datastore, tags=tags, source_path=str(func_entry_path), non_pipeline_inputs=non_pipeline_inputs)\n\n        @wraps(func)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> PipelineJob:\n            _dsl_settings_stack.push()\n            try:\n                provided_positional_kwargs = _validate_args(func, args, kwargs, non_pipeline_inputs)\n                pipeline_parameters = {k: v for (k, v) in provided_positional_kwargs.items() if k not in non_pipeline_inputs}\n                pipeline_builder._update_inputs(pipeline_parameters)\n                non_pipeline_params_dict = {k: v for (k, v) in provided_positional_kwargs.items() if k in non_pipeline_inputs}\n                pipeline_component = pipeline_builder.build(user_provided_kwargs=provided_positional_kwargs, non_pipeline_inputs_dict=non_pipeline_params_dict, non_pipeline_inputs=non_pipeline_inputs)\n            finally:\n                dsl_settings = _dsl_settings_stack.pop()\n            if dsl_settings.init_job_set:\n                job_settings['on_init'] = dsl_settings.init_job_name(pipeline_component.jobs)\n            if dsl_settings.finalize_job_set:\n                job_settings['on_finalize'] = dsl_settings.finalize_job_name(pipeline_component.jobs)\n            common_init_args = {'experiment_name': experiment_name, 'component': pipeline_component, 'inputs': pipeline_parameters, 'tags': tags}\n            if _is_inside_dsl_pipeline_func() or get_component:\n                if job_settings.get('on_init') is not None or job_settings.get('on_finalize') is not None:\n                    raise UserErrorException('On_init/on_finalize is not supported for pipeline component.')\n                built_pipeline = Pipeline(_from_component_func=True, **common_init_args)\n                if job_settings:\n                    module_logger.warning('Job settings %s on pipeline function %r are ignored when using inside PipelineJob.', job_settings, func.__name__)\n            else:\n                built_pipeline = PipelineJob(jobs=pipeline_component.jobs, compute=compute, settings=PipelineJobSettings(**job_settings), **common_init_args)\n            return built_pipeline\n        wrapper._is_dsl_func = True\n        wrapper._job_settings = job_settings\n        wrapper._pipeline_builder = pipeline_builder\n        return wrapper\n    if func is not None:\n        return pipeline_decorator(func)\n    return pipeline_decorator"
        ]
    }
]