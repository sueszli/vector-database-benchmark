"""This module contains functions to perform segmentation on a list of topics."""
import itertools
import logging
from gensim.topic_coherence.text_analysis import CorpusAccumulator, WordOccurrenceAccumulator, ParallelWordOccurrenceAccumulator, WordVectorsAccumulator
logger = logging.getLogger(__name__)

def p_boolean_document(corpus, segmented_topics):
    if False:
        while True:
            i = 10
    "Perform the boolean document probability estimation. Boolean document estimates the probability of a single word\n    as the number of documents in which the word occurs divided by the total number of documents.\n\n    Parameters\n    ----------\n    corpus : iterable of list of (int, int)\n        The corpus of documents.\n    segmented_topics: list of (int, int).\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\n\n    Returns\n    -------\n    :class:`~gensim.topic_coherence.text_analysis.CorpusAccumulator`\n        Word occurrence accumulator instance that can be used to lookup token frequencies and co-occurrence frequencies.\n\n    Examples\n    ---------\n    .. sourcecode:: pycon\n\n        >>> from gensim.topic_coherence import probability_estimation\n        >>> from gensim.corpora.hashdictionary import HashDictionary\n        >>>\n        >>>\n        >>> texts = [\n        ...     ['human', 'interface', 'computer'],\n        ...     ['eps', 'user', 'interface', 'system'],\n        ...     ['system', 'human', 'system', 'eps'],\n        ...     ['user', 'response', 'time'],\n        ...     ['trees'],\n        ...     ['graph', 'trees']\n        ... ]\n        >>> dictionary = HashDictionary(texts)\n        >>> w2id = dictionary.token2id\n        >>>\n        >>> # create segmented_topics\n        >>> segmented_topics = [\n        ...     [\n        ...         (w2id['system'], w2id['graph']),\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['computer'], w2id['system'])\n        ...     ],\n        ...     [\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['user'], w2id['graph']),\n        ...         (w2id['user'], w2id['computer'])]\n        ... ]\n        >>> # create corpus\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\n        >>>\n        >>> result = probability_estimation.p_boolean_document(corpus, segmented_topics)\n        >>> result.index_to_dict()\n        {10608: set([0]), 12736: set([1, 3]), 18451: set([5]), 5798: set([1, 2])}\n\n    "
    top_ids = unique_ids_from_segments(segmented_topics)
    return CorpusAccumulator(top_ids).accumulate(corpus)

def p_boolean_sliding_window(texts, segmented_topics, dictionary, window_size, processes=1):
    if False:
        i = 10
        return i + 15
    "Perform the boolean sliding window probability estimation.\n\n    Parameters\n    ----------\n    texts : iterable of iterable of str\n        Input text\n    segmented_topics: list of (int, int)\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\n    dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n        Gensim dictionary mapping of the tokens and ids.\n    window_size : int\n        Size of the sliding window, 110 found out to be the ideal size for large corpora.\n    processes : int, optional\n        Number of process that will be used for\n        :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\n\n    Notes\n    -----\n    Boolean sliding window determines word counts using a sliding window. The window\n    moves over  the documents one word token per step. Each step defines a new virtual\n    document  by copying the window content. Boolean document is applied to these virtual\n    documents to compute word probabilities.\n\n    Returns\n    -------\n    :class:`~gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator`\n        if `processes` = 1 OR\n    :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\n        otherwise. This is word occurrence accumulator instance that can be used to lookup\n        token frequencies and co-occurrence frequencies.\n\n    Examples\n    ---------\n    .. sourcecode:: pycon\n\n        >>> from gensim.topic_coherence import probability_estimation\n        >>> from gensim.corpora.hashdictionary import HashDictionary\n        >>>\n        >>>\n        >>> texts = [\n        ...     ['human', 'interface', 'computer'],\n        ...     ['eps', 'user', 'interface', 'system'],\n        ...     ['system', 'human', 'system', 'eps'],\n        ...     ['user', 'response', 'time'],\n        ...     ['trees'],\n        ...     ['graph', 'trees']\n        ... ]\n        >>> dictionary = HashDictionary(texts)\n        >>> w2id = dictionary.token2id\n\n        >>>\n        >>> # create segmented_topics\n        >>> segmented_topics = [\n        ...     [\n        ...         (w2id['system'], w2id['graph']),\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['computer'], w2id['system'])\n        ...     ],\n        ...     [\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['user'], w2id['graph']),\n        ...         (w2id['user'], w2id['computer'])]\n        ... ]\n        >>> # create corpus\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\n        >>> accumulator = probability_estimation.p_boolean_sliding_window(texts, segmented_topics, dictionary, 2)\n        >>>\n        >>> (accumulator[w2id['computer']], accumulator[w2id['user']], accumulator[w2id['system']])\n        (1, 3, 4)\n\n    "
    top_ids = unique_ids_from_segments(segmented_topics)
    if processes <= 1:
        accumulator = WordOccurrenceAccumulator(top_ids, dictionary)
    else:
        accumulator = ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)
    logger.info('using %s to estimate probabilities from sliding windows', accumulator)
    return accumulator.accumulate(texts, window_size)

def p_word2vec(texts, segmented_topics, dictionary, window_size=None, processes=1, model=None):
    if False:
        i = 10
        return i + 15
    "Train word2vec model on `texts` if `model` is not None.\n\n    Parameters\n    ----------\n    texts : iterable of iterable of str\n        Input text\n    segmented_topics : iterable of iterable of str\n        Output from the segmentation of topics. Could be simply topics too.\n    dictionary : :class:`~gensim.corpora.dictionary`\n        Gensim dictionary mapping of the tokens and ids.\n    window_size : int, optional\n        Size of the sliding window.\n    processes : int, optional\n        Number of processes to use.\n    model : :class:`~gensim.models.word2vec.Word2Vec` or :class:`~gensim.models.keyedvectors.KeyedVectors`, optional\n        If None, a new Word2Vec model is trained on the given text corpus. Otherwise,\n        it should be a pre-trained Word2Vec context vectors.\n\n    Returns\n    -------\n    :class:`~gensim.topic_coherence.text_analysis.WordVectorsAccumulator`\n        Text accumulator with trained context vectors.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.topic_coherence import probability_estimation\n        >>> from gensim.corpora.hashdictionary import HashDictionary\n        >>> from gensim.models import word2vec\n        >>>\n        >>> texts = [\n        ...     ['human', 'interface', 'computer'],\n        ...     ['eps', 'user', 'interface', 'system'],\n        ...     ['system', 'human', 'system', 'eps'],\n        ...     ['user', 'response', 'time'],\n        ...     ['trees'],\n        ...     ['graph', 'trees']\n        ... ]\n        >>> dictionary = HashDictionary(texts)\n        >>> w2id = dictionary.token2id\n\n        >>>\n        >>> # create segmented_topics\n        >>> segmented_topics = [\n        ...     [\n        ...         (w2id['system'], w2id['graph']),\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['computer'], w2id['system'])\n        ...     ],\n        ...     [\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['user'], w2id['graph']),\n        ...         (w2id['user'], w2id['computer'])]\n        ... ]\n        >>> # create corpus\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\n        >>> sentences = [\n        ...     ['human', 'interface', 'computer'],\n        ...     ['survey', 'user', 'computer', 'system', 'response', 'time']\n        ... ]\n        >>> model = word2vec.Word2Vec(sentences, vector_size=100, min_count=1)\n        >>> accumulator = probability_estimation.p_word2vec(texts, segmented_topics, dictionary, 2, 1, model)\n\n    "
    top_ids = unique_ids_from_segments(segmented_topics)
    accumulator = WordVectorsAccumulator(top_ids, dictionary, model, window=window_size, workers=processes)
    return accumulator.accumulate(texts, window_size)

def unique_ids_from_segments(segmented_topics):
    if False:
        i = 10
        return i + 15
    'Return the set of all unique ids in a list of segmented topics.\n\n    Parameters\n    ----------\n    segmented_topics: list of (int, int).\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\n\n    Returns\n    -------\n    set\n        Set of unique ids across all topic segments.\n\n    Example\n    -------\n    .. sourcecode:: pycon\n\n        >>> from gensim.topic_coherence import probability_estimation\n        >>>\n        >>> segmentation = [[(1, 2)]]\n        >>> probability_estimation.unique_ids_from_segments(segmentation)\n        set([1, 2])\n\n    '
    unique_ids = set()
    for s_i in segmented_topics:
        for word_id in itertools.chain.from_iterable(s_i):
            if hasattr(word_id, '__iter__'):
                unique_ids.update(word_id)
            else:
                unique_ids.add(word_id)
    return unique_ids