[
    {
        "func_name": "_combine_bins",
        "original": "def _combine_bins(edge_index, x):\n    \"\"\"group columns into bins using sum\n\n    This is mainly a helper function for combining probabilities into cells.\n    It similar to `np.add.reduceat(x, edge_index, axis=-1)` except for the\n    treatment of the last index and last cell.\n\n    Parameters\n    ----------\n    edge_index : array_like\n         This defines the (zero-based) indices for the columns that are be\n         combined. Each index in `edge_index` except the last is the starting\n         index for a bin. The largest index in a bin is the next edge_index-1.\n    x : 1d or 2d array\n        array for which columns are combined. If x is 1-dimensional that it\n        will be treated as a 2-d row vector.\n\n    Returns\n    -------\n    x_new : ndarray\n    k_li : ndarray\n        Count of columns combined in bin.\n\n\n    Examples\n    --------\n    >>> dia.combine_bins([0,1,5], np.arange(4))\n    (array([0, 6]), array([1, 4]))\n\n    this aggregates to two bins with the sum of 1 and 4 elements\n    >>> np.arange(4)[0].sum()\n    0\n    >>> np.arange(4)[1:5].sum()\n    6\n\n    If the rightmost index is smaller than len(x)+1, then the remaining\n    columns will not be included.\n\n    >>> dia.combine_bins([0,1,3], np.arange(4))\n    (array([0, 3]), array([1, 2]))\n    \"\"\"\n    x = np.asarray(x)\n    if x.ndim == 1:\n        is_1d = True\n        x = x[None, :]\n    else:\n        is_1d = False\n    xli = []\n    kli = []\n    for bin_idx in range(len(edge_index) - 1):\n        (i, j) = edge_index[bin_idx:bin_idx + 2]\n        xli.append(x[:, i:j].sum(1))\n        kli.append(j - i)\n    x_new = np.column_stack(xli)\n    if is_1d:\n        x_new = x_new.squeeze()\n    return (x_new, np.asarray(kli))",
        "mutated": [
            "def _combine_bins(edge_index, x):\n    if False:\n        i = 10\n    'group columns into bins using sum\\n\\n    This is mainly a helper function for combining probabilities into cells.\\n    It similar to `np.add.reduceat(x, edge_index, axis=-1)` except for the\\n    treatment of the last index and last cell.\\n\\n    Parameters\\n    ----------\\n    edge_index : array_like\\n         This defines the (zero-based) indices for the columns that are be\\n         combined. Each index in `edge_index` except the last is the starting\\n         index for a bin. The largest index in a bin is the next edge_index-1.\\n    x : 1d or 2d array\\n        array for which columns are combined. If x is 1-dimensional that it\\n        will be treated as a 2-d row vector.\\n\\n    Returns\\n    -------\\n    x_new : ndarray\\n    k_li : ndarray\\n        Count of columns combined in bin.\\n\\n\\n    Examples\\n    --------\\n    >>> dia.combine_bins([0,1,5], np.arange(4))\\n    (array([0, 6]), array([1, 4]))\\n\\n    this aggregates to two bins with the sum of 1 and 4 elements\\n    >>> np.arange(4)[0].sum()\\n    0\\n    >>> np.arange(4)[1:5].sum()\\n    6\\n\\n    If the rightmost index is smaller than len(x)+1, then the remaining\\n    columns will not be included.\\n\\n    >>> dia.combine_bins([0,1,3], np.arange(4))\\n    (array([0, 3]), array([1, 2]))\\n    '\n    x = np.asarray(x)\n    if x.ndim == 1:\n        is_1d = True\n        x = x[None, :]\n    else:\n        is_1d = False\n    xli = []\n    kli = []\n    for bin_idx in range(len(edge_index) - 1):\n        (i, j) = edge_index[bin_idx:bin_idx + 2]\n        xli.append(x[:, i:j].sum(1))\n        kli.append(j - i)\n    x_new = np.column_stack(xli)\n    if is_1d:\n        x_new = x_new.squeeze()\n    return (x_new, np.asarray(kli))",
            "def _combine_bins(edge_index, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'group columns into bins using sum\\n\\n    This is mainly a helper function for combining probabilities into cells.\\n    It similar to `np.add.reduceat(x, edge_index, axis=-1)` except for the\\n    treatment of the last index and last cell.\\n\\n    Parameters\\n    ----------\\n    edge_index : array_like\\n         This defines the (zero-based) indices for the columns that are be\\n         combined. Each index in `edge_index` except the last is the starting\\n         index for a bin. The largest index in a bin is the next edge_index-1.\\n    x : 1d or 2d array\\n        array for which columns are combined. If x is 1-dimensional that it\\n        will be treated as a 2-d row vector.\\n\\n    Returns\\n    -------\\n    x_new : ndarray\\n    k_li : ndarray\\n        Count of columns combined in bin.\\n\\n\\n    Examples\\n    --------\\n    >>> dia.combine_bins([0,1,5], np.arange(4))\\n    (array([0, 6]), array([1, 4]))\\n\\n    this aggregates to two bins with the sum of 1 and 4 elements\\n    >>> np.arange(4)[0].sum()\\n    0\\n    >>> np.arange(4)[1:5].sum()\\n    6\\n\\n    If the rightmost index is smaller than len(x)+1, then the remaining\\n    columns will not be included.\\n\\n    >>> dia.combine_bins([0,1,3], np.arange(4))\\n    (array([0, 3]), array([1, 2]))\\n    '\n    x = np.asarray(x)\n    if x.ndim == 1:\n        is_1d = True\n        x = x[None, :]\n    else:\n        is_1d = False\n    xli = []\n    kli = []\n    for bin_idx in range(len(edge_index) - 1):\n        (i, j) = edge_index[bin_idx:bin_idx + 2]\n        xli.append(x[:, i:j].sum(1))\n        kli.append(j - i)\n    x_new = np.column_stack(xli)\n    if is_1d:\n        x_new = x_new.squeeze()\n    return (x_new, np.asarray(kli))",
            "def _combine_bins(edge_index, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'group columns into bins using sum\\n\\n    This is mainly a helper function for combining probabilities into cells.\\n    It similar to `np.add.reduceat(x, edge_index, axis=-1)` except for the\\n    treatment of the last index and last cell.\\n\\n    Parameters\\n    ----------\\n    edge_index : array_like\\n         This defines the (zero-based) indices for the columns that are be\\n         combined. Each index in `edge_index` except the last is the starting\\n         index for a bin. The largest index in a bin is the next edge_index-1.\\n    x : 1d or 2d array\\n        array for which columns are combined. If x is 1-dimensional that it\\n        will be treated as a 2-d row vector.\\n\\n    Returns\\n    -------\\n    x_new : ndarray\\n    k_li : ndarray\\n        Count of columns combined in bin.\\n\\n\\n    Examples\\n    --------\\n    >>> dia.combine_bins([0,1,5], np.arange(4))\\n    (array([0, 6]), array([1, 4]))\\n\\n    this aggregates to two bins with the sum of 1 and 4 elements\\n    >>> np.arange(4)[0].sum()\\n    0\\n    >>> np.arange(4)[1:5].sum()\\n    6\\n\\n    If the rightmost index is smaller than len(x)+1, then the remaining\\n    columns will not be included.\\n\\n    >>> dia.combine_bins([0,1,3], np.arange(4))\\n    (array([0, 3]), array([1, 2]))\\n    '\n    x = np.asarray(x)\n    if x.ndim == 1:\n        is_1d = True\n        x = x[None, :]\n    else:\n        is_1d = False\n    xli = []\n    kli = []\n    for bin_idx in range(len(edge_index) - 1):\n        (i, j) = edge_index[bin_idx:bin_idx + 2]\n        xli.append(x[:, i:j].sum(1))\n        kli.append(j - i)\n    x_new = np.column_stack(xli)\n    if is_1d:\n        x_new = x_new.squeeze()\n    return (x_new, np.asarray(kli))",
            "def _combine_bins(edge_index, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'group columns into bins using sum\\n\\n    This is mainly a helper function for combining probabilities into cells.\\n    It similar to `np.add.reduceat(x, edge_index, axis=-1)` except for the\\n    treatment of the last index and last cell.\\n\\n    Parameters\\n    ----------\\n    edge_index : array_like\\n         This defines the (zero-based) indices for the columns that are be\\n         combined. Each index in `edge_index` except the last is the starting\\n         index for a bin. The largest index in a bin is the next edge_index-1.\\n    x : 1d or 2d array\\n        array for which columns are combined. If x is 1-dimensional that it\\n        will be treated as a 2-d row vector.\\n\\n    Returns\\n    -------\\n    x_new : ndarray\\n    k_li : ndarray\\n        Count of columns combined in bin.\\n\\n\\n    Examples\\n    --------\\n    >>> dia.combine_bins([0,1,5], np.arange(4))\\n    (array([0, 6]), array([1, 4]))\\n\\n    this aggregates to two bins with the sum of 1 and 4 elements\\n    >>> np.arange(4)[0].sum()\\n    0\\n    >>> np.arange(4)[1:5].sum()\\n    6\\n\\n    If the rightmost index is smaller than len(x)+1, then the remaining\\n    columns will not be included.\\n\\n    >>> dia.combine_bins([0,1,3], np.arange(4))\\n    (array([0, 3]), array([1, 2]))\\n    '\n    x = np.asarray(x)\n    if x.ndim == 1:\n        is_1d = True\n        x = x[None, :]\n    else:\n        is_1d = False\n    xli = []\n    kli = []\n    for bin_idx in range(len(edge_index) - 1):\n        (i, j) = edge_index[bin_idx:bin_idx + 2]\n        xli.append(x[:, i:j].sum(1))\n        kli.append(j - i)\n    x_new = np.column_stack(xli)\n    if is_1d:\n        x_new = x_new.squeeze()\n    return (x_new, np.asarray(kli))",
            "def _combine_bins(edge_index, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'group columns into bins using sum\\n\\n    This is mainly a helper function for combining probabilities into cells.\\n    It similar to `np.add.reduceat(x, edge_index, axis=-1)` except for the\\n    treatment of the last index and last cell.\\n\\n    Parameters\\n    ----------\\n    edge_index : array_like\\n         This defines the (zero-based) indices for the columns that are be\\n         combined. Each index in `edge_index` except the last is the starting\\n         index for a bin. The largest index in a bin is the next edge_index-1.\\n    x : 1d or 2d array\\n        array for which columns are combined. If x is 1-dimensional that it\\n        will be treated as a 2-d row vector.\\n\\n    Returns\\n    -------\\n    x_new : ndarray\\n    k_li : ndarray\\n        Count of columns combined in bin.\\n\\n\\n    Examples\\n    --------\\n    >>> dia.combine_bins([0,1,5], np.arange(4))\\n    (array([0, 6]), array([1, 4]))\\n\\n    this aggregates to two bins with the sum of 1 and 4 elements\\n    >>> np.arange(4)[0].sum()\\n    0\\n    >>> np.arange(4)[1:5].sum()\\n    6\\n\\n    If the rightmost index is smaller than len(x)+1, then the remaining\\n    columns will not be included.\\n\\n    >>> dia.combine_bins([0,1,3], np.arange(4))\\n    (array([0, 3]), array([1, 2]))\\n    '\n    x = np.asarray(x)\n    if x.ndim == 1:\n        is_1d = True\n        x = x[None, :]\n    else:\n        is_1d = False\n    xli = []\n    kli = []\n    for bin_idx in range(len(edge_index) - 1):\n        (i, j) = edge_index[bin_idx:bin_idx + 2]\n        xli.append(x[:, i:j].sum(1))\n        kli.append(j - i)\n    x_new = np.column_stack(xli)\n    if is_1d:\n        x_new = x_new.squeeze()\n    return (x_new, np.asarray(kli))"
        ]
    },
    {
        "func_name": "plot_probs",
        "original": "def plot_probs(freq, probs_predicted, label='predicted', upp_xlim=None, fig=None):\n    \"\"\"diagnostic plots for comparing two lists of discrete probabilities\n\n    Parameters\n    ----------\n    freq, probs_predicted : nd_arrays\n        two arrays of probabilities, this can be any probabilities for\n        the same events, default is designed for comparing predicted\n        and observed probabilities\n    label : str or tuple\n        If string, then it will be used as the label for probs_predicted and\n        \"freq\" is used for the other probabilities.\n        If label is a tuple of strings, then the first is they are used as\n        label for both probabilities\n\n    upp_xlim : None or int\n        If it is not None, then the xlim of the first two plots are set to\n        (0, upp_xlim), otherwise the matplotlib default is used\n    fig : None or matplotlib figure instance\n        If fig is provided, then the axes will be added to it in a (3,1)\n        subplots, otherwise a matplotlib figure instance is created\n\n    Returns\n    -------\n    Figure\n        The figure contains 3 subplot with probabilities, cumulative\n        probabilities and a PP-plot\n    \"\"\"\n    if isinstance(label, list):\n        (label0, label1) = label\n    else:\n        (label0, label1) = ('freq', label)\n    if fig is None:\n        import matplotlib.pyplot as plt\n        fig = plt.figure(figsize=(8, 12))\n    ax1 = fig.add_subplot(311)\n    ax1.plot(freq, '-o', label=label0)\n    ax1.plot(probs_predicted, '-d', label=label1)\n    if upp_xlim is not None:\n        ax1.set_xlim(0, upp_xlim)\n    ax1.legend()\n    ax1.set_title('probabilities')\n    ax2 = fig.add_subplot(312)\n    ax2.plot(np.cumsum(freq), '-o', label=label0)\n    ax2.plot(np.cumsum(probs_predicted), '-d', label=label1)\n    if upp_xlim is not None:\n        ax2.set_xlim(0, upp_xlim)\n    ax2.legend()\n    ax2.set_title('cumulative probabilities')\n    ax3 = fig.add_subplot(313)\n    ax3.plot(np.cumsum(probs_predicted), np.cumsum(freq), 'o')\n    ax3.plot(np.arange(len(freq)) / len(freq), np.arange(len(freq)) / len(freq))\n    ax3.set_title('PP-plot')\n    ax3.set_xlabel(label1)\n    ax3.set_ylabel(label0)\n    return fig",
        "mutated": [
            "def plot_probs(freq, probs_predicted, label='predicted', upp_xlim=None, fig=None):\n    if False:\n        i = 10\n    'diagnostic plots for comparing two lists of discrete probabilities\\n\\n    Parameters\\n    ----------\\n    freq, probs_predicted : nd_arrays\\n        two arrays of probabilities, this can be any probabilities for\\n        the same events, default is designed for comparing predicted\\n        and observed probabilities\\n    label : str or tuple\\n        If string, then it will be used as the label for probs_predicted and\\n        \"freq\" is used for the other probabilities.\\n        If label is a tuple of strings, then the first is they are used as\\n        label for both probabilities\\n\\n    upp_xlim : None or int\\n        If it is not None, then the xlim of the first two plots are set to\\n        (0, upp_xlim), otherwise the matplotlib default is used\\n    fig : None or matplotlib figure instance\\n        If fig is provided, then the axes will be added to it in a (3,1)\\n        subplots, otherwise a matplotlib figure instance is created\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure contains 3 subplot with probabilities, cumulative\\n        probabilities and a PP-plot\\n    '\n    if isinstance(label, list):\n        (label0, label1) = label\n    else:\n        (label0, label1) = ('freq', label)\n    if fig is None:\n        import matplotlib.pyplot as plt\n        fig = plt.figure(figsize=(8, 12))\n    ax1 = fig.add_subplot(311)\n    ax1.plot(freq, '-o', label=label0)\n    ax1.plot(probs_predicted, '-d', label=label1)\n    if upp_xlim is not None:\n        ax1.set_xlim(0, upp_xlim)\n    ax1.legend()\n    ax1.set_title('probabilities')\n    ax2 = fig.add_subplot(312)\n    ax2.plot(np.cumsum(freq), '-o', label=label0)\n    ax2.plot(np.cumsum(probs_predicted), '-d', label=label1)\n    if upp_xlim is not None:\n        ax2.set_xlim(0, upp_xlim)\n    ax2.legend()\n    ax2.set_title('cumulative probabilities')\n    ax3 = fig.add_subplot(313)\n    ax3.plot(np.cumsum(probs_predicted), np.cumsum(freq), 'o')\n    ax3.plot(np.arange(len(freq)) / len(freq), np.arange(len(freq)) / len(freq))\n    ax3.set_title('PP-plot')\n    ax3.set_xlabel(label1)\n    ax3.set_ylabel(label0)\n    return fig",
            "def plot_probs(freq, probs_predicted, label='predicted', upp_xlim=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'diagnostic plots for comparing two lists of discrete probabilities\\n\\n    Parameters\\n    ----------\\n    freq, probs_predicted : nd_arrays\\n        two arrays of probabilities, this can be any probabilities for\\n        the same events, default is designed for comparing predicted\\n        and observed probabilities\\n    label : str or tuple\\n        If string, then it will be used as the label for probs_predicted and\\n        \"freq\" is used for the other probabilities.\\n        If label is a tuple of strings, then the first is they are used as\\n        label for both probabilities\\n\\n    upp_xlim : None or int\\n        If it is not None, then the xlim of the first two plots are set to\\n        (0, upp_xlim), otherwise the matplotlib default is used\\n    fig : None or matplotlib figure instance\\n        If fig is provided, then the axes will be added to it in a (3,1)\\n        subplots, otherwise a matplotlib figure instance is created\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure contains 3 subplot with probabilities, cumulative\\n        probabilities and a PP-plot\\n    '\n    if isinstance(label, list):\n        (label0, label1) = label\n    else:\n        (label0, label1) = ('freq', label)\n    if fig is None:\n        import matplotlib.pyplot as plt\n        fig = plt.figure(figsize=(8, 12))\n    ax1 = fig.add_subplot(311)\n    ax1.plot(freq, '-o', label=label0)\n    ax1.plot(probs_predicted, '-d', label=label1)\n    if upp_xlim is not None:\n        ax1.set_xlim(0, upp_xlim)\n    ax1.legend()\n    ax1.set_title('probabilities')\n    ax2 = fig.add_subplot(312)\n    ax2.plot(np.cumsum(freq), '-o', label=label0)\n    ax2.plot(np.cumsum(probs_predicted), '-d', label=label1)\n    if upp_xlim is not None:\n        ax2.set_xlim(0, upp_xlim)\n    ax2.legend()\n    ax2.set_title('cumulative probabilities')\n    ax3 = fig.add_subplot(313)\n    ax3.plot(np.cumsum(probs_predicted), np.cumsum(freq), 'o')\n    ax3.plot(np.arange(len(freq)) / len(freq), np.arange(len(freq)) / len(freq))\n    ax3.set_title('PP-plot')\n    ax3.set_xlabel(label1)\n    ax3.set_ylabel(label0)\n    return fig",
            "def plot_probs(freq, probs_predicted, label='predicted', upp_xlim=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'diagnostic plots for comparing two lists of discrete probabilities\\n\\n    Parameters\\n    ----------\\n    freq, probs_predicted : nd_arrays\\n        two arrays of probabilities, this can be any probabilities for\\n        the same events, default is designed for comparing predicted\\n        and observed probabilities\\n    label : str or tuple\\n        If string, then it will be used as the label for probs_predicted and\\n        \"freq\" is used for the other probabilities.\\n        If label is a tuple of strings, then the first is they are used as\\n        label for both probabilities\\n\\n    upp_xlim : None or int\\n        If it is not None, then the xlim of the first two plots are set to\\n        (0, upp_xlim), otherwise the matplotlib default is used\\n    fig : None or matplotlib figure instance\\n        If fig is provided, then the axes will be added to it in a (3,1)\\n        subplots, otherwise a matplotlib figure instance is created\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure contains 3 subplot with probabilities, cumulative\\n        probabilities and a PP-plot\\n    '\n    if isinstance(label, list):\n        (label0, label1) = label\n    else:\n        (label0, label1) = ('freq', label)\n    if fig is None:\n        import matplotlib.pyplot as plt\n        fig = plt.figure(figsize=(8, 12))\n    ax1 = fig.add_subplot(311)\n    ax1.plot(freq, '-o', label=label0)\n    ax1.plot(probs_predicted, '-d', label=label1)\n    if upp_xlim is not None:\n        ax1.set_xlim(0, upp_xlim)\n    ax1.legend()\n    ax1.set_title('probabilities')\n    ax2 = fig.add_subplot(312)\n    ax2.plot(np.cumsum(freq), '-o', label=label0)\n    ax2.plot(np.cumsum(probs_predicted), '-d', label=label1)\n    if upp_xlim is not None:\n        ax2.set_xlim(0, upp_xlim)\n    ax2.legend()\n    ax2.set_title('cumulative probabilities')\n    ax3 = fig.add_subplot(313)\n    ax3.plot(np.cumsum(probs_predicted), np.cumsum(freq), 'o')\n    ax3.plot(np.arange(len(freq)) / len(freq), np.arange(len(freq)) / len(freq))\n    ax3.set_title('PP-plot')\n    ax3.set_xlabel(label1)\n    ax3.set_ylabel(label0)\n    return fig",
            "def plot_probs(freq, probs_predicted, label='predicted', upp_xlim=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'diagnostic plots for comparing two lists of discrete probabilities\\n\\n    Parameters\\n    ----------\\n    freq, probs_predicted : nd_arrays\\n        two arrays of probabilities, this can be any probabilities for\\n        the same events, default is designed for comparing predicted\\n        and observed probabilities\\n    label : str or tuple\\n        If string, then it will be used as the label for probs_predicted and\\n        \"freq\" is used for the other probabilities.\\n        If label is a tuple of strings, then the first is they are used as\\n        label for both probabilities\\n\\n    upp_xlim : None or int\\n        If it is not None, then the xlim of the first two plots are set to\\n        (0, upp_xlim), otherwise the matplotlib default is used\\n    fig : None or matplotlib figure instance\\n        If fig is provided, then the axes will be added to it in a (3,1)\\n        subplots, otherwise a matplotlib figure instance is created\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure contains 3 subplot with probabilities, cumulative\\n        probabilities and a PP-plot\\n    '\n    if isinstance(label, list):\n        (label0, label1) = label\n    else:\n        (label0, label1) = ('freq', label)\n    if fig is None:\n        import matplotlib.pyplot as plt\n        fig = plt.figure(figsize=(8, 12))\n    ax1 = fig.add_subplot(311)\n    ax1.plot(freq, '-o', label=label0)\n    ax1.plot(probs_predicted, '-d', label=label1)\n    if upp_xlim is not None:\n        ax1.set_xlim(0, upp_xlim)\n    ax1.legend()\n    ax1.set_title('probabilities')\n    ax2 = fig.add_subplot(312)\n    ax2.plot(np.cumsum(freq), '-o', label=label0)\n    ax2.plot(np.cumsum(probs_predicted), '-d', label=label1)\n    if upp_xlim is not None:\n        ax2.set_xlim(0, upp_xlim)\n    ax2.legend()\n    ax2.set_title('cumulative probabilities')\n    ax3 = fig.add_subplot(313)\n    ax3.plot(np.cumsum(probs_predicted), np.cumsum(freq), 'o')\n    ax3.plot(np.arange(len(freq)) / len(freq), np.arange(len(freq)) / len(freq))\n    ax3.set_title('PP-plot')\n    ax3.set_xlabel(label1)\n    ax3.set_ylabel(label0)\n    return fig",
            "def plot_probs(freq, probs_predicted, label='predicted', upp_xlim=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'diagnostic plots for comparing two lists of discrete probabilities\\n\\n    Parameters\\n    ----------\\n    freq, probs_predicted : nd_arrays\\n        two arrays of probabilities, this can be any probabilities for\\n        the same events, default is designed for comparing predicted\\n        and observed probabilities\\n    label : str or tuple\\n        If string, then it will be used as the label for probs_predicted and\\n        \"freq\" is used for the other probabilities.\\n        If label is a tuple of strings, then the first is they are used as\\n        label for both probabilities\\n\\n    upp_xlim : None or int\\n        If it is not None, then the xlim of the first two plots are set to\\n        (0, upp_xlim), otherwise the matplotlib default is used\\n    fig : None or matplotlib figure instance\\n        If fig is provided, then the axes will be added to it in a (3,1)\\n        subplots, otherwise a matplotlib figure instance is created\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure contains 3 subplot with probabilities, cumulative\\n        probabilities and a PP-plot\\n    '\n    if isinstance(label, list):\n        (label0, label1) = label\n    else:\n        (label0, label1) = ('freq', label)\n    if fig is None:\n        import matplotlib.pyplot as plt\n        fig = plt.figure(figsize=(8, 12))\n    ax1 = fig.add_subplot(311)\n    ax1.plot(freq, '-o', label=label0)\n    ax1.plot(probs_predicted, '-d', label=label1)\n    if upp_xlim is not None:\n        ax1.set_xlim(0, upp_xlim)\n    ax1.legend()\n    ax1.set_title('probabilities')\n    ax2 = fig.add_subplot(312)\n    ax2.plot(np.cumsum(freq), '-o', label=label0)\n    ax2.plot(np.cumsum(probs_predicted), '-d', label=label1)\n    if upp_xlim is not None:\n        ax2.set_xlim(0, upp_xlim)\n    ax2.legend()\n    ax2.set_title('cumulative probabilities')\n    ax3 = fig.add_subplot(313)\n    ax3.plot(np.cumsum(probs_predicted), np.cumsum(freq), 'o')\n    ax3.plot(np.arange(len(freq)) / len(freq), np.arange(len(freq)) / len(freq))\n    ax3.set_title('PP-plot')\n    ax3.set_xlabel(label1)\n    ax3.set_ylabel(label0)\n    return fig"
        ]
    },
    {
        "func_name": "test_chisquare_prob",
        "original": "def test_chisquare_prob(results, probs, bin_edges=None, method=None):\n    \"\"\"\n    chisquare test for predicted probabilities using cmt-opg\n\n    Parameters\n    ----------\n    results : results instance\n        Instance of a count regression results\n    probs : ndarray\n        Array of predicted probabilities with observations\n        in rows and event counts in columns\n    bin_edges : None or array\n        intervals to combine several counts into cells\n        see combine_bins\n\n    Returns\n    -------\n    (api not stable, replace by test-results class)\n    statistic : float\n        chisquare statistic for tes\n    p-value : float\n        p-value of test\n    df : int\n        degrees of freedom for chisquare distribution\n    extras : ???\n        currently returns a tuple with some intermediate results\n        (diff, res_aux)\n\n    Notes\n    -----\n\n    Status : experimental, no verified unit tests, needs to be generalized\n    currently only OPG version with auxiliary regression is implemented\n\n    Assumes counts are np.arange(probs.shape[1]), i.e. consecutive\n    integers starting at zero.\n\n    Auxiliary regression drops the last column of binned probs to avoid\n    that probabilities sum to 1.\n\n    References\n    ----------\n    .. [1] Andrews, Donald W. K. 1988a. \u201cChi-Square Diagnostic Tests for\n           Econometric Models: Theory.\u201d Econometrica 56 (6): 1419\u201353.\n           https://doi.org/10.2307/1913105.\n\n    .. [2] Andrews, Donald W. K. 1988b. \u201cChi-Square Diagnostic Tests for\n           Econometric Models.\u201d Journal of Econometrics 37 (1): 135\u201356.\n           https://doi.org/10.1016/0304-4076(88)90079-6.\n\n    .. [3] Manj\u00f3n, M., and O. Mart\u00ednez. 2014. \u201cThe Chi-Squared Goodness-of-Fit\n           Test for Count-Data Models.\u201d Stata Journal 14 (4): 798\u2013816.\n    \"\"\"\n    res = results\n    score_obs = results.model.score_obs(results.params)\n    d_ind = (res.model.endog[:, None] == np.arange(probs.shape[1])).astype(int)\n    if bin_edges is not None:\n        (d_ind_bins, k_bins) = _combine_bins(bin_edges, d_ind)\n        (probs_bins, k_bins) = _combine_bins(bin_edges, probs)\n        k_bins = probs_bins.shape[-1]\n    else:\n        (d_ind_bins, k_bins) = (d_ind, d_ind.shape[1])\n        probs_bins = probs\n    diff1 = d_ind_bins - probs_bins\n    x_aux = np.column_stack((score_obs, diff1[:, :-1]))\n    nobs = x_aux.shape[0]\n    res_aux = OLS(np.ones(nobs), x_aux).fit()\n    chi2_stat = nobs * (1 - res_aux.ssr / res_aux.uncentered_tss)\n    df = res_aux.model.rank - score_obs.shape[1]\n    if df < k_bins - 1:\n        import warnings\n        warnings.warn('auxiliary model is rank deficient')\n    statistic = chi2_stat\n    pvalue = stats.chi2.sf(chi2_stat, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, diff1=diff1, res_aux=res_aux, distribution='chi2')\n    return res",
        "mutated": [
            "def test_chisquare_prob(results, probs, bin_edges=None, method=None):\n    if False:\n        i = 10\n    '\\n    chisquare test for predicted probabilities using cmt-opg\\n\\n    Parameters\\n    ----------\\n    results : results instance\\n        Instance of a count regression results\\n    probs : ndarray\\n        Array of predicted probabilities with observations\\n        in rows and event counts in columns\\n    bin_edges : None or array\\n        intervals to combine several counts into cells\\n        see combine_bins\\n\\n    Returns\\n    -------\\n    (api not stable, replace by test-results class)\\n    statistic : float\\n        chisquare statistic for tes\\n    p-value : float\\n        p-value of test\\n    df : int\\n        degrees of freedom for chisquare distribution\\n    extras : ???\\n        currently returns a tuple with some intermediate results\\n        (diff, res_aux)\\n\\n    Notes\\n    -----\\n\\n    Status : experimental, no verified unit tests, needs to be generalized\\n    currently only OPG version with auxiliary regression is implemented\\n\\n    Assumes counts are np.arange(probs.shape[1]), i.e. consecutive\\n    integers starting at zero.\\n\\n    Auxiliary regression drops the last column of binned probs to avoid\\n    that probabilities sum to 1.\\n\\n    References\\n    ----------\\n    .. [1] Andrews, Donald W. K. 1988a. \u201cChi-Square Diagnostic Tests for\\n           Econometric Models: Theory.\u201d Econometrica 56 (6): 1419\u201353.\\n           https://doi.org/10.2307/1913105.\\n\\n    .. [2] Andrews, Donald W. K. 1988b. \u201cChi-Square Diagnostic Tests for\\n           Econometric Models.\u201d Journal of Econometrics 37 (1): 135\u201356.\\n           https://doi.org/10.1016/0304-4076(88)90079-6.\\n\\n    .. [3] Manj\u00f3n, M., and O. Mart\u00ednez. 2014. \u201cThe Chi-Squared Goodness-of-Fit\\n           Test for Count-Data Models.\u201d Stata Journal 14 (4): 798\u2013816.\\n    '\n    res = results\n    score_obs = results.model.score_obs(results.params)\n    d_ind = (res.model.endog[:, None] == np.arange(probs.shape[1])).astype(int)\n    if bin_edges is not None:\n        (d_ind_bins, k_bins) = _combine_bins(bin_edges, d_ind)\n        (probs_bins, k_bins) = _combine_bins(bin_edges, probs)\n        k_bins = probs_bins.shape[-1]\n    else:\n        (d_ind_bins, k_bins) = (d_ind, d_ind.shape[1])\n        probs_bins = probs\n    diff1 = d_ind_bins - probs_bins\n    x_aux = np.column_stack((score_obs, diff1[:, :-1]))\n    nobs = x_aux.shape[0]\n    res_aux = OLS(np.ones(nobs), x_aux).fit()\n    chi2_stat = nobs * (1 - res_aux.ssr / res_aux.uncentered_tss)\n    df = res_aux.model.rank - score_obs.shape[1]\n    if df < k_bins - 1:\n        import warnings\n        warnings.warn('auxiliary model is rank deficient')\n    statistic = chi2_stat\n    pvalue = stats.chi2.sf(chi2_stat, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, diff1=diff1, res_aux=res_aux, distribution='chi2')\n    return res",
            "def test_chisquare_prob(results, probs, bin_edges=None, method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    chisquare test for predicted probabilities using cmt-opg\\n\\n    Parameters\\n    ----------\\n    results : results instance\\n        Instance of a count regression results\\n    probs : ndarray\\n        Array of predicted probabilities with observations\\n        in rows and event counts in columns\\n    bin_edges : None or array\\n        intervals to combine several counts into cells\\n        see combine_bins\\n\\n    Returns\\n    -------\\n    (api not stable, replace by test-results class)\\n    statistic : float\\n        chisquare statistic for tes\\n    p-value : float\\n        p-value of test\\n    df : int\\n        degrees of freedom for chisquare distribution\\n    extras : ???\\n        currently returns a tuple with some intermediate results\\n        (diff, res_aux)\\n\\n    Notes\\n    -----\\n\\n    Status : experimental, no verified unit tests, needs to be generalized\\n    currently only OPG version with auxiliary regression is implemented\\n\\n    Assumes counts are np.arange(probs.shape[1]), i.e. consecutive\\n    integers starting at zero.\\n\\n    Auxiliary regression drops the last column of binned probs to avoid\\n    that probabilities sum to 1.\\n\\n    References\\n    ----------\\n    .. [1] Andrews, Donald W. K. 1988a. \u201cChi-Square Diagnostic Tests for\\n           Econometric Models: Theory.\u201d Econometrica 56 (6): 1419\u201353.\\n           https://doi.org/10.2307/1913105.\\n\\n    .. [2] Andrews, Donald W. K. 1988b. \u201cChi-Square Diagnostic Tests for\\n           Econometric Models.\u201d Journal of Econometrics 37 (1): 135\u201356.\\n           https://doi.org/10.1016/0304-4076(88)90079-6.\\n\\n    .. [3] Manj\u00f3n, M., and O. Mart\u00ednez. 2014. \u201cThe Chi-Squared Goodness-of-Fit\\n           Test for Count-Data Models.\u201d Stata Journal 14 (4): 798\u2013816.\\n    '\n    res = results\n    score_obs = results.model.score_obs(results.params)\n    d_ind = (res.model.endog[:, None] == np.arange(probs.shape[1])).astype(int)\n    if bin_edges is not None:\n        (d_ind_bins, k_bins) = _combine_bins(bin_edges, d_ind)\n        (probs_bins, k_bins) = _combine_bins(bin_edges, probs)\n        k_bins = probs_bins.shape[-1]\n    else:\n        (d_ind_bins, k_bins) = (d_ind, d_ind.shape[1])\n        probs_bins = probs\n    diff1 = d_ind_bins - probs_bins\n    x_aux = np.column_stack((score_obs, diff1[:, :-1]))\n    nobs = x_aux.shape[0]\n    res_aux = OLS(np.ones(nobs), x_aux).fit()\n    chi2_stat = nobs * (1 - res_aux.ssr / res_aux.uncentered_tss)\n    df = res_aux.model.rank - score_obs.shape[1]\n    if df < k_bins - 1:\n        import warnings\n        warnings.warn('auxiliary model is rank deficient')\n    statistic = chi2_stat\n    pvalue = stats.chi2.sf(chi2_stat, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, diff1=diff1, res_aux=res_aux, distribution='chi2')\n    return res",
            "def test_chisquare_prob(results, probs, bin_edges=None, method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    chisquare test for predicted probabilities using cmt-opg\\n\\n    Parameters\\n    ----------\\n    results : results instance\\n        Instance of a count regression results\\n    probs : ndarray\\n        Array of predicted probabilities with observations\\n        in rows and event counts in columns\\n    bin_edges : None or array\\n        intervals to combine several counts into cells\\n        see combine_bins\\n\\n    Returns\\n    -------\\n    (api not stable, replace by test-results class)\\n    statistic : float\\n        chisquare statistic for tes\\n    p-value : float\\n        p-value of test\\n    df : int\\n        degrees of freedom for chisquare distribution\\n    extras : ???\\n        currently returns a tuple with some intermediate results\\n        (diff, res_aux)\\n\\n    Notes\\n    -----\\n\\n    Status : experimental, no verified unit tests, needs to be generalized\\n    currently only OPG version with auxiliary regression is implemented\\n\\n    Assumes counts are np.arange(probs.shape[1]), i.e. consecutive\\n    integers starting at zero.\\n\\n    Auxiliary regression drops the last column of binned probs to avoid\\n    that probabilities sum to 1.\\n\\n    References\\n    ----------\\n    .. [1] Andrews, Donald W. K. 1988a. \u201cChi-Square Diagnostic Tests for\\n           Econometric Models: Theory.\u201d Econometrica 56 (6): 1419\u201353.\\n           https://doi.org/10.2307/1913105.\\n\\n    .. [2] Andrews, Donald W. K. 1988b. \u201cChi-Square Diagnostic Tests for\\n           Econometric Models.\u201d Journal of Econometrics 37 (1): 135\u201356.\\n           https://doi.org/10.1016/0304-4076(88)90079-6.\\n\\n    .. [3] Manj\u00f3n, M., and O. Mart\u00ednez. 2014. \u201cThe Chi-Squared Goodness-of-Fit\\n           Test for Count-Data Models.\u201d Stata Journal 14 (4): 798\u2013816.\\n    '\n    res = results\n    score_obs = results.model.score_obs(results.params)\n    d_ind = (res.model.endog[:, None] == np.arange(probs.shape[1])).astype(int)\n    if bin_edges is not None:\n        (d_ind_bins, k_bins) = _combine_bins(bin_edges, d_ind)\n        (probs_bins, k_bins) = _combine_bins(bin_edges, probs)\n        k_bins = probs_bins.shape[-1]\n    else:\n        (d_ind_bins, k_bins) = (d_ind, d_ind.shape[1])\n        probs_bins = probs\n    diff1 = d_ind_bins - probs_bins\n    x_aux = np.column_stack((score_obs, diff1[:, :-1]))\n    nobs = x_aux.shape[0]\n    res_aux = OLS(np.ones(nobs), x_aux).fit()\n    chi2_stat = nobs * (1 - res_aux.ssr / res_aux.uncentered_tss)\n    df = res_aux.model.rank - score_obs.shape[1]\n    if df < k_bins - 1:\n        import warnings\n        warnings.warn('auxiliary model is rank deficient')\n    statistic = chi2_stat\n    pvalue = stats.chi2.sf(chi2_stat, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, diff1=diff1, res_aux=res_aux, distribution='chi2')\n    return res",
            "def test_chisquare_prob(results, probs, bin_edges=None, method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    chisquare test for predicted probabilities using cmt-opg\\n\\n    Parameters\\n    ----------\\n    results : results instance\\n        Instance of a count regression results\\n    probs : ndarray\\n        Array of predicted probabilities with observations\\n        in rows and event counts in columns\\n    bin_edges : None or array\\n        intervals to combine several counts into cells\\n        see combine_bins\\n\\n    Returns\\n    -------\\n    (api not stable, replace by test-results class)\\n    statistic : float\\n        chisquare statistic for tes\\n    p-value : float\\n        p-value of test\\n    df : int\\n        degrees of freedom for chisquare distribution\\n    extras : ???\\n        currently returns a tuple with some intermediate results\\n        (diff, res_aux)\\n\\n    Notes\\n    -----\\n\\n    Status : experimental, no verified unit tests, needs to be generalized\\n    currently only OPG version with auxiliary regression is implemented\\n\\n    Assumes counts are np.arange(probs.shape[1]), i.e. consecutive\\n    integers starting at zero.\\n\\n    Auxiliary regression drops the last column of binned probs to avoid\\n    that probabilities sum to 1.\\n\\n    References\\n    ----------\\n    .. [1] Andrews, Donald W. K. 1988a. \u201cChi-Square Diagnostic Tests for\\n           Econometric Models: Theory.\u201d Econometrica 56 (6): 1419\u201353.\\n           https://doi.org/10.2307/1913105.\\n\\n    .. [2] Andrews, Donald W. K. 1988b. \u201cChi-Square Diagnostic Tests for\\n           Econometric Models.\u201d Journal of Econometrics 37 (1): 135\u201356.\\n           https://doi.org/10.1016/0304-4076(88)90079-6.\\n\\n    .. [3] Manj\u00f3n, M., and O. Mart\u00ednez. 2014. \u201cThe Chi-Squared Goodness-of-Fit\\n           Test for Count-Data Models.\u201d Stata Journal 14 (4): 798\u2013816.\\n    '\n    res = results\n    score_obs = results.model.score_obs(results.params)\n    d_ind = (res.model.endog[:, None] == np.arange(probs.shape[1])).astype(int)\n    if bin_edges is not None:\n        (d_ind_bins, k_bins) = _combine_bins(bin_edges, d_ind)\n        (probs_bins, k_bins) = _combine_bins(bin_edges, probs)\n        k_bins = probs_bins.shape[-1]\n    else:\n        (d_ind_bins, k_bins) = (d_ind, d_ind.shape[1])\n        probs_bins = probs\n    diff1 = d_ind_bins - probs_bins\n    x_aux = np.column_stack((score_obs, diff1[:, :-1]))\n    nobs = x_aux.shape[0]\n    res_aux = OLS(np.ones(nobs), x_aux).fit()\n    chi2_stat = nobs * (1 - res_aux.ssr / res_aux.uncentered_tss)\n    df = res_aux.model.rank - score_obs.shape[1]\n    if df < k_bins - 1:\n        import warnings\n        warnings.warn('auxiliary model is rank deficient')\n    statistic = chi2_stat\n    pvalue = stats.chi2.sf(chi2_stat, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, diff1=diff1, res_aux=res_aux, distribution='chi2')\n    return res",
            "def test_chisquare_prob(results, probs, bin_edges=None, method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    chisquare test for predicted probabilities using cmt-opg\\n\\n    Parameters\\n    ----------\\n    results : results instance\\n        Instance of a count regression results\\n    probs : ndarray\\n        Array of predicted probabilities with observations\\n        in rows and event counts in columns\\n    bin_edges : None or array\\n        intervals to combine several counts into cells\\n        see combine_bins\\n\\n    Returns\\n    -------\\n    (api not stable, replace by test-results class)\\n    statistic : float\\n        chisquare statistic for tes\\n    p-value : float\\n        p-value of test\\n    df : int\\n        degrees of freedom for chisquare distribution\\n    extras : ???\\n        currently returns a tuple with some intermediate results\\n        (diff, res_aux)\\n\\n    Notes\\n    -----\\n\\n    Status : experimental, no verified unit tests, needs to be generalized\\n    currently only OPG version with auxiliary regression is implemented\\n\\n    Assumes counts are np.arange(probs.shape[1]), i.e. consecutive\\n    integers starting at zero.\\n\\n    Auxiliary regression drops the last column of binned probs to avoid\\n    that probabilities sum to 1.\\n\\n    References\\n    ----------\\n    .. [1] Andrews, Donald W. K. 1988a. \u201cChi-Square Diagnostic Tests for\\n           Econometric Models: Theory.\u201d Econometrica 56 (6): 1419\u201353.\\n           https://doi.org/10.2307/1913105.\\n\\n    .. [2] Andrews, Donald W. K. 1988b. \u201cChi-Square Diagnostic Tests for\\n           Econometric Models.\u201d Journal of Econometrics 37 (1): 135\u201356.\\n           https://doi.org/10.1016/0304-4076(88)90079-6.\\n\\n    .. [3] Manj\u00f3n, M., and O. Mart\u00ednez. 2014. \u201cThe Chi-Squared Goodness-of-Fit\\n           Test for Count-Data Models.\u201d Stata Journal 14 (4): 798\u2013816.\\n    '\n    res = results\n    score_obs = results.model.score_obs(results.params)\n    d_ind = (res.model.endog[:, None] == np.arange(probs.shape[1])).astype(int)\n    if bin_edges is not None:\n        (d_ind_bins, k_bins) = _combine_bins(bin_edges, d_ind)\n        (probs_bins, k_bins) = _combine_bins(bin_edges, probs)\n        k_bins = probs_bins.shape[-1]\n    else:\n        (d_ind_bins, k_bins) = (d_ind, d_ind.shape[1])\n        probs_bins = probs\n    diff1 = d_ind_bins - probs_bins\n    x_aux = np.column_stack((score_obs, diff1[:, :-1]))\n    nobs = x_aux.shape[0]\n    res_aux = OLS(np.ones(nobs), x_aux).fit()\n    chi2_stat = nobs * (1 - res_aux.ssr / res_aux.uncentered_tss)\n    df = res_aux.model.rank - score_obs.shape[1]\n    if df < k_bins - 1:\n        import warnings\n        warnings.warn('auxiliary model is rank deficient')\n    statistic = chi2_stat\n    pvalue = stats.chi2.sf(chi2_stat, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, diff1=diff1, res_aux=res_aux, distribution='chi2')\n    return res"
        ]
    },
    {
        "func_name": "summary_frame",
        "original": "def summary_frame(self):\n    frame = pd.DataFrame({'statistic': self.statistic, 'pvalue': self.pvalue, 'method': self.method, 'alternative': self.alternative})\n    return frame",
        "mutated": [
            "def summary_frame(self):\n    if False:\n        i = 10\n    frame = pd.DataFrame({'statistic': self.statistic, 'pvalue': self.pvalue, 'method': self.method, 'alternative': self.alternative})\n    return frame",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    frame = pd.DataFrame({'statistic': self.statistic, 'pvalue': self.pvalue, 'method': self.method, 'alternative': self.alternative})\n    return frame",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    frame = pd.DataFrame({'statistic': self.statistic, 'pvalue': self.pvalue, 'method': self.method, 'alternative': self.alternative})\n    return frame",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    frame = pd.DataFrame({'statistic': self.statistic, 'pvalue': self.pvalue, 'method': self.method, 'alternative': self.alternative})\n    return frame",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    frame = pd.DataFrame({'statistic': self.statistic, 'pvalue': self.pvalue, 'method': self.method, 'alternative': self.alternative})\n    return frame"
        ]
    },
    {
        "func_name": "test_poisson_dispersion",
        "original": "def test_poisson_dispersion(results, method='all', _old=False):\n    \"\"\"Score/LM type tests for Poisson variance assumptions\n\n    Null Hypothesis is\n\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\n    H1: var(y) ~= E(y)\n\n    The tests are based on the constrained model, i.e. the Poisson model.\n    The tests differ in their assumed alternatives, and in their maintained\n    assumptions.\n\n    Parameters\n    ----------\n    results : Poisson results instance\n        This can be a results instance for either a discrete Poisson or a GLM\n        with family Poisson.\n    method : str\n        Not used yet. Currently results for all methods are returned.\n    _old : bool\n        Temporary keyword for backwards compatibility, will be removed\n        in future version of statsmodels.\n\n    Returns\n    -------\n    res : instance\n        The instance of DispersionResults has the hypothesis test results,\n        statistic, pvalue, method, alternative, as main attributes and a\n        summary_frame method that returns the results as pandas DataFrame.\n\n    \"\"\"\n    if method not in ['all']:\n        raise ValueError(f'unknown method \"{method}\"')\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    var_resid_endog = resid2 - endog\n    var_resid_fitted = resid2 - fitted\n    std1 = np.sqrt(2 * (fitted ** 2).sum())\n    var_resid_endog_sum = var_resid_endog.sum()\n    dean_a = var_resid_fitted.sum() / std1\n    dean_b = var_resid_endog_sum / std1\n    dean_c = (var_resid_endog / fitted).sum() / np.sqrt(2 * nobs)\n    pval_dean_a = 2 * stats.norm.sf(np.abs(dean_a))\n    pval_dean_b = 2 * stats.norm.sf(np.abs(dean_b))\n    pval_dean_c = 2 * stats.norm.sf(np.abs(dean_c))\n    results_all = [[dean_a, pval_dean_a], [dean_b, pval_dean_b], [dean_c, pval_dean_c]]\n    description = [['Dean A', 'mu (1 + a mu)'], ['Dean B', 'mu (1 + a mu)'], ['Dean C', 'mu (1 + a)']]\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_nb2, pval_ols_nb2])\n    description.append(['CT nb2', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_nb1, pval_ols_nb1])\n    description.append(['CT nb1', 'mu (1 + a)'])\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_hc1_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_hc1_nb2, pval_ols_hc1_nb2])\n    description.append(['CT nb2 HC3', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, np.ones(len(endog_v))).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_hc1_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_hc1_nb1, pval_ols_hc1_nb1])\n    description.append(['CT nb1 HC3', 'mu (1 + a)'])\n    results_all = np.array(results_all)\n    if _old:\n        return (results_all, description)\n    else:\n        res = DispersionResults(statistic=results_all[:, 0], pvalue=results_all[:, 1], method=[i[0] for i in description], alternative=[i[1] for i in description], name='Poisson Dispersion Test')\n        return res",
        "mutated": [
            "def test_poisson_dispersion(results, method='all', _old=False):\n    if False:\n        i = 10\n    'Score/LM type tests for Poisson variance assumptions\\n\\n    Null Hypothesis is\\n\\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\\n    H1: var(y) ~= E(y)\\n\\n    The tests are based on the constrained model, i.e. the Poisson model.\\n    The tests differ in their assumed alternatives, and in their maintained\\n    assumptions.\\n\\n    Parameters\\n    ----------\\n    results : Poisson results instance\\n        This can be a results instance for either a discrete Poisson or a GLM\\n        with family Poisson.\\n    method : str\\n        Not used yet. Currently results for all methods are returned.\\n    _old : bool\\n        Temporary keyword for backwards compatibility, will be removed\\n        in future version of statsmodels.\\n\\n    Returns\\n    -------\\n    res : instance\\n        The instance of DispersionResults has the hypothesis test results,\\n        statistic, pvalue, method, alternative, as main attributes and a\\n        summary_frame method that returns the results as pandas DataFrame.\\n\\n    '\n    if method not in ['all']:\n        raise ValueError(f'unknown method \"{method}\"')\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    var_resid_endog = resid2 - endog\n    var_resid_fitted = resid2 - fitted\n    std1 = np.sqrt(2 * (fitted ** 2).sum())\n    var_resid_endog_sum = var_resid_endog.sum()\n    dean_a = var_resid_fitted.sum() / std1\n    dean_b = var_resid_endog_sum / std1\n    dean_c = (var_resid_endog / fitted).sum() / np.sqrt(2 * nobs)\n    pval_dean_a = 2 * stats.norm.sf(np.abs(dean_a))\n    pval_dean_b = 2 * stats.norm.sf(np.abs(dean_b))\n    pval_dean_c = 2 * stats.norm.sf(np.abs(dean_c))\n    results_all = [[dean_a, pval_dean_a], [dean_b, pval_dean_b], [dean_c, pval_dean_c]]\n    description = [['Dean A', 'mu (1 + a mu)'], ['Dean B', 'mu (1 + a mu)'], ['Dean C', 'mu (1 + a)']]\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_nb2, pval_ols_nb2])\n    description.append(['CT nb2', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_nb1, pval_ols_nb1])\n    description.append(['CT nb1', 'mu (1 + a)'])\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_hc1_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_hc1_nb2, pval_ols_hc1_nb2])\n    description.append(['CT nb2 HC3', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, np.ones(len(endog_v))).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_hc1_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_hc1_nb1, pval_ols_hc1_nb1])\n    description.append(['CT nb1 HC3', 'mu (1 + a)'])\n    results_all = np.array(results_all)\n    if _old:\n        return (results_all, description)\n    else:\n        res = DispersionResults(statistic=results_all[:, 0], pvalue=results_all[:, 1], method=[i[0] for i in description], alternative=[i[1] for i in description], name='Poisson Dispersion Test')\n        return res",
            "def test_poisson_dispersion(results, method='all', _old=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Score/LM type tests for Poisson variance assumptions\\n\\n    Null Hypothesis is\\n\\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\\n    H1: var(y) ~= E(y)\\n\\n    The tests are based on the constrained model, i.e. the Poisson model.\\n    The tests differ in their assumed alternatives, and in their maintained\\n    assumptions.\\n\\n    Parameters\\n    ----------\\n    results : Poisson results instance\\n        This can be a results instance for either a discrete Poisson or a GLM\\n        with family Poisson.\\n    method : str\\n        Not used yet. Currently results for all methods are returned.\\n    _old : bool\\n        Temporary keyword for backwards compatibility, will be removed\\n        in future version of statsmodels.\\n\\n    Returns\\n    -------\\n    res : instance\\n        The instance of DispersionResults has the hypothesis test results,\\n        statistic, pvalue, method, alternative, as main attributes and a\\n        summary_frame method that returns the results as pandas DataFrame.\\n\\n    '\n    if method not in ['all']:\n        raise ValueError(f'unknown method \"{method}\"')\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    var_resid_endog = resid2 - endog\n    var_resid_fitted = resid2 - fitted\n    std1 = np.sqrt(2 * (fitted ** 2).sum())\n    var_resid_endog_sum = var_resid_endog.sum()\n    dean_a = var_resid_fitted.sum() / std1\n    dean_b = var_resid_endog_sum / std1\n    dean_c = (var_resid_endog / fitted).sum() / np.sqrt(2 * nobs)\n    pval_dean_a = 2 * stats.norm.sf(np.abs(dean_a))\n    pval_dean_b = 2 * stats.norm.sf(np.abs(dean_b))\n    pval_dean_c = 2 * stats.norm.sf(np.abs(dean_c))\n    results_all = [[dean_a, pval_dean_a], [dean_b, pval_dean_b], [dean_c, pval_dean_c]]\n    description = [['Dean A', 'mu (1 + a mu)'], ['Dean B', 'mu (1 + a mu)'], ['Dean C', 'mu (1 + a)']]\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_nb2, pval_ols_nb2])\n    description.append(['CT nb2', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_nb1, pval_ols_nb1])\n    description.append(['CT nb1', 'mu (1 + a)'])\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_hc1_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_hc1_nb2, pval_ols_hc1_nb2])\n    description.append(['CT nb2 HC3', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, np.ones(len(endog_v))).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_hc1_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_hc1_nb1, pval_ols_hc1_nb1])\n    description.append(['CT nb1 HC3', 'mu (1 + a)'])\n    results_all = np.array(results_all)\n    if _old:\n        return (results_all, description)\n    else:\n        res = DispersionResults(statistic=results_all[:, 0], pvalue=results_all[:, 1], method=[i[0] for i in description], alternative=[i[1] for i in description], name='Poisson Dispersion Test')\n        return res",
            "def test_poisson_dispersion(results, method='all', _old=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Score/LM type tests for Poisson variance assumptions\\n\\n    Null Hypothesis is\\n\\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\\n    H1: var(y) ~= E(y)\\n\\n    The tests are based on the constrained model, i.e. the Poisson model.\\n    The tests differ in their assumed alternatives, and in their maintained\\n    assumptions.\\n\\n    Parameters\\n    ----------\\n    results : Poisson results instance\\n        This can be a results instance for either a discrete Poisson or a GLM\\n        with family Poisson.\\n    method : str\\n        Not used yet. Currently results for all methods are returned.\\n    _old : bool\\n        Temporary keyword for backwards compatibility, will be removed\\n        in future version of statsmodels.\\n\\n    Returns\\n    -------\\n    res : instance\\n        The instance of DispersionResults has the hypothesis test results,\\n        statistic, pvalue, method, alternative, as main attributes and a\\n        summary_frame method that returns the results as pandas DataFrame.\\n\\n    '\n    if method not in ['all']:\n        raise ValueError(f'unknown method \"{method}\"')\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    var_resid_endog = resid2 - endog\n    var_resid_fitted = resid2 - fitted\n    std1 = np.sqrt(2 * (fitted ** 2).sum())\n    var_resid_endog_sum = var_resid_endog.sum()\n    dean_a = var_resid_fitted.sum() / std1\n    dean_b = var_resid_endog_sum / std1\n    dean_c = (var_resid_endog / fitted).sum() / np.sqrt(2 * nobs)\n    pval_dean_a = 2 * stats.norm.sf(np.abs(dean_a))\n    pval_dean_b = 2 * stats.norm.sf(np.abs(dean_b))\n    pval_dean_c = 2 * stats.norm.sf(np.abs(dean_c))\n    results_all = [[dean_a, pval_dean_a], [dean_b, pval_dean_b], [dean_c, pval_dean_c]]\n    description = [['Dean A', 'mu (1 + a mu)'], ['Dean B', 'mu (1 + a mu)'], ['Dean C', 'mu (1 + a)']]\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_nb2, pval_ols_nb2])\n    description.append(['CT nb2', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_nb1, pval_ols_nb1])\n    description.append(['CT nb1', 'mu (1 + a)'])\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_hc1_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_hc1_nb2, pval_ols_hc1_nb2])\n    description.append(['CT nb2 HC3', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, np.ones(len(endog_v))).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_hc1_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_hc1_nb1, pval_ols_hc1_nb1])\n    description.append(['CT nb1 HC3', 'mu (1 + a)'])\n    results_all = np.array(results_all)\n    if _old:\n        return (results_all, description)\n    else:\n        res = DispersionResults(statistic=results_all[:, 0], pvalue=results_all[:, 1], method=[i[0] for i in description], alternative=[i[1] for i in description], name='Poisson Dispersion Test')\n        return res",
            "def test_poisson_dispersion(results, method='all', _old=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Score/LM type tests for Poisson variance assumptions\\n\\n    Null Hypothesis is\\n\\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\\n    H1: var(y) ~= E(y)\\n\\n    The tests are based on the constrained model, i.e. the Poisson model.\\n    The tests differ in their assumed alternatives, and in their maintained\\n    assumptions.\\n\\n    Parameters\\n    ----------\\n    results : Poisson results instance\\n        This can be a results instance for either a discrete Poisson or a GLM\\n        with family Poisson.\\n    method : str\\n        Not used yet. Currently results for all methods are returned.\\n    _old : bool\\n        Temporary keyword for backwards compatibility, will be removed\\n        in future version of statsmodels.\\n\\n    Returns\\n    -------\\n    res : instance\\n        The instance of DispersionResults has the hypothesis test results,\\n        statistic, pvalue, method, alternative, as main attributes and a\\n        summary_frame method that returns the results as pandas DataFrame.\\n\\n    '\n    if method not in ['all']:\n        raise ValueError(f'unknown method \"{method}\"')\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    var_resid_endog = resid2 - endog\n    var_resid_fitted = resid2 - fitted\n    std1 = np.sqrt(2 * (fitted ** 2).sum())\n    var_resid_endog_sum = var_resid_endog.sum()\n    dean_a = var_resid_fitted.sum() / std1\n    dean_b = var_resid_endog_sum / std1\n    dean_c = (var_resid_endog / fitted).sum() / np.sqrt(2 * nobs)\n    pval_dean_a = 2 * stats.norm.sf(np.abs(dean_a))\n    pval_dean_b = 2 * stats.norm.sf(np.abs(dean_b))\n    pval_dean_c = 2 * stats.norm.sf(np.abs(dean_c))\n    results_all = [[dean_a, pval_dean_a], [dean_b, pval_dean_b], [dean_c, pval_dean_c]]\n    description = [['Dean A', 'mu (1 + a mu)'], ['Dean B', 'mu (1 + a mu)'], ['Dean C', 'mu (1 + a)']]\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_nb2, pval_ols_nb2])\n    description.append(['CT nb2', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_nb1, pval_ols_nb1])\n    description.append(['CT nb1', 'mu (1 + a)'])\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_hc1_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_hc1_nb2, pval_ols_hc1_nb2])\n    description.append(['CT nb2 HC3', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, np.ones(len(endog_v))).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_hc1_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_hc1_nb1, pval_ols_hc1_nb1])\n    description.append(['CT nb1 HC3', 'mu (1 + a)'])\n    results_all = np.array(results_all)\n    if _old:\n        return (results_all, description)\n    else:\n        res = DispersionResults(statistic=results_all[:, 0], pvalue=results_all[:, 1], method=[i[0] for i in description], alternative=[i[1] for i in description], name='Poisson Dispersion Test')\n        return res",
            "def test_poisson_dispersion(results, method='all', _old=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Score/LM type tests for Poisson variance assumptions\\n\\n    Null Hypothesis is\\n\\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\\n    H1: var(y) ~= E(y)\\n\\n    The tests are based on the constrained model, i.e. the Poisson model.\\n    The tests differ in their assumed alternatives, and in their maintained\\n    assumptions.\\n\\n    Parameters\\n    ----------\\n    results : Poisson results instance\\n        This can be a results instance for either a discrete Poisson or a GLM\\n        with family Poisson.\\n    method : str\\n        Not used yet. Currently results for all methods are returned.\\n    _old : bool\\n        Temporary keyword for backwards compatibility, will be removed\\n        in future version of statsmodels.\\n\\n    Returns\\n    -------\\n    res : instance\\n        The instance of DispersionResults has the hypothesis test results,\\n        statistic, pvalue, method, alternative, as main attributes and a\\n        summary_frame method that returns the results as pandas DataFrame.\\n\\n    '\n    if method not in ['all']:\n        raise ValueError(f'unknown method \"{method}\"')\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    var_resid_endog = resid2 - endog\n    var_resid_fitted = resid2 - fitted\n    std1 = np.sqrt(2 * (fitted ** 2).sum())\n    var_resid_endog_sum = var_resid_endog.sum()\n    dean_a = var_resid_fitted.sum() / std1\n    dean_b = var_resid_endog_sum / std1\n    dean_c = (var_resid_endog / fitted).sum() / np.sqrt(2 * nobs)\n    pval_dean_a = 2 * stats.norm.sf(np.abs(dean_a))\n    pval_dean_b = 2 * stats.norm.sf(np.abs(dean_b))\n    pval_dean_c = 2 * stats.norm.sf(np.abs(dean_c))\n    results_all = [[dean_a, pval_dean_a], [dean_b, pval_dean_b], [dean_c, pval_dean_c]]\n    description = [['Dean A', 'mu (1 + a mu)'], ['Dean B', 'mu (1 + a mu)'], ['Dean C', 'mu (1 + a)']]\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_nb2, pval_ols_nb2])\n    description.append(['CT nb2', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, fitted).fit(use_t=False)\n    stat_ols_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_nb1, pval_ols_nb1])\n    description.append(['CT nb1', 'mu (1 + a)'])\n    endog_v = var_resid_endog / fitted\n    res_ols_nb2 = OLS(endog_v, fitted).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb2 = res_ols_nb2.tvalues[0]\n    pval_ols_hc1_nb2 = res_ols_nb2.pvalues[0]\n    results_all.append([stat_ols_hc1_nb2, pval_ols_hc1_nb2])\n    description.append(['CT nb2 HC3', 'mu (1 + a mu)'])\n    res_ols_nb1 = OLS(endog_v, np.ones(len(endog_v))).fit(cov_type='HC3', use_t=False)\n    stat_ols_hc1_nb1 = res_ols_nb1.tvalues[0]\n    pval_ols_hc1_nb1 = res_ols_nb1.pvalues[0]\n    results_all.append([stat_ols_hc1_nb1, pval_ols_hc1_nb1])\n    description.append(['CT nb1 HC3', 'mu (1 + a)'])\n    results_all = np.array(results_all)\n    if _old:\n        return (results_all, description)\n    else:\n        res = DispersionResults(statistic=results_all[:, 0], pvalue=results_all[:, 1], method=[i[0] for i in description], alternative=[i[1] for i in description], name='Poisson Dispersion Test')\n        return res"
        ]
    },
    {
        "func_name": "_test_poisson_dispersion_generic",
        "original": "def _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    \"\"\"A variable addition test for the variance function\n\n    This uses an artificial regression to calculate a variant of an LM or\n    generalized score test for the specification of the variance assumption\n    in a Poisson model. The performed test is a Wald test on the coefficients\n    of the `exog_new_test`.\n\n    Warning: insufficiently tested, especially for options\n    \"\"\"\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    if use_endog:\n        var_resid = resid2 - endog\n    else:\n        var_resid = resid2 - fitted\n    endog_v = var_resid / fitted\n    k_constraints = exog_new_test.shape[1]\n    ex_list = [exog_new_test]\n    if include_score:\n        score_obs = results.model.score_obs(results.params)\n        ex_list.append(score_obs)\n    if exog_new_control is not None:\n        ex_list.append(score_obs)\n    if len(ex_list) > 1:\n        ex = np.column_stack(ex_list)\n        use_wald = True\n    else:\n        ex = ex_list[0]\n        use_wald = False\n    res_ols = OLS(endog_v, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    if use_wald:\n        k_vars = ex.shape[1]\n        constraints = np.eye(k_constraints, k_vars)\n        ht = res_ols.wald_test(constraints)\n        stat_ols = ht.statistic\n        pval_ols = ht.pvalue\n    else:\n        nobs = endog_v.shape[0]\n        rsquared_noncentered = 1 - res_ols.ssr / res_ols.uncentered_tss\n        stat_ols = nobs * rsquared_noncentered\n        pval_ols = stats.chi2.sf(stat_ols, k_constraints)\n    return (stat_ols, pval_ols)",
        "mutated": [
            "def _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    if False:\n        i = 10\n    'A variable addition test for the variance function\\n\\n    This uses an artificial regression to calculate a variant of an LM or\\n    generalized score test for the specification of the variance assumption\\n    in a Poisson model. The performed test is a Wald test on the coefficients\\n    of the `exog_new_test`.\\n\\n    Warning: insufficiently tested, especially for options\\n    '\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    if use_endog:\n        var_resid = resid2 - endog\n    else:\n        var_resid = resid2 - fitted\n    endog_v = var_resid / fitted\n    k_constraints = exog_new_test.shape[1]\n    ex_list = [exog_new_test]\n    if include_score:\n        score_obs = results.model.score_obs(results.params)\n        ex_list.append(score_obs)\n    if exog_new_control is not None:\n        ex_list.append(score_obs)\n    if len(ex_list) > 1:\n        ex = np.column_stack(ex_list)\n        use_wald = True\n    else:\n        ex = ex_list[0]\n        use_wald = False\n    res_ols = OLS(endog_v, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    if use_wald:\n        k_vars = ex.shape[1]\n        constraints = np.eye(k_constraints, k_vars)\n        ht = res_ols.wald_test(constraints)\n        stat_ols = ht.statistic\n        pval_ols = ht.pvalue\n    else:\n        nobs = endog_v.shape[0]\n        rsquared_noncentered = 1 - res_ols.ssr / res_ols.uncentered_tss\n        stat_ols = nobs * rsquared_noncentered\n        pval_ols = stats.chi2.sf(stat_ols, k_constraints)\n    return (stat_ols, pval_ols)",
            "def _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'A variable addition test for the variance function\\n\\n    This uses an artificial regression to calculate a variant of an LM or\\n    generalized score test for the specification of the variance assumption\\n    in a Poisson model. The performed test is a Wald test on the coefficients\\n    of the `exog_new_test`.\\n\\n    Warning: insufficiently tested, especially for options\\n    '\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    if use_endog:\n        var_resid = resid2 - endog\n    else:\n        var_resid = resid2 - fitted\n    endog_v = var_resid / fitted\n    k_constraints = exog_new_test.shape[1]\n    ex_list = [exog_new_test]\n    if include_score:\n        score_obs = results.model.score_obs(results.params)\n        ex_list.append(score_obs)\n    if exog_new_control is not None:\n        ex_list.append(score_obs)\n    if len(ex_list) > 1:\n        ex = np.column_stack(ex_list)\n        use_wald = True\n    else:\n        ex = ex_list[0]\n        use_wald = False\n    res_ols = OLS(endog_v, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    if use_wald:\n        k_vars = ex.shape[1]\n        constraints = np.eye(k_constraints, k_vars)\n        ht = res_ols.wald_test(constraints)\n        stat_ols = ht.statistic\n        pval_ols = ht.pvalue\n    else:\n        nobs = endog_v.shape[0]\n        rsquared_noncentered = 1 - res_ols.ssr / res_ols.uncentered_tss\n        stat_ols = nobs * rsquared_noncentered\n        pval_ols = stats.chi2.sf(stat_ols, k_constraints)\n    return (stat_ols, pval_ols)",
            "def _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'A variable addition test for the variance function\\n\\n    This uses an artificial regression to calculate a variant of an LM or\\n    generalized score test for the specification of the variance assumption\\n    in a Poisson model. The performed test is a Wald test on the coefficients\\n    of the `exog_new_test`.\\n\\n    Warning: insufficiently tested, especially for options\\n    '\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    if use_endog:\n        var_resid = resid2 - endog\n    else:\n        var_resid = resid2 - fitted\n    endog_v = var_resid / fitted\n    k_constraints = exog_new_test.shape[1]\n    ex_list = [exog_new_test]\n    if include_score:\n        score_obs = results.model.score_obs(results.params)\n        ex_list.append(score_obs)\n    if exog_new_control is not None:\n        ex_list.append(score_obs)\n    if len(ex_list) > 1:\n        ex = np.column_stack(ex_list)\n        use_wald = True\n    else:\n        ex = ex_list[0]\n        use_wald = False\n    res_ols = OLS(endog_v, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    if use_wald:\n        k_vars = ex.shape[1]\n        constraints = np.eye(k_constraints, k_vars)\n        ht = res_ols.wald_test(constraints)\n        stat_ols = ht.statistic\n        pval_ols = ht.pvalue\n    else:\n        nobs = endog_v.shape[0]\n        rsquared_noncentered = 1 - res_ols.ssr / res_ols.uncentered_tss\n        stat_ols = nobs * rsquared_noncentered\n        pval_ols = stats.chi2.sf(stat_ols, k_constraints)\n    return (stat_ols, pval_ols)",
            "def _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'A variable addition test for the variance function\\n\\n    This uses an artificial regression to calculate a variant of an LM or\\n    generalized score test for the specification of the variance assumption\\n    in a Poisson model. The performed test is a Wald test on the coefficients\\n    of the `exog_new_test`.\\n\\n    Warning: insufficiently tested, especially for options\\n    '\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    if use_endog:\n        var_resid = resid2 - endog\n    else:\n        var_resid = resid2 - fitted\n    endog_v = var_resid / fitted\n    k_constraints = exog_new_test.shape[1]\n    ex_list = [exog_new_test]\n    if include_score:\n        score_obs = results.model.score_obs(results.params)\n        ex_list.append(score_obs)\n    if exog_new_control is not None:\n        ex_list.append(score_obs)\n    if len(ex_list) > 1:\n        ex = np.column_stack(ex_list)\n        use_wald = True\n    else:\n        ex = ex_list[0]\n        use_wald = False\n    res_ols = OLS(endog_v, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    if use_wald:\n        k_vars = ex.shape[1]\n        constraints = np.eye(k_constraints, k_vars)\n        ht = res_ols.wald_test(constraints)\n        stat_ols = ht.statistic\n        pval_ols = ht.pvalue\n    else:\n        nobs = endog_v.shape[0]\n        rsquared_noncentered = 1 - res_ols.ssr / res_ols.uncentered_tss\n        stat_ols = nobs * rsquared_noncentered\n        pval_ols = stats.chi2.sf(stat_ols, k_constraints)\n    return (stat_ols, pval_ols)",
            "def _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'A variable addition test for the variance function\\n\\n    This uses an artificial regression to calculate a variant of an LM or\\n    generalized score test for the specification of the variance assumption\\n    in a Poisson model. The performed test is a Wald test on the coefficients\\n    of the `exog_new_test`.\\n\\n    Warning: insufficiently tested, especially for options\\n    '\n    if hasattr(results, '_results'):\n        results = results._results\n    endog = results.model.endog\n    nobs = endog.shape[0]\n    fitted = results.predict()\n    resid2 = results.resid_response ** 2\n    if use_endog:\n        var_resid = resid2 - endog\n    else:\n        var_resid = resid2 - fitted\n    endog_v = var_resid / fitted\n    k_constraints = exog_new_test.shape[1]\n    ex_list = [exog_new_test]\n    if include_score:\n        score_obs = results.model.score_obs(results.params)\n        ex_list.append(score_obs)\n    if exog_new_control is not None:\n        ex_list.append(score_obs)\n    if len(ex_list) > 1:\n        ex = np.column_stack(ex_list)\n        use_wald = True\n    else:\n        ex = ex_list[0]\n        use_wald = False\n    res_ols = OLS(endog_v, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    if use_wald:\n        k_vars = ex.shape[1]\n        constraints = np.eye(k_constraints, k_vars)\n        ht = res_ols.wald_test(constraints)\n        stat_ols = ht.statistic\n        pval_ols = ht.pvalue\n    else:\n        nobs = endog_v.shape[0]\n        rsquared_noncentered = 1 - res_ols.ssr / res_ols.uncentered_tss\n        stat_ols = nobs * rsquared_noncentered\n        pval_ols = stats.chi2.sf(stat_ols, k_constraints)\n    return (stat_ols, pval_ols)"
        ]
    },
    {
        "func_name": "test_poisson_zeroinflation_jh",
        "original": "def test_poisson_zeroinflation_jh(results_poisson, exog_infl=None):\n    \"\"\"score test for zero inflation or deflation in Poisson\n\n    This implements Jansakul and Hinde 2009 score test\n    for excess zeros against a zero modified Poisson\n    alternative. They use a linear link function for the\n    inflation model to allow for zero deflation.\n\n    Parameters\n    ----------\n    results_poisson: results instance\n        The test is only valid if the results instance is a Poisson\n        model.\n    exog_infl : ndarray\n        Explanatory variables for the zero inflated or zero modified\n        alternative. I exog_infl is None, then the inflation\n        probability is assumed to be constant.\n\n    Returns\n    -------\n    score test results based on chisquare distribution\n\n    Notes\n    -----\n    This is a score test based on the null hypothesis that\n    the true model is Poisson. It will also reject for\n    other deviations from a Poisson model if those affect\n    the zero probabilities, e.g. in the direction of\n    excess dispersion as in the Negative Binomial\n    or Generalized Poisson model.\n    Therefore, rejection in this test does not imply that\n    zero-inflated Poisson is the appropriate model.\n\n    Status: experimental, no verified unit tests,\n\n    TODO: If the zero modification probability is assumed\n    to be constant under the alternative, then we only have\n    a scalar test score and we can use one-sided tests to\n    distinguish zero inflation and deflation from the\n    two-sided deviations. (The general one-sided case is\n    difficult.)\n    In this case the test specializes to the test by Broek\n\n    References\n    ----------\n    .. [1] Jansakul, N., and J. P. Hinde. 2002. \u201cScore Tests for Zero-Inflated\n           Poisson Models.\u201d Computational Statistics & Data Analysis 40 (1):\n           75\u201396. https://doi.org/10.1016/S0167-9473(01)00104-9.\n    \"\"\"\n    if not isinstance(results_poisson.model, Poisson):\n        import warnings\n        warnings.warn('Test is only valid if model is Poisson')\n    nobs = results_poisson.model.endog.shape[0]\n    if exog_infl is None:\n        exog_infl = np.ones((nobs, 1))\n    endog = results_poisson.model.endog\n    exog = results_poisson.model.exog\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    cov_poi = results_poisson.cov_params()\n    cross_derivative = (exog_infl.T * -mu).dot(exog).T\n    cov_infl = (exog_infl.T * ((1 - prob_zero) / prob_zero)).dot(exog_infl)\n    score_obs_infl = exog_infl * (((endog == 0) - prob_zero) / prob_zero)[:, None]\n    score_infl = score_obs_infl.sum(0)\n    cov_score_infl = cov_infl - cross_derivative.T.dot(cov_poi).dot(cross_derivative)\n    cov_score_infl_inv = np.linalg.pinv(cov_score_infl)\n    statistic = score_infl.dot(cov_score_infl_inv).dot(score_infl)\n    df2 = np.linalg.matrix_rank(cov_score_infl)\n    df = exog_infl.shape[1]\n    pvalue = stats.chi2.sf(statistic, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, rank_score=df2, distribution='chi2')\n    return res",
        "mutated": [
            "def test_poisson_zeroinflation_jh(results_poisson, exog_infl=None):\n    if False:\n        i = 10\n    'score test for zero inflation or deflation in Poisson\\n\\n    This implements Jansakul and Hinde 2009 score test\\n    for excess zeros against a zero modified Poisson\\n    alternative. They use a linear link function for the\\n    inflation model to allow for zero deflation.\\n\\n    Parameters\\n    ----------\\n    results_poisson: results instance\\n        The test is only valid if the results instance is a Poisson\\n        model.\\n    exog_infl : ndarray\\n        Explanatory variables for the zero inflated or zero modified\\n        alternative. I exog_infl is None, then the inflation\\n        probability is assumed to be constant.\\n\\n    Returns\\n    -------\\n    score test results based on chisquare distribution\\n\\n    Notes\\n    -----\\n    This is a score test based on the null hypothesis that\\n    the true model is Poisson. It will also reject for\\n    other deviations from a Poisson model if those affect\\n    the zero probabilities, e.g. in the direction of\\n    excess dispersion as in the Negative Binomial\\n    or Generalized Poisson model.\\n    Therefore, rejection in this test does not imply that\\n    zero-inflated Poisson is the appropriate model.\\n\\n    Status: experimental, no verified unit tests,\\n\\n    TODO: If the zero modification probability is assumed\\n    to be constant under the alternative, then we only have\\n    a scalar test score and we can use one-sided tests to\\n    distinguish zero inflation and deflation from the\\n    two-sided deviations. (The general one-sided case is\\n    difficult.)\\n    In this case the test specializes to the test by Broek\\n\\n    References\\n    ----------\\n    .. [1] Jansakul, N., and J. P. Hinde. 2002. \u201cScore Tests for Zero-Inflated\\n           Poisson Models.\u201d Computational Statistics & Data Analysis 40 (1):\\n           75\u201396. https://doi.org/10.1016/S0167-9473(01)00104-9.\\n    '\n    if not isinstance(results_poisson.model, Poisson):\n        import warnings\n        warnings.warn('Test is only valid if model is Poisson')\n    nobs = results_poisson.model.endog.shape[0]\n    if exog_infl is None:\n        exog_infl = np.ones((nobs, 1))\n    endog = results_poisson.model.endog\n    exog = results_poisson.model.exog\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    cov_poi = results_poisson.cov_params()\n    cross_derivative = (exog_infl.T * -mu).dot(exog).T\n    cov_infl = (exog_infl.T * ((1 - prob_zero) / prob_zero)).dot(exog_infl)\n    score_obs_infl = exog_infl * (((endog == 0) - prob_zero) / prob_zero)[:, None]\n    score_infl = score_obs_infl.sum(0)\n    cov_score_infl = cov_infl - cross_derivative.T.dot(cov_poi).dot(cross_derivative)\n    cov_score_infl_inv = np.linalg.pinv(cov_score_infl)\n    statistic = score_infl.dot(cov_score_infl_inv).dot(score_infl)\n    df2 = np.linalg.matrix_rank(cov_score_infl)\n    df = exog_infl.shape[1]\n    pvalue = stats.chi2.sf(statistic, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, rank_score=df2, distribution='chi2')\n    return res",
            "def test_poisson_zeroinflation_jh(results_poisson, exog_infl=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'score test for zero inflation or deflation in Poisson\\n\\n    This implements Jansakul and Hinde 2009 score test\\n    for excess zeros against a zero modified Poisson\\n    alternative. They use a linear link function for the\\n    inflation model to allow for zero deflation.\\n\\n    Parameters\\n    ----------\\n    results_poisson: results instance\\n        The test is only valid if the results instance is a Poisson\\n        model.\\n    exog_infl : ndarray\\n        Explanatory variables for the zero inflated or zero modified\\n        alternative. I exog_infl is None, then the inflation\\n        probability is assumed to be constant.\\n\\n    Returns\\n    -------\\n    score test results based on chisquare distribution\\n\\n    Notes\\n    -----\\n    This is a score test based on the null hypothesis that\\n    the true model is Poisson. It will also reject for\\n    other deviations from a Poisson model if those affect\\n    the zero probabilities, e.g. in the direction of\\n    excess dispersion as in the Negative Binomial\\n    or Generalized Poisson model.\\n    Therefore, rejection in this test does not imply that\\n    zero-inflated Poisson is the appropriate model.\\n\\n    Status: experimental, no verified unit tests,\\n\\n    TODO: If the zero modification probability is assumed\\n    to be constant under the alternative, then we only have\\n    a scalar test score and we can use one-sided tests to\\n    distinguish zero inflation and deflation from the\\n    two-sided deviations. (The general one-sided case is\\n    difficult.)\\n    In this case the test specializes to the test by Broek\\n\\n    References\\n    ----------\\n    .. [1] Jansakul, N., and J. P. Hinde. 2002. \u201cScore Tests for Zero-Inflated\\n           Poisson Models.\u201d Computational Statistics & Data Analysis 40 (1):\\n           75\u201396. https://doi.org/10.1016/S0167-9473(01)00104-9.\\n    '\n    if not isinstance(results_poisson.model, Poisson):\n        import warnings\n        warnings.warn('Test is only valid if model is Poisson')\n    nobs = results_poisson.model.endog.shape[0]\n    if exog_infl is None:\n        exog_infl = np.ones((nobs, 1))\n    endog = results_poisson.model.endog\n    exog = results_poisson.model.exog\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    cov_poi = results_poisson.cov_params()\n    cross_derivative = (exog_infl.T * -mu).dot(exog).T\n    cov_infl = (exog_infl.T * ((1 - prob_zero) / prob_zero)).dot(exog_infl)\n    score_obs_infl = exog_infl * (((endog == 0) - prob_zero) / prob_zero)[:, None]\n    score_infl = score_obs_infl.sum(0)\n    cov_score_infl = cov_infl - cross_derivative.T.dot(cov_poi).dot(cross_derivative)\n    cov_score_infl_inv = np.linalg.pinv(cov_score_infl)\n    statistic = score_infl.dot(cov_score_infl_inv).dot(score_infl)\n    df2 = np.linalg.matrix_rank(cov_score_infl)\n    df = exog_infl.shape[1]\n    pvalue = stats.chi2.sf(statistic, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, rank_score=df2, distribution='chi2')\n    return res",
            "def test_poisson_zeroinflation_jh(results_poisson, exog_infl=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'score test for zero inflation or deflation in Poisson\\n\\n    This implements Jansakul and Hinde 2009 score test\\n    for excess zeros against a zero modified Poisson\\n    alternative. They use a linear link function for the\\n    inflation model to allow for zero deflation.\\n\\n    Parameters\\n    ----------\\n    results_poisson: results instance\\n        The test is only valid if the results instance is a Poisson\\n        model.\\n    exog_infl : ndarray\\n        Explanatory variables for the zero inflated or zero modified\\n        alternative. I exog_infl is None, then the inflation\\n        probability is assumed to be constant.\\n\\n    Returns\\n    -------\\n    score test results based on chisquare distribution\\n\\n    Notes\\n    -----\\n    This is a score test based on the null hypothesis that\\n    the true model is Poisson. It will also reject for\\n    other deviations from a Poisson model if those affect\\n    the zero probabilities, e.g. in the direction of\\n    excess dispersion as in the Negative Binomial\\n    or Generalized Poisson model.\\n    Therefore, rejection in this test does not imply that\\n    zero-inflated Poisson is the appropriate model.\\n\\n    Status: experimental, no verified unit tests,\\n\\n    TODO: If the zero modification probability is assumed\\n    to be constant under the alternative, then we only have\\n    a scalar test score and we can use one-sided tests to\\n    distinguish zero inflation and deflation from the\\n    two-sided deviations. (The general one-sided case is\\n    difficult.)\\n    In this case the test specializes to the test by Broek\\n\\n    References\\n    ----------\\n    .. [1] Jansakul, N., and J. P. Hinde. 2002. \u201cScore Tests for Zero-Inflated\\n           Poisson Models.\u201d Computational Statistics & Data Analysis 40 (1):\\n           75\u201396. https://doi.org/10.1016/S0167-9473(01)00104-9.\\n    '\n    if not isinstance(results_poisson.model, Poisson):\n        import warnings\n        warnings.warn('Test is only valid if model is Poisson')\n    nobs = results_poisson.model.endog.shape[0]\n    if exog_infl is None:\n        exog_infl = np.ones((nobs, 1))\n    endog = results_poisson.model.endog\n    exog = results_poisson.model.exog\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    cov_poi = results_poisson.cov_params()\n    cross_derivative = (exog_infl.T * -mu).dot(exog).T\n    cov_infl = (exog_infl.T * ((1 - prob_zero) / prob_zero)).dot(exog_infl)\n    score_obs_infl = exog_infl * (((endog == 0) - prob_zero) / prob_zero)[:, None]\n    score_infl = score_obs_infl.sum(0)\n    cov_score_infl = cov_infl - cross_derivative.T.dot(cov_poi).dot(cross_derivative)\n    cov_score_infl_inv = np.linalg.pinv(cov_score_infl)\n    statistic = score_infl.dot(cov_score_infl_inv).dot(score_infl)\n    df2 = np.linalg.matrix_rank(cov_score_infl)\n    df = exog_infl.shape[1]\n    pvalue = stats.chi2.sf(statistic, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, rank_score=df2, distribution='chi2')\n    return res",
            "def test_poisson_zeroinflation_jh(results_poisson, exog_infl=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'score test for zero inflation or deflation in Poisson\\n\\n    This implements Jansakul and Hinde 2009 score test\\n    for excess zeros against a zero modified Poisson\\n    alternative. They use a linear link function for the\\n    inflation model to allow for zero deflation.\\n\\n    Parameters\\n    ----------\\n    results_poisson: results instance\\n        The test is only valid if the results instance is a Poisson\\n        model.\\n    exog_infl : ndarray\\n        Explanatory variables for the zero inflated or zero modified\\n        alternative. I exog_infl is None, then the inflation\\n        probability is assumed to be constant.\\n\\n    Returns\\n    -------\\n    score test results based on chisquare distribution\\n\\n    Notes\\n    -----\\n    This is a score test based on the null hypothesis that\\n    the true model is Poisson. It will also reject for\\n    other deviations from a Poisson model if those affect\\n    the zero probabilities, e.g. in the direction of\\n    excess dispersion as in the Negative Binomial\\n    or Generalized Poisson model.\\n    Therefore, rejection in this test does not imply that\\n    zero-inflated Poisson is the appropriate model.\\n\\n    Status: experimental, no verified unit tests,\\n\\n    TODO: If the zero modification probability is assumed\\n    to be constant under the alternative, then we only have\\n    a scalar test score and we can use one-sided tests to\\n    distinguish zero inflation and deflation from the\\n    two-sided deviations. (The general one-sided case is\\n    difficult.)\\n    In this case the test specializes to the test by Broek\\n\\n    References\\n    ----------\\n    .. [1] Jansakul, N., and J. P. Hinde. 2002. \u201cScore Tests for Zero-Inflated\\n           Poisson Models.\u201d Computational Statistics & Data Analysis 40 (1):\\n           75\u201396. https://doi.org/10.1016/S0167-9473(01)00104-9.\\n    '\n    if not isinstance(results_poisson.model, Poisson):\n        import warnings\n        warnings.warn('Test is only valid if model is Poisson')\n    nobs = results_poisson.model.endog.shape[0]\n    if exog_infl is None:\n        exog_infl = np.ones((nobs, 1))\n    endog = results_poisson.model.endog\n    exog = results_poisson.model.exog\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    cov_poi = results_poisson.cov_params()\n    cross_derivative = (exog_infl.T * -mu).dot(exog).T\n    cov_infl = (exog_infl.T * ((1 - prob_zero) / prob_zero)).dot(exog_infl)\n    score_obs_infl = exog_infl * (((endog == 0) - prob_zero) / prob_zero)[:, None]\n    score_infl = score_obs_infl.sum(0)\n    cov_score_infl = cov_infl - cross_derivative.T.dot(cov_poi).dot(cross_derivative)\n    cov_score_infl_inv = np.linalg.pinv(cov_score_infl)\n    statistic = score_infl.dot(cov_score_infl_inv).dot(score_infl)\n    df2 = np.linalg.matrix_rank(cov_score_infl)\n    df = exog_infl.shape[1]\n    pvalue = stats.chi2.sf(statistic, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, rank_score=df2, distribution='chi2')\n    return res",
            "def test_poisson_zeroinflation_jh(results_poisson, exog_infl=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'score test for zero inflation or deflation in Poisson\\n\\n    This implements Jansakul and Hinde 2009 score test\\n    for excess zeros against a zero modified Poisson\\n    alternative. They use a linear link function for the\\n    inflation model to allow for zero deflation.\\n\\n    Parameters\\n    ----------\\n    results_poisson: results instance\\n        The test is only valid if the results instance is a Poisson\\n        model.\\n    exog_infl : ndarray\\n        Explanatory variables for the zero inflated or zero modified\\n        alternative. I exog_infl is None, then the inflation\\n        probability is assumed to be constant.\\n\\n    Returns\\n    -------\\n    score test results based on chisquare distribution\\n\\n    Notes\\n    -----\\n    This is a score test based on the null hypothesis that\\n    the true model is Poisson. It will also reject for\\n    other deviations from a Poisson model if those affect\\n    the zero probabilities, e.g. in the direction of\\n    excess dispersion as in the Negative Binomial\\n    or Generalized Poisson model.\\n    Therefore, rejection in this test does not imply that\\n    zero-inflated Poisson is the appropriate model.\\n\\n    Status: experimental, no verified unit tests,\\n\\n    TODO: If the zero modification probability is assumed\\n    to be constant under the alternative, then we only have\\n    a scalar test score and we can use one-sided tests to\\n    distinguish zero inflation and deflation from the\\n    two-sided deviations. (The general one-sided case is\\n    difficult.)\\n    In this case the test specializes to the test by Broek\\n\\n    References\\n    ----------\\n    .. [1] Jansakul, N., and J. P. Hinde. 2002. \u201cScore Tests for Zero-Inflated\\n           Poisson Models.\u201d Computational Statistics & Data Analysis 40 (1):\\n           75\u201396. https://doi.org/10.1016/S0167-9473(01)00104-9.\\n    '\n    if not isinstance(results_poisson.model, Poisson):\n        import warnings\n        warnings.warn('Test is only valid if model is Poisson')\n    nobs = results_poisson.model.endog.shape[0]\n    if exog_infl is None:\n        exog_infl = np.ones((nobs, 1))\n    endog = results_poisson.model.endog\n    exog = results_poisson.model.exog\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    cov_poi = results_poisson.cov_params()\n    cross_derivative = (exog_infl.T * -mu).dot(exog).T\n    cov_infl = (exog_infl.T * ((1 - prob_zero) / prob_zero)).dot(exog_infl)\n    score_obs_infl = exog_infl * (((endog == 0) - prob_zero) / prob_zero)[:, None]\n    score_infl = score_obs_infl.sum(0)\n    cov_score_infl = cov_infl - cross_derivative.T.dot(cov_poi).dot(cross_derivative)\n    cov_score_infl_inv = np.linalg.pinv(cov_score_infl)\n    statistic = score_infl.dot(cov_score_infl_inv).dot(score_infl)\n    df2 = np.linalg.matrix_rank(cov_score_infl)\n    df = exog_infl.shape[1]\n    pvalue = stats.chi2.sf(statistic, df)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue, df=df, rank_score=df2, distribution='chi2')\n    return res"
        ]
    },
    {
        "func_name": "test_poisson_zeroinflation_broek",
        "original": "def test_poisson_zeroinflation_broek(results_poisson):\n    \"\"\"score test for zero modification in Poisson, special case\n\n    This assumes that the Poisson model has a constant and that\n    the zero modification probability is constant.\n\n    This is a special case of test_poisson_zeroinflation derived by\n    van den Broek 1995.\n\n    The test reports two sided and one sided alternatives based on\n    the normal distribution of the test statistic.\n\n    References\n    ----------\n    .. [1] Broek, Jan van den. 1995. \u201cA Score Test for Zero Inflation in a\n           Poisson Distribution.\u201d Biometrics 51 (2): 738\u201343.\n           https://doi.org/10.2307/2532959.\n\n    \"\"\"\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    endog = results_poisson.model.endog\n    score = (((endog == 0) - prob_zero) / prob_zero).sum()\n    var_score = ((1 - prob_zero) / prob_zero).sum() - endog.sum()\n    statistic = score / np.sqrt(var_score)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res",
        "mutated": [
            "def test_poisson_zeroinflation_broek(results_poisson):\n    if False:\n        i = 10\n    'score test for zero modification in Poisson, special case\\n\\n    This assumes that the Poisson model has a constant and that\\n    the zero modification probability is constant.\\n\\n    This is a special case of test_poisson_zeroinflation derived by\\n    van den Broek 1995.\\n\\n    The test reports two sided and one sided alternatives based on\\n    the normal distribution of the test statistic.\\n\\n    References\\n    ----------\\n    .. [1] Broek, Jan van den. 1995. \u201cA Score Test for Zero Inflation in a\\n           Poisson Distribution.\u201d Biometrics 51 (2): 738\u201343.\\n           https://doi.org/10.2307/2532959.\\n\\n    '\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    endog = results_poisson.model.endog\n    score = (((endog == 0) - prob_zero) / prob_zero).sum()\n    var_score = ((1 - prob_zero) / prob_zero).sum() - endog.sum()\n    statistic = score / np.sqrt(var_score)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res",
            "def test_poisson_zeroinflation_broek(results_poisson):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'score test for zero modification in Poisson, special case\\n\\n    This assumes that the Poisson model has a constant and that\\n    the zero modification probability is constant.\\n\\n    This is a special case of test_poisson_zeroinflation derived by\\n    van den Broek 1995.\\n\\n    The test reports two sided and one sided alternatives based on\\n    the normal distribution of the test statistic.\\n\\n    References\\n    ----------\\n    .. [1] Broek, Jan van den. 1995. \u201cA Score Test for Zero Inflation in a\\n           Poisson Distribution.\u201d Biometrics 51 (2): 738\u201343.\\n           https://doi.org/10.2307/2532959.\\n\\n    '\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    endog = results_poisson.model.endog\n    score = (((endog == 0) - prob_zero) / prob_zero).sum()\n    var_score = ((1 - prob_zero) / prob_zero).sum() - endog.sum()\n    statistic = score / np.sqrt(var_score)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res",
            "def test_poisson_zeroinflation_broek(results_poisson):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'score test for zero modification in Poisson, special case\\n\\n    This assumes that the Poisson model has a constant and that\\n    the zero modification probability is constant.\\n\\n    This is a special case of test_poisson_zeroinflation derived by\\n    van den Broek 1995.\\n\\n    The test reports two sided and one sided alternatives based on\\n    the normal distribution of the test statistic.\\n\\n    References\\n    ----------\\n    .. [1] Broek, Jan van den. 1995. \u201cA Score Test for Zero Inflation in a\\n           Poisson Distribution.\u201d Biometrics 51 (2): 738\u201343.\\n           https://doi.org/10.2307/2532959.\\n\\n    '\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    endog = results_poisson.model.endog\n    score = (((endog == 0) - prob_zero) / prob_zero).sum()\n    var_score = ((1 - prob_zero) / prob_zero).sum() - endog.sum()\n    statistic = score / np.sqrt(var_score)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res",
            "def test_poisson_zeroinflation_broek(results_poisson):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'score test for zero modification in Poisson, special case\\n\\n    This assumes that the Poisson model has a constant and that\\n    the zero modification probability is constant.\\n\\n    This is a special case of test_poisson_zeroinflation derived by\\n    van den Broek 1995.\\n\\n    The test reports two sided and one sided alternatives based on\\n    the normal distribution of the test statistic.\\n\\n    References\\n    ----------\\n    .. [1] Broek, Jan van den. 1995. \u201cA Score Test for Zero Inflation in a\\n           Poisson Distribution.\u201d Biometrics 51 (2): 738\u201343.\\n           https://doi.org/10.2307/2532959.\\n\\n    '\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    endog = results_poisson.model.endog\n    score = (((endog == 0) - prob_zero) / prob_zero).sum()\n    var_score = ((1 - prob_zero) / prob_zero).sum() - endog.sum()\n    statistic = score / np.sqrt(var_score)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res",
            "def test_poisson_zeroinflation_broek(results_poisson):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'score test for zero modification in Poisson, special case\\n\\n    This assumes that the Poisson model has a constant and that\\n    the zero modification probability is constant.\\n\\n    This is a special case of test_poisson_zeroinflation derived by\\n    van den Broek 1995.\\n\\n    The test reports two sided and one sided alternatives based on\\n    the normal distribution of the test statistic.\\n\\n    References\\n    ----------\\n    .. [1] Broek, Jan van den. 1995. \u201cA Score Test for Zero Inflation in a\\n           Poisson Distribution.\u201d Biometrics 51 (2): 738\u201343.\\n           https://doi.org/10.2307/2532959.\\n\\n    '\n    mu = results_poisson.predict()\n    prob_zero = np.exp(-mu)\n    endog = results_poisson.model.endog\n    score = (((endog == 0) - prob_zero) / prob_zero).sum()\n    var_score = ((1 - prob_zero) / prob_zero).sum() - endog.sum()\n    statistic = score / np.sqrt(var_score)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res"
        ]
    },
    {
        "func_name": "test_poisson_zeros",
        "original": "def test_poisson_zeros(results):\n    \"\"\"Test for excess zeros in Poisson regression model.\n\n    The test is implemented following Tang and Tang [1]_ equ. (12) which is\n    based on the test derived in He et al 2019 [2]_.\n\n    References\n    ----------\n\n    .. [1] Tang, Yi, and Wan Tang. 2018. \u201cTesting Modified Zeros for Poisson\n           Regression Models:\u201d Statistical Methods in Medical Research,\n           September. https://doi.org/10.1177/0962280218796253.\n\n    .. [2] He, Hua, Hui Zhang, Peng Ye, and Wan Tang. 2019. \u201cA Test of Inflated\n           Zeros for Poisson Regression Models.\u201d Statistical Methods in\n           Medical Research 28 (4): 1157\u201369.\n           https://doi.org/10.1177/0962280217749991.\n\n    \"\"\"\n    x = results.model.exog\n    mean = results.predict()\n    prob0 = np.exp(-mean)\n    counts = (results.model.endog == 0).astype(int)\n    diff = counts.sum() - prob0.sum()\n    var1 = prob0 @ (1 - prob0)\n    pm = prob0 * mean\n    c = np.linalg.inv(x.T * mean @ x)\n    pmx = pm @ x\n    var2 = pmx @ c @ pmx\n    var = var1 - var2\n    statistic = diff / np.sqrt(var)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res",
        "mutated": [
            "def test_poisson_zeros(results):\n    if False:\n        i = 10\n    'Test for excess zeros in Poisson regression model.\\n\\n    The test is implemented following Tang and Tang [1]_ equ. (12) which is\\n    based on the test derived in He et al 2019 [2]_.\\n\\n    References\\n    ----------\\n\\n    .. [1] Tang, Yi, and Wan Tang. 2018. \u201cTesting Modified Zeros for Poisson\\n           Regression Models:\u201d Statistical Methods in Medical Research,\\n           September. https://doi.org/10.1177/0962280218796253.\\n\\n    .. [2] He, Hua, Hui Zhang, Peng Ye, and Wan Tang. 2019. \u201cA Test of Inflated\\n           Zeros for Poisson Regression Models.\u201d Statistical Methods in\\n           Medical Research 28 (4): 1157\u201369.\\n           https://doi.org/10.1177/0962280217749991.\\n\\n    '\n    x = results.model.exog\n    mean = results.predict()\n    prob0 = np.exp(-mean)\n    counts = (results.model.endog == 0).astype(int)\n    diff = counts.sum() - prob0.sum()\n    var1 = prob0 @ (1 - prob0)\n    pm = prob0 * mean\n    c = np.linalg.inv(x.T * mean @ x)\n    pmx = pm @ x\n    var2 = pmx @ c @ pmx\n    var = var1 - var2\n    statistic = diff / np.sqrt(var)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res",
            "def test_poisson_zeros(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test for excess zeros in Poisson regression model.\\n\\n    The test is implemented following Tang and Tang [1]_ equ. (12) which is\\n    based on the test derived in He et al 2019 [2]_.\\n\\n    References\\n    ----------\\n\\n    .. [1] Tang, Yi, and Wan Tang. 2018. \u201cTesting Modified Zeros for Poisson\\n           Regression Models:\u201d Statistical Methods in Medical Research,\\n           September. https://doi.org/10.1177/0962280218796253.\\n\\n    .. [2] He, Hua, Hui Zhang, Peng Ye, and Wan Tang. 2019. \u201cA Test of Inflated\\n           Zeros for Poisson Regression Models.\u201d Statistical Methods in\\n           Medical Research 28 (4): 1157\u201369.\\n           https://doi.org/10.1177/0962280217749991.\\n\\n    '\n    x = results.model.exog\n    mean = results.predict()\n    prob0 = np.exp(-mean)\n    counts = (results.model.endog == 0).astype(int)\n    diff = counts.sum() - prob0.sum()\n    var1 = prob0 @ (1 - prob0)\n    pm = prob0 * mean\n    c = np.linalg.inv(x.T * mean @ x)\n    pmx = pm @ x\n    var2 = pmx @ c @ pmx\n    var = var1 - var2\n    statistic = diff / np.sqrt(var)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res",
            "def test_poisson_zeros(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test for excess zeros in Poisson regression model.\\n\\n    The test is implemented following Tang and Tang [1]_ equ. (12) which is\\n    based on the test derived in He et al 2019 [2]_.\\n\\n    References\\n    ----------\\n\\n    .. [1] Tang, Yi, and Wan Tang. 2018. \u201cTesting Modified Zeros for Poisson\\n           Regression Models:\u201d Statistical Methods in Medical Research,\\n           September. https://doi.org/10.1177/0962280218796253.\\n\\n    .. [2] He, Hua, Hui Zhang, Peng Ye, and Wan Tang. 2019. \u201cA Test of Inflated\\n           Zeros for Poisson Regression Models.\u201d Statistical Methods in\\n           Medical Research 28 (4): 1157\u201369.\\n           https://doi.org/10.1177/0962280217749991.\\n\\n    '\n    x = results.model.exog\n    mean = results.predict()\n    prob0 = np.exp(-mean)\n    counts = (results.model.endog == 0).astype(int)\n    diff = counts.sum() - prob0.sum()\n    var1 = prob0 @ (1 - prob0)\n    pm = prob0 * mean\n    c = np.linalg.inv(x.T * mean @ x)\n    pmx = pm @ x\n    var2 = pmx @ c @ pmx\n    var = var1 - var2\n    statistic = diff / np.sqrt(var)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res",
            "def test_poisson_zeros(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test for excess zeros in Poisson regression model.\\n\\n    The test is implemented following Tang and Tang [1]_ equ. (12) which is\\n    based on the test derived in He et al 2019 [2]_.\\n\\n    References\\n    ----------\\n\\n    .. [1] Tang, Yi, and Wan Tang. 2018. \u201cTesting Modified Zeros for Poisson\\n           Regression Models:\u201d Statistical Methods in Medical Research,\\n           September. https://doi.org/10.1177/0962280218796253.\\n\\n    .. [2] He, Hua, Hui Zhang, Peng Ye, and Wan Tang. 2019. \u201cA Test of Inflated\\n           Zeros for Poisson Regression Models.\u201d Statistical Methods in\\n           Medical Research 28 (4): 1157\u201369.\\n           https://doi.org/10.1177/0962280217749991.\\n\\n    '\n    x = results.model.exog\n    mean = results.predict()\n    prob0 = np.exp(-mean)\n    counts = (results.model.endog == 0).astype(int)\n    diff = counts.sum() - prob0.sum()\n    var1 = prob0 @ (1 - prob0)\n    pm = prob0 * mean\n    c = np.linalg.inv(x.T * mean @ x)\n    pmx = pm @ x\n    var2 = pmx @ c @ pmx\n    var = var1 - var2\n    statistic = diff / np.sqrt(var)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res",
            "def test_poisson_zeros(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test for excess zeros in Poisson regression model.\\n\\n    The test is implemented following Tang and Tang [1]_ equ. (12) which is\\n    based on the test derived in He et al 2019 [2]_.\\n\\n    References\\n    ----------\\n\\n    .. [1] Tang, Yi, and Wan Tang. 2018. \u201cTesting Modified Zeros for Poisson\\n           Regression Models:\u201d Statistical Methods in Medical Research,\\n           September. https://doi.org/10.1177/0962280218796253.\\n\\n    .. [2] He, Hua, Hui Zhang, Peng Ye, and Wan Tang. 2019. \u201cA Test of Inflated\\n           Zeros for Poisson Regression Models.\u201d Statistical Methods in\\n           Medical Research 28 (4): 1157\u201369.\\n           https://doi.org/10.1177/0962280217749991.\\n\\n    '\n    x = results.model.exog\n    mean = results.predict()\n    prob0 = np.exp(-mean)\n    counts = (results.model.endog == 0).astype(int)\n    diff = counts.sum() - prob0.sum()\n    var1 = prob0 @ (1 - prob0)\n    pm = prob0 * mean\n    c = np.linalg.inv(x.T * mean @ x)\n    pmx = pm @ x\n    var2 = pmx @ c @ pmx\n    var = var1 - var2\n    statistic = diff / np.sqrt(var)\n    pvalue_two = 2 * stats.norm.sf(np.abs(statistic))\n    pvalue_upp = stats.norm.sf(statistic)\n    pvalue_low = stats.norm.cdf(statistic)\n    res = HolderTuple(statistic=statistic, pvalue=pvalue_two, pvalue_smaller=pvalue_upp, pvalue_larger=pvalue_low, chi2=statistic ** 2, pvalue_chi2=stats.chi2.sf(statistic ** 2, 1), df_chi2=1, distribution='normal')\n    return res"
        ]
    }
]