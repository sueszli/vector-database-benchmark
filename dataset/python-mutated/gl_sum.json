[
    {
        "func_name": "__init__",
        "original": "def __init__(self, input_size=1024, output_size=1024, freq=10000, heads=1, pos_enc=None):\n    \"\"\" The basic (multi-head) Attention 'cell' containing the learnable parameters of Q, K and V\n\n        :param int input_size: Feature input size of Q, K, V.\n        :param int output_size: Feature -hidden- size of Q, K, V.\n        :param int freq: The frequency of the sinusoidal positional encoding.\n        :param int heads: Number of heads for the attention module.\n        :param str | None pos_enc: The type of the positional encoding [supported: Absolute, Relative].\n        \"\"\"\n    super(SelfAttention, self).__init__()\n    self.permitted_encodings = ['absolute', 'relative']\n    if pos_enc is not None:\n        pos_enc = pos_enc.lower()\n        assert pos_enc in self.permitted_encodings, f'Supported encodings: {(*self.permitted_encodings,)}'\n    self.input_size = input_size\n    self.output_size = output_size\n    self.heads = heads\n    self.pos_enc = pos_enc\n    self.freq = freq\n    (self.Wk, self.Wq, self.Wv) = (nn.ModuleList(), nn.ModuleList(), nn.ModuleList())\n    for _ in range(self.heads):\n        self.Wk.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wq.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wv.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n    self.out = nn.Linear(in_features=output_size, out_features=input_size, bias=False)\n    self.softmax = nn.Softmax(dim=-1)\n    self.drop = nn.Dropout(p=0.5)",
        "mutated": [
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, heads=1, pos_enc=None):\n    if False:\n        i = 10\n    \" The basic (multi-head) Attention 'cell' containing the learnable parameters of Q, K and V\\n\\n        :param int input_size: Feature input size of Q, K, V.\\n        :param int output_size: Feature -hidden- size of Q, K, V.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param int heads: Number of heads for the attention module.\\n        :param str | None pos_enc: The type of the positional encoding [supported: Absolute, Relative].\\n        \"\n    super(SelfAttention, self).__init__()\n    self.permitted_encodings = ['absolute', 'relative']\n    if pos_enc is not None:\n        pos_enc = pos_enc.lower()\n        assert pos_enc in self.permitted_encodings, f'Supported encodings: {(*self.permitted_encodings,)}'\n    self.input_size = input_size\n    self.output_size = output_size\n    self.heads = heads\n    self.pos_enc = pos_enc\n    self.freq = freq\n    (self.Wk, self.Wq, self.Wv) = (nn.ModuleList(), nn.ModuleList(), nn.ModuleList())\n    for _ in range(self.heads):\n        self.Wk.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wq.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wv.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n    self.out = nn.Linear(in_features=output_size, out_features=input_size, bias=False)\n    self.softmax = nn.Softmax(dim=-1)\n    self.drop = nn.Dropout(p=0.5)",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, heads=1, pos_enc=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \" The basic (multi-head) Attention 'cell' containing the learnable parameters of Q, K and V\\n\\n        :param int input_size: Feature input size of Q, K, V.\\n        :param int output_size: Feature -hidden- size of Q, K, V.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param int heads: Number of heads for the attention module.\\n        :param str | None pos_enc: The type of the positional encoding [supported: Absolute, Relative].\\n        \"\n    super(SelfAttention, self).__init__()\n    self.permitted_encodings = ['absolute', 'relative']\n    if pos_enc is not None:\n        pos_enc = pos_enc.lower()\n        assert pos_enc in self.permitted_encodings, f'Supported encodings: {(*self.permitted_encodings,)}'\n    self.input_size = input_size\n    self.output_size = output_size\n    self.heads = heads\n    self.pos_enc = pos_enc\n    self.freq = freq\n    (self.Wk, self.Wq, self.Wv) = (nn.ModuleList(), nn.ModuleList(), nn.ModuleList())\n    for _ in range(self.heads):\n        self.Wk.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wq.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wv.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n    self.out = nn.Linear(in_features=output_size, out_features=input_size, bias=False)\n    self.softmax = nn.Softmax(dim=-1)\n    self.drop = nn.Dropout(p=0.5)",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, heads=1, pos_enc=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \" The basic (multi-head) Attention 'cell' containing the learnable parameters of Q, K and V\\n\\n        :param int input_size: Feature input size of Q, K, V.\\n        :param int output_size: Feature -hidden- size of Q, K, V.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param int heads: Number of heads for the attention module.\\n        :param str | None pos_enc: The type of the positional encoding [supported: Absolute, Relative].\\n        \"\n    super(SelfAttention, self).__init__()\n    self.permitted_encodings = ['absolute', 'relative']\n    if pos_enc is not None:\n        pos_enc = pos_enc.lower()\n        assert pos_enc in self.permitted_encodings, f'Supported encodings: {(*self.permitted_encodings,)}'\n    self.input_size = input_size\n    self.output_size = output_size\n    self.heads = heads\n    self.pos_enc = pos_enc\n    self.freq = freq\n    (self.Wk, self.Wq, self.Wv) = (nn.ModuleList(), nn.ModuleList(), nn.ModuleList())\n    for _ in range(self.heads):\n        self.Wk.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wq.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wv.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n    self.out = nn.Linear(in_features=output_size, out_features=input_size, bias=False)\n    self.softmax = nn.Softmax(dim=-1)\n    self.drop = nn.Dropout(p=0.5)",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, heads=1, pos_enc=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \" The basic (multi-head) Attention 'cell' containing the learnable parameters of Q, K and V\\n\\n        :param int input_size: Feature input size of Q, K, V.\\n        :param int output_size: Feature -hidden- size of Q, K, V.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param int heads: Number of heads for the attention module.\\n        :param str | None pos_enc: The type of the positional encoding [supported: Absolute, Relative].\\n        \"\n    super(SelfAttention, self).__init__()\n    self.permitted_encodings = ['absolute', 'relative']\n    if pos_enc is not None:\n        pos_enc = pos_enc.lower()\n        assert pos_enc in self.permitted_encodings, f'Supported encodings: {(*self.permitted_encodings,)}'\n    self.input_size = input_size\n    self.output_size = output_size\n    self.heads = heads\n    self.pos_enc = pos_enc\n    self.freq = freq\n    (self.Wk, self.Wq, self.Wv) = (nn.ModuleList(), nn.ModuleList(), nn.ModuleList())\n    for _ in range(self.heads):\n        self.Wk.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wq.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wv.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n    self.out = nn.Linear(in_features=output_size, out_features=input_size, bias=False)\n    self.softmax = nn.Softmax(dim=-1)\n    self.drop = nn.Dropout(p=0.5)",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, heads=1, pos_enc=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \" The basic (multi-head) Attention 'cell' containing the learnable parameters of Q, K and V\\n\\n        :param int input_size: Feature input size of Q, K, V.\\n        :param int output_size: Feature -hidden- size of Q, K, V.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param int heads: Number of heads for the attention module.\\n        :param str | None pos_enc: The type of the positional encoding [supported: Absolute, Relative].\\n        \"\n    super(SelfAttention, self).__init__()\n    self.permitted_encodings = ['absolute', 'relative']\n    if pos_enc is not None:\n        pos_enc = pos_enc.lower()\n        assert pos_enc in self.permitted_encodings, f'Supported encodings: {(*self.permitted_encodings,)}'\n    self.input_size = input_size\n    self.output_size = output_size\n    self.heads = heads\n    self.pos_enc = pos_enc\n    self.freq = freq\n    (self.Wk, self.Wq, self.Wv) = (nn.ModuleList(), nn.ModuleList(), nn.ModuleList())\n    for _ in range(self.heads):\n        self.Wk.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wq.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n        self.Wv.append(nn.Linear(in_features=input_size, out_features=output_size // heads, bias=False))\n    self.out = nn.Linear(in_features=output_size, out_features=input_size, bias=False)\n    self.softmax = nn.Softmax(dim=-1)\n    self.drop = nn.Dropout(p=0.5)"
        ]
    },
    {
        "func_name": "getAbsolutePosition",
        "original": "def getAbsolutePosition(self, T):\n    \"\"\"Calculate the sinusoidal positional encoding based on the absolute position of each considered frame.\n        Based on 'Attention is all you need' paper (https://arxiv.org/abs/1706.03762)\n\n        :param int T: Number of frames contained in Q, K and V\n        :return: Tensor with shape [T, T]\n        \"\"\"\n    freq = self.freq\n    d = self.input_size\n    pos = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    pos = pos.reshape(pos.shape[0], 1)\n    pos = pos.repeat_interleave(i.shape[0], dim=1)\n    i = i.repeat(pos.shape[0], 1)\n    AP = torch.zeros(T, T, device=self.out.weight.device)\n    AP[pos, 2 * i] = torch.sin(pos / freq ** (2 * i / d))\n    AP[pos, 2 * i + 1] = torch.cos(pos / freq ** (2 * i / d))\n    return AP",
        "mutated": [
            "def getAbsolutePosition(self, T):\n    if False:\n        i = 10\n    \"Calculate the sinusoidal positional encoding based on the absolute position of each considered frame.\\n        Based on 'Attention is all you need' paper (https://arxiv.org/abs/1706.03762)\\n\\n        :param int T: Number of frames contained in Q, K and V\\n        :return: Tensor with shape [T, T]\\n        \"\n    freq = self.freq\n    d = self.input_size\n    pos = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    pos = pos.reshape(pos.shape[0], 1)\n    pos = pos.repeat_interleave(i.shape[0], dim=1)\n    i = i.repeat(pos.shape[0], 1)\n    AP = torch.zeros(T, T, device=self.out.weight.device)\n    AP[pos, 2 * i] = torch.sin(pos / freq ** (2 * i / d))\n    AP[pos, 2 * i + 1] = torch.cos(pos / freq ** (2 * i / d))\n    return AP",
            "def getAbsolutePosition(self, T):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Calculate the sinusoidal positional encoding based on the absolute position of each considered frame.\\n        Based on 'Attention is all you need' paper (https://arxiv.org/abs/1706.03762)\\n\\n        :param int T: Number of frames contained in Q, K and V\\n        :return: Tensor with shape [T, T]\\n        \"\n    freq = self.freq\n    d = self.input_size\n    pos = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    pos = pos.reshape(pos.shape[0], 1)\n    pos = pos.repeat_interleave(i.shape[0], dim=1)\n    i = i.repeat(pos.shape[0], 1)\n    AP = torch.zeros(T, T, device=self.out.weight.device)\n    AP[pos, 2 * i] = torch.sin(pos / freq ** (2 * i / d))\n    AP[pos, 2 * i + 1] = torch.cos(pos / freq ** (2 * i / d))\n    return AP",
            "def getAbsolutePosition(self, T):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Calculate the sinusoidal positional encoding based on the absolute position of each considered frame.\\n        Based on 'Attention is all you need' paper (https://arxiv.org/abs/1706.03762)\\n\\n        :param int T: Number of frames contained in Q, K and V\\n        :return: Tensor with shape [T, T]\\n        \"\n    freq = self.freq\n    d = self.input_size\n    pos = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    pos = pos.reshape(pos.shape[0], 1)\n    pos = pos.repeat_interleave(i.shape[0], dim=1)\n    i = i.repeat(pos.shape[0], 1)\n    AP = torch.zeros(T, T, device=self.out.weight.device)\n    AP[pos, 2 * i] = torch.sin(pos / freq ** (2 * i / d))\n    AP[pos, 2 * i + 1] = torch.cos(pos / freq ** (2 * i / d))\n    return AP",
            "def getAbsolutePosition(self, T):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Calculate the sinusoidal positional encoding based on the absolute position of each considered frame.\\n        Based on 'Attention is all you need' paper (https://arxiv.org/abs/1706.03762)\\n\\n        :param int T: Number of frames contained in Q, K and V\\n        :return: Tensor with shape [T, T]\\n        \"\n    freq = self.freq\n    d = self.input_size\n    pos = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    pos = pos.reshape(pos.shape[0], 1)\n    pos = pos.repeat_interleave(i.shape[0], dim=1)\n    i = i.repeat(pos.shape[0], 1)\n    AP = torch.zeros(T, T, device=self.out.weight.device)\n    AP[pos, 2 * i] = torch.sin(pos / freq ** (2 * i / d))\n    AP[pos, 2 * i + 1] = torch.cos(pos / freq ** (2 * i / d))\n    return AP",
            "def getAbsolutePosition(self, T):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Calculate the sinusoidal positional encoding based on the absolute position of each considered frame.\\n        Based on 'Attention is all you need' paper (https://arxiv.org/abs/1706.03762)\\n\\n        :param int T: Number of frames contained in Q, K and V\\n        :return: Tensor with shape [T, T]\\n        \"\n    freq = self.freq\n    d = self.input_size\n    pos = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    pos = pos.reshape(pos.shape[0], 1)\n    pos = pos.repeat_interleave(i.shape[0], dim=1)\n    i = i.repeat(pos.shape[0], 1)\n    AP = torch.zeros(T, T, device=self.out.weight.device)\n    AP[pos, 2 * i] = torch.sin(pos / freq ** (2 * i / d))\n    AP[pos, 2 * i + 1] = torch.cos(pos / freq ** (2 * i / d))\n    return AP"
        ]
    },
    {
        "func_name": "getRelativePosition",
        "original": "def getRelativePosition(self, T):\n    \"\"\"Calculate the sinusoidal positional encoding based on the relative position of each considered frame.\n        r_pos calculations as here: https://theaisummer.com/positional-embeddings/\n\n        :param int T: Number of frames contained in Q, K and V\n        :return: Tensor with shape [T, T]\n        \"\"\"\n    freq = self.freq\n    d = 2 * T\n    min_rpos = -(T - 1)\n    i = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    j = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = i.reshape(i.shape[0], 1)\n    i = i.repeat_interleave(i.shape[0], dim=1)\n    j = j.repeat(i.shape[0], 1)\n    r_pos = j - i - min_rpos\n    RP = torch.zeros(T, T, device=self.out.weight.device)\n    idx = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    RP[:, 2 * idx] = torch.sin(r_pos[:, 2 * idx] / freq ** ((i[:, 2 * idx] + j[:, 2 * idx]) / d))\n    RP[:, 2 * idx + 1] = torch.cos(r_pos[:, 2 * idx + 1] / freq ** ((i[:, 2 * idx + 1] + j[:, 2 * idx + 1]) / d))\n    return RP",
        "mutated": [
            "def getRelativePosition(self, T):\n    if False:\n        i = 10\n    'Calculate the sinusoidal positional encoding based on the relative position of each considered frame.\\n        r_pos calculations as here: https://theaisummer.com/positional-embeddings/\\n\\n        :param int T: Number of frames contained in Q, K and V\\n        :return: Tensor with shape [T, T]\\n        '\n    freq = self.freq\n    d = 2 * T\n    min_rpos = -(T - 1)\n    i = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    j = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = i.reshape(i.shape[0], 1)\n    i = i.repeat_interleave(i.shape[0], dim=1)\n    j = j.repeat(i.shape[0], 1)\n    r_pos = j - i - min_rpos\n    RP = torch.zeros(T, T, device=self.out.weight.device)\n    idx = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    RP[:, 2 * idx] = torch.sin(r_pos[:, 2 * idx] / freq ** ((i[:, 2 * idx] + j[:, 2 * idx]) / d))\n    RP[:, 2 * idx + 1] = torch.cos(r_pos[:, 2 * idx + 1] / freq ** ((i[:, 2 * idx + 1] + j[:, 2 * idx + 1]) / d))\n    return RP",
            "def getRelativePosition(self, T):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate the sinusoidal positional encoding based on the relative position of each considered frame.\\n        r_pos calculations as here: https://theaisummer.com/positional-embeddings/\\n\\n        :param int T: Number of frames contained in Q, K and V\\n        :return: Tensor with shape [T, T]\\n        '\n    freq = self.freq\n    d = 2 * T\n    min_rpos = -(T - 1)\n    i = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    j = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = i.reshape(i.shape[0], 1)\n    i = i.repeat_interleave(i.shape[0], dim=1)\n    j = j.repeat(i.shape[0], 1)\n    r_pos = j - i - min_rpos\n    RP = torch.zeros(T, T, device=self.out.weight.device)\n    idx = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    RP[:, 2 * idx] = torch.sin(r_pos[:, 2 * idx] / freq ** ((i[:, 2 * idx] + j[:, 2 * idx]) / d))\n    RP[:, 2 * idx + 1] = torch.cos(r_pos[:, 2 * idx + 1] / freq ** ((i[:, 2 * idx + 1] + j[:, 2 * idx + 1]) / d))\n    return RP",
            "def getRelativePosition(self, T):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate the sinusoidal positional encoding based on the relative position of each considered frame.\\n        r_pos calculations as here: https://theaisummer.com/positional-embeddings/\\n\\n        :param int T: Number of frames contained in Q, K and V\\n        :return: Tensor with shape [T, T]\\n        '\n    freq = self.freq\n    d = 2 * T\n    min_rpos = -(T - 1)\n    i = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    j = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = i.reshape(i.shape[0], 1)\n    i = i.repeat_interleave(i.shape[0], dim=1)\n    j = j.repeat(i.shape[0], 1)\n    r_pos = j - i - min_rpos\n    RP = torch.zeros(T, T, device=self.out.weight.device)\n    idx = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    RP[:, 2 * idx] = torch.sin(r_pos[:, 2 * idx] / freq ** ((i[:, 2 * idx] + j[:, 2 * idx]) / d))\n    RP[:, 2 * idx + 1] = torch.cos(r_pos[:, 2 * idx + 1] / freq ** ((i[:, 2 * idx + 1] + j[:, 2 * idx + 1]) / d))\n    return RP",
            "def getRelativePosition(self, T):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate the sinusoidal positional encoding based on the relative position of each considered frame.\\n        r_pos calculations as here: https://theaisummer.com/positional-embeddings/\\n\\n        :param int T: Number of frames contained in Q, K and V\\n        :return: Tensor with shape [T, T]\\n        '\n    freq = self.freq\n    d = 2 * T\n    min_rpos = -(T - 1)\n    i = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    j = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = i.reshape(i.shape[0], 1)\n    i = i.repeat_interleave(i.shape[0], dim=1)\n    j = j.repeat(i.shape[0], 1)\n    r_pos = j - i - min_rpos\n    RP = torch.zeros(T, T, device=self.out.weight.device)\n    idx = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    RP[:, 2 * idx] = torch.sin(r_pos[:, 2 * idx] / freq ** ((i[:, 2 * idx] + j[:, 2 * idx]) / d))\n    RP[:, 2 * idx + 1] = torch.cos(r_pos[:, 2 * idx + 1] / freq ** ((i[:, 2 * idx + 1] + j[:, 2 * idx + 1]) / d))\n    return RP",
            "def getRelativePosition(self, T):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate the sinusoidal positional encoding based on the relative position of each considered frame.\\n        r_pos calculations as here: https://theaisummer.com/positional-embeddings/\\n\\n        :param int T: Number of frames contained in Q, K and V\\n        :return: Tensor with shape [T, T]\\n        '\n    freq = self.freq\n    d = 2 * T\n    min_rpos = -(T - 1)\n    i = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    j = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n    i = i.reshape(i.shape[0], 1)\n    i = i.repeat_interleave(i.shape[0], dim=1)\n    j = j.repeat(i.shape[0], 1)\n    r_pos = j - i - min_rpos\n    RP = torch.zeros(T, T, device=self.out.weight.device)\n    idx = torch.tensor([k for k in range(T // 2)], device=self.out.weight.device)\n    RP[:, 2 * idx] = torch.sin(r_pos[:, 2 * idx] / freq ** ((i[:, 2 * idx] + j[:, 2 * idx]) / d))\n    RP[:, 2 * idx + 1] = torch.cos(r_pos[:, 2 * idx + 1] / freq ** ((i[:, 2 * idx + 1] + j[:, 2 * idx + 1]) / d))\n    return RP"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    \"\"\" Compute the weighted frame features, based on either the global or local (multi-head) attention mechanism.\n\n        :param torch.tensor x: Frame features with shape [T, input_size]\n        :return: A tuple of:\n                    y: Weighted features based on the attention weights, with shape [T, input_size]\n                    att_weights : The attention weights (before dropout), with shape [T, T]\n        \"\"\"\n    outputs = []\n    for head in range(self.heads):\n        K = self.Wk[head](x)\n        Q = self.Wq[head](x)\n        V = self.Wv[head](x)\n        energies = torch.matmul(Q, K.transpose(1, 0))\n        if self.pos_enc is not None:\n            if self.pos_enc == 'absolute':\n                AP = self.getAbsolutePosition(T=energies.shape[0])\n                energies = energies + AP\n            elif self.pos_enc == 'relative':\n                RP = self.getRelativePosition(T=energies.shape[0])\n                energies = energies + RP\n        att_weights = self.softmax(energies)\n        _att_weights = self.drop(att_weights)\n        y = torch.matmul(_att_weights, V)\n        outputs.append(y)\n    y = self.out(torch.cat(outputs, dim=1))\n    return (y, att_weights.clone())",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    ' Compute the weighted frame features, based on either the global or local (multi-head) attention mechanism.\\n\\n        :param torch.tensor x: Frame features with shape [T, input_size]\\n        :return: A tuple of:\\n                    y: Weighted features based on the attention weights, with shape [T, input_size]\\n                    att_weights : The attention weights (before dropout), with shape [T, T]\\n        '\n    outputs = []\n    for head in range(self.heads):\n        K = self.Wk[head](x)\n        Q = self.Wq[head](x)\n        V = self.Wv[head](x)\n        energies = torch.matmul(Q, K.transpose(1, 0))\n        if self.pos_enc is not None:\n            if self.pos_enc == 'absolute':\n                AP = self.getAbsolutePosition(T=energies.shape[0])\n                energies = energies + AP\n            elif self.pos_enc == 'relative':\n                RP = self.getRelativePosition(T=energies.shape[0])\n                energies = energies + RP\n        att_weights = self.softmax(energies)\n        _att_weights = self.drop(att_weights)\n        y = torch.matmul(_att_weights, V)\n        outputs.append(y)\n    y = self.out(torch.cat(outputs, dim=1))\n    return (y, att_weights.clone())",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Compute the weighted frame features, based on either the global or local (multi-head) attention mechanism.\\n\\n        :param torch.tensor x: Frame features with shape [T, input_size]\\n        :return: A tuple of:\\n                    y: Weighted features based on the attention weights, with shape [T, input_size]\\n                    att_weights : The attention weights (before dropout), with shape [T, T]\\n        '\n    outputs = []\n    for head in range(self.heads):\n        K = self.Wk[head](x)\n        Q = self.Wq[head](x)\n        V = self.Wv[head](x)\n        energies = torch.matmul(Q, K.transpose(1, 0))\n        if self.pos_enc is not None:\n            if self.pos_enc == 'absolute':\n                AP = self.getAbsolutePosition(T=energies.shape[0])\n                energies = energies + AP\n            elif self.pos_enc == 'relative':\n                RP = self.getRelativePosition(T=energies.shape[0])\n                energies = energies + RP\n        att_weights = self.softmax(energies)\n        _att_weights = self.drop(att_weights)\n        y = torch.matmul(_att_weights, V)\n        outputs.append(y)\n    y = self.out(torch.cat(outputs, dim=1))\n    return (y, att_weights.clone())",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Compute the weighted frame features, based on either the global or local (multi-head) attention mechanism.\\n\\n        :param torch.tensor x: Frame features with shape [T, input_size]\\n        :return: A tuple of:\\n                    y: Weighted features based on the attention weights, with shape [T, input_size]\\n                    att_weights : The attention weights (before dropout), with shape [T, T]\\n        '\n    outputs = []\n    for head in range(self.heads):\n        K = self.Wk[head](x)\n        Q = self.Wq[head](x)\n        V = self.Wv[head](x)\n        energies = torch.matmul(Q, K.transpose(1, 0))\n        if self.pos_enc is not None:\n            if self.pos_enc == 'absolute':\n                AP = self.getAbsolutePosition(T=energies.shape[0])\n                energies = energies + AP\n            elif self.pos_enc == 'relative':\n                RP = self.getRelativePosition(T=energies.shape[0])\n                energies = energies + RP\n        att_weights = self.softmax(energies)\n        _att_weights = self.drop(att_weights)\n        y = torch.matmul(_att_weights, V)\n        outputs.append(y)\n    y = self.out(torch.cat(outputs, dim=1))\n    return (y, att_weights.clone())",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Compute the weighted frame features, based on either the global or local (multi-head) attention mechanism.\\n\\n        :param torch.tensor x: Frame features with shape [T, input_size]\\n        :return: A tuple of:\\n                    y: Weighted features based on the attention weights, with shape [T, input_size]\\n                    att_weights : The attention weights (before dropout), with shape [T, T]\\n        '\n    outputs = []\n    for head in range(self.heads):\n        K = self.Wk[head](x)\n        Q = self.Wq[head](x)\n        V = self.Wv[head](x)\n        energies = torch.matmul(Q, K.transpose(1, 0))\n        if self.pos_enc is not None:\n            if self.pos_enc == 'absolute':\n                AP = self.getAbsolutePosition(T=energies.shape[0])\n                energies = energies + AP\n            elif self.pos_enc == 'relative':\n                RP = self.getRelativePosition(T=energies.shape[0])\n                energies = energies + RP\n        att_weights = self.softmax(energies)\n        _att_weights = self.drop(att_weights)\n        y = torch.matmul(_att_weights, V)\n        outputs.append(y)\n    y = self.out(torch.cat(outputs, dim=1))\n    return (y, att_weights.clone())",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Compute the weighted frame features, based on either the global or local (multi-head) attention mechanism.\\n\\n        :param torch.tensor x: Frame features with shape [T, input_size]\\n        :return: A tuple of:\\n                    y: Weighted features based on the attention weights, with shape [T, input_size]\\n                    att_weights : The attention weights (before dropout), with shape [T, T]\\n        '\n    outputs = []\n    for head in range(self.heads):\n        K = self.Wk[head](x)\n        Q = self.Wq[head](x)\n        V = self.Wv[head](x)\n        energies = torch.matmul(Q, K.transpose(1, 0))\n        if self.pos_enc is not None:\n            if self.pos_enc == 'absolute':\n                AP = self.getAbsolutePosition(T=energies.shape[0])\n                energies = energies + AP\n            elif self.pos_enc == 'relative':\n                RP = self.getRelativePosition(T=energies.shape[0])\n                energies = energies + RP\n        att_weights = self.softmax(energies)\n        _att_weights = self.drop(att_weights)\n        y = torch.matmul(_att_weights, V)\n        outputs.append(y)\n    y = self.out(torch.cat(outputs, dim=1))\n    return (y, att_weights.clone())"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    \"\"\" Class wrapping the MultiAttention part of PGL-SUM; its key modules and parameters.\n\n        :param int input_size: The expected input feature size.\n        :param int output_size: The hidden feature size of the attention mechanisms.\n        :param int freq: The frequency of the sinusoidal positional encoding.\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\n        :param None | int num_segments: The selected number of segments to split the videos.\n        :param int heads: The selected number of global heads.\n        :param None | str fusion: The selected type of feature fusion.\n        \"\"\"\n    super(MultiAttention, self).__init__()\n    self.attention = SelfAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, heads=heads)\n    self.num_segments = num_segments\n    if self.num_segments is not None:\n        assert self.num_segments >= 2, 'num_segments must be None or 2+'\n        self.local_attention = nn.ModuleList()\n        for _ in range(self.num_segments):\n            self.local_attention.append(SelfAttention(input_size=input_size, output_size=output_size // num_segments, freq=freq, pos_enc=pos_enc, heads=4))\n    self.permitted_fusions = ['add', 'mult', 'avg', 'max']\n    self.fusion = fusion\n    if self.fusion is not None:\n        self.fusion = self.fusion.lower()\n        assert self.fusion in self.permitted_fusions, f'Fusion method must be: {(*self.permitted_fusions,)}'",
        "mutated": [
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    if False:\n        i = 10\n    ' Class wrapping the MultiAttention part of PGL-SUM; its key modules and parameters.\\n\\n        :param int input_size: The expected input feature size.\\n        :param int output_size: The hidden feature size of the attention mechanisms.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\\n        :param None | int num_segments: The selected number of segments to split the videos.\\n        :param int heads: The selected number of global heads.\\n        :param None | str fusion: The selected type of feature fusion.\\n        '\n    super(MultiAttention, self).__init__()\n    self.attention = SelfAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, heads=heads)\n    self.num_segments = num_segments\n    if self.num_segments is not None:\n        assert self.num_segments >= 2, 'num_segments must be None or 2+'\n        self.local_attention = nn.ModuleList()\n        for _ in range(self.num_segments):\n            self.local_attention.append(SelfAttention(input_size=input_size, output_size=output_size // num_segments, freq=freq, pos_enc=pos_enc, heads=4))\n    self.permitted_fusions = ['add', 'mult', 'avg', 'max']\n    self.fusion = fusion\n    if self.fusion is not None:\n        self.fusion = self.fusion.lower()\n        assert self.fusion in self.permitted_fusions, f'Fusion method must be: {(*self.permitted_fusions,)}'",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Class wrapping the MultiAttention part of PGL-SUM; its key modules and parameters.\\n\\n        :param int input_size: The expected input feature size.\\n        :param int output_size: The hidden feature size of the attention mechanisms.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\\n        :param None | int num_segments: The selected number of segments to split the videos.\\n        :param int heads: The selected number of global heads.\\n        :param None | str fusion: The selected type of feature fusion.\\n        '\n    super(MultiAttention, self).__init__()\n    self.attention = SelfAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, heads=heads)\n    self.num_segments = num_segments\n    if self.num_segments is not None:\n        assert self.num_segments >= 2, 'num_segments must be None or 2+'\n        self.local_attention = nn.ModuleList()\n        for _ in range(self.num_segments):\n            self.local_attention.append(SelfAttention(input_size=input_size, output_size=output_size // num_segments, freq=freq, pos_enc=pos_enc, heads=4))\n    self.permitted_fusions = ['add', 'mult', 'avg', 'max']\n    self.fusion = fusion\n    if self.fusion is not None:\n        self.fusion = self.fusion.lower()\n        assert self.fusion in self.permitted_fusions, f'Fusion method must be: {(*self.permitted_fusions,)}'",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Class wrapping the MultiAttention part of PGL-SUM; its key modules and parameters.\\n\\n        :param int input_size: The expected input feature size.\\n        :param int output_size: The hidden feature size of the attention mechanisms.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\\n        :param None | int num_segments: The selected number of segments to split the videos.\\n        :param int heads: The selected number of global heads.\\n        :param None | str fusion: The selected type of feature fusion.\\n        '\n    super(MultiAttention, self).__init__()\n    self.attention = SelfAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, heads=heads)\n    self.num_segments = num_segments\n    if self.num_segments is not None:\n        assert self.num_segments >= 2, 'num_segments must be None or 2+'\n        self.local_attention = nn.ModuleList()\n        for _ in range(self.num_segments):\n            self.local_attention.append(SelfAttention(input_size=input_size, output_size=output_size // num_segments, freq=freq, pos_enc=pos_enc, heads=4))\n    self.permitted_fusions = ['add', 'mult', 'avg', 'max']\n    self.fusion = fusion\n    if self.fusion is not None:\n        self.fusion = self.fusion.lower()\n        assert self.fusion in self.permitted_fusions, f'Fusion method must be: {(*self.permitted_fusions,)}'",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Class wrapping the MultiAttention part of PGL-SUM; its key modules and parameters.\\n\\n        :param int input_size: The expected input feature size.\\n        :param int output_size: The hidden feature size of the attention mechanisms.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\\n        :param None | int num_segments: The selected number of segments to split the videos.\\n        :param int heads: The selected number of global heads.\\n        :param None | str fusion: The selected type of feature fusion.\\n        '\n    super(MultiAttention, self).__init__()\n    self.attention = SelfAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, heads=heads)\n    self.num_segments = num_segments\n    if self.num_segments is not None:\n        assert self.num_segments >= 2, 'num_segments must be None or 2+'\n        self.local_attention = nn.ModuleList()\n        for _ in range(self.num_segments):\n            self.local_attention.append(SelfAttention(input_size=input_size, output_size=output_size // num_segments, freq=freq, pos_enc=pos_enc, heads=4))\n    self.permitted_fusions = ['add', 'mult', 'avg', 'max']\n    self.fusion = fusion\n    if self.fusion is not None:\n        self.fusion = self.fusion.lower()\n        assert self.fusion in self.permitted_fusions, f'Fusion method must be: {(*self.permitted_fusions,)}'",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Class wrapping the MultiAttention part of PGL-SUM; its key modules and parameters.\\n\\n        :param int input_size: The expected input feature size.\\n        :param int output_size: The hidden feature size of the attention mechanisms.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\\n        :param None | int num_segments: The selected number of segments to split the videos.\\n        :param int heads: The selected number of global heads.\\n        :param None | str fusion: The selected type of feature fusion.\\n        '\n    super(MultiAttention, self).__init__()\n    self.attention = SelfAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, heads=heads)\n    self.num_segments = num_segments\n    if self.num_segments is not None:\n        assert self.num_segments >= 2, 'num_segments must be None or 2+'\n        self.local_attention = nn.ModuleList()\n        for _ in range(self.num_segments):\n            self.local_attention.append(SelfAttention(input_size=input_size, output_size=output_size // num_segments, freq=freq, pos_enc=pos_enc, heads=4))\n    self.permitted_fusions = ['add', 'mult', 'avg', 'max']\n    self.fusion = fusion\n    if self.fusion is not None:\n        self.fusion = self.fusion.lower()\n        assert self.fusion in self.permitted_fusions, f'Fusion method must be: {(*self.permitted_fusions,)}'"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    \"\"\" Compute the weighted frame features, based on the global and locals (multi-head) attention mechanisms.\n\n        :param torch.Tensor x: Tensor with shape [T, input_size] containing the frame features.\n        :return: A tuple of:\n            weighted_value: Tensor with shape [T, input_size] containing the weighted frame features.\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\n        \"\"\"\n    (weighted_value, attn_weights) = self.attention(x)\n    if self.num_segments is not None and self.fusion is not None:\n        segment_size = math.ceil(x.shape[0] / self.num_segments)\n        for segment in range(self.num_segments):\n            left_pos = segment * segment_size\n            right_pos = (segment + 1) * segment_size\n            local_x = x[left_pos:right_pos]\n            (weighted_local_value, attn_local_weights) = self.local_attention[segment](local_x)\n            weighted_value[left_pos:right_pos] = F.normalize(weighted_value[left_pos:right_pos].clone(), p=2, dim=1)\n            weighted_local_value = F.normalize(weighted_local_value, p=2, dim=1)\n            if self.fusion == 'add':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n            elif self.fusion == 'mult':\n                weighted_value[left_pos:right_pos] *= weighted_local_value\n            elif self.fusion == 'avg':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n                weighted_value[left_pos:right_pos] /= 2\n            elif self.fusion == 'max':\n                weighted_value[left_pos:right_pos] = torch.max(weighted_value[left_pos:right_pos].clone(), weighted_local_value)\n    return (weighted_value, attn_weights)",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    ' Compute the weighted frame features, based on the global and locals (multi-head) attention mechanisms.\\n\\n        :param torch.Tensor x: Tensor with shape [T, input_size] containing the frame features.\\n        :return: A tuple of:\\n            weighted_value: Tensor with shape [T, input_size] containing the weighted frame features.\\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\\n        '\n    (weighted_value, attn_weights) = self.attention(x)\n    if self.num_segments is not None and self.fusion is not None:\n        segment_size = math.ceil(x.shape[0] / self.num_segments)\n        for segment in range(self.num_segments):\n            left_pos = segment * segment_size\n            right_pos = (segment + 1) * segment_size\n            local_x = x[left_pos:right_pos]\n            (weighted_local_value, attn_local_weights) = self.local_attention[segment](local_x)\n            weighted_value[left_pos:right_pos] = F.normalize(weighted_value[left_pos:right_pos].clone(), p=2, dim=1)\n            weighted_local_value = F.normalize(weighted_local_value, p=2, dim=1)\n            if self.fusion == 'add':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n            elif self.fusion == 'mult':\n                weighted_value[left_pos:right_pos] *= weighted_local_value\n            elif self.fusion == 'avg':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n                weighted_value[left_pos:right_pos] /= 2\n            elif self.fusion == 'max':\n                weighted_value[left_pos:right_pos] = torch.max(weighted_value[left_pos:right_pos].clone(), weighted_local_value)\n    return (weighted_value, attn_weights)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Compute the weighted frame features, based on the global and locals (multi-head) attention mechanisms.\\n\\n        :param torch.Tensor x: Tensor with shape [T, input_size] containing the frame features.\\n        :return: A tuple of:\\n            weighted_value: Tensor with shape [T, input_size] containing the weighted frame features.\\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\\n        '\n    (weighted_value, attn_weights) = self.attention(x)\n    if self.num_segments is not None and self.fusion is not None:\n        segment_size = math.ceil(x.shape[0] / self.num_segments)\n        for segment in range(self.num_segments):\n            left_pos = segment * segment_size\n            right_pos = (segment + 1) * segment_size\n            local_x = x[left_pos:right_pos]\n            (weighted_local_value, attn_local_weights) = self.local_attention[segment](local_x)\n            weighted_value[left_pos:right_pos] = F.normalize(weighted_value[left_pos:right_pos].clone(), p=2, dim=1)\n            weighted_local_value = F.normalize(weighted_local_value, p=2, dim=1)\n            if self.fusion == 'add':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n            elif self.fusion == 'mult':\n                weighted_value[left_pos:right_pos] *= weighted_local_value\n            elif self.fusion == 'avg':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n                weighted_value[left_pos:right_pos] /= 2\n            elif self.fusion == 'max':\n                weighted_value[left_pos:right_pos] = torch.max(weighted_value[left_pos:right_pos].clone(), weighted_local_value)\n    return (weighted_value, attn_weights)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Compute the weighted frame features, based on the global and locals (multi-head) attention mechanisms.\\n\\n        :param torch.Tensor x: Tensor with shape [T, input_size] containing the frame features.\\n        :return: A tuple of:\\n            weighted_value: Tensor with shape [T, input_size] containing the weighted frame features.\\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\\n        '\n    (weighted_value, attn_weights) = self.attention(x)\n    if self.num_segments is not None and self.fusion is not None:\n        segment_size = math.ceil(x.shape[0] / self.num_segments)\n        for segment in range(self.num_segments):\n            left_pos = segment * segment_size\n            right_pos = (segment + 1) * segment_size\n            local_x = x[left_pos:right_pos]\n            (weighted_local_value, attn_local_weights) = self.local_attention[segment](local_x)\n            weighted_value[left_pos:right_pos] = F.normalize(weighted_value[left_pos:right_pos].clone(), p=2, dim=1)\n            weighted_local_value = F.normalize(weighted_local_value, p=2, dim=1)\n            if self.fusion == 'add':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n            elif self.fusion == 'mult':\n                weighted_value[left_pos:right_pos] *= weighted_local_value\n            elif self.fusion == 'avg':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n                weighted_value[left_pos:right_pos] /= 2\n            elif self.fusion == 'max':\n                weighted_value[left_pos:right_pos] = torch.max(weighted_value[left_pos:right_pos].clone(), weighted_local_value)\n    return (weighted_value, attn_weights)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Compute the weighted frame features, based on the global and locals (multi-head) attention mechanisms.\\n\\n        :param torch.Tensor x: Tensor with shape [T, input_size] containing the frame features.\\n        :return: A tuple of:\\n            weighted_value: Tensor with shape [T, input_size] containing the weighted frame features.\\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\\n        '\n    (weighted_value, attn_weights) = self.attention(x)\n    if self.num_segments is not None and self.fusion is not None:\n        segment_size = math.ceil(x.shape[0] / self.num_segments)\n        for segment in range(self.num_segments):\n            left_pos = segment * segment_size\n            right_pos = (segment + 1) * segment_size\n            local_x = x[left_pos:right_pos]\n            (weighted_local_value, attn_local_weights) = self.local_attention[segment](local_x)\n            weighted_value[left_pos:right_pos] = F.normalize(weighted_value[left_pos:right_pos].clone(), p=2, dim=1)\n            weighted_local_value = F.normalize(weighted_local_value, p=2, dim=1)\n            if self.fusion == 'add':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n            elif self.fusion == 'mult':\n                weighted_value[left_pos:right_pos] *= weighted_local_value\n            elif self.fusion == 'avg':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n                weighted_value[left_pos:right_pos] /= 2\n            elif self.fusion == 'max':\n                weighted_value[left_pos:right_pos] = torch.max(weighted_value[left_pos:right_pos].clone(), weighted_local_value)\n    return (weighted_value, attn_weights)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Compute the weighted frame features, based on the global and locals (multi-head) attention mechanisms.\\n\\n        :param torch.Tensor x: Tensor with shape [T, input_size] containing the frame features.\\n        :return: A tuple of:\\n            weighted_value: Tensor with shape [T, input_size] containing the weighted frame features.\\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\\n        '\n    (weighted_value, attn_weights) = self.attention(x)\n    if self.num_segments is not None and self.fusion is not None:\n        segment_size = math.ceil(x.shape[0] / self.num_segments)\n        for segment in range(self.num_segments):\n            left_pos = segment * segment_size\n            right_pos = (segment + 1) * segment_size\n            local_x = x[left_pos:right_pos]\n            (weighted_local_value, attn_local_weights) = self.local_attention[segment](local_x)\n            weighted_value[left_pos:right_pos] = F.normalize(weighted_value[left_pos:right_pos].clone(), p=2, dim=1)\n            weighted_local_value = F.normalize(weighted_local_value, p=2, dim=1)\n            if self.fusion == 'add':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n            elif self.fusion == 'mult':\n                weighted_value[left_pos:right_pos] *= weighted_local_value\n            elif self.fusion == 'avg':\n                weighted_value[left_pos:right_pos] += weighted_local_value\n                weighted_value[left_pos:right_pos] /= 2\n            elif self.fusion == 'max':\n                weighted_value[left_pos:right_pos] = torch.max(weighted_value[left_pos:right_pos].clone(), weighted_local_value)\n    return (weighted_value, attn_weights)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    \"\"\" Class wrapping the PGL-SUM model; its key modules and parameters.\n\n        :param int input_size: The expected input feature size.\n        :param int output_size: The hidden feature size of the attention mechanisms.\n        :param int freq: The frequency of the sinusoidal positional encoding.\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\n        :param None | int num_segments: The selected number of segments to split the videos.\n        :param int heads: The selected number of global heads.\n        :param None | str fusion: The selected type of feature fusion.\n        \"\"\"\n    super(PGL_SUM, self).__init__()\n    self.attention = MultiAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, num_segments=num_segments, heads=heads, fusion=fusion)\n    self.linear_1 = nn.Linear(in_features=input_size, out_features=input_size)\n    self.linear_2 = nn.Linear(in_features=self.linear_1.out_features, out_features=1)\n    self.drop = nn.Dropout(p=0.5)\n    self.norm_y = nn.LayerNorm(normalized_shape=input_size, eps=1e-06)\n    self.norm_linear = nn.LayerNorm(normalized_shape=self.linear_1.out_features, eps=1e-06)\n    self.relu = nn.ReLU()\n    self.sigmoid = nn.Sigmoid()",
        "mutated": [
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    if False:\n        i = 10\n    ' Class wrapping the PGL-SUM model; its key modules and parameters.\\n\\n        :param int input_size: The expected input feature size.\\n        :param int output_size: The hidden feature size of the attention mechanisms.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\\n        :param None | int num_segments: The selected number of segments to split the videos.\\n        :param int heads: The selected number of global heads.\\n        :param None | str fusion: The selected type of feature fusion.\\n        '\n    super(PGL_SUM, self).__init__()\n    self.attention = MultiAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, num_segments=num_segments, heads=heads, fusion=fusion)\n    self.linear_1 = nn.Linear(in_features=input_size, out_features=input_size)\n    self.linear_2 = nn.Linear(in_features=self.linear_1.out_features, out_features=1)\n    self.drop = nn.Dropout(p=0.5)\n    self.norm_y = nn.LayerNorm(normalized_shape=input_size, eps=1e-06)\n    self.norm_linear = nn.LayerNorm(normalized_shape=self.linear_1.out_features, eps=1e-06)\n    self.relu = nn.ReLU()\n    self.sigmoid = nn.Sigmoid()",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Class wrapping the PGL-SUM model; its key modules and parameters.\\n\\n        :param int input_size: The expected input feature size.\\n        :param int output_size: The hidden feature size of the attention mechanisms.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\\n        :param None | int num_segments: The selected number of segments to split the videos.\\n        :param int heads: The selected number of global heads.\\n        :param None | str fusion: The selected type of feature fusion.\\n        '\n    super(PGL_SUM, self).__init__()\n    self.attention = MultiAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, num_segments=num_segments, heads=heads, fusion=fusion)\n    self.linear_1 = nn.Linear(in_features=input_size, out_features=input_size)\n    self.linear_2 = nn.Linear(in_features=self.linear_1.out_features, out_features=1)\n    self.drop = nn.Dropout(p=0.5)\n    self.norm_y = nn.LayerNorm(normalized_shape=input_size, eps=1e-06)\n    self.norm_linear = nn.LayerNorm(normalized_shape=self.linear_1.out_features, eps=1e-06)\n    self.relu = nn.ReLU()\n    self.sigmoid = nn.Sigmoid()",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Class wrapping the PGL-SUM model; its key modules and parameters.\\n\\n        :param int input_size: The expected input feature size.\\n        :param int output_size: The hidden feature size of the attention mechanisms.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\\n        :param None | int num_segments: The selected number of segments to split the videos.\\n        :param int heads: The selected number of global heads.\\n        :param None | str fusion: The selected type of feature fusion.\\n        '\n    super(PGL_SUM, self).__init__()\n    self.attention = MultiAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, num_segments=num_segments, heads=heads, fusion=fusion)\n    self.linear_1 = nn.Linear(in_features=input_size, out_features=input_size)\n    self.linear_2 = nn.Linear(in_features=self.linear_1.out_features, out_features=1)\n    self.drop = nn.Dropout(p=0.5)\n    self.norm_y = nn.LayerNorm(normalized_shape=input_size, eps=1e-06)\n    self.norm_linear = nn.LayerNorm(normalized_shape=self.linear_1.out_features, eps=1e-06)\n    self.relu = nn.ReLU()\n    self.sigmoid = nn.Sigmoid()",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Class wrapping the PGL-SUM model; its key modules and parameters.\\n\\n        :param int input_size: The expected input feature size.\\n        :param int output_size: The hidden feature size of the attention mechanisms.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\\n        :param None | int num_segments: The selected number of segments to split the videos.\\n        :param int heads: The selected number of global heads.\\n        :param None | str fusion: The selected type of feature fusion.\\n        '\n    super(PGL_SUM, self).__init__()\n    self.attention = MultiAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, num_segments=num_segments, heads=heads, fusion=fusion)\n    self.linear_1 = nn.Linear(in_features=input_size, out_features=input_size)\n    self.linear_2 = nn.Linear(in_features=self.linear_1.out_features, out_features=1)\n    self.drop = nn.Dropout(p=0.5)\n    self.norm_y = nn.LayerNorm(normalized_shape=input_size, eps=1e-06)\n    self.norm_linear = nn.LayerNorm(normalized_shape=self.linear_1.out_features, eps=1e-06)\n    self.relu = nn.ReLU()\n    self.sigmoid = nn.Sigmoid()",
            "def __init__(self, input_size=1024, output_size=1024, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Class wrapping the PGL-SUM model; its key modules and parameters.\\n\\n        :param int input_size: The expected input feature size.\\n        :param int output_size: The hidden feature size of the attention mechanisms.\\n        :param int freq: The frequency of the sinusoidal positional encoding.\\n        :param None | str pos_enc: The selected positional encoding [absolute, relative].\\n        :param None | int num_segments: The selected number of segments to split the videos.\\n        :param int heads: The selected number of global heads.\\n        :param None | str fusion: The selected type of feature fusion.\\n        '\n    super(PGL_SUM, self).__init__()\n    self.attention = MultiAttention(input_size=input_size, output_size=output_size, freq=freq, pos_enc=pos_enc, num_segments=num_segments, heads=heads, fusion=fusion)\n    self.linear_1 = nn.Linear(in_features=input_size, out_features=input_size)\n    self.linear_2 = nn.Linear(in_features=self.linear_1.out_features, out_features=1)\n    self.drop = nn.Dropout(p=0.5)\n    self.norm_y = nn.LayerNorm(normalized_shape=input_size, eps=1e-06)\n    self.norm_linear = nn.LayerNorm(normalized_shape=self.linear_1.out_features, eps=1e-06)\n    self.relu = nn.ReLU()\n    self.sigmoid = nn.Sigmoid()"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, frame_features):\n    \"\"\" Produce frames importance scores from the frame features, using the PGL-SUM model.\n\n        :param torch.Tensor frame_features: Tensor of shape [T, input_size] containing the frame features produced by\n        using the pool5 layer of GoogleNet.\n        :return: A tuple of:\n            y: Tensor with shape [1, T] containing the frames importance scores in [0, 1].\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\n        \"\"\"\n    frame_features = frame_features.reshape(-1, frame_features.shape[-1])\n    residual = frame_features\n    (weighted_value, attn_weights) = self.attention(frame_features)\n    y = weighted_value + residual\n    y = self.drop(y)\n    y = self.norm_y(y)\n    y = self.linear_1(y)\n    y = self.relu(y)\n    y = self.drop(y)\n    y = self.norm_linear(y)\n    y = self.linear_2(y)\n    y = self.sigmoid(y)\n    y = y.view(1, -1)\n    return (y, attn_weights)",
        "mutated": [
            "def forward(self, frame_features):\n    if False:\n        i = 10\n    ' Produce frames importance scores from the frame features, using the PGL-SUM model.\\n\\n        :param torch.Tensor frame_features: Tensor of shape [T, input_size] containing the frame features produced by\\n        using the pool5 layer of GoogleNet.\\n        :return: A tuple of:\\n            y: Tensor with shape [1, T] containing the frames importance scores in [0, 1].\\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\\n        '\n    frame_features = frame_features.reshape(-1, frame_features.shape[-1])\n    residual = frame_features\n    (weighted_value, attn_weights) = self.attention(frame_features)\n    y = weighted_value + residual\n    y = self.drop(y)\n    y = self.norm_y(y)\n    y = self.linear_1(y)\n    y = self.relu(y)\n    y = self.drop(y)\n    y = self.norm_linear(y)\n    y = self.linear_2(y)\n    y = self.sigmoid(y)\n    y = y.view(1, -1)\n    return (y, attn_weights)",
            "def forward(self, frame_features):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Produce frames importance scores from the frame features, using the PGL-SUM model.\\n\\n        :param torch.Tensor frame_features: Tensor of shape [T, input_size] containing the frame features produced by\\n        using the pool5 layer of GoogleNet.\\n        :return: A tuple of:\\n            y: Tensor with shape [1, T] containing the frames importance scores in [0, 1].\\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\\n        '\n    frame_features = frame_features.reshape(-1, frame_features.shape[-1])\n    residual = frame_features\n    (weighted_value, attn_weights) = self.attention(frame_features)\n    y = weighted_value + residual\n    y = self.drop(y)\n    y = self.norm_y(y)\n    y = self.linear_1(y)\n    y = self.relu(y)\n    y = self.drop(y)\n    y = self.norm_linear(y)\n    y = self.linear_2(y)\n    y = self.sigmoid(y)\n    y = y.view(1, -1)\n    return (y, attn_weights)",
            "def forward(self, frame_features):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Produce frames importance scores from the frame features, using the PGL-SUM model.\\n\\n        :param torch.Tensor frame_features: Tensor of shape [T, input_size] containing the frame features produced by\\n        using the pool5 layer of GoogleNet.\\n        :return: A tuple of:\\n            y: Tensor with shape [1, T] containing the frames importance scores in [0, 1].\\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\\n        '\n    frame_features = frame_features.reshape(-1, frame_features.shape[-1])\n    residual = frame_features\n    (weighted_value, attn_weights) = self.attention(frame_features)\n    y = weighted_value + residual\n    y = self.drop(y)\n    y = self.norm_y(y)\n    y = self.linear_1(y)\n    y = self.relu(y)\n    y = self.drop(y)\n    y = self.norm_linear(y)\n    y = self.linear_2(y)\n    y = self.sigmoid(y)\n    y = y.view(1, -1)\n    return (y, attn_weights)",
            "def forward(self, frame_features):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Produce frames importance scores from the frame features, using the PGL-SUM model.\\n\\n        :param torch.Tensor frame_features: Tensor of shape [T, input_size] containing the frame features produced by\\n        using the pool5 layer of GoogleNet.\\n        :return: A tuple of:\\n            y: Tensor with shape [1, T] containing the frames importance scores in [0, 1].\\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\\n        '\n    frame_features = frame_features.reshape(-1, frame_features.shape[-1])\n    residual = frame_features\n    (weighted_value, attn_weights) = self.attention(frame_features)\n    y = weighted_value + residual\n    y = self.drop(y)\n    y = self.norm_y(y)\n    y = self.linear_1(y)\n    y = self.relu(y)\n    y = self.drop(y)\n    y = self.norm_linear(y)\n    y = self.linear_2(y)\n    y = self.sigmoid(y)\n    y = y.view(1, -1)\n    return (y, attn_weights)",
            "def forward(self, frame_features):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Produce frames importance scores from the frame features, using the PGL-SUM model.\\n\\n        :param torch.Tensor frame_features: Tensor of shape [T, input_size] containing the frame features produced by\\n        using the pool5 layer of GoogleNet.\\n        :return: A tuple of:\\n            y: Tensor with shape [1, T] containing the frames importance scores in [0, 1].\\n            attn_weights: Tensor with shape [T, T] containing the attention weights.\\n        '\n    frame_features = frame_features.reshape(-1, frame_features.shape[-1])\n    residual = frame_features\n    (weighted_value, attn_weights) = self.attention(frame_features)\n    y = weighted_value + residual\n    y = self.drop(y)\n    y = self.norm_y(y)\n    y = self.linear_1(y)\n    y = self.relu(y)\n    y = self.drop(y)\n    y = self.norm_linear(y)\n    y = self.linear_2(y)\n    y = self.sigmoid(y)\n    y = y.view(1, -1)\n    return (y, attn_weights)"
        ]
    }
]