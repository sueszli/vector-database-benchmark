[
    {
        "func_name": "_static_stop_gradient",
        "original": "@staticmethod\ndef _static_stop_gradient(x: Union[ivy.Container, ivy.Array, ivy.NativeArray], /, *, preserve_type: Union[bool, ivy.Container]=True, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, out: Optional[ivy.Container]=None) -> ivy.Container:\n    \"\"\"\n        ivy.Container static method variant of ivy.stop_gradient. This method simply\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\n        this method with minimal changes.\n\n        Parameters\n        ----------\n        x\n            Array or Container for which to stop the gradient.\n        key_chains\n            The key-chains to apply or not apply the method to. Default is ``None``.\n        to_apply\n            If True, the method will be applied to key_chains, otherwise key_chains\n            will be skipped. Default is ``True``.\n        prune_unapplied\n            Whether to prune key_chains for which the function was not applied.\n            Default is ``False``.\n        map_sequences\n            Whether to also map method to sequences (lists, tuples).\n            Default is ``False``.\n        preserve_type\n            Whether to preserve gradient computation on ivy.Array instances. Default is\n            True.\n        out\n            optional output array, for writing the result to. It must have a shape\n            that the inputs broadcast to.\n\n        Returns\n        -------\n        ret\n            The same array x, but with no gradient information.\n\n        Examples\n        --------\n        With one :class:`ivy.Container` inputs:\n\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\n        ...                      b=ivy.array([3., 4., 5.]))\n        >>> y = ivy.Container.static_stop_gradient(x, preserve_type=False)\n        >>> print(y)\n        {\n            a: ivy.array([0., 1., 2.]),\n            b: ivy.array([3., 4., 5.])\n        }\n\n        With multiple :class:`ivy.Container` inputs:\n\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\n        ...                      b=ivy.array([3., 4., 5.]))\n        >>> ivy.Container.static_stop_gradient(x, preserve_type=True, out=x)\n        >>> print(x)\n        {\n            a: ivy.array([0., 1., 2.]),\n            b: ivy.array([3., 4., 5.])\n        }\n        \"\"\"\n    return ContainerBase.cont_multi_map_in_function('stop_gradient', x, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)",
        "mutated": [
            "@staticmethod\ndef _static_stop_gradient(x: Union[ivy.Container, ivy.Array, ivy.NativeArray], /, *, preserve_type: Union[bool, ivy.Container]=True, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n    '\\n        ivy.Container static method variant of ivy.stop_gradient. This method simply\\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\\n        this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        x\\n            Array or Container for which to stop the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        preserve_type\\n            Whether to preserve gradient computation on ivy.Array instances. Default is\\n            True.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The same array x, but with no gradient information.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> y = ivy.Container.static_stop_gradient(x, preserve_type=False)\\n        >>> print(y)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> ivy.Container.static_stop_gradient(x, preserve_type=True, out=x)\\n        >>> print(x)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return ContainerBase.cont_multi_map_in_function('stop_gradient', x, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)",
            "@staticmethod\ndef _static_stop_gradient(x: Union[ivy.Container, ivy.Array, ivy.NativeArray], /, *, preserve_type: Union[bool, ivy.Container]=True, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        ivy.Container static method variant of ivy.stop_gradient. This method simply\\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\\n        this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        x\\n            Array or Container for which to stop the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        preserve_type\\n            Whether to preserve gradient computation on ivy.Array instances. Default is\\n            True.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The same array x, but with no gradient information.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> y = ivy.Container.static_stop_gradient(x, preserve_type=False)\\n        >>> print(y)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> ivy.Container.static_stop_gradient(x, preserve_type=True, out=x)\\n        >>> print(x)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return ContainerBase.cont_multi_map_in_function('stop_gradient', x, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)",
            "@staticmethod\ndef _static_stop_gradient(x: Union[ivy.Container, ivy.Array, ivy.NativeArray], /, *, preserve_type: Union[bool, ivy.Container]=True, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        ivy.Container static method variant of ivy.stop_gradient. This method simply\\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\\n        this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        x\\n            Array or Container for which to stop the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        preserve_type\\n            Whether to preserve gradient computation on ivy.Array instances. Default is\\n            True.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The same array x, but with no gradient information.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> y = ivy.Container.static_stop_gradient(x, preserve_type=False)\\n        >>> print(y)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> ivy.Container.static_stop_gradient(x, preserve_type=True, out=x)\\n        >>> print(x)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return ContainerBase.cont_multi_map_in_function('stop_gradient', x, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)",
            "@staticmethod\ndef _static_stop_gradient(x: Union[ivy.Container, ivy.Array, ivy.NativeArray], /, *, preserve_type: Union[bool, ivy.Container]=True, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        ivy.Container static method variant of ivy.stop_gradient. This method simply\\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\\n        this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        x\\n            Array or Container for which to stop the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        preserve_type\\n            Whether to preserve gradient computation on ivy.Array instances. Default is\\n            True.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The same array x, but with no gradient information.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> y = ivy.Container.static_stop_gradient(x, preserve_type=False)\\n        >>> print(y)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> ivy.Container.static_stop_gradient(x, preserve_type=True, out=x)\\n        >>> print(x)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return ContainerBase.cont_multi_map_in_function('stop_gradient', x, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)",
            "@staticmethod\ndef _static_stop_gradient(x: Union[ivy.Container, ivy.Array, ivy.NativeArray], /, *, preserve_type: Union[bool, ivy.Container]=True, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        ivy.Container static method variant of ivy.stop_gradient. This method simply\\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\\n        this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        x\\n            Array or Container for which to stop the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        preserve_type\\n            Whether to preserve gradient computation on ivy.Array instances. Default is\\n            True.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The same array x, but with no gradient information.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> y = ivy.Container.static_stop_gradient(x, preserve_type=False)\\n        >>> print(y)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> ivy.Container.static_stop_gradient(x, preserve_type=True, out=x)\\n        >>> print(x)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return ContainerBase.cont_multi_map_in_function('stop_gradient', x, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)"
        ]
    },
    {
        "func_name": "stop_gradient",
        "original": "def stop_gradient(self: ivy.Container, /, *, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, preserve_type: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    \"\"\"\n        ivy.Container instance method variant of ivy.stop_gradient. This method simply\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\n        this method with minimal changes.\n\n        Parameters\n        ----------\n        self\n            Container for which to stop the gradient.\n        key_chains\n            The key-chains to apply or not apply the method to. Default is ``None``.\n        to_apply\n            If True, the method will be applied to key_chains, otherwise key_chains\n            will be skipped. Default is ``True``.\n        prune_unapplied\n            Whether to prune key_chains for which the function was not applied.\n            Default is ``False``.\n        map_sequences\n            Whether to also map method to sequences (lists, tuples).\n            Default is ``False``.\n        preserve_type\n            Whether to preserve gradient computation on ivy.Array instances. Default is\n            True.\n        out\n            optional output array, for writing the result to. It must have a shape\n            that the inputs broadcast to.\n\n        Returns\n        -------\n        ret\n            The same array x, but with no gradient information.\n\n        Examples\n        --------\n        With one :class:`ivy.Container` inputs:\n\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\n        ...                      b=ivy.array([3., 4., 5.]))\n        >>> y = x.stop_gradient(preserve_type=False)\n        >>> print(y)\n        {\n            a: ivy.array([0., 1., 2.]),\n            b: ivy.array([3., 4., 5.])\n        }\n\n        With multiple :class:`ivy.Container` inputs:\n\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\n        ...                      b=ivy.array([3., 4., 5.]))\n        >>> x.stop_gradient(preserve_type=True, out=x)\n        >>> print(x)\n        {\n            a: ivy.array([0., 1., 2.]),\n            b: ivy.array([3., 4., 5.])\n        }\n        \"\"\"\n    return self._static_stop_gradient(self, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)",
        "mutated": [
            "def stop_gradient(self: ivy.Container, /, *, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, preserve_type: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n    '\\n        ivy.Container instance method variant of ivy.stop_gradient. This method simply\\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\\n        this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Container for which to stop the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        preserve_type\\n            Whether to preserve gradient computation on ivy.Array instances. Default is\\n            True.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The same array x, but with no gradient information.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> y = x.stop_gradient(preserve_type=False)\\n        >>> print(y)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> x.stop_gradient(preserve_type=True, out=x)\\n        >>> print(x)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return self._static_stop_gradient(self, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)",
            "def stop_gradient(self: ivy.Container, /, *, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, preserve_type: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        ivy.Container instance method variant of ivy.stop_gradient. This method simply\\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\\n        this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Container for which to stop the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        preserve_type\\n            Whether to preserve gradient computation on ivy.Array instances. Default is\\n            True.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The same array x, but with no gradient information.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> y = x.stop_gradient(preserve_type=False)\\n        >>> print(y)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> x.stop_gradient(preserve_type=True, out=x)\\n        >>> print(x)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return self._static_stop_gradient(self, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)",
            "def stop_gradient(self: ivy.Container, /, *, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, preserve_type: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        ivy.Container instance method variant of ivy.stop_gradient. This method simply\\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\\n        this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Container for which to stop the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        preserve_type\\n            Whether to preserve gradient computation on ivy.Array instances. Default is\\n            True.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The same array x, but with no gradient information.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> y = x.stop_gradient(preserve_type=False)\\n        >>> print(y)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> x.stop_gradient(preserve_type=True, out=x)\\n        >>> print(x)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return self._static_stop_gradient(self, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)",
            "def stop_gradient(self: ivy.Container, /, *, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, preserve_type: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        ivy.Container instance method variant of ivy.stop_gradient. This method simply\\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\\n        this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Container for which to stop the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        preserve_type\\n            Whether to preserve gradient computation on ivy.Array instances. Default is\\n            True.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The same array x, but with no gradient information.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> y = x.stop_gradient(preserve_type=False)\\n        >>> print(y)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> x.stop_gradient(preserve_type=True, out=x)\\n        >>> print(x)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return self._static_stop_gradient(self, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)",
            "def stop_gradient(self: ivy.Container, /, *, key_chains: Optional[Union[List[str], Dict[str, str], ivy.Container]]=None, to_apply: Union[bool, ivy.Container]=True, prune_unapplied: Union[bool, ivy.Container]=False, map_sequences: Union[bool, ivy.Container]=False, preserve_type: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        ivy.Container instance method variant of ivy.stop_gradient. This method simply\\n        wraps the function, and so the docstring for ivy.stop_gradient also applies to\\n        this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Container for which to stop the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        preserve_type\\n            Whether to preserve gradient computation on ivy.Array instances. Default is\\n            True.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The same array x, but with no gradient information.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> y = x.stop_gradient(preserve_type=False)\\n        >>> print(y)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> x.stop_gradient(preserve_type=True, out=x)\\n        >>> print(x)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return self._static_stop_gradient(self, key_chains=key_chains, to_apply=to_apply, prune_unapplied=prune_unapplied, map_sequences=map_sequences, preserve_type=preserve_type, out=out)"
        ]
    },
    {
        "func_name": "adam_step",
        "original": "def adam_step(self: ivy.Container, mw: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, float, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, out: Optional[ivy.Container]=None) -> ivy.Container:\n    \"\"\"\n        ivy.Container instance method variant of ivy.adam_step. This method simply wraps\n        the function, and so the docstring for ivy.adam_step also applies to this method\n        with minimal changes.\n\n        Parameters\n        ----------\n        self\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\n        mw\n            running average of the gradients.\n        vw\n            running average of second moments of the gradients.\n        step\n            training step.\n        beta1\n            gradient forgetting factor (Default value = 0.9).\n        beta2\n            second moment of gradient forgetting factor (Default value = 0.999).\n        epsilon\n            divisor during adam update, preventing division by zero\n            (Default value = 1e-7).\n        out\n            optional output container, for writing the result to. It must have a shape\n            that the inputs broadcast to.\n\n        Returns\n        -------\n        ret\n            The adam step delta.\n\n        Examples\n        --------\n        With one :class:`ivy.Container` input:\n\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\n        ...                         b=ivy.array([3., 4., 5.]))\n        >>> mw = ivy.array([1., 4., 9.])\n        >>> vw = ivy.array([0.,])\n        >>> step = ivy.array([3.4])\n        >>> beta1 = 0.87\n        >>> beta2 = 0.976\n        >>> epsilon = 1e-5\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\n        ...                                     epsilon=epsilon)\n        >>> print(adam_step_delta)\n        ({\n            a: ivy.array([6.49e+04, 1.74e+01, 1.95e+01]),\n            b: ivy.array([2.02, 4.82, 8.17])\n        }, {\n            a: ivy.array([0.87, 3.61, 8.09]),\n            b: ivy.array([1.26, 4., 8.48])\n        }, {\n            a: ivy.array([0., 0.024, 0.096]),\n            b: ivy.array([0.216, 0.384, 0.6])\n        })\n\n        With multiple :class:`ivy.Container` inputs:\n\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\n        ...                        b=ivy.array([3., 4., 5.]))\n        >>> mw = ivy.Container(a=ivy.array([0., 0., 0.]),\n        ...                    b=ivy.array([0., 0., 0.]))\n        >>> vw = ivy.Container(a=ivy.array([0.,]),\n        ...                    b=ivy.array([0.,]))\n        >>> step = ivy.array([3.4])\n        >>> beta1 = 0.87\n        >>> beta2 = 0.976\n        >>> epsilon = 1e-5\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\n        ...                                     epsilon=epsilon)\n        >>> print(adam_step_delta)\n        ({\n            a: ivy.array([0., 0.626, 0.626]),\n            b: ivy.array([0.626, 0.626, 0.626])\n        }, {\n            a: ivy.array([0., 0.13, 0.26]),\n            b: ivy.array([0.39, 0.52, 0.65])\n        }, {\n            a: ivy.array([0., 0.024, 0.096]),\n            b: ivy.array([0.216, 0.384, 0.6])\n        })\n        \"\"\"\n    return ivy.adam_step(self, mw, vw, step, beta1=beta1, beta2=beta2, epsilon=epsilon, out=out)",
        "mutated": [
            "def adam_step(self: ivy.Container, mw: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, float, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n    '\\n        ivy.Container instance method variant of ivy.adam_step. This method simply wraps\\n        the function, and so the docstring for ivy.adam_step also applies to this method\\n        with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        mw\\n            running average of the gradients.\\n        vw\\n            running average of second moments of the gradients.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The adam step delta.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` input:\\n\\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                         b=ivy.array([3., 4., 5.]))\\n        >>> mw = ivy.array([1., 4., 9.])\\n        >>> vw = ivy.array([0.,])\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.87\\n        >>> beta2 = 0.976\\n        >>> epsilon = 1e-5\\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\\n        ...                                     epsilon=epsilon)\\n        >>> print(adam_step_delta)\\n        ({\\n            a: ivy.array([6.49e+04, 1.74e+01, 1.95e+01]),\\n            b: ivy.array([2.02, 4.82, 8.17])\\n        }, {\\n            a: ivy.array([0.87, 3.61, 8.09]),\\n            b: ivy.array([1.26, 4., 8.48])\\n        }, {\\n            a: ivy.array([0., 0.024, 0.096]),\\n            b: ivy.array([0.216, 0.384, 0.6])\\n        })\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                        b=ivy.array([3., 4., 5.]))\\n        >>> mw = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                    b=ivy.array([0., 0., 0.]))\\n        >>> vw = ivy.Container(a=ivy.array([0.,]),\\n        ...                    b=ivy.array([0.,]))\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.87\\n        >>> beta2 = 0.976\\n        >>> epsilon = 1e-5\\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\\n        ...                                     epsilon=epsilon)\\n        >>> print(adam_step_delta)\\n        ({\\n            a: ivy.array([0., 0.626, 0.626]),\\n            b: ivy.array([0.626, 0.626, 0.626])\\n        }, {\\n            a: ivy.array([0., 0.13, 0.26]),\\n            b: ivy.array([0.39, 0.52, 0.65])\\n        }, {\\n            a: ivy.array([0., 0.024, 0.096]),\\n            b: ivy.array([0.216, 0.384, 0.6])\\n        })\\n        '\n    return ivy.adam_step(self, mw, vw, step, beta1=beta1, beta2=beta2, epsilon=epsilon, out=out)",
            "def adam_step(self: ivy.Container, mw: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, float, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        ivy.Container instance method variant of ivy.adam_step. This method simply wraps\\n        the function, and so the docstring for ivy.adam_step also applies to this method\\n        with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        mw\\n            running average of the gradients.\\n        vw\\n            running average of second moments of the gradients.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The adam step delta.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` input:\\n\\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                         b=ivy.array([3., 4., 5.]))\\n        >>> mw = ivy.array([1., 4., 9.])\\n        >>> vw = ivy.array([0.,])\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.87\\n        >>> beta2 = 0.976\\n        >>> epsilon = 1e-5\\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\\n        ...                                     epsilon=epsilon)\\n        >>> print(adam_step_delta)\\n        ({\\n            a: ivy.array([6.49e+04, 1.74e+01, 1.95e+01]),\\n            b: ivy.array([2.02, 4.82, 8.17])\\n        }, {\\n            a: ivy.array([0.87, 3.61, 8.09]),\\n            b: ivy.array([1.26, 4., 8.48])\\n        }, {\\n            a: ivy.array([0., 0.024, 0.096]),\\n            b: ivy.array([0.216, 0.384, 0.6])\\n        })\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                        b=ivy.array([3., 4., 5.]))\\n        >>> mw = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                    b=ivy.array([0., 0., 0.]))\\n        >>> vw = ivy.Container(a=ivy.array([0.,]),\\n        ...                    b=ivy.array([0.,]))\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.87\\n        >>> beta2 = 0.976\\n        >>> epsilon = 1e-5\\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\\n        ...                                     epsilon=epsilon)\\n        >>> print(adam_step_delta)\\n        ({\\n            a: ivy.array([0., 0.626, 0.626]),\\n            b: ivy.array([0.626, 0.626, 0.626])\\n        }, {\\n            a: ivy.array([0., 0.13, 0.26]),\\n            b: ivy.array([0.39, 0.52, 0.65])\\n        }, {\\n            a: ivy.array([0., 0.024, 0.096]),\\n            b: ivy.array([0.216, 0.384, 0.6])\\n        })\\n        '\n    return ivy.adam_step(self, mw, vw, step, beta1=beta1, beta2=beta2, epsilon=epsilon, out=out)",
            "def adam_step(self: ivy.Container, mw: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, float, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        ivy.Container instance method variant of ivy.adam_step. This method simply wraps\\n        the function, and so the docstring for ivy.adam_step also applies to this method\\n        with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        mw\\n            running average of the gradients.\\n        vw\\n            running average of second moments of the gradients.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The adam step delta.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` input:\\n\\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                         b=ivy.array([3., 4., 5.]))\\n        >>> mw = ivy.array([1., 4., 9.])\\n        >>> vw = ivy.array([0.,])\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.87\\n        >>> beta2 = 0.976\\n        >>> epsilon = 1e-5\\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\\n        ...                                     epsilon=epsilon)\\n        >>> print(adam_step_delta)\\n        ({\\n            a: ivy.array([6.49e+04, 1.74e+01, 1.95e+01]),\\n            b: ivy.array([2.02, 4.82, 8.17])\\n        }, {\\n            a: ivy.array([0.87, 3.61, 8.09]),\\n            b: ivy.array([1.26, 4., 8.48])\\n        }, {\\n            a: ivy.array([0., 0.024, 0.096]),\\n            b: ivy.array([0.216, 0.384, 0.6])\\n        })\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                        b=ivy.array([3., 4., 5.]))\\n        >>> mw = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                    b=ivy.array([0., 0., 0.]))\\n        >>> vw = ivy.Container(a=ivy.array([0.,]),\\n        ...                    b=ivy.array([0.,]))\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.87\\n        >>> beta2 = 0.976\\n        >>> epsilon = 1e-5\\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\\n        ...                                     epsilon=epsilon)\\n        >>> print(adam_step_delta)\\n        ({\\n            a: ivy.array([0., 0.626, 0.626]),\\n            b: ivy.array([0.626, 0.626, 0.626])\\n        }, {\\n            a: ivy.array([0., 0.13, 0.26]),\\n            b: ivy.array([0.39, 0.52, 0.65])\\n        }, {\\n            a: ivy.array([0., 0.024, 0.096]),\\n            b: ivy.array([0.216, 0.384, 0.6])\\n        })\\n        '\n    return ivy.adam_step(self, mw, vw, step, beta1=beta1, beta2=beta2, epsilon=epsilon, out=out)",
            "def adam_step(self: ivy.Container, mw: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, float, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        ivy.Container instance method variant of ivy.adam_step. This method simply wraps\\n        the function, and so the docstring for ivy.adam_step also applies to this method\\n        with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        mw\\n            running average of the gradients.\\n        vw\\n            running average of second moments of the gradients.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The adam step delta.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` input:\\n\\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                         b=ivy.array([3., 4., 5.]))\\n        >>> mw = ivy.array([1., 4., 9.])\\n        >>> vw = ivy.array([0.,])\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.87\\n        >>> beta2 = 0.976\\n        >>> epsilon = 1e-5\\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\\n        ...                                     epsilon=epsilon)\\n        >>> print(adam_step_delta)\\n        ({\\n            a: ivy.array([6.49e+04, 1.74e+01, 1.95e+01]),\\n            b: ivy.array([2.02, 4.82, 8.17])\\n        }, {\\n            a: ivy.array([0.87, 3.61, 8.09]),\\n            b: ivy.array([1.26, 4., 8.48])\\n        }, {\\n            a: ivy.array([0., 0.024, 0.096]),\\n            b: ivy.array([0.216, 0.384, 0.6])\\n        })\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                        b=ivy.array([3., 4., 5.]))\\n        >>> mw = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                    b=ivy.array([0., 0., 0.]))\\n        >>> vw = ivy.Container(a=ivy.array([0.,]),\\n        ...                    b=ivy.array([0.,]))\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.87\\n        >>> beta2 = 0.976\\n        >>> epsilon = 1e-5\\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\\n        ...                                     epsilon=epsilon)\\n        >>> print(adam_step_delta)\\n        ({\\n            a: ivy.array([0., 0.626, 0.626]),\\n            b: ivy.array([0.626, 0.626, 0.626])\\n        }, {\\n            a: ivy.array([0., 0.13, 0.26]),\\n            b: ivy.array([0.39, 0.52, 0.65])\\n        }, {\\n            a: ivy.array([0., 0.024, 0.096]),\\n            b: ivy.array([0.216, 0.384, 0.6])\\n        })\\n        '\n    return ivy.adam_step(self, mw, vw, step, beta1=beta1, beta2=beta2, epsilon=epsilon, out=out)",
            "def adam_step(self: ivy.Container, mw: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, float, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        ivy.Container instance method variant of ivy.adam_step. This method simply wraps\\n        the function, and so the docstring for ivy.adam_step also applies to this method\\n        with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        mw\\n            running average of the gradients.\\n        vw\\n            running average of second moments of the gradients.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The adam step delta.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` input:\\n\\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                         b=ivy.array([3., 4., 5.]))\\n        >>> mw = ivy.array([1., 4., 9.])\\n        >>> vw = ivy.array([0.,])\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.87\\n        >>> beta2 = 0.976\\n        >>> epsilon = 1e-5\\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\\n        ...                                     epsilon=epsilon)\\n        >>> print(adam_step_delta)\\n        ({\\n            a: ivy.array([6.49e+04, 1.74e+01, 1.95e+01]),\\n            b: ivy.array([2.02, 4.82, 8.17])\\n        }, {\\n            a: ivy.array([0.87, 3.61, 8.09]),\\n            b: ivy.array([1.26, 4., 8.48])\\n        }, {\\n            a: ivy.array([0., 0.024, 0.096]),\\n            b: ivy.array([0.216, 0.384, 0.6])\\n        })\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> dcdw = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                        b=ivy.array([3., 4., 5.]))\\n        >>> mw = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                    b=ivy.array([0., 0., 0.]))\\n        >>> vw = ivy.Container(a=ivy.array([0.,]),\\n        ...                    b=ivy.array([0.,]))\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.87\\n        >>> beta2 = 0.976\\n        >>> epsilon = 1e-5\\n        >>> adam_step_delta = dcdw.adam_step(mw, vw, step, beta1=beta1, beta2=beta2,\\n        ...                                     epsilon=epsilon)\\n        >>> print(adam_step_delta)\\n        ({\\n            a: ivy.array([0., 0.626, 0.626]),\\n            b: ivy.array([0.626, 0.626, 0.626])\\n        }, {\\n            a: ivy.array([0., 0.13, 0.26]),\\n            b: ivy.array([0.39, 0.52, 0.65])\\n        }, {\\n            a: ivy.array([0., 0.024, 0.096]),\\n            b: ivy.array([0.216, 0.384, 0.6])\\n        })\\n        '\n    return ivy.adam_step(self, mw, vw, step, beta1=beta1, beta2=beta2, epsilon=epsilon, out=out)"
        ]
    },
    {
        "func_name": "optimizer_update",
        "original": "def optimizer_update(self: ivy.Container, effective_grad: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    \"\"\"\n        Update weights ws of some function, given the true or effective derivatives of\n        some cost c with respect to ws, [dc/dw for w in ws].\n\n        Parameters\n        ----------\n        self\n            Weights of the function to be updated.\n        effective_grad\n            Effective gradients of the cost c with respect to the weights ws,\n            [dc/dw for w in ws].\n        lr\n            Learning rate(s), the rate(s) at which the weights should be updated\n            relative to the gradient.\n        stop_gradients\n            Whether to stop the gradients of the variables after each gradient step.\n            Default is ``True``.\n        out\n            optional output container, for writing the result to. It must have a shape\n            that the inputs broadcast to.\n\n        Returns\n        -------\n        ret\n            The new function weights ws_new, following the optimizer updates.\n\n        Examples\n        --------\n        With one :class:`ivy.Container` input:\n\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\n        ...                    b=ivy.array([3., 4., 5.]))\n        >>> effective_grad = ivy.array([0., 0., 0.])\n        >>> lr = 3e-4\n        >>> ws_new = w.optimizer_update(effective_grad, lr)\n        >>> print(ws_new)\n        {\n            a: ivy.array([0., 1., 2.]),\n            b: ivy.array([3., 4., 5.])\n        }\n\n        With multiple :class:`ivy.Container` inputs:\n\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\n        ...                      b=ivy.array([3., 4., 5.]))\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\n        ...                                   b=ivy.array([0., 0., 0.]))\n        >>> lr = 3e-4\n        >>> ws_new = w.optimizer_update(effective_grad, lr, out=w)\n        >>> print(w)\n        {\n            a: ivy.array([0., 1., 2.]),\n            b: ivy.array([3., 4., 5.])\n        }\n\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\n        ...                    b=ivy.array([3., 4., 5.]))\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\n        ...                                b=ivy.array([0., 0., 0.]))\n        >>> lr = ivy.array([3e-4])\n        >>> ws_new = w.optimizer_update(effective_grad, lr, stop_gradients=False)\n        >>> print(ws_new)\n        {\n            a: ivy.array([0., 1., 2.]),\n            b: ivy.array([3., 4., 5.])\n        }\n        \"\"\"\n    return ivy.optimizer_update(self, effective_grad, lr, stop_gradients=stop_gradients, out=out)",
        "mutated": [
            "def optimizer_update(self: ivy.Container, effective_grad: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n    '\\n        Update weights ws of some function, given the true or effective derivatives of\\n        some cost c with respect to ws, [dc/dw for w in ws].\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        effective_grad\\n            Effective gradients of the cost c with respect to the weights ws,\\n            [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the optimizer updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` input:\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                    b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.array([0., 0., 0.])\\n        >>> lr = 3e-4\\n        >>> ws_new = w.optimizer_update(effective_grad, lr)\\n        >>> print(ws_new)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                                   b=ivy.array([0., 0., 0.]))\\n        >>> lr = 3e-4\\n        >>> ws_new = w.optimizer_update(effective_grad, lr, out=w)\\n        >>> print(w)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                    b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                                b=ivy.array([0., 0., 0.]))\\n        >>> lr = ivy.array([3e-4])\\n        >>> ws_new = w.optimizer_update(effective_grad, lr, stop_gradients=False)\\n        >>> print(ws_new)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return ivy.optimizer_update(self, effective_grad, lr, stop_gradients=stop_gradients, out=out)",
            "def optimizer_update(self: ivy.Container, effective_grad: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update weights ws of some function, given the true or effective derivatives of\\n        some cost c with respect to ws, [dc/dw for w in ws].\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        effective_grad\\n            Effective gradients of the cost c with respect to the weights ws,\\n            [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the optimizer updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` input:\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                    b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.array([0., 0., 0.])\\n        >>> lr = 3e-4\\n        >>> ws_new = w.optimizer_update(effective_grad, lr)\\n        >>> print(ws_new)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                                   b=ivy.array([0., 0., 0.]))\\n        >>> lr = 3e-4\\n        >>> ws_new = w.optimizer_update(effective_grad, lr, out=w)\\n        >>> print(w)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                    b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                                b=ivy.array([0., 0., 0.]))\\n        >>> lr = ivy.array([3e-4])\\n        >>> ws_new = w.optimizer_update(effective_grad, lr, stop_gradients=False)\\n        >>> print(ws_new)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return ivy.optimizer_update(self, effective_grad, lr, stop_gradients=stop_gradients, out=out)",
            "def optimizer_update(self: ivy.Container, effective_grad: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update weights ws of some function, given the true or effective derivatives of\\n        some cost c with respect to ws, [dc/dw for w in ws].\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        effective_grad\\n            Effective gradients of the cost c with respect to the weights ws,\\n            [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the optimizer updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` input:\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                    b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.array([0., 0., 0.])\\n        >>> lr = 3e-4\\n        >>> ws_new = w.optimizer_update(effective_grad, lr)\\n        >>> print(ws_new)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                                   b=ivy.array([0., 0., 0.]))\\n        >>> lr = 3e-4\\n        >>> ws_new = w.optimizer_update(effective_grad, lr, out=w)\\n        >>> print(w)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                    b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                                b=ivy.array([0., 0., 0.]))\\n        >>> lr = ivy.array([3e-4])\\n        >>> ws_new = w.optimizer_update(effective_grad, lr, stop_gradients=False)\\n        >>> print(ws_new)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return ivy.optimizer_update(self, effective_grad, lr, stop_gradients=stop_gradients, out=out)",
            "def optimizer_update(self: ivy.Container, effective_grad: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update weights ws of some function, given the true or effective derivatives of\\n        some cost c with respect to ws, [dc/dw for w in ws].\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        effective_grad\\n            Effective gradients of the cost c with respect to the weights ws,\\n            [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the optimizer updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` input:\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                    b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.array([0., 0., 0.])\\n        >>> lr = 3e-4\\n        >>> ws_new = w.optimizer_update(effective_grad, lr)\\n        >>> print(ws_new)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                                   b=ivy.array([0., 0., 0.]))\\n        >>> lr = 3e-4\\n        >>> ws_new = w.optimizer_update(effective_grad, lr, out=w)\\n        >>> print(w)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                    b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                                b=ivy.array([0., 0., 0.]))\\n        >>> lr = ivy.array([3e-4])\\n        >>> ws_new = w.optimizer_update(effective_grad, lr, stop_gradients=False)\\n        >>> print(ws_new)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return ivy.optimizer_update(self, effective_grad, lr, stop_gradients=stop_gradients, out=out)",
            "def optimizer_update(self: ivy.Container, effective_grad: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update weights ws of some function, given the true or effective derivatives of\\n        some cost c with respect to ws, [dc/dw for w in ws].\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        effective_grad\\n            Effective gradients of the cost c with respect to the weights ws,\\n            [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the optimizer updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` input:\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                    b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.array([0., 0., 0.])\\n        >>> lr = 3e-4\\n        >>> ws_new = w.optimizer_update(effective_grad, lr)\\n        >>> print(ws_new)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                      b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                                   b=ivy.array([0., 0., 0.]))\\n        >>> lr = 3e-4\\n        >>> ws_new = w.optimizer_update(effective_grad, lr, out=w)\\n        >>> print(w)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n\\n        >>> w = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                    b=ivy.array([3., 4., 5.]))\\n        >>> effective_grad = ivy.Container(a=ivy.array([0., 0., 0.]),\\n        ...                                b=ivy.array([0., 0., 0.]))\\n        >>> lr = ivy.array([3e-4])\\n        >>> ws_new = w.optimizer_update(effective_grad, lr, stop_gradients=False)\\n        >>> print(ws_new)\\n        {\\n            a: ivy.array([0., 1., 2.]),\\n            b: ivy.array([3., 4., 5.])\\n        }\\n        '\n    return ivy.optimizer_update(self, effective_grad, lr, stop_gradients=stop_gradients, out=out)"
        ]
    },
    {
        "func_name": "gradient_descent_update",
        "original": "def gradient_descent_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: ivy.Container=None) -> ivy.Container:\n    \"\"\"\n        ivy.Container instance method variant of ivy.gradient_descent_update. This\n        method simply wraps the function, and so the docstring for\n        ivy.gradient_descent_update also applies to this method with minimal changes.\n\n        Parameters\n        ----------\n        self\n            Weights of the function to be updated.\n        dcdw\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\n        lr\n            Learning rate(s), the rate(s) at which the weights should be\n            updated relative to the gradient.\n        key_chains\n            The key-chains to apply or not apply the method to. Default is ``None``.\n        to_apply\n            If True, the method will be applied to key_chains, otherwise key_chains\n            will be skipped. Default is ``True``.\n        prune_unapplied\n            Whether to prune key_chains for which the function was not applied.\n            Default is ``False``.\n        map_sequences\n            Whether to also map method to sequences (lists, tuples).\n            Default is ``False``.\n        stop_gradients\n            Whether to stop the gradients of the variables after each gradient step.\n            Default is ``True``.\n        out\n            optional output container, for writing the result to. It must have a shape\n            that the inputs broadcast to.\n\n        Returns\n        -------\n        ret\n            The new weights, following the gradient descent updates.\n\n        Examples\n        --------\n        With one :class:`ivy.Container` inputs:\n\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\n        >>> dcdw = ivy.array([0.5, 0.2, 0.1])\n        >>> lr = ivy.array(0.3)\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\n        >>> print(w_new)\n        {\n            a: ivy.array([0.85, 1.94, 2.97]),\n            b: ivy.array([3.33, 5.66, 1.95])\n        }\n\n        With multiple :class:`ivy.Container` inputs:\n\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\n        >>> dcdw = ivy.Container(a=ivy.array([0.5, 0.2, 0.1]),\n        ...                         b=ivy.array([2., 3.42, 1.69]))\n        >>> lr = ivy.array(0.3)\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\n        >>> print(w_new)\n        {\n            a: ivy.array([0.85, 1.94, 2.97]),\n            b: ivy.array([2.88, 4.69, 1.47])\n        }\n        \"\"\"\n    return ivy.gradient_descent_update(self, dcdw, lr, stop_gradients=stop_gradients, out=out)",
        "mutated": [
            "def gradient_descent_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: ivy.Container=None) -> ivy.Container:\n    if False:\n        i = 10\n    '\\n        ivy.Container instance method variant of ivy.gradient_descent_update. This\\n        method simply wraps the function, and so the docstring for\\n        ivy.gradient_descent_update also applies to this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be\\n            updated relative to the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new weights, following the gradient descent updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\\n        >>> dcdw = ivy.array([0.5, 0.2, 0.1])\\n        >>> lr = ivy.array(0.3)\\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\\n        >>> print(w_new)\\n        {\\n            a: ivy.array([0.85, 1.94, 2.97]),\\n            b: ivy.array([3.33, 5.66, 1.95])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.5, 0.2, 0.1]),\\n        ...                         b=ivy.array([2., 3.42, 1.69]))\\n        >>> lr = ivy.array(0.3)\\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\\n        >>> print(w_new)\\n        {\\n            a: ivy.array([0.85, 1.94, 2.97]),\\n            b: ivy.array([2.88, 4.69, 1.47])\\n        }\\n        '\n    return ivy.gradient_descent_update(self, dcdw, lr, stop_gradients=stop_gradients, out=out)",
            "def gradient_descent_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: ivy.Container=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        ivy.Container instance method variant of ivy.gradient_descent_update. This\\n        method simply wraps the function, and so the docstring for\\n        ivy.gradient_descent_update also applies to this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be\\n            updated relative to the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new weights, following the gradient descent updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\\n        >>> dcdw = ivy.array([0.5, 0.2, 0.1])\\n        >>> lr = ivy.array(0.3)\\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\\n        >>> print(w_new)\\n        {\\n            a: ivy.array([0.85, 1.94, 2.97]),\\n            b: ivy.array([3.33, 5.66, 1.95])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.5, 0.2, 0.1]),\\n        ...                         b=ivy.array([2., 3.42, 1.69]))\\n        >>> lr = ivy.array(0.3)\\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\\n        >>> print(w_new)\\n        {\\n            a: ivy.array([0.85, 1.94, 2.97]),\\n            b: ivy.array([2.88, 4.69, 1.47])\\n        }\\n        '\n    return ivy.gradient_descent_update(self, dcdw, lr, stop_gradients=stop_gradients, out=out)",
            "def gradient_descent_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: ivy.Container=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        ivy.Container instance method variant of ivy.gradient_descent_update. This\\n        method simply wraps the function, and so the docstring for\\n        ivy.gradient_descent_update also applies to this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be\\n            updated relative to the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new weights, following the gradient descent updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\\n        >>> dcdw = ivy.array([0.5, 0.2, 0.1])\\n        >>> lr = ivy.array(0.3)\\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\\n        >>> print(w_new)\\n        {\\n            a: ivy.array([0.85, 1.94, 2.97]),\\n            b: ivy.array([3.33, 5.66, 1.95])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.5, 0.2, 0.1]),\\n        ...                         b=ivy.array([2., 3.42, 1.69]))\\n        >>> lr = ivy.array(0.3)\\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\\n        >>> print(w_new)\\n        {\\n            a: ivy.array([0.85, 1.94, 2.97]),\\n            b: ivy.array([2.88, 4.69, 1.47])\\n        }\\n        '\n    return ivy.gradient_descent_update(self, dcdw, lr, stop_gradients=stop_gradients, out=out)",
            "def gradient_descent_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: ivy.Container=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        ivy.Container instance method variant of ivy.gradient_descent_update. This\\n        method simply wraps the function, and so the docstring for\\n        ivy.gradient_descent_update also applies to this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be\\n            updated relative to the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new weights, following the gradient descent updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\\n        >>> dcdw = ivy.array([0.5, 0.2, 0.1])\\n        >>> lr = ivy.array(0.3)\\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\\n        >>> print(w_new)\\n        {\\n            a: ivy.array([0.85, 1.94, 2.97]),\\n            b: ivy.array([3.33, 5.66, 1.95])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.5, 0.2, 0.1]),\\n        ...                         b=ivy.array([2., 3.42, 1.69]))\\n        >>> lr = ivy.array(0.3)\\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\\n        >>> print(w_new)\\n        {\\n            a: ivy.array([0.85, 1.94, 2.97]),\\n            b: ivy.array([2.88, 4.69, 1.47])\\n        }\\n        '\n    return ivy.gradient_descent_update(self, dcdw, lr, stop_gradients=stop_gradients, out=out)",
            "def gradient_descent_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, stop_gradients: Union[bool, ivy.Container]=True, out: ivy.Container=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        ivy.Container instance method variant of ivy.gradient_descent_update. This\\n        method simply wraps the function, and so the docstring for\\n        ivy.gradient_descent_update also applies to this method with minimal changes.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be\\n            updated relative to the gradient.\\n        key_chains\\n            The key-chains to apply or not apply the method to. Default is ``None``.\\n        to_apply\\n            If True, the method will be applied to key_chains, otherwise key_chains\\n            will be skipped. Default is ``True``.\\n        prune_unapplied\\n            Whether to prune key_chains for which the function was not applied.\\n            Default is ``False``.\\n        map_sequences\\n            Whether to also map method to sequences (lists, tuples).\\n            Default is ``False``.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new weights, following the gradient descent updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\\n        >>> dcdw = ivy.array([0.5, 0.2, 0.1])\\n        >>> lr = ivy.array(0.3)\\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\\n        >>> print(w_new)\\n        {\\n            a: ivy.array([0.85, 1.94, 2.97]),\\n            b: ivy.array([3.33, 5.66, 1.95])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]),\\n        ...                      b=ivy.array([3.48, 5.72, 1.98]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.5, 0.2, 0.1]),\\n        ...                         b=ivy.array([2., 3.42, 1.69]))\\n        >>> lr = ivy.array(0.3)\\n        >>> w_new = w.gradient_descent_update(dcdw, lr)\\n        >>> print(w_new)\\n        {\\n            a: ivy.array([0.85, 1.94, 2.97]),\\n            b: ivy.array([2.88, 4.69, 1.47])\\n        }\\n        '\n    return ivy.gradient_descent_update(self, dcdw, lr, stop_gradients=stop_gradients, out=out)"
        ]
    },
    {
        "func_name": "lars_update",
        "original": "def lars_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None):\n    \"\"\"\n        Update weights ws of some function, given the derivatives of some cost c with\n        respect to ws, [dc/dw for w in ws], by applying Layerwise Adaptive Rate Scaling\n        (LARS) method.\n\n        Parameters\n        ----------\n        self\n            Weights of the function to be updated.\n        dcdw\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\n        lr\n            Learning rate, the rate at which the weights should be updated relative to\n            the gradient.\n        decay_lambda\n            The factor used for weight decay. Default is zero.\n        stop_gradients\n            Whether to stop the gradients of the variables after each gradient step.\n            Default is ``True``.\n        out\n            optional output container, for writing the result to. It must have a shape\n            that the inputs broadcast to.\n\n        Returns\n        -------\n        ret\n            The new function weights ws_new, following the LARS updates.\n\n        Examples\n        --------\n        With one :class:`ivy.Container` inputs:\n\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\n        >>> dcdw = ivy.array([0.2, 0.4, 0.1])\n        >>> lr = ivy.array(0.1)\n        >>> new_weights = w.lars_update(dcdw, lr)\n        >>> print(new_weights)\n        {\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\n            b: ivy.array([1.1324538, 2.56490755, 4.96622658])\n        }\n\n        With multiple :class:`ivy.Container` inputs:\n\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\n        >>> dcdw = ivy.Container(a=ivy.array([0.2, 0.4, 0.1]),\n        ...                       b=ivy.array([0.3,0.1,0.2]))\n        >>> lr = ivy.array(0.1)\n        >>> new_weights = w.lars_update(dcdw, lr)\n        >>> print(new_weights)\n        {\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\n            b: ivy.array([0.90848625, 2.93616199, 4.77232409])\n        }\n        \"\"\"\n    return ivy.lars_update(self, dcdw, lr, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)",
        "mutated": [
            "def lars_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None):\n    if False:\n        i = 10\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, [dc/dw for w in ws], by applying Layerwise Adaptive Rate Scaling\\n        (LARS) method.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate, the rate at which the weights should be updated relative to\\n            the gradient.\\n        decay_lambda\\n            The factor used for weight decay. Default is zero.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the LARS updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\\n        >>> dcdw = ivy.array([0.2, 0.4, 0.1])\\n        >>> lr = ivy.array(0.1)\\n        >>> new_weights = w.lars_update(dcdw, lr)\\n        >>> print(new_weights)\\n        {\\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\\n            b: ivy.array([1.1324538, 2.56490755, 4.96622658])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.2, 0.4, 0.1]),\\n        ...                       b=ivy.array([0.3,0.1,0.2]))\\n        >>> lr = ivy.array(0.1)\\n        >>> new_weights = w.lars_update(dcdw, lr)\\n        >>> print(new_weights)\\n        {\\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\\n            b: ivy.array([0.90848625, 2.93616199, 4.77232409])\\n        }\\n        '\n    return ivy.lars_update(self, dcdw, lr, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)",
            "def lars_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, [dc/dw for w in ws], by applying Layerwise Adaptive Rate Scaling\\n        (LARS) method.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate, the rate at which the weights should be updated relative to\\n            the gradient.\\n        decay_lambda\\n            The factor used for weight decay. Default is zero.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the LARS updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\\n        >>> dcdw = ivy.array([0.2, 0.4, 0.1])\\n        >>> lr = ivy.array(0.1)\\n        >>> new_weights = w.lars_update(dcdw, lr)\\n        >>> print(new_weights)\\n        {\\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\\n            b: ivy.array([1.1324538, 2.56490755, 4.96622658])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.2, 0.4, 0.1]),\\n        ...                       b=ivy.array([0.3,0.1,0.2]))\\n        >>> lr = ivy.array(0.1)\\n        >>> new_weights = w.lars_update(dcdw, lr)\\n        >>> print(new_weights)\\n        {\\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\\n            b: ivy.array([0.90848625, 2.93616199, 4.77232409])\\n        }\\n        '\n    return ivy.lars_update(self, dcdw, lr, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)",
            "def lars_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, [dc/dw for w in ws], by applying Layerwise Adaptive Rate Scaling\\n        (LARS) method.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate, the rate at which the weights should be updated relative to\\n            the gradient.\\n        decay_lambda\\n            The factor used for weight decay. Default is zero.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the LARS updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\\n        >>> dcdw = ivy.array([0.2, 0.4, 0.1])\\n        >>> lr = ivy.array(0.1)\\n        >>> new_weights = w.lars_update(dcdw, lr)\\n        >>> print(new_weights)\\n        {\\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\\n            b: ivy.array([1.1324538, 2.56490755, 4.96622658])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.2, 0.4, 0.1]),\\n        ...                       b=ivy.array([0.3,0.1,0.2]))\\n        >>> lr = ivy.array(0.1)\\n        >>> new_weights = w.lars_update(dcdw, lr)\\n        >>> print(new_weights)\\n        {\\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\\n            b: ivy.array([0.90848625, 2.93616199, 4.77232409])\\n        }\\n        '\n    return ivy.lars_update(self, dcdw, lr, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)",
            "def lars_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, [dc/dw for w in ws], by applying Layerwise Adaptive Rate Scaling\\n        (LARS) method.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate, the rate at which the weights should be updated relative to\\n            the gradient.\\n        decay_lambda\\n            The factor used for weight decay. Default is zero.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the LARS updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\\n        >>> dcdw = ivy.array([0.2, 0.4, 0.1])\\n        >>> lr = ivy.array(0.1)\\n        >>> new_weights = w.lars_update(dcdw, lr)\\n        >>> print(new_weights)\\n        {\\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\\n            b: ivy.array([1.1324538, 2.56490755, 4.96622658])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.2, 0.4, 0.1]),\\n        ...                       b=ivy.array([0.3,0.1,0.2]))\\n        >>> lr = ivy.array(0.1)\\n        >>> new_weights = w.lars_update(dcdw, lr)\\n        >>> print(new_weights)\\n        {\\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\\n            b: ivy.array([0.90848625, 2.93616199, 4.77232409])\\n        }\\n        '\n    return ivy.lars_update(self, dcdw, lr, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)",
            "def lars_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], /, *, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, [dc/dw for w in ws], by applying Layerwise Adaptive Rate Scaling\\n        (LARS) method.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate, the rate at which the weights should be updated relative to\\n            the gradient.\\n        decay_lambda\\n            The factor used for weight decay. Default is zero.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the LARS updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\\n        >>> dcdw = ivy.array([0.2, 0.4, 0.1])\\n        >>> lr = ivy.array(0.1)\\n        >>> new_weights = w.lars_update(dcdw, lr)\\n        >>> print(new_weights)\\n        {\\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\\n            b: ivy.array([1.1324538, 2.56490755, 4.96622658])\\n        }\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([3.2, 2.6, 1.3]),\\n        ...                    b=ivy.array([1.4, 3.1, 5.1]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.2, 0.4, 0.1]),\\n        ...                       b=ivy.array([0.3,0.1,0.2]))\\n        >>> lr = ivy.array(0.1)\\n        >>> new_weights = w.lars_update(dcdw, lr)\\n        >>> print(new_weights)\\n        {\\n            a: ivy.array([3.01132035, 2.22264051, 1.2056601]),\\n            b: ivy.array([0.90848625, 2.93616199, 4.77232409])\\n        }\\n        '\n    return ivy.lars_update(self, dcdw, lr, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)"
        ]
    },
    {
        "func_name": "adam_update",
        "original": "def adam_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    \"\"\"\n        Update weights ws of some function, given the derivatives of some cost c with\n        respect to ws, using ADAM update. `[reference]\n\n        <https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam>`_\n\n        Parameters\n        ----------\n        self\n            Weights of the function to be updated.\n        dcdw\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\n        lr\n            Learning rate(s), the rate(s) at which the weights should be updated\n            relative to the gradient.\n        mw_tm1\n            running average of the gradients, from the previous time-step.\n        vw_tm1\n            running average of second moments of the gradients, from the previous\n            time-step.\n        step\n            training step.\n        beta1\n            gradient forgetting factor (Default value = 0.9).\n        beta2\n            second moment of gradient forgetting factor (Default value = 0.999).\n        epsilon\n            divisor during adam update, preventing division by zero\n            (Default value = 1e-7).\n        stop_gradients\n            Whether to stop the gradients of the variables after each gradient step.\n            Default is ``True``.\n        out\n            optional output container, for writing the result to. It must have a shape\n            that the inputs broadcast to.\n\n        Returns\n        -------\n        ret\n            The new function weights ws_new, and also new mw and vw, following the adam\n            updates.\n\n        Examples\n        --------\n        With one :class:`ivy.Container` inputs:\n\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\n        >>> dcdw = ivy.array([1., 0.2, 0.4])\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\n        >>> vw_tm1 = ivy.array([0.])\n        >>> lr = ivy.array(0.01)\n        >>> step = 2\n        >>> updated_weights = w.adam_update(dcdw, mw_tm1, vw_tm1, lr, step)\n        >>> print(updated_weights)\n        ({\n            a: ivy.array([1., 2., 3.]),\n            b: ivy.array([4., 5., 6.])\n        }, ivy.array([0.1 , 0.02, 0.04]), ivy.array([0.01099, 0.01003, 0.01015]))\n\n        With multiple :class:`ivy.Container` inputs:\n\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\n        ...                   b=ivy.array([3., 4., 5.]))\n        >>> dcdw = ivy.Container(a=ivy.array([0.1,0.3,0.3]),\n        ...                      b=ivy.array([0.3,0.2,0.2]))\n        >>> lr = ivy.array(0.001)\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\n        ...                        b=ivy.array([0.,0.,0.]))\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\n        ...                        b=ivy.array([0.,]))\n        >>> step = 3\n        >>> beta1 = 0.9\n        >>> beta2 = 0.999\n        >>> epsilon = 1e-7\n        >>> stop_gradients = False\n        >>> updated_weights = w.adam_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\n        ...                                beta2=beta2, epsilon=epsilon,\n        ...                                stop_gradients=stop_gradients)\n        >>> print(updated_weights)\n        ({\n            a: ivy.array([0.99936122, 1.99936116, 2.99936128]),\n            b: ivy.array([3.99936128, 4.99936104, 5.99936104])\n        }, {\n            a: ivy.array([0.01, 0.03, 0.03]),\n            b: ivy.array([0.03, 0.02, 0.02])\n        }, {\n            a: ivy.array([1.00000016e-05, 9.00000086e-05, 9.00000086e-05]),\n            b: ivy.array([9.00000086e-05, 4.00000063e-05, 4.00000063e-05])\n        })\n        \"\"\"\n    return ivy.adam_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, stop_gradients=stop_gradients, out=out)",
        "mutated": [
            "def adam_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, using ADAM update. `[reference]\\n\\n        <https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam>`_\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        mw_tm1\\n            running average of the gradients, from the previous time-step.\\n        vw_tm1\\n            running average of second moments of the gradients, from the previous\\n            time-step.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, and also new mw and vw, following the adam\\n            updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\\n        >>> dcdw = ivy.array([1., 0.2, 0.4])\\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\\n        >>> vw_tm1 = ivy.array([0.])\\n        >>> lr = ivy.array(0.01)\\n        >>> step = 2\\n        >>> updated_weights = w.adam_update(dcdw, mw_tm1, vw_tm1, lr, step)\\n        >>> print(updated_weights)\\n        ({\\n            a: ivy.array([1., 2., 3.]),\\n            b: ivy.array([4., 5., 6.])\\n        }, ivy.array([0.1 , 0.02, 0.04]), ivy.array([0.01099, 0.01003, 0.01015]))\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                   b=ivy.array([3., 4., 5.]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.1,0.3,0.3]),\\n        ...                      b=ivy.array([0.3,0.2,0.2]))\\n        >>> lr = ivy.array(0.001)\\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\\n        ...                        b=ivy.array([0.,0.,0.]))\\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\\n        ...                        b=ivy.array([0.,]))\\n        >>> step = 3\\n        >>> beta1 = 0.9\\n        >>> beta2 = 0.999\\n        >>> epsilon = 1e-7\\n        >>> stop_gradients = False\\n        >>> updated_weights = w.adam_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\\n        ...                                beta2=beta2, epsilon=epsilon,\\n        ...                                stop_gradients=stop_gradients)\\n        >>> print(updated_weights)\\n        ({\\n            a: ivy.array([0.99936122, 1.99936116, 2.99936128]),\\n            b: ivy.array([3.99936128, 4.99936104, 5.99936104])\\n        }, {\\n            a: ivy.array([0.01, 0.03, 0.03]),\\n            b: ivy.array([0.03, 0.02, 0.02])\\n        }, {\\n            a: ivy.array([1.00000016e-05, 9.00000086e-05, 9.00000086e-05]),\\n            b: ivy.array([9.00000086e-05, 4.00000063e-05, 4.00000063e-05])\\n        })\\n        '\n    return ivy.adam_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, stop_gradients=stop_gradients, out=out)",
            "def adam_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, using ADAM update. `[reference]\\n\\n        <https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam>`_\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        mw_tm1\\n            running average of the gradients, from the previous time-step.\\n        vw_tm1\\n            running average of second moments of the gradients, from the previous\\n            time-step.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, and also new mw and vw, following the adam\\n            updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\\n        >>> dcdw = ivy.array([1., 0.2, 0.4])\\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\\n        >>> vw_tm1 = ivy.array([0.])\\n        >>> lr = ivy.array(0.01)\\n        >>> step = 2\\n        >>> updated_weights = w.adam_update(dcdw, mw_tm1, vw_tm1, lr, step)\\n        >>> print(updated_weights)\\n        ({\\n            a: ivy.array([1., 2., 3.]),\\n            b: ivy.array([4., 5., 6.])\\n        }, ivy.array([0.1 , 0.02, 0.04]), ivy.array([0.01099, 0.01003, 0.01015]))\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                   b=ivy.array([3., 4., 5.]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.1,0.3,0.3]),\\n        ...                      b=ivy.array([0.3,0.2,0.2]))\\n        >>> lr = ivy.array(0.001)\\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\\n        ...                        b=ivy.array([0.,0.,0.]))\\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\\n        ...                        b=ivy.array([0.,]))\\n        >>> step = 3\\n        >>> beta1 = 0.9\\n        >>> beta2 = 0.999\\n        >>> epsilon = 1e-7\\n        >>> stop_gradients = False\\n        >>> updated_weights = w.adam_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\\n        ...                                beta2=beta2, epsilon=epsilon,\\n        ...                                stop_gradients=stop_gradients)\\n        >>> print(updated_weights)\\n        ({\\n            a: ivy.array([0.99936122, 1.99936116, 2.99936128]),\\n            b: ivy.array([3.99936128, 4.99936104, 5.99936104])\\n        }, {\\n            a: ivy.array([0.01, 0.03, 0.03]),\\n            b: ivy.array([0.03, 0.02, 0.02])\\n        }, {\\n            a: ivy.array([1.00000016e-05, 9.00000086e-05, 9.00000086e-05]),\\n            b: ivy.array([9.00000086e-05, 4.00000063e-05, 4.00000063e-05])\\n        })\\n        '\n    return ivy.adam_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, stop_gradients=stop_gradients, out=out)",
            "def adam_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, using ADAM update. `[reference]\\n\\n        <https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam>`_\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        mw_tm1\\n            running average of the gradients, from the previous time-step.\\n        vw_tm1\\n            running average of second moments of the gradients, from the previous\\n            time-step.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, and also new mw and vw, following the adam\\n            updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\\n        >>> dcdw = ivy.array([1., 0.2, 0.4])\\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\\n        >>> vw_tm1 = ivy.array([0.])\\n        >>> lr = ivy.array(0.01)\\n        >>> step = 2\\n        >>> updated_weights = w.adam_update(dcdw, mw_tm1, vw_tm1, lr, step)\\n        >>> print(updated_weights)\\n        ({\\n            a: ivy.array([1., 2., 3.]),\\n            b: ivy.array([4., 5., 6.])\\n        }, ivy.array([0.1 , 0.02, 0.04]), ivy.array([0.01099, 0.01003, 0.01015]))\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                   b=ivy.array([3., 4., 5.]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.1,0.3,0.3]),\\n        ...                      b=ivy.array([0.3,0.2,0.2]))\\n        >>> lr = ivy.array(0.001)\\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\\n        ...                        b=ivy.array([0.,0.,0.]))\\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\\n        ...                        b=ivy.array([0.,]))\\n        >>> step = 3\\n        >>> beta1 = 0.9\\n        >>> beta2 = 0.999\\n        >>> epsilon = 1e-7\\n        >>> stop_gradients = False\\n        >>> updated_weights = w.adam_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\\n        ...                                beta2=beta2, epsilon=epsilon,\\n        ...                                stop_gradients=stop_gradients)\\n        >>> print(updated_weights)\\n        ({\\n            a: ivy.array([0.99936122, 1.99936116, 2.99936128]),\\n            b: ivy.array([3.99936128, 4.99936104, 5.99936104])\\n        }, {\\n            a: ivy.array([0.01, 0.03, 0.03]),\\n            b: ivy.array([0.03, 0.02, 0.02])\\n        }, {\\n            a: ivy.array([1.00000016e-05, 9.00000086e-05, 9.00000086e-05]),\\n            b: ivy.array([9.00000086e-05, 4.00000063e-05, 4.00000063e-05])\\n        })\\n        '\n    return ivy.adam_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, stop_gradients=stop_gradients, out=out)",
            "def adam_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, using ADAM update. `[reference]\\n\\n        <https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam>`_\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        mw_tm1\\n            running average of the gradients, from the previous time-step.\\n        vw_tm1\\n            running average of second moments of the gradients, from the previous\\n            time-step.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, and also new mw and vw, following the adam\\n            updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\\n        >>> dcdw = ivy.array([1., 0.2, 0.4])\\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\\n        >>> vw_tm1 = ivy.array([0.])\\n        >>> lr = ivy.array(0.01)\\n        >>> step = 2\\n        >>> updated_weights = w.adam_update(dcdw, mw_tm1, vw_tm1, lr, step)\\n        >>> print(updated_weights)\\n        ({\\n            a: ivy.array([1., 2., 3.]),\\n            b: ivy.array([4., 5., 6.])\\n        }, ivy.array([0.1 , 0.02, 0.04]), ivy.array([0.01099, 0.01003, 0.01015]))\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                   b=ivy.array([3., 4., 5.]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.1,0.3,0.3]),\\n        ...                      b=ivy.array([0.3,0.2,0.2]))\\n        >>> lr = ivy.array(0.001)\\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\\n        ...                        b=ivy.array([0.,0.,0.]))\\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\\n        ...                        b=ivy.array([0.,]))\\n        >>> step = 3\\n        >>> beta1 = 0.9\\n        >>> beta2 = 0.999\\n        >>> epsilon = 1e-7\\n        >>> stop_gradients = False\\n        >>> updated_weights = w.adam_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\\n        ...                                beta2=beta2, epsilon=epsilon,\\n        ...                                stop_gradients=stop_gradients)\\n        >>> print(updated_weights)\\n        ({\\n            a: ivy.array([0.99936122, 1.99936116, 2.99936128]),\\n            b: ivy.array([3.99936128, 4.99936104, 5.99936104])\\n        }, {\\n            a: ivy.array([0.01, 0.03, 0.03]),\\n            b: ivy.array([0.03, 0.02, 0.02])\\n        }, {\\n            a: ivy.array([1.00000016e-05, 9.00000086e-05, 9.00000086e-05]),\\n            b: ivy.array([9.00000086e-05, 4.00000063e-05, 4.00000063e-05])\\n        })\\n        '\n    return ivy.adam_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, stop_gradients=stop_gradients, out=out)",
            "def adam_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, using ADAM update. `[reference]\\n\\n        <https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam>`_\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        mw_tm1\\n            running average of the gradients, from the previous time-step.\\n        vw_tm1\\n            running average of second moments of the gradients, from the previous\\n            time-step.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output container, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, and also new mw and vw, following the adam\\n            updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\\n        >>> dcdw = ivy.array([1., 0.2, 0.4])\\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\\n        >>> vw_tm1 = ivy.array([0.])\\n        >>> lr = ivy.array(0.01)\\n        >>> step = 2\\n        >>> updated_weights = w.adam_update(dcdw, mw_tm1, vw_tm1, lr, step)\\n        >>> print(updated_weights)\\n        ({\\n            a: ivy.array([1., 2., 3.]),\\n            b: ivy.array([4., 5., 6.])\\n        }, ivy.array([0.1 , 0.02, 0.04]), ivy.array([0.01099, 0.01003, 0.01015]))\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> x = ivy.Container(a=ivy.array([0., 1., 2.]),\\n        ...                   b=ivy.array([3., 4., 5.]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.1,0.3,0.3]),\\n        ...                      b=ivy.array([0.3,0.2,0.2]))\\n        >>> lr = ivy.array(0.001)\\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\\n        ...                        b=ivy.array([0.,0.,0.]))\\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\\n        ...                        b=ivy.array([0.,]))\\n        >>> step = 3\\n        >>> beta1 = 0.9\\n        >>> beta2 = 0.999\\n        >>> epsilon = 1e-7\\n        >>> stop_gradients = False\\n        >>> updated_weights = w.adam_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\\n        ...                                beta2=beta2, epsilon=epsilon,\\n        ...                                stop_gradients=stop_gradients)\\n        >>> print(updated_weights)\\n        ({\\n            a: ivy.array([0.99936122, 1.99936116, 2.99936128]),\\n            b: ivy.array([3.99936128, 4.99936104, 5.99936104])\\n        }, {\\n            a: ivy.array([0.01, 0.03, 0.03]),\\n            b: ivy.array([0.03, 0.02, 0.02])\\n        }, {\\n            a: ivy.array([1.00000016e-05, 9.00000086e-05, 9.00000086e-05]),\\n            b: ivy.array([9.00000086e-05, 4.00000063e-05, 4.00000063e-05])\\n        })\\n        '\n    return ivy.adam_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, stop_gradients=stop_gradients, out=out)"
        ]
    },
    {
        "func_name": "lamb_update",
        "original": "def lamb_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, max_trust_ratio: Union[int, float, ivy.Container]=10, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    \"\"\"\n        Update weights ws of some function, given the derivatives of some cost c with\n        respect to ws, [dc/dw for w in ws], by applying LAMB method.\n\n        Parameters\n        ----------\n        self\n            Weights of the function to be updated.\n        dcdw\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\n        lr\n            Learning rate(s), the rate(s) at which the weights should be updated\n            relative to the gradient.\n        mw_tm1\n            running average of the gradients, from the previous time-step.\n        vw_tm1\n            running average of second moments of the gradients, from the previous\n            time-step.\n        step\n            training step.\n        beta1\n            gradient forgetting factor (Default value = 0.9).\n        beta2\n            second moment of gradient forgetting factor (Default value = 0.999).\n        epsilon\n            divisor during adam update, preventing division by zero\n            (Default value = 1e-7).\n        max_trust_ratio\n            The maximum value for the trust ratio. Default is 10.\n        decay_lambda\n            The factor used for weight decay. Default is zero.\n        stop_gradients\n            Whether to stop the gradients of the variables after each gradient step.\n            Default is ``True``.\n        out\n            optional output array, for writing the result to. It must have a shape\n            that the inputs broadcast to.\n\n        Returns\n        -------\n        ret\n            The new function weights ws_new, following the LAMB updates.\n\n        Examples\n        --------\n        With one :class:`ivy.Container` inputs:\n\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\n        >>> dcdw = ivy.array([3., 4., 5.])\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\n        >>> vw_tm1 = ivy.array([0.])\n        >>> lr = ivy.array(1.)\n        >>> step = ivy.array([2])\n        >>> new_weights = w.lamb_update(dcdw, mw_tm1, vw_tm1, lr, step)\n        >>> print(new_weights)\n        ({\n            a: ivy.array([1., 2., 3.]),\n            b: ivy.array([4., 5., 6.])\n        }, ivy.array([0.3, 0.4, 0.5]), ivy.array([1.01, 1.01, 1.02]))\n\n        With multiple :class:`ivy.Container` inputs:\n\n        >>> w = ivy.Container(a=ivy.array([1.,3.,5.]),\n        ...                      b=ivy.array([3.,4.,2.]))\n        >>> dcdw = ivy.Container(a=ivy.array([0.2,0.3,0.6]),\n        ...                         b=ivy.array([0.6,0.4,0.7]))\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\n        ...                           b=ivy.array([0.,0.,0.]))\n\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\n        ...                           b=ivy.array([0.,]))\n        >>> step = ivy.array([3.4])\n        >>> beta1 = 0.9\n        >>> beta2 = 0.999\n        >>> epsilon = 1e-7\n        >>> max_trust_ratio = 10\n        >>> decay_lambda = 0\n        >>> stop_gradients = True\n        >>> lr = ivy.array(0.5)\n        >>> new_weights = w.lamb_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\n        ...                                beta2=beta2, epsilon=epsilon,\n        ...                                max_trust_ratio=max_trust_ratio,\n        ...                                decay_lambda=decay_lambda,\n        ...                                stop_gradients=stop_gradients)\n        >>> print(new_weights)\n        ({\n            a: ivy.array([-0.708, 1.29, 3.29]),\n            b: ivy.array([1.45, 2.45, 0.445])\n        }, {\n            a: ivy.array([0.02, 0.03, 0.06]),\n            b: ivy.array([0.06, 0.04, 0.07])\n        }, {\n            a: ivy.array([4.0e-05, 9.0e-05, 3.6e-04]),\n            b: ivy.array([0.00036, 0.00016, 0.00049])\n        })\n        \"\"\"\n    return ivy.lamb_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, max_trust_ratio=max_trust_ratio, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)",
        "mutated": [
            "def lamb_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, max_trust_ratio: Union[int, float, ivy.Container]=10, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, [dc/dw for w in ws], by applying LAMB method.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        mw_tm1\\n            running average of the gradients, from the previous time-step.\\n        vw_tm1\\n            running average of second moments of the gradients, from the previous\\n            time-step.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        max_trust_ratio\\n            The maximum value for the trust ratio. Default is 10.\\n        decay_lambda\\n            The factor used for weight decay. Default is zero.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the LAMB updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\\n        >>> dcdw = ivy.array([3., 4., 5.])\\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\\n        >>> vw_tm1 = ivy.array([0.])\\n        >>> lr = ivy.array(1.)\\n        >>> step = ivy.array([2])\\n        >>> new_weights = w.lamb_update(dcdw, mw_tm1, vw_tm1, lr, step)\\n        >>> print(new_weights)\\n        ({\\n            a: ivy.array([1., 2., 3.]),\\n            b: ivy.array([4., 5., 6.])\\n        }, ivy.array([0.3, 0.4, 0.5]), ivy.array([1.01, 1.01, 1.02]))\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1.,3.,5.]),\\n        ...                      b=ivy.array([3.,4.,2.]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.2,0.3,0.6]),\\n        ...                         b=ivy.array([0.6,0.4,0.7]))\\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\\n        ...                           b=ivy.array([0.,0.,0.]))\\n\\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\\n        ...                           b=ivy.array([0.,]))\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.9\\n        >>> beta2 = 0.999\\n        >>> epsilon = 1e-7\\n        >>> max_trust_ratio = 10\\n        >>> decay_lambda = 0\\n        >>> stop_gradients = True\\n        >>> lr = ivy.array(0.5)\\n        >>> new_weights = w.lamb_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\\n        ...                                beta2=beta2, epsilon=epsilon,\\n        ...                                max_trust_ratio=max_trust_ratio,\\n        ...                                decay_lambda=decay_lambda,\\n        ...                                stop_gradients=stop_gradients)\\n        >>> print(new_weights)\\n        ({\\n            a: ivy.array([-0.708, 1.29, 3.29]),\\n            b: ivy.array([1.45, 2.45, 0.445])\\n        }, {\\n            a: ivy.array([0.02, 0.03, 0.06]),\\n            b: ivy.array([0.06, 0.04, 0.07])\\n        }, {\\n            a: ivy.array([4.0e-05, 9.0e-05, 3.6e-04]),\\n            b: ivy.array([0.00036, 0.00016, 0.00049])\\n        })\\n        '\n    return ivy.lamb_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, max_trust_ratio=max_trust_ratio, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)",
            "def lamb_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, max_trust_ratio: Union[int, float, ivy.Container]=10, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, [dc/dw for w in ws], by applying LAMB method.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        mw_tm1\\n            running average of the gradients, from the previous time-step.\\n        vw_tm1\\n            running average of second moments of the gradients, from the previous\\n            time-step.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        max_trust_ratio\\n            The maximum value for the trust ratio. Default is 10.\\n        decay_lambda\\n            The factor used for weight decay. Default is zero.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the LAMB updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\\n        >>> dcdw = ivy.array([3., 4., 5.])\\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\\n        >>> vw_tm1 = ivy.array([0.])\\n        >>> lr = ivy.array(1.)\\n        >>> step = ivy.array([2])\\n        >>> new_weights = w.lamb_update(dcdw, mw_tm1, vw_tm1, lr, step)\\n        >>> print(new_weights)\\n        ({\\n            a: ivy.array([1., 2., 3.]),\\n            b: ivy.array([4., 5., 6.])\\n        }, ivy.array([0.3, 0.4, 0.5]), ivy.array([1.01, 1.01, 1.02]))\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1.,3.,5.]),\\n        ...                      b=ivy.array([3.,4.,2.]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.2,0.3,0.6]),\\n        ...                         b=ivy.array([0.6,0.4,0.7]))\\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\\n        ...                           b=ivy.array([0.,0.,0.]))\\n\\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\\n        ...                           b=ivy.array([0.,]))\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.9\\n        >>> beta2 = 0.999\\n        >>> epsilon = 1e-7\\n        >>> max_trust_ratio = 10\\n        >>> decay_lambda = 0\\n        >>> stop_gradients = True\\n        >>> lr = ivy.array(0.5)\\n        >>> new_weights = w.lamb_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\\n        ...                                beta2=beta2, epsilon=epsilon,\\n        ...                                max_trust_ratio=max_trust_ratio,\\n        ...                                decay_lambda=decay_lambda,\\n        ...                                stop_gradients=stop_gradients)\\n        >>> print(new_weights)\\n        ({\\n            a: ivy.array([-0.708, 1.29, 3.29]),\\n            b: ivy.array([1.45, 2.45, 0.445])\\n        }, {\\n            a: ivy.array([0.02, 0.03, 0.06]),\\n            b: ivy.array([0.06, 0.04, 0.07])\\n        }, {\\n            a: ivy.array([4.0e-05, 9.0e-05, 3.6e-04]),\\n            b: ivy.array([0.00036, 0.00016, 0.00049])\\n        })\\n        '\n    return ivy.lamb_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, max_trust_ratio=max_trust_ratio, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)",
            "def lamb_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, max_trust_ratio: Union[int, float, ivy.Container]=10, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, [dc/dw for w in ws], by applying LAMB method.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        mw_tm1\\n            running average of the gradients, from the previous time-step.\\n        vw_tm1\\n            running average of second moments of the gradients, from the previous\\n            time-step.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        max_trust_ratio\\n            The maximum value for the trust ratio. Default is 10.\\n        decay_lambda\\n            The factor used for weight decay. Default is zero.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the LAMB updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\\n        >>> dcdw = ivy.array([3., 4., 5.])\\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\\n        >>> vw_tm1 = ivy.array([0.])\\n        >>> lr = ivy.array(1.)\\n        >>> step = ivy.array([2])\\n        >>> new_weights = w.lamb_update(dcdw, mw_tm1, vw_tm1, lr, step)\\n        >>> print(new_weights)\\n        ({\\n            a: ivy.array([1., 2., 3.]),\\n            b: ivy.array([4., 5., 6.])\\n        }, ivy.array([0.3, 0.4, 0.5]), ivy.array([1.01, 1.01, 1.02]))\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1.,3.,5.]),\\n        ...                      b=ivy.array([3.,4.,2.]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.2,0.3,0.6]),\\n        ...                         b=ivy.array([0.6,0.4,0.7]))\\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\\n        ...                           b=ivy.array([0.,0.,0.]))\\n\\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\\n        ...                           b=ivy.array([0.,]))\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.9\\n        >>> beta2 = 0.999\\n        >>> epsilon = 1e-7\\n        >>> max_trust_ratio = 10\\n        >>> decay_lambda = 0\\n        >>> stop_gradients = True\\n        >>> lr = ivy.array(0.5)\\n        >>> new_weights = w.lamb_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\\n        ...                                beta2=beta2, epsilon=epsilon,\\n        ...                                max_trust_ratio=max_trust_ratio,\\n        ...                                decay_lambda=decay_lambda,\\n        ...                                stop_gradients=stop_gradients)\\n        >>> print(new_weights)\\n        ({\\n            a: ivy.array([-0.708, 1.29, 3.29]),\\n            b: ivy.array([1.45, 2.45, 0.445])\\n        }, {\\n            a: ivy.array([0.02, 0.03, 0.06]),\\n            b: ivy.array([0.06, 0.04, 0.07])\\n        }, {\\n            a: ivy.array([4.0e-05, 9.0e-05, 3.6e-04]),\\n            b: ivy.array([0.00036, 0.00016, 0.00049])\\n        })\\n        '\n    return ivy.lamb_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, max_trust_ratio=max_trust_ratio, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)",
            "def lamb_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, max_trust_ratio: Union[int, float, ivy.Container]=10, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, [dc/dw for w in ws], by applying LAMB method.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        mw_tm1\\n            running average of the gradients, from the previous time-step.\\n        vw_tm1\\n            running average of second moments of the gradients, from the previous\\n            time-step.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        max_trust_ratio\\n            The maximum value for the trust ratio. Default is 10.\\n        decay_lambda\\n            The factor used for weight decay. Default is zero.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the LAMB updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\\n        >>> dcdw = ivy.array([3., 4., 5.])\\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\\n        >>> vw_tm1 = ivy.array([0.])\\n        >>> lr = ivy.array(1.)\\n        >>> step = ivy.array([2])\\n        >>> new_weights = w.lamb_update(dcdw, mw_tm1, vw_tm1, lr, step)\\n        >>> print(new_weights)\\n        ({\\n            a: ivy.array([1., 2., 3.]),\\n            b: ivy.array([4., 5., 6.])\\n        }, ivy.array([0.3, 0.4, 0.5]), ivy.array([1.01, 1.01, 1.02]))\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1.,3.,5.]),\\n        ...                      b=ivy.array([3.,4.,2.]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.2,0.3,0.6]),\\n        ...                         b=ivy.array([0.6,0.4,0.7]))\\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\\n        ...                           b=ivy.array([0.,0.,0.]))\\n\\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\\n        ...                           b=ivy.array([0.,]))\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.9\\n        >>> beta2 = 0.999\\n        >>> epsilon = 1e-7\\n        >>> max_trust_ratio = 10\\n        >>> decay_lambda = 0\\n        >>> stop_gradients = True\\n        >>> lr = ivy.array(0.5)\\n        >>> new_weights = w.lamb_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\\n        ...                                beta2=beta2, epsilon=epsilon,\\n        ...                                max_trust_ratio=max_trust_ratio,\\n        ...                                decay_lambda=decay_lambda,\\n        ...                                stop_gradients=stop_gradients)\\n        >>> print(new_weights)\\n        ({\\n            a: ivy.array([-0.708, 1.29, 3.29]),\\n            b: ivy.array([1.45, 2.45, 0.445])\\n        }, {\\n            a: ivy.array([0.02, 0.03, 0.06]),\\n            b: ivy.array([0.06, 0.04, 0.07])\\n        }, {\\n            a: ivy.array([4.0e-05, 9.0e-05, 3.6e-04]),\\n            b: ivy.array([0.00036, 0.00016, 0.00049])\\n        })\\n        '\n    return ivy.lamb_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, max_trust_ratio=max_trust_ratio, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)",
            "def lamb_update(self: ivy.Container, dcdw: Union[ivy.Array, ivy.NativeArray, ivy.Container], lr: Union[float, ivy.Array, ivy.NativeArray, ivy.Container], mw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], vw_tm1: Union[ivy.Array, ivy.NativeArray, ivy.Container], step: Union[int, ivy.Container], /, *, beta1: Union[float, ivy.Container]=0.9, beta2: Union[float, ivy.Container]=0.999, epsilon: Union[float, ivy.Container]=1e-07, max_trust_ratio: Union[int, float, ivy.Container]=10, decay_lambda: Union[float, ivy.Container]=0, stop_gradients: Union[bool, ivy.Container]=True, out: Optional[ivy.Container]=None) -> ivy.Container:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update weights ws of some function, given the derivatives of some cost c with\\n        respect to ws, [dc/dw for w in ws], by applying LAMB method.\\n\\n        Parameters\\n        ----------\\n        self\\n            Weights of the function to be updated.\\n        dcdw\\n            Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].\\n        lr\\n            Learning rate(s), the rate(s) at which the weights should be updated\\n            relative to the gradient.\\n        mw_tm1\\n            running average of the gradients, from the previous time-step.\\n        vw_tm1\\n            running average of second moments of the gradients, from the previous\\n            time-step.\\n        step\\n            training step.\\n        beta1\\n            gradient forgetting factor (Default value = 0.9).\\n        beta2\\n            second moment of gradient forgetting factor (Default value = 0.999).\\n        epsilon\\n            divisor during adam update, preventing division by zero\\n            (Default value = 1e-7).\\n        max_trust_ratio\\n            The maximum value for the trust ratio. Default is 10.\\n        decay_lambda\\n            The factor used for weight decay. Default is zero.\\n        stop_gradients\\n            Whether to stop the gradients of the variables after each gradient step.\\n            Default is ``True``.\\n        out\\n            optional output array, for writing the result to. It must have a shape\\n            that the inputs broadcast to.\\n\\n        Returns\\n        -------\\n        ret\\n            The new function weights ws_new, following the LAMB updates.\\n\\n        Examples\\n        --------\\n        With one :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1., 2., 3.]), b=ivy.array([4., 5., 6.]))\\n        >>> dcdw = ivy.array([3., 4., 5.])\\n        >>> mw_tm1 = ivy.array([0., 0., 0.])\\n        >>> vw_tm1 = ivy.array([0.])\\n        >>> lr = ivy.array(1.)\\n        >>> step = ivy.array([2])\\n        >>> new_weights = w.lamb_update(dcdw, mw_tm1, vw_tm1, lr, step)\\n        >>> print(new_weights)\\n        ({\\n            a: ivy.array([1., 2., 3.]),\\n            b: ivy.array([4., 5., 6.])\\n        }, ivy.array([0.3, 0.4, 0.5]), ivy.array([1.01, 1.01, 1.02]))\\n\\n        With multiple :class:`ivy.Container` inputs:\\n\\n        >>> w = ivy.Container(a=ivy.array([1.,3.,5.]),\\n        ...                      b=ivy.array([3.,4.,2.]))\\n        >>> dcdw = ivy.Container(a=ivy.array([0.2,0.3,0.6]),\\n        ...                         b=ivy.array([0.6,0.4,0.7]))\\n        >>> mw_tm1 = ivy.Container(a=ivy.array([0.,0.,0.]),\\n        ...                           b=ivy.array([0.,0.,0.]))\\n\\n        >>> vw_tm1 = ivy.Container(a=ivy.array([0.,]),\\n        ...                           b=ivy.array([0.,]))\\n        >>> step = ivy.array([3.4])\\n        >>> beta1 = 0.9\\n        >>> beta2 = 0.999\\n        >>> epsilon = 1e-7\\n        >>> max_trust_ratio = 10\\n        >>> decay_lambda = 0\\n        >>> stop_gradients = True\\n        >>> lr = ivy.array(0.5)\\n        >>> new_weights = w.lamb_update(dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1,\\n        ...                                beta2=beta2, epsilon=epsilon,\\n        ...                                max_trust_ratio=max_trust_ratio,\\n        ...                                decay_lambda=decay_lambda,\\n        ...                                stop_gradients=stop_gradients)\\n        >>> print(new_weights)\\n        ({\\n            a: ivy.array([-0.708, 1.29, 3.29]),\\n            b: ivy.array([1.45, 2.45, 0.445])\\n        }, {\\n            a: ivy.array([0.02, 0.03, 0.06]),\\n            b: ivy.array([0.06, 0.04, 0.07])\\n        }, {\\n            a: ivy.array([4.0e-05, 9.0e-05, 3.6e-04]),\\n            b: ivy.array([0.00036, 0.00016, 0.00049])\\n        })\\n        '\n    return ivy.lamb_update(self, dcdw, lr, mw_tm1, vw_tm1, step, beta1=beta1, beta2=beta2, epsilon=epsilon, max_trust_ratio=max_trust_ratio, decay_lambda=decay_lambda, stop_gradients=stop_gradients, out=out)"
        ]
    }
]