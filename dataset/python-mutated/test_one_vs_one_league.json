[
    {
        "func_name": "get_random_result",
        "original": "def get_random_result():\n    ran = random.random()\n    if ran < 1.0 / 3:\n        return 'wins'\n    elif ran < 1.0 / 2:\n        return 'losses'\n    else:\n        return 'draws'",
        "mutated": [
            "def get_random_result():\n    if False:\n        i = 10\n    ran = random.random()\n    if ran < 1.0 / 3:\n        return 'wins'\n    elif ran < 1.0 / 2:\n        return 'losses'\n    else:\n        return 'draws'",
            "def get_random_result():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ran = random.random()\n    if ran < 1.0 / 3:\n        return 'wins'\n    elif ran < 1.0 / 2:\n        return 'losses'\n    else:\n        return 'draws'",
            "def get_random_result():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ran = random.random()\n    if ran < 1.0 / 3:\n        return 'wins'\n    elif ran < 1.0 / 2:\n        return 'losses'\n    else:\n        return 'draws'",
            "def get_random_result():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ran = random.random()\n    if ran < 1.0 / 3:\n        return 'wins'\n    elif ran < 1.0 / 2:\n        return 'losses'\n    else:\n        return 'draws'",
            "def get_random_result():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ran = random.random()\n    if ran < 1.0 / 3:\n        return 'wins'\n    elif ran < 1.0 / 2:\n        return 'losses'\n    else:\n        return 'draws'"
        ]
    },
    {
        "func_name": "test_naive",
        "original": "def test_naive(self):\n    league = create_league(one_vs_one_league_default_config.league)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    active_player_ids = [p.player_id for p in league.active_players]\n    assert set(active_player_ids) == set(league.active_players_ids)\n    active_player_id = active_player_ids[0]\n    active_player_ckpt = league.active_players[0].checkpoint_path\n    tmp = torch.tensor([1, 2, 3])\n    path_policy = one_vs_one_league_default_config.league.path_policy\n    torch.save(tmp, active_player_ckpt)\n    assert not league.judge_snapshot(active_player_id)\n    player_update_dict = {'player_id': active_player_id, 'train_iteration': one_vs_one_league_default_config.league.naive_sp_player.one_phase_step * 2}\n    league.update_active_player(player_update_dict)\n    assert league.judge_snapshot(active_player_id)\n    historical_player_ids = [p.player_id for p in league.historical_players]\n    assert len(historical_player_ids) == 1\n    historical_player_id = historical_player_ids[0]\n    vs_active = False\n    vs_historical = False\n    while True:\n        collect_job_info = league.get_job_info(active_player_id, eval_flag=False)\n        assert collect_job_info['agent_num'] == 2\n        assert len(collect_job_info['checkpoint_path']) == 2\n        assert collect_job_info['launch_player'] == active_player_id\n        assert collect_job_info['player_id'][0] == active_player_id\n        if collect_job_info['player_active_flag'][1]:\n            assert collect_job_info['player_id'][1] == collect_job_info['player_id'][0]\n            vs_active = True\n        else:\n            assert collect_job_info['player_id'][1] == historical_player_id\n            vs_historical = True\n        if vs_active and vs_historical:\n            break\n    eval_job_info = league.get_job_info(active_player_id, eval_flag=True)\n    assert eval_job_info['agent_num'] == 1\n    assert len(eval_job_info['checkpoint_path']) == 1\n    assert eval_job_info['launch_player'] == active_player_id\n    assert eval_job_info['player_id'][0] == active_player_id\n    assert len(eval_job_info['player_id']) == 1\n    assert len(eval_job_info['player_active_flag']) == 1\n    assert eval_job_info['eval_opponent'] in league.active_players[0]._eval_opponent_difficulty\n    episode_num = 5\n    env_num = 8\n    player_id = [active_player_id, historical_player_id]\n    result = [[get_random_result() for __ in range(8)] for _ in range(5)]\n    payoff_update_info = {'launch_player': active_player_id, 'player_id': player_id, 'episode_num': episode_num, 'env_num': env_num, 'result': result}\n    league.finish_job(payoff_update_info)\n    wins = 0\n    games = episode_num * env_num\n    for i in result:\n        for j in i:\n            if j == 'wins':\n                wins += 1\n    league.payoff[league.active_players[0], league.historical_players[0]] == wins / games\n    os.popen('rm -rf {}'.format(path_policy))\n    print('Finish!')",
        "mutated": [
            "def test_naive(self):\n    if False:\n        i = 10\n    league = create_league(one_vs_one_league_default_config.league)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    active_player_ids = [p.player_id for p in league.active_players]\n    assert set(active_player_ids) == set(league.active_players_ids)\n    active_player_id = active_player_ids[0]\n    active_player_ckpt = league.active_players[0].checkpoint_path\n    tmp = torch.tensor([1, 2, 3])\n    path_policy = one_vs_one_league_default_config.league.path_policy\n    torch.save(tmp, active_player_ckpt)\n    assert not league.judge_snapshot(active_player_id)\n    player_update_dict = {'player_id': active_player_id, 'train_iteration': one_vs_one_league_default_config.league.naive_sp_player.one_phase_step * 2}\n    league.update_active_player(player_update_dict)\n    assert league.judge_snapshot(active_player_id)\n    historical_player_ids = [p.player_id for p in league.historical_players]\n    assert len(historical_player_ids) == 1\n    historical_player_id = historical_player_ids[0]\n    vs_active = False\n    vs_historical = False\n    while True:\n        collect_job_info = league.get_job_info(active_player_id, eval_flag=False)\n        assert collect_job_info['agent_num'] == 2\n        assert len(collect_job_info['checkpoint_path']) == 2\n        assert collect_job_info['launch_player'] == active_player_id\n        assert collect_job_info['player_id'][0] == active_player_id\n        if collect_job_info['player_active_flag'][1]:\n            assert collect_job_info['player_id'][1] == collect_job_info['player_id'][0]\n            vs_active = True\n        else:\n            assert collect_job_info['player_id'][1] == historical_player_id\n            vs_historical = True\n        if vs_active and vs_historical:\n            break\n    eval_job_info = league.get_job_info(active_player_id, eval_flag=True)\n    assert eval_job_info['agent_num'] == 1\n    assert len(eval_job_info['checkpoint_path']) == 1\n    assert eval_job_info['launch_player'] == active_player_id\n    assert eval_job_info['player_id'][0] == active_player_id\n    assert len(eval_job_info['player_id']) == 1\n    assert len(eval_job_info['player_active_flag']) == 1\n    assert eval_job_info['eval_opponent'] in league.active_players[0]._eval_opponent_difficulty\n    episode_num = 5\n    env_num = 8\n    player_id = [active_player_id, historical_player_id]\n    result = [[get_random_result() for __ in range(8)] for _ in range(5)]\n    payoff_update_info = {'launch_player': active_player_id, 'player_id': player_id, 'episode_num': episode_num, 'env_num': env_num, 'result': result}\n    league.finish_job(payoff_update_info)\n    wins = 0\n    games = episode_num * env_num\n    for i in result:\n        for j in i:\n            if j == 'wins':\n                wins += 1\n    league.payoff[league.active_players[0], league.historical_players[0]] == wins / games\n    os.popen('rm -rf {}'.format(path_policy))\n    print('Finish!')",
            "def test_naive(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    league = create_league(one_vs_one_league_default_config.league)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    active_player_ids = [p.player_id for p in league.active_players]\n    assert set(active_player_ids) == set(league.active_players_ids)\n    active_player_id = active_player_ids[0]\n    active_player_ckpt = league.active_players[0].checkpoint_path\n    tmp = torch.tensor([1, 2, 3])\n    path_policy = one_vs_one_league_default_config.league.path_policy\n    torch.save(tmp, active_player_ckpt)\n    assert not league.judge_snapshot(active_player_id)\n    player_update_dict = {'player_id': active_player_id, 'train_iteration': one_vs_one_league_default_config.league.naive_sp_player.one_phase_step * 2}\n    league.update_active_player(player_update_dict)\n    assert league.judge_snapshot(active_player_id)\n    historical_player_ids = [p.player_id for p in league.historical_players]\n    assert len(historical_player_ids) == 1\n    historical_player_id = historical_player_ids[0]\n    vs_active = False\n    vs_historical = False\n    while True:\n        collect_job_info = league.get_job_info(active_player_id, eval_flag=False)\n        assert collect_job_info['agent_num'] == 2\n        assert len(collect_job_info['checkpoint_path']) == 2\n        assert collect_job_info['launch_player'] == active_player_id\n        assert collect_job_info['player_id'][0] == active_player_id\n        if collect_job_info['player_active_flag'][1]:\n            assert collect_job_info['player_id'][1] == collect_job_info['player_id'][0]\n            vs_active = True\n        else:\n            assert collect_job_info['player_id'][1] == historical_player_id\n            vs_historical = True\n        if vs_active and vs_historical:\n            break\n    eval_job_info = league.get_job_info(active_player_id, eval_flag=True)\n    assert eval_job_info['agent_num'] == 1\n    assert len(eval_job_info['checkpoint_path']) == 1\n    assert eval_job_info['launch_player'] == active_player_id\n    assert eval_job_info['player_id'][0] == active_player_id\n    assert len(eval_job_info['player_id']) == 1\n    assert len(eval_job_info['player_active_flag']) == 1\n    assert eval_job_info['eval_opponent'] in league.active_players[0]._eval_opponent_difficulty\n    episode_num = 5\n    env_num = 8\n    player_id = [active_player_id, historical_player_id]\n    result = [[get_random_result() for __ in range(8)] for _ in range(5)]\n    payoff_update_info = {'launch_player': active_player_id, 'player_id': player_id, 'episode_num': episode_num, 'env_num': env_num, 'result': result}\n    league.finish_job(payoff_update_info)\n    wins = 0\n    games = episode_num * env_num\n    for i in result:\n        for j in i:\n            if j == 'wins':\n                wins += 1\n    league.payoff[league.active_players[0], league.historical_players[0]] == wins / games\n    os.popen('rm -rf {}'.format(path_policy))\n    print('Finish!')",
            "def test_naive(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    league = create_league(one_vs_one_league_default_config.league)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    active_player_ids = [p.player_id for p in league.active_players]\n    assert set(active_player_ids) == set(league.active_players_ids)\n    active_player_id = active_player_ids[0]\n    active_player_ckpt = league.active_players[0].checkpoint_path\n    tmp = torch.tensor([1, 2, 3])\n    path_policy = one_vs_one_league_default_config.league.path_policy\n    torch.save(tmp, active_player_ckpt)\n    assert not league.judge_snapshot(active_player_id)\n    player_update_dict = {'player_id': active_player_id, 'train_iteration': one_vs_one_league_default_config.league.naive_sp_player.one_phase_step * 2}\n    league.update_active_player(player_update_dict)\n    assert league.judge_snapshot(active_player_id)\n    historical_player_ids = [p.player_id for p in league.historical_players]\n    assert len(historical_player_ids) == 1\n    historical_player_id = historical_player_ids[0]\n    vs_active = False\n    vs_historical = False\n    while True:\n        collect_job_info = league.get_job_info(active_player_id, eval_flag=False)\n        assert collect_job_info['agent_num'] == 2\n        assert len(collect_job_info['checkpoint_path']) == 2\n        assert collect_job_info['launch_player'] == active_player_id\n        assert collect_job_info['player_id'][0] == active_player_id\n        if collect_job_info['player_active_flag'][1]:\n            assert collect_job_info['player_id'][1] == collect_job_info['player_id'][0]\n            vs_active = True\n        else:\n            assert collect_job_info['player_id'][1] == historical_player_id\n            vs_historical = True\n        if vs_active and vs_historical:\n            break\n    eval_job_info = league.get_job_info(active_player_id, eval_flag=True)\n    assert eval_job_info['agent_num'] == 1\n    assert len(eval_job_info['checkpoint_path']) == 1\n    assert eval_job_info['launch_player'] == active_player_id\n    assert eval_job_info['player_id'][0] == active_player_id\n    assert len(eval_job_info['player_id']) == 1\n    assert len(eval_job_info['player_active_flag']) == 1\n    assert eval_job_info['eval_opponent'] in league.active_players[0]._eval_opponent_difficulty\n    episode_num = 5\n    env_num = 8\n    player_id = [active_player_id, historical_player_id]\n    result = [[get_random_result() for __ in range(8)] for _ in range(5)]\n    payoff_update_info = {'launch_player': active_player_id, 'player_id': player_id, 'episode_num': episode_num, 'env_num': env_num, 'result': result}\n    league.finish_job(payoff_update_info)\n    wins = 0\n    games = episode_num * env_num\n    for i in result:\n        for j in i:\n            if j == 'wins':\n                wins += 1\n    league.payoff[league.active_players[0], league.historical_players[0]] == wins / games\n    os.popen('rm -rf {}'.format(path_policy))\n    print('Finish!')",
            "def test_naive(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    league = create_league(one_vs_one_league_default_config.league)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    active_player_ids = [p.player_id for p in league.active_players]\n    assert set(active_player_ids) == set(league.active_players_ids)\n    active_player_id = active_player_ids[0]\n    active_player_ckpt = league.active_players[0].checkpoint_path\n    tmp = torch.tensor([1, 2, 3])\n    path_policy = one_vs_one_league_default_config.league.path_policy\n    torch.save(tmp, active_player_ckpt)\n    assert not league.judge_snapshot(active_player_id)\n    player_update_dict = {'player_id': active_player_id, 'train_iteration': one_vs_one_league_default_config.league.naive_sp_player.one_phase_step * 2}\n    league.update_active_player(player_update_dict)\n    assert league.judge_snapshot(active_player_id)\n    historical_player_ids = [p.player_id for p in league.historical_players]\n    assert len(historical_player_ids) == 1\n    historical_player_id = historical_player_ids[0]\n    vs_active = False\n    vs_historical = False\n    while True:\n        collect_job_info = league.get_job_info(active_player_id, eval_flag=False)\n        assert collect_job_info['agent_num'] == 2\n        assert len(collect_job_info['checkpoint_path']) == 2\n        assert collect_job_info['launch_player'] == active_player_id\n        assert collect_job_info['player_id'][0] == active_player_id\n        if collect_job_info['player_active_flag'][1]:\n            assert collect_job_info['player_id'][1] == collect_job_info['player_id'][0]\n            vs_active = True\n        else:\n            assert collect_job_info['player_id'][1] == historical_player_id\n            vs_historical = True\n        if vs_active and vs_historical:\n            break\n    eval_job_info = league.get_job_info(active_player_id, eval_flag=True)\n    assert eval_job_info['agent_num'] == 1\n    assert len(eval_job_info['checkpoint_path']) == 1\n    assert eval_job_info['launch_player'] == active_player_id\n    assert eval_job_info['player_id'][0] == active_player_id\n    assert len(eval_job_info['player_id']) == 1\n    assert len(eval_job_info['player_active_flag']) == 1\n    assert eval_job_info['eval_opponent'] in league.active_players[0]._eval_opponent_difficulty\n    episode_num = 5\n    env_num = 8\n    player_id = [active_player_id, historical_player_id]\n    result = [[get_random_result() for __ in range(8)] for _ in range(5)]\n    payoff_update_info = {'launch_player': active_player_id, 'player_id': player_id, 'episode_num': episode_num, 'env_num': env_num, 'result': result}\n    league.finish_job(payoff_update_info)\n    wins = 0\n    games = episode_num * env_num\n    for i in result:\n        for j in i:\n            if j == 'wins':\n                wins += 1\n    league.payoff[league.active_players[0], league.historical_players[0]] == wins / games\n    os.popen('rm -rf {}'.format(path_policy))\n    print('Finish!')",
            "def test_naive(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    league = create_league(one_vs_one_league_default_config.league)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    active_player_ids = [p.player_id for p in league.active_players]\n    assert set(active_player_ids) == set(league.active_players_ids)\n    active_player_id = active_player_ids[0]\n    active_player_ckpt = league.active_players[0].checkpoint_path\n    tmp = torch.tensor([1, 2, 3])\n    path_policy = one_vs_one_league_default_config.league.path_policy\n    torch.save(tmp, active_player_ckpt)\n    assert not league.judge_snapshot(active_player_id)\n    player_update_dict = {'player_id': active_player_id, 'train_iteration': one_vs_one_league_default_config.league.naive_sp_player.one_phase_step * 2}\n    league.update_active_player(player_update_dict)\n    assert league.judge_snapshot(active_player_id)\n    historical_player_ids = [p.player_id for p in league.historical_players]\n    assert len(historical_player_ids) == 1\n    historical_player_id = historical_player_ids[0]\n    vs_active = False\n    vs_historical = False\n    while True:\n        collect_job_info = league.get_job_info(active_player_id, eval_flag=False)\n        assert collect_job_info['agent_num'] == 2\n        assert len(collect_job_info['checkpoint_path']) == 2\n        assert collect_job_info['launch_player'] == active_player_id\n        assert collect_job_info['player_id'][0] == active_player_id\n        if collect_job_info['player_active_flag'][1]:\n            assert collect_job_info['player_id'][1] == collect_job_info['player_id'][0]\n            vs_active = True\n        else:\n            assert collect_job_info['player_id'][1] == historical_player_id\n            vs_historical = True\n        if vs_active and vs_historical:\n            break\n    eval_job_info = league.get_job_info(active_player_id, eval_flag=True)\n    assert eval_job_info['agent_num'] == 1\n    assert len(eval_job_info['checkpoint_path']) == 1\n    assert eval_job_info['launch_player'] == active_player_id\n    assert eval_job_info['player_id'][0] == active_player_id\n    assert len(eval_job_info['player_id']) == 1\n    assert len(eval_job_info['player_active_flag']) == 1\n    assert eval_job_info['eval_opponent'] in league.active_players[0]._eval_opponent_difficulty\n    episode_num = 5\n    env_num = 8\n    player_id = [active_player_id, historical_player_id]\n    result = [[get_random_result() for __ in range(8)] for _ in range(5)]\n    payoff_update_info = {'launch_player': active_player_id, 'player_id': player_id, 'episode_num': episode_num, 'env_num': env_num, 'result': result}\n    league.finish_job(payoff_update_info)\n    wins = 0\n    games = episode_num * env_num\n    for i in result:\n        for j in i:\n            if j == 'wins':\n                wins += 1\n    league.payoff[league.active_players[0], league.historical_players[0]] == wins / games\n    os.popen('rm -rf {}'.format(path_policy))\n    print('Finish!')"
        ]
    },
    {
        "func_name": "test_league_info",
        "original": "def test_league_info(self):\n    cfg = copy.deepcopy(one_vs_one_league_default_config.league)\n    cfg.path_policy = 'test_league_info'\n    league = create_league(cfg)\n    active_player_id = [p.player_id for p in league.active_players][0]\n    active_player_ckpt = [p.checkpoint_path for p in league.active_players][0]\n    tmp = torch.tensor([1, 2, 3])\n    torch.save(tmp, active_player_ckpt)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    print('\\n')\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    league.judge_snapshot(active_player_id, force=True)\n    for i in range(10):\n        job = league.get_job_info(active_player_id, eval_flag=False)\n        payoff_update_info = {'launch_player': active_player_id, 'player_id': job['player_id'], 'episode_num': 2, 'env_num': 4, 'result': [[get_random_result() for __ in range(4)] for _ in range(2)]}\n        league.finish_job(payoff_update_info)\n        if job['player_id'][0] != job['player_id'][1]:\n            win_loss_result = sum(payoff_update_info['result'], [])\n            home = league.get_player_by_id(job['player_id'][0])\n            away = league.get_player_by_id(job['player_id'][1])\n            (home.rating, away.rating) = league.metric_env.rate_1vs1(home.rating, away.rating, win_loss_result)\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    os.popen('rm -rf {}'.format(cfg.path_policy))",
        "mutated": [
            "def test_league_info(self):\n    if False:\n        i = 10\n    cfg = copy.deepcopy(one_vs_one_league_default_config.league)\n    cfg.path_policy = 'test_league_info'\n    league = create_league(cfg)\n    active_player_id = [p.player_id for p in league.active_players][0]\n    active_player_ckpt = [p.checkpoint_path for p in league.active_players][0]\n    tmp = torch.tensor([1, 2, 3])\n    torch.save(tmp, active_player_ckpt)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    print('\\n')\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    league.judge_snapshot(active_player_id, force=True)\n    for i in range(10):\n        job = league.get_job_info(active_player_id, eval_flag=False)\n        payoff_update_info = {'launch_player': active_player_id, 'player_id': job['player_id'], 'episode_num': 2, 'env_num': 4, 'result': [[get_random_result() for __ in range(4)] for _ in range(2)]}\n        league.finish_job(payoff_update_info)\n        if job['player_id'][0] != job['player_id'][1]:\n            win_loss_result = sum(payoff_update_info['result'], [])\n            home = league.get_player_by_id(job['player_id'][0])\n            away = league.get_player_by_id(job['player_id'][1])\n            (home.rating, away.rating) = league.metric_env.rate_1vs1(home.rating, away.rating, win_loss_result)\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    os.popen('rm -rf {}'.format(cfg.path_policy))",
            "def test_league_info(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cfg = copy.deepcopy(one_vs_one_league_default_config.league)\n    cfg.path_policy = 'test_league_info'\n    league = create_league(cfg)\n    active_player_id = [p.player_id for p in league.active_players][0]\n    active_player_ckpt = [p.checkpoint_path for p in league.active_players][0]\n    tmp = torch.tensor([1, 2, 3])\n    torch.save(tmp, active_player_ckpt)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    print('\\n')\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    league.judge_snapshot(active_player_id, force=True)\n    for i in range(10):\n        job = league.get_job_info(active_player_id, eval_flag=False)\n        payoff_update_info = {'launch_player': active_player_id, 'player_id': job['player_id'], 'episode_num': 2, 'env_num': 4, 'result': [[get_random_result() for __ in range(4)] for _ in range(2)]}\n        league.finish_job(payoff_update_info)\n        if job['player_id'][0] != job['player_id'][1]:\n            win_loss_result = sum(payoff_update_info['result'], [])\n            home = league.get_player_by_id(job['player_id'][0])\n            away = league.get_player_by_id(job['player_id'][1])\n            (home.rating, away.rating) = league.metric_env.rate_1vs1(home.rating, away.rating, win_loss_result)\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    os.popen('rm -rf {}'.format(cfg.path_policy))",
            "def test_league_info(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cfg = copy.deepcopy(one_vs_one_league_default_config.league)\n    cfg.path_policy = 'test_league_info'\n    league = create_league(cfg)\n    active_player_id = [p.player_id for p in league.active_players][0]\n    active_player_ckpt = [p.checkpoint_path for p in league.active_players][0]\n    tmp = torch.tensor([1, 2, 3])\n    torch.save(tmp, active_player_ckpt)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    print('\\n')\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    league.judge_snapshot(active_player_id, force=True)\n    for i in range(10):\n        job = league.get_job_info(active_player_id, eval_flag=False)\n        payoff_update_info = {'launch_player': active_player_id, 'player_id': job['player_id'], 'episode_num': 2, 'env_num': 4, 'result': [[get_random_result() for __ in range(4)] for _ in range(2)]}\n        league.finish_job(payoff_update_info)\n        if job['player_id'][0] != job['player_id'][1]:\n            win_loss_result = sum(payoff_update_info['result'], [])\n            home = league.get_player_by_id(job['player_id'][0])\n            away = league.get_player_by_id(job['player_id'][1])\n            (home.rating, away.rating) = league.metric_env.rate_1vs1(home.rating, away.rating, win_loss_result)\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    os.popen('rm -rf {}'.format(cfg.path_policy))",
            "def test_league_info(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cfg = copy.deepcopy(one_vs_one_league_default_config.league)\n    cfg.path_policy = 'test_league_info'\n    league = create_league(cfg)\n    active_player_id = [p.player_id for p in league.active_players][0]\n    active_player_ckpt = [p.checkpoint_path for p in league.active_players][0]\n    tmp = torch.tensor([1, 2, 3])\n    torch.save(tmp, active_player_ckpt)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    print('\\n')\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    league.judge_snapshot(active_player_id, force=True)\n    for i in range(10):\n        job = league.get_job_info(active_player_id, eval_flag=False)\n        payoff_update_info = {'launch_player': active_player_id, 'player_id': job['player_id'], 'episode_num': 2, 'env_num': 4, 'result': [[get_random_result() for __ in range(4)] for _ in range(2)]}\n        league.finish_job(payoff_update_info)\n        if job['player_id'][0] != job['player_id'][1]:\n            win_loss_result = sum(payoff_update_info['result'], [])\n            home = league.get_player_by_id(job['player_id'][0])\n            away = league.get_player_by_id(job['player_id'][1])\n            (home.rating, away.rating) = league.metric_env.rate_1vs1(home.rating, away.rating, win_loss_result)\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    os.popen('rm -rf {}'.format(cfg.path_policy))",
            "def test_league_info(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cfg = copy.deepcopy(one_vs_one_league_default_config.league)\n    cfg.path_policy = 'test_league_info'\n    league = create_league(cfg)\n    active_player_id = [p.player_id for p in league.active_players][0]\n    active_player_ckpt = [p.checkpoint_path for p in league.active_players][0]\n    tmp = torch.tensor([1, 2, 3])\n    torch.save(tmp, active_player_ckpt)\n    assert len(league.active_players) == 1\n    assert len(league.historical_players) == 0\n    print('\\n')\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    league.judge_snapshot(active_player_id, force=True)\n    for i in range(10):\n        job = league.get_job_info(active_player_id, eval_flag=False)\n        payoff_update_info = {'launch_player': active_player_id, 'player_id': job['player_id'], 'episode_num': 2, 'env_num': 4, 'result': [[get_random_result() for __ in range(4)] for _ in range(2)]}\n        league.finish_job(payoff_update_info)\n        if job['player_id'][0] != job['player_id'][1]:\n            win_loss_result = sum(payoff_update_info['result'], [])\n            home = league.get_player_by_id(job['player_id'][0])\n            away = league.get_player_by_id(job['player_id'][1])\n            (home.rating, away.rating) = league.metric_env.rate_1vs1(home.rating, away.rating, win_loss_result)\n    print(repr(league.payoff))\n    print(league.player_rank(string=True))\n    os.popen('rm -rf {}'.format(cfg.path_policy))"
        ]
    }
]