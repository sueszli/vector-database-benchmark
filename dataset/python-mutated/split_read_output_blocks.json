[
    {
        "func_name": "compute_additional_split_factor",
        "original": "def compute_additional_split_factor(datasource_or_legacy_reader: Union[Datasource, Reader], parallelism: int, mem_size: int, target_max_block_size: int, cur_additional_split_factor: Optional[int]=None) -> Tuple[int, str, int, Optional[int]]:\n    ctx = DataContext.get_current()\n    (parallelism, reason, _, _) = _autodetect_parallelism(parallelism, target_max_block_size, ctx, datasource_or_legacy_reader, mem_size)\n    num_read_tasks = len(datasource_or_legacy_reader.get_read_tasks(parallelism))\n    expected_block_size = None\n    if mem_size:\n        expected_block_size = mem_size / num_read_tasks\n        logger.get_logger().debug(f'Expected in-memory size {mem_size}, block size {expected_block_size}')\n        size_based_splits = round(max(1, expected_block_size / target_max_block_size))\n    else:\n        size_based_splits = 1\n    if cur_additional_split_factor:\n        size_based_splits *= cur_additional_split_factor\n    logger.get_logger().debug(f'Size based split factor {size_based_splits}')\n    estimated_num_blocks = num_read_tasks * size_based_splits\n    logger.get_logger().debug(f'Blocks after size splits {estimated_num_blocks}')\n    available_cpu_slots = ray_available_resources().get('CPU', 1)\n    if parallelism and num_read_tasks >= available_cpu_slots * 4 and (num_read_tasks >= 5000):\n        logger.get_logger().warn(f'{WARN_PREFIX} The requested parallelism of {parallelism} is more than 4x the number of available CPU slots in the cluster of {available_cpu_slots}. This can lead to slowdowns during the data reading phase due to excessive task creation. Reduce the parallelism to match with the available CPU slots in the cluster, or set parallelism to -1 for Ray Data to automatically determine the parallelism. You can ignore this message if the cluster is expected to autoscale.')\n    if estimated_num_blocks < parallelism and estimated_num_blocks > 0:\n        k = math.ceil(parallelism / estimated_num_blocks)\n        estimated_num_blocks = estimated_num_blocks * k\n        return (parallelism, reason, estimated_num_blocks, k)\n    return (parallelism, reason, estimated_num_blocks, None)",
        "mutated": [
            "def compute_additional_split_factor(datasource_or_legacy_reader: Union[Datasource, Reader], parallelism: int, mem_size: int, target_max_block_size: int, cur_additional_split_factor: Optional[int]=None) -> Tuple[int, str, int, Optional[int]]:\n    if False:\n        i = 10\n    ctx = DataContext.get_current()\n    (parallelism, reason, _, _) = _autodetect_parallelism(parallelism, target_max_block_size, ctx, datasource_or_legacy_reader, mem_size)\n    num_read_tasks = len(datasource_or_legacy_reader.get_read_tasks(parallelism))\n    expected_block_size = None\n    if mem_size:\n        expected_block_size = mem_size / num_read_tasks\n        logger.get_logger().debug(f'Expected in-memory size {mem_size}, block size {expected_block_size}')\n        size_based_splits = round(max(1, expected_block_size / target_max_block_size))\n    else:\n        size_based_splits = 1\n    if cur_additional_split_factor:\n        size_based_splits *= cur_additional_split_factor\n    logger.get_logger().debug(f'Size based split factor {size_based_splits}')\n    estimated_num_blocks = num_read_tasks * size_based_splits\n    logger.get_logger().debug(f'Blocks after size splits {estimated_num_blocks}')\n    available_cpu_slots = ray_available_resources().get('CPU', 1)\n    if parallelism and num_read_tasks >= available_cpu_slots * 4 and (num_read_tasks >= 5000):\n        logger.get_logger().warn(f'{WARN_PREFIX} The requested parallelism of {parallelism} is more than 4x the number of available CPU slots in the cluster of {available_cpu_slots}. This can lead to slowdowns during the data reading phase due to excessive task creation. Reduce the parallelism to match with the available CPU slots in the cluster, or set parallelism to -1 for Ray Data to automatically determine the parallelism. You can ignore this message if the cluster is expected to autoscale.')\n    if estimated_num_blocks < parallelism and estimated_num_blocks > 0:\n        k = math.ceil(parallelism / estimated_num_blocks)\n        estimated_num_blocks = estimated_num_blocks * k\n        return (parallelism, reason, estimated_num_blocks, k)\n    return (parallelism, reason, estimated_num_blocks, None)",
            "def compute_additional_split_factor(datasource_or_legacy_reader: Union[Datasource, Reader], parallelism: int, mem_size: int, target_max_block_size: int, cur_additional_split_factor: Optional[int]=None) -> Tuple[int, str, int, Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ctx = DataContext.get_current()\n    (parallelism, reason, _, _) = _autodetect_parallelism(parallelism, target_max_block_size, ctx, datasource_or_legacy_reader, mem_size)\n    num_read_tasks = len(datasource_or_legacy_reader.get_read_tasks(parallelism))\n    expected_block_size = None\n    if mem_size:\n        expected_block_size = mem_size / num_read_tasks\n        logger.get_logger().debug(f'Expected in-memory size {mem_size}, block size {expected_block_size}')\n        size_based_splits = round(max(1, expected_block_size / target_max_block_size))\n    else:\n        size_based_splits = 1\n    if cur_additional_split_factor:\n        size_based_splits *= cur_additional_split_factor\n    logger.get_logger().debug(f'Size based split factor {size_based_splits}')\n    estimated_num_blocks = num_read_tasks * size_based_splits\n    logger.get_logger().debug(f'Blocks after size splits {estimated_num_blocks}')\n    available_cpu_slots = ray_available_resources().get('CPU', 1)\n    if parallelism and num_read_tasks >= available_cpu_slots * 4 and (num_read_tasks >= 5000):\n        logger.get_logger().warn(f'{WARN_PREFIX} The requested parallelism of {parallelism} is more than 4x the number of available CPU slots in the cluster of {available_cpu_slots}. This can lead to slowdowns during the data reading phase due to excessive task creation. Reduce the parallelism to match with the available CPU slots in the cluster, or set parallelism to -1 for Ray Data to automatically determine the parallelism. You can ignore this message if the cluster is expected to autoscale.')\n    if estimated_num_blocks < parallelism and estimated_num_blocks > 0:\n        k = math.ceil(parallelism / estimated_num_blocks)\n        estimated_num_blocks = estimated_num_blocks * k\n        return (parallelism, reason, estimated_num_blocks, k)\n    return (parallelism, reason, estimated_num_blocks, None)",
            "def compute_additional_split_factor(datasource_or_legacy_reader: Union[Datasource, Reader], parallelism: int, mem_size: int, target_max_block_size: int, cur_additional_split_factor: Optional[int]=None) -> Tuple[int, str, int, Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ctx = DataContext.get_current()\n    (parallelism, reason, _, _) = _autodetect_parallelism(parallelism, target_max_block_size, ctx, datasource_or_legacy_reader, mem_size)\n    num_read_tasks = len(datasource_or_legacy_reader.get_read_tasks(parallelism))\n    expected_block_size = None\n    if mem_size:\n        expected_block_size = mem_size / num_read_tasks\n        logger.get_logger().debug(f'Expected in-memory size {mem_size}, block size {expected_block_size}')\n        size_based_splits = round(max(1, expected_block_size / target_max_block_size))\n    else:\n        size_based_splits = 1\n    if cur_additional_split_factor:\n        size_based_splits *= cur_additional_split_factor\n    logger.get_logger().debug(f'Size based split factor {size_based_splits}')\n    estimated_num_blocks = num_read_tasks * size_based_splits\n    logger.get_logger().debug(f'Blocks after size splits {estimated_num_blocks}')\n    available_cpu_slots = ray_available_resources().get('CPU', 1)\n    if parallelism and num_read_tasks >= available_cpu_slots * 4 and (num_read_tasks >= 5000):\n        logger.get_logger().warn(f'{WARN_PREFIX} The requested parallelism of {parallelism} is more than 4x the number of available CPU slots in the cluster of {available_cpu_slots}. This can lead to slowdowns during the data reading phase due to excessive task creation. Reduce the parallelism to match with the available CPU slots in the cluster, or set parallelism to -1 for Ray Data to automatically determine the parallelism. You can ignore this message if the cluster is expected to autoscale.')\n    if estimated_num_blocks < parallelism and estimated_num_blocks > 0:\n        k = math.ceil(parallelism / estimated_num_blocks)\n        estimated_num_blocks = estimated_num_blocks * k\n        return (parallelism, reason, estimated_num_blocks, k)\n    return (parallelism, reason, estimated_num_blocks, None)",
            "def compute_additional_split_factor(datasource_or_legacy_reader: Union[Datasource, Reader], parallelism: int, mem_size: int, target_max_block_size: int, cur_additional_split_factor: Optional[int]=None) -> Tuple[int, str, int, Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ctx = DataContext.get_current()\n    (parallelism, reason, _, _) = _autodetect_parallelism(parallelism, target_max_block_size, ctx, datasource_or_legacy_reader, mem_size)\n    num_read_tasks = len(datasource_or_legacy_reader.get_read_tasks(parallelism))\n    expected_block_size = None\n    if mem_size:\n        expected_block_size = mem_size / num_read_tasks\n        logger.get_logger().debug(f'Expected in-memory size {mem_size}, block size {expected_block_size}')\n        size_based_splits = round(max(1, expected_block_size / target_max_block_size))\n    else:\n        size_based_splits = 1\n    if cur_additional_split_factor:\n        size_based_splits *= cur_additional_split_factor\n    logger.get_logger().debug(f'Size based split factor {size_based_splits}')\n    estimated_num_blocks = num_read_tasks * size_based_splits\n    logger.get_logger().debug(f'Blocks after size splits {estimated_num_blocks}')\n    available_cpu_slots = ray_available_resources().get('CPU', 1)\n    if parallelism and num_read_tasks >= available_cpu_slots * 4 and (num_read_tasks >= 5000):\n        logger.get_logger().warn(f'{WARN_PREFIX} The requested parallelism of {parallelism} is more than 4x the number of available CPU slots in the cluster of {available_cpu_slots}. This can lead to slowdowns during the data reading phase due to excessive task creation. Reduce the parallelism to match with the available CPU slots in the cluster, or set parallelism to -1 for Ray Data to automatically determine the parallelism. You can ignore this message if the cluster is expected to autoscale.')\n    if estimated_num_blocks < parallelism and estimated_num_blocks > 0:\n        k = math.ceil(parallelism / estimated_num_blocks)\n        estimated_num_blocks = estimated_num_blocks * k\n        return (parallelism, reason, estimated_num_blocks, k)\n    return (parallelism, reason, estimated_num_blocks, None)",
            "def compute_additional_split_factor(datasource_or_legacy_reader: Union[Datasource, Reader], parallelism: int, mem_size: int, target_max_block_size: int, cur_additional_split_factor: Optional[int]=None) -> Tuple[int, str, int, Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ctx = DataContext.get_current()\n    (parallelism, reason, _, _) = _autodetect_parallelism(parallelism, target_max_block_size, ctx, datasource_or_legacy_reader, mem_size)\n    num_read_tasks = len(datasource_or_legacy_reader.get_read_tasks(parallelism))\n    expected_block_size = None\n    if mem_size:\n        expected_block_size = mem_size / num_read_tasks\n        logger.get_logger().debug(f'Expected in-memory size {mem_size}, block size {expected_block_size}')\n        size_based_splits = round(max(1, expected_block_size / target_max_block_size))\n    else:\n        size_based_splits = 1\n    if cur_additional_split_factor:\n        size_based_splits *= cur_additional_split_factor\n    logger.get_logger().debug(f'Size based split factor {size_based_splits}')\n    estimated_num_blocks = num_read_tasks * size_based_splits\n    logger.get_logger().debug(f'Blocks after size splits {estimated_num_blocks}')\n    available_cpu_slots = ray_available_resources().get('CPU', 1)\n    if parallelism and num_read_tasks >= available_cpu_slots * 4 and (num_read_tasks >= 5000):\n        logger.get_logger().warn(f'{WARN_PREFIX} The requested parallelism of {parallelism} is more than 4x the number of available CPU slots in the cluster of {available_cpu_slots}. This can lead to slowdowns during the data reading phase due to excessive task creation. Reduce the parallelism to match with the available CPU slots in the cluster, or set parallelism to -1 for Ray Data to automatically determine the parallelism. You can ignore this message if the cluster is expected to autoscale.')\n    if estimated_num_blocks < parallelism and estimated_num_blocks > 0:\n        k = math.ceil(parallelism / estimated_num_blocks)\n        estimated_num_blocks = estimated_num_blocks * k\n        return (parallelism, reason, estimated_num_blocks, k)\n    return (parallelism, reason, estimated_num_blocks, None)"
        ]
    },
    {
        "func_name": "apply",
        "original": "def apply(self, plan: PhysicalPlan) -> PhysicalPlan:\n    ops = [plan.dag]\n    while len(ops) == 1 and (not isinstance(ops[0], InputDataBuffer)):\n        logical_op = plan.op_map[ops[0]]\n        if isinstance(logical_op, Read):\n            self._split_read_op_if_needed(ops[0], logical_op)\n        ops = ops[0].input_dependencies\n    return plan",
        "mutated": [
            "def apply(self, plan: PhysicalPlan) -> PhysicalPlan:\n    if False:\n        i = 10\n    ops = [plan.dag]\n    while len(ops) == 1 and (not isinstance(ops[0], InputDataBuffer)):\n        logical_op = plan.op_map[ops[0]]\n        if isinstance(logical_op, Read):\n            self._split_read_op_if_needed(ops[0], logical_op)\n        ops = ops[0].input_dependencies\n    return plan",
            "def apply(self, plan: PhysicalPlan) -> PhysicalPlan:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ops = [plan.dag]\n    while len(ops) == 1 and (not isinstance(ops[0], InputDataBuffer)):\n        logical_op = plan.op_map[ops[0]]\n        if isinstance(logical_op, Read):\n            self._split_read_op_if_needed(ops[0], logical_op)\n        ops = ops[0].input_dependencies\n    return plan",
            "def apply(self, plan: PhysicalPlan) -> PhysicalPlan:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ops = [plan.dag]\n    while len(ops) == 1 and (not isinstance(ops[0], InputDataBuffer)):\n        logical_op = plan.op_map[ops[0]]\n        if isinstance(logical_op, Read):\n            self._split_read_op_if_needed(ops[0], logical_op)\n        ops = ops[0].input_dependencies\n    return plan",
            "def apply(self, plan: PhysicalPlan) -> PhysicalPlan:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ops = [plan.dag]\n    while len(ops) == 1 and (not isinstance(ops[0], InputDataBuffer)):\n        logical_op = plan.op_map[ops[0]]\n        if isinstance(logical_op, Read):\n            self._split_read_op_if_needed(ops[0], logical_op)\n        ops = ops[0].input_dependencies\n    return plan",
            "def apply(self, plan: PhysicalPlan) -> PhysicalPlan:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ops = [plan.dag]\n    while len(ops) == 1 and (not isinstance(ops[0], InputDataBuffer)):\n        logical_op = plan.op_map[ops[0]]\n        if isinstance(logical_op, Read):\n            self._split_read_op_if_needed(ops[0], logical_op)\n        ops = ops[0].input_dependencies\n    return plan"
        ]
    },
    {
        "func_name": "_split_read_op_if_needed",
        "original": "def _split_read_op_if_needed(self, op: PhysicalOperator, logical_op: Read):\n    (detected_parallelism, reason, estimated_num_blocks, k) = compute_additional_split_factor(logical_op._datasource_or_legacy_reader, logical_op._parallelism, logical_op._mem_size, op.actual_target_max_block_size, op._additional_split_factor)\n    if logical_op._parallelism == -1:\n        assert reason != ''\n        logger.get_logger().info(f'Using autodetected parallelism={detected_parallelism} for stage {logical_op.name} to satisfy {reason}.')\n    if k is not None:\n        logger.get_logger().info(f'To satisfy the requested parallelism of {detected_parallelism}, each read task output is split into {k} smaller blocks.')\n    if k is not None:\n        op.set_additional_split_factor(k)\n    logger.get_logger().debug(f'Estimated num output blocks {estimated_num_blocks}')\n    assert len(op.input_dependencies) == 1\n    up_op = op.input_dependencies[0]\n    assert isinstance(up_op, InputDataBuffer)\n    up_op._set_num_output_blocks(estimated_num_blocks)",
        "mutated": [
            "def _split_read_op_if_needed(self, op: PhysicalOperator, logical_op: Read):\n    if False:\n        i = 10\n    (detected_parallelism, reason, estimated_num_blocks, k) = compute_additional_split_factor(logical_op._datasource_or_legacy_reader, logical_op._parallelism, logical_op._mem_size, op.actual_target_max_block_size, op._additional_split_factor)\n    if logical_op._parallelism == -1:\n        assert reason != ''\n        logger.get_logger().info(f'Using autodetected parallelism={detected_parallelism} for stage {logical_op.name} to satisfy {reason}.')\n    if k is not None:\n        logger.get_logger().info(f'To satisfy the requested parallelism of {detected_parallelism}, each read task output is split into {k} smaller blocks.')\n    if k is not None:\n        op.set_additional_split_factor(k)\n    logger.get_logger().debug(f'Estimated num output blocks {estimated_num_blocks}')\n    assert len(op.input_dependencies) == 1\n    up_op = op.input_dependencies[0]\n    assert isinstance(up_op, InputDataBuffer)\n    up_op._set_num_output_blocks(estimated_num_blocks)",
            "def _split_read_op_if_needed(self, op: PhysicalOperator, logical_op: Read):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (detected_parallelism, reason, estimated_num_blocks, k) = compute_additional_split_factor(logical_op._datasource_or_legacy_reader, logical_op._parallelism, logical_op._mem_size, op.actual_target_max_block_size, op._additional_split_factor)\n    if logical_op._parallelism == -1:\n        assert reason != ''\n        logger.get_logger().info(f'Using autodetected parallelism={detected_parallelism} for stage {logical_op.name} to satisfy {reason}.')\n    if k is not None:\n        logger.get_logger().info(f'To satisfy the requested parallelism of {detected_parallelism}, each read task output is split into {k} smaller blocks.')\n    if k is not None:\n        op.set_additional_split_factor(k)\n    logger.get_logger().debug(f'Estimated num output blocks {estimated_num_blocks}')\n    assert len(op.input_dependencies) == 1\n    up_op = op.input_dependencies[0]\n    assert isinstance(up_op, InputDataBuffer)\n    up_op._set_num_output_blocks(estimated_num_blocks)",
            "def _split_read_op_if_needed(self, op: PhysicalOperator, logical_op: Read):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (detected_parallelism, reason, estimated_num_blocks, k) = compute_additional_split_factor(logical_op._datasource_or_legacy_reader, logical_op._parallelism, logical_op._mem_size, op.actual_target_max_block_size, op._additional_split_factor)\n    if logical_op._parallelism == -1:\n        assert reason != ''\n        logger.get_logger().info(f'Using autodetected parallelism={detected_parallelism} for stage {logical_op.name} to satisfy {reason}.')\n    if k is not None:\n        logger.get_logger().info(f'To satisfy the requested parallelism of {detected_parallelism}, each read task output is split into {k} smaller blocks.')\n    if k is not None:\n        op.set_additional_split_factor(k)\n    logger.get_logger().debug(f'Estimated num output blocks {estimated_num_blocks}')\n    assert len(op.input_dependencies) == 1\n    up_op = op.input_dependencies[0]\n    assert isinstance(up_op, InputDataBuffer)\n    up_op._set_num_output_blocks(estimated_num_blocks)",
            "def _split_read_op_if_needed(self, op: PhysicalOperator, logical_op: Read):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (detected_parallelism, reason, estimated_num_blocks, k) = compute_additional_split_factor(logical_op._datasource_or_legacy_reader, logical_op._parallelism, logical_op._mem_size, op.actual_target_max_block_size, op._additional_split_factor)\n    if logical_op._parallelism == -1:\n        assert reason != ''\n        logger.get_logger().info(f'Using autodetected parallelism={detected_parallelism} for stage {logical_op.name} to satisfy {reason}.')\n    if k is not None:\n        logger.get_logger().info(f'To satisfy the requested parallelism of {detected_parallelism}, each read task output is split into {k} smaller blocks.')\n    if k is not None:\n        op.set_additional_split_factor(k)\n    logger.get_logger().debug(f'Estimated num output blocks {estimated_num_blocks}')\n    assert len(op.input_dependencies) == 1\n    up_op = op.input_dependencies[0]\n    assert isinstance(up_op, InputDataBuffer)\n    up_op._set_num_output_blocks(estimated_num_blocks)",
            "def _split_read_op_if_needed(self, op: PhysicalOperator, logical_op: Read):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (detected_parallelism, reason, estimated_num_blocks, k) = compute_additional_split_factor(logical_op._datasource_or_legacy_reader, logical_op._parallelism, logical_op._mem_size, op.actual_target_max_block_size, op._additional_split_factor)\n    if logical_op._parallelism == -1:\n        assert reason != ''\n        logger.get_logger().info(f'Using autodetected parallelism={detected_parallelism} for stage {logical_op.name} to satisfy {reason}.')\n    if k is not None:\n        logger.get_logger().info(f'To satisfy the requested parallelism of {detected_parallelism}, each read task output is split into {k} smaller blocks.')\n    if k is not None:\n        op.set_additional_split_factor(k)\n    logger.get_logger().debug(f'Estimated num output blocks {estimated_num_blocks}')\n    assert len(op.input_dependencies) == 1\n    up_op = op.input_dependencies[0]\n    assert isinstance(up_op, InputDataBuffer)\n    up_op._set_num_output_blocks(estimated_num_blocks)"
        ]
    }
]