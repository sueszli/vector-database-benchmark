[
    {
        "func_name": "export_onnx",
        "original": "@torch.no_grad()\ndef export_onnx(self, output_path: str, opset: int=14, fp16: bool=False):\n    \"\"\"Export the model as onnx format files.\n\n        Args:\n            model_path: The model id or local path.\n            output_dir: The output dir.\n            opset: The version of the ONNX operator set to use.\n            fp16: Whether to use float16.\n        \"\"\"\n    output_path = Path(output_path)\n    dtype = torch.float16 if fp16 else torch.float32\n    if fp16 and torch.cuda.is_available():\n        device = 'cuda'\n    elif fp16 and (not torch.cuda.is_available()):\n        raise ValueError('`float16` model export is only supported on GPUs with CUDA')\n    else:\n        device = 'cpu'\n    self.model = self.model.to(device)\n    num_tokens = self.model.text_encoder.config.max_position_embeddings\n    text_hidden_size = self.model.text_encoder.config.hidden_size\n    text_input = self.model.tokenizer('A sample prompt', padding='max_length', max_length=self.model.tokenizer.model_max_length, truncation=True, return_tensors='pt')\n    self.export_help(self.model.text_encoder, model_args=text_input.input_ids.to(device=device, dtype=torch.int32), output_path=output_path / 'text_encoder' / 'model.onnx', ordered_input_names=['input_ids'], output_names=['last_hidden_state', 'pooler_output'], dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}}, opset=opset)\n    del self.model.text_encoder\n    unet_in_channels = self.model.unet.config.in_channels\n    unet_sample_size = self.model.unet.config.sample_size\n    unet_path = output_path / 'unet' / 'model.onnx'\n    self.export_help(self.model.unet, model_args=(torch.randn(2, unet_in_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), torch.randn(2).to(device=device, dtype=dtype), torch.randn(2, num_tokens, text_hidden_size).to(device=device, dtype=dtype), False), output_path=unet_path, ordered_input_names=['sample', 'timestep', 'encoder_hidden_states', 'return_dict'], output_names=['out_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'timestep': {0: 'batch'}, 'encoder_hidden_states': {0: 'batch', 1: 'sequence'}}, opset=opset, use_external_data_format=True)\n    unet_model_path = str(unet_path.absolute().as_posix())\n    unet_dir = os.path.dirname(unet_model_path)\n    unet = onnx.load(unet_model_path)\n    shutil.rmtree(unet_dir)\n    os.mkdir(unet_dir)\n    onnx.save_model(unet, unet_model_path, save_as_external_data=True, all_tensors_to_one_file=True, location='weights.pb', convert_attribute=False)\n    del self.model.unet\n    vae_encoder = self.model.vae\n    vae_in_channels = vae_encoder.config.in_channels\n    vae_sample_size = vae_encoder.config.sample_size\n    vae_encoder.forward = lambda sample, return_dict: vae_encoder.encode(sample, return_dict)[0].sample()\n    self.export_help(vae_encoder, model_args=(torch.randn(1, vae_in_channels, vae_sample_size, vae_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_encoder' / 'model.onnx', ordered_input_names=['sample', 'return_dict'], output_names=['latent_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    vae_decoder = self.model.vae\n    vae_latent_channels = vae_decoder.config.latent_channels\n    vae_out_channels = vae_decoder.config.out_channels\n    vae_decoder.forward = vae_encoder.decode\n    self.export_help(vae_decoder, model_args=(torch.randn(1, vae_latent_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_decoder' / 'model.onnx', ordered_input_names=['latent_sample', 'return_dict'], output_names=['sample'], dynamic_axes={'latent_sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    del self.model.vae\n    if self.model.safety_checker is not None:\n        safety_checker = self.model.safety_checker\n        clip_num_channels = safety_checker.config.vision_config.num_channels\n        clip_image_size = safety_checker.config.vision_config.image_size\n        safety_checker.forward = safety_checker.forward_onnx\n        self.export_help(self.model.safety_checker, model_args=(torch.randn(1, clip_num_channels, clip_image_size, clip_image_size).to(device=device, dtype=dtype), torch.randn(1, vae_sample_size, vae_sample_size, vae_out_channels).to(device=device, dtype=dtype)), output_path=output_path / 'safety_checker' / 'model.onnx', ordered_input_names=['clip_input', 'images'], output_names=['out_images', 'has_nsfw_concepts'], dynamic_axes={'clip_input': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'images': {0: 'batch', 1: 'height', 2: 'width', 3: 'channels'}}, opset=opset)\n        del self.model.safety_checker\n        safety_checker = OnnxRuntimeModel.from_pretrained(output_path / 'safety_checker')\n        feature_extractor = self.model.feature_extractor\n    else:\n        safety_checker = None\n        feature_extractor = None\n    onnx_pipeline = OnnxStableDiffusionPipeline(vae_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_encoder'), vae_decoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_decoder'), text_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'text_encoder'), tokenizer=self.model.tokenizer, unet=OnnxRuntimeModel.from_pretrained(output_path / 'unet'), scheduler=self.model.noise_scheduler, safety_checker=safety_checker, feature_extractor=feature_extractor, requires_safety_checker=safety_checker is not None)\n    onnx_pipeline.save_pretrained(output_path)\n    print('ONNX pipeline model saved to', output_path)\n    del self.model\n    del onnx_pipeline\n    _ = OnnxStableDiffusionPipeline.from_pretrained(output_path, provider='CPUExecutionProvider')\n    print('ONNX pipeline model is loadable')",
        "mutated": [
            "@torch.no_grad()\ndef export_onnx(self, output_path: str, opset: int=14, fp16: bool=False):\n    if False:\n        i = 10\n    'Export the model as onnx format files.\\n\\n        Args:\\n            model_path: The model id or local path.\\n            output_dir: The output dir.\\n            opset: The version of the ONNX operator set to use.\\n            fp16: Whether to use float16.\\n        '\n    output_path = Path(output_path)\n    dtype = torch.float16 if fp16 else torch.float32\n    if fp16 and torch.cuda.is_available():\n        device = 'cuda'\n    elif fp16 and (not torch.cuda.is_available()):\n        raise ValueError('`float16` model export is only supported on GPUs with CUDA')\n    else:\n        device = 'cpu'\n    self.model = self.model.to(device)\n    num_tokens = self.model.text_encoder.config.max_position_embeddings\n    text_hidden_size = self.model.text_encoder.config.hidden_size\n    text_input = self.model.tokenizer('A sample prompt', padding='max_length', max_length=self.model.tokenizer.model_max_length, truncation=True, return_tensors='pt')\n    self.export_help(self.model.text_encoder, model_args=text_input.input_ids.to(device=device, dtype=torch.int32), output_path=output_path / 'text_encoder' / 'model.onnx', ordered_input_names=['input_ids'], output_names=['last_hidden_state', 'pooler_output'], dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}}, opset=opset)\n    del self.model.text_encoder\n    unet_in_channels = self.model.unet.config.in_channels\n    unet_sample_size = self.model.unet.config.sample_size\n    unet_path = output_path / 'unet' / 'model.onnx'\n    self.export_help(self.model.unet, model_args=(torch.randn(2, unet_in_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), torch.randn(2).to(device=device, dtype=dtype), torch.randn(2, num_tokens, text_hidden_size).to(device=device, dtype=dtype), False), output_path=unet_path, ordered_input_names=['sample', 'timestep', 'encoder_hidden_states', 'return_dict'], output_names=['out_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'timestep': {0: 'batch'}, 'encoder_hidden_states': {0: 'batch', 1: 'sequence'}}, opset=opset, use_external_data_format=True)\n    unet_model_path = str(unet_path.absolute().as_posix())\n    unet_dir = os.path.dirname(unet_model_path)\n    unet = onnx.load(unet_model_path)\n    shutil.rmtree(unet_dir)\n    os.mkdir(unet_dir)\n    onnx.save_model(unet, unet_model_path, save_as_external_data=True, all_tensors_to_one_file=True, location='weights.pb', convert_attribute=False)\n    del self.model.unet\n    vae_encoder = self.model.vae\n    vae_in_channels = vae_encoder.config.in_channels\n    vae_sample_size = vae_encoder.config.sample_size\n    vae_encoder.forward = lambda sample, return_dict: vae_encoder.encode(sample, return_dict)[0].sample()\n    self.export_help(vae_encoder, model_args=(torch.randn(1, vae_in_channels, vae_sample_size, vae_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_encoder' / 'model.onnx', ordered_input_names=['sample', 'return_dict'], output_names=['latent_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    vae_decoder = self.model.vae\n    vae_latent_channels = vae_decoder.config.latent_channels\n    vae_out_channels = vae_decoder.config.out_channels\n    vae_decoder.forward = vae_encoder.decode\n    self.export_help(vae_decoder, model_args=(torch.randn(1, vae_latent_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_decoder' / 'model.onnx', ordered_input_names=['latent_sample', 'return_dict'], output_names=['sample'], dynamic_axes={'latent_sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    del self.model.vae\n    if self.model.safety_checker is not None:\n        safety_checker = self.model.safety_checker\n        clip_num_channels = safety_checker.config.vision_config.num_channels\n        clip_image_size = safety_checker.config.vision_config.image_size\n        safety_checker.forward = safety_checker.forward_onnx\n        self.export_help(self.model.safety_checker, model_args=(torch.randn(1, clip_num_channels, clip_image_size, clip_image_size).to(device=device, dtype=dtype), torch.randn(1, vae_sample_size, vae_sample_size, vae_out_channels).to(device=device, dtype=dtype)), output_path=output_path / 'safety_checker' / 'model.onnx', ordered_input_names=['clip_input', 'images'], output_names=['out_images', 'has_nsfw_concepts'], dynamic_axes={'clip_input': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'images': {0: 'batch', 1: 'height', 2: 'width', 3: 'channels'}}, opset=opset)\n        del self.model.safety_checker\n        safety_checker = OnnxRuntimeModel.from_pretrained(output_path / 'safety_checker')\n        feature_extractor = self.model.feature_extractor\n    else:\n        safety_checker = None\n        feature_extractor = None\n    onnx_pipeline = OnnxStableDiffusionPipeline(vae_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_encoder'), vae_decoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_decoder'), text_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'text_encoder'), tokenizer=self.model.tokenizer, unet=OnnxRuntimeModel.from_pretrained(output_path / 'unet'), scheduler=self.model.noise_scheduler, safety_checker=safety_checker, feature_extractor=feature_extractor, requires_safety_checker=safety_checker is not None)\n    onnx_pipeline.save_pretrained(output_path)\n    print('ONNX pipeline model saved to', output_path)\n    del self.model\n    del onnx_pipeline\n    _ = OnnxStableDiffusionPipeline.from_pretrained(output_path, provider='CPUExecutionProvider')\n    print('ONNX pipeline model is loadable')",
            "@torch.no_grad()\ndef export_onnx(self, output_path: str, opset: int=14, fp16: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Export the model as onnx format files.\\n\\n        Args:\\n            model_path: The model id or local path.\\n            output_dir: The output dir.\\n            opset: The version of the ONNX operator set to use.\\n            fp16: Whether to use float16.\\n        '\n    output_path = Path(output_path)\n    dtype = torch.float16 if fp16 else torch.float32\n    if fp16 and torch.cuda.is_available():\n        device = 'cuda'\n    elif fp16 and (not torch.cuda.is_available()):\n        raise ValueError('`float16` model export is only supported on GPUs with CUDA')\n    else:\n        device = 'cpu'\n    self.model = self.model.to(device)\n    num_tokens = self.model.text_encoder.config.max_position_embeddings\n    text_hidden_size = self.model.text_encoder.config.hidden_size\n    text_input = self.model.tokenizer('A sample prompt', padding='max_length', max_length=self.model.tokenizer.model_max_length, truncation=True, return_tensors='pt')\n    self.export_help(self.model.text_encoder, model_args=text_input.input_ids.to(device=device, dtype=torch.int32), output_path=output_path / 'text_encoder' / 'model.onnx', ordered_input_names=['input_ids'], output_names=['last_hidden_state', 'pooler_output'], dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}}, opset=opset)\n    del self.model.text_encoder\n    unet_in_channels = self.model.unet.config.in_channels\n    unet_sample_size = self.model.unet.config.sample_size\n    unet_path = output_path / 'unet' / 'model.onnx'\n    self.export_help(self.model.unet, model_args=(torch.randn(2, unet_in_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), torch.randn(2).to(device=device, dtype=dtype), torch.randn(2, num_tokens, text_hidden_size).to(device=device, dtype=dtype), False), output_path=unet_path, ordered_input_names=['sample', 'timestep', 'encoder_hidden_states', 'return_dict'], output_names=['out_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'timestep': {0: 'batch'}, 'encoder_hidden_states': {0: 'batch', 1: 'sequence'}}, opset=opset, use_external_data_format=True)\n    unet_model_path = str(unet_path.absolute().as_posix())\n    unet_dir = os.path.dirname(unet_model_path)\n    unet = onnx.load(unet_model_path)\n    shutil.rmtree(unet_dir)\n    os.mkdir(unet_dir)\n    onnx.save_model(unet, unet_model_path, save_as_external_data=True, all_tensors_to_one_file=True, location='weights.pb', convert_attribute=False)\n    del self.model.unet\n    vae_encoder = self.model.vae\n    vae_in_channels = vae_encoder.config.in_channels\n    vae_sample_size = vae_encoder.config.sample_size\n    vae_encoder.forward = lambda sample, return_dict: vae_encoder.encode(sample, return_dict)[0].sample()\n    self.export_help(vae_encoder, model_args=(torch.randn(1, vae_in_channels, vae_sample_size, vae_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_encoder' / 'model.onnx', ordered_input_names=['sample', 'return_dict'], output_names=['latent_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    vae_decoder = self.model.vae\n    vae_latent_channels = vae_decoder.config.latent_channels\n    vae_out_channels = vae_decoder.config.out_channels\n    vae_decoder.forward = vae_encoder.decode\n    self.export_help(vae_decoder, model_args=(torch.randn(1, vae_latent_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_decoder' / 'model.onnx', ordered_input_names=['latent_sample', 'return_dict'], output_names=['sample'], dynamic_axes={'latent_sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    del self.model.vae\n    if self.model.safety_checker is not None:\n        safety_checker = self.model.safety_checker\n        clip_num_channels = safety_checker.config.vision_config.num_channels\n        clip_image_size = safety_checker.config.vision_config.image_size\n        safety_checker.forward = safety_checker.forward_onnx\n        self.export_help(self.model.safety_checker, model_args=(torch.randn(1, clip_num_channels, clip_image_size, clip_image_size).to(device=device, dtype=dtype), torch.randn(1, vae_sample_size, vae_sample_size, vae_out_channels).to(device=device, dtype=dtype)), output_path=output_path / 'safety_checker' / 'model.onnx', ordered_input_names=['clip_input', 'images'], output_names=['out_images', 'has_nsfw_concepts'], dynamic_axes={'clip_input': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'images': {0: 'batch', 1: 'height', 2: 'width', 3: 'channels'}}, opset=opset)\n        del self.model.safety_checker\n        safety_checker = OnnxRuntimeModel.from_pretrained(output_path / 'safety_checker')\n        feature_extractor = self.model.feature_extractor\n    else:\n        safety_checker = None\n        feature_extractor = None\n    onnx_pipeline = OnnxStableDiffusionPipeline(vae_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_encoder'), vae_decoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_decoder'), text_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'text_encoder'), tokenizer=self.model.tokenizer, unet=OnnxRuntimeModel.from_pretrained(output_path / 'unet'), scheduler=self.model.noise_scheduler, safety_checker=safety_checker, feature_extractor=feature_extractor, requires_safety_checker=safety_checker is not None)\n    onnx_pipeline.save_pretrained(output_path)\n    print('ONNX pipeline model saved to', output_path)\n    del self.model\n    del onnx_pipeline\n    _ = OnnxStableDiffusionPipeline.from_pretrained(output_path, provider='CPUExecutionProvider')\n    print('ONNX pipeline model is loadable')",
            "@torch.no_grad()\ndef export_onnx(self, output_path: str, opset: int=14, fp16: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Export the model as onnx format files.\\n\\n        Args:\\n            model_path: The model id or local path.\\n            output_dir: The output dir.\\n            opset: The version of the ONNX operator set to use.\\n            fp16: Whether to use float16.\\n        '\n    output_path = Path(output_path)\n    dtype = torch.float16 if fp16 else torch.float32\n    if fp16 and torch.cuda.is_available():\n        device = 'cuda'\n    elif fp16 and (not torch.cuda.is_available()):\n        raise ValueError('`float16` model export is only supported on GPUs with CUDA')\n    else:\n        device = 'cpu'\n    self.model = self.model.to(device)\n    num_tokens = self.model.text_encoder.config.max_position_embeddings\n    text_hidden_size = self.model.text_encoder.config.hidden_size\n    text_input = self.model.tokenizer('A sample prompt', padding='max_length', max_length=self.model.tokenizer.model_max_length, truncation=True, return_tensors='pt')\n    self.export_help(self.model.text_encoder, model_args=text_input.input_ids.to(device=device, dtype=torch.int32), output_path=output_path / 'text_encoder' / 'model.onnx', ordered_input_names=['input_ids'], output_names=['last_hidden_state', 'pooler_output'], dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}}, opset=opset)\n    del self.model.text_encoder\n    unet_in_channels = self.model.unet.config.in_channels\n    unet_sample_size = self.model.unet.config.sample_size\n    unet_path = output_path / 'unet' / 'model.onnx'\n    self.export_help(self.model.unet, model_args=(torch.randn(2, unet_in_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), torch.randn(2).to(device=device, dtype=dtype), torch.randn(2, num_tokens, text_hidden_size).to(device=device, dtype=dtype), False), output_path=unet_path, ordered_input_names=['sample', 'timestep', 'encoder_hidden_states', 'return_dict'], output_names=['out_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'timestep': {0: 'batch'}, 'encoder_hidden_states': {0: 'batch', 1: 'sequence'}}, opset=opset, use_external_data_format=True)\n    unet_model_path = str(unet_path.absolute().as_posix())\n    unet_dir = os.path.dirname(unet_model_path)\n    unet = onnx.load(unet_model_path)\n    shutil.rmtree(unet_dir)\n    os.mkdir(unet_dir)\n    onnx.save_model(unet, unet_model_path, save_as_external_data=True, all_tensors_to_one_file=True, location='weights.pb', convert_attribute=False)\n    del self.model.unet\n    vae_encoder = self.model.vae\n    vae_in_channels = vae_encoder.config.in_channels\n    vae_sample_size = vae_encoder.config.sample_size\n    vae_encoder.forward = lambda sample, return_dict: vae_encoder.encode(sample, return_dict)[0].sample()\n    self.export_help(vae_encoder, model_args=(torch.randn(1, vae_in_channels, vae_sample_size, vae_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_encoder' / 'model.onnx', ordered_input_names=['sample', 'return_dict'], output_names=['latent_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    vae_decoder = self.model.vae\n    vae_latent_channels = vae_decoder.config.latent_channels\n    vae_out_channels = vae_decoder.config.out_channels\n    vae_decoder.forward = vae_encoder.decode\n    self.export_help(vae_decoder, model_args=(torch.randn(1, vae_latent_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_decoder' / 'model.onnx', ordered_input_names=['latent_sample', 'return_dict'], output_names=['sample'], dynamic_axes={'latent_sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    del self.model.vae\n    if self.model.safety_checker is not None:\n        safety_checker = self.model.safety_checker\n        clip_num_channels = safety_checker.config.vision_config.num_channels\n        clip_image_size = safety_checker.config.vision_config.image_size\n        safety_checker.forward = safety_checker.forward_onnx\n        self.export_help(self.model.safety_checker, model_args=(torch.randn(1, clip_num_channels, clip_image_size, clip_image_size).to(device=device, dtype=dtype), torch.randn(1, vae_sample_size, vae_sample_size, vae_out_channels).to(device=device, dtype=dtype)), output_path=output_path / 'safety_checker' / 'model.onnx', ordered_input_names=['clip_input', 'images'], output_names=['out_images', 'has_nsfw_concepts'], dynamic_axes={'clip_input': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'images': {0: 'batch', 1: 'height', 2: 'width', 3: 'channels'}}, opset=opset)\n        del self.model.safety_checker\n        safety_checker = OnnxRuntimeModel.from_pretrained(output_path / 'safety_checker')\n        feature_extractor = self.model.feature_extractor\n    else:\n        safety_checker = None\n        feature_extractor = None\n    onnx_pipeline = OnnxStableDiffusionPipeline(vae_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_encoder'), vae_decoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_decoder'), text_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'text_encoder'), tokenizer=self.model.tokenizer, unet=OnnxRuntimeModel.from_pretrained(output_path / 'unet'), scheduler=self.model.noise_scheduler, safety_checker=safety_checker, feature_extractor=feature_extractor, requires_safety_checker=safety_checker is not None)\n    onnx_pipeline.save_pretrained(output_path)\n    print('ONNX pipeline model saved to', output_path)\n    del self.model\n    del onnx_pipeline\n    _ = OnnxStableDiffusionPipeline.from_pretrained(output_path, provider='CPUExecutionProvider')\n    print('ONNX pipeline model is loadable')",
            "@torch.no_grad()\ndef export_onnx(self, output_path: str, opset: int=14, fp16: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Export the model as onnx format files.\\n\\n        Args:\\n            model_path: The model id or local path.\\n            output_dir: The output dir.\\n            opset: The version of the ONNX operator set to use.\\n            fp16: Whether to use float16.\\n        '\n    output_path = Path(output_path)\n    dtype = torch.float16 if fp16 else torch.float32\n    if fp16 and torch.cuda.is_available():\n        device = 'cuda'\n    elif fp16 and (not torch.cuda.is_available()):\n        raise ValueError('`float16` model export is only supported on GPUs with CUDA')\n    else:\n        device = 'cpu'\n    self.model = self.model.to(device)\n    num_tokens = self.model.text_encoder.config.max_position_embeddings\n    text_hidden_size = self.model.text_encoder.config.hidden_size\n    text_input = self.model.tokenizer('A sample prompt', padding='max_length', max_length=self.model.tokenizer.model_max_length, truncation=True, return_tensors='pt')\n    self.export_help(self.model.text_encoder, model_args=text_input.input_ids.to(device=device, dtype=torch.int32), output_path=output_path / 'text_encoder' / 'model.onnx', ordered_input_names=['input_ids'], output_names=['last_hidden_state', 'pooler_output'], dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}}, opset=opset)\n    del self.model.text_encoder\n    unet_in_channels = self.model.unet.config.in_channels\n    unet_sample_size = self.model.unet.config.sample_size\n    unet_path = output_path / 'unet' / 'model.onnx'\n    self.export_help(self.model.unet, model_args=(torch.randn(2, unet_in_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), torch.randn(2).to(device=device, dtype=dtype), torch.randn(2, num_tokens, text_hidden_size).to(device=device, dtype=dtype), False), output_path=unet_path, ordered_input_names=['sample', 'timestep', 'encoder_hidden_states', 'return_dict'], output_names=['out_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'timestep': {0: 'batch'}, 'encoder_hidden_states': {0: 'batch', 1: 'sequence'}}, opset=opset, use_external_data_format=True)\n    unet_model_path = str(unet_path.absolute().as_posix())\n    unet_dir = os.path.dirname(unet_model_path)\n    unet = onnx.load(unet_model_path)\n    shutil.rmtree(unet_dir)\n    os.mkdir(unet_dir)\n    onnx.save_model(unet, unet_model_path, save_as_external_data=True, all_tensors_to_one_file=True, location='weights.pb', convert_attribute=False)\n    del self.model.unet\n    vae_encoder = self.model.vae\n    vae_in_channels = vae_encoder.config.in_channels\n    vae_sample_size = vae_encoder.config.sample_size\n    vae_encoder.forward = lambda sample, return_dict: vae_encoder.encode(sample, return_dict)[0].sample()\n    self.export_help(vae_encoder, model_args=(torch.randn(1, vae_in_channels, vae_sample_size, vae_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_encoder' / 'model.onnx', ordered_input_names=['sample', 'return_dict'], output_names=['latent_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    vae_decoder = self.model.vae\n    vae_latent_channels = vae_decoder.config.latent_channels\n    vae_out_channels = vae_decoder.config.out_channels\n    vae_decoder.forward = vae_encoder.decode\n    self.export_help(vae_decoder, model_args=(torch.randn(1, vae_latent_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_decoder' / 'model.onnx', ordered_input_names=['latent_sample', 'return_dict'], output_names=['sample'], dynamic_axes={'latent_sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    del self.model.vae\n    if self.model.safety_checker is not None:\n        safety_checker = self.model.safety_checker\n        clip_num_channels = safety_checker.config.vision_config.num_channels\n        clip_image_size = safety_checker.config.vision_config.image_size\n        safety_checker.forward = safety_checker.forward_onnx\n        self.export_help(self.model.safety_checker, model_args=(torch.randn(1, clip_num_channels, clip_image_size, clip_image_size).to(device=device, dtype=dtype), torch.randn(1, vae_sample_size, vae_sample_size, vae_out_channels).to(device=device, dtype=dtype)), output_path=output_path / 'safety_checker' / 'model.onnx', ordered_input_names=['clip_input', 'images'], output_names=['out_images', 'has_nsfw_concepts'], dynamic_axes={'clip_input': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'images': {0: 'batch', 1: 'height', 2: 'width', 3: 'channels'}}, opset=opset)\n        del self.model.safety_checker\n        safety_checker = OnnxRuntimeModel.from_pretrained(output_path / 'safety_checker')\n        feature_extractor = self.model.feature_extractor\n    else:\n        safety_checker = None\n        feature_extractor = None\n    onnx_pipeline = OnnxStableDiffusionPipeline(vae_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_encoder'), vae_decoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_decoder'), text_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'text_encoder'), tokenizer=self.model.tokenizer, unet=OnnxRuntimeModel.from_pretrained(output_path / 'unet'), scheduler=self.model.noise_scheduler, safety_checker=safety_checker, feature_extractor=feature_extractor, requires_safety_checker=safety_checker is not None)\n    onnx_pipeline.save_pretrained(output_path)\n    print('ONNX pipeline model saved to', output_path)\n    del self.model\n    del onnx_pipeline\n    _ = OnnxStableDiffusionPipeline.from_pretrained(output_path, provider='CPUExecutionProvider')\n    print('ONNX pipeline model is loadable')",
            "@torch.no_grad()\ndef export_onnx(self, output_path: str, opset: int=14, fp16: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Export the model as onnx format files.\\n\\n        Args:\\n            model_path: The model id or local path.\\n            output_dir: The output dir.\\n            opset: The version of the ONNX operator set to use.\\n            fp16: Whether to use float16.\\n        '\n    output_path = Path(output_path)\n    dtype = torch.float16 if fp16 else torch.float32\n    if fp16 and torch.cuda.is_available():\n        device = 'cuda'\n    elif fp16 and (not torch.cuda.is_available()):\n        raise ValueError('`float16` model export is only supported on GPUs with CUDA')\n    else:\n        device = 'cpu'\n    self.model = self.model.to(device)\n    num_tokens = self.model.text_encoder.config.max_position_embeddings\n    text_hidden_size = self.model.text_encoder.config.hidden_size\n    text_input = self.model.tokenizer('A sample prompt', padding='max_length', max_length=self.model.tokenizer.model_max_length, truncation=True, return_tensors='pt')\n    self.export_help(self.model.text_encoder, model_args=text_input.input_ids.to(device=device, dtype=torch.int32), output_path=output_path / 'text_encoder' / 'model.onnx', ordered_input_names=['input_ids'], output_names=['last_hidden_state', 'pooler_output'], dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}}, opset=opset)\n    del self.model.text_encoder\n    unet_in_channels = self.model.unet.config.in_channels\n    unet_sample_size = self.model.unet.config.sample_size\n    unet_path = output_path / 'unet' / 'model.onnx'\n    self.export_help(self.model.unet, model_args=(torch.randn(2, unet_in_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), torch.randn(2).to(device=device, dtype=dtype), torch.randn(2, num_tokens, text_hidden_size).to(device=device, dtype=dtype), False), output_path=unet_path, ordered_input_names=['sample', 'timestep', 'encoder_hidden_states', 'return_dict'], output_names=['out_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'timestep': {0: 'batch'}, 'encoder_hidden_states': {0: 'batch', 1: 'sequence'}}, opset=opset, use_external_data_format=True)\n    unet_model_path = str(unet_path.absolute().as_posix())\n    unet_dir = os.path.dirname(unet_model_path)\n    unet = onnx.load(unet_model_path)\n    shutil.rmtree(unet_dir)\n    os.mkdir(unet_dir)\n    onnx.save_model(unet, unet_model_path, save_as_external_data=True, all_tensors_to_one_file=True, location='weights.pb', convert_attribute=False)\n    del self.model.unet\n    vae_encoder = self.model.vae\n    vae_in_channels = vae_encoder.config.in_channels\n    vae_sample_size = vae_encoder.config.sample_size\n    vae_encoder.forward = lambda sample, return_dict: vae_encoder.encode(sample, return_dict)[0].sample()\n    self.export_help(vae_encoder, model_args=(torch.randn(1, vae_in_channels, vae_sample_size, vae_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_encoder' / 'model.onnx', ordered_input_names=['sample', 'return_dict'], output_names=['latent_sample'], dynamic_axes={'sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    vae_decoder = self.model.vae\n    vae_latent_channels = vae_decoder.config.latent_channels\n    vae_out_channels = vae_decoder.config.out_channels\n    vae_decoder.forward = vae_encoder.decode\n    self.export_help(vae_decoder, model_args=(torch.randn(1, vae_latent_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype), False), output_path=output_path / 'vae_decoder' / 'model.onnx', ordered_input_names=['latent_sample', 'return_dict'], output_names=['sample'], dynamic_axes={'latent_sample': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}}, opset=opset)\n    del self.model.vae\n    if self.model.safety_checker is not None:\n        safety_checker = self.model.safety_checker\n        clip_num_channels = safety_checker.config.vision_config.num_channels\n        clip_image_size = safety_checker.config.vision_config.image_size\n        safety_checker.forward = safety_checker.forward_onnx\n        self.export_help(self.model.safety_checker, model_args=(torch.randn(1, clip_num_channels, clip_image_size, clip_image_size).to(device=device, dtype=dtype), torch.randn(1, vae_sample_size, vae_sample_size, vae_out_channels).to(device=device, dtype=dtype)), output_path=output_path / 'safety_checker' / 'model.onnx', ordered_input_names=['clip_input', 'images'], output_names=['out_images', 'has_nsfw_concepts'], dynamic_axes={'clip_input': {0: 'batch', 1: 'channels', 2: 'height', 3: 'width'}, 'images': {0: 'batch', 1: 'height', 2: 'width', 3: 'channels'}}, opset=opset)\n        del self.model.safety_checker\n        safety_checker = OnnxRuntimeModel.from_pretrained(output_path / 'safety_checker')\n        feature_extractor = self.model.feature_extractor\n    else:\n        safety_checker = None\n        feature_extractor = None\n    onnx_pipeline = OnnxStableDiffusionPipeline(vae_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_encoder'), vae_decoder=OnnxRuntimeModel.from_pretrained(output_path / 'vae_decoder'), text_encoder=OnnxRuntimeModel.from_pretrained(output_path / 'text_encoder'), tokenizer=self.model.tokenizer, unet=OnnxRuntimeModel.from_pretrained(output_path / 'unet'), scheduler=self.model.noise_scheduler, safety_checker=safety_checker, feature_extractor=feature_extractor, requires_safety_checker=safety_checker is not None)\n    onnx_pipeline.save_pretrained(output_path)\n    print('ONNX pipeline model saved to', output_path)\n    del self.model\n    del onnx_pipeline\n    _ = OnnxStableDiffusionPipeline.from_pretrained(output_path, provider='CPUExecutionProvider')\n    print('ONNX pipeline model is loadable')"
        ]
    },
    {
        "func_name": "export_help",
        "original": "def export_help(self, model, model_args: tuple, output_path: Path, ordered_input_names, output_names, dynamic_axes, opset, use_external_data_format=False):\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    is_torch_less_than_1_11 = version.parse(version.parse(torch.__version__).base_version) < version.parse('1.11')\n    if is_torch_less_than_1_11:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, use_external_data_format=use_external_data_format, enable_onnx_checker=True, opset_version=opset)\n    else:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, opset_version=opset)",
        "mutated": [
            "def export_help(self, model, model_args: tuple, output_path: Path, ordered_input_names, output_names, dynamic_axes, opset, use_external_data_format=False):\n    if False:\n        i = 10\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    is_torch_less_than_1_11 = version.parse(version.parse(torch.__version__).base_version) < version.parse('1.11')\n    if is_torch_less_than_1_11:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, use_external_data_format=use_external_data_format, enable_onnx_checker=True, opset_version=opset)\n    else:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, opset_version=opset)",
            "def export_help(self, model, model_args: tuple, output_path: Path, ordered_input_names, output_names, dynamic_axes, opset, use_external_data_format=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    is_torch_less_than_1_11 = version.parse(version.parse(torch.__version__).base_version) < version.parse('1.11')\n    if is_torch_less_than_1_11:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, use_external_data_format=use_external_data_format, enable_onnx_checker=True, opset_version=opset)\n    else:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, opset_version=opset)",
            "def export_help(self, model, model_args: tuple, output_path: Path, ordered_input_names, output_names, dynamic_axes, opset, use_external_data_format=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    is_torch_less_than_1_11 = version.parse(version.parse(torch.__version__).base_version) < version.parse('1.11')\n    if is_torch_less_than_1_11:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, use_external_data_format=use_external_data_format, enable_onnx_checker=True, opset_version=opset)\n    else:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, opset_version=opset)",
            "def export_help(self, model, model_args: tuple, output_path: Path, ordered_input_names, output_names, dynamic_axes, opset, use_external_data_format=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    is_torch_less_than_1_11 = version.parse(version.parse(torch.__version__).base_version) < version.parse('1.11')\n    if is_torch_less_than_1_11:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, use_external_data_format=use_external_data_format, enable_onnx_checker=True, opset_version=opset)\n    else:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, opset_version=opset)",
            "def export_help(self, model, model_args: tuple, output_path: Path, ordered_input_names, output_names, dynamic_axes, opset, use_external_data_format=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    is_torch_less_than_1_11 = version.parse(version.parse(torch.__version__).base_version) < version.parse('1.11')\n    if is_torch_less_than_1_11:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, use_external_data_format=use_external_data_format, enable_onnx_checker=True, opset_version=opset)\n    else:\n        export(model, model_args, f=output_path.as_posix(), input_names=ordered_input_names, output_names=output_names, dynamic_axes=dynamic_axes, do_constant_folding=True, opset_version=opset)"
        ]
    }
]