[
    {
        "func_name": "forward_grad",
        "original": "@framework.static_only\ndef forward_grad(outputs, inputs, grad_inputs=None):\n    \"\"\"Forward mode of automatic differentiation.\n\n    Note:\n        **ONLY available in the static graph mode and primitive operators.**\n\n    Args:\n        outputs(Tensor|Sequence[Tensor]): The output tensor or tensors.\n        inputs(Tensor|Sequence[Tensor]): The input tensor or tensors.\n        grad_inputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\n            Tensors of inputs which has the same shape with inputs, Defaults to\n            None, in this case is equivalent to all ones.\n\n    Returns:\n        grad_outputs(Tensor|Sequence[Tensor]): The gradients for outputs.\n\n    Examples:\n\n        .. code-block:: python\n\n            >>> import numpy as np\n            >>> import paddle\n\n            >>> paddle.enable_static()\n            >>> paddle.incubate.autograd.enable_prim()\n\n            >>> startup_program = paddle.static.Program()\n            >>> main_program = paddle.static.Program()\n\n            >>> with paddle.static.program_guard(main_program, startup_program):\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\n            ...     y = x * x\n            ...     y_grad = paddle.incubate.autograd.forward_grad(y, x)\n            ...     paddle.incubate.autograd.prim2orig()\n            ...\n            >>> exe = paddle.static.Executor()\n            >>> exe.run(startup_program)\n            >>> y_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[y_grad])\n            >>> print(y_grad)\n            [array([4.], dtype=float32)]\n\n            >>> paddle.incubate.autograd.disable_prim()\n            >>> paddle.disable_static()\n    \"\"\"\n    if not utils.prim_enabled():\n        raise RuntimeError('forward_grad must be running on primitiveoperators, use enable_prim to turn it on.')\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, xs_dot) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_inputs))\n    block = framework.default_main_program().current_block()\n    if any((x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and targets should exist in current block of main program.')\n    primx.orig2prim(block)\n    ad = primx.Transform(ys[0].block)\n    (_, ys_dot) = ad.linearize(xs, ys, xs_dot)\n    return ys_dot[0] if isinstance(outputs, framework.Variable) else ys_dot",
        "mutated": [
            "@framework.static_only\ndef forward_grad(outputs, inputs, grad_inputs=None):\n    if False:\n        i = 10\n    \"Forward mode of automatic differentiation.\\n\\n    Note:\\n        **ONLY available in the static graph mode and primitive operators.**\\n\\n    Args:\\n        outputs(Tensor|Sequence[Tensor]): The output tensor or tensors.\\n        inputs(Tensor|Sequence[Tensor]): The input tensor or tensors.\\n        grad_inputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\\n            Tensors of inputs which has the same shape with inputs, Defaults to\\n            None, in this case is equivalent to all ones.\\n\\n    Returns:\\n        grad_outputs(Tensor|Sequence[Tensor]): The gradients for outputs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import numpy as np\\n            >>> import paddle\\n\\n            >>> paddle.enable_static()\\n            >>> paddle.incubate.autograd.enable_prim()\\n\\n            >>> startup_program = paddle.static.Program()\\n            >>> main_program = paddle.static.Program()\\n\\n            >>> with paddle.static.program_guard(main_program, startup_program):\\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\\n            ...     y = x * x\\n            ...     y_grad = paddle.incubate.autograd.forward_grad(y, x)\\n            ...     paddle.incubate.autograd.prim2orig()\\n            ...\\n            >>> exe = paddle.static.Executor()\\n            >>> exe.run(startup_program)\\n            >>> y_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[y_grad])\\n            >>> print(y_grad)\\n            [array([4.], dtype=float32)]\\n\\n            >>> paddle.incubate.autograd.disable_prim()\\n            >>> paddle.disable_static()\\n    \"\n    if not utils.prim_enabled():\n        raise RuntimeError('forward_grad must be running on primitiveoperators, use enable_prim to turn it on.')\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, xs_dot) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_inputs))\n    block = framework.default_main_program().current_block()\n    if any((x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and targets should exist in current block of main program.')\n    primx.orig2prim(block)\n    ad = primx.Transform(ys[0].block)\n    (_, ys_dot) = ad.linearize(xs, ys, xs_dot)\n    return ys_dot[0] if isinstance(outputs, framework.Variable) else ys_dot",
            "@framework.static_only\ndef forward_grad(outputs, inputs, grad_inputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Forward mode of automatic differentiation.\\n\\n    Note:\\n        **ONLY available in the static graph mode and primitive operators.**\\n\\n    Args:\\n        outputs(Tensor|Sequence[Tensor]): The output tensor or tensors.\\n        inputs(Tensor|Sequence[Tensor]): The input tensor or tensors.\\n        grad_inputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\\n            Tensors of inputs which has the same shape with inputs, Defaults to\\n            None, in this case is equivalent to all ones.\\n\\n    Returns:\\n        grad_outputs(Tensor|Sequence[Tensor]): The gradients for outputs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import numpy as np\\n            >>> import paddle\\n\\n            >>> paddle.enable_static()\\n            >>> paddle.incubate.autograd.enable_prim()\\n\\n            >>> startup_program = paddle.static.Program()\\n            >>> main_program = paddle.static.Program()\\n\\n            >>> with paddle.static.program_guard(main_program, startup_program):\\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\\n            ...     y = x * x\\n            ...     y_grad = paddle.incubate.autograd.forward_grad(y, x)\\n            ...     paddle.incubate.autograd.prim2orig()\\n            ...\\n            >>> exe = paddle.static.Executor()\\n            >>> exe.run(startup_program)\\n            >>> y_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[y_grad])\\n            >>> print(y_grad)\\n            [array([4.], dtype=float32)]\\n\\n            >>> paddle.incubate.autograd.disable_prim()\\n            >>> paddle.disable_static()\\n    \"\n    if not utils.prim_enabled():\n        raise RuntimeError('forward_grad must be running on primitiveoperators, use enable_prim to turn it on.')\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, xs_dot) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_inputs))\n    block = framework.default_main_program().current_block()\n    if any((x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and targets should exist in current block of main program.')\n    primx.orig2prim(block)\n    ad = primx.Transform(ys[0].block)\n    (_, ys_dot) = ad.linearize(xs, ys, xs_dot)\n    return ys_dot[0] if isinstance(outputs, framework.Variable) else ys_dot",
            "@framework.static_only\ndef forward_grad(outputs, inputs, grad_inputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Forward mode of automatic differentiation.\\n\\n    Note:\\n        **ONLY available in the static graph mode and primitive operators.**\\n\\n    Args:\\n        outputs(Tensor|Sequence[Tensor]): The output tensor or tensors.\\n        inputs(Tensor|Sequence[Tensor]): The input tensor or tensors.\\n        grad_inputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\\n            Tensors of inputs which has the same shape with inputs, Defaults to\\n            None, in this case is equivalent to all ones.\\n\\n    Returns:\\n        grad_outputs(Tensor|Sequence[Tensor]): The gradients for outputs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import numpy as np\\n            >>> import paddle\\n\\n            >>> paddle.enable_static()\\n            >>> paddle.incubate.autograd.enable_prim()\\n\\n            >>> startup_program = paddle.static.Program()\\n            >>> main_program = paddle.static.Program()\\n\\n            >>> with paddle.static.program_guard(main_program, startup_program):\\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\\n            ...     y = x * x\\n            ...     y_grad = paddle.incubate.autograd.forward_grad(y, x)\\n            ...     paddle.incubate.autograd.prim2orig()\\n            ...\\n            >>> exe = paddle.static.Executor()\\n            >>> exe.run(startup_program)\\n            >>> y_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[y_grad])\\n            >>> print(y_grad)\\n            [array([4.], dtype=float32)]\\n\\n            >>> paddle.incubate.autograd.disable_prim()\\n            >>> paddle.disable_static()\\n    \"\n    if not utils.prim_enabled():\n        raise RuntimeError('forward_grad must be running on primitiveoperators, use enable_prim to turn it on.')\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, xs_dot) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_inputs))\n    block = framework.default_main_program().current_block()\n    if any((x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and targets should exist in current block of main program.')\n    primx.orig2prim(block)\n    ad = primx.Transform(ys[0].block)\n    (_, ys_dot) = ad.linearize(xs, ys, xs_dot)\n    return ys_dot[0] if isinstance(outputs, framework.Variable) else ys_dot",
            "@framework.static_only\ndef forward_grad(outputs, inputs, grad_inputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Forward mode of automatic differentiation.\\n\\n    Note:\\n        **ONLY available in the static graph mode and primitive operators.**\\n\\n    Args:\\n        outputs(Tensor|Sequence[Tensor]): The output tensor or tensors.\\n        inputs(Tensor|Sequence[Tensor]): The input tensor or tensors.\\n        grad_inputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\\n            Tensors of inputs which has the same shape with inputs, Defaults to\\n            None, in this case is equivalent to all ones.\\n\\n    Returns:\\n        grad_outputs(Tensor|Sequence[Tensor]): The gradients for outputs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import numpy as np\\n            >>> import paddle\\n\\n            >>> paddle.enable_static()\\n            >>> paddle.incubate.autograd.enable_prim()\\n\\n            >>> startup_program = paddle.static.Program()\\n            >>> main_program = paddle.static.Program()\\n\\n            >>> with paddle.static.program_guard(main_program, startup_program):\\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\\n            ...     y = x * x\\n            ...     y_grad = paddle.incubate.autograd.forward_grad(y, x)\\n            ...     paddle.incubate.autograd.prim2orig()\\n            ...\\n            >>> exe = paddle.static.Executor()\\n            >>> exe.run(startup_program)\\n            >>> y_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[y_grad])\\n            >>> print(y_grad)\\n            [array([4.], dtype=float32)]\\n\\n            >>> paddle.incubate.autograd.disable_prim()\\n            >>> paddle.disable_static()\\n    \"\n    if not utils.prim_enabled():\n        raise RuntimeError('forward_grad must be running on primitiveoperators, use enable_prim to turn it on.')\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, xs_dot) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_inputs))\n    block = framework.default_main_program().current_block()\n    if any((x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and targets should exist in current block of main program.')\n    primx.orig2prim(block)\n    ad = primx.Transform(ys[0].block)\n    (_, ys_dot) = ad.linearize(xs, ys, xs_dot)\n    return ys_dot[0] if isinstance(outputs, framework.Variable) else ys_dot",
            "@framework.static_only\ndef forward_grad(outputs, inputs, grad_inputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Forward mode of automatic differentiation.\\n\\n    Note:\\n        **ONLY available in the static graph mode and primitive operators.**\\n\\n    Args:\\n        outputs(Tensor|Sequence[Tensor]): The output tensor or tensors.\\n        inputs(Tensor|Sequence[Tensor]): The input tensor or tensors.\\n        grad_inputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\\n            Tensors of inputs which has the same shape with inputs, Defaults to\\n            None, in this case is equivalent to all ones.\\n\\n    Returns:\\n        grad_outputs(Tensor|Sequence[Tensor]): The gradients for outputs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import numpy as np\\n            >>> import paddle\\n\\n            >>> paddle.enable_static()\\n            >>> paddle.incubate.autograd.enable_prim()\\n\\n            >>> startup_program = paddle.static.Program()\\n            >>> main_program = paddle.static.Program()\\n\\n            >>> with paddle.static.program_guard(main_program, startup_program):\\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\\n            ...     y = x * x\\n            ...     y_grad = paddle.incubate.autograd.forward_grad(y, x)\\n            ...     paddle.incubate.autograd.prim2orig()\\n            ...\\n            >>> exe = paddle.static.Executor()\\n            >>> exe.run(startup_program)\\n            >>> y_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[y_grad])\\n            >>> print(y_grad)\\n            [array([4.], dtype=float32)]\\n\\n            >>> paddle.incubate.autograd.disable_prim()\\n            >>> paddle.disable_static()\\n    \"\n    if not utils.prim_enabled():\n        raise RuntimeError('forward_grad must be running on primitiveoperators, use enable_prim to turn it on.')\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, xs_dot) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_inputs))\n    block = framework.default_main_program().current_block()\n    if any((x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and targets should exist in current block of main program.')\n    primx.orig2prim(block)\n    ad = primx.Transform(ys[0].block)\n    (_, ys_dot) = ad.linearize(xs, ys, xs_dot)\n    return ys_dot[0] if isinstance(outputs, framework.Variable) else ys_dot"
        ]
    },
    {
        "func_name": "grad",
        "original": "@framework.static_only\ndef grad(outputs, inputs, grad_outputs=None):\n    \"\"\"Reverse mode of automatic differentiation.\n\n    Note:\n        **ONLY available in the static graph mode and primitive operators**\n\n    Args:\n        outputs(Tensor|Sequence[Tensor]): The output Tensor or Tensors.\n        inputs(Tensor|Sequence[Tensor]): The input Tensor or Tensors.\n        grad_outputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\n            Tensors of outputs which has the same shape with outputs, Defaults\n            to None, in this case is equivalent to all ones.\n\n    Returns:\n        grad_inputs(Tensor|Tensors): The gradients for inputs.\n\n    Examples:\n\n        .. code-block:: python\n\n            >>> import numpy as np\n            >>> import paddle\n\n            >>> paddle.enable_static()\n            >>> paddle.incubate.autograd.enable_prim()\n\n            >>> startup_program = paddle.static.Program()\n            >>> main_program = paddle.static.Program()\n            >>> with paddle.static.program_guard(main_program, startup_program):\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\n            ...     x.stop_gradients = False\n            ...     y = x * x\n            ...     x_grad = paddle.incubate.autograd.grad(y, x)\n            ...     paddle.incubate.autograd.prim2orig()\n            ...\n            >>> exe = paddle.static.Executor()\n            >>> exe.run(startup_program)\n            >>> x_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[x_grad])\n            >>> print(x_grad)\n            [array([4.], dtype=float32)]\n\n            >>> paddle.incubate.autograd.disable_prim()\n            >>> paddle.disable_static()\n    \"\"\"\n    if not utils.prim_enabled():\n        grad_inputs = backward.gradients(outputs, inputs, grad_outputs)\n        if isinstance(inputs, framework.Variable) and isinstance(grad_inputs, typing.Sequence) and (len(grad_inputs) > 0):\n            return grad_inputs[0]\n        else:\n            return grad_inputs\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, ys_bar) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_outputs))\n    block = framework.default_main_program().current_block()\n    if any((x is not None and x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and outputs should be None or in current block of main program')\n    primx.orig2prim(block)\n    ad = primx.Transform(block)\n    (xs_dot, ys_dot) = ad.linearize(xs, ys)\n    if any((var is None for var in ys_dot)):\n        raise RuntimeError('Grads cannot be computed. The given outputs does not depend on inputs')\n    (ys_bar, xs_bar) = ad.transpose(ys_dot, xs_dot, ys_bar)\n    op_indexes = []\n    for var in xs_dot:\n        if var is not None:\n            op_index = block.ops.index(var.op)\n            if op_index < 0:\n                raise ValueError(f'op_index should be greater than or equal to 0, but op_index={op_index}.')\n            op_indexes.append(op_index)\n    ad.erase_ops(sorted(op_indexes))\n    ad.erase_dots(xs_dot)\n    return xs_bar[0] if isinstance(inputs, framework.Variable) else xs_bar",
        "mutated": [
            "@framework.static_only\ndef grad(outputs, inputs, grad_outputs=None):\n    if False:\n        i = 10\n    \"Reverse mode of automatic differentiation.\\n\\n    Note:\\n        **ONLY available in the static graph mode and primitive operators**\\n\\n    Args:\\n        outputs(Tensor|Sequence[Tensor]): The output Tensor or Tensors.\\n        inputs(Tensor|Sequence[Tensor]): The input Tensor or Tensors.\\n        grad_outputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\\n            Tensors of outputs which has the same shape with outputs, Defaults\\n            to None, in this case is equivalent to all ones.\\n\\n    Returns:\\n        grad_inputs(Tensor|Tensors): The gradients for inputs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import numpy as np\\n            >>> import paddle\\n\\n            >>> paddle.enable_static()\\n            >>> paddle.incubate.autograd.enable_prim()\\n\\n            >>> startup_program = paddle.static.Program()\\n            >>> main_program = paddle.static.Program()\\n            >>> with paddle.static.program_guard(main_program, startup_program):\\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\\n            ...     x.stop_gradients = False\\n            ...     y = x * x\\n            ...     x_grad = paddle.incubate.autograd.grad(y, x)\\n            ...     paddle.incubate.autograd.prim2orig()\\n            ...\\n            >>> exe = paddle.static.Executor()\\n            >>> exe.run(startup_program)\\n            >>> x_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[x_grad])\\n            >>> print(x_grad)\\n            [array([4.], dtype=float32)]\\n\\n            >>> paddle.incubate.autograd.disable_prim()\\n            >>> paddle.disable_static()\\n    \"\n    if not utils.prim_enabled():\n        grad_inputs = backward.gradients(outputs, inputs, grad_outputs)\n        if isinstance(inputs, framework.Variable) and isinstance(grad_inputs, typing.Sequence) and (len(grad_inputs) > 0):\n            return grad_inputs[0]\n        else:\n            return grad_inputs\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, ys_bar) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_outputs))\n    block = framework.default_main_program().current_block()\n    if any((x is not None and x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and outputs should be None or in current block of main program')\n    primx.orig2prim(block)\n    ad = primx.Transform(block)\n    (xs_dot, ys_dot) = ad.linearize(xs, ys)\n    if any((var is None for var in ys_dot)):\n        raise RuntimeError('Grads cannot be computed. The given outputs does not depend on inputs')\n    (ys_bar, xs_bar) = ad.transpose(ys_dot, xs_dot, ys_bar)\n    op_indexes = []\n    for var in xs_dot:\n        if var is not None:\n            op_index = block.ops.index(var.op)\n            if op_index < 0:\n                raise ValueError(f'op_index should be greater than or equal to 0, but op_index={op_index}.')\n            op_indexes.append(op_index)\n    ad.erase_ops(sorted(op_indexes))\n    ad.erase_dots(xs_dot)\n    return xs_bar[0] if isinstance(inputs, framework.Variable) else xs_bar",
            "@framework.static_only\ndef grad(outputs, inputs, grad_outputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Reverse mode of automatic differentiation.\\n\\n    Note:\\n        **ONLY available in the static graph mode and primitive operators**\\n\\n    Args:\\n        outputs(Tensor|Sequence[Tensor]): The output Tensor or Tensors.\\n        inputs(Tensor|Sequence[Tensor]): The input Tensor or Tensors.\\n        grad_outputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\\n            Tensors of outputs which has the same shape with outputs, Defaults\\n            to None, in this case is equivalent to all ones.\\n\\n    Returns:\\n        grad_inputs(Tensor|Tensors): The gradients for inputs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import numpy as np\\n            >>> import paddle\\n\\n            >>> paddle.enable_static()\\n            >>> paddle.incubate.autograd.enable_prim()\\n\\n            >>> startup_program = paddle.static.Program()\\n            >>> main_program = paddle.static.Program()\\n            >>> with paddle.static.program_guard(main_program, startup_program):\\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\\n            ...     x.stop_gradients = False\\n            ...     y = x * x\\n            ...     x_grad = paddle.incubate.autograd.grad(y, x)\\n            ...     paddle.incubate.autograd.prim2orig()\\n            ...\\n            >>> exe = paddle.static.Executor()\\n            >>> exe.run(startup_program)\\n            >>> x_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[x_grad])\\n            >>> print(x_grad)\\n            [array([4.], dtype=float32)]\\n\\n            >>> paddle.incubate.autograd.disable_prim()\\n            >>> paddle.disable_static()\\n    \"\n    if not utils.prim_enabled():\n        grad_inputs = backward.gradients(outputs, inputs, grad_outputs)\n        if isinstance(inputs, framework.Variable) and isinstance(grad_inputs, typing.Sequence) and (len(grad_inputs) > 0):\n            return grad_inputs[0]\n        else:\n            return grad_inputs\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, ys_bar) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_outputs))\n    block = framework.default_main_program().current_block()\n    if any((x is not None and x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and outputs should be None or in current block of main program')\n    primx.orig2prim(block)\n    ad = primx.Transform(block)\n    (xs_dot, ys_dot) = ad.linearize(xs, ys)\n    if any((var is None for var in ys_dot)):\n        raise RuntimeError('Grads cannot be computed. The given outputs does not depend on inputs')\n    (ys_bar, xs_bar) = ad.transpose(ys_dot, xs_dot, ys_bar)\n    op_indexes = []\n    for var in xs_dot:\n        if var is not None:\n            op_index = block.ops.index(var.op)\n            if op_index < 0:\n                raise ValueError(f'op_index should be greater than or equal to 0, but op_index={op_index}.')\n            op_indexes.append(op_index)\n    ad.erase_ops(sorted(op_indexes))\n    ad.erase_dots(xs_dot)\n    return xs_bar[0] if isinstance(inputs, framework.Variable) else xs_bar",
            "@framework.static_only\ndef grad(outputs, inputs, grad_outputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Reverse mode of automatic differentiation.\\n\\n    Note:\\n        **ONLY available in the static graph mode and primitive operators**\\n\\n    Args:\\n        outputs(Tensor|Sequence[Tensor]): The output Tensor or Tensors.\\n        inputs(Tensor|Sequence[Tensor]): The input Tensor or Tensors.\\n        grad_outputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\\n            Tensors of outputs which has the same shape with outputs, Defaults\\n            to None, in this case is equivalent to all ones.\\n\\n    Returns:\\n        grad_inputs(Tensor|Tensors): The gradients for inputs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import numpy as np\\n            >>> import paddle\\n\\n            >>> paddle.enable_static()\\n            >>> paddle.incubate.autograd.enable_prim()\\n\\n            >>> startup_program = paddle.static.Program()\\n            >>> main_program = paddle.static.Program()\\n            >>> with paddle.static.program_guard(main_program, startup_program):\\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\\n            ...     x.stop_gradients = False\\n            ...     y = x * x\\n            ...     x_grad = paddle.incubate.autograd.grad(y, x)\\n            ...     paddle.incubate.autograd.prim2orig()\\n            ...\\n            >>> exe = paddle.static.Executor()\\n            >>> exe.run(startup_program)\\n            >>> x_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[x_grad])\\n            >>> print(x_grad)\\n            [array([4.], dtype=float32)]\\n\\n            >>> paddle.incubate.autograd.disable_prim()\\n            >>> paddle.disable_static()\\n    \"\n    if not utils.prim_enabled():\n        grad_inputs = backward.gradients(outputs, inputs, grad_outputs)\n        if isinstance(inputs, framework.Variable) and isinstance(grad_inputs, typing.Sequence) and (len(grad_inputs) > 0):\n            return grad_inputs[0]\n        else:\n            return grad_inputs\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, ys_bar) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_outputs))\n    block = framework.default_main_program().current_block()\n    if any((x is not None and x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and outputs should be None or in current block of main program')\n    primx.orig2prim(block)\n    ad = primx.Transform(block)\n    (xs_dot, ys_dot) = ad.linearize(xs, ys)\n    if any((var is None for var in ys_dot)):\n        raise RuntimeError('Grads cannot be computed. The given outputs does not depend on inputs')\n    (ys_bar, xs_bar) = ad.transpose(ys_dot, xs_dot, ys_bar)\n    op_indexes = []\n    for var in xs_dot:\n        if var is not None:\n            op_index = block.ops.index(var.op)\n            if op_index < 0:\n                raise ValueError(f'op_index should be greater than or equal to 0, but op_index={op_index}.')\n            op_indexes.append(op_index)\n    ad.erase_ops(sorted(op_indexes))\n    ad.erase_dots(xs_dot)\n    return xs_bar[0] if isinstance(inputs, framework.Variable) else xs_bar",
            "@framework.static_only\ndef grad(outputs, inputs, grad_outputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Reverse mode of automatic differentiation.\\n\\n    Note:\\n        **ONLY available in the static graph mode and primitive operators**\\n\\n    Args:\\n        outputs(Tensor|Sequence[Tensor]): The output Tensor or Tensors.\\n        inputs(Tensor|Sequence[Tensor]): The input Tensor or Tensors.\\n        grad_outputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\\n            Tensors of outputs which has the same shape with outputs, Defaults\\n            to None, in this case is equivalent to all ones.\\n\\n    Returns:\\n        grad_inputs(Tensor|Tensors): The gradients for inputs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import numpy as np\\n            >>> import paddle\\n\\n            >>> paddle.enable_static()\\n            >>> paddle.incubate.autograd.enable_prim()\\n\\n            >>> startup_program = paddle.static.Program()\\n            >>> main_program = paddle.static.Program()\\n            >>> with paddle.static.program_guard(main_program, startup_program):\\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\\n            ...     x.stop_gradients = False\\n            ...     y = x * x\\n            ...     x_grad = paddle.incubate.autograd.grad(y, x)\\n            ...     paddle.incubate.autograd.prim2orig()\\n            ...\\n            >>> exe = paddle.static.Executor()\\n            >>> exe.run(startup_program)\\n            >>> x_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[x_grad])\\n            >>> print(x_grad)\\n            [array([4.], dtype=float32)]\\n\\n            >>> paddle.incubate.autograd.disable_prim()\\n            >>> paddle.disable_static()\\n    \"\n    if not utils.prim_enabled():\n        grad_inputs = backward.gradients(outputs, inputs, grad_outputs)\n        if isinstance(inputs, framework.Variable) and isinstance(grad_inputs, typing.Sequence) and (len(grad_inputs) > 0):\n            return grad_inputs[0]\n        else:\n            return grad_inputs\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, ys_bar) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_outputs))\n    block = framework.default_main_program().current_block()\n    if any((x is not None and x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and outputs should be None or in current block of main program')\n    primx.orig2prim(block)\n    ad = primx.Transform(block)\n    (xs_dot, ys_dot) = ad.linearize(xs, ys)\n    if any((var is None for var in ys_dot)):\n        raise RuntimeError('Grads cannot be computed. The given outputs does not depend on inputs')\n    (ys_bar, xs_bar) = ad.transpose(ys_dot, xs_dot, ys_bar)\n    op_indexes = []\n    for var in xs_dot:\n        if var is not None:\n            op_index = block.ops.index(var.op)\n            if op_index < 0:\n                raise ValueError(f'op_index should be greater than or equal to 0, but op_index={op_index}.')\n            op_indexes.append(op_index)\n    ad.erase_ops(sorted(op_indexes))\n    ad.erase_dots(xs_dot)\n    return xs_bar[0] if isinstance(inputs, framework.Variable) else xs_bar",
            "@framework.static_only\ndef grad(outputs, inputs, grad_outputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Reverse mode of automatic differentiation.\\n\\n    Note:\\n        **ONLY available in the static graph mode and primitive operators**\\n\\n    Args:\\n        outputs(Tensor|Sequence[Tensor]): The output Tensor or Tensors.\\n        inputs(Tensor|Sequence[Tensor]): The input Tensor or Tensors.\\n        grad_outputs(Tensor|Sequence[Tensor]): Optional, the gradient Tensor or\\n            Tensors of outputs which has the same shape with outputs, Defaults\\n            to None, in this case is equivalent to all ones.\\n\\n    Returns:\\n        grad_inputs(Tensor|Tensors): The gradients for inputs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import numpy as np\\n            >>> import paddle\\n\\n            >>> paddle.enable_static()\\n            >>> paddle.incubate.autograd.enable_prim()\\n\\n            >>> startup_program = paddle.static.Program()\\n            >>> main_program = paddle.static.Program()\\n            >>> with paddle.static.program_guard(main_program, startup_program):\\n            ...     x = paddle.static.data('x', shape=[1], dtype='float32')\\n            ...     x.stop_gradients = False\\n            ...     y = x * x\\n            ...     x_grad = paddle.incubate.autograd.grad(y, x)\\n            ...     paddle.incubate.autograd.prim2orig()\\n            ...\\n            >>> exe = paddle.static.Executor()\\n            >>> exe.run(startup_program)\\n            >>> x_grad = exe.run(main_program, feed={'x': np.array([2.]).astype('float32')}, fetch_list=[x_grad])\\n            >>> print(x_grad)\\n            [array([4.], dtype=float32)]\\n\\n            >>> paddle.incubate.autograd.disable_prim()\\n            >>> paddle.disable_static()\\n    \"\n    if not utils.prim_enabled():\n        grad_inputs = backward.gradients(outputs, inputs, grad_outputs)\n        if isinstance(inputs, framework.Variable) and isinstance(grad_inputs, typing.Sequence) and (len(grad_inputs) > 0):\n            return grad_inputs[0]\n        else:\n            return grad_inputs\n    if not isinstance(outputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected outputs is Tensor|Sequence[Tesnor], but got {type(outputs)}.')\n    if not isinstance(inputs, (framework.Variable, typing.Sequence)):\n        raise TypeError(f'Expected inputs is Tensor|Sequence[Tesnor], but got {type(inputs)}.')\n    (ys, xs, ys_bar) = (utils.as_tensors(outputs), utils.as_tensors(inputs), utils.as_tensors(grad_outputs))\n    block = framework.default_main_program().current_block()\n    if any((x is not None and x.block != block for x in xs + ys)):\n        raise RuntimeError('Variable in inputs and outputs should be None or in current block of main program')\n    primx.orig2prim(block)\n    ad = primx.Transform(block)\n    (xs_dot, ys_dot) = ad.linearize(xs, ys)\n    if any((var is None for var in ys_dot)):\n        raise RuntimeError('Grads cannot be computed. The given outputs does not depend on inputs')\n    (ys_bar, xs_bar) = ad.transpose(ys_dot, xs_dot, ys_bar)\n    op_indexes = []\n    for var in xs_dot:\n        if var is not None:\n            op_index = block.ops.index(var.op)\n            if op_index < 0:\n                raise ValueError(f'op_index should be greater than or equal to 0, but op_index={op_index}.')\n            op_indexes.append(op_index)\n    ad.erase_ops(sorted(op_indexes))\n    ad.erase_dots(xs_dot)\n    return xs_bar[0] if isinstance(inputs, framework.Variable) else xs_bar"
        ]
    },
    {
        "func_name": "to_prim",
        "original": "@framework.static_only\ndef to_prim(blocks, blacklist=frozenset(), whitelist=frozenset(), start_idx=-1, backward_length=-1):\n    \"\"\"Search nonbasic ops which have be registered composite rules and replace them with primitive ops.\n    The operators in blacklist will be excluded from program when lowering into primitives, and only the\n    operators in whitelist will be lowering. The priority of blacklist is higher than whitelist, it means\n    an operator both in blacklist and whitelist will not be lowering.\n\n    The finally set that will be lowering is:\n        (blocks.ops & ops have decomposite rule & whitelist) - blacklist\n\n    Args:\n        blacklist(frozenset): The Operators that will be exclude when lowering into primitives.\n        whitelist(frozenset): Only the operators in whitelist will be lowering into primitives.\n        start_idx(int): If start_idx exceeds -1, ops[start_idx:] will be processed. Default: -1.\n        backward_length(int): If backward_length exceeds -1, ops[:-backward_length] will be processed. Default: -1.\n    \"\"\"\n    if not core._is_fwd_prim_enabled():\n        return\n    if isinstance(blocks, paddle.base.framework.Block):\n        logging.info('Atomize composite op to primitive ops begin.')\n        main_program = blocks.program\n    elif isinstance(blocks, typing.Sequence):\n        for item in blocks:\n            if not isinstance(item, paddle.base.framework.Block):\n                raise TypeError(f'Expect block or sequence of blocks, but sequence contains {type(item)}.')\n        main_program = blocks[0].program\n    else:\n        raise TypeError(f'Expect block or sequence of blocks, but got {type(blocks)}.')\n    if not isinstance(blacklist, (set, frozenset)):\n        raise TypeError(f'Expected type of blacklisst is set|frozenset, but got {type(blacklist)}.')\n    if not isinstance(whitelist, (set, frozenset)):\n        raise TypeError(f'Expected type of whiltelist is set|frozenset, but got {type(whitelist)}.')\n    blacklist = prim_config['forward_blacklist'] | blacklist\n    with framework.program_guard(main_program):\n        logging.info('Lowering composite forward ops begin...')\n        if len(blacklist) > 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist and x.type not in blacklist\n        elif len(blacklist) > 0 and len(whitelist) == 0:\n            filter_ = lambda x: x.type not in blacklist\n        elif len(blacklist) == 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist\n        else:\n            filter_ = lambda x: True\n        primx._lower_composite(blocks, filter_, start_idx=start_idx, backward_length=backward_length)\n        replace_ops = prim_config['composite_ops_record']\n        logging.info(f'Lowering composite forward ops finish: {replace_ops}')",
        "mutated": [
            "@framework.static_only\ndef to_prim(blocks, blacklist=frozenset(), whitelist=frozenset(), start_idx=-1, backward_length=-1):\n    if False:\n        i = 10\n    'Search nonbasic ops which have be registered composite rules and replace them with primitive ops.\\n    The operators in blacklist will be excluded from program when lowering into primitives, and only the\\n    operators in whitelist will be lowering. The priority of blacklist is higher than whitelist, it means\\n    an operator both in blacklist and whitelist will not be lowering.\\n\\n    The finally set that will be lowering is:\\n        (blocks.ops & ops have decomposite rule & whitelist) - blacklist\\n\\n    Args:\\n        blacklist(frozenset): The Operators that will be exclude when lowering into primitives.\\n        whitelist(frozenset): Only the operators in whitelist will be lowering into primitives.\\n        start_idx(int): If start_idx exceeds -1, ops[start_idx:] will be processed. Default: -1.\\n        backward_length(int): If backward_length exceeds -1, ops[:-backward_length] will be processed. Default: -1.\\n    '\n    if not core._is_fwd_prim_enabled():\n        return\n    if isinstance(blocks, paddle.base.framework.Block):\n        logging.info('Atomize composite op to primitive ops begin.')\n        main_program = blocks.program\n    elif isinstance(blocks, typing.Sequence):\n        for item in blocks:\n            if not isinstance(item, paddle.base.framework.Block):\n                raise TypeError(f'Expect block or sequence of blocks, but sequence contains {type(item)}.')\n        main_program = blocks[0].program\n    else:\n        raise TypeError(f'Expect block or sequence of blocks, but got {type(blocks)}.')\n    if not isinstance(blacklist, (set, frozenset)):\n        raise TypeError(f'Expected type of blacklisst is set|frozenset, but got {type(blacklist)}.')\n    if not isinstance(whitelist, (set, frozenset)):\n        raise TypeError(f'Expected type of whiltelist is set|frozenset, but got {type(whitelist)}.')\n    blacklist = prim_config['forward_blacklist'] | blacklist\n    with framework.program_guard(main_program):\n        logging.info('Lowering composite forward ops begin...')\n        if len(blacklist) > 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist and x.type not in blacklist\n        elif len(blacklist) > 0 and len(whitelist) == 0:\n            filter_ = lambda x: x.type not in blacklist\n        elif len(blacklist) == 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist\n        else:\n            filter_ = lambda x: True\n        primx._lower_composite(blocks, filter_, start_idx=start_idx, backward_length=backward_length)\n        replace_ops = prim_config['composite_ops_record']\n        logging.info(f'Lowering composite forward ops finish: {replace_ops}')",
            "@framework.static_only\ndef to_prim(blocks, blacklist=frozenset(), whitelist=frozenset(), start_idx=-1, backward_length=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Search nonbasic ops which have be registered composite rules and replace them with primitive ops.\\n    The operators in blacklist will be excluded from program when lowering into primitives, and only the\\n    operators in whitelist will be lowering. The priority of blacklist is higher than whitelist, it means\\n    an operator both in blacklist and whitelist will not be lowering.\\n\\n    The finally set that will be lowering is:\\n        (blocks.ops & ops have decomposite rule & whitelist) - blacklist\\n\\n    Args:\\n        blacklist(frozenset): The Operators that will be exclude when lowering into primitives.\\n        whitelist(frozenset): Only the operators in whitelist will be lowering into primitives.\\n        start_idx(int): If start_idx exceeds -1, ops[start_idx:] will be processed. Default: -1.\\n        backward_length(int): If backward_length exceeds -1, ops[:-backward_length] will be processed. Default: -1.\\n    '\n    if not core._is_fwd_prim_enabled():\n        return\n    if isinstance(blocks, paddle.base.framework.Block):\n        logging.info('Atomize composite op to primitive ops begin.')\n        main_program = blocks.program\n    elif isinstance(blocks, typing.Sequence):\n        for item in blocks:\n            if not isinstance(item, paddle.base.framework.Block):\n                raise TypeError(f'Expect block or sequence of blocks, but sequence contains {type(item)}.')\n        main_program = blocks[0].program\n    else:\n        raise TypeError(f'Expect block or sequence of blocks, but got {type(blocks)}.')\n    if not isinstance(blacklist, (set, frozenset)):\n        raise TypeError(f'Expected type of blacklisst is set|frozenset, but got {type(blacklist)}.')\n    if not isinstance(whitelist, (set, frozenset)):\n        raise TypeError(f'Expected type of whiltelist is set|frozenset, but got {type(whitelist)}.')\n    blacklist = prim_config['forward_blacklist'] | blacklist\n    with framework.program_guard(main_program):\n        logging.info('Lowering composite forward ops begin...')\n        if len(blacklist) > 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist and x.type not in blacklist\n        elif len(blacklist) > 0 and len(whitelist) == 0:\n            filter_ = lambda x: x.type not in blacklist\n        elif len(blacklist) == 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist\n        else:\n            filter_ = lambda x: True\n        primx._lower_composite(blocks, filter_, start_idx=start_idx, backward_length=backward_length)\n        replace_ops = prim_config['composite_ops_record']\n        logging.info(f'Lowering composite forward ops finish: {replace_ops}')",
            "@framework.static_only\ndef to_prim(blocks, blacklist=frozenset(), whitelist=frozenset(), start_idx=-1, backward_length=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Search nonbasic ops which have be registered composite rules and replace them with primitive ops.\\n    The operators in blacklist will be excluded from program when lowering into primitives, and only the\\n    operators in whitelist will be lowering. The priority of blacklist is higher than whitelist, it means\\n    an operator both in blacklist and whitelist will not be lowering.\\n\\n    The finally set that will be lowering is:\\n        (blocks.ops & ops have decomposite rule & whitelist) - blacklist\\n\\n    Args:\\n        blacklist(frozenset): The Operators that will be exclude when lowering into primitives.\\n        whitelist(frozenset): Only the operators in whitelist will be lowering into primitives.\\n        start_idx(int): If start_idx exceeds -1, ops[start_idx:] will be processed. Default: -1.\\n        backward_length(int): If backward_length exceeds -1, ops[:-backward_length] will be processed. Default: -1.\\n    '\n    if not core._is_fwd_prim_enabled():\n        return\n    if isinstance(blocks, paddle.base.framework.Block):\n        logging.info('Atomize composite op to primitive ops begin.')\n        main_program = blocks.program\n    elif isinstance(blocks, typing.Sequence):\n        for item in blocks:\n            if not isinstance(item, paddle.base.framework.Block):\n                raise TypeError(f'Expect block or sequence of blocks, but sequence contains {type(item)}.')\n        main_program = blocks[0].program\n    else:\n        raise TypeError(f'Expect block or sequence of blocks, but got {type(blocks)}.')\n    if not isinstance(blacklist, (set, frozenset)):\n        raise TypeError(f'Expected type of blacklisst is set|frozenset, but got {type(blacklist)}.')\n    if not isinstance(whitelist, (set, frozenset)):\n        raise TypeError(f'Expected type of whiltelist is set|frozenset, but got {type(whitelist)}.')\n    blacklist = prim_config['forward_blacklist'] | blacklist\n    with framework.program_guard(main_program):\n        logging.info('Lowering composite forward ops begin...')\n        if len(blacklist) > 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist and x.type not in blacklist\n        elif len(blacklist) > 0 and len(whitelist) == 0:\n            filter_ = lambda x: x.type not in blacklist\n        elif len(blacklist) == 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist\n        else:\n            filter_ = lambda x: True\n        primx._lower_composite(blocks, filter_, start_idx=start_idx, backward_length=backward_length)\n        replace_ops = prim_config['composite_ops_record']\n        logging.info(f'Lowering composite forward ops finish: {replace_ops}')",
            "@framework.static_only\ndef to_prim(blocks, blacklist=frozenset(), whitelist=frozenset(), start_idx=-1, backward_length=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Search nonbasic ops which have be registered composite rules and replace them with primitive ops.\\n    The operators in blacklist will be excluded from program when lowering into primitives, and only the\\n    operators in whitelist will be lowering. The priority of blacklist is higher than whitelist, it means\\n    an operator both in blacklist and whitelist will not be lowering.\\n\\n    The finally set that will be lowering is:\\n        (blocks.ops & ops have decomposite rule & whitelist) - blacklist\\n\\n    Args:\\n        blacklist(frozenset): The Operators that will be exclude when lowering into primitives.\\n        whitelist(frozenset): Only the operators in whitelist will be lowering into primitives.\\n        start_idx(int): If start_idx exceeds -1, ops[start_idx:] will be processed. Default: -1.\\n        backward_length(int): If backward_length exceeds -1, ops[:-backward_length] will be processed. Default: -1.\\n    '\n    if not core._is_fwd_prim_enabled():\n        return\n    if isinstance(blocks, paddle.base.framework.Block):\n        logging.info('Atomize composite op to primitive ops begin.')\n        main_program = blocks.program\n    elif isinstance(blocks, typing.Sequence):\n        for item in blocks:\n            if not isinstance(item, paddle.base.framework.Block):\n                raise TypeError(f'Expect block or sequence of blocks, but sequence contains {type(item)}.')\n        main_program = blocks[0].program\n    else:\n        raise TypeError(f'Expect block or sequence of blocks, but got {type(blocks)}.')\n    if not isinstance(blacklist, (set, frozenset)):\n        raise TypeError(f'Expected type of blacklisst is set|frozenset, but got {type(blacklist)}.')\n    if not isinstance(whitelist, (set, frozenset)):\n        raise TypeError(f'Expected type of whiltelist is set|frozenset, but got {type(whitelist)}.')\n    blacklist = prim_config['forward_blacklist'] | blacklist\n    with framework.program_guard(main_program):\n        logging.info('Lowering composite forward ops begin...')\n        if len(blacklist) > 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist and x.type not in blacklist\n        elif len(blacklist) > 0 and len(whitelist) == 0:\n            filter_ = lambda x: x.type not in blacklist\n        elif len(blacklist) == 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist\n        else:\n            filter_ = lambda x: True\n        primx._lower_composite(blocks, filter_, start_idx=start_idx, backward_length=backward_length)\n        replace_ops = prim_config['composite_ops_record']\n        logging.info(f'Lowering composite forward ops finish: {replace_ops}')",
            "@framework.static_only\ndef to_prim(blocks, blacklist=frozenset(), whitelist=frozenset(), start_idx=-1, backward_length=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Search nonbasic ops which have be registered composite rules and replace them with primitive ops.\\n    The operators in blacklist will be excluded from program when lowering into primitives, and only the\\n    operators in whitelist will be lowering. The priority of blacklist is higher than whitelist, it means\\n    an operator both in blacklist and whitelist will not be lowering.\\n\\n    The finally set that will be lowering is:\\n        (blocks.ops & ops have decomposite rule & whitelist) - blacklist\\n\\n    Args:\\n        blacklist(frozenset): The Operators that will be exclude when lowering into primitives.\\n        whitelist(frozenset): Only the operators in whitelist will be lowering into primitives.\\n        start_idx(int): If start_idx exceeds -1, ops[start_idx:] will be processed. Default: -1.\\n        backward_length(int): If backward_length exceeds -1, ops[:-backward_length] will be processed. Default: -1.\\n    '\n    if not core._is_fwd_prim_enabled():\n        return\n    if isinstance(blocks, paddle.base.framework.Block):\n        logging.info('Atomize composite op to primitive ops begin.')\n        main_program = blocks.program\n    elif isinstance(blocks, typing.Sequence):\n        for item in blocks:\n            if not isinstance(item, paddle.base.framework.Block):\n                raise TypeError(f'Expect block or sequence of blocks, but sequence contains {type(item)}.')\n        main_program = blocks[0].program\n    else:\n        raise TypeError(f'Expect block or sequence of blocks, but got {type(blocks)}.')\n    if not isinstance(blacklist, (set, frozenset)):\n        raise TypeError(f'Expected type of blacklisst is set|frozenset, but got {type(blacklist)}.')\n    if not isinstance(whitelist, (set, frozenset)):\n        raise TypeError(f'Expected type of whiltelist is set|frozenset, but got {type(whitelist)}.')\n    blacklist = prim_config['forward_blacklist'] | blacklist\n    with framework.program_guard(main_program):\n        logging.info('Lowering composite forward ops begin...')\n        if len(blacklist) > 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist and x.type not in blacklist\n        elif len(blacklist) > 0 and len(whitelist) == 0:\n            filter_ = lambda x: x.type not in blacklist\n        elif len(blacklist) == 0 and len(whitelist) > 0:\n            filter_ = lambda x: x.type in whitelist\n        else:\n            filter_ = lambda x: True\n        primx._lower_composite(blocks, filter_, start_idx=start_idx, backward_length=backward_length)\n        replace_ops = prim_config['composite_ops_record']\n        logging.info(f'Lowering composite forward ops finish: {replace_ops}')"
        ]
    }
]