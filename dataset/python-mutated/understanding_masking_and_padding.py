"""
Title: Understanding masking & padding
Authors: Scott Zhu, Francois Chollet
Date created: 2019/07/16
Last modified: 2023/06/25
Description: Complete guide to using mask-aware sequence layers in Keras.
Accelerator: None
"""
'\n## Setup\n'
import numpy as np
import keras
from keras import ops
from keras import layers
"\n## Introduction\n\n**Masking** is a way to tell sequence-processing layers that certain timesteps\nin an input are missing, and thus should be skipped when processing the data.\n\n**Padding** is a special form of masking where the masked steps are at the start or\nthe end of a sequence. Padding comes from the need to encode sequence data into\ncontiguous batches: in order to make all sequences in a batch fit a given standard\nlength, it is necessary to pad or truncate some sequences.\n\nLet's take a close look.\n"
'\n## Padding sequence data\n\nWhen processing sequence data, it is very common for individual samples to have\ndifferent lengths. Consider the following example (text tokenized as words):\n\n```\n[\n  ["Hello", "world", "!"],\n  ["How", "are", "you", "doing", "today"],\n  ["The", "weather", "will", "be", "nice", "tomorrow"],\n]\n```\n\nAfter vocabulary lookup, the data might be vectorized as integers, e.g.:\n\n```\n[\n  [71, 1331, 4231]\n  [73, 8, 3215, 55, 927],\n  [83, 91, 1, 645, 1253, 927],\n]\n```\n\nThe data is a nested list where individual samples have length 3, 5, and 6,\nrespectively. Since the input data for a deep learning model must be a single tensor\n(of shape e.g. `(batch_size, 6, vocab_size)` in this case), samples that are shorter\nthan the longest item need to be padded with some placeholder value (alternatively,\none might also truncate long samples before padding short samples).\n\nKeras provides a utility function to truncate and pad Python lists to a common length:\n`keras.utils.pad_sequences`.\n'
raw_inputs = [[711, 632, 71], [73, 8, 3215, 55, 927], [83, 91, 1, 645, 1253, 927]]
padded_inputs = keras.utils.pad_sequences(raw_inputs, padding='post')
print(padded_inputs)
'\n## Masking\n\nNow that all samples have a uniform length, the model must be informed that some part\nof the data is actually padding and should be ignored. That mechanism is **masking**.\n\nThere are three ways to introduce input masks in Keras models:\n\n- Add a `keras.layers.Masking` layer.\n- Configure a `keras.layers.Embedding` layer with `mask_zero=True`.\n- Pass a `mask` argument manually when calling layers that support this argument (e.g.\nRNN layers).\n'
'\n## Mask-generating layers: `Embedding` and `Masking`\n\nUnder the hood, these layers will create a mask tensor (2D tensor with shape `(batch,\nsequence_length)`), and attach it to the tensor output returned by the `Masking` or\n`Embedding` layer.\n'
embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)
masked_output = embedding(padded_inputs)
print(masked_output._keras_mask)
masking_layer = layers.Masking()
unmasked_embedding = ops.cast(ops.tile(ops.expand_dims(padded_inputs, axis=-1), [1, 1, 10]), dtype='float32')
masked_embedding = masking_layer(unmasked_embedding)
print(masked_embedding._keras_mask)
'\nAs you can see from the printed result, the mask is a 2D boolean tensor with shape\n`(batch_size, sequence_length)`, where each individual `False` entry indicates that\nthe corresponding timestep should be ignored during processing.\n'
'\n## Mask propagation in the Functional API and Sequential API\n\nWhen using the Functional API or the Sequential API, a mask generated by an `Embedding`\nor `Masking` layer will be propagated through the network for any layer that is\ncapable of using them (for example, RNN layers). Keras will automatically fetch the\nmask corresponding to an input and pass it to any layer that knows how to use it.\n\nFor instance, in the following Sequential model, the `LSTM` layer will automatically\nreceive a mask, which means it will ignore padded values:\n'
model = keras.Sequential([layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True), layers.LSTM(32)])
'\nThis is also the case for the following Functional API model:\n'
inputs = keras.Input(shape=(None,), dtype='int32')
x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)
outputs = layers.LSTM(32)(x)
model = keras.Model(inputs, outputs)
'\n## Passing mask tensors directly to layers\n'
'\nLayers that can handle masks (such as the `LSTM` layer) have a `mask` argument in their\n`__call__` method.\n\nMeanwhile, layers that produce a mask (e.g. `Embedding`) expose a `compute_mask(input,\nprevious_mask)` method which you can call.\n\nThus, you can pass the output of the `compute_mask()` method of a mask-producing layer\nto the `__call__` method of a mask-consuming layer, like this:\n\n'

class MyLayer(layers.Layer):

    def __init__(self, **kwargs):
        if False:
            while True:
                i = 10
        super().__init__(**kwargs)
        self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)
        self.lstm = layers.LSTM(32)

    def call(self, inputs):
        if False:
            print('Hello World!')
        x = self.embedding(inputs)
        mask = self.embedding.compute_mask(inputs)
        output = self.lstm(x, mask=mask)
        return output
layer = MyLayer()
x = np.random.random((32, 10)) * 100
x = x.astype('int32')
layer(x)
'\n## Supporting masking in your custom layers\n'
'\nSometimes, you may need to write layers that generate a mask (like `Embedding`), or\nlayers that need to modify the current mask.\n\nFor instance, any layer that produces a tensor with a different time dimension than its\ninput, such as a `Concatenate` layer that concatenates on the time dimension, will\nneed to modify the current mask so that downstream layers will be able to properly\ntake masked timesteps into account.\n\nTo do this, your layer should implement the `layer.compute_mask()` method, which\nproduces a new mask given the input and the current mask.\n\nHere is an example of a `TemporalSplit` layer that needs to modify the current mask.\n'

class TemporalSplit(keras.layers.Layer):
    """Split the input tensor into 2 tensors along the time dimension."""

    def call(self, inputs):
        if False:
            while True:
                i = 10
        return ops.split(inputs, 2, axis=1)

    def compute_mask(self, inputs, mask=None):
        if False:
            return 10
        if mask is None:
            return None
        return ops.split(mask, 2, axis=1)
(first_half, second_half) = TemporalSplit()(masked_embedding)
print(first_half._keras_mask)
print(second_half._keras_mask)
'\nHere is another example of a `CustomEmbedding` layer that is capable of generating a\nmask from input values:\n'

class CustomEmbedding(keras.layers.Layer):

    def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):
        if False:
            print('Hello World!')
        super().__init__(**kwargs)
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.mask_zero = mask_zero

    def build(self, input_shape):
        if False:
            print('Hello World!')
        self.embeddings = self.add_weight(shape=(self.input_dim, self.output_dim), initializer='random_normal', dtype='float32')

    def call(self, inputs):
        if False:
            while True:
                i = 10
        inputs = ops.cast(inputs, 'int32')
        return ops.take(self.embeddings, inputs)

    def compute_mask(self, inputs, mask=None):
        if False:
            for i in range(10):
                print('nop')
        if not self.mask_zero:
            return None
        return ops.not_equal(inputs, 0)
layer = CustomEmbedding(10, 32, mask_zero=True)
x = np.random.random((3, 10)) * 9
x = x.astype('int32')
y = layer(x)
mask = layer.compute_mask(x)
print(mask)
'\nNote: For more details about format limitations related to masking, see the \n[serialization guide](/guides/serialization_and_saving).\n'
"\n## Opting-in to mask propagation on compatible layers\n\nMost layers don't modify the time dimension, so don't need to modify the current mask.\nHowever, they may still want to be able to **propagate** the current mask, unchanged,\nto the next layer. **This is an opt-in behavior.** By default, a custom layer will\ndestroy the current mask (since the framework has no way to tell whether propagating\nthe mask is safe to do).\n\nIf you have a custom layer that does not modify the time dimension, and if you want it\nto be able to propagate the current input mask, you should set `self.supports_masking\n= True` in the layer constructor. In this case, the default behavior of\n`compute_mask()` is to just pass the current mask through.\n\nHere's an example of a layer that is whitelisted for mask propagation:\n\n"

class MyActivation(keras.layers.Layer):

    def __init__(self, **kwargs):
        if False:
            return 10
        super().__init__(**kwargs)
        self.supports_masking = True

    def call(self, inputs):
        if False:
            return 10
        return ops.relu(inputs)
'\nYou can now use this custom layer in-between a mask-generating layer (like `Embedding`)\nand a mask-consuming layer (like `LSTM`), and it will pass the mask along so that it\nreaches the mask-consuming layer.\n'
inputs = keras.Input(shape=(None,), dtype='int32')
x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)
x = MyActivation()(x)
print('Mask found:', x._keras_mask)
outputs = layers.LSTM(32)(x)
model = keras.Model(inputs, outputs)
y = model(np.random.randint(0, 5000, size=(32, 100)))
"\n## Writing layers that need mask information\n\nSome layers are mask *consumers*: they accept a `mask` argument in `call` and use it to\ndetermine whether to skip certain time steps.\n\nTo write such a layer, you can simply add a `mask=None` argument in your `call`\nsignature. The mask associated with the inputs will be passed to your layer whenever\nit is available.\n\nHere's a simple example below: a layer that computes a softmax over the time dimension\n(axis 1) of an input sequence, while discarding masked timesteps.\n"

class TemporalSoftmax(keras.layers.Layer):

    def __init__(self, **kwargs):
        if False:
            print('Hello World!')
        super().__init__(**kwargs)
        self.supports_masking = True

    def call(self, inputs, mask=None):
        if False:
            for i in range(10):
                print('nop')
        assert mask is not None
        broadcast_float_mask = ops.expand_dims(ops.cast(mask, 'float32'), -1)
        inputs_exp = ops.exp(inputs) * broadcast_float_mask
        inputs_sum = ops.sum(inputs_exp * broadcast_float_mask, axis=-1, keepdims=True)
        return inputs_exp / inputs_sum
inputs = keras.Input(shape=(None,), dtype='int32')
x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs)
x = layers.Dense(1)(x)
outputs = TemporalSoftmax()(x)
model = keras.Model(inputs, outputs)
y = model(np.random.randint(0, 10, size=(32, 100)))
'\n## Summary\n\nThat is all you need to know about padding & masking in Keras. To recap:\n\n- "Masking" is how layers are able to know when to skip / ignore certain timesteps in\nsequence inputs.\n- Some layers are mask-generators: `Embedding` can generate a mask from input values\n(if `mask_zero=True`), and so can the `Masking` layer.\n- Some layers are mask-consumers: they expose a `mask` argument in their `__call__`\nmethod. This is the case for RNN layers.\n- In the Functional API and Sequential API, mask information is propagated\nautomatically.\n- When using layers in a standalone way, you can pass the `mask` arguments to layers\nmanually.\n- You can easily write layers that modify the current mask, that generate a new mask,\nor that consume the mask associated with the inputs.\n'