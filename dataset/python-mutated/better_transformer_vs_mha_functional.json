[
    {
        "func_name": "benchmark_torch_function",
        "original": "def benchmark_torch_function(iters, f, *args, **kwargs):\n    f(*args, **kwargs)\n    f(*args, **kwargs)\n    torch.cuda.synchronize()\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    start_event.record()\n    for _ in range(iters):\n        f(*args, **kwargs)\n    end_event.record()\n    torch.cuda.synchronize()\n    return (start_event.elapsed_time(end_event) * 1000 / iters, *f(*args, **kwargs))",
        "mutated": [
            "def benchmark_torch_function(iters, f, *args, **kwargs):\n    if False:\n        i = 10\n    f(*args, **kwargs)\n    f(*args, **kwargs)\n    torch.cuda.synchronize()\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    start_event.record()\n    for _ in range(iters):\n        f(*args, **kwargs)\n    end_event.record()\n    torch.cuda.synchronize()\n    return (start_event.elapsed_time(end_event) * 1000 / iters, *f(*args, **kwargs))",
            "def benchmark_torch_function(iters, f, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    f(*args, **kwargs)\n    f(*args, **kwargs)\n    torch.cuda.synchronize()\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    start_event.record()\n    for _ in range(iters):\n        f(*args, **kwargs)\n    end_event.record()\n    torch.cuda.synchronize()\n    return (start_event.elapsed_time(end_event) * 1000 / iters, *f(*args, **kwargs))",
            "def benchmark_torch_function(iters, f, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    f(*args, **kwargs)\n    f(*args, **kwargs)\n    torch.cuda.synchronize()\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    start_event.record()\n    for _ in range(iters):\n        f(*args, **kwargs)\n    end_event.record()\n    torch.cuda.synchronize()\n    return (start_event.elapsed_time(end_event) * 1000 / iters, *f(*args, **kwargs))",
            "def benchmark_torch_function(iters, f, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    f(*args, **kwargs)\n    f(*args, **kwargs)\n    torch.cuda.synchronize()\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    start_event.record()\n    for _ in range(iters):\n        f(*args, **kwargs)\n    end_event.record()\n    torch.cuda.synchronize()\n    return (start_event.elapsed_time(end_event) * 1000 / iters, *f(*args, **kwargs))",
            "def benchmark_torch_function(iters, f, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    f(*args, **kwargs)\n    f(*args, **kwargs)\n    torch.cuda.synchronize()\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    start_event.record()\n    for _ in range(iters):\n        f(*args, **kwargs)\n    end_event.record()\n    torch.cuda.synchronize()\n    return (start_event.elapsed_time(end_event) * 1000 / iters, *f(*args, **kwargs))"
        ]
    },
    {
        "func_name": "run",
        "original": "def run(a: int, b: int, iters: int, batch_size: int, sequence_length: int, embed_dim: int, num_heads: int, device: str, dtype: str, block_size: int, seed):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    from scipy.stats import beta\n    lengths = beta.rvs(a, b, size=batch_size) * (sequence_length + block_size - 1) // block_size\n    lengths = list(map(int, list(lengths)))\n    lengths = [l * block_size for l in lengths]\n    lengths = [max(l, block_size) for l in lengths]\n    lengths[random.randint(0, batch_size - 1)] = sequence_length\n    q = [torch.randn(l, embed_dim, device=device, dtype=dtype) for l in lengths]\n    q = torch.nested.nested_tensor(q, device=device, dtype=dtype)\n    (k, v) = (q, q)\n    qkv = torch.nn.Linear(embed_dim, 3 * embed_dim, device=device, dtype=dtype)\n    proj = torch.nn.Linear(embed_dim, embed_dim, device=device, dtype=dtype)\n    native_mha = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, device=device, dtype=dtype).eval()\n    native_mha.in_proj_weight = qkv.weight\n    native_mha.in_proj_bias = qkv.bias\n    native_mha.out_proj.weight = proj.weight\n    native_mha.out_proj.bias = proj.bias\n    q_mask = torch.nested.to_padded_tensor(torch.nested.nested_tensor([torch.tensor([True] * length, dtype=torch.bool) for length in lengths]), 0)\n    q_mask = q_mask.cuda()\n    if q_mask.size(1) == 0:\n        return None\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True):\n        with torch.inference_mode():\n            (time_native_mha_fast, y_native_mha_fast, _) = benchmark_torch_function(iters, native_mha, q, k, v, need_weights=False)\n    q = q.to_padded_tensor(0)\n    k = q\n    v = q\n    (time_native_mha_slow, y_native_mha_slow, _) = benchmark_torch_function(iters, native_mha, q, k, v, key_padding_mask=~q_mask, need_weights=False)\n    if y_native_mha_fast.is_nested:\n        y_native_mha_fast = torch.nested.to_padded_tensor(y_native_mha_fast, 0)\n    y_native_mha_fast = y_native_mha_fast * q_mask.unsqueeze(-1)\n    if y_native_mha_slow.is_nested:\n        y_native_mha_slow = torch.nested.to_padded_tensor(y_native_mha_slow, 0)\n    y_native_mha_slow = y_native_mha_slow * q_mask.unsqueeze(-1)\n    entry_name = f'batch:{batch_size}_seq_len:{sequence_length}_n_heads:{num_heads}_embed_dim:{embed_dim}'\n    try:\n        torch.testing.assert_close(y_native_mha_fast, y_native_mha_slow, atol=0.001, rtol=0.001)\n    except AssertionError as e:\n        error_dict[entry_name] += 1\n        pprint(error_dict)\n    padding = 1 - q_mask.float().mean().item()\n    speedup_fast_internal = time_native_mha_slow / time_native_mha_fast\n    result_entry = OrderedDict()\n    result_entry['dtype'] = dtype\n    result_entry['batch_size'] = batch_size\n    result_entry['sequence_length'] = sequence_length\n    result_entry['n_heads'] = num_heads\n    result_entry['embed_dim'] = embed_dim\n    result_entry['time_native_mha_slow(\u03bcs)'] = f'{time_native_mha_slow:.3f}'\n    result_entry['time_native_mha_fast (\u03bcs)'] = f'{time_native_mha_fast:.3f}'\n    result_entry['speedup flash_mha v native_mha'] = f'{speedup_fast_internal:.3f}'\n    result_entry['padding'] = f'{padding:.3f}'\n    return result_entry",
        "mutated": [
            "def run(a: int, b: int, iters: int, batch_size: int, sequence_length: int, embed_dim: int, num_heads: int, device: str, dtype: str, block_size: int, seed):\n    if False:\n        i = 10\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    from scipy.stats import beta\n    lengths = beta.rvs(a, b, size=batch_size) * (sequence_length + block_size - 1) // block_size\n    lengths = list(map(int, list(lengths)))\n    lengths = [l * block_size for l in lengths]\n    lengths = [max(l, block_size) for l in lengths]\n    lengths[random.randint(0, batch_size - 1)] = sequence_length\n    q = [torch.randn(l, embed_dim, device=device, dtype=dtype) for l in lengths]\n    q = torch.nested.nested_tensor(q, device=device, dtype=dtype)\n    (k, v) = (q, q)\n    qkv = torch.nn.Linear(embed_dim, 3 * embed_dim, device=device, dtype=dtype)\n    proj = torch.nn.Linear(embed_dim, embed_dim, device=device, dtype=dtype)\n    native_mha = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, device=device, dtype=dtype).eval()\n    native_mha.in_proj_weight = qkv.weight\n    native_mha.in_proj_bias = qkv.bias\n    native_mha.out_proj.weight = proj.weight\n    native_mha.out_proj.bias = proj.bias\n    q_mask = torch.nested.to_padded_tensor(torch.nested.nested_tensor([torch.tensor([True] * length, dtype=torch.bool) for length in lengths]), 0)\n    q_mask = q_mask.cuda()\n    if q_mask.size(1) == 0:\n        return None\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True):\n        with torch.inference_mode():\n            (time_native_mha_fast, y_native_mha_fast, _) = benchmark_torch_function(iters, native_mha, q, k, v, need_weights=False)\n    q = q.to_padded_tensor(0)\n    k = q\n    v = q\n    (time_native_mha_slow, y_native_mha_slow, _) = benchmark_torch_function(iters, native_mha, q, k, v, key_padding_mask=~q_mask, need_weights=False)\n    if y_native_mha_fast.is_nested:\n        y_native_mha_fast = torch.nested.to_padded_tensor(y_native_mha_fast, 0)\n    y_native_mha_fast = y_native_mha_fast * q_mask.unsqueeze(-1)\n    if y_native_mha_slow.is_nested:\n        y_native_mha_slow = torch.nested.to_padded_tensor(y_native_mha_slow, 0)\n    y_native_mha_slow = y_native_mha_slow * q_mask.unsqueeze(-1)\n    entry_name = f'batch:{batch_size}_seq_len:{sequence_length}_n_heads:{num_heads}_embed_dim:{embed_dim}'\n    try:\n        torch.testing.assert_close(y_native_mha_fast, y_native_mha_slow, atol=0.001, rtol=0.001)\n    except AssertionError as e:\n        error_dict[entry_name] += 1\n        pprint(error_dict)\n    padding = 1 - q_mask.float().mean().item()\n    speedup_fast_internal = time_native_mha_slow / time_native_mha_fast\n    result_entry = OrderedDict()\n    result_entry['dtype'] = dtype\n    result_entry['batch_size'] = batch_size\n    result_entry['sequence_length'] = sequence_length\n    result_entry['n_heads'] = num_heads\n    result_entry['embed_dim'] = embed_dim\n    result_entry['time_native_mha_slow(\u03bcs)'] = f'{time_native_mha_slow:.3f}'\n    result_entry['time_native_mha_fast (\u03bcs)'] = f'{time_native_mha_fast:.3f}'\n    result_entry['speedup flash_mha v native_mha'] = f'{speedup_fast_internal:.3f}'\n    result_entry['padding'] = f'{padding:.3f}'\n    return result_entry",
            "def run(a: int, b: int, iters: int, batch_size: int, sequence_length: int, embed_dim: int, num_heads: int, device: str, dtype: str, block_size: int, seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    from scipy.stats import beta\n    lengths = beta.rvs(a, b, size=batch_size) * (sequence_length + block_size - 1) // block_size\n    lengths = list(map(int, list(lengths)))\n    lengths = [l * block_size for l in lengths]\n    lengths = [max(l, block_size) for l in lengths]\n    lengths[random.randint(0, batch_size - 1)] = sequence_length\n    q = [torch.randn(l, embed_dim, device=device, dtype=dtype) for l in lengths]\n    q = torch.nested.nested_tensor(q, device=device, dtype=dtype)\n    (k, v) = (q, q)\n    qkv = torch.nn.Linear(embed_dim, 3 * embed_dim, device=device, dtype=dtype)\n    proj = torch.nn.Linear(embed_dim, embed_dim, device=device, dtype=dtype)\n    native_mha = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, device=device, dtype=dtype).eval()\n    native_mha.in_proj_weight = qkv.weight\n    native_mha.in_proj_bias = qkv.bias\n    native_mha.out_proj.weight = proj.weight\n    native_mha.out_proj.bias = proj.bias\n    q_mask = torch.nested.to_padded_tensor(torch.nested.nested_tensor([torch.tensor([True] * length, dtype=torch.bool) for length in lengths]), 0)\n    q_mask = q_mask.cuda()\n    if q_mask.size(1) == 0:\n        return None\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True):\n        with torch.inference_mode():\n            (time_native_mha_fast, y_native_mha_fast, _) = benchmark_torch_function(iters, native_mha, q, k, v, need_weights=False)\n    q = q.to_padded_tensor(0)\n    k = q\n    v = q\n    (time_native_mha_slow, y_native_mha_slow, _) = benchmark_torch_function(iters, native_mha, q, k, v, key_padding_mask=~q_mask, need_weights=False)\n    if y_native_mha_fast.is_nested:\n        y_native_mha_fast = torch.nested.to_padded_tensor(y_native_mha_fast, 0)\n    y_native_mha_fast = y_native_mha_fast * q_mask.unsqueeze(-1)\n    if y_native_mha_slow.is_nested:\n        y_native_mha_slow = torch.nested.to_padded_tensor(y_native_mha_slow, 0)\n    y_native_mha_slow = y_native_mha_slow * q_mask.unsqueeze(-1)\n    entry_name = f'batch:{batch_size}_seq_len:{sequence_length}_n_heads:{num_heads}_embed_dim:{embed_dim}'\n    try:\n        torch.testing.assert_close(y_native_mha_fast, y_native_mha_slow, atol=0.001, rtol=0.001)\n    except AssertionError as e:\n        error_dict[entry_name] += 1\n        pprint(error_dict)\n    padding = 1 - q_mask.float().mean().item()\n    speedup_fast_internal = time_native_mha_slow / time_native_mha_fast\n    result_entry = OrderedDict()\n    result_entry['dtype'] = dtype\n    result_entry['batch_size'] = batch_size\n    result_entry['sequence_length'] = sequence_length\n    result_entry['n_heads'] = num_heads\n    result_entry['embed_dim'] = embed_dim\n    result_entry['time_native_mha_slow(\u03bcs)'] = f'{time_native_mha_slow:.3f}'\n    result_entry['time_native_mha_fast (\u03bcs)'] = f'{time_native_mha_fast:.3f}'\n    result_entry['speedup flash_mha v native_mha'] = f'{speedup_fast_internal:.3f}'\n    result_entry['padding'] = f'{padding:.3f}'\n    return result_entry",
            "def run(a: int, b: int, iters: int, batch_size: int, sequence_length: int, embed_dim: int, num_heads: int, device: str, dtype: str, block_size: int, seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    from scipy.stats import beta\n    lengths = beta.rvs(a, b, size=batch_size) * (sequence_length + block_size - 1) // block_size\n    lengths = list(map(int, list(lengths)))\n    lengths = [l * block_size for l in lengths]\n    lengths = [max(l, block_size) for l in lengths]\n    lengths[random.randint(0, batch_size - 1)] = sequence_length\n    q = [torch.randn(l, embed_dim, device=device, dtype=dtype) for l in lengths]\n    q = torch.nested.nested_tensor(q, device=device, dtype=dtype)\n    (k, v) = (q, q)\n    qkv = torch.nn.Linear(embed_dim, 3 * embed_dim, device=device, dtype=dtype)\n    proj = torch.nn.Linear(embed_dim, embed_dim, device=device, dtype=dtype)\n    native_mha = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, device=device, dtype=dtype).eval()\n    native_mha.in_proj_weight = qkv.weight\n    native_mha.in_proj_bias = qkv.bias\n    native_mha.out_proj.weight = proj.weight\n    native_mha.out_proj.bias = proj.bias\n    q_mask = torch.nested.to_padded_tensor(torch.nested.nested_tensor([torch.tensor([True] * length, dtype=torch.bool) for length in lengths]), 0)\n    q_mask = q_mask.cuda()\n    if q_mask.size(1) == 0:\n        return None\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True):\n        with torch.inference_mode():\n            (time_native_mha_fast, y_native_mha_fast, _) = benchmark_torch_function(iters, native_mha, q, k, v, need_weights=False)\n    q = q.to_padded_tensor(0)\n    k = q\n    v = q\n    (time_native_mha_slow, y_native_mha_slow, _) = benchmark_torch_function(iters, native_mha, q, k, v, key_padding_mask=~q_mask, need_weights=False)\n    if y_native_mha_fast.is_nested:\n        y_native_mha_fast = torch.nested.to_padded_tensor(y_native_mha_fast, 0)\n    y_native_mha_fast = y_native_mha_fast * q_mask.unsqueeze(-1)\n    if y_native_mha_slow.is_nested:\n        y_native_mha_slow = torch.nested.to_padded_tensor(y_native_mha_slow, 0)\n    y_native_mha_slow = y_native_mha_slow * q_mask.unsqueeze(-1)\n    entry_name = f'batch:{batch_size}_seq_len:{sequence_length}_n_heads:{num_heads}_embed_dim:{embed_dim}'\n    try:\n        torch.testing.assert_close(y_native_mha_fast, y_native_mha_slow, atol=0.001, rtol=0.001)\n    except AssertionError as e:\n        error_dict[entry_name] += 1\n        pprint(error_dict)\n    padding = 1 - q_mask.float().mean().item()\n    speedup_fast_internal = time_native_mha_slow / time_native_mha_fast\n    result_entry = OrderedDict()\n    result_entry['dtype'] = dtype\n    result_entry['batch_size'] = batch_size\n    result_entry['sequence_length'] = sequence_length\n    result_entry['n_heads'] = num_heads\n    result_entry['embed_dim'] = embed_dim\n    result_entry['time_native_mha_slow(\u03bcs)'] = f'{time_native_mha_slow:.3f}'\n    result_entry['time_native_mha_fast (\u03bcs)'] = f'{time_native_mha_fast:.3f}'\n    result_entry['speedup flash_mha v native_mha'] = f'{speedup_fast_internal:.3f}'\n    result_entry['padding'] = f'{padding:.3f}'\n    return result_entry",
            "def run(a: int, b: int, iters: int, batch_size: int, sequence_length: int, embed_dim: int, num_heads: int, device: str, dtype: str, block_size: int, seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    from scipy.stats import beta\n    lengths = beta.rvs(a, b, size=batch_size) * (sequence_length + block_size - 1) // block_size\n    lengths = list(map(int, list(lengths)))\n    lengths = [l * block_size for l in lengths]\n    lengths = [max(l, block_size) for l in lengths]\n    lengths[random.randint(0, batch_size - 1)] = sequence_length\n    q = [torch.randn(l, embed_dim, device=device, dtype=dtype) for l in lengths]\n    q = torch.nested.nested_tensor(q, device=device, dtype=dtype)\n    (k, v) = (q, q)\n    qkv = torch.nn.Linear(embed_dim, 3 * embed_dim, device=device, dtype=dtype)\n    proj = torch.nn.Linear(embed_dim, embed_dim, device=device, dtype=dtype)\n    native_mha = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, device=device, dtype=dtype).eval()\n    native_mha.in_proj_weight = qkv.weight\n    native_mha.in_proj_bias = qkv.bias\n    native_mha.out_proj.weight = proj.weight\n    native_mha.out_proj.bias = proj.bias\n    q_mask = torch.nested.to_padded_tensor(torch.nested.nested_tensor([torch.tensor([True] * length, dtype=torch.bool) for length in lengths]), 0)\n    q_mask = q_mask.cuda()\n    if q_mask.size(1) == 0:\n        return None\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True):\n        with torch.inference_mode():\n            (time_native_mha_fast, y_native_mha_fast, _) = benchmark_torch_function(iters, native_mha, q, k, v, need_weights=False)\n    q = q.to_padded_tensor(0)\n    k = q\n    v = q\n    (time_native_mha_slow, y_native_mha_slow, _) = benchmark_torch_function(iters, native_mha, q, k, v, key_padding_mask=~q_mask, need_weights=False)\n    if y_native_mha_fast.is_nested:\n        y_native_mha_fast = torch.nested.to_padded_tensor(y_native_mha_fast, 0)\n    y_native_mha_fast = y_native_mha_fast * q_mask.unsqueeze(-1)\n    if y_native_mha_slow.is_nested:\n        y_native_mha_slow = torch.nested.to_padded_tensor(y_native_mha_slow, 0)\n    y_native_mha_slow = y_native_mha_slow * q_mask.unsqueeze(-1)\n    entry_name = f'batch:{batch_size}_seq_len:{sequence_length}_n_heads:{num_heads}_embed_dim:{embed_dim}'\n    try:\n        torch.testing.assert_close(y_native_mha_fast, y_native_mha_slow, atol=0.001, rtol=0.001)\n    except AssertionError as e:\n        error_dict[entry_name] += 1\n        pprint(error_dict)\n    padding = 1 - q_mask.float().mean().item()\n    speedup_fast_internal = time_native_mha_slow / time_native_mha_fast\n    result_entry = OrderedDict()\n    result_entry['dtype'] = dtype\n    result_entry['batch_size'] = batch_size\n    result_entry['sequence_length'] = sequence_length\n    result_entry['n_heads'] = num_heads\n    result_entry['embed_dim'] = embed_dim\n    result_entry['time_native_mha_slow(\u03bcs)'] = f'{time_native_mha_slow:.3f}'\n    result_entry['time_native_mha_fast (\u03bcs)'] = f'{time_native_mha_fast:.3f}'\n    result_entry['speedup flash_mha v native_mha'] = f'{speedup_fast_internal:.3f}'\n    result_entry['padding'] = f'{padding:.3f}'\n    return result_entry",
            "def run(a: int, b: int, iters: int, batch_size: int, sequence_length: int, embed_dim: int, num_heads: int, device: str, dtype: str, block_size: int, seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    from scipy.stats import beta\n    lengths = beta.rvs(a, b, size=batch_size) * (sequence_length + block_size - 1) // block_size\n    lengths = list(map(int, list(lengths)))\n    lengths = [l * block_size for l in lengths]\n    lengths = [max(l, block_size) for l in lengths]\n    lengths[random.randint(0, batch_size - 1)] = sequence_length\n    q = [torch.randn(l, embed_dim, device=device, dtype=dtype) for l in lengths]\n    q = torch.nested.nested_tensor(q, device=device, dtype=dtype)\n    (k, v) = (q, q)\n    qkv = torch.nn.Linear(embed_dim, 3 * embed_dim, device=device, dtype=dtype)\n    proj = torch.nn.Linear(embed_dim, embed_dim, device=device, dtype=dtype)\n    native_mha = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, device=device, dtype=dtype).eval()\n    native_mha.in_proj_weight = qkv.weight\n    native_mha.in_proj_bias = qkv.bias\n    native_mha.out_proj.weight = proj.weight\n    native_mha.out_proj.bias = proj.bias\n    q_mask = torch.nested.to_padded_tensor(torch.nested.nested_tensor([torch.tensor([True] * length, dtype=torch.bool) for length in lengths]), 0)\n    q_mask = q_mask.cuda()\n    if q_mask.size(1) == 0:\n        return None\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True):\n        with torch.inference_mode():\n            (time_native_mha_fast, y_native_mha_fast, _) = benchmark_torch_function(iters, native_mha, q, k, v, need_weights=False)\n    q = q.to_padded_tensor(0)\n    k = q\n    v = q\n    (time_native_mha_slow, y_native_mha_slow, _) = benchmark_torch_function(iters, native_mha, q, k, v, key_padding_mask=~q_mask, need_weights=False)\n    if y_native_mha_fast.is_nested:\n        y_native_mha_fast = torch.nested.to_padded_tensor(y_native_mha_fast, 0)\n    y_native_mha_fast = y_native_mha_fast * q_mask.unsqueeze(-1)\n    if y_native_mha_slow.is_nested:\n        y_native_mha_slow = torch.nested.to_padded_tensor(y_native_mha_slow, 0)\n    y_native_mha_slow = y_native_mha_slow * q_mask.unsqueeze(-1)\n    entry_name = f'batch:{batch_size}_seq_len:{sequence_length}_n_heads:{num_heads}_embed_dim:{embed_dim}'\n    try:\n        torch.testing.assert_close(y_native_mha_fast, y_native_mha_slow, atol=0.001, rtol=0.001)\n    except AssertionError as e:\n        error_dict[entry_name] += 1\n        pprint(error_dict)\n    padding = 1 - q_mask.float().mean().item()\n    speedup_fast_internal = time_native_mha_slow / time_native_mha_fast\n    result_entry = OrderedDict()\n    result_entry['dtype'] = dtype\n    result_entry['batch_size'] = batch_size\n    result_entry['sequence_length'] = sequence_length\n    result_entry['n_heads'] = num_heads\n    result_entry['embed_dim'] = embed_dim\n    result_entry['time_native_mha_slow(\u03bcs)'] = f'{time_native_mha_slow:.3f}'\n    result_entry['time_native_mha_fast (\u03bcs)'] = f'{time_native_mha_fast:.3f}'\n    result_entry['speedup flash_mha v native_mha'] = f'{speedup_fast_internal:.3f}'\n    result_entry['padding'] = f'{padding:.3f}'\n    return result_entry"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(save_path: Optional[Path], error_path: Optional[Path]):\n    table = PrettyTable()\n    entries = defaultdict(list)\n    print('CUDA device: ', torch.cuda.get_device_name(0))\n    iters = 100\n    header = None\n    batch_sizes = [16, 32, 64, 128, 256]\n    sequence_lengths = [64, 128, 256, 512]\n    embed_dims = [512, 1024]\n    num_heads_list = [8, 16]\n    betas = range(1, 64, 4)\n    for (batch_size, sequence_length, embed_dim, num_heads, block_size, b) in tqdm(list(itertools.product(batch_sizes, sequence_lengths, embed_dims, num_heads_list, [2], betas))):\n        seed = 26214\n        entry = run(1, b * 0.05, iters, batch_size, sequence_length, embed_dim, num_heads, 'cuda', torch.float16, block_size, seed)\n        if entry is None:\n            continue\n        if header is None:\n            table.field_names = list(entry.keys())\n            header = list(entry.keys())\n        row = []\n        for (k, v) in entry.items():\n            row.append(v)\n            entries[k].append(v)\n        table.add_row(row)\n    print(table)\n    pprint(error_dict)\n    csv_string = table.get_csv_string()\n    if save_path is not None:\n        with open(save_path, 'w') as csvfile:\n            csvfile.write(csv_string)\n    print(f'Total errors: {sum(error_dict.values())}')\n    if error_path is not None:\n        with open(error_path, 'w') as file:\n            file.write(json.dumps(error_dict))",
        "mutated": [
            "def main(save_path: Optional[Path], error_path: Optional[Path]):\n    if False:\n        i = 10\n    table = PrettyTable()\n    entries = defaultdict(list)\n    print('CUDA device: ', torch.cuda.get_device_name(0))\n    iters = 100\n    header = None\n    batch_sizes = [16, 32, 64, 128, 256]\n    sequence_lengths = [64, 128, 256, 512]\n    embed_dims = [512, 1024]\n    num_heads_list = [8, 16]\n    betas = range(1, 64, 4)\n    for (batch_size, sequence_length, embed_dim, num_heads, block_size, b) in tqdm(list(itertools.product(batch_sizes, sequence_lengths, embed_dims, num_heads_list, [2], betas))):\n        seed = 26214\n        entry = run(1, b * 0.05, iters, batch_size, sequence_length, embed_dim, num_heads, 'cuda', torch.float16, block_size, seed)\n        if entry is None:\n            continue\n        if header is None:\n            table.field_names = list(entry.keys())\n            header = list(entry.keys())\n        row = []\n        for (k, v) in entry.items():\n            row.append(v)\n            entries[k].append(v)\n        table.add_row(row)\n    print(table)\n    pprint(error_dict)\n    csv_string = table.get_csv_string()\n    if save_path is not None:\n        with open(save_path, 'w') as csvfile:\n            csvfile.write(csv_string)\n    print(f'Total errors: {sum(error_dict.values())}')\n    if error_path is not None:\n        with open(error_path, 'w') as file:\n            file.write(json.dumps(error_dict))",
            "def main(save_path: Optional[Path], error_path: Optional[Path]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    table = PrettyTable()\n    entries = defaultdict(list)\n    print('CUDA device: ', torch.cuda.get_device_name(0))\n    iters = 100\n    header = None\n    batch_sizes = [16, 32, 64, 128, 256]\n    sequence_lengths = [64, 128, 256, 512]\n    embed_dims = [512, 1024]\n    num_heads_list = [8, 16]\n    betas = range(1, 64, 4)\n    for (batch_size, sequence_length, embed_dim, num_heads, block_size, b) in tqdm(list(itertools.product(batch_sizes, sequence_lengths, embed_dims, num_heads_list, [2], betas))):\n        seed = 26214\n        entry = run(1, b * 0.05, iters, batch_size, sequence_length, embed_dim, num_heads, 'cuda', torch.float16, block_size, seed)\n        if entry is None:\n            continue\n        if header is None:\n            table.field_names = list(entry.keys())\n            header = list(entry.keys())\n        row = []\n        for (k, v) in entry.items():\n            row.append(v)\n            entries[k].append(v)\n        table.add_row(row)\n    print(table)\n    pprint(error_dict)\n    csv_string = table.get_csv_string()\n    if save_path is not None:\n        with open(save_path, 'w') as csvfile:\n            csvfile.write(csv_string)\n    print(f'Total errors: {sum(error_dict.values())}')\n    if error_path is not None:\n        with open(error_path, 'w') as file:\n            file.write(json.dumps(error_dict))",
            "def main(save_path: Optional[Path], error_path: Optional[Path]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    table = PrettyTable()\n    entries = defaultdict(list)\n    print('CUDA device: ', torch.cuda.get_device_name(0))\n    iters = 100\n    header = None\n    batch_sizes = [16, 32, 64, 128, 256]\n    sequence_lengths = [64, 128, 256, 512]\n    embed_dims = [512, 1024]\n    num_heads_list = [8, 16]\n    betas = range(1, 64, 4)\n    for (batch_size, sequence_length, embed_dim, num_heads, block_size, b) in tqdm(list(itertools.product(batch_sizes, sequence_lengths, embed_dims, num_heads_list, [2], betas))):\n        seed = 26214\n        entry = run(1, b * 0.05, iters, batch_size, sequence_length, embed_dim, num_heads, 'cuda', torch.float16, block_size, seed)\n        if entry is None:\n            continue\n        if header is None:\n            table.field_names = list(entry.keys())\n            header = list(entry.keys())\n        row = []\n        for (k, v) in entry.items():\n            row.append(v)\n            entries[k].append(v)\n        table.add_row(row)\n    print(table)\n    pprint(error_dict)\n    csv_string = table.get_csv_string()\n    if save_path is not None:\n        with open(save_path, 'w') as csvfile:\n            csvfile.write(csv_string)\n    print(f'Total errors: {sum(error_dict.values())}')\n    if error_path is not None:\n        with open(error_path, 'w') as file:\n            file.write(json.dumps(error_dict))",
            "def main(save_path: Optional[Path], error_path: Optional[Path]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    table = PrettyTable()\n    entries = defaultdict(list)\n    print('CUDA device: ', torch.cuda.get_device_name(0))\n    iters = 100\n    header = None\n    batch_sizes = [16, 32, 64, 128, 256]\n    sequence_lengths = [64, 128, 256, 512]\n    embed_dims = [512, 1024]\n    num_heads_list = [8, 16]\n    betas = range(1, 64, 4)\n    for (batch_size, sequence_length, embed_dim, num_heads, block_size, b) in tqdm(list(itertools.product(batch_sizes, sequence_lengths, embed_dims, num_heads_list, [2], betas))):\n        seed = 26214\n        entry = run(1, b * 0.05, iters, batch_size, sequence_length, embed_dim, num_heads, 'cuda', torch.float16, block_size, seed)\n        if entry is None:\n            continue\n        if header is None:\n            table.field_names = list(entry.keys())\n            header = list(entry.keys())\n        row = []\n        for (k, v) in entry.items():\n            row.append(v)\n            entries[k].append(v)\n        table.add_row(row)\n    print(table)\n    pprint(error_dict)\n    csv_string = table.get_csv_string()\n    if save_path is not None:\n        with open(save_path, 'w') as csvfile:\n            csvfile.write(csv_string)\n    print(f'Total errors: {sum(error_dict.values())}')\n    if error_path is not None:\n        with open(error_path, 'w') as file:\n            file.write(json.dumps(error_dict))",
            "def main(save_path: Optional[Path], error_path: Optional[Path]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    table = PrettyTable()\n    entries = defaultdict(list)\n    print('CUDA device: ', torch.cuda.get_device_name(0))\n    iters = 100\n    header = None\n    batch_sizes = [16, 32, 64, 128, 256]\n    sequence_lengths = [64, 128, 256, 512]\n    embed_dims = [512, 1024]\n    num_heads_list = [8, 16]\n    betas = range(1, 64, 4)\n    for (batch_size, sequence_length, embed_dim, num_heads, block_size, b) in tqdm(list(itertools.product(batch_sizes, sequence_lengths, embed_dims, num_heads_list, [2], betas))):\n        seed = 26214\n        entry = run(1, b * 0.05, iters, batch_size, sequence_length, embed_dim, num_heads, 'cuda', torch.float16, block_size, seed)\n        if entry is None:\n            continue\n        if header is None:\n            table.field_names = list(entry.keys())\n            header = list(entry.keys())\n        row = []\n        for (k, v) in entry.items():\n            row.append(v)\n            entries[k].append(v)\n        table.add_row(row)\n    print(table)\n    pprint(error_dict)\n    csv_string = table.get_csv_string()\n    if save_path is not None:\n        with open(save_path, 'w') as csvfile:\n            csvfile.write(csv_string)\n    print(f'Total errors: {sum(error_dict.values())}')\n    if error_path is not None:\n        with open(error_path, 'w') as file:\n            file.write(json.dumps(error_dict))"
        ]
    }
]