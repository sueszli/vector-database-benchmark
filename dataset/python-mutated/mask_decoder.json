[
    {
        "func_name": "__init__",
        "original": "def __init__(self, *, transformer_dim: int, transformer: Module, num_multimask_outputs: int=3, activation: type[Module]=nn.GELU, iou_head_depth: int=3, iou_head_hidden_dim: int=256) -> None:\n    \"\"\"Predicts masks given an image and prompt embeddings, using a transformer architecture.\n\n        Args:\n            transformer_dim: the channel dimension of the transformer\n            transformer: the transformer used to predict masks\n            num_multimask_outputs: the number of masks to predict when disambiguating masks\n            activation: the type of activation to use when upscaling masks\n            iou_head_depth: the depth of the MLP used to predict mask quality\n            iou_head_hidden_dim: the hidden dimension of the MLP used to predict mask quality\n        \"\"\"\n    super().__init__()\n    self.transformer_dim = transformer_dim\n    self.transformer = transformer\n    self.num_multimask_outputs = num_multimask_outputs\n    self.iou_token = nn.Embedding(1, transformer_dim)\n    self.num_mask_tokens = num_multimask_outputs + 1\n    self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n    self.output_upscaling = nn.Sequential(nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2), LayerNorm2d(transformer_dim // 4), activation(), nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2), activation())\n    self.output_hypernetworks_mlps = nn.ModuleList([MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for i in range(self.num_mask_tokens)])\n    self.iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth)",
        "mutated": [
            "def __init__(self, *, transformer_dim: int, transformer: Module, num_multimask_outputs: int=3, activation: type[Module]=nn.GELU, iou_head_depth: int=3, iou_head_hidden_dim: int=256) -> None:\n    if False:\n        i = 10\n    'Predicts masks given an image and prompt embeddings, using a transformer architecture.\\n\\n        Args:\\n            transformer_dim: the channel dimension of the transformer\\n            transformer: the transformer used to predict masks\\n            num_multimask_outputs: the number of masks to predict when disambiguating masks\\n            activation: the type of activation to use when upscaling masks\\n            iou_head_depth: the depth of the MLP used to predict mask quality\\n            iou_head_hidden_dim: the hidden dimension of the MLP used to predict mask quality\\n        '\n    super().__init__()\n    self.transformer_dim = transformer_dim\n    self.transformer = transformer\n    self.num_multimask_outputs = num_multimask_outputs\n    self.iou_token = nn.Embedding(1, transformer_dim)\n    self.num_mask_tokens = num_multimask_outputs + 1\n    self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n    self.output_upscaling = nn.Sequential(nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2), LayerNorm2d(transformer_dim // 4), activation(), nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2), activation())\n    self.output_hypernetworks_mlps = nn.ModuleList([MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for i in range(self.num_mask_tokens)])\n    self.iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth)",
            "def __init__(self, *, transformer_dim: int, transformer: Module, num_multimask_outputs: int=3, activation: type[Module]=nn.GELU, iou_head_depth: int=3, iou_head_hidden_dim: int=256) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Predicts masks given an image and prompt embeddings, using a transformer architecture.\\n\\n        Args:\\n            transformer_dim: the channel dimension of the transformer\\n            transformer: the transformer used to predict masks\\n            num_multimask_outputs: the number of masks to predict when disambiguating masks\\n            activation: the type of activation to use when upscaling masks\\n            iou_head_depth: the depth of the MLP used to predict mask quality\\n            iou_head_hidden_dim: the hidden dimension of the MLP used to predict mask quality\\n        '\n    super().__init__()\n    self.transformer_dim = transformer_dim\n    self.transformer = transformer\n    self.num_multimask_outputs = num_multimask_outputs\n    self.iou_token = nn.Embedding(1, transformer_dim)\n    self.num_mask_tokens = num_multimask_outputs + 1\n    self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n    self.output_upscaling = nn.Sequential(nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2), LayerNorm2d(transformer_dim // 4), activation(), nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2), activation())\n    self.output_hypernetworks_mlps = nn.ModuleList([MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for i in range(self.num_mask_tokens)])\n    self.iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth)",
            "def __init__(self, *, transformer_dim: int, transformer: Module, num_multimask_outputs: int=3, activation: type[Module]=nn.GELU, iou_head_depth: int=3, iou_head_hidden_dim: int=256) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Predicts masks given an image and prompt embeddings, using a transformer architecture.\\n\\n        Args:\\n            transformer_dim: the channel dimension of the transformer\\n            transformer: the transformer used to predict masks\\n            num_multimask_outputs: the number of masks to predict when disambiguating masks\\n            activation: the type of activation to use when upscaling masks\\n            iou_head_depth: the depth of the MLP used to predict mask quality\\n            iou_head_hidden_dim: the hidden dimension of the MLP used to predict mask quality\\n        '\n    super().__init__()\n    self.transformer_dim = transformer_dim\n    self.transformer = transformer\n    self.num_multimask_outputs = num_multimask_outputs\n    self.iou_token = nn.Embedding(1, transformer_dim)\n    self.num_mask_tokens = num_multimask_outputs + 1\n    self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n    self.output_upscaling = nn.Sequential(nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2), LayerNorm2d(transformer_dim // 4), activation(), nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2), activation())\n    self.output_hypernetworks_mlps = nn.ModuleList([MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for i in range(self.num_mask_tokens)])\n    self.iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth)",
            "def __init__(self, *, transformer_dim: int, transformer: Module, num_multimask_outputs: int=3, activation: type[Module]=nn.GELU, iou_head_depth: int=3, iou_head_hidden_dim: int=256) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Predicts masks given an image and prompt embeddings, using a transformer architecture.\\n\\n        Args:\\n            transformer_dim: the channel dimension of the transformer\\n            transformer: the transformer used to predict masks\\n            num_multimask_outputs: the number of masks to predict when disambiguating masks\\n            activation: the type of activation to use when upscaling masks\\n            iou_head_depth: the depth of the MLP used to predict mask quality\\n            iou_head_hidden_dim: the hidden dimension of the MLP used to predict mask quality\\n        '\n    super().__init__()\n    self.transformer_dim = transformer_dim\n    self.transformer = transformer\n    self.num_multimask_outputs = num_multimask_outputs\n    self.iou_token = nn.Embedding(1, transformer_dim)\n    self.num_mask_tokens = num_multimask_outputs + 1\n    self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n    self.output_upscaling = nn.Sequential(nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2), LayerNorm2d(transformer_dim // 4), activation(), nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2), activation())\n    self.output_hypernetworks_mlps = nn.ModuleList([MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for i in range(self.num_mask_tokens)])\n    self.iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth)",
            "def __init__(self, *, transformer_dim: int, transformer: Module, num_multimask_outputs: int=3, activation: type[Module]=nn.GELU, iou_head_depth: int=3, iou_head_hidden_dim: int=256) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Predicts masks given an image and prompt embeddings, using a transformer architecture.\\n\\n        Args:\\n            transformer_dim: the channel dimension of the transformer\\n            transformer: the transformer used to predict masks\\n            num_multimask_outputs: the number of masks to predict when disambiguating masks\\n            activation: the type of activation to use when upscaling masks\\n            iou_head_depth: the depth of the MLP used to predict mask quality\\n            iou_head_hidden_dim: the hidden dimension of the MLP used to predict mask quality\\n        '\n    super().__init__()\n    self.transformer_dim = transformer_dim\n    self.transformer = transformer\n    self.num_multimask_outputs = num_multimask_outputs\n    self.iou_token = nn.Embedding(1, transformer_dim)\n    self.num_mask_tokens = num_multimask_outputs + 1\n    self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n    self.output_upscaling = nn.Sequential(nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2), LayerNorm2d(transformer_dim // 4), activation(), nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2), activation())\n    self.output_hypernetworks_mlps = nn.ModuleList([MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for i in range(self.num_mask_tokens)])\n    self.iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor, multimask_output: bool) -> tuple[Tensor, Tensor]:\n    \"\"\"Predict masks given image and prompt embeddings.\n\n        Args:\n            image_embeddings: the embeddings from the image encoder\n            image_pe: positional encoding with the shape of image_embeddings\n            sparse_prompt_embeddings: the embeddings of the points and boxes\n            dense_prompt_embeddings: the embeddings of the mask inputs\n            multimask_output: Whether to return multiple masks or a single mask.\n\n        Returns:\n            batched predicted masks\n            batched predictions of mask quality\n        \"\"\"\n    (masks, iou_pred) = self.predict_masks(image_embeddings=image_embeddings, image_pe=image_pe, sparse_prompt_embeddings=sparse_prompt_embeddings, dense_prompt_embeddings=dense_prompt_embeddings)\n    if multimask_output:\n        mask_slice = slice(1, None)\n    else:\n        mask_slice = slice(0, 1)\n    masks = masks[:, mask_slice, :, :]\n    iou_pred = iou_pred[:, mask_slice]\n    return (masks, iou_pred)",
        "mutated": [
            "def forward(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor, multimask_output: bool) -> tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n    'Predict masks given image and prompt embeddings.\\n\\n        Args:\\n            image_embeddings: the embeddings from the image encoder\\n            image_pe: positional encoding with the shape of image_embeddings\\n            sparse_prompt_embeddings: the embeddings of the points and boxes\\n            dense_prompt_embeddings: the embeddings of the mask inputs\\n            multimask_output: Whether to return multiple masks or a single mask.\\n\\n        Returns:\\n            batched predicted masks\\n            batched predictions of mask quality\\n        '\n    (masks, iou_pred) = self.predict_masks(image_embeddings=image_embeddings, image_pe=image_pe, sparse_prompt_embeddings=sparse_prompt_embeddings, dense_prompt_embeddings=dense_prompt_embeddings)\n    if multimask_output:\n        mask_slice = slice(1, None)\n    else:\n        mask_slice = slice(0, 1)\n    masks = masks[:, mask_slice, :, :]\n    iou_pred = iou_pred[:, mask_slice]\n    return (masks, iou_pred)",
            "def forward(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor, multimask_output: bool) -> tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Predict masks given image and prompt embeddings.\\n\\n        Args:\\n            image_embeddings: the embeddings from the image encoder\\n            image_pe: positional encoding with the shape of image_embeddings\\n            sparse_prompt_embeddings: the embeddings of the points and boxes\\n            dense_prompt_embeddings: the embeddings of the mask inputs\\n            multimask_output: Whether to return multiple masks or a single mask.\\n\\n        Returns:\\n            batched predicted masks\\n            batched predictions of mask quality\\n        '\n    (masks, iou_pred) = self.predict_masks(image_embeddings=image_embeddings, image_pe=image_pe, sparse_prompt_embeddings=sparse_prompt_embeddings, dense_prompt_embeddings=dense_prompt_embeddings)\n    if multimask_output:\n        mask_slice = slice(1, None)\n    else:\n        mask_slice = slice(0, 1)\n    masks = masks[:, mask_slice, :, :]\n    iou_pred = iou_pred[:, mask_slice]\n    return (masks, iou_pred)",
            "def forward(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor, multimask_output: bool) -> tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Predict masks given image and prompt embeddings.\\n\\n        Args:\\n            image_embeddings: the embeddings from the image encoder\\n            image_pe: positional encoding with the shape of image_embeddings\\n            sparse_prompt_embeddings: the embeddings of the points and boxes\\n            dense_prompt_embeddings: the embeddings of the mask inputs\\n            multimask_output: Whether to return multiple masks or a single mask.\\n\\n        Returns:\\n            batched predicted masks\\n            batched predictions of mask quality\\n        '\n    (masks, iou_pred) = self.predict_masks(image_embeddings=image_embeddings, image_pe=image_pe, sparse_prompt_embeddings=sparse_prompt_embeddings, dense_prompt_embeddings=dense_prompt_embeddings)\n    if multimask_output:\n        mask_slice = slice(1, None)\n    else:\n        mask_slice = slice(0, 1)\n    masks = masks[:, mask_slice, :, :]\n    iou_pred = iou_pred[:, mask_slice]\n    return (masks, iou_pred)",
            "def forward(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor, multimask_output: bool) -> tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Predict masks given image and prompt embeddings.\\n\\n        Args:\\n            image_embeddings: the embeddings from the image encoder\\n            image_pe: positional encoding with the shape of image_embeddings\\n            sparse_prompt_embeddings: the embeddings of the points and boxes\\n            dense_prompt_embeddings: the embeddings of the mask inputs\\n            multimask_output: Whether to return multiple masks or a single mask.\\n\\n        Returns:\\n            batched predicted masks\\n            batched predictions of mask quality\\n        '\n    (masks, iou_pred) = self.predict_masks(image_embeddings=image_embeddings, image_pe=image_pe, sparse_prompt_embeddings=sparse_prompt_embeddings, dense_prompt_embeddings=dense_prompt_embeddings)\n    if multimask_output:\n        mask_slice = slice(1, None)\n    else:\n        mask_slice = slice(0, 1)\n    masks = masks[:, mask_slice, :, :]\n    iou_pred = iou_pred[:, mask_slice]\n    return (masks, iou_pred)",
            "def forward(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor, multimask_output: bool) -> tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Predict masks given image and prompt embeddings.\\n\\n        Args:\\n            image_embeddings: the embeddings from the image encoder\\n            image_pe: positional encoding with the shape of image_embeddings\\n            sparse_prompt_embeddings: the embeddings of the points and boxes\\n            dense_prompt_embeddings: the embeddings of the mask inputs\\n            multimask_output: Whether to return multiple masks or a single mask.\\n\\n        Returns:\\n            batched predicted masks\\n            batched predictions of mask quality\\n        '\n    (masks, iou_pred) = self.predict_masks(image_embeddings=image_embeddings, image_pe=image_pe, sparse_prompt_embeddings=sparse_prompt_embeddings, dense_prompt_embeddings=dense_prompt_embeddings)\n    if multimask_output:\n        mask_slice = slice(1, None)\n    else:\n        mask_slice = slice(0, 1)\n    masks = masks[:, mask_slice, :, :]\n    iou_pred = iou_pred[:, mask_slice]\n    return (masks, iou_pred)"
        ]
    },
    {
        "func_name": "predict_masks",
        "original": "def predict_masks(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor) -> tuple[Tensor, Tensor]:\n    \"\"\"Predicts masks.\n\n        See 'forward' for more details.\n        \"\"\"\n    output_tokens = concatenate([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n    output_tokens = output_tokens[None, ...].expand(sparse_prompt_embeddings.size(0), -1, -1)\n    tokens = concatenate((output_tokens, sparse_prompt_embeddings), dim=1)\n    src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n    src = src + dense_prompt_embeddings\n    pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n    (b, c, h, w) = src.shape\n    (hs, src) = self.transformer(src, pos_src, tokens)\n    iou_token_out = hs[:, 0, :]\n    mask_tokens_out = hs[:, 1:1 + self.num_mask_tokens, :]\n    src = src.transpose(1, 2).view(b, c, h, w)\n    upscaled_embedding = self.output_upscaling(src)\n    hyper_in_list: list[Tensor] = []\n    for i in range(self.num_mask_tokens):\n        hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n    hyper_in = stack(hyper_in_list, dim=1)\n    (b, c, h, w) = upscaled_embedding.shape\n    masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n    iou_pred = self.iou_prediction_head(iou_token_out)\n    return (masks, iou_pred)",
        "mutated": [
            "def predict_masks(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor) -> tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n    \"Predicts masks.\\n\\n        See 'forward' for more details.\\n        \"\n    output_tokens = concatenate([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n    output_tokens = output_tokens[None, ...].expand(sparse_prompt_embeddings.size(0), -1, -1)\n    tokens = concatenate((output_tokens, sparse_prompt_embeddings), dim=1)\n    src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n    src = src + dense_prompt_embeddings\n    pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n    (b, c, h, w) = src.shape\n    (hs, src) = self.transformer(src, pos_src, tokens)\n    iou_token_out = hs[:, 0, :]\n    mask_tokens_out = hs[:, 1:1 + self.num_mask_tokens, :]\n    src = src.transpose(1, 2).view(b, c, h, w)\n    upscaled_embedding = self.output_upscaling(src)\n    hyper_in_list: list[Tensor] = []\n    for i in range(self.num_mask_tokens):\n        hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n    hyper_in = stack(hyper_in_list, dim=1)\n    (b, c, h, w) = upscaled_embedding.shape\n    masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n    iou_pred = self.iou_prediction_head(iou_token_out)\n    return (masks, iou_pred)",
            "def predict_masks(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor) -> tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Predicts masks.\\n\\n        See 'forward' for more details.\\n        \"\n    output_tokens = concatenate([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n    output_tokens = output_tokens[None, ...].expand(sparse_prompt_embeddings.size(0), -1, -1)\n    tokens = concatenate((output_tokens, sparse_prompt_embeddings), dim=1)\n    src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n    src = src + dense_prompt_embeddings\n    pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n    (b, c, h, w) = src.shape\n    (hs, src) = self.transformer(src, pos_src, tokens)\n    iou_token_out = hs[:, 0, :]\n    mask_tokens_out = hs[:, 1:1 + self.num_mask_tokens, :]\n    src = src.transpose(1, 2).view(b, c, h, w)\n    upscaled_embedding = self.output_upscaling(src)\n    hyper_in_list: list[Tensor] = []\n    for i in range(self.num_mask_tokens):\n        hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n    hyper_in = stack(hyper_in_list, dim=1)\n    (b, c, h, w) = upscaled_embedding.shape\n    masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n    iou_pred = self.iou_prediction_head(iou_token_out)\n    return (masks, iou_pred)",
            "def predict_masks(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor) -> tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Predicts masks.\\n\\n        See 'forward' for more details.\\n        \"\n    output_tokens = concatenate([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n    output_tokens = output_tokens[None, ...].expand(sparse_prompt_embeddings.size(0), -1, -1)\n    tokens = concatenate((output_tokens, sparse_prompt_embeddings), dim=1)\n    src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n    src = src + dense_prompt_embeddings\n    pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n    (b, c, h, w) = src.shape\n    (hs, src) = self.transformer(src, pos_src, tokens)\n    iou_token_out = hs[:, 0, :]\n    mask_tokens_out = hs[:, 1:1 + self.num_mask_tokens, :]\n    src = src.transpose(1, 2).view(b, c, h, w)\n    upscaled_embedding = self.output_upscaling(src)\n    hyper_in_list: list[Tensor] = []\n    for i in range(self.num_mask_tokens):\n        hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n    hyper_in = stack(hyper_in_list, dim=1)\n    (b, c, h, w) = upscaled_embedding.shape\n    masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n    iou_pred = self.iou_prediction_head(iou_token_out)\n    return (masks, iou_pred)",
            "def predict_masks(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor) -> tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Predicts masks.\\n\\n        See 'forward' for more details.\\n        \"\n    output_tokens = concatenate([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n    output_tokens = output_tokens[None, ...].expand(sparse_prompt_embeddings.size(0), -1, -1)\n    tokens = concatenate((output_tokens, sparse_prompt_embeddings), dim=1)\n    src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n    src = src + dense_prompt_embeddings\n    pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n    (b, c, h, w) = src.shape\n    (hs, src) = self.transformer(src, pos_src, tokens)\n    iou_token_out = hs[:, 0, :]\n    mask_tokens_out = hs[:, 1:1 + self.num_mask_tokens, :]\n    src = src.transpose(1, 2).view(b, c, h, w)\n    upscaled_embedding = self.output_upscaling(src)\n    hyper_in_list: list[Tensor] = []\n    for i in range(self.num_mask_tokens):\n        hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n    hyper_in = stack(hyper_in_list, dim=1)\n    (b, c, h, w) = upscaled_embedding.shape\n    masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n    iou_pred = self.iou_prediction_head(iou_token_out)\n    return (masks, iou_pred)",
            "def predict_masks(self, image_embeddings: Tensor, image_pe: Tensor, sparse_prompt_embeddings: Tensor, dense_prompt_embeddings: Tensor) -> tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Predicts masks.\\n\\n        See 'forward' for more details.\\n        \"\n    output_tokens = concatenate([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n    output_tokens = output_tokens[None, ...].expand(sparse_prompt_embeddings.size(0), -1, -1)\n    tokens = concatenate((output_tokens, sparse_prompt_embeddings), dim=1)\n    src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n    src = src + dense_prompt_embeddings\n    pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n    (b, c, h, w) = src.shape\n    (hs, src) = self.transformer(src, pos_src, tokens)\n    iou_token_out = hs[:, 0, :]\n    mask_tokens_out = hs[:, 1:1 + self.num_mask_tokens, :]\n    src = src.transpose(1, 2).view(b, c, h, w)\n    upscaled_embedding = self.output_upscaling(src)\n    hyper_in_list: list[Tensor] = []\n    for i in range(self.num_mask_tokens):\n        hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n    hyper_in = stack(hyper_in_list, dim=1)\n    (b, c, h, w) = upscaled_embedding.shape\n    masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n    iou_pred = self.iou_prediction_head(iou_token_out)\n    return (masks, iou_pred)"
        ]
    }
]