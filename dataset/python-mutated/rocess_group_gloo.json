[
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()"
        ]
    },
    {
        "func_name": "config",
        "original": "def config(self):\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
        "mutated": [
            "def config(self):\n    if False:\n        i = 10\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)"
        ]
    },
    {
        "func_name": "test_gather",
        "original": "def test_gather(root):\n    tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n    tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n    if pg.rank() == root:\n        task = pg.gather(tensor_y[root], tensor_x, root, True)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, tensor_y)\n    else:\n        task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n        task.wait()",
        "mutated": [
            "def test_gather(root):\n    if False:\n        i = 10\n    tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n    tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n    if pg.rank() == root:\n        task = pg.gather(tensor_y[root], tensor_x, root, True)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, tensor_y)\n    else:\n        task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n        task.wait()",
            "def test_gather(root):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n    tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n    if pg.rank() == root:\n        task = pg.gather(tensor_y[root], tensor_x, root, True)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, tensor_y)\n    else:\n        task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n        task.wait()",
            "def test_gather(root):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n    tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n    if pg.rank() == root:\n        task = pg.gather(tensor_y[root], tensor_x, root, True)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, tensor_y)\n    else:\n        task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n        task.wait()",
            "def test_gather(root):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n    tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n    if pg.rank() == root:\n        task = pg.gather(tensor_y[root], tensor_x, root, True)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, tensor_y)\n    else:\n        task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n        task.wait()",
            "def test_gather(root):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n    tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n    if pg.rank() == root:\n        task = pg.gather(tensor_y[root], tensor_x, root, True)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, tensor_y)\n    else:\n        task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n        task.wait()"
        ]
    },
    {
        "func_name": "test_create_process_group_gloo",
        "original": "def test_create_process_group_gloo(self):\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6272, is_master, nranks, 30)\n    pg = paddle.base.core.ProcessGroupGloo.create(store, rank, nranks)\n    paddle.device.set_device('cpu')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = x + y\n    if rank == 0:\n        task = pg.allreduce(tensor_x)\n        task.wait()\n        np.testing.assert_equal(tensor_x, sum_result)\n    else:\n        task = pg.allreduce(tensor_y)\n        task.wait()\n        np.testing.assert_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if rank == 0:\n        task = pg.allreduce(tensor_x, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = pg.allreduce(tensor_y, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if rank == 0:\n        task = pg.broadcast(tensor_x, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = pg.broadcast(tensor_y, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y_1 = paddle.to_tensor(y)\n    tensor_y_2 = deepcopy(tensor_y_1)\n    send_recv_result_1 = paddle.assign(tensor_x)\n    send_recv_result_2 = paddle.assign(tensor_y_2)\n    if pg.rank() == 0:\n        task = pg.send(tensor_x, pg.size() - 1, True)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.recv(tensor_y_1, 0, True)\n        np.testing.assert_array_equal(send_recv_result_1, tensor_y_1)\n    if pg.rank() == 0:\n        task = pg.recv(tensor_x, pg.size() - 1, True)\n        np.testing.assert_array_equal(send_recv_result_2, tensor_x)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.send(tensor_y_2, 0, True)\n    print('test send_recv api ok')\n    if pg.rank() == 0:\n        task = pg.barrier()\n        task.wait()\n    else:\n        task = pg.barrier()\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = pg.all_gather(tensor_y, tensor_out)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n\n    def test_gather(root):\n        tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n        tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n        if pg.rank() == root:\n            task = pg.gather(tensor_y[root], tensor_x, root, True)\n            task.wait()\n            np.testing.assert_array_equal(tensor_x, tensor_y)\n        else:\n            task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n            task.wait()\n    test_gather(0)\n    test_gather(pg.size() - 1)\n    print('test gather api ok\\n')",
        "mutated": [
            "def test_create_process_group_gloo(self):\n    if False:\n        i = 10\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6272, is_master, nranks, 30)\n    pg = paddle.base.core.ProcessGroupGloo.create(store, rank, nranks)\n    paddle.device.set_device('cpu')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = x + y\n    if rank == 0:\n        task = pg.allreduce(tensor_x)\n        task.wait()\n        np.testing.assert_equal(tensor_x, sum_result)\n    else:\n        task = pg.allreduce(tensor_y)\n        task.wait()\n        np.testing.assert_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if rank == 0:\n        task = pg.allreduce(tensor_x, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = pg.allreduce(tensor_y, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if rank == 0:\n        task = pg.broadcast(tensor_x, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = pg.broadcast(tensor_y, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y_1 = paddle.to_tensor(y)\n    tensor_y_2 = deepcopy(tensor_y_1)\n    send_recv_result_1 = paddle.assign(tensor_x)\n    send_recv_result_2 = paddle.assign(tensor_y_2)\n    if pg.rank() == 0:\n        task = pg.send(tensor_x, pg.size() - 1, True)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.recv(tensor_y_1, 0, True)\n        np.testing.assert_array_equal(send_recv_result_1, tensor_y_1)\n    if pg.rank() == 0:\n        task = pg.recv(tensor_x, pg.size() - 1, True)\n        np.testing.assert_array_equal(send_recv_result_2, tensor_x)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.send(tensor_y_2, 0, True)\n    print('test send_recv api ok')\n    if pg.rank() == 0:\n        task = pg.barrier()\n        task.wait()\n    else:\n        task = pg.barrier()\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = pg.all_gather(tensor_y, tensor_out)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n\n    def test_gather(root):\n        tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n        tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n        if pg.rank() == root:\n            task = pg.gather(tensor_y[root], tensor_x, root, True)\n            task.wait()\n            np.testing.assert_array_equal(tensor_x, tensor_y)\n        else:\n            task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n            task.wait()\n    test_gather(0)\n    test_gather(pg.size() - 1)\n    print('test gather api ok\\n')",
            "def test_create_process_group_gloo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6272, is_master, nranks, 30)\n    pg = paddle.base.core.ProcessGroupGloo.create(store, rank, nranks)\n    paddle.device.set_device('cpu')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = x + y\n    if rank == 0:\n        task = pg.allreduce(tensor_x)\n        task.wait()\n        np.testing.assert_equal(tensor_x, sum_result)\n    else:\n        task = pg.allreduce(tensor_y)\n        task.wait()\n        np.testing.assert_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if rank == 0:\n        task = pg.allreduce(tensor_x, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = pg.allreduce(tensor_y, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if rank == 0:\n        task = pg.broadcast(tensor_x, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = pg.broadcast(tensor_y, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y_1 = paddle.to_tensor(y)\n    tensor_y_2 = deepcopy(tensor_y_1)\n    send_recv_result_1 = paddle.assign(tensor_x)\n    send_recv_result_2 = paddle.assign(tensor_y_2)\n    if pg.rank() == 0:\n        task = pg.send(tensor_x, pg.size() - 1, True)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.recv(tensor_y_1, 0, True)\n        np.testing.assert_array_equal(send_recv_result_1, tensor_y_1)\n    if pg.rank() == 0:\n        task = pg.recv(tensor_x, pg.size() - 1, True)\n        np.testing.assert_array_equal(send_recv_result_2, tensor_x)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.send(tensor_y_2, 0, True)\n    print('test send_recv api ok')\n    if pg.rank() == 0:\n        task = pg.barrier()\n        task.wait()\n    else:\n        task = pg.barrier()\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = pg.all_gather(tensor_y, tensor_out)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n\n    def test_gather(root):\n        tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n        tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n        if pg.rank() == root:\n            task = pg.gather(tensor_y[root], tensor_x, root, True)\n            task.wait()\n            np.testing.assert_array_equal(tensor_x, tensor_y)\n        else:\n            task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n            task.wait()\n    test_gather(0)\n    test_gather(pg.size() - 1)\n    print('test gather api ok\\n')",
            "def test_create_process_group_gloo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6272, is_master, nranks, 30)\n    pg = paddle.base.core.ProcessGroupGloo.create(store, rank, nranks)\n    paddle.device.set_device('cpu')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = x + y\n    if rank == 0:\n        task = pg.allreduce(tensor_x)\n        task.wait()\n        np.testing.assert_equal(tensor_x, sum_result)\n    else:\n        task = pg.allreduce(tensor_y)\n        task.wait()\n        np.testing.assert_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if rank == 0:\n        task = pg.allreduce(tensor_x, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = pg.allreduce(tensor_y, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if rank == 0:\n        task = pg.broadcast(tensor_x, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = pg.broadcast(tensor_y, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y_1 = paddle.to_tensor(y)\n    tensor_y_2 = deepcopy(tensor_y_1)\n    send_recv_result_1 = paddle.assign(tensor_x)\n    send_recv_result_2 = paddle.assign(tensor_y_2)\n    if pg.rank() == 0:\n        task = pg.send(tensor_x, pg.size() - 1, True)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.recv(tensor_y_1, 0, True)\n        np.testing.assert_array_equal(send_recv_result_1, tensor_y_1)\n    if pg.rank() == 0:\n        task = pg.recv(tensor_x, pg.size() - 1, True)\n        np.testing.assert_array_equal(send_recv_result_2, tensor_x)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.send(tensor_y_2, 0, True)\n    print('test send_recv api ok')\n    if pg.rank() == 0:\n        task = pg.barrier()\n        task.wait()\n    else:\n        task = pg.barrier()\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = pg.all_gather(tensor_y, tensor_out)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n\n    def test_gather(root):\n        tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n        tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n        if pg.rank() == root:\n            task = pg.gather(tensor_y[root], tensor_x, root, True)\n            task.wait()\n            np.testing.assert_array_equal(tensor_x, tensor_y)\n        else:\n            task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n            task.wait()\n    test_gather(0)\n    test_gather(pg.size() - 1)\n    print('test gather api ok\\n')",
            "def test_create_process_group_gloo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6272, is_master, nranks, 30)\n    pg = paddle.base.core.ProcessGroupGloo.create(store, rank, nranks)\n    paddle.device.set_device('cpu')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = x + y\n    if rank == 0:\n        task = pg.allreduce(tensor_x)\n        task.wait()\n        np.testing.assert_equal(tensor_x, sum_result)\n    else:\n        task = pg.allreduce(tensor_y)\n        task.wait()\n        np.testing.assert_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if rank == 0:\n        task = pg.allreduce(tensor_x, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = pg.allreduce(tensor_y, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if rank == 0:\n        task = pg.broadcast(tensor_x, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = pg.broadcast(tensor_y, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y_1 = paddle.to_tensor(y)\n    tensor_y_2 = deepcopy(tensor_y_1)\n    send_recv_result_1 = paddle.assign(tensor_x)\n    send_recv_result_2 = paddle.assign(tensor_y_2)\n    if pg.rank() == 0:\n        task = pg.send(tensor_x, pg.size() - 1, True)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.recv(tensor_y_1, 0, True)\n        np.testing.assert_array_equal(send_recv_result_1, tensor_y_1)\n    if pg.rank() == 0:\n        task = pg.recv(tensor_x, pg.size() - 1, True)\n        np.testing.assert_array_equal(send_recv_result_2, tensor_x)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.send(tensor_y_2, 0, True)\n    print('test send_recv api ok')\n    if pg.rank() == 0:\n        task = pg.barrier()\n        task.wait()\n    else:\n        task = pg.barrier()\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = pg.all_gather(tensor_y, tensor_out)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n\n    def test_gather(root):\n        tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n        tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n        if pg.rank() == root:\n            task = pg.gather(tensor_y[root], tensor_x, root, True)\n            task.wait()\n            np.testing.assert_array_equal(tensor_x, tensor_y)\n        else:\n            task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n            task.wait()\n    test_gather(0)\n    test_gather(pg.size() - 1)\n    print('test gather api ok\\n')",
            "def test_create_process_group_gloo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6272, is_master, nranks, 30)\n    pg = paddle.base.core.ProcessGroupGloo.create(store, rank, nranks)\n    paddle.device.set_device('cpu')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = x + y\n    if rank == 0:\n        task = pg.allreduce(tensor_x)\n        task.wait()\n        np.testing.assert_equal(tensor_x, sum_result)\n    else:\n        task = pg.allreduce(tensor_y)\n        task.wait()\n        np.testing.assert_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if rank == 0:\n        task = pg.allreduce(tensor_x, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = pg.allreduce(tensor_y, core.ReduceOp.MAX)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if rank == 0:\n        task = pg.broadcast(tensor_x, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = pg.broadcast(tensor_y, 0)\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y_1 = paddle.to_tensor(y)\n    tensor_y_2 = deepcopy(tensor_y_1)\n    send_recv_result_1 = paddle.assign(tensor_x)\n    send_recv_result_2 = paddle.assign(tensor_y_2)\n    if pg.rank() == 0:\n        task = pg.send(tensor_x, pg.size() - 1, True)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.recv(tensor_y_1, 0, True)\n        np.testing.assert_array_equal(send_recv_result_1, tensor_y_1)\n    if pg.rank() == 0:\n        task = pg.recv(tensor_x, pg.size() - 1, True)\n        np.testing.assert_array_equal(send_recv_result_2, tensor_x)\n    elif pg.rank() == pg.size() - 1:\n        task = pg.send(tensor_y_2, 0, True)\n    print('test send_recv api ok')\n    if pg.rank() == 0:\n        task = pg.barrier()\n        task.wait()\n    else:\n        task = pg.barrier()\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = pg.all_gather(tensor_y, tensor_out)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n\n    def test_gather(root):\n        tensor_x = [paddle.zeros(self.shape).astype(self.dtype) for _ in range(pg.size())]\n        tensor_y = [paddle.to_tensor(np.random.random(self.shape).astype(self.dtype)) for _ in range(pg.size())]\n        if pg.rank() == root:\n            task = pg.gather(tensor_y[root], tensor_x, root, True)\n            task.wait()\n            np.testing.assert_array_equal(tensor_x, tensor_y)\n        else:\n            task = pg.gather(tensor_y[pg.rank()], tensor_x, root, True)\n            task.wait()\n    test_gather(0)\n    test_gather(pg.size() - 1)\n    print('test gather api ok\\n')"
        ]
    }
]