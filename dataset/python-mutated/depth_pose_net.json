[
    {
        "func_name": "__init__",
        "original": "def __init__(self, version=None, min_depth=0.1, max_depth=100, **kwargs):\n    super().__init__()\n    self.min_depth = min_depth\n    self.max_depth = max_depth\n    assert 'it' in version\n    self.iters = int(version.split('-')[0].split('it')[1])\n    self.is_high = 'h' in version\n    self.out_normalize = 'out' in version\n    self.seq_len = 4\n    for str in version.split('-'):\n        if 'seq' in str:\n            self.seq_len = int(str.split('seq')[1])\n    self.iters = self.iters // self.seq_len\n    self.inter_sup = 'inter' in version\n    print(f'=======iters:{self.iters}, sub_seq_len:{self.seq_len}, inter_sup: {self.inter_sup}, is_high:{self.is_high}, out_norm:{self.out_normalize}, max_depth:{self.max_depth} min_depth:{self.min_depth}========')\n    if self.out_normalize:\n        self.scale_inv_depth = partial(disp_to_depth, min_depth=self.min_depth, max_depth=self.max_depth)\n    else:\n        self.scale_inv_depth = lambda x: (x, None)\n    self.foutput_dim = 128\n    self.feat_ratio = 8\n    self.fnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.depth_head = DepthHead(input_dim=self.foutput_dim, hidden_dim=self.foutput_dim, scale=False)\n    self.pose_head = PoseHead(input_dim=self.foutput_dim * 2, hidden_dim=self.foutput_dim)\n    self.upmask_net = UpMaskNet(hidden_dim=self.foutput_dim, ratio=self.feat_ratio)\n    self.hdim = 128 if self.is_high else 64\n    self.cdim = 32\n    self.update_block_depth = BasicUpdateBlockDepth(hidden_dim=self.hdim, cost_dim=self.foutput_dim, ratio=self.feat_ratio, context_dim=self.cdim)\n    self.update_block_pose = BasicUpdateBlockPose(hidden_dim=self.hdim, cost_dim=self.foutput_dim, context_dim=self.cdim)\n    self.cnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.cnet_depth = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=1)\n    self.cnet_pose = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=2)",
        "mutated": [
            "def __init__(self, version=None, min_depth=0.1, max_depth=100, **kwargs):\n    if False:\n        i = 10\n    super().__init__()\n    self.min_depth = min_depth\n    self.max_depth = max_depth\n    assert 'it' in version\n    self.iters = int(version.split('-')[0].split('it')[1])\n    self.is_high = 'h' in version\n    self.out_normalize = 'out' in version\n    self.seq_len = 4\n    for str in version.split('-'):\n        if 'seq' in str:\n            self.seq_len = int(str.split('seq')[1])\n    self.iters = self.iters // self.seq_len\n    self.inter_sup = 'inter' in version\n    print(f'=======iters:{self.iters}, sub_seq_len:{self.seq_len}, inter_sup: {self.inter_sup}, is_high:{self.is_high}, out_norm:{self.out_normalize}, max_depth:{self.max_depth} min_depth:{self.min_depth}========')\n    if self.out_normalize:\n        self.scale_inv_depth = partial(disp_to_depth, min_depth=self.min_depth, max_depth=self.max_depth)\n    else:\n        self.scale_inv_depth = lambda x: (x, None)\n    self.foutput_dim = 128\n    self.feat_ratio = 8\n    self.fnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.depth_head = DepthHead(input_dim=self.foutput_dim, hidden_dim=self.foutput_dim, scale=False)\n    self.pose_head = PoseHead(input_dim=self.foutput_dim * 2, hidden_dim=self.foutput_dim)\n    self.upmask_net = UpMaskNet(hidden_dim=self.foutput_dim, ratio=self.feat_ratio)\n    self.hdim = 128 if self.is_high else 64\n    self.cdim = 32\n    self.update_block_depth = BasicUpdateBlockDepth(hidden_dim=self.hdim, cost_dim=self.foutput_dim, ratio=self.feat_ratio, context_dim=self.cdim)\n    self.update_block_pose = BasicUpdateBlockPose(hidden_dim=self.hdim, cost_dim=self.foutput_dim, context_dim=self.cdim)\n    self.cnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.cnet_depth = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=1)\n    self.cnet_pose = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=2)",
            "def __init__(self, version=None, min_depth=0.1, max_depth=100, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.min_depth = min_depth\n    self.max_depth = max_depth\n    assert 'it' in version\n    self.iters = int(version.split('-')[0].split('it')[1])\n    self.is_high = 'h' in version\n    self.out_normalize = 'out' in version\n    self.seq_len = 4\n    for str in version.split('-'):\n        if 'seq' in str:\n            self.seq_len = int(str.split('seq')[1])\n    self.iters = self.iters // self.seq_len\n    self.inter_sup = 'inter' in version\n    print(f'=======iters:{self.iters}, sub_seq_len:{self.seq_len}, inter_sup: {self.inter_sup}, is_high:{self.is_high}, out_norm:{self.out_normalize}, max_depth:{self.max_depth} min_depth:{self.min_depth}========')\n    if self.out_normalize:\n        self.scale_inv_depth = partial(disp_to_depth, min_depth=self.min_depth, max_depth=self.max_depth)\n    else:\n        self.scale_inv_depth = lambda x: (x, None)\n    self.foutput_dim = 128\n    self.feat_ratio = 8\n    self.fnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.depth_head = DepthHead(input_dim=self.foutput_dim, hidden_dim=self.foutput_dim, scale=False)\n    self.pose_head = PoseHead(input_dim=self.foutput_dim * 2, hidden_dim=self.foutput_dim)\n    self.upmask_net = UpMaskNet(hidden_dim=self.foutput_dim, ratio=self.feat_ratio)\n    self.hdim = 128 if self.is_high else 64\n    self.cdim = 32\n    self.update_block_depth = BasicUpdateBlockDepth(hidden_dim=self.hdim, cost_dim=self.foutput_dim, ratio=self.feat_ratio, context_dim=self.cdim)\n    self.update_block_pose = BasicUpdateBlockPose(hidden_dim=self.hdim, cost_dim=self.foutput_dim, context_dim=self.cdim)\n    self.cnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.cnet_depth = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=1)\n    self.cnet_pose = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=2)",
            "def __init__(self, version=None, min_depth=0.1, max_depth=100, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.min_depth = min_depth\n    self.max_depth = max_depth\n    assert 'it' in version\n    self.iters = int(version.split('-')[0].split('it')[1])\n    self.is_high = 'h' in version\n    self.out_normalize = 'out' in version\n    self.seq_len = 4\n    for str in version.split('-'):\n        if 'seq' in str:\n            self.seq_len = int(str.split('seq')[1])\n    self.iters = self.iters // self.seq_len\n    self.inter_sup = 'inter' in version\n    print(f'=======iters:{self.iters}, sub_seq_len:{self.seq_len}, inter_sup: {self.inter_sup}, is_high:{self.is_high}, out_norm:{self.out_normalize}, max_depth:{self.max_depth} min_depth:{self.min_depth}========')\n    if self.out_normalize:\n        self.scale_inv_depth = partial(disp_to_depth, min_depth=self.min_depth, max_depth=self.max_depth)\n    else:\n        self.scale_inv_depth = lambda x: (x, None)\n    self.foutput_dim = 128\n    self.feat_ratio = 8\n    self.fnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.depth_head = DepthHead(input_dim=self.foutput_dim, hidden_dim=self.foutput_dim, scale=False)\n    self.pose_head = PoseHead(input_dim=self.foutput_dim * 2, hidden_dim=self.foutput_dim)\n    self.upmask_net = UpMaskNet(hidden_dim=self.foutput_dim, ratio=self.feat_ratio)\n    self.hdim = 128 if self.is_high else 64\n    self.cdim = 32\n    self.update_block_depth = BasicUpdateBlockDepth(hidden_dim=self.hdim, cost_dim=self.foutput_dim, ratio=self.feat_ratio, context_dim=self.cdim)\n    self.update_block_pose = BasicUpdateBlockPose(hidden_dim=self.hdim, cost_dim=self.foutput_dim, context_dim=self.cdim)\n    self.cnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.cnet_depth = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=1)\n    self.cnet_pose = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=2)",
            "def __init__(self, version=None, min_depth=0.1, max_depth=100, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.min_depth = min_depth\n    self.max_depth = max_depth\n    assert 'it' in version\n    self.iters = int(version.split('-')[0].split('it')[1])\n    self.is_high = 'h' in version\n    self.out_normalize = 'out' in version\n    self.seq_len = 4\n    for str in version.split('-'):\n        if 'seq' in str:\n            self.seq_len = int(str.split('seq')[1])\n    self.iters = self.iters // self.seq_len\n    self.inter_sup = 'inter' in version\n    print(f'=======iters:{self.iters}, sub_seq_len:{self.seq_len}, inter_sup: {self.inter_sup}, is_high:{self.is_high}, out_norm:{self.out_normalize}, max_depth:{self.max_depth} min_depth:{self.min_depth}========')\n    if self.out_normalize:\n        self.scale_inv_depth = partial(disp_to_depth, min_depth=self.min_depth, max_depth=self.max_depth)\n    else:\n        self.scale_inv_depth = lambda x: (x, None)\n    self.foutput_dim = 128\n    self.feat_ratio = 8\n    self.fnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.depth_head = DepthHead(input_dim=self.foutput_dim, hidden_dim=self.foutput_dim, scale=False)\n    self.pose_head = PoseHead(input_dim=self.foutput_dim * 2, hidden_dim=self.foutput_dim)\n    self.upmask_net = UpMaskNet(hidden_dim=self.foutput_dim, ratio=self.feat_ratio)\n    self.hdim = 128 if self.is_high else 64\n    self.cdim = 32\n    self.update_block_depth = BasicUpdateBlockDepth(hidden_dim=self.hdim, cost_dim=self.foutput_dim, ratio=self.feat_ratio, context_dim=self.cdim)\n    self.update_block_pose = BasicUpdateBlockPose(hidden_dim=self.hdim, cost_dim=self.foutput_dim, context_dim=self.cdim)\n    self.cnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.cnet_depth = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=1)\n    self.cnet_pose = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=2)",
            "def __init__(self, version=None, min_depth=0.1, max_depth=100, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.min_depth = min_depth\n    self.max_depth = max_depth\n    assert 'it' in version\n    self.iters = int(version.split('-')[0].split('it')[1])\n    self.is_high = 'h' in version\n    self.out_normalize = 'out' in version\n    self.seq_len = 4\n    for str in version.split('-'):\n        if 'seq' in str:\n            self.seq_len = int(str.split('seq')[1])\n    self.iters = self.iters // self.seq_len\n    self.inter_sup = 'inter' in version\n    print(f'=======iters:{self.iters}, sub_seq_len:{self.seq_len}, inter_sup: {self.inter_sup}, is_high:{self.is_high}, out_norm:{self.out_normalize}, max_depth:{self.max_depth} min_depth:{self.min_depth}========')\n    if self.out_normalize:\n        self.scale_inv_depth = partial(disp_to_depth, min_depth=self.min_depth, max_depth=self.max_depth)\n    else:\n        self.scale_inv_depth = lambda x: (x, None)\n    self.foutput_dim = 128\n    self.feat_ratio = 8\n    self.fnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.depth_head = DepthHead(input_dim=self.foutput_dim, hidden_dim=self.foutput_dim, scale=False)\n    self.pose_head = PoseHead(input_dim=self.foutput_dim * 2, hidden_dim=self.foutput_dim)\n    self.upmask_net = UpMaskNet(hidden_dim=self.foutput_dim, ratio=self.feat_ratio)\n    self.hdim = 128 if self.is_high else 64\n    self.cdim = 32\n    self.update_block_depth = BasicUpdateBlockDepth(hidden_dim=self.hdim, cost_dim=self.foutput_dim, ratio=self.feat_ratio, context_dim=self.cdim)\n    self.update_block_pose = BasicUpdateBlockPose(hidden_dim=self.hdim, cost_dim=self.foutput_dim, context_dim=self.cdim)\n    self.cnet = ResNetEncoder(out_chs=self.foutput_dim, stride=self.feat_ratio)\n    self.cnet_depth = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=1)\n    self.cnet_pose = ResNetEncoder(out_chs=self.hdim + self.cdim, stride=self.feat_ratio, num_input_images=2)"
        ]
    },
    {
        "func_name": "upsample_depth",
        "original": "def upsample_depth(self, depth, mask, ratio=8):\n    \"\"\" Upsample depth field [H/ratio, W/ratio, 2] -> [H, W, 2] using convex combination \"\"\"\n    (N, _, H, W) = depth.shape\n    mask = mask.view(N, 1, 9, ratio, ratio, H, W)\n    mask = torch.softmax(mask, dim=2)\n    up_flow = F.unfold(depth, [3, 3], padding=1)\n    up_flow = up_flow.view(N, 1, 9, 1, 1, H, W)\n    up_flow = torch.sum(mask * up_flow, dim=2)\n    up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n    return up_flow.reshape(N, 1, ratio * H, ratio * W)",
        "mutated": [
            "def upsample_depth(self, depth, mask, ratio=8):\n    if False:\n        i = 10\n    ' Upsample depth field [H/ratio, W/ratio, 2] -> [H, W, 2] using convex combination '\n    (N, _, H, W) = depth.shape\n    mask = mask.view(N, 1, 9, ratio, ratio, H, W)\n    mask = torch.softmax(mask, dim=2)\n    up_flow = F.unfold(depth, [3, 3], padding=1)\n    up_flow = up_flow.view(N, 1, 9, 1, 1, H, W)\n    up_flow = torch.sum(mask * up_flow, dim=2)\n    up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n    return up_flow.reshape(N, 1, ratio * H, ratio * W)",
            "def upsample_depth(self, depth, mask, ratio=8):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Upsample depth field [H/ratio, W/ratio, 2] -> [H, W, 2] using convex combination '\n    (N, _, H, W) = depth.shape\n    mask = mask.view(N, 1, 9, ratio, ratio, H, W)\n    mask = torch.softmax(mask, dim=2)\n    up_flow = F.unfold(depth, [3, 3], padding=1)\n    up_flow = up_flow.view(N, 1, 9, 1, 1, H, W)\n    up_flow = torch.sum(mask * up_flow, dim=2)\n    up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n    return up_flow.reshape(N, 1, ratio * H, ratio * W)",
            "def upsample_depth(self, depth, mask, ratio=8):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Upsample depth field [H/ratio, W/ratio, 2] -> [H, W, 2] using convex combination '\n    (N, _, H, W) = depth.shape\n    mask = mask.view(N, 1, 9, ratio, ratio, H, W)\n    mask = torch.softmax(mask, dim=2)\n    up_flow = F.unfold(depth, [3, 3], padding=1)\n    up_flow = up_flow.view(N, 1, 9, 1, 1, H, W)\n    up_flow = torch.sum(mask * up_flow, dim=2)\n    up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n    return up_flow.reshape(N, 1, ratio * H, ratio * W)",
            "def upsample_depth(self, depth, mask, ratio=8):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Upsample depth field [H/ratio, W/ratio, 2] -> [H, W, 2] using convex combination '\n    (N, _, H, W) = depth.shape\n    mask = mask.view(N, 1, 9, ratio, ratio, H, W)\n    mask = torch.softmax(mask, dim=2)\n    up_flow = F.unfold(depth, [3, 3], padding=1)\n    up_flow = up_flow.view(N, 1, 9, 1, 1, H, W)\n    up_flow = torch.sum(mask * up_flow, dim=2)\n    up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n    return up_flow.reshape(N, 1, ratio * H, ratio * W)",
            "def upsample_depth(self, depth, mask, ratio=8):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Upsample depth field [H/ratio, W/ratio, 2] -> [H, W, 2] using convex combination '\n    (N, _, H, W) = depth.shape\n    mask = mask.view(N, 1, 9, ratio, ratio, H, W)\n    mask = torch.softmax(mask, dim=2)\n    up_flow = F.unfold(depth, [3, 3], padding=1)\n    up_flow = up_flow.view(N, 1, 9, 1, 1, H, W)\n    up_flow = torch.sum(mask * up_flow, dim=2)\n    up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n    return up_flow.reshape(N, 1, ratio * H, ratio * W)"
        ]
    },
    {
        "func_name": "get_cost_each",
        "original": "def get_cost_each(self, pose, fmap, fmap_ref, depth, K, ref_K, scale_factor):\n    \"\"\"\n            depth: (b, 1, h, w)\n            fmap, fmap_ref: (b, c, h, w)\n        \"\"\"\n    pose = Pose.from_vec(pose, 'euler')\n    device = depth.device\n    cam = Camera(K=K.float()).scaled(scale_factor).to(device)\n    ref_cam = Camera(K=ref_K.float(), Tcw=pose).scaled(scale_factor).to(device)\n    world_points = cam.reconstruct(depth, frame='w')\n    ref_coords = ref_cam.project(world_points, frame='w', normalize=True)\n    fmap_warped = F.grid_sample(fmap_ref, ref_coords, mode='bilinear', padding_mode='zeros', align_corners=True)\n    cost = (fmap - fmap_warped) ** 2\n    return cost",
        "mutated": [
            "def get_cost_each(self, pose, fmap, fmap_ref, depth, K, ref_K, scale_factor):\n    if False:\n        i = 10\n    '\\n            depth: (b, 1, h, w)\\n            fmap, fmap_ref: (b, c, h, w)\\n        '\n    pose = Pose.from_vec(pose, 'euler')\n    device = depth.device\n    cam = Camera(K=K.float()).scaled(scale_factor).to(device)\n    ref_cam = Camera(K=ref_K.float(), Tcw=pose).scaled(scale_factor).to(device)\n    world_points = cam.reconstruct(depth, frame='w')\n    ref_coords = ref_cam.project(world_points, frame='w', normalize=True)\n    fmap_warped = F.grid_sample(fmap_ref, ref_coords, mode='bilinear', padding_mode='zeros', align_corners=True)\n    cost = (fmap - fmap_warped) ** 2\n    return cost",
            "def get_cost_each(self, pose, fmap, fmap_ref, depth, K, ref_K, scale_factor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n            depth: (b, 1, h, w)\\n            fmap, fmap_ref: (b, c, h, w)\\n        '\n    pose = Pose.from_vec(pose, 'euler')\n    device = depth.device\n    cam = Camera(K=K.float()).scaled(scale_factor).to(device)\n    ref_cam = Camera(K=ref_K.float(), Tcw=pose).scaled(scale_factor).to(device)\n    world_points = cam.reconstruct(depth, frame='w')\n    ref_coords = ref_cam.project(world_points, frame='w', normalize=True)\n    fmap_warped = F.grid_sample(fmap_ref, ref_coords, mode='bilinear', padding_mode='zeros', align_corners=True)\n    cost = (fmap - fmap_warped) ** 2\n    return cost",
            "def get_cost_each(self, pose, fmap, fmap_ref, depth, K, ref_K, scale_factor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n            depth: (b, 1, h, w)\\n            fmap, fmap_ref: (b, c, h, w)\\n        '\n    pose = Pose.from_vec(pose, 'euler')\n    device = depth.device\n    cam = Camera(K=K.float()).scaled(scale_factor).to(device)\n    ref_cam = Camera(K=ref_K.float(), Tcw=pose).scaled(scale_factor).to(device)\n    world_points = cam.reconstruct(depth, frame='w')\n    ref_coords = ref_cam.project(world_points, frame='w', normalize=True)\n    fmap_warped = F.grid_sample(fmap_ref, ref_coords, mode='bilinear', padding_mode='zeros', align_corners=True)\n    cost = (fmap - fmap_warped) ** 2\n    return cost",
            "def get_cost_each(self, pose, fmap, fmap_ref, depth, K, ref_K, scale_factor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n            depth: (b, 1, h, w)\\n            fmap, fmap_ref: (b, c, h, w)\\n        '\n    pose = Pose.from_vec(pose, 'euler')\n    device = depth.device\n    cam = Camera(K=K.float()).scaled(scale_factor).to(device)\n    ref_cam = Camera(K=ref_K.float(), Tcw=pose).scaled(scale_factor).to(device)\n    world_points = cam.reconstruct(depth, frame='w')\n    ref_coords = ref_cam.project(world_points, frame='w', normalize=True)\n    fmap_warped = F.grid_sample(fmap_ref, ref_coords, mode='bilinear', padding_mode='zeros', align_corners=True)\n    cost = (fmap - fmap_warped) ** 2\n    return cost",
            "def get_cost_each(self, pose, fmap, fmap_ref, depth, K, ref_K, scale_factor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n            depth: (b, 1, h, w)\\n            fmap, fmap_ref: (b, c, h, w)\\n        '\n    pose = Pose.from_vec(pose, 'euler')\n    device = depth.device\n    cam = Camera(K=K.float()).scaled(scale_factor).to(device)\n    ref_cam = Camera(K=ref_K.float(), Tcw=pose).scaled(scale_factor).to(device)\n    world_points = cam.reconstruct(depth, frame='w')\n    ref_coords = ref_cam.project(world_points, frame='w', normalize=True)\n    fmap_warped = F.grid_sample(fmap_ref, ref_coords, mode='bilinear', padding_mode='zeros', align_corners=True)\n    cost = (fmap - fmap_warped) ** 2\n    return cost"
        ]
    },
    {
        "func_name": "depth_cost_calc",
        "original": "def depth_cost_calc(self, inv_depth, fmap, fmaps_ref, pose_list, K, ref_K, scale_factor):\n    cost_list = []\n    for (pose, fmap_r) in zip(pose_list, fmaps_ref):\n        cost = self.get_cost_each(pose, fmap, fmap_r, inv2depth(inv_depth), K, ref_K, scale_factor)\n        cost_list.append(cost)\n    cost = torch.stack(cost_list, dim=1).mean(dim=1)\n    return cost",
        "mutated": [
            "def depth_cost_calc(self, inv_depth, fmap, fmaps_ref, pose_list, K, ref_K, scale_factor):\n    if False:\n        i = 10\n    cost_list = []\n    for (pose, fmap_r) in zip(pose_list, fmaps_ref):\n        cost = self.get_cost_each(pose, fmap, fmap_r, inv2depth(inv_depth), K, ref_K, scale_factor)\n        cost_list.append(cost)\n    cost = torch.stack(cost_list, dim=1).mean(dim=1)\n    return cost",
            "def depth_cost_calc(self, inv_depth, fmap, fmaps_ref, pose_list, K, ref_K, scale_factor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cost_list = []\n    for (pose, fmap_r) in zip(pose_list, fmaps_ref):\n        cost = self.get_cost_each(pose, fmap, fmap_r, inv2depth(inv_depth), K, ref_K, scale_factor)\n        cost_list.append(cost)\n    cost = torch.stack(cost_list, dim=1).mean(dim=1)\n    return cost",
            "def depth_cost_calc(self, inv_depth, fmap, fmaps_ref, pose_list, K, ref_K, scale_factor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cost_list = []\n    for (pose, fmap_r) in zip(pose_list, fmaps_ref):\n        cost = self.get_cost_each(pose, fmap, fmap_r, inv2depth(inv_depth), K, ref_K, scale_factor)\n        cost_list.append(cost)\n    cost = torch.stack(cost_list, dim=1).mean(dim=1)\n    return cost",
            "def depth_cost_calc(self, inv_depth, fmap, fmaps_ref, pose_list, K, ref_K, scale_factor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cost_list = []\n    for (pose, fmap_r) in zip(pose_list, fmaps_ref):\n        cost = self.get_cost_each(pose, fmap, fmap_r, inv2depth(inv_depth), K, ref_K, scale_factor)\n        cost_list.append(cost)\n    cost = torch.stack(cost_list, dim=1).mean(dim=1)\n    return cost",
            "def depth_cost_calc(self, inv_depth, fmap, fmaps_ref, pose_list, K, ref_K, scale_factor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cost_list = []\n    for (pose, fmap_r) in zip(pose_list, fmaps_ref):\n        cost = self.get_cost_each(pose, fmap, fmap_r, inv2depth(inv_depth), K, ref_K, scale_factor)\n        cost_list.append(cost)\n    cost = torch.stack(cost_list, dim=1).mean(dim=1)\n    return cost"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, target_image, ref_imgs, intrinsics):\n    \"\"\" Estimate inv depth and  poses \"\"\"\n    fmaps = self.fnet(torch.cat([target_image] + ref_imgs, dim=0))\n    fmaps = torch.split(fmaps, [target_image.shape[0]] * (1 + len(ref_imgs)), dim=0)\n    (fmap1, fmaps_ref) = (fmaps[0], fmaps[1:])\n    assert target_image.shape[2] / fmap1.shape[2] == self.feat_ratio\n    pose_list_init = []\n    for fmap_ref in fmaps_ref:\n        pose_list_init.append(self.pose_head(torch.cat([fmap1, fmap_ref], dim=1)))\n    inv_depth_init = self.depth_head(fmap1, act_fn=F.sigmoid)\n    up_mask = self.upmask_net(fmap1)\n    inv_depth_up_init = self.upsample_depth(inv_depth_init, up_mask, ratio=self.feat_ratio)\n    inv_depth_predictions = [self.scale_inv_depth(inv_depth_up_init)[0]]\n    pose_predictions = [[pose.clone() for pose in pose_list_init]]\n    if self.iters > 0:\n        cnet_depth = self.cnet_depth(target_image)\n        (hidden_d, inp_d) = torch.split(cnet_depth, [self.hdim, self.cdim], dim=1)\n        hidden_d = torch.tanh(hidden_d)\n        inp_d = torch.relu(inp_d)\n        img_pairs = []\n        for ref_img in ref_imgs:\n            img_pairs.append(torch.cat([target_image, ref_img], dim=1))\n        cnet_pose_list = self.cnet_pose(img_pairs)\n        (hidden_p_list, inp_p_list) = ([], [])\n        for cnet_pose in cnet_pose_list:\n            (hidden_p, inp_p) = torch.split(cnet_pose, [self.hdim, self.cdim], dim=1)\n            hidden_p_list.append(torch.tanh(hidden_p))\n            inp_p_list.append(torch.relu(inp_p))\n    pose_list = pose_list_init\n    inv_depth = inv_depth_init\n    inv_depth_up = None\n    for itr in range(self.iters):\n        inv_depth = inv_depth.detach()\n        pose_list = [pose.detach() for pose in pose_list]\n        pose_cost_func_list = []\n        for fmap_ref in fmaps_ref:\n            pose_cost_func_list.append(partial(self.get_cost_each, fmap=fmap1, fmap_ref=fmap_ref, depth=inv2depth(self.scale_inv_depth(inv_depth)[0]), K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio))\n        depth_cost_func = partial(self.depth_cost_calc, fmap=fmap1, fmaps_ref=fmaps_ref, pose_list=pose_list, K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio)\n        (hidden_d, up_mask_seqs, inv_depth_seqs) = self.update_block_depth(hidden_d, depth_cost_func, inv_depth, inp_d, seq_len=self.seq_len, scale_func=self.scale_inv_depth)\n        if not self.inter_sup:\n            (up_mask_seqs, inv_depth_seqs) = ([up_mask_seqs[-1]], [inv_depth_seqs[-1]])\n        for (up_mask_i, inv_depth_i) in zip(up_mask_seqs, inv_depth_seqs):\n            inv_depth_up = self.upsample_depth(inv_depth_i, up_mask_i, ratio=self.feat_ratio)\n            inv_depth_predictions.append(self.scale_inv_depth(inv_depth_up)[0])\n        inv_depth = inv_depth_seqs[-1]\n        pose_list_seqs = [None] * len(pose_list)\n        for (i, (pose, hidden_p)) in enumerate(zip(pose_list, hidden_p_list)):\n            (hidden_p, pose_seqs) = self.update_block_pose(hidden_p, pose_cost_func_list[i], pose, inp_p_list[i], seq_len=self.seq_len)\n            hidden_p_list[i] = hidden_p\n            if not self.inter_sup:\n                pose_seqs = [pose_seqs[-1]]\n            pose_list_seqs[i] = pose_seqs\n        for pose_list_i in zip(*pose_list_seqs):\n            pose_predictions.append([pose.clone() for pose in pose_list_i])\n        pose_list = list(zip(*pose_list_seqs))[-1]\n    if not self.training:\n        return (inv_depth_predictions[-1], torch.stack(pose_predictions[-1], dim=1).view(target_image.shape[0], len(ref_imgs), 6))\n    return (inv_depth_predictions, torch.stack([torch.stack(poses_ref, dim=1) for poses_ref in pose_predictions], dim=2))",
        "mutated": [
            "def forward(self, target_image, ref_imgs, intrinsics):\n    if False:\n        i = 10\n    ' Estimate inv depth and  poses '\n    fmaps = self.fnet(torch.cat([target_image] + ref_imgs, dim=0))\n    fmaps = torch.split(fmaps, [target_image.shape[0]] * (1 + len(ref_imgs)), dim=0)\n    (fmap1, fmaps_ref) = (fmaps[0], fmaps[1:])\n    assert target_image.shape[2] / fmap1.shape[2] == self.feat_ratio\n    pose_list_init = []\n    for fmap_ref in fmaps_ref:\n        pose_list_init.append(self.pose_head(torch.cat([fmap1, fmap_ref], dim=1)))\n    inv_depth_init = self.depth_head(fmap1, act_fn=F.sigmoid)\n    up_mask = self.upmask_net(fmap1)\n    inv_depth_up_init = self.upsample_depth(inv_depth_init, up_mask, ratio=self.feat_ratio)\n    inv_depth_predictions = [self.scale_inv_depth(inv_depth_up_init)[0]]\n    pose_predictions = [[pose.clone() for pose in pose_list_init]]\n    if self.iters > 0:\n        cnet_depth = self.cnet_depth(target_image)\n        (hidden_d, inp_d) = torch.split(cnet_depth, [self.hdim, self.cdim], dim=1)\n        hidden_d = torch.tanh(hidden_d)\n        inp_d = torch.relu(inp_d)\n        img_pairs = []\n        for ref_img in ref_imgs:\n            img_pairs.append(torch.cat([target_image, ref_img], dim=1))\n        cnet_pose_list = self.cnet_pose(img_pairs)\n        (hidden_p_list, inp_p_list) = ([], [])\n        for cnet_pose in cnet_pose_list:\n            (hidden_p, inp_p) = torch.split(cnet_pose, [self.hdim, self.cdim], dim=1)\n            hidden_p_list.append(torch.tanh(hidden_p))\n            inp_p_list.append(torch.relu(inp_p))\n    pose_list = pose_list_init\n    inv_depth = inv_depth_init\n    inv_depth_up = None\n    for itr in range(self.iters):\n        inv_depth = inv_depth.detach()\n        pose_list = [pose.detach() for pose in pose_list]\n        pose_cost_func_list = []\n        for fmap_ref in fmaps_ref:\n            pose_cost_func_list.append(partial(self.get_cost_each, fmap=fmap1, fmap_ref=fmap_ref, depth=inv2depth(self.scale_inv_depth(inv_depth)[0]), K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio))\n        depth_cost_func = partial(self.depth_cost_calc, fmap=fmap1, fmaps_ref=fmaps_ref, pose_list=pose_list, K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio)\n        (hidden_d, up_mask_seqs, inv_depth_seqs) = self.update_block_depth(hidden_d, depth_cost_func, inv_depth, inp_d, seq_len=self.seq_len, scale_func=self.scale_inv_depth)\n        if not self.inter_sup:\n            (up_mask_seqs, inv_depth_seqs) = ([up_mask_seqs[-1]], [inv_depth_seqs[-1]])\n        for (up_mask_i, inv_depth_i) in zip(up_mask_seqs, inv_depth_seqs):\n            inv_depth_up = self.upsample_depth(inv_depth_i, up_mask_i, ratio=self.feat_ratio)\n            inv_depth_predictions.append(self.scale_inv_depth(inv_depth_up)[0])\n        inv_depth = inv_depth_seqs[-1]\n        pose_list_seqs = [None] * len(pose_list)\n        for (i, (pose, hidden_p)) in enumerate(zip(pose_list, hidden_p_list)):\n            (hidden_p, pose_seqs) = self.update_block_pose(hidden_p, pose_cost_func_list[i], pose, inp_p_list[i], seq_len=self.seq_len)\n            hidden_p_list[i] = hidden_p\n            if not self.inter_sup:\n                pose_seqs = [pose_seqs[-1]]\n            pose_list_seqs[i] = pose_seqs\n        for pose_list_i in zip(*pose_list_seqs):\n            pose_predictions.append([pose.clone() for pose in pose_list_i])\n        pose_list = list(zip(*pose_list_seqs))[-1]\n    if not self.training:\n        return (inv_depth_predictions[-1], torch.stack(pose_predictions[-1], dim=1).view(target_image.shape[0], len(ref_imgs), 6))\n    return (inv_depth_predictions, torch.stack([torch.stack(poses_ref, dim=1) for poses_ref in pose_predictions], dim=2))",
            "def forward(self, target_image, ref_imgs, intrinsics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Estimate inv depth and  poses '\n    fmaps = self.fnet(torch.cat([target_image] + ref_imgs, dim=0))\n    fmaps = torch.split(fmaps, [target_image.shape[0]] * (1 + len(ref_imgs)), dim=0)\n    (fmap1, fmaps_ref) = (fmaps[0], fmaps[1:])\n    assert target_image.shape[2] / fmap1.shape[2] == self.feat_ratio\n    pose_list_init = []\n    for fmap_ref in fmaps_ref:\n        pose_list_init.append(self.pose_head(torch.cat([fmap1, fmap_ref], dim=1)))\n    inv_depth_init = self.depth_head(fmap1, act_fn=F.sigmoid)\n    up_mask = self.upmask_net(fmap1)\n    inv_depth_up_init = self.upsample_depth(inv_depth_init, up_mask, ratio=self.feat_ratio)\n    inv_depth_predictions = [self.scale_inv_depth(inv_depth_up_init)[0]]\n    pose_predictions = [[pose.clone() for pose in pose_list_init]]\n    if self.iters > 0:\n        cnet_depth = self.cnet_depth(target_image)\n        (hidden_d, inp_d) = torch.split(cnet_depth, [self.hdim, self.cdim], dim=1)\n        hidden_d = torch.tanh(hidden_d)\n        inp_d = torch.relu(inp_d)\n        img_pairs = []\n        for ref_img in ref_imgs:\n            img_pairs.append(torch.cat([target_image, ref_img], dim=1))\n        cnet_pose_list = self.cnet_pose(img_pairs)\n        (hidden_p_list, inp_p_list) = ([], [])\n        for cnet_pose in cnet_pose_list:\n            (hidden_p, inp_p) = torch.split(cnet_pose, [self.hdim, self.cdim], dim=1)\n            hidden_p_list.append(torch.tanh(hidden_p))\n            inp_p_list.append(torch.relu(inp_p))\n    pose_list = pose_list_init\n    inv_depth = inv_depth_init\n    inv_depth_up = None\n    for itr in range(self.iters):\n        inv_depth = inv_depth.detach()\n        pose_list = [pose.detach() for pose in pose_list]\n        pose_cost_func_list = []\n        for fmap_ref in fmaps_ref:\n            pose_cost_func_list.append(partial(self.get_cost_each, fmap=fmap1, fmap_ref=fmap_ref, depth=inv2depth(self.scale_inv_depth(inv_depth)[0]), K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio))\n        depth_cost_func = partial(self.depth_cost_calc, fmap=fmap1, fmaps_ref=fmaps_ref, pose_list=pose_list, K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio)\n        (hidden_d, up_mask_seqs, inv_depth_seqs) = self.update_block_depth(hidden_d, depth_cost_func, inv_depth, inp_d, seq_len=self.seq_len, scale_func=self.scale_inv_depth)\n        if not self.inter_sup:\n            (up_mask_seqs, inv_depth_seqs) = ([up_mask_seqs[-1]], [inv_depth_seqs[-1]])\n        for (up_mask_i, inv_depth_i) in zip(up_mask_seqs, inv_depth_seqs):\n            inv_depth_up = self.upsample_depth(inv_depth_i, up_mask_i, ratio=self.feat_ratio)\n            inv_depth_predictions.append(self.scale_inv_depth(inv_depth_up)[0])\n        inv_depth = inv_depth_seqs[-1]\n        pose_list_seqs = [None] * len(pose_list)\n        for (i, (pose, hidden_p)) in enumerate(zip(pose_list, hidden_p_list)):\n            (hidden_p, pose_seqs) = self.update_block_pose(hidden_p, pose_cost_func_list[i], pose, inp_p_list[i], seq_len=self.seq_len)\n            hidden_p_list[i] = hidden_p\n            if not self.inter_sup:\n                pose_seqs = [pose_seqs[-1]]\n            pose_list_seqs[i] = pose_seqs\n        for pose_list_i in zip(*pose_list_seqs):\n            pose_predictions.append([pose.clone() for pose in pose_list_i])\n        pose_list = list(zip(*pose_list_seqs))[-1]\n    if not self.training:\n        return (inv_depth_predictions[-1], torch.stack(pose_predictions[-1], dim=1).view(target_image.shape[0], len(ref_imgs), 6))\n    return (inv_depth_predictions, torch.stack([torch.stack(poses_ref, dim=1) for poses_ref in pose_predictions], dim=2))",
            "def forward(self, target_image, ref_imgs, intrinsics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Estimate inv depth and  poses '\n    fmaps = self.fnet(torch.cat([target_image] + ref_imgs, dim=0))\n    fmaps = torch.split(fmaps, [target_image.shape[0]] * (1 + len(ref_imgs)), dim=0)\n    (fmap1, fmaps_ref) = (fmaps[0], fmaps[1:])\n    assert target_image.shape[2] / fmap1.shape[2] == self.feat_ratio\n    pose_list_init = []\n    for fmap_ref in fmaps_ref:\n        pose_list_init.append(self.pose_head(torch.cat([fmap1, fmap_ref], dim=1)))\n    inv_depth_init = self.depth_head(fmap1, act_fn=F.sigmoid)\n    up_mask = self.upmask_net(fmap1)\n    inv_depth_up_init = self.upsample_depth(inv_depth_init, up_mask, ratio=self.feat_ratio)\n    inv_depth_predictions = [self.scale_inv_depth(inv_depth_up_init)[0]]\n    pose_predictions = [[pose.clone() for pose in pose_list_init]]\n    if self.iters > 0:\n        cnet_depth = self.cnet_depth(target_image)\n        (hidden_d, inp_d) = torch.split(cnet_depth, [self.hdim, self.cdim], dim=1)\n        hidden_d = torch.tanh(hidden_d)\n        inp_d = torch.relu(inp_d)\n        img_pairs = []\n        for ref_img in ref_imgs:\n            img_pairs.append(torch.cat([target_image, ref_img], dim=1))\n        cnet_pose_list = self.cnet_pose(img_pairs)\n        (hidden_p_list, inp_p_list) = ([], [])\n        for cnet_pose in cnet_pose_list:\n            (hidden_p, inp_p) = torch.split(cnet_pose, [self.hdim, self.cdim], dim=1)\n            hidden_p_list.append(torch.tanh(hidden_p))\n            inp_p_list.append(torch.relu(inp_p))\n    pose_list = pose_list_init\n    inv_depth = inv_depth_init\n    inv_depth_up = None\n    for itr in range(self.iters):\n        inv_depth = inv_depth.detach()\n        pose_list = [pose.detach() for pose in pose_list]\n        pose_cost_func_list = []\n        for fmap_ref in fmaps_ref:\n            pose_cost_func_list.append(partial(self.get_cost_each, fmap=fmap1, fmap_ref=fmap_ref, depth=inv2depth(self.scale_inv_depth(inv_depth)[0]), K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio))\n        depth_cost_func = partial(self.depth_cost_calc, fmap=fmap1, fmaps_ref=fmaps_ref, pose_list=pose_list, K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio)\n        (hidden_d, up_mask_seqs, inv_depth_seqs) = self.update_block_depth(hidden_d, depth_cost_func, inv_depth, inp_d, seq_len=self.seq_len, scale_func=self.scale_inv_depth)\n        if not self.inter_sup:\n            (up_mask_seqs, inv_depth_seqs) = ([up_mask_seqs[-1]], [inv_depth_seqs[-1]])\n        for (up_mask_i, inv_depth_i) in zip(up_mask_seqs, inv_depth_seqs):\n            inv_depth_up = self.upsample_depth(inv_depth_i, up_mask_i, ratio=self.feat_ratio)\n            inv_depth_predictions.append(self.scale_inv_depth(inv_depth_up)[0])\n        inv_depth = inv_depth_seqs[-1]\n        pose_list_seqs = [None] * len(pose_list)\n        for (i, (pose, hidden_p)) in enumerate(zip(pose_list, hidden_p_list)):\n            (hidden_p, pose_seqs) = self.update_block_pose(hidden_p, pose_cost_func_list[i], pose, inp_p_list[i], seq_len=self.seq_len)\n            hidden_p_list[i] = hidden_p\n            if not self.inter_sup:\n                pose_seqs = [pose_seqs[-1]]\n            pose_list_seqs[i] = pose_seqs\n        for pose_list_i in zip(*pose_list_seqs):\n            pose_predictions.append([pose.clone() for pose in pose_list_i])\n        pose_list = list(zip(*pose_list_seqs))[-1]\n    if not self.training:\n        return (inv_depth_predictions[-1], torch.stack(pose_predictions[-1], dim=1).view(target_image.shape[0], len(ref_imgs), 6))\n    return (inv_depth_predictions, torch.stack([torch.stack(poses_ref, dim=1) for poses_ref in pose_predictions], dim=2))",
            "def forward(self, target_image, ref_imgs, intrinsics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Estimate inv depth and  poses '\n    fmaps = self.fnet(torch.cat([target_image] + ref_imgs, dim=0))\n    fmaps = torch.split(fmaps, [target_image.shape[0]] * (1 + len(ref_imgs)), dim=0)\n    (fmap1, fmaps_ref) = (fmaps[0], fmaps[1:])\n    assert target_image.shape[2] / fmap1.shape[2] == self.feat_ratio\n    pose_list_init = []\n    for fmap_ref in fmaps_ref:\n        pose_list_init.append(self.pose_head(torch.cat([fmap1, fmap_ref], dim=1)))\n    inv_depth_init = self.depth_head(fmap1, act_fn=F.sigmoid)\n    up_mask = self.upmask_net(fmap1)\n    inv_depth_up_init = self.upsample_depth(inv_depth_init, up_mask, ratio=self.feat_ratio)\n    inv_depth_predictions = [self.scale_inv_depth(inv_depth_up_init)[0]]\n    pose_predictions = [[pose.clone() for pose in pose_list_init]]\n    if self.iters > 0:\n        cnet_depth = self.cnet_depth(target_image)\n        (hidden_d, inp_d) = torch.split(cnet_depth, [self.hdim, self.cdim], dim=1)\n        hidden_d = torch.tanh(hidden_d)\n        inp_d = torch.relu(inp_d)\n        img_pairs = []\n        for ref_img in ref_imgs:\n            img_pairs.append(torch.cat([target_image, ref_img], dim=1))\n        cnet_pose_list = self.cnet_pose(img_pairs)\n        (hidden_p_list, inp_p_list) = ([], [])\n        for cnet_pose in cnet_pose_list:\n            (hidden_p, inp_p) = torch.split(cnet_pose, [self.hdim, self.cdim], dim=1)\n            hidden_p_list.append(torch.tanh(hidden_p))\n            inp_p_list.append(torch.relu(inp_p))\n    pose_list = pose_list_init\n    inv_depth = inv_depth_init\n    inv_depth_up = None\n    for itr in range(self.iters):\n        inv_depth = inv_depth.detach()\n        pose_list = [pose.detach() for pose in pose_list]\n        pose_cost_func_list = []\n        for fmap_ref in fmaps_ref:\n            pose_cost_func_list.append(partial(self.get_cost_each, fmap=fmap1, fmap_ref=fmap_ref, depth=inv2depth(self.scale_inv_depth(inv_depth)[0]), K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio))\n        depth_cost_func = partial(self.depth_cost_calc, fmap=fmap1, fmaps_ref=fmaps_ref, pose_list=pose_list, K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio)\n        (hidden_d, up_mask_seqs, inv_depth_seqs) = self.update_block_depth(hidden_d, depth_cost_func, inv_depth, inp_d, seq_len=self.seq_len, scale_func=self.scale_inv_depth)\n        if not self.inter_sup:\n            (up_mask_seqs, inv_depth_seqs) = ([up_mask_seqs[-1]], [inv_depth_seqs[-1]])\n        for (up_mask_i, inv_depth_i) in zip(up_mask_seqs, inv_depth_seqs):\n            inv_depth_up = self.upsample_depth(inv_depth_i, up_mask_i, ratio=self.feat_ratio)\n            inv_depth_predictions.append(self.scale_inv_depth(inv_depth_up)[0])\n        inv_depth = inv_depth_seqs[-1]\n        pose_list_seqs = [None] * len(pose_list)\n        for (i, (pose, hidden_p)) in enumerate(zip(pose_list, hidden_p_list)):\n            (hidden_p, pose_seqs) = self.update_block_pose(hidden_p, pose_cost_func_list[i], pose, inp_p_list[i], seq_len=self.seq_len)\n            hidden_p_list[i] = hidden_p\n            if not self.inter_sup:\n                pose_seqs = [pose_seqs[-1]]\n            pose_list_seqs[i] = pose_seqs\n        for pose_list_i in zip(*pose_list_seqs):\n            pose_predictions.append([pose.clone() for pose in pose_list_i])\n        pose_list = list(zip(*pose_list_seqs))[-1]\n    if not self.training:\n        return (inv_depth_predictions[-1], torch.stack(pose_predictions[-1], dim=1).view(target_image.shape[0], len(ref_imgs), 6))\n    return (inv_depth_predictions, torch.stack([torch.stack(poses_ref, dim=1) for poses_ref in pose_predictions], dim=2))",
            "def forward(self, target_image, ref_imgs, intrinsics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Estimate inv depth and  poses '\n    fmaps = self.fnet(torch.cat([target_image] + ref_imgs, dim=0))\n    fmaps = torch.split(fmaps, [target_image.shape[0]] * (1 + len(ref_imgs)), dim=0)\n    (fmap1, fmaps_ref) = (fmaps[0], fmaps[1:])\n    assert target_image.shape[2] / fmap1.shape[2] == self.feat_ratio\n    pose_list_init = []\n    for fmap_ref in fmaps_ref:\n        pose_list_init.append(self.pose_head(torch.cat([fmap1, fmap_ref], dim=1)))\n    inv_depth_init = self.depth_head(fmap1, act_fn=F.sigmoid)\n    up_mask = self.upmask_net(fmap1)\n    inv_depth_up_init = self.upsample_depth(inv_depth_init, up_mask, ratio=self.feat_ratio)\n    inv_depth_predictions = [self.scale_inv_depth(inv_depth_up_init)[0]]\n    pose_predictions = [[pose.clone() for pose in pose_list_init]]\n    if self.iters > 0:\n        cnet_depth = self.cnet_depth(target_image)\n        (hidden_d, inp_d) = torch.split(cnet_depth, [self.hdim, self.cdim], dim=1)\n        hidden_d = torch.tanh(hidden_d)\n        inp_d = torch.relu(inp_d)\n        img_pairs = []\n        for ref_img in ref_imgs:\n            img_pairs.append(torch.cat([target_image, ref_img], dim=1))\n        cnet_pose_list = self.cnet_pose(img_pairs)\n        (hidden_p_list, inp_p_list) = ([], [])\n        for cnet_pose in cnet_pose_list:\n            (hidden_p, inp_p) = torch.split(cnet_pose, [self.hdim, self.cdim], dim=1)\n            hidden_p_list.append(torch.tanh(hidden_p))\n            inp_p_list.append(torch.relu(inp_p))\n    pose_list = pose_list_init\n    inv_depth = inv_depth_init\n    inv_depth_up = None\n    for itr in range(self.iters):\n        inv_depth = inv_depth.detach()\n        pose_list = [pose.detach() for pose in pose_list]\n        pose_cost_func_list = []\n        for fmap_ref in fmaps_ref:\n            pose_cost_func_list.append(partial(self.get_cost_each, fmap=fmap1, fmap_ref=fmap_ref, depth=inv2depth(self.scale_inv_depth(inv_depth)[0]), K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio))\n        depth_cost_func = partial(self.depth_cost_calc, fmap=fmap1, fmaps_ref=fmaps_ref, pose_list=pose_list, K=intrinsics, ref_K=intrinsics, scale_factor=1.0 / self.feat_ratio)\n        (hidden_d, up_mask_seqs, inv_depth_seqs) = self.update_block_depth(hidden_d, depth_cost_func, inv_depth, inp_d, seq_len=self.seq_len, scale_func=self.scale_inv_depth)\n        if not self.inter_sup:\n            (up_mask_seqs, inv_depth_seqs) = ([up_mask_seqs[-1]], [inv_depth_seqs[-1]])\n        for (up_mask_i, inv_depth_i) in zip(up_mask_seqs, inv_depth_seqs):\n            inv_depth_up = self.upsample_depth(inv_depth_i, up_mask_i, ratio=self.feat_ratio)\n            inv_depth_predictions.append(self.scale_inv_depth(inv_depth_up)[0])\n        inv_depth = inv_depth_seqs[-1]\n        pose_list_seqs = [None] * len(pose_list)\n        for (i, (pose, hidden_p)) in enumerate(zip(pose_list, hidden_p_list)):\n            (hidden_p, pose_seqs) = self.update_block_pose(hidden_p, pose_cost_func_list[i], pose, inp_p_list[i], seq_len=self.seq_len)\n            hidden_p_list[i] = hidden_p\n            if not self.inter_sup:\n                pose_seqs = [pose_seqs[-1]]\n            pose_list_seqs[i] = pose_seqs\n        for pose_list_i in zip(*pose_list_seqs):\n            pose_predictions.append([pose.clone() for pose in pose_list_i])\n        pose_list = list(zip(*pose_list_seqs))[-1]\n    if not self.training:\n        return (inv_depth_predictions[-1], torch.stack(pose_predictions[-1], dim=1).view(target_image.shape[0], len(ref_imgs), 6))\n    return (inv_depth_predictions, torch.stack([torch.stack(poses_ref, dim=1) for poses_ref in pose_predictions], dim=2))"
        ]
    }
]