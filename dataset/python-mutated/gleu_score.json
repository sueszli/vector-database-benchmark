[
    {
        "func_name": "sentence_gleu",
        "original": "def sentence_gleu(references, hypothesis, min_len=1, max_len=4):\n    \"\"\"\n    Calculates the sentence level GLEU (Google-BLEU) score described in\n\n        Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\n        Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,\n        Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,\n        Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,\n        George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith,\n        Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\n        Jeffrey Dean. (2016) Google\u2019s Neural Machine Translation System:\n        Bridging the Gap between Human and Machine Translation.\n        eprint arXiv:1609.08144. https://arxiv.org/pdf/1609.08144v2.pdf\n        Retrieved on 27 Oct 2016.\n\n    From Wu et al. (2016):\n        \"The BLEU score has some undesirable properties when used for single\n         sentences, as it was designed to be a corpus measure. We therefore\n         use a slightly different score for our RL experiments which we call\n         the 'GLEU score'. For the GLEU score, we record all sub-sequences of\n         1, 2, 3 or 4 tokens in output and target sequence (n-grams). We then\n         compute a recall, which is the ratio of the number of matching n-grams\n         to the number of total n-grams in the target (ground truth) sequence,\n         and a precision, which is the ratio of the number of matching n-grams\n         to the number of total n-grams in the generated output sequence. Then\n         GLEU score is simply the minimum of recall and precision. This GLEU\n         score's range is always between 0 (no matches) and 1 (all match) and\n         it is symmetrical when switching output and target. According to\n         our experiments, GLEU score correlates quite well with the BLEU\n         metric on a corpus level but does not have its drawbacks for our per\n         sentence reward objective.\"\n\n    Note: The initial implementation only allowed a single reference, but now\n          a list of references is required (which is consistent with\n          bleu_score.sentence_bleu()).\n\n    The infamous \"the the the ... \" example\n\n        >>> ref = 'the cat is on the mat'.split()\n        >>> hyp = 'the the the the the the the'.split()\n        >>> sentence_gleu([ref], hyp)  # doctest: +ELLIPSIS\n        0.0909...\n\n    An example to evaluate normal machine translation outputs\n\n        >>> ref1 = str('It is a guide to action that ensures that the military '\n        ...            'will forever heed Party commands').split()\n        >>> hyp1 = str('It is a guide to action which ensures that the military '\n        ...            'always obeys the commands of the party').split()\n        >>> hyp2 = str('It is to insure the troops forever hearing the activity '\n        ...            'guidebook that party direct').split()\n        >>> sentence_gleu([ref1], hyp1) # doctest: +ELLIPSIS\n        0.4393...\n        >>> sentence_gleu([ref1], hyp2) # doctest: +ELLIPSIS\n        0.1206...\n\n    :param references: a list of reference sentences\n    :type references: list(list(str))\n    :param hypothesis: a hypothesis sentence\n    :type hypothesis: list(str)\n    :param min_len: The minimum order of n-gram this function should extract.\n    :type min_len: int\n    :param max_len: The maximum order of n-gram this function should extract.\n    :type max_len: int\n    :return: the sentence level GLEU score.\n    :rtype: float\n    \"\"\"\n    return corpus_gleu([references], [hypothesis], min_len=min_len, max_len=max_len)",
        "mutated": [
            "def sentence_gleu(references, hypothesis, min_len=1, max_len=4):\n    if False:\n        i = 10\n    '\\n    Calculates the sentence level GLEU (Google-BLEU) score described in\\n\\n        Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\\n        Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,\\n        Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,\\n        Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,\\n        George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith,\\n        Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\\n        Jeffrey Dean. (2016) Google\u2019s Neural Machine Translation System:\\n        Bridging the Gap between Human and Machine Translation.\\n        eprint arXiv:1609.08144. https://arxiv.org/pdf/1609.08144v2.pdf\\n        Retrieved on 27 Oct 2016.\\n\\n    From Wu et al. (2016):\\n        \"The BLEU score has some undesirable properties when used for single\\n         sentences, as it was designed to be a corpus measure. We therefore\\n         use a slightly different score for our RL experiments which we call\\n         the \\'GLEU score\\'. For the GLEU score, we record all sub-sequences of\\n         1, 2, 3 or 4 tokens in output and target sequence (n-grams). We then\\n         compute a recall, which is the ratio of the number of matching n-grams\\n         to the number of total n-grams in the target (ground truth) sequence,\\n         and a precision, which is the ratio of the number of matching n-grams\\n         to the number of total n-grams in the generated output sequence. Then\\n         GLEU score is simply the minimum of recall and precision. This GLEU\\n         score\\'s range is always between 0 (no matches) and 1 (all match) and\\n         it is symmetrical when switching output and target. According to\\n         our experiments, GLEU score correlates quite well with the BLEU\\n         metric on a corpus level but does not have its drawbacks for our per\\n         sentence reward objective.\"\\n\\n    Note: The initial implementation only allowed a single reference, but now\\n          a list of references is required (which is consistent with\\n          bleu_score.sentence_bleu()).\\n\\n    The infamous \"the the the ... \" example\\n\\n        >>> ref = \\'the cat is on the mat\\'.split()\\n        >>> hyp = \\'the the the the the the the\\'.split()\\n        >>> sentence_gleu([ref], hyp)  # doctest: +ELLIPSIS\\n        0.0909...\\n\\n    An example to evaluate normal machine translation outputs\\n\\n        >>> ref1 = str(\\'It is a guide to action that ensures that the military \\'\\n        ...            \\'will forever heed Party commands\\').split()\\n        >>> hyp1 = str(\\'It is a guide to action which ensures that the military \\'\\n        ...            \\'always obeys the commands of the party\\').split()\\n        >>> hyp2 = str(\\'It is to insure the troops forever hearing the activity \\'\\n        ...            \\'guidebook that party direct\\').split()\\n        >>> sentence_gleu([ref1], hyp1) # doctest: +ELLIPSIS\\n        0.4393...\\n        >>> sentence_gleu([ref1], hyp2) # doctest: +ELLIPSIS\\n        0.1206...\\n\\n    :param references: a list of reference sentences\\n    :type references: list(list(str))\\n    :param hypothesis: a hypothesis sentence\\n    :type hypothesis: list(str)\\n    :param min_len: The minimum order of n-gram this function should extract.\\n    :type min_len: int\\n    :param max_len: The maximum order of n-gram this function should extract.\\n    :type max_len: int\\n    :return: the sentence level GLEU score.\\n    :rtype: float\\n    '\n    return corpus_gleu([references], [hypothesis], min_len=min_len, max_len=max_len)",
            "def sentence_gleu(references, hypothesis, min_len=1, max_len=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculates the sentence level GLEU (Google-BLEU) score described in\\n\\n        Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\\n        Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,\\n        Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,\\n        Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,\\n        George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith,\\n        Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\\n        Jeffrey Dean. (2016) Google\u2019s Neural Machine Translation System:\\n        Bridging the Gap between Human and Machine Translation.\\n        eprint arXiv:1609.08144. https://arxiv.org/pdf/1609.08144v2.pdf\\n        Retrieved on 27 Oct 2016.\\n\\n    From Wu et al. (2016):\\n        \"The BLEU score has some undesirable properties when used for single\\n         sentences, as it was designed to be a corpus measure. We therefore\\n         use a slightly different score for our RL experiments which we call\\n         the \\'GLEU score\\'. For the GLEU score, we record all sub-sequences of\\n         1, 2, 3 or 4 tokens in output and target sequence (n-grams). We then\\n         compute a recall, which is the ratio of the number of matching n-grams\\n         to the number of total n-grams in the target (ground truth) sequence,\\n         and a precision, which is the ratio of the number of matching n-grams\\n         to the number of total n-grams in the generated output sequence. Then\\n         GLEU score is simply the minimum of recall and precision. This GLEU\\n         score\\'s range is always between 0 (no matches) and 1 (all match) and\\n         it is symmetrical when switching output and target. According to\\n         our experiments, GLEU score correlates quite well with the BLEU\\n         metric on a corpus level but does not have its drawbacks for our per\\n         sentence reward objective.\"\\n\\n    Note: The initial implementation only allowed a single reference, but now\\n          a list of references is required (which is consistent with\\n          bleu_score.sentence_bleu()).\\n\\n    The infamous \"the the the ... \" example\\n\\n        >>> ref = \\'the cat is on the mat\\'.split()\\n        >>> hyp = \\'the the the the the the the\\'.split()\\n        >>> sentence_gleu([ref], hyp)  # doctest: +ELLIPSIS\\n        0.0909...\\n\\n    An example to evaluate normal machine translation outputs\\n\\n        >>> ref1 = str(\\'It is a guide to action that ensures that the military \\'\\n        ...            \\'will forever heed Party commands\\').split()\\n        >>> hyp1 = str(\\'It is a guide to action which ensures that the military \\'\\n        ...            \\'always obeys the commands of the party\\').split()\\n        >>> hyp2 = str(\\'It is to insure the troops forever hearing the activity \\'\\n        ...            \\'guidebook that party direct\\').split()\\n        >>> sentence_gleu([ref1], hyp1) # doctest: +ELLIPSIS\\n        0.4393...\\n        >>> sentence_gleu([ref1], hyp2) # doctest: +ELLIPSIS\\n        0.1206...\\n\\n    :param references: a list of reference sentences\\n    :type references: list(list(str))\\n    :param hypothesis: a hypothesis sentence\\n    :type hypothesis: list(str)\\n    :param min_len: The minimum order of n-gram this function should extract.\\n    :type min_len: int\\n    :param max_len: The maximum order of n-gram this function should extract.\\n    :type max_len: int\\n    :return: the sentence level GLEU score.\\n    :rtype: float\\n    '\n    return corpus_gleu([references], [hypothesis], min_len=min_len, max_len=max_len)",
            "def sentence_gleu(references, hypothesis, min_len=1, max_len=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculates the sentence level GLEU (Google-BLEU) score described in\\n\\n        Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\\n        Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,\\n        Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,\\n        Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,\\n        George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith,\\n        Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\\n        Jeffrey Dean. (2016) Google\u2019s Neural Machine Translation System:\\n        Bridging the Gap between Human and Machine Translation.\\n        eprint arXiv:1609.08144. https://arxiv.org/pdf/1609.08144v2.pdf\\n        Retrieved on 27 Oct 2016.\\n\\n    From Wu et al. (2016):\\n        \"The BLEU score has some undesirable properties when used for single\\n         sentences, as it was designed to be a corpus measure. We therefore\\n         use a slightly different score for our RL experiments which we call\\n         the \\'GLEU score\\'. For the GLEU score, we record all sub-sequences of\\n         1, 2, 3 or 4 tokens in output and target sequence (n-grams). We then\\n         compute a recall, which is the ratio of the number of matching n-grams\\n         to the number of total n-grams in the target (ground truth) sequence,\\n         and a precision, which is the ratio of the number of matching n-grams\\n         to the number of total n-grams in the generated output sequence. Then\\n         GLEU score is simply the minimum of recall and precision. This GLEU\\n         score\\'s range is always between 0 (no matches) and 1 (all match) and\\n         it is symmetrical when switching output and target. According to\\n         our experiments, GLEU score correlates quite well with the BLEU\\n         metric on a corpus level but does not have its drawbacks for our per\\n         sentence reward objective.\"\\n\\n    Note: The initial implementation only allowed a single reference, but now\\n          a list of references is required (which is consistent with\\n          bleu_score.sentence_bleu()).\\n\\n    The infamous \"the the the ... \" example\\n\\n        >>> ref = \\'the cat is on the mat\\'.split()\\n        >>> hyp = \\'the the the the the the the\\'.split()\\n        >>> sentence_gleu([ref], hyp)  # doctest: +ELLIPSIS\\n        0.0909...\\n\\n    An example to evaluate normal machine translation outputs\\n\\n        >>> ref1 = str(\\'It is a guide to action that ensures that the military \\'\\n        ...            \\'will forever heed Party commands\\').split()\\n        >>> hyp1 = str(\\'It is a guide to action which ensures that the military \\'\\n        ...            \\'always obeys the commands of the party\\').split()\\n        >>> hyp2 = str(\\'It is to insure the troops forever hearing the activity \\'\\n        ...            \\'guidebook that party direct\\').split()\\n        >>> sentence_gleu([ref1], hyp1) # doctest: +ELLIPSIS\\n        0.4393...\\n        >>> sentence_gleu([ref1], hyp2) # doctest: +ELLIPSIS\\n        0.1206...\\n\\n    :param references: a list of reference sentences\\n    :type references: list(list(str))\\n    :param hypothesis: a hypothesis sentence\\n    :type hypothesis: list(str)\\n    :param min_len: The minimum order of n-gram this function should extract.\\n    :type min_len: int\\n    :param max_len: The maximum order of n-gram this function should extract.\\n    :type max_len: int\\n    :return: the sentence level GLEU score.\\n    :rtype: float\\n    '\n    return corpus_gleu([references], [hypothesis], min_len=min_len, max_len=max_len)",
            "def sentence_gleu(references, hypothesis, min_len=1, max_len=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculates the sentence level GLEU (Google-BLEU) score described in\\n\\n        Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\\n        Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,\\n        Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,\\n        Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,\\n        George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith,\\n        Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\\n        Jeffrey Dean. (2016) Google\u2019s Neural Machine Translation System:\\n        Bridging the Gap between Human and Machine Translation.\\n        eprint arXiv:1609.08144. https://arxiv.org/pdf/1609.08144v2.pdf\\n        Retrieved on 27 Oct 2016.\\n\\n    From Wu et al. (2016):\\n        \"The BLEU score has some undesirable properties when used for single\\n         sentences, as it was designed to be a corpus measure. We therefore\\n         use a slightly different score for our RL experiments which we call\\n         the \\'GLEU score\\'. For the GLEU score, we record all sub-sequences of\\n         1, 2, 3 or 4 tokens in output and target sequence (n-grams). We then\\n         compute a recall, which is the ratio of the number of matching n-grams\\n         to the number of total n-grams in the target (ground truth) sequence,\\n         and a precision, which is the ratio of the number of matching n-grams\\n         to the number of total n-grams in the generated output sequence. Then\\n         GLEU score is simply the minimum of recall and precision. This GLEU\\n         score\\'s range is always between 0 (no matches) and 1 (all match) and\\n         it is symmetrical when switching output and target. According to\\n         our experiments, GLEU score correlates quite well with the BLEU\\n         metric on a corpus level but does not have its drawbacks for our per\\n         sentence reward objective.\"\\n\\n    Note: The initial implementation only allowed a single reference, but now\\n          a list of references is required (which is consistent with\\n          bleu_score.sentence_bleu()).\\n\\n    The infamous \"the the the ... \" example\\n\\n        >>> ref = \\'the cat is on the mat\\'.split()\\n        >>> hyp = \\'the the the the the the the\\'.split()\\n        >>> sentence_gleu([ref], hyp)  # doctest: +ELLIPSIS\\n        0.0909...\\n\\n    An example to evaluate normal machine translation outputs\\n\\n        >>> ref1 = str(\\'It is a guide to action that ensures that the military \\'\\n        ...            \\'will forever heed Party commands\\').split()\\n        >>> hyp1 = str(\\'It is a guide to action which ensures that the military \\'\\n        ...            \\'always obeys the commands of the party\\').split()\\n        >>> hyp2 = str(\\'It is to insure the troops forever hearing the activity \\'\\n        ...            \\'guidebook that party direct\\').split()\\n        >>> sentence_gleu([ref1], hyp1) # doctest: +ELLIPSIS\\n        0.4393...\\n        >>> sentence_gleu([ref1], hyp2) # doctest: +ELLIPSIS\\n        0.1206...\\n\\n    :param references: a list of reference sentences\\n    :type references: list(list(str))\\n    :param hypothesis: a hypothesis sentence\\n    :type hypothesis: list(str)\\n    :param min_len: The minimum order of n-gram this function should extract.\\n    :type min_len: int\\n    :param max_len: The maximum order of n-gram this function should extract.\\n    :type max_len: int\\n    :return: the sentence level GLEU score.\\n    :rtype: float\\n    '\n    return corpus_gleu([references], [hypothesis], min_len=min_len, max_len=max_len)",
            "def sentence_gleu(references, hypothesis, min_len=1, max_len=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculates the sentence level GLEU (Google-BLEU) score described in\\n\\n        Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\\n        Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,\\n        Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,\\n        Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,\\n        George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith,\\n        Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\\n        Jeffrey Dean. (2016) Google\u2019s Neural Machine Translation System:\\n        Bridging the Gap between Human and Machine Translation.\\n        eprint arXiv:1609.08144. https://arxiv.org/pdf/1609.08144v2.pdf\\n        Retrieved on 27 Oct 2016.\\n\\n    From Wu et al. (2016):\\n        \"The BLEU score has some undesirable properties when used for single\\n         sentences, as it was designed to be a corpus measure. We therefore\\n         use a slightly different score for our RL experiments which we call\\n         the \\'GLEU score\\'. For the GLEU score, we record all sub-sequences of\\n         1, 2, 3 or 4 tokens in output and target sequence (n-grams). We then\\n         compute a recall, which is the ratio of the number of matching n-grams\\n         to the number of total n-grams in the target (ground truth) sequence,\\n         and a precision, which is the ratio of the number of matching n-grams\\n         to the number of total n-grams in the generated output sequence. Then\\n         GLEU score is simply the minimum of recall and precision. This GLEU\\n         score\\'s range is always between 0 (no matches) and 1 (all match) and\\n         it is symmetrical when switching output and target. According to\\n         our experiments, GLEU score correlates quite well with the BLEU\\n         metric on a corpus level but does not have its drawbacks for our per\\n         sentence reward objective.\"\\n\\n    Note: The initial implementation only allowed a single reference, but now\\n          a list of references is required (which is consistent with\\n          bleu_score.sentence_bleu()).\\n\\n    The infamous \"the the the ... \" example\\n\\n        >>> ref = \\'the cat is on the mat\\'.split()\\n        >>> hyp = \\'the the the the the the the\\'.split()\\n        >>> sentence_gleu([ref], hyp)  # doctest: +ELLIPSIS\\n        0.0909...\\n\\n    An example to evaluate normal machine translation outputs\\n\\n        >>> ref1 = str(\\'It is a guide to action that ensures that the military \\'\\n        ...            \\'will forever heed Party commands\\').split()\\n        >>> hyp1 = str(\\'It is a guide to action which ensures that the military \\'\\n        ...            \\'always obeys the commands of the party\\').split()\\n        >>> hyp2 = str(\\'It is to insure the troops forever hearing the activity \\'\\n        ...            \\'guidebook that party direct\\').split()\\n        >>> sentence_gleu([ref1], hyp1) # doctest: +ELLIPSIS\\n        0.4393...\\n        >>> sentence_gleu([ref1], hyp2) # doctest: +ELLIPSIS\\n        0.1206...\\n\\n    :param references: a list of reference sentences\\n    :type references: list(list(str))\\n    :param hypothesis: a hypothesis sentence\\n    :type hypothesis: list(str)\\n    :param min_len: The minimum order of n-gram this function should extract.\\n    :type min_len: int\\n    :param max_len: The maximum order of n-gram this function should extract.\\n    :type max_len: int\\n    :return: the sentence level GLEU score.\\n    :rtype: float\\n    '\n    return corpus_gleu([references], [hypothesis], min_len=min_len, max_len=max_len)"
        ]
    },
    {
        "func_name": "corpus_gleu",
        "original": "def corpus_gleu(list_of_references, hypotheses, min_len=1, max_len=4):\n    \"\"\"\n    Calculate a single corpus-level GLEU score (aka. system-level GLEU) for all\n    the hypotheses and their respective references.\n\n    Instead of averaging the sentence level GLEU scores (i.e. macro-average\n    precision), Wu et al. (2016) sum up the matching tokens and the max of\n    hypothesis and reference tokens for each sentence, then compute using the\n    aggregate values.\n\n    From Mike Schuster (via email):\n        \"For the corpus, we just add up the two statistics n_match and\n         n_all = max(n_all_output, n_all_target) for all sentences, then\n         calculate gleu_score = n_match / n_all, so it is not just a mean of\n         the sentence gleu scores (in our case, longer sentences count more,\n         which I think makes sense as they are more difficult to translate).\"\n\n    >>> hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n    ...         'ensures', 'that', 'the', 'military', 'always',\n    ...         'obeys', 'the', 'commands', 'of', 'the', 'party']\n    >>> ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n    ...          'ensures', 'that', 'the', 'military', 'will', 'forever',\n    ...          'heed', 'Party', 'commands']\n    >>> ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n    ...          'guarantees', 'the', 'military', 'forces', 'always',\n    ...          'being', 'under', 'the', 'command', 'of', 'the', 'Party']\n    >>> ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n    ...          'army', 'always', 'to', 'heed', 'the', 'directions',\n    ...          'of', 'the', 'party']\n\n    >>> hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\n    ...         'interested', 'in', 'world', 'history']\n    >>> ref2a = ['he', 'was', 'interested', 'in', 'world', 'history',\n    ...          'because', 'he', 'read', 'the', 'book']\n\n    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\n    >>> hypotheses = [hyp1, hyp2]\n    >>> corpus_gleu(list_of_references, hypotheses) # doctest: +ELLIPSIS\n    0.5673...\n\n    The example below show that corpus_gleu() is different from averaging\n    sentence_gleu() for hypotheses\n\n    >>> score1 = sentence_gleu([ref1a], hyp1)\n    >>> score2 = sentence_gleu([ref2a], hyp2)\n    >>> (score1 + score2) / 2 # doctest: +ELLIPSIS\n    0.6144...\n\n    :param list_of_references: a list of reference sentences, w.r.t. hypotheses\n    :type list_of_references: list(list(list(str)))\n    :param hypotheses: a list of hypothesis sentences\n    :type hypotheses: list(list(str))\n    :param min_len: The minimum order of n-gram this function should extract.\n    :type min_len: int\n    :param max_len: The maximum order of n-gram this function should extract.\n    :type max_len: int\n    :return: The corpus-level GLEU score.\n    :rtype: float\n    \"\"\"\n    assert len(list_of_references) == len(hypotheses), 'The number of hypotheses and their reference(s) should be the same'\n    corpus_n_match = 0\n    corpus_n_all = 0\n    for (references, hypothesis) in zip(list_of_references, hypotheses):\n        hyp_ngrams = Counter(everygrams(hypothesis, min_len, max_len))\n        tpfp = sum(hyp_ngrams.values())\n        hyp_counts = []\n        for reference in references:\n            ref_ngrams = Counter(everygrams(reference, min_len, max_len))\n            tpfn = sum(ref_ngrams.values())\n            overlap_ngrams = ref_ngrams & hyp_ngrams\n            tp = sum(overlap_ngrams.values())\n            n_all = max(tpfp, tpfn)\n            if n_all > 0:\n                hyp_counts.append((tp, n_all))\n        if hyp_counts:\n            (n_match, n_all) = max(hyp_counts, key=lambda hc: hc[0] / hc[1])\n            corpus_n_match += n_match\n            corpus_n_all += n_all\n    if corpus_n_all == 0:\n        gleu_score = 0.0\n    else:\n        gleu_score = corpus_n_match / corpus_n_all\n    return gleu_score",
        "mutated": [
            "def corpus_gleu(list_of_references, hypotheses, min_len=1, max_len=4):\n    if False:\n        i = 10\n    '\\n    Calculate a single corpus-level GLEU score (aka. system-level GLEU) for all\\n    the hypotheses and their respective references.\\n\\n    Instead of averaging the sentence level GLEU scores (i.e. macro-average\\n    precision), Wu et al. (2016) sum up the matching tokens and the max of\\n    hypothesis and reference tokens for each sentence, then compute using the\\n    aggregate values.\\n\\n    From Mike Schuster (via email):\\n        \"For the corpus, we just add up the two statistics n_match and\\n         n_all = max(n_all_output, n_all_target) for all sentences, then\\n         calculate gleu_score = n_match / n_all, so it is not just a mean of\\n         the sentence gleu scores (in our case, longer sentences count more,\\n         which I think makes sense as they are more difficult to translate).\"\\n\\n    >>> hyp1 = [\\'It\\', \\'is\\', \\'a\\', \\'guide\\', \\'to\\', \\'action\\', \\'which\\',\\n    ...         \\'ensures\\', \\'that\\', \\'the\\', \\'military\\', \\'always\\',\\n    ...         \\'obeys\\', \\'the\\', \\'commands\\', \\'of\\', \\'the\\', \\'party\\']\\n    >>> ref1a = [\\'It\\', \\'is\\', \\'a\\', \\'guide\\', \\'to\\', \\'action\\', \\'that\\',\\n    ...          \\'ensures\\', \\'that\\', \\'the\\', \\'military\\', \\'will\\', \\'forever\\',\\n    ...          \\'heed\\', \\'Party\\', \\'commands\\']\\n    >>> ref1b = [\\'It\\', \\'is\\', \\'the\\', \\'guiding\\', \\'principle\\', \\'which\\',\\n    ...          \\'guarantees\\', \\'the\\', \\'military\\', \\'forces\\', \\'always\\',\\n    ...          \\'being\\', \\'under\\', \\'the\\', \\'command\\', \\'of\\', \\'the\\', \\'Party\\']\\n    >>> ref1c = [\\'It\\', \\'is\\', \\'the\\', \\'practical\\', \\'guide\\', \\'for\\', \\'the\\',\\n    ...          \\'army\\', \\'always\\', \\'to\\', \\'heed\\', \\'the\\', \\'directions\\',\\n    ...          \\'of\\', \\'the\\', \\'party\\']\\n\\n    >>> hyp2 = [\\'he\\', \\'read\\', \\'the\\', \\'book\\', \\'because\\', \\'he\\', \\'was\\',\\n    ...         \\'interested\\', \\'in\\', \\'world\\', \\'history\\']\\n    >>> ref2a = [\\'he\\', \\'was\\', \\'interested\\', \\'in\\', \\'world\\', \\'history\\',\\n    ...          \\'because\\', \\'he\\', \\'read\\', \\'the\\', \\'book\\']\\n\\n    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\\n    >>> hypotheses = [hyp1, hyp2]\\n    >>> corpus_gleu(list_of_references, hypotheses) # doctest: +ELLIPSIS\\n    0.5673...\\n\\n    The example below show that corpus_gleu() is different from averaging\\n    sentence_gleu() for hypotheses\\n\\n    >>> score1 = sentence_gleu([ref1a], hyp1)\\n    >>> score2 = sentence_gleu([ref2a], hyp2)\\n    >>> (score1 + score2) / 2 # doctest: +ELLIPSIS\\n    0.6144...\\n\\n    :param list_of_references: a list of reference sentences, w.r.t. hypotheses\\n    :type list_of_references: list(list(list(str)))\\n    :param hypotheses: a list of hypothesis sentences\\n    :type hypotheses: list(list(str))\\n    :param min_len: The minimum order of n-gram this function should extract.\\n    :type min_len: int\\n    :param max_len: The maximum order of n-gram this function should extract.\\n    :type max_len: int\\n    :return: The corpus-level GLEU score.\\n    :rtype: float\\n    '\n    assert len(list_of_references) == len(hypotheses), 'The number of hypotheses and their reference(s) should be the same'\n    corpus_n_match = 0\n    corpus_n_all = 0\n    for (references, hypothesis) in zip(list_of_references, hypotheses):\n        hyp_ngrams = Counter(everygrams(hypothesis, min_len, max_len))\n        tpfp = sum(hyp_ngrams.values())\n        hyp_counts = []\n        for reference in references:\n            ref_ngrams = Counter(everygrams(reference, min_len, max_len))\n            tpfn = sum(ref_ngrams.values())\n            overlap_ngrams = ref_ngrams & hyp_ngrams\n            tp = sum(overlap_ngrams.values())\n            n_all = max(tpfp, tpfn)\n            if n_all > 0:\n                hyp_counts.append((tp, n_all))\n        if hyp_counts:\n            (n_match, n_all) = max(hyp_counts, key=lambda hc: hc[0] / hc[1])\n            corpus_n_match += n_match\n            corpus_n_all += n_all\n    if corpus_n_all == 0:\n        gleu_score = 0.0\n    else:\n        gleu_score = corpus_n_match / corpus_n_all\n    return gleu_score",
            "def corpus_gleu(list_of_references, hypotheses, min_len=1, max_len=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculate a single corpus-level GLEU score (aka. system-level GLEU) for all\\n    the hypotheses and their respective references.\\n\\n    Instead of averaging the sentence level GLEU scores (i.e. macro-average\\n    precision), Wu et al. (2016) sum up the matching tokens and the max of\\n    hypothesis and reference tokens for each sentence, then compute using the\\n    aggregate values.\\n\\n    From Mike Schuster (via email):\\n        \"For the corpus, we just add up the two statistics n_match and\\n         n_all = max(n_all_output, n_all_target) for all sentences, then\\n         calculate gleu_score = n_match / n_all, so it is not just a mean of\\n         the sentence gleu scores (in our case, longer sentences count more,\\n         which I think makes sense as they are more difficult to translate).\"\\n\\n    >>> hyp1 = [\\'It\\', \\'is\\', \\'a\\', \\'guide\\', \\'to\\', \\'action\\', \\'which\\',\\n    ...         \\'ensures\\', \\'that\\', \\'the\\', \\'military\\', \\'always\\',\\n    ...         \\'obeys\\', \\'the\\', \\'commands\\', \\'of\\', \\'the\\', \\'party\\']\\n    >>> ref1a = [\\'It\\', \\'is\\', \\'a\\', \\'guide\\', \\'to\\', \\'action\\', \\'that\\',\\n    ...          \\'ensures\\', \\'that\\', \\'the\\', \\'military\\', \\'will\\', \\'forever\\',\\n    ...          \\'heed\\', \\'Party\\', \\'commands\\']\\n    >>> ref1b = [\\'It\\', \\'is\\', \\'the\\', \\'guiding\\', \\'principle\\', \\'which\\',\\n    ...          \\'guarantees\\', \\'the\\', \\'military\\', \\'forces\\', \\'always\\',\\n    ...          \\'being\\', \\'under\\', \\'the\\', \\'command\\', \\'of\\', \\'the\\', \\'Party\\']\\n    >>> ref1c = [\\'It\\', \\'is\\', \\'the\\', \\'practical\\', \\'guide\\', \\'for\\', \\'the\\',\\n    ...          \\'army\\', \\'always\\', \\'to\\', \\'heed\\', \\'the\\', \\'directions\\',\\n    ...          \\'of\\', \\'the\\', \\'party\\']\\n\\n    >>> hyp2 = [\\'he\\', \\'read\\', \\'the\\', \\'book\\', \\'because\\', \\'he\\', \\'was\\',\\n    ...         \\'interested\\', \\'in\\', \\'world\\', \\'history\\']\\n    >>> ref2a = [\\'he\\', \\'was\\', \\'interested\\', \\'in\\', \\'world\\', \\'history\\',\\n    ...          \\'because\\', \\'he\\', \\'read\\', \\'the\\', \\'book\\']\\n\\n    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\\n    >>> hypotheses = [hyp1, hyp2]\\n    >>> corpus_gleu(list_of_references, hypotheses) # doctest: +ELLIPSIS\\n    0.5673...\\n\\n    The example below show that corpus_gleu() is different from averaging\\n    sentence_gleu() for hypotheses\\n\\n    >>> score1 = sentence_gleu([ref1a], hyp1)\\n    >>> score2 = sentence_gleu([ref2a], hyp2)\\n    >>> (score1 + score2) / 2 # doctest: +ELLIPSIS\\n    0.6144...\\n\\n    :param list_of_references: a list of reference sentences, w.r.t. hypotheses\\n    :type list_of_references: list(list(list(str)))\\n    :param hypotheses: a list of hypothesis sentences\\n    :type hypotheses: list(list(str))\\n    :param min_len: The minimum order of n-gram this function should extract.\\n    :type min_len: int\\n    :param max_len: The maximum order of n-gram this function should extract.\\n    :type max_len: int\\n    :return: The corpus-level GLEU score.\\n    :rtype: float\\n    '\n    assert len(list_of_references) == len(hypotheses), 'The number of hypotheses and their reference(s) should be the same'\n    corpus_n_match = 0\n    corpus_n_all = 0\n    for (references, hypothesis) in zip(list_of_references, hypotheses):\n        hyp_ngrams = Counter(everygrams(hypothesis, min_len, max_len))\n        tpfp = sum(hyp_ngrams.values())\n        hyp_counts = []\n        for reference in references:\n            ref_ngrams = Counter(everygrams(reference, min_len, max_len))\n            tpfn = sum(ref_ngrams.values())\n            overlap_ngrams = ref_ngrams & hyp_ngrams\n            tp = sum(overlap_ngrams.values())\n            n_all = max(tpfp, tpfn)\n            if n_all > 0:\n                hyp_counts.append((tp, n_all))\n        if hyp_counts:\n            (n_match, n_all) = max(hyp_counts, key=lambda hc: hc[0] / hc[1])\n            corpus_n_match += n_match\n            corpus_n_all += n_all\n    if corpus_n_all == 0:\n        gleu_score = 0.0\n    else:\n        gleu_score = corpus_n_match / corpus_n_all\n    return gleu_score",
            "def corpus_gleu(list_of_references, hypotheses, min_len=1, max_len=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculate a single corpus-level GLEU score (aka. system-level GLEU) for all\\n    the hypotheses and their respective references.\\n\\n    Instead of averaging the sentence level GLEU scores (i.e. macro-average\\n    precision), Wu et al. (2016) sum up the matching tokens and the max of\\n    hypothesis and reference tokens for each sentence, then compute using the\\n    aggregate values.\\n\\n    From Mike Schuster (via email):\\n        \"For the corpus, we just add up the two statistics n_match and\\n         n_all = max(n_all_output, n_all_target) for all sentences, then\\n         calculate gleu_score = n_match / n_all, so it is not just a mean of\\n         the sentence gleu scores (in our case, longer sentences count more,\\n         which I think makes sense as they are more difficult to translate).\"\\n\\n    >>> hyp1 = [\\'It\\', \\'is\\', \\'a\\', \\'guide\\', \\'to\\', \\'action\\', \\'which\\',\\n    ...         \\'ensures\\', \\'that\\', \\'the\\', \\'military\\', \\'always\\',\\n    ...         \\'obeys\\', \\'the\\', \\'commands\\', \\'of\\', \\'the\\', \\'party\\']\\n    >>> ref1a = [\\'It\\', \\'is\\', \\'a\\', \\'guide\\', \\'to\\', \\'action\\', \\'that\\',\\n    ...          \\'ensures\\', \\'that\\', \\'the\\', \\'military\\', \\'will\\', \\'forever\\',\\n    ...          \\'heed\\', \\'Party\\', \\'commands\\']\\n    >>> ref1b = [\\'It\\', \\'is\\', \\'the\\', \\'guiding\\', \\'principle\\', \\'which\\',\\n    ...          \\'guarantees\\', \\'the\\', \\'military\\', \\'forces\\', \\'always\\',\\n    ...          \\'being\\', \\'under\\', \\'the\\', \\'command\\', \\'of\\', \\'the\\', \\'Party\\']\\n    >>> ref1c = [\\'It\\', \\'is\\', \\'the\\', \\'practical\\', \\'guide\\', \\'for\\', \\'the\\',\\n    ...          \\'army\\', \\'always\\', \\'to\\', \\'heed\\', \\'the\\', \\'directions\\',\\n    ...          \\'of\\', \\'the\\', \\'party\\']\\n\\n    >>> hyp2 = [\\'he\\', \\'read\\', \\'the\\', \\'book\\', \\'because\\', \\'he\\', \\'was\\',\\n    ...         \\'interested\\', \\'in\\', \\'world\\', \\'history\\']\\n    >>> ref2a = [\\'he\\', \\'was\\', \\'interested\\', \\'in\\', \\'world\\', \\'history\\',\\n    ...          \\'because\\', \\'he\\', \\'read\\', \\'the\\', \\'book\\']\\n\\n    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\\n    >>> hypotheses = [hyp1, hyp2]\\n    >>> corpus_gleu(list_of_references, hypotheses) # doctest: +ELLIPSIS\\n    0.5673...\\n\\n    The example below show that corpus_gleu() is different from averaging\\n    sentence_gleu() for hypotheses\\n\\n    >>> score1 = sentence_gleu([ref1a], hyp1)\\n    >>> score2 = sentence_gleu([ref2a], hyp2)\\n    >>> (score1 + score2) / 2 # doctest: +ELLIPSIS\\n    0.6144...\\n\\n    :param list_of_references: a list of reference sentences, w.r.t. hypotheses\\n    :type list_of_references: list(list(list(str)))\\n    :param hypotheses: a list of hypothesis sentences\\n    :type hypotheses: list(list(str))\\n    :param min_len: The minimum order of n-gram this function should extract.\\n    :type min_len: int\\n    :param max_len: The maximum order of n-gram this function should extract.\\n    :type max_len: int\\n    :return: The corpus-level GLEU score.\\n    :rtype: float\\n    '\n    assert len(list_of_references) == len(hypotheses), 'The number of hypotheses and their reference(s) should be the same'\n    corpus_n_match = 0\n    corpus_n_all = 0\n    for (references, hypothesis) in zip(list_of_references, hypotheses):\n        hyp_ngrams = Counter(everygrams(hypothesis, min_len, max_len))\n        tpfp = sum(hyp_ngrams.values())\n        hyp_counts = []\n        for reference in references:\n            ref_ngrams = Counter(everygrams(reference, min_len, max_len))\n            tpfn = sum(ref_ngrams.values())\n            overlap_ngrams = ref_ngrams & hyp_ngrams\n            tp = sum(overlap_ngrams.values())\n            n_all = max(tpfp, tpfn)\n            if n_all > 0:\n                hyp_counts.append((tp, n_all))\n        if hyp_counts:\n            (n_match, n_all) = max(hyp_counts, key=lambda hc: hc[0] / hc[1])\n            corpus_n_match += n_match\n            corpus_n_all += n_all\n    if corpus_n_all == 0:\n        gleu_score = 0.0\n    else:\n        gleu_score = corpus_n_match / corpus_n_all\n    return gleu_score",
            "def corpus_gleu(list_of_references, hypotheses, min_len=1, max_len=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculate a single corpus-level GLEU score (aka. system-level GLEU) for all\\n    the hypotheses and their respective references.\\n\\n    Instead of averaging the sentence level GLEU scores (i.e. macro-average\\n    precision), Wu et al. (2016) sum up the matching tokens and the max of\\n    hypothesis and reference tokens for each sentence, then compute using the\\n    aggregate values.\\n\\n    From Mike Schuster (via email):\\n        \"For the corpus, we just add up the two statistics n_match and\\n         n_all = max(n_all_output, n_all_target) for all sentences, then\\n         calculate gleu_score = n_match / n_all, so it is not just a mean of\\n         the sentence gleu scores (in our case, longer sentences count more,\\n         which I think makes sense as they are more difficult to translate).\"\\n\\n    >>> hyp1 = [\\'It\\', \\'is\\', \\'a\\', \\'guide\\', \\'to\\', \\'action\\', \\'which\\',\\n    ...         \\'ensures\\', \\'that\\', \\'the\\', \\'military\\', \\'always\\',\\n    ...         \\'obeys\\', \\'the\\', \\'commands\\', \\'of\\', \\'the\\', \\'party\\']\\n    >>> ref1a = [\\'It\\', \\'is\\', \\'a\\', \\'guide\\', \\'to\\', \\'action\\', \\'that\\',\\n    ...          \\'ensures\\', \\'that\\', \\'the\\', \\'military\\', \\'will\\', \\'forever\\',\\n    ...          \\'heed\\', \\'Party\\', \\'commands\\']\\n    >>> ref1b = [\\'It\\', \\'is\\', \\'the\\', \\'guiding\\', \\'principle\\', \\'which\\',\\n    ...          \\'guarantees\\', \\'the\\', \\'military\\', \\'forces\\', \\'always\\',\\n    ...          \\'being\\', \\'under\\', \\'the\\', \\'command\\', \\'of\\', \\'the\\', \\'Party\\']\\n    >>> ref1c = [\\'It\\', \\'is\\', \\'the\\', \\'practical\\', \\'guide\\', \\'for\\', \\'the\\',\\n    ...          \\'army\\', \\'always\\', \\'to\\', \\'heed\\', \\'the\\', \\'directions\\',\\n    ...          \\'of\\', \\'the\\', \\'party\\']\\n\\n    >>> hyp2 = [\\'he\\', \\'read\\', \\'the\\', \\'book\\', \\'because\\', \\'he\\', \\'was\\',\\n    ...         \\'interested\\', \\'in\\', \\'world\\', \\'history\\']\\n    >>> ref2a = [\\'he\\', \\'was\\', \\'interested\\', \\'in\\', \\'world\\', \\'history\\',\\n    ...          \\'because\\', \\'he\\', \\'read\\', \\'the\\', \\'book\\']\\n\\n    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\\n    >>> hypotheses = [hyp1, hyp2]\\n    >>> corpus_gleu(list_of_references, hypotheses) # doctest: +ELLIPSIS\\n    0.5673...\\n\\n    The example below show that corpus_gleu() is different from averaging\\n    sentence_gleu() for hypotheses\\n\\n    >>> score1 = sentence_gleu([ref1a], hyp1)\\n    >>> score2 = sentence_gleu([ref2a], hyp2)\\n    >>> (score1 + score2) / 2 # doctest: +ELLIPSIS\\n    0.6144...\\n\\n    :param list_of_references: a list of reference sentences, w.r.t. hypotheses\\n    :type list_of_references: list(list(list(str)))\\n    :param hypotheses: a list of hypothesis sentences\\n    :type hypotheses: list(list(str))\\n    :param min_len: The minimum order of n-gram this function should extract.\\n    :type min_len: int\\n    :param max_len: The maximum order of n-gram this function should extract.\\n    :type max_len: int\\n    :return: The corpus-level GLEU score.\\n    :rtype: float\\n    '\n    assert len(list_of_references) == len(hypotheses), 'The number of hypotheses and their reference(s) should be the same'\n    corpus_n_match = 0\n    corpus_n_all = 0\n    for (references, hypothesis) in zip(list_of_references, hypotheses):\n        hyp_ngrams = Counter(everygrams(hypothesis, min_len, max_len))\n        tpfp = sum(hyp_ngrams.values())\n        hyp_counts = []\n        for reference in references:\n            ref_ngrams = Counter(everygrams(reference, min_len, max_len))\n            tpfn = sum(ref_ngrams.values())\n            overlap_ngrams = ref_ngrams & hyp_ngrams\n            tp = sum(overlap_ngrams.values())\n            n_all = max(tpfp, tpfn)\n            if n_all > 0:\n                hyp_counts.append((tp, n_all))\n        if hyp_counts:\n            (n_match, n_all) = max(hyp_counts, key=lambda hc: hc[0] / hc[1])\n            corpus_n_match += n_match\n            corpus_n_all += n_all\n    if corpus_n_all == 0:\n        gleu_score = 0.0\n    else:\n        gleu_score = corpus_n_match / corpus_n_all\n    return gleu_score",
            "def corpus_gleu(list_of_references, hypotheses, min_len=1, max_len=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculate a single corpus-level GLEU score (aka. system-level GLEU) for all\\n    the hypotheses and their respective references.\\n\\n    Instead of averaging the sentence level GLEU scores (i.e. macro-average\\n    precision), Wu et al. (2016) sum up the matching tokens and the max of\\n    hypothesis and reference tokens for each sentence, then compute using the\\n    aggregate values.\\n\\n    From Mike Schuster (via email):\\n        \"For the corpus, we just add up the two statistics n_match and\\n         n_all = max(n_all_output, n_all_target) for all sentences, then\\n         calculate gleu_score = n_match / n_all, so it is not just a mean of\\n         the sentence gleu scores (in our case, longer sentences count more,\\n         which I think makes sense as they are more difficult to translate).\"\\n\\n    >>> hyp1 = [\\'It\\', \\'is\\', \\'a\\', \\'guide\\', \\'to\\', \\'action\\', \\'which\\',\\n    ...         \\'ensures\\', \\'that\\', \\'the\\', \\'military\\', \\'always\\',\\n    ...         \\'obeys\\', \\'the\\', \\'commands\\', \\'of\\', \\'the\\', \\'party\\']\\n    >>> ref1a = [\\'It\\', \\'is\\', \\'a\\', \\'guide\\', \\'to\\', \\'action\\', \\'that\\',\\n    ...          \\'ensures\\', \\'that\\', \\'the\\', \\'military\\', \\'will\\', \\'forever\\',\\n    ...          \\'heed\\', \\'Party\\', \\'commands\\']\\n    >>> ref1b = [\\'It\\', \\'is\\', \\'the\\', \\'guiding\\', \\'principle\\', \\'which\\',\\n    ...          \\'guarantees\\', \\'the\\', \\'military\\', \\'forces\\', \\'always\\',\\n    ...          \\'being\\', \\'under\\', \\'the\\', \\'command\\', \\'of\\', \\'the\\', \\'Party\\']\\n    >>> ref1c = [\\'It\\', \\'is\\', \\'the\\', \\'practical\\', \\'guide\\', \\'for\\', \\'the\\',\\n    ...          \\'army\\', \\'always\\', \\'to\\', \\'heed\\', \\'the\\', \\'directions\\',\\n    ...          \\'of\\', \\'the\\', \\'party\\']\\n\\n    >>> hyp2 = [\\'he\\', \\'read\\', \\'the\\', \\'book\\', \\'because\\', \\'he\\', \\'was\\',\\n    ...         \\'interested\\', \\'in\\', \\'world\\', \\'history\\']\\n    >>> ref2a = [\\'he\\', \\'was\\', \\'interested\\', \\'in\\', \\'world\\', \\'history\\',\\n    ...          \\'because\\', \\'he\\', \\'read\\', \\'the\\', \\'book\\']\\n\\n    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\\n    >>> hypotheses = [hyp1, hyp2]\\n    >>> corpus_gleu(list_of_references, hypotheses) # doctest: +ELLIPSIS\\n    0.5673...\\n\\n    The example below show that corpus_gleu() is different from averaging\\n    sentence_gleu() for hypotheses\\n\\n    >>> score1 = sentence_gleu([ref1a], hyp1)\\n    >>> score2 = sentence_gleu([ref2a], hyp2)\\n    >>> (score1 + score2) / 2 # doctest: +ELLIPSIS\\n    0.6144...\\n\\n    :param list_of_references: a list of reference sentences, w.r.t. hypotheses\\n    :type list_of_references: list(list(list(str)))\\n    :param hypotheses: a list of hypothesis sentences\\n    :type hypotheses: list(list(str))\\n    :param min_len: The minimum order of n-gram this function should extract.\\n    :type min_len: int\\n    :param max_len: The maximum order of n-gram this function should extract.\\n    :type max_len: int\\n    :return: The corpus-level GLEU score.\\n    :rtype: float\\n    '\n    assert len(list_of_references) == len(hypotheses), 'The number of hypotheses and their reference(s) should be the same'\n    corpus_n_match = 0\n    corpus_n_all = 0\n    for (references, hypothesis) in zip(list_of_references, hypotheses):\n        hyp_ngrams = Counter(everygrams(hypothesis, min_len, max_len))\n        tpfp = sum(hyp_ngrams.values())\n        hyp_counts = []\n        for reference in references:\n            ref_ngrams = Counter(everygrams(reference, min_len, max_len))\n            tpfn = sum(ref_ngrams.values())\n            overlap_ngrams = ref_ngrams & hyp_ngrams\n            tp = sum(overlap_ngrams.values())\n            n_all = max(tpfp, tpfn)\n            if n_all > 0:\n                hyp_counts.append((tp, n_all))\n        if hyp_counts:\n            (n_match, n_all) = max(hyp_counts, key=lambda hc: hc[0] / hc[1])\n            corpus_n_match += n_match\n            corpus_n_all += n_all\n    if corpus_n_all == 0:\n        gleu_score = 0.0\n    else:\n        gleu_score = corpus_n_match / corpus_n_all\n    return gleu_score"
        ]
    }
]