[
    {
        "func_name": "send_event",
        "original": "def send_event(msg):\n    try:\n        resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n        return resp\n    except requests.Timeout:\n        return 500",
        "mutated": [
            "def send_event(msg):\n    if False:\n        i = 10\n    try:\n        resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n        return resp\n    except requests.Timeout:\n        return 500",
            "def send_event(msg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    try:\n        resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n        return resp\n    except requests.Timeout:\n        return 500",
            "def send_event(msg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    try:\n        resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n        return resp\n    except requests.Timeout:\n        return 500",
            "def send_event(msg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    try:\n        resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n        return resp\n    except requests.Timeout:\n        return 500",
            "def send_event(msg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    try:\n        resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n        return resp\n    except requests.Timeout:\n        return 500"
        ]
    },
    {
        "func_name": "check_app_running",
        "original": "def check_app_running():\n    status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n    assert status.status == 'RUNNING'\n    return True",
        "mutated": [
            "def check_app_running():\n    if False:\n        i = 10\n    status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n    assert status.status == 'RUNNING'\n    return True",
            "def check_app_running():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n    assert status.status == 'RUNNING'\n    return True",
            "def check_app_running():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n    assert status.status == 'RUNNING'\n    return True",
            "def check_app_running():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n    assert status.status == 'RUNNING'\n    return True",
            "def check_app_running():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n    assert status.status == 'RUNNING'\n    return True"
        ]
    },
    {
        "func_name": "test_cluster_crash_before_checkpoint",
        "original": "@pytest.mark.parametrize('workflow_start_regular_shared_serve', [{'num_cpus': 4}], indirect=True)\ndef test_cluster_crash_before_checkpoint(workflow_start_regular_shared_serve):\n    \"\"\"If the cluster crashed before the event was checkpointed, after the cluster restarted\n    and the workflow resumed, the new event message is processed by the workflow.\n    \"\"\"\n\n    class CustomHTTPListener(HTTPListener):\n\n        async def poll_for_event(self, event_key):\n            workflow_id = workflow_context.get_current_workflow_id()\n            if event_key is None:\n                raise WorkflowEventHandleError(workflow_id, 'poll_for_event() needs event_key')\n            payload = await self.handle.get_event_payload.remote(workflow_id, event_key)\n            if utils.check_global_mark('after_cluster_restarted'):\n                return payload\n            else:\n                utils.set_global_mark('simulate_cluster_crash')\n                await asyncio.sleep(10000)\n    from ray._private import storage\n    from ray.workflow.tests.utils import skip_client_mode_test\n    storage_uri = storage._storage_uri\n    skip_client_mode_test()\n\n    def send_event(msg):\n        try:\n            resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n            return resp\n        except requests.Timeout:\n            return 500\n    event_promise = workflow.wait_for_event(CustomHTTPListener, event_key='event_key')\n    workflow.run_async(event_promise, workflow_id='workflow_test_cluster_crash_before_checkpoint')\n\n    def check_app_running():\n        status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n        assert status.status == 'RUNNING'\n        return True\n    wait_for_condition(check_app_running)\n    test_msg = 'first_try'\n    while True:\n        res = send_event(test_msg)\n        if not isinstance(res, int):\n            if res.status_code == 404:\n                sleep(0.5)\n            else:\n                break\n        else:\n            break\n    while not utils.check_global_mark('simulate_cluster_crash'):\n        sleep(0.1)\n    if utils.check_global_mark('simulate_cluster_crash'):\n        serve.delete(common.HTTP_EVENT_PROVIDER_NAME)\n        serve.shutdown()\n        ray.shutdown()\n        subprocess.check_output(['ray', 'stop', '--force'])\n        ray.init(num_cpus=4, storage=storage_uri)\n        serve.start(detached=True)\n        utils.set_global_mark('after_cluster_restarted')\n        workflow.resume_async(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        status_after_resume = workflow.get_status(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        wait_for_condition(check_app_running)\n        assert status_after_resume == WorkflowStatus.RUNNING\n        test_msg = 'second_try'\n        while True:\n            res = send_event(test_msg)\n            if not isinstance(res, int):\n                if res.status_code == 404:\n                    sleep(0.5)\n                else:\n                    break\n            else:\n                break\n        (key, event_message) = workflow.get_output(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        assert event_message == 'second_try'\n    else:\n        assert False",
        "mutated": [
            "@pytest.mark.parametrize('workflow_start_regular_shared_serve', [{'num_cpus': 4}], indirect=True)\ndef test_cluster_crash_before_checkpoint(workflow_start_regular_shared_serve):\n    if False:\n        i = 10\n    'If the cluster crashed before the event was checkpointed, after the cluster restarted\\n    and the workflow resumed, the new event message is processed by the workflow.\\n    '\n\n    class CustomHTTPListener(HTTPListener):\n\n        async def poll_for_event(self, event_key):\n            workflow_id = workflow_context.get_current_workflow_id()\n            if event_key is None:\n                raise WorkflowEventHandleError(workflow_id, 'poll_for_event() needs event_key')\n            payload = await self.handle.get_event_payload.remote(workflow_id, event_key)\n            if utils.check_global_mark('after_cluster_restarted'):\n                return payload\n            else:\n                utils.set_global_mark('simulate_cluster_crash')\n                await asyncio.sleep(10000)\n    from ray._private import storage\n    from ray.workflow.tests.utils import skip_client_mode_test\n    storage_uri = storage._storage_uri\n    skip_client_mode_test()\n\n    def send_event(msg):\n        try:\n            resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n            return resp\n        except requests.Timeout:\n            return 500\n    event_promise = workflow.wait_for_event(CustomHTTPListener, event_key='event_key')\n    workflow.run_async(event_promise, workflow_id='workflow_test_cluster_crash_before_checkpoint')\n\n    def check_app_running():\n        status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n        assert status.status == 'RUNNING'\n        return True\n    wait_for_condition(check_app_running)\n    test_msg = 'first_try'\n    while True:\n        res = send_event(test_msg)\n        if not isinstance(res, int):\n            if res.status_code == 404:\n                sleep(0.5)\n            else:\n                break\n        else:\n            break\n    while not utils.check_global_mark('simulate_cluster_crash'):\n        sleep(0.1)\n    if utils.check_global_mark('simulate_cluster_crash'):\n        serve.delete(common.HTTP_EVENT_PROVIDER_NAME)\n        serve.shutdown()\n        ray.shutdown()\n        subprocess.check_output(['ray', 'stop', '--force'])\n        ray.init(num_cpus=4, storage=storage_uri)\n        serve.start(detached=True)\n        utils.set_global_mark('after_cluster_restarted')\n        workflow.resume_async(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        status_after_resume = workflow.get_status(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        wait_for_condition(check_app_running)\n        assert status_after_resume == WorkflowStatus.RUNNING\n        test_msg = 'second_try'\n        while True:\n            res = send_event(test_msg)\n            if not isinstance(res, int):\n                if res.status_code == 404:\n                    sleep(0.5)\n                else:\n                    break\n            else:\n                break\n        (key, event_message) = workflow.get_output(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        assert event_message == 'second_try'\n    else:\n        assert False",
            "@pytest.mark.parametrize('workflow_start_regular_shared_serve', [{'num_cpus': 4}], indirect=True)\ndef test_cluster_crash_before_checkpoint(workflow_start_regular_shared_serve):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'If the cluster crashed before the event was checkpointed, after the cluster restarted\\n    and the workflow resumed, the new event message is processed by the workflow.\\n    '\n\n    class CustomHTTPListener(HTTPListener):\n\n        async def poll_for_event(self, event_key):\n            workflow_id = workflow_context.get_current_workflow_id()\n            if event_key is None:\n                raise WorkflowEventHandleError(workflow_id, 'poll_for_event() needs event_key')\n            payload = await self.handle.get_event_payload.remote(workflow_id, event_key)\n            if utils.check_global_mark('after_cluster_restarted'):\n                return payload\n            else:\n                utils.set_global_mark('simulate_cluster_crash')\n                await asyncio.sleep(10000)\n    from ray._private import storage\n    from ray.workflow.tests.utils import skip_client_mode_test\n    storage_uri = storage._storage_uri\n    skip_client_mode_test()\n\n    def send_event(msg):\n        try:\n            resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n            return resp\n        except requests.Timeout:\n            return 500\n    event_promise = workflow.wait_for_event(CustomHTTPListener, event_key='event_key')\n    workflow.run_async(event_promise, workflow_id='workflow_test_cluster_crash_before_checkpoint')\n\n    def check_app_running():\n        status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n        assert status.status == 'RUNNING'\n        return True\n    wait_for_condition(check_app_running)\n    test_msg = 'first_try'\n    while True:\n        res = send_event(test_msg)\n        if not isinstance(res, int):\n            if res.status_code == 404:\n                sleep(0.5)\n            else:\n                break\n        else:\n            break\n    while not utils.check_global_mark('simulate_cluster_crash'):\n        sleep(0.1)\n    if utils.check_global_mark('simulate_cluster_crash'):\n        serve.delete(common.HTTP_EVENT_PROVIDER_NAME)\n        serve.shutdown()\n        ray.shutdown()\n        subprocess.check_output(['ray', 'stop', '--force'])\n        ray.init(num_cpus=4, storage=storage_uri)\n        serve.start(detached=True)\n        utils.set_global_mark('after_cluster_restarted')\n        workflow.resume_async(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        status_after_resume = workflow.get_status(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        wait_for_condition(check_app_running)\n        assert status_after_resume == WorkflowStatus.RUNNING\n        test_msg = 'second_try'\n        while True:\n            res = send_event(test_msg)\n            if not isinstance(res, int):\n                if res.status_code == 404:\n                    sleep(0.5)\n                else:\n                    break\n            else:\n                break\n        (key, event_message) = workflow.get_output(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        assert event_message == 'second_try'\n    else:\n        assert False",
            "@pytest.mark.parametrize('workflow_start_regular_shared_serve', [{'num_cpus': 4}], indirect=True)\ndef test_cluster_crash_before_checkpoint(workflow_start_regular_shared_serve):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'If the cluster crashed before the event was checkpointed, after the cluster restarted\\n    and the workflow resumed, the new event message is processed by the workflow.\\n    '\n\n    class CustomHTTPListener(HTTPListener):\n\n        async def poll_for_event(self, event_key):\n            workflow_id = workflow_context.get_current_workflow_id()\n            if event_key is None:\n                raise WorkflowEventHandleError(workflow_id, 'poll_for_event() needs event_key')\n            payload = await self.handle.get_event_payload.remote(workflow_id, event_key)\n            if utils.check_global_mark('after_cluster_restarted'):\n                return payload\n            else:\n                utils.set_global_mark('simulate_cluster_crash')\n                await asyncio.sleep(10000)\n    from ray._private import storage\n    from ray.workflow.tests.utils import skip_client_mode_test\n    storage_uri = storage._storage_uri\n    skip_client_mode_test()\n\n    def send_event(msg):\n        try:\n            resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n            return resp\n        except requests.Timeout:\n            return 500\n    event_promise = workflow.wait_for_event(CustomHTTPListener, event_key='event_key')\n    workflow.run_async(event_promise, workflow_id='workflow_test_cluster_crash_before_checkpoint')\n\n    def check_app_running():\n        status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n        assert status.status == 'RUNNING'\n        return True\n    wait_for_condition(check_app_running)\n    test_msg = 'first_try'\n    while True:\n        res = send_event(test_msg)\n        if not isinstance(res, int):\n            if res.status_code == 404:\n                sleep(0.5)\n            else:\n                break\n        else:\n            break\n    while not utils.check_global_mark('simulate_cluster_crash'):\n        sleep(0.1)\n    if utils.check_global_mark('simulate_cluster_crash'):\n        serve.delete(common.HTTP_EVENT_PROVIDER_NAME)\n        serve.shutdown()\n        ray.shutdown()\n        subprocess.check_output(['ray', 'stop', '--force'])\n        ray.init(num_cpus=4, storage=storage_uri)\n        serve.start(detached=True)\n        utils.set_global_mark('after_cluster_restarted')\n        workflow.resume_async(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        status_after_resume = workflow.get_status(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        wait_for_condition(check_app_running)\n        assert status_after_resume == WorkflowStatus.RUNNING\n        test_msg = 'second_try'\n        while True:\n            res = send_event(test_msg)\n            if not isinstance(res, int):\n                if res.status_code == 404:\n                    sleep(0.5)\n                else:\n                    break\n            else:\n                break\n        (key, event_message) = workflow.get_output(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        assert event_message == 'second_try'\n    else:\n        assert False",
            "@pytest.mark.parametrize('workflow_start_regular_shared_serve', [{'num_cpus': 4}], indirect=True)\ndef test_cluster_crash_before_checkpoint(workflow_start_regular_shared_serve):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'If the cluster crashed before the event was checkpointed, after the cluster restarted\\n    and the workflow resumed, the new event message is processed by the workflow.\\n    '\n\n    class CustomHTTPListener(HTTPListener):\n\n        async def poll_for_event(self, event_key):\n            workflow_id = workflow_context.get_current_workflow_id()\n            if event_key is None:\n                raise WorkflowEventHandleError(workflow_id, 'poll_for_event() needs event_key')\n            payload = await self.handle.get_event_payload.remote(workflow_id, event_key)\n            if utils.check_global_mark('after_cluster_restarted'):\n                return payload\n            else:\n                utils.set_global_mark('simulate_cluster_crash')\n                await asyncio.sleep(10000)\n    from ray._private import storage\n    from ray.workflow.tests.utils import skip_client_mode_test\n    storage_uri = storage._storage_uri\n    skip_client_mode_test()\n\n    def send_event(msg):\n        try:\n            resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n            return resp\n        except requests.Timeout:\n            return 500\n    event_promise = workflow.wait_for_event(CustomHTTPListener, event_key='event_key')\n    workflow.run_async(event_promise, workflow_id='workflow_test_cluster_crash_before_checkpoint')\n\n    def check_app_running():\n        status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n        assert status.status == 'RUNNING'\n        return True\n    wait_for_condition(check_app_running)\n    test_msg = 'first_try'\n    while True:\n        res = send_event(test_msg)\n        if not isinstance(res, int):\n            if res.status_code == 404:\n                sleep(0.5)\n            else:\n                break\n        else:\n            break\n    while not utils.check_global_mark('simulate_cluster_crash'):\n        sleep(0.1)\n    if utils.check_global_mark('simulate_cluster_crash'):\n        serve.delete(common.HTTP_EVENT_PROVIDER_NAME)\n        serve.shutdown()\n        ray.shutdown()\n        subprocess.check_output(['ray', 'stop', '--force'])\n        ray.init(num_cpus=4, storage=storage_uri)\n        serve.start(detached=True)\n        utils.set_global_mark('after_cluster_restarted')\n        workflow.resume_async(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        status_after_resume = workflow.get_status(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        wait_for_condition(check_app_running)\n        assert status_after_resume == WorkflowStatus.RUNNING\n        test_msg = 'second_try'\n        while True:\n            res = send_event(test_msg)\n            if not isinstance(res, int):\n                if res.status_code == 404:\n                    sleep(0.5)\n                else:\n                    break\n            else:\n                break\n        (key, event_message) = workflow.get_output(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        assert event_message == 'second_try'\n    else:\n        assert False",
            "@pytest.mark.parametrize('workflow_start_regular_shared_serve', [{'num_cpus': 4}], indirect=True)\ndef test_cluster_crash_before_checkpoint(workflow_start_regular_shared_serve):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'If the cluster crashed before the event was checkpointed, after the cluster restarted\\n    and the workflow resumed, the new event message is processed by the workflow.\\n    '\n\n    class CustomHTTPListener(HTTPListener):\n\n        async def poll_for_event(self, event_key):\n            workflow_id = workflow_context.get_current_workflow_id()\n            if event_key is None:\n                raise WorkflowEventHandleError(workflow_id, 'poll_for_event() needs event_key')\n            payload = await self.handle.get_event_payload.remote(workflow_id, event_key)\n            if utils.check_global_mark('after_cluster_restarted'):\n                return payload\n            else:\n                utils.set_global_mark('simulate_cluster_crash')\n                await asyncio.sleep(10000)\n    from ray._private import storage\n    from ray.workflow.tests.utils import skip_client_mode_test\n    storage_uri = storage._storage_uri\n    skip_client_mode_test()\n\n    def send_event(msg):\n        try:\n            resp = requests.post('http://127.0.0.1:8000/event/send_event/' + 'workflow_test_cluster_crash_before_checkpoint', json={'event_key': 'event_key', 'event_payload': msg}, timeout=5)\n            return resp\n        except requests.Timeout:\n            return 500\n    event_promise = workflow.wait_for_event(CustomHTTPListener, event_key='event_key')\n    workflow.run_async(event_promise, workflow_id='workflow_test_cluster_crash_before_checkpoint')\n\n    def check_app_running():\n        status = serve.status().applications[common.HTTP_EVENT_PROVIDER_NAME]\n        assert status.status == 'RUNNING'\n        return True\n    wait_for_condition(check_app_running)\n    test_msg = 'first_try'\n    while True:\n        res = send_event(test_msg)\n        if not isinstance(res, int):\n            if res.status_code == 404:\n                sleep(0.5)\n            else:\n                break\n        else:\n            break\n    while not utils.check_global_mark('simulate_cluster_crash'):\n        sleep(0.1)\n    if utils.check_global_mark('simulate_cluster_crash'):\n        serve.delete(common.HTTP_EVENT_PROVIDER_NAME)\n        serve.shutdown()\n        ray.shutdown()\n        subprocess.check_output(['ray', 'stop', '--force'])\n        ray.init(num_cpus=4, storage=storage_uri)\n        serve.start(detached=True)\n        utils.set_global_mark('after_cluster_restarted')\n        workflow.resume_async(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        status_after_resume = workflow.get_status(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        wait_for_condition(check_app_running)\n        assert status_after_resume == WorkflowStatus.RUNNING\n        test_msg = 'second_try'\n        while True:\n            res = send_event(test_msg)\n            if not isinstance(res, int):\n                if res.status_code == 404:\n                    sleep(0.5)\n                else:\n                    break\n            else:\n                break\n        (key, event_message) = workflow.get_output(workflow_id='workflow_test_cluster_crash_before_checkpoint')\n        assert event_message == 'second_try'\n    else:\n        assert False"
        ]
    }
]