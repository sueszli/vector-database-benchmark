[
    {
        "func_name": "__init__",
        "original": "def __init__(self, elements, name=None):\n    if not elements:\n        raise ValueError('Invalid `elements`. `elements` should not be empty.')\n    if not isinstance(elements, list):\n        raise ValueError('Invalid `elements`. `elements` must be a list.')\n    elements = [structure.normalize_element(element) for element in elements]\n    type_specs = [structure.type_spec_from_value(element) for element in elements]\n    num_elements = len(elements)\n    for i in range(1, num_elements):\n        nest.assert_same_structure(type_specs[0], type_specs[i])\n    flattened_type_specs = [nest.flatten(type_spec) for type_spec in type_specs]\n    num_tensors_per_element = len(flattened_type_specs[0])\n    flattened_structure = [None] * num_tensors_per_element\n    for i in range(num_tensors_per_element):\n        flattened_structure[i] = flattened_type_specs[0][i]\n        for j in range(1, num_elements):\n            flattened_structure[i] = flattened_structure[i].most_specific_common_supertype([flattened_type_specs[j][i]])\n    if not isinstance(type_specs[0], dataset_ops.DatasetSpec):\n        self._tensors = list(itertools.chain.from_iterable([nest.flatten(element) for element in elements]))\n    else:\n        self._tensors = [x._variant_tensor for x in elements]\n    self._structure = nest.pack_sequence_as(type_specs[0], flattened_structure)\n    self._name = name\n    variant_tensor = gen_experimental_dataset_ops.list_dataset(self._tensors, output_types=self._flat_types, output_shapes=self._flat_shapes, metadata=self._metadata.SerializeToString())\n    super(_ListDataset, self).__init__(variant_tensor)",
        "mutated": [
            "def __init__(self, elements, name=None):\n    if False:\n        i = 10\n    if not elements:\n        raise ValueError('Invalid `elements`. `elements` should not be empty.')\n    if not isinstance(elements, list):\n        raise ValueError('Invalid `elements`. `elements` must be a list.')\n    elements = [structure.normalize_element(element) for element in elements]\n    type_specs = [structure.type_spec_from_value(element) for element in elements]\n    num_elements = len(elements)\n    for i in range(1, num_elements):\n        nest.assert_same_structure(type_specs[0], type_specs[i])\n    flattened_type_specs = [nest.flatten(type_spec) for type_spec in type_specs]\n    num_tensors_per_element = len(flattened_type_specs[0])\n    flattened_structure = [None] * num_tensors_per_element\n    for i in range(num_tensors_per_element):\n        flattened_structure[i] = flattened_type_specs[0][i]\n        for j in range(1, num_elements):\n            flattened_structure[i] = flattened_structure[i].most_specific_common_supertype([flattened_type_specs[j][i]])\n    if not isinstance(type_specs[0], dataset_ops.DatasetSpec):\n        self._tensors = list(itertools.chain.from_iterable([nest.flatten(element) for element in elements]))\n    else:\n        self._tensors = [x._variant_tensor for x in elements]\n    self._structure = nest.pack_sequence_as(type_specs[0], flattened_structure)\n    self._name = name\n    variant_tensor = gen_experimental_dataset_ops.list_dataset(self._tensors, output_types=self._flat_types, output_shapes=self._flat_shapes, metadata=self._metadata.SerializeToString())\n    super(_ListDataset, self).__init__(variant_tensor)",
            "def __init__(self, elements, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not elements:\n        raise ValueError('Invalid `elements`. `elements` should not be empty.')\n    if not isinstance(elements, list):\n        raise ValueError('Invalid `elements`. `elements` must be a list.')\n    elements = [structure.normalize_element(element) for element in elements]\n    type_specs = [structure.type_spec_from_value(element) for element in elements]\n    num_elements = len(elements)\n    for i in range(1, num_elements):\n        nest.assert_same_structure(type_specs[0], type_specs[i])\n    flattened_type_specs = [nest.flatten(type_spec) for type_spec in type_specs]\n    num_tensors_per_element = len(flattened_type_specs[0])\n    flattened_structure = [None] * num_tensors_per_element\n    for i in range(num_tensors_per_element):\n        flattened_structure[i] = flattened_type_specs[0][i]\n        for j in range(1, num_elements):\n            flattened_structure[i] = flattened_structure[i].most_specific_common_supertype([flattened_type_specs[j][i]])\n    if not isinstance(type_specs[0], dataset_ops.DatasetSpec):\n        self._tensors = list(itertools.chain.from_iterable([nest.flatten(element) for element in elements]))\n    else:\n        self._tensors = [x._variant_tensor for x in elements]\n    self._structure = nest.pack_sequence_as(type_specs[0], flattened_structure)\n    self._name = name\n    variant_tensor = gen_experimental_dataset_ops.list_dataset(self._tensors, output_types=self._flat_types, output_shapes=self._flat_shapes, metadata=self._metadata.SerializeToString())\n    super(_ListDataset, self).__init__(variant_tensor)",
            "def __init__(self, elements, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not elements:\n        raise ValueError('Invalid `elements`. `elements` should not be empty.')\n    if not isinstance(elements, list):\n        raise ValueError('Invalid `elements`. `elements` must be a list.')\n    elements = [structure.normalize_element(element) for element in elements]\n    type_specs = [structure.type_spec_from_value(element) for element in elements]\n    num_elements = len(elements)\n    for i in range(1, num_elements):\n        nest.assert_same_structure(type_specs[0], type_specs[i])\n    flattened_type_specs = [nest.flatten(type_spec) for type_spec in type_specs]\n    num_tensors_per_element = len(flattened_type_specs[0])\n    flattened_structure = [None] * num_tensors_per_element\n    for i in range(num_tensors_per_element):\n        flattened_structure[i] = flattened_type_specs[0][i]\n        for j in range(1, num_elements):\n            flattened_structure[i] = flattened_structure[i].most_specific_common_supertype([flattened_type_specs[j][i]])\n    if not isinstance(type_specs[0], dataset_ops.DatasetSpec):\n        self._tensors = list(itertools.chain.from_iterable([nest.flatten(element) for element in elements]))\n    else:\n        self._tensors = [x._variant_tensor for x in elements]\n    self._structure = nest.pack_sequence_as(type_specs[0], flattened_structure)\n    self._name = name\n    variant_tensor = gen_experimental_dataset_ops.list_dataset(self._tensors, output_types=self._flat_types, output_shapes=self._flat_shapes, metadata=self._metadata.SerializeToString())\n    super(_ListDataset, self).__init__(variant_tensor)",
            "def __init__(self, elements, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not elements:\n        raise ValueError('Invalid `elements`. `elements` should not be empty.')\n    if not isinstance(elements, list):\n        raise ValueError('Invalid `elements`. `elements` must be a list.')\n    elements = [structure.normalize_element(element) for element in elements]\n    type_specs = [structure.type_spec_from_value(element) for element in elements]\n    num_elements = len(elements)\n    for i in range(1, num_elements):\n        nest.assert_same_structure(type_specs[0], type_specs[i])\n    flattened_type_specs = [nest.flatten(type_spec) for type_spec in type_specs]\n    num_tensors_per_element = len(flattened_type_specs[0])\n    flattened_structure = [None] * num_tensors_per_element\n    for i in range(num_tensors_per_element):\n        flattened_structure[i] = flattened_type_specs[0][i]\n        for j in range(1, num_elements):\n            flattened_structure[i] = flattened_structure[i].most_specific_common_supertype([flattened_type_specs[j][i]])\n    if not isinstance(type_specs[0], dataset_ops.DatasetSpec):\n        self._tensors = list(itertools.chain.from_iterable([nest.flatten(element) for element in elements]))\n    else:\n        self._tensors = [x._variant_tensor for x in elements]\n    self._structure = nest.pack_sequence_as(type_specs[0], flattened_structure)\n    self._name = name\n    variant_tensor = gen_experimental_dataset_ops.list_dataset(self._tensors, output_types=self._flat_types, output_shapes=self._flat_shapes, metadata=self._metadata.SerializeToString())\n    super(_ListDataset, self).__init__(variant_tensor)",
            "def __init__(self, elements, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not elements:\n        raise ValueError('Invalid `elements`. `elements` should not be empty.')\n    if not isinstance(elements, list):\n        raise ValueError('Invalid `elements`. `elements` must be a list.')\n    elements = [structure.normalize_element(element) for element in elements]\n    type_specs = [structure.type_spec_from_value(element) for element in elements]\n    num_elements = len(elements)\n    for i in range(1, num_elements):\n        nest.assert_same_structure(type_specs[0], type_specs[i])\n    flattened_type_specs = [nest.flatten(type_spec) for type_spec in type_specs]\n    num_tensors_per_element = len(flattened_type_specs[0])\n    flattened_structure = [None] * num_tensors_per_element\n    for i in range(num_tensors_per_element):\n        flattened_structure[i] = flattened_type_specs[0][i]\n        for j in range(1, num_elements):\n            flattened_structure[i] = flattened_structure[i].most_specific_common_supertype([flattened_type_specs[j][i]])\n    if not isinstance(type_specs[0], dataset_ops.DatasetSpec):\n        self._tensors = list(itertools.chain.from_iterable([nest.flatten(element) for element in elements]))\n    else:\n        self._tensors = [x._variant_tensor for x in elements]\n    self._structure = nest.pack_sequence_as(type_specs[0], flattened_structure)\n    self._name = name\n    variant_tensor = gen_experimental_dataset_ops.list_dataset(self._tensors, output_types=self._flat_types, output_shapes=self._flat_shapes, metadata=self._metadata.SerializeToString())\n    super(_ListDataset, self).__init__(variant_tensor)"
        ]
    },
    {
        "func_name": "element_spec",
        "original": "@property\ndef element_spec(self):\n    return self._structure",
        "mutated": [
            "@property\ndef element_spec(self):\n    if False:\n        i = 10\n    return self._structure",
            "@property\ndef element_spec(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self._structure",
            "@property\ndef element_spec(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self._structure",
            "@property\ndef element_spec(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self._structure",
            "@property\ndef element_spec(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self._structure"
        ]
    },
    {
        "func_name": "from_list",
        "original": "@tf_export('data.experimental.from_list')\ndef from_list(elements, name=None):\n    \"\"\"Creates a `Dataset` comprising the given list of elements.\n\n  The returned dataset will produce the items in the list one by one. The\n  functionality is identical to `Dataset.from_tensor_slices` when elements are\n  scalars, but different when elements have structure. Consider the following\n  example.\n\n  >>> dataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])\n  >>> list(dataset.as_numpy_iterator())\n  [(1, b'a'), (2, b'b'), (3, b'c')]\n\n  To get the same output with `from_tensor_slices`, the data needs to be\n  reorganized:\n\n  >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], ['a', 'b', 'c']))\n  >>> list(dataset.as_numpy_iterator())\n  [(1, b'a'), (2, b'b'), (3, b'c')]\n\n  Unlike `from_tensor_slices`, `from_list` supports non-rectangular input:\n\n  >>> dataset = tf.data.experimental.from_list([[1], [2, 3]])\n  >>> list(dataset.as_numpy_iterator())\n  [array([1], dtype=int32), array([2, 3], dtype=int32)]\n\n  Achieving the same with `from_tensor_slices` requires the use of ragged\n  tensors.\n\n  `from_list` can be more performant than `from_tensor_slices` in some cases,\n  since it avoids the need for data slicing each epoch. However, it can also be\n  less performant, because data is stored as many small tensors rather than a\n  few large tensors as in `from_tensor_slices`. The general guidance is to\n  prefer `from_list` from a performance perspective when the number of elements\n  is small (less than 1000).\n\n  Args:\n    elements: A list of elements whose components have the same nested\n      structure.\n    name: (Optional.) A name for the tf.data operation.\n\n  Returns:\n    Dataset: A `Dataset` of the `elements`.\n  \"\"\"\n    return _ListDataset(elements, name)",
        "mutated": [
            "@tf_export('data.experimental.from_list')\ndef from_list(elements, name=None):\n    if False:\n        i = 10\n    \"Creates a `Dataset` comprising the given list of elements.\\n\\n  The returned dataset will produce the items in the list one by one. The\\n  functionality is identical to `Dataset.from_tensor_slices` when elements are\\n  scalars, but different when elements have structure. Consider the following\\n  example.\\n\\n  >>> dataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])\\n  >>> list(dataset.as_numpy_iterator())\\n  [(1, b'a'), (2, b'b'), (3, b'c')]\\n\\n  To get the same output with `from_tensor_slices`, the data needs to be\\n  reorganized:\\n\\n  >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], ['a', 'b', 'c']))\\n  >>> list(dataset.as_numpy_iterator())\\n  [(1, b'a'), (2, b'b'), (3, b'c')]\\n\\n  Unlike `from_tensor_slices`, `from_list` supports non-rectangular input:\\n\\n  >>> dataset = tf.data.experimental.from_list([[1], [2, 3]])\\n  >>> list(dataset.as_numpy_iterator())\\n  [array([1], dtype=int32), array([2, 3], dtype=int32)]\\n\\n  Achieving the same with `from_tensor_slices` requires the use of ragged\\n  tensors.\\n\\n  `from_list` can be more performant than `from_tensor_slices` in some cases,\\n  since it avoids the need for data slicing each epoch. However, it can also be\\n  less performant, because data is stored as many small tensors rather than a\\n  few large tensors as in `from_tensor_slices`. The general guidance is to\\n  prefer `from_list` from a performance perspective when the number of elements\\n  is small (less than 1000).\\n\\n  Args:\\n    elements: A list of elements whose components have the same nested\\n      structure.\\n    name: (Optional.) A name for the tf.data operation.\\n\\n  Returns:\\n    Dataset: A `Dataset` of the `elements`.\\n  \"\n    return _ListDataset(elements, name)",
            "@tf_export('data.experimental.from_list')\ndef from_list(elements, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Creates a `Dataset` comprising the given list of elements.\\n\\n  The returned dataset will produce the items in the list one by one. The\\n  functionality is identical to `Dataset.from_tensor_slices` when elements are\\n  scalars, but different when elements have structure. Consider the following\\n  example.\\n\\n  >>> dataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])\\n  >>> list(dataset.as_numpy_iterator())\\n  [(1, b'a'), (2, b'b'), (3, b'c')]\\n\\n  To get the same output with `from_tensor_slices`, the data needs to be\\n  reorganized:\\n\\n  >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], ['a', 'b', 'c']))\\n  >>> list(dataset.as_numpy_iterator())\\n  [(1, b'a'), (2, b'b'), (3, b'c')]\\n\\n  Unlike `from_tensor_slices`, `from_list` supports non-rectangular input:\\n\\n  >>> dataset = tf.data.experimental.from_list([[1], [2, 3]])\\n  >>> list(dataset.as_numpy_iterator())\\n  [array([1], dtype=int32), array([2, 3], dtype=int32)]\\n\\n  Achieving the same with `from_tensor_slices` requires the use of ragged\\n  tensors.\\n\\n  `from_list` can be more performant than `from_tensor_slices` in some cases,\\n  since it avoids the need for data slicing each epoch. However, it can also be\\n  less performant, because data is stored as many small tensors rather than a\\n  few large tensors as in `from_tensor_slices`. The general guidance is to\\n  prefer `from_list` from a performance perspective when the number of elements\\n  is small (less than 1000).\\n\\n  Args:\\n    elements: A list of elements whose components have the same nested\\n      structure.\\n    name: (Optional.) A name for the tf.data operation.\\n\\n  Returns:\\n    Dataset: A `Dataset` of the `elements`.\\n  \"\n    return _ListDataset(elements, name)",
            "@tf_export('data.experimental.from_list')\ndef from_list(elements, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Creates a `Dataset` comprising the given list of elements.\\n\\n  The returned dataset will produce the items in the list one by one. The\\n  functionality is identical to `Dataset.from_tensor_slices` when elements are\\n  scalars, but different when elements have structure. Consider the following\\n  example.\\n\\n  >>> dataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])\\n  >>> list(dataset.as_numpy_iterator())\\n  [(1, b'a'), (2, b'b'), (3, b'c')]\\n\\n  To get the same output with `from_tensor_slices`, the data needs to be\\n  reorganized:\\n\\n  >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], ['a', 'b', 'c']))\\n  >>> list(dataset.as_numpy_iterator())\\n  [(1, b'a'), (2, b'b'), (3, b'c')]\\n\\n  Unlike `from_tensor_slices`, `from_list` supports non-rectangular input:\\n\\n  >>> dataset = tf.data.experimental.from_list([[1], [2, 3]])\\n  >>> list(dataset.as_numpy_iterator())\\n  [array([1], dtype=int32), array([2, 3], dtype=int32)]\\n\\n  Achieving the same with `from_tensor_slices` requires the use of ragged\\n  tensors.\\n\\n  `from_list` can be more performant than `from_tensor_slices` in some cases,\\n  since it avoids the need for data slicing each epoch. However, it can also be\\n  less performant, because data is stored as many small tensors rather than a\\n  few large tensors as in `from_tensor_slices`. The general guidance is to\\n  prefer `from_list` from a performance perspective when the number of elements\\n  is small (less than 1000).\\n\\n  Args:\\n    elements: A list of elements whose components have the same nested\\n      structure.\\n    name: (Optional.) A name for the tf.data operation.\\n\\n  Returns:\\n    Dataset: A `Dataset` of the `elements`.\\n  \"\n    return _ListDataset(elements, name)",
            "@tf_export('data.experimental.from_list')\ndef from_list(elements, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Creates a `Dataset` comprising the given list of elements.\\n\\n  The returned dataset will produce the items in the list one by one. The\\n  functionality is identical to `Dataset.from_tensor_slices` when elements are\\n  scalars, but different when elements have structure. Consider the following\\n  example.\\n\\n  >>> dataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])\\n  >>> list(dataset.as_numpy_iterator())\\n  [(1, b'a'), (2, b'b'), (3, b'c')]\\n\\n  To get the same output with `from_tensor_slices`, the data needs to be\\n  reorganized:\\n\\n  >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], ['a', 'b', 'c']))\\n  >>> list(dataset.as_numpy_iterator())\\n  [(1, b'a'), (2, b'b'), (3, b'c')]\\n\\n  Unlike `from_tensor_slices`, `from_list` supports non-rectangular input:\\n\\n  >>> dataset = tf.data.experimental.from_list([[1], [2, 3]])\\n  >>> list(dataset.as_numpy_iterator())\\n  [array([1], dtype=int32), array([2, 3], dtype=int32)]\\n\\n  Achieving the same with `from_tensor_slices` requires the use of ragged\\n  tensors.\\n\\n  `from_list` can be more performant than `from_tensor_slices` in some cases,\\n  since it avoids the need for data slicing each epoch. However, it can also be\\n  less performant, because data is stored as many small tensors rather than a\\n  few large tensors as in `from_tensor_slices`. The general guidance is to\\n  prefer `from_list` from a performance perspective when the number of elements\\n  is small (less than 1000).\\n\\n  Args:\\n    elements: A list of elements whose components have the same nested\\n      structure.\\n    name: (Optional.) A name for the tf.data operation.\\n\\n  Returns:\\n    Dataset: A `Dataset` of the `elements`.\\n  \"\n    return _ListDataset(elements, name)",
            "@tf_export('data.experimental.from_list')\ndef from_list(elements, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Creates a `Dataset` comprising the given list of elements.\\n\\n  The returned dataset will produce the items in the list one by one. The\\n  functionality is identical to `Dataset.from_tensor_slices` when elements are\\n  scalars, but different when elements have structure. Consider the following\\n  example.\\n\\n  >>> dataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])\\n  >>> list(dataset.as_numpy_iterator())\\n  [(1, b'a'), (2, b'b'), (3, b'c')]\\n\\n  To get the same output with `from_tensor_slices`, the data needs to be\\n  reorganized:\\n\\n  >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], ['a', 'b', 'c']))\\n  >>> list(dataset.as_numpy_iterator())\\n  [(1, b'a'), (2, b'b'), (3, b'c')]\\n\\n  Unlike `from_tensor_slices`, `from_list` supports non-rectangular input:\\n\\n  >>> dataset = tf.data.experimental.from_list([[1], [2, 3]])\\n  >>> list(dataset.as_numpy_iterator())\\n  [array([1], dtype=int32), array([2, 3], dtype=int32)]\\n\\n  Achieving the same with `from_tensor_slices` requires the use of ragged\\n  tensors.\\n\\n  `from_list` can be more performant than `from_tensor_slices` in some cases,\\n  since it avoids the need for data slicing each epoch. However, it can also be\\n  less performant, because data is stored as many small tensors rather than a\\n  few large tensors as in `from_tensor_slices`. The general guidance is to\\n  prefer `from_list` from a performance perspective when the number of elements\\n  is small (less than 1000).\\n\\n  Args:\\n    elements: A list of elements whose components have the same nested\\n      structure.\\n    name: (Optional.) A name for the tf.data operation.\\n\\n  Returns:\\n    Dataset: A `Dataset` of the `elements`.\\n  \"\n    return _ListDataset(elements, name)"
        ]
    }
]