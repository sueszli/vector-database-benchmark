[
    {
        "func_name": "from_query",
        "original": "def from_query(query, client_project=None, credentials=None):\n    '''Make a query to Google BigQuery and get the result as a Vaex DataFrame.\n\n    :param str query: The SQL query.\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\n    :rtype: DataFrame\n\n    Example\n\n    >>> import os\n    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../path/to/project_access_key.json'\n    >>> from vaex.contrib.io.gbq import from_query\n\n    >>> query = \"\"\"\n        select * from `bigquery-public-data.ml_datasets.iris`\n        where species = \"virginica\"\n    \"\"\"\n\n    >>> df = from_query(query=query)\n    >>> df.head(3)\n    #    sepal_length    sepal_width    petal_length    petal_width  species\n    0             4.9            2.5             4.5            1.7  virginica\n    1             5.7            2.5             5              2    virginica\n    2             6              2.2             5              1.5  virginica\n\n    '''\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    job = client.query(query=query)\n    return vaex.from_arrow_table(job.to_arrow())",
        "mutated": [
            "def from_query(query, client_project=None, credentials=None):\n    if False:\n        i = 10\n    'Make a query to Google BigQuery and get the result as a Vaex DataFrame.\\n\\n    :param str query: The SQL query.\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :rtype: DataFrame\\n\\n    Example\\n\\n    >>> import os\\n    os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'../path/to/project_access_key.json\\'\\n    >>> from vaex.contrib.io.gbq import from_query\\n\\n    >>> query = \"\"\"\\n        select * from `bigquery-public-data.ml_datasets.iris`\\n        where species = \"virginica\"\\n    \"\"\"\\n\\n    >>> df = from_query(query=query)\\n    >>> df.head(3)\\n    #    sepal_length    sepal_width    petal_length    petal_width  species\\n    0             4.9            2.5             4.5            1.7  virginica\\n    1             5.7            2.5             5              2    virginica\\n    2             6              2.2             5              1.5  virginica\\n\\n    '\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    job = client.query(query=query)\n    return vaex.from_arrow_table(job.to_arrow())",
            "def from_query(query, client_project=None, credentials=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Make a query to Google BigQuery and get the result as a Vaex DataFrame.\\n\\n    :param str query: The SQL query.\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :rtype: DataFrame\\n\\n    Example\\n\\n    >>> import os\\n    os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'../path/to/project_access_key.json\\'\\n    >>> from vaex.contrib.io.gbq import from_query\\n\\n    >>> query = \"\"\"\\n        select * from `bigquery-public-data.ml_datasets.iris`\\n        where species = \"virginica\"\\n    \"\"\"\\n\\n    >>> df = from_query(query=query)\\n    >>> df.head(3)\\n    #    sepal_length    sepal_width    petal_length    petal_width  species\\n    0             4.9            2.5             4.5            1.7  virginica\\n    1             5.7            2.5             5              2    virginica\\n    2             6              2.2             5              1.5  virginica\\n\\n    '\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    job = client.query(query=query)\n    return vaex.from_arrow_table(job.to_arrow())",
            "def from_query(query, client_project=None, credentials=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Make a query to Google BigQuery and get the result as a Vaex DataFrame.\\n\\n    :param str query: The SQL query.\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :rtype: DataFrame\\n\\n    Example\\n\\n    >>> import os\\n    os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'../path/to/project_access_key.json\\'\\n    >>> from vaex.contrib.io.gbq import from_query\\n\\n    >>> query = \"\"\"\\n        select * from `bigquery-public-data.ml_datasets.iris`\\n        where species = \"virginica\"\\n    \"\"\"\\n\\n    >>> df = from_query(query=query)\\n    >>> df.head(3)\\n    #    sepal_length    sepal_width    petal_length    petal_width  species\\n    0             4.9            2.5             4.5            1.7  virginica\\n    1             5.7            2.5             5              2    virginica\\n    2             6              2.2             5              1.5  virginica\\n\\n    '\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    job = client.query(query=query)\n    return vaex.from_arrow_table(job.to_arrow())",
            "def from_query(query, client_project=None, credentials=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Make a query to Google BigQuery and get the result as a Vaex DataFrame.\\n\\n    :param str query: The SQL query.\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :rtype: DataFrame\\n\\n    Example\\n\\n    >>> import os\\n    os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'../path/to/project_access_key.json\\'\\n    >>> from vaex.contrib.io.gbq import from_query\\n\\n    >>> query = \"\"\"\\n        select * from `bigquery-public-data.ml_datasets.iris`\\n        where species = \"virginica\"\\n    \"\"\"\\n\\n    >>> df = from_query(query=query)\\n    >>> df.head(3)\\n    #    sepal_length    sepal_width    petal_length    petal_width  species\\n    0             4.9            2.5             4.5            1.7  virginica\\n    1             5.7            2.5             5              2    virginica\\n    2             6              2.2             5              1.5  virginica\\n\\n    '\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    job = client.query(query=query)\n    return vaex.from_arrow_table(job.to_arrow())",
            "def from_query(query, client_project=None, credentials=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Make a query to Google BigQuery and get the result as a Vaex DataFrame.\\n\\n    :param str query: The SQL query.\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :rtype: DataFrame\\n\\n    Example\\n\\n    >>> import os\\n    os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'../path/to/project_access_key.json\\'\\n    >>> from vaex.contrib.io.gbq import from_query\\n\\n    >>> query = \"\"\"\\n        select * from `bigquery-public-data.ml_datasets.iris`\\n        where species = \"virginica\"\\n    \"\"\"\\n\\n    >>> df = from_query(query=query)\\n    >>> df.head(3)\\n    #    sepal_length    sepal_width    petal_length    petal_width  species\\n    0             4.9            2.5             4.5            1.7  virginica\\n    1             5.7            2.5             5              2    virginica\\n    2             6              2.2             5              1.5  virginica\\n\\n    '\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    job = client.query(query=query)\n    return vaex.from_arrow_table(job.to_arrow())"
        ]
    },
    {
        "func_name": "from_table",
        "original": "@docsubst\ndef from_table(project, dataset, table, columns=None, condition=None, export=None, fs=None, fs_options=None, client_project=None, credentials=None):\n    \"\"\"Download (stream) an entire Google BigQuery table locally.\n\n    :param str project: The Google BigQuery project that owns the table.\n    :param str dataset: The dataset the table is part of.\n    :param str table: The name of the table\n    :param list columns: A list of columns (field names) to download. If None, all columns will be downloaded.\n    :param str condition: SQL text filtering statement, similar to a WHERE clause in a query. Aggregates are not supported.\n    :param str export: Pass an filename or path to download the table as an Apache Arrow file, and leverage memory mapping. If `None` the DataFrame is in memory.\n    :param fs: Valid if export is not None. {fs}\n    :param fs: Valid if export is not None. {fs_options}\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, it will be set with the same value as `project`.\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\n    :rtype: DataFrame\n\n    Example:\n\n    >>> import os\n    >>> os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../path/to/project_access_key.json'\n    >>> from vaex.contrib.io.gbq import from_table\n\n    >>> client_project = 'my_project_id'\n    >>> project = 'bigquery-public-data'\n    >>> dataset = 'ml_datasets'\n    >>> table = 'iris'\n    >>> columns = ['species', 'sepal_width', 'petal_width']\n    >>> conditions = 'species = \"virginica\"'\n    >>> df = from_table(project=project,\n                                            dataset=dataset,\n                                            table=table,\n                                            columns=columns,\n                                            condition=conditions,\n                                            client_project=client_project)\n    >>> df.head(3)\n    #    sepal_width    petal_width  species\n    0            2.5            1.7  virginica\n    1            2.5            2    virginica\n    2            2.2            1.5  virginica\n    >>>\n\n    \"\"\"\n    bq_table = f'projects/{project}/datasets/{dataset}/tables/{table}'\n    req_sess = google.cloud.bigquery_storage.types.ReadSession(table=bq_table, data_format=google.cloud.bigquery_storage.types.DataFormat.ARROW)\n    req_sess.read_options.selected_fields = columns\n    req_sess.read_options.row_restriction = condition\n    client = google.cloud.bigquery_storage.BigQueryReadClient(credentials=credentials)\n    parent = f'projects/{client_project or project}'\n    session = client.create_read_session(parent=parent, read_session=req_sess, max_stream_count=1)\n    reader = client.read_rows(session.streams[0].name)\n    if export is None:\n        arrow_table = reader.to_arrow(session)\n        return vaex.from_arrow_table(arrow_table)\n    else:\n        pages = reader.rows(session).pages\n        first_batch = pages.__next__().to_arrow()\n        schema = first_batch.schema\n        with vaex.file.open(path=export, mode='wb', fs=fs, fs_options=fs_options) as sink:\n            with pa.RecordBatchStreamWriter(sink, schema) as writer:\n                writer.write_batch(first_batch)\n                for page in pages:\n                    batch = page.to_arrow()\n                    writer.write_batch(batch)\n        return vaex.open(export)",
        "mutated": [
            "@docsubst\ndef from_table(project, dataset, table, columns=None, condition=None, export=None, fs=None, fs_options=None, client_project=None, credentials=None):\n    if False:\n        i = 10\n    'Download (stream) an entire Google BigQuery table locally.\\n\\n    :param str project: The Google BigQuery project that owns the table.\\n    :param str dataset: The dataset the table is part of.\\n    :param str table: The name of the table\\n    :param list columns: A list of columns (field names) to download. If None, all columns will be downloaded.\\n    :param str condition: SQL text filtering statement, similar to a WHERE clause in a query. Aggregates are not supported.\\n    :param str export: Pass an filename or path to download the table as an Apache Arrow file, and leverage memory mapping. If `None` the DataFrame is in memory.\\n    :param fs: Valid if export is not None. {fs}\\n    :param fs: Valid if export is not None. {fs_options}\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, it will be set with the same value as `project`.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :rtype: DataFrame\\n\\n    Example:\\n\\n    >>> import os\\n    >>> os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'../path/to/project_access_key.json\\'\\n    >>> from vaex.contrib.io.gbq import from_table\\n\\n    >>> client_project = \\'my_project_id\\'\\n    >>> project = \\'bigquery-public-data\\'\\n    >>> dataset = \\'ml_datasets\\'\\n    >>> table = \\'iris\\'\\n    >>> columns = [\\'species\\', \\'sepal_width\\', \\'petal_width\\']\\n    >>> conditions = \\'species = \"virginica\"\\'\\n    >>> df = from_table(project=project,\\n                                            dataset=dataset,\\n                                            table=table,\\n                                            columns=columns,\\n                                            condition=conditions,\\n                                            client_project=client_project)\\n    >>> df.head(3)\\n    #    sepal_width    petal_width  species\\n    0            2.5            1.7  virginica\\n    1            2.5            2    virginica\\n    2            2.2            1.5  virginica\\n    >>>\\n\\n    '\n    bq_table = f'projects/{project}/datasets/{dataset}/tables/{table}'\n    req_sess = google.cloud.bigquery_storage.types.ReadSession(table=bq_table, data_format=google.cloud.bigquery_storage.types.DataFormat.ARROW)\n    req_sess.read_options.selected_fields = columns\n    req_sess.read_options.row_restriction = condition\n    client = google.cloud.bigquery_storage.BigQueryReadClient(credentials=credentials)\n    parent = f'projects/{client_project or project}'\n    session = client.create_read_session(parent=parent, read_session=req_sess, max_stream_count=1)\n    reader = client.read_rows(session.streams[0].name)\n    if export is None:\n        arrow_table = reader.to_arrow(session)\n        return vaex.from_arrow_table(arrow_table)\n    else:\n        pages = reader.rows(session).pages\n        first_batch = pages.__next__().to_arrow()\n        schema = first_batch.schema\n        with vaex.file.open(path=export, mode='wb', fs=fs, fs_options=fs_options) as sink:\n            with pa.RecordBatchStreamWriter(sink, schema) as writer:\n                writer.write_batch(first_batch)\n                for page in pages:\n                    batch = page.to_arrow()\n                    writer.write_batch(batch)\n        return vaex.open(export)",
            "@docsubst\ndef from_table(project, dataset, table, columns=None, condition=None, export=None, fs=None, fs_options=None, client_project=None, credentials=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Download (stream) an entire Google BigQuery table locally.\\n\\n    :param str project: The Google BigQuery project that owns the table.\\n    :param str dataset: The dataset the table is part of.\\n    :param str table: The name of the table\\n    :param list columns: A list of columns (field names) to download. If None, all columns will be downloaded.\\n    :param str condition: SQL text filtering statement, similar to a WHERE clause in a query. Aggregates are not supported.\\n    :param str export: Pass an filename or path to download the table as an Apache Arrow file, and leverage memory mapping. If `None` the DataFrame is in memory.\\n    :param fs: Valid if export is not None. {fs}\\n    :param fs: Valid if export is not None. {fs_options}\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, it will be set with the same value as `project`.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :rtype: DataFrame\\n\\n    Example:\\n\\n    >>> import os\\n    >>> os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'../path/to/project_access_key.json\\'\\n    >>> from vaex.contrib.io.gbq import from_table\\n\\n    >>> client_project = \\'my_project_id\\'\\n    >>> project = \\'bigquery-public-data\\'\\n    >>> dataset = \\'ml_datasets\\'\\n    >>> table = \\'iris\\'\\n    >>> columns = [\\'species\\', \\'sepal_width\\', \\'petal_width\\']\\n    >>> conditions = \\'species = \"virginica\"\\'\\n    >>> df = from_table(project=project,\\n                                            dataset=dataset,\\n                                            table=table,\\n                                            columns=columns,\\n                                            condition=conditions,\\n                                            client_project=client_project)\\n    >>> df.head(3)\\n    #    sepal_width    petal_width  species\\n    0            2.5            1.7  virginica\\n    1            2.5            2    virginica\\n    2            2.2            1.5  virginica\\n    >>>\\n\\n    '\n    bq_table = f'projects/{project}/datasets/{dataset}/tables/{table}'\n    req_sess = google.cloud.bigquery_storage.types.ReadSession(table=bq_table, data_format=google.cloud.bigquery_storage.types.DataFormat.ARROW)\n    req_sess.read_options.selected_fields = columns\n    req_sess.read_options.row_restriction = condition\n    client = google.cloud.bigquery_storage.BigQueryReadClient(credentials=credentials)\n    parent = f'projects/{client_project or project}'\n    session = client.create_read_session(parent=parent, read_session=req_sess, max_stream_count=1)\n    reader = client.read_rows(session.streams[0].name)\n    if export is None:\n        arrow_table = reader.to_arrow(session)\n        return vaex.from_arrow_table(arrow_table)\n    else:\n        pages = reader.rows(session).pages\n        first_batch = pages.__next__().to_arrow()\n        schema = first_batch.schema\n        with vaex.file.open(path=export, mode='wb', fs=fs, fs_options=fs_options) as sink:\n            with pa.RecordBatchStreamWriter(sink, schema) as writer:\n                writer.write_batch(first_batch)\n                for page in pages:\n                    batch = page.to_arrow()\n                    writer.write_batch(batch)\n        return vaex.open(export)",
            "@docsubst\ndef from_table(project, dataset, table, columns=None, condition=None, export=None, fs=None, fs_options=None, client_project=None, credentials=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Download (stream) an entire Google BigQuery table locally.\\n\\n    :param str project: The Google BigQuery project that owns the table.\\n    :param str dataset: The dataset the table is part of.\\n    :param str table: The name of the table\\n    :param list columns: A list of columns (field names) to download. If None, all columns will be downloaded.\\n    :param str condition: SQL text filtering statement, similar to a WHERE clause in a query. Aggregates are not supported.\\n    :param str export: Pass an filename or path to download the table as an Apache Arrow file, and leverage memory mapping. If `None` the DataFrame is in memory.\\n    :param fs: Valid if export is not None. {fs}\\n    :param fs: Valid if export is not None. {fs_options}\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, it will be set with the same value as `project`.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :rtype: DataFrame\\n\\n    Example:\\n\\n    >>> import os\\n    >>> os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'../path/to/project_access_key.json\\'\\n    >>> from vaex.contrib.io.gbq import from_table\\n\\n    >>> client_project = \\'my_project_id\\'\\n    >>> project = \\'bigquery-public-data\\'\\n    >>> dataset = \\'ml_datasets\\'\\n    >>> table = \\'iris\\'\\n    >>> columns = [\\'species\\', \\'sepal_width\\', \\'petal_width\\']\\n    >>> conditions = \\'species = \"virginica\"\\'\\n    >>> df = from_table(project=project,\\n                                            dataset=dataset,\\n                                            table=table,\\n                                            columns=columns,\\n                                            condition=conditions,\\n                                            client_project=client_project)\\n    >>> df.head(3)\\n    #    sepal_width    petal_width  species\\n    0            2.5            1.7  virginica\\n    1            2.5            2    virginica\\n    2            2.2            1.5  virginica\\n    >>>\\n\\n    '\n    bq_table = f'projects/{project}/datasets/{dataset}/tables/{table}'\n    req_sess = google.cloud.bigquery_storage.types.ReadSession(table=bq_table, data_format=google.cloud.bigquery_storage.types.DataFormat.ARROW)\n    req_sess.read_options.selected_fields = columns\n    req_sess.read_options.row_restriction = condition\n    client = google.cloud.bigquery_storage.BigQueryReadClient(credentials=credentials)\n    parent = f'projects/{client_project or project}'\n    session = client.create_read_session(parent=parent, read_session=req_sess, max_stream_count=1)\n    reader = client.read_rows(session.streams[0].name)\n    if export is None:\n        arrow_table = reader.to_arrow(session)\n        return vaex.from_arrow_table(arrow_table)\n    else:\n        pages = reader.rows(session).pages\n        first_batch = pages.__next__().to_arrow()\n        schema = first_batch.schema\n        with vaex.file.open(path=export, mode='wb', fs=fs, fs_options=fs_options) as sink:\n            with pa.RecordBatchStreamWriter(sink, schema) as writer:\n                writer.write_batch(first_batch)\n                for page in pages:\n                    batch = page.to_arrow()\n                    writer.write_batch(batch)\n        return vaex.open(export)",
            "@docsubst\ndef from_table(project, dataset, table, columns=None, condition=None, export=None, fs=None, fs_options=None, client_project=None, credentials=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Download (stream) an entire Google BigQuery table locally.\\n\\n    :param str project: The Google BigQuery project that owns the table.\\n    :param str dataset: The dataset the table is part of.\\n    :param str table: The name of the table\\n    :param list columns: A list of columns (field names) to download. If None, all columns will be downloaded.\\n    :param str condition: SQL text filtering statement, similar to a WHERE clause in a query. Aggregates are not supported.\\n    :param str export: Pass an filename or path to download the table as an Apache Arrow file, and leverage memory mapping. If `None` the DataFrame is in memory.\\n    :param fs: Valid if export is not None. {fs}\\n    :param fs: Valid if export is not None. {fs_options}\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, it will be set with the same value as `project`.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :rtype: DataFrame\\n\\n    Example:\\n\\n    >>> import os\\n    >>> os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'../path/to/project_access_key.json\\'\\n    >>> from vaex.contrib.io.gbq import from_table\\n\\n    >>> client_project = \\'my_project_id\\'\\n    >>> project = \\'bigquery-public-data\\'\\n    >>> dataset = \\'ml_datasets\\'\\n    >>> table = \\'iris\\'\\n    >>> columns = [\\'species\\', \\'sepal_width\\', \\'petal_width\\']\\n    >>> conditions = \\'species = \"virginica\"\\'\\n    >>> df = from_table(project=project,\\n                                            dataset=dataset,\\n                                            table=table,\\n                                            columns=columns,\\n                                            condition=conditions,\\n                                            client_project=client_project)\\n    >>> df.head(3)\\n    #    sepal_width    petal_width  species\\n    0            2.5            1.7  virginica\\n    1            2.5            2    virginica\\n    2            2.2            1.5  virginica\\n    >>>\\n\\n    '\n    bq_table = f'projects/{project}/datasets/{dataset}/tables/{table}'\n    req_sess = google.cloud.bigquery_storage.types.ReadSession(table=bq_table, data_format=google.cloud.bigquery_storage.types.DataFormat.ARROW)\n    req_sess.read_options.selected_fields = columns\n    req_sess.read_options.row_restriction = condition\n    client = google.cloud.bigquery_storage.BigQueryReadClient(credentials=credentials)\n    parent = f'projects/{client_project or project}'\n    session = client.create_read_session(parent=parent, read_session=req_sess, max_stream_count=1)\n    reader = client.read_rows(session.streams[0].name)\n    if export is None:\n        arrow_table = reader.to_arrow(session)\n        return vaex.from_arrow_table(arrow_table)\n    else:\n        pages = reader.rows(session).pages\n        first_batch = pages.__next__().to_arrow()\n        schema = first_batch.schema\n        with vaex.file.open(path=export, mode='wb', fs=fs, fs_options=fs_options) as sink:\n            with pa.RecordBatchStreamWriter(sink, schema) as writer:\n                writer.write_batch(first_batch)\n                for page in pages:\n                    batch = page.to_arrow()\n                    writer.write_batch(batch)\n        return vaex.open(export)",
            "@docsubst\ndef from_table(project, dataset, table, columns=None, condition=None, export=None, fs=None, fs_options=None, client_project=None, credentials=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Download (stream) an entire Google BigQuery table locally.\\n\\n    :param str project: The Google BigQuery project that owns the table.\\n    :param str dataset: The dataset the table is part of.\\n    :param str table: The name of the table\\n    :param list columns: A list of columns (field names) to download. If None, all columns will be downloaded.\\n    :param str condition: SQL text filtering statement, similar to a WHERE clause in a query. Aggregates are not supported.\\n    :param str export: Pass an filename or path to download the table as an Apache Arrow file, and leverage memory mapping. If `None` the DataFrame is in memory.\\n    :param fs: Valid if export is not None. {fs}\\n    :param fs: Valid if export is not None. {fs_options}\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, it will be set with the same value as `project`.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :rtype: DataFrame\\n\\n    Example:\\n\\n    >>> import os\\n    >>> os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'../path/to/project_access_key.json\\'\\n    >>> from vaex.contrib.io.gbq import from_table\\n\\n    >>> client_project = \\'my_project_id\\'\\n    >>> project = \\'bigquery-public-data\\'\\n    >>> dataset = \\'ml_datasets\\'\\n    >>> table = \\'iris\\'\\n    >>> columns = [\\'species\\', \\'sepal_width\\', \\'petal_width\\']\\n    >>> conditions = \\'species = \"virginica\"\\'\\n    >>> df = from_table(project=project,\\n                                            dataset=dataset,\\n                                            table=table,\\n                                            columns=columns,\\n                                            condition=conditions,\\n                                            client_project=client_project)\\n    >>> df.head(3)\\n    #    sepal_width    petal_width  species\\n    0            2.5            1.7  virginica\\n    1            2.5            2    virginica\\n    2            2.2            1.5  virginica\\n    >>>\\n\\n    '\n    bq_table = f'projects/{project}/datasets/{dataset}/tables/{table}'\n    req_sess = google.cloud.bigquery_storage.types.ReadSession(table=bq_table, data_format=google.cloud.bigquery_storage.types.DataFormat.ARROW)\n    req_sess.read_options.selected_fields = columns\n    req_sess.read_options.row_restriction = condition\n    client = google.cloud.bigquery_storage.BigQueryReadClient(credentials=credentials)\n    parent = f'projects/{client_project or project}'\n    session = client.create_read_session(parent=parent, read_session=req_sess, max_stream_count=1)\n    reader = client.read_rows(session.streams[0].name)\n    if export is None:\n        arrow_table = reader.to_arrow(session)\n        return vaex.from_arrow_table(arrow_table)\n    else:\n        pages = reader.rows(session).pages\n        first_batch = pages.__next__().to_arrow()\n        schema = first_batch.schema\n        with vaex.file.open(path=export, mode='wb', fs=fs, fs_options=fs_options) as sink:\n            with pa.RecordBatchStreamWriter(sink, schema) as writer:\n                writer.write_batch(first_batch)\n                for page in pages:\n                    batch = page.to_arrow()\n                    writer.write_batch(batch)\n        return vaex.open(export)"
        ]
    },
    {
        "func_name": "to_table",
        "original": "def to_table(df, dataset, table, job_config=None, client_project=None, credentials=None, chunk_size=None, progress=None):\n    \"\"\"Upload a Vaex DataFrame to a Google BigQuery Table.\n\n    Note that the upload creates a temporary parquet file on the local disk, which is then upload to\n    Google BigQuery.\n\n    :param DataFrame df: The Vaex DataFrame to be uploaded.\n    :param str dataset: The name of the dataset to which the table belongs\n    :param str table: The name of the table\n    :param job_config: Optional, an instance of google.cloud.bigquery.job.load.LoadJobConfig\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\n    :param chunk_size: In case the local disk space is limited, export the dataset in chunks.\n                       This is considerably slower than a single file upload and it should be avoided.\n    :param progress: Valid only if chunk_size is not None. A callable that takes one argument (a floating point value between 0 and 1) indicating the progress, calculations are cancelled when this callable returns False\n\n    Example:\n\n    >>> import os\n    >>> os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../path/to/project_access_key.json'\n    >>> import vaex\n    >>> from vaex.contrib.io.gbq import to_table\n\n    >>> df = vaex.example()\n    >>> dataset = 'my_dataset'\n    >>> table = 'my_table'\n\n    >>> to_table(df=df, dataset=dataset, table=table)\n\n    \"\"\"\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    if job_config is not None:\n        assert isinstance(job_config, google.cloud.bigquery.job.load.LoadJobConfig)\n        job_config.source_format = google.cloud.bigquery.SourceFormat.PARQUET\n    else:\n        job_config = google.cloud.bigquery.LoadJobConfig(source_format=google.cloud.bigquery.SourceFormat.PARQUET)\n    table_bq = f'{dataset}.{table}'\n    if chunk_size is None:\n        with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n            df.export_parquet('tmp.parquet')\n            with open('tmp.parquet', 'rb') as source_file:\n                job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n            job.result()\n    else:\n        progressbar = vaex.utils.progressbars(progress)\n        n_samples = len(df)\n        for (i1, i2, table) in df.to_arrow_table(chunk_size=chunk_size):\n            progressbar(i1 / n_samples)\n            with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n                pq.write_table(table, 'tmp.parquet')\n                with open('tmp.parquet', 'rb') as source_file:\n                    job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n                job.result()\n        progressbar(1.0)",
        "mutated": [
            "def to_table(df, dataset, table, job_config=None, client_project=None, credentials=None, chunk_size=None, progress=None):\n    if False:\n        i = 10\n    \"Upload a Vaex DataFrame to a Google BigQuery Table.\\n\\n    Note that the upload creates a temporary parquet file on the local disk, which is then upload to\\n    Google BigQuery.\\n\\n    :param DataFrame df: The Vaex DataFrame to be uploaded.\\n    :param str dataset: The name of the dataset to which the table belongs\\n    :param str table: The name of the table\\n    :param job_config: Optional, an instance of google.cloud.bigquery.job.load.LoadJobConfig\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :param chunk_size: In case the local disk space is limited, export the dataset in chunks.\\n                       This is considerably slower than a single file upload and it should be avoided.\\n    :param progress: Valid only if chunk_size is not None. A callable that takes one argument (a floating point value between 0 and 1) indicating the progress, calculations are cancelled when this callable returns False\\n\\n    Example:\\n\\n    >>> import os\\n    >>> os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../path/to/project_access_key.json'\\n    >>> import vaex\\n    >>> from vaex.contrib.io.gbq import to_table\\n\\n    >>> df = vaex.example()\\n    >>> dataset = 'my_dataset'\\n    >>> table = 'my_table'\\n\\n    >>> to_table(df=df, dataset=dataset, table=table)\\n\\n    \"\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    if job_config is not None:\n        assert isinstance(job_config, google.cloud.bigquery.job.load.LoadJobConfig)\n        job_config.source_format = google.cloud.bigquery.SourceFormat.PARQUET\n    else:\n        job_config = google.cloud.bigquery.LoadJobConfig(source_format=google.cloud.bigquery.SourceFormat.PARQUET)\n    table_bq = f'{dataset}.{table}'\n    if chunk_size is None:\n        with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n            df.export_parquet('tmp.parquet')\n            with open('tmp.parquet', 'rb') as source_file:\n                job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n            job.result()\n    else:\n        progressbar = vaex.utils.progressbars(progress)\n        n_samples = len(df)\n        for (i1, i2, table) in df.to_arrow_table(chunk_size=chunk_size):\n            progressbar(i1 / n_samples)\n            with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n                pq.write_table(table, 'tmp.parquet')\n                with open('tmp.parquet', 'rb') as source_file:\n                    job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n                job.result()\n        progressbar(1.0)",
            "def to_table(df, dataset, table, job_config=None, client_project=None, credentials=None, chunk_size=None, progress=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Upload a Vaex DataFrame to a Google BigQuery Table.\\n\\n    Note that the upload creates a temporary parquet file on the local disk, which is then upload to\\n    Google BigQuery.\\n\\n    :param DataFrame df: The Vaex DataFrame to be uploaded.\\n    :param str dataset: The name of the dataset to which the table belongs\\n    :param str table: The name of the table\\n    :param job_config: Optional, an instance of google.cloud.bigquery.job.load.LoadJobConfig\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :param chunk_size: In case the local disk space is limited, export the dataset in chunks.\\n                       This is considerably slower than a single file upload and it should be avoided.\\n    :param progress: Valid only if chunk_size is not None. A callable that takes one argument (a floating point value between 0 and 1) indicating the progress, calculations are cancelled when this callable returns False\\n\\n    Example:\\n\\n    >>> import os\\n    >>> os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../path/to/project_access_key.json'\\n    >>> import vaex\\n    >>> from vaex.contrib.io.gbq import to_table\\n\\n    >>> df = vaex.example()\\n    >>> dataset = 'my_dataset'\\n    >>> table = 'my_table'\\n\\n    >>> to_table(df=df, dataset=dataset, table=table)\\n\\n    \"\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    if job_config is not None:\n        assert isinstance(job_config, google.cloud.bigquery.job.load.LoadJobConfig)\n        job_config.source_format = google.cloud.bigquery.SourceFormat.PARQUET\n    else:\n        job_config = google.cloud.bigquery.LoadJobConfig(source_format=google.cloud.bigquery.SourceFormat.PARQUET)\n    table_bq = f'{dataset}.{table}'\n    if chunk_size is None:\n        with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n            df.export_parquet('tmp.parquet')\n            with open('tmp.parquet', 'rb') as source_file:\n                job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n            job.result()\n    else:\n        progressbar = vaex.utils.progressbars(progress)\n        n_samples = len(df)\n        for (i1, i2, table) in df.to_arrow_table(chunk_size=chunk_size):\n            progressbar(i1 / n_samples)\n            with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n                pq.write_table(table, 'tmp.parquet')\n                with open('tmp.parquet', 'rb') as source_file:\n                    job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n                job.result()\n        progressbar(1.0)",
            "def to_table(df, dataset, table, job_config=None, client_project=None, credentials=None, chunk_size=None, progress=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Upload a Vaex DataFrame to a Google BigQuery Table.\\n\\n    Note that the upload creates a temporary parquet file on the local disk, which is then upload to\\n    Google BigQuery.\\n\\n    :param DataFrame df: The Vaex DataFrame to be uploaded.\\n    :param str dataset: The name of the dataset to which the table belongs\\n    :param str table: The name of the table\\n    :param job_config: Optional, an instance of google.cloud.bigquery.job.load.LoadJobConfig\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :param chunk_size: In case the local disk space is limited, export the dataset in chunks.\\n                       This is considerably slower than a single file upload and it should be avoided.\\n    :param progress: Valid only if chunk_size is not None. A callable that takes one argument (a floating point value between 0 and 1) indicating the progress, calculations are cancelled when this callable returns False\\n\\n    Example:\\n\\n    >>> import os\\n    >>> os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../path/to/project_access_key.json'\\n    >>> import vaex\\n    >>> from vaex.contrib.io.gbq import to_table\\n\\n    >>> df = vaex.example()\\n    >>> dataset = 'my_dataset'\\n    >>> table = 'my_table'\\n\\n    >>> to_table(df=df, dataset=dataset, table=table)\\n\\n    \"\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    if job_config is not None:\n        assert isinstance(job_config, google.cloud.bigquery.job.load.LoadJobConfig)\n        job_config.source_format = google.cloud.bigquery.SourceFormat.PARQUET\n    else:\n        job_config = google.cloud.bigquery.LoadJobConfig(source_format=google.cloud.bigquery.SourceFormat.PARQUET)\n    table_bq = f'{dataset}.{table}'\n    if chunk_size is None:\n        with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n            df.export_parquet('tmp.parquet')\n            with open('tmp.parquet', 'rb') as source_file:\n                job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n            job.result()\n    else:\n        progressbar = vaex.utils.progressbars(progress)\n        n_samples = len(df)\n        for (i1, i2, table) in df.to_arrow_table(chunk_size=chunk_size):\n            progressbar(i1 / n_samples)\n            with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n                pq.write_table(table, 'tmp.parquet')\n                with open('tmp.parquet', 'rb') as source_file:\n                    job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n                job.result()\n        progressbar(1.0)",
            "def to_table(df, dataset, table, job_config=None, client_project=None, credentials=None, chunk_size=None, progress=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Upload a Vaex DataFrame to a Google BigQuery Table.\\n\\n    Note that the upload creates a temporary parquet file on the local disk, which is then upload to\\n    Google BigQuery.\\n\\n    :param DataFrame df: The Vaex DataFrame to be uploaded.\\n    :param str dataset: The name of the dataset to which the table belongs\\n    :param str table: The name of the table\\n    :param job_config: Optional, an instance of google.cloud.bigquery.job.load.LoadJobConfig\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :param chunk_size: In case the local disk space is limited, export the dataset in chunks.\\n                       This is considerably slower than a single file upload and it should be avoided.\\n    :param progress: Valid only if chunk_size is not None. A callable that takes one argument (a floating point value between 0 and 1) indicating the progress, calculations are cancelled when this callable returns False\\n\\n    Example:\\n\\n    >>> import os\\n    >>> os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../path/to/project_access_key.json'\\n    >>> import vaex\\n    >>> from vaex.contrib.io.gbq import to_table\\n\\n    >>> df = vaex.example()\\n    >>> dataset = 'my_dataset'\\n    >>> table = 'my_table'\\n\\n    >>> to_table(df=df, dataset=dataset, table=table)\\n\\n    \"\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    if job_config is not None:\n        assert isinstance(job_config, google.cloud.bigquery.job.load.LoadJobConfig)\n        job_config.source_format = google.cloud.bigquery.SourceFormat.PARQUET\n    else:\n        job_config = google.cloud.bigquery.LoadJobConfig(source_format=google.cloud.bigquery.SourceFormat.PARQUET)\n    table_bq = f'{dataset}.{table}'\n    if chunk_size is None:\n        with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n            df.export_parquet('tmp.parquet')\n            with open('tmp.parquet', 'rb') as source_file:\n                job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n            job.result()\n    else:\n        progressbar = vaex.utils.progressbars(progress)\n        n_samples = len(df)\n        for (i1, i2, table) in df.to_arrow_table(chunk_size=chunk_size):\n            progressbar(i1 / n_samples)\n            with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n                pq.write_table(table, 'tmp.parquet')\n                with open('tmp.parquet', 'rb') as source_file:\n                    job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n                job.result()\n        progressbar(1.0)",
            "def to_table(df, dataset, table, job_config=None, client_project=None, credentials=None, chunk_size=None, progress=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Upload a Vaex DataFrame to a Google BigQuery Table.\\n\\n    Note that the upload creates a temporary parquet file on the local disk, which is then upload to\\n    Google BigQuery.\\n\\n    :param DataFrame df: The Vaex DataFrame to be uploaded.\\n    :param str dataset: The name of the dataset to which the table belongs\\n    :param str table: The name of the table\\n    :param job_config: Optional, an instance of google.cloud.bigquery.job.load.LoadJobConfig\\n    :param str client_project: The ID of the project that executes the query. Will be passed when creating a job. If `None`, falls back to the default inferred from the environment.\\n    :param credentials: The authorization credentials to attach to requests. See google.auth.credentials.Credentials for more details.\\n    :param chunk_size: In case the local disk space is limited, export the dataset in chunks.\\n                       This is considerably slower than a single file upload and it should be avoided.\\n    :param progress: Valid only if chunk_size is not None. A callable that takes one argument (a floating point value between 0 and 1) indicating the progress, calculations are cancelled when this callable returns False\\n\\n    Example:\\n\\n    >>> import os\\n    >>> os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../path/to/project_access_key.json'\\n    >>> import vaex\\n    >>> from vaex.contrib.io.gbq import to_table\\n\\n    >>> df = vaex.example()\\n    >>> dataset = 'my_dataset'\\n    >>> table = 'my_table'\\n\\n    >>> to_table(df=df, dataset=dataset, table=table)\\n\\n    \"\n    client = google.cloud.bigquery.Client(project=client_project, credentials=credentials)\n    if job_config is not None:\n        assert isinstance(job_config, google.cloud.bigquery.job.load.LoadJobConfig)\n        job_config.source_format = google.cloud.bigquery.SourceFormat.PARQUET\n    else:\n        job_config = google.cloud.bigquery.LoadJobConfig(source_format=google.cloud.bigquery.SourceFormat.PARQUET)\n    table_bq = f'{dataset}.{table}'\n    if chunk_size is None:\n        with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n            df.export_parquet('tmp.parquet')\n            with open('tmp.parquet', 'rb') as source_file:\n                job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n            job.result()\n    else:\n        progressbar = vaex.utils.progressbars(progress)\n        n_samples = len(df)\n        for (i1, i2, table) in df.to_arrow_table(chunk_size=chunk_size):\n            progressbar(i1 / n_samples)\n            with tempfile.TemporaryFile(suffix='.parquet') as tmp:\n                pq.write_table(table, 'tmp.parquet')\n                with open('tmp.parquet', 'rb') as source_file:\n                    job = client.load_table_from_file(source_file, table_bq, job_config=job_config)\n                job.result()\n        progressbar(1.0)"
        ]
    }
]