[
    {
        "func_name": "get_style_embedding",
        "original": "def get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors='pt')\n    input_ids = prompt['input_ids']\n    token_type_ids = prompt['token_type_ids']\n    attention_mask = prompt['attention_mask']\n    with torch.no_grad():\n        output = style_encoder(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n    style_embedding = output['pooled_output'].cpu().squeeze().numpy()\n    return style_embedding",
        "mutated": [
            "def get_style_embedding(prompt, tokenizer, style_encoder):\n    if False:\n        i = 10\n    prompt = tokenizer([prompt], return_tensors='pt')\n    input_ids = prompt['input_ids']\n    token_type_ids = prompt['token_type_ids']\n    attention_mask = prompt['attention_mask']\n    with torch.no_grad():\n        output = style_encoder(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n    style_embedding = output['pooled_output'].cpu().squeeze().numpy()\n    return style_embedding",
            "def get_style_embedding(prompt, tokenizer, style_encoder):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    prompt = tokenizer([prompt], return_tensors='pt')\n    input_ids = prompt['input_ids']\n    token_type_ids = prompt['token_type_ids']\n    attention_mask = prompt['attention_mask']\n    with torch.no_grad():\n        output = style_encoder(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n    style_embedding = output['pooled_output'].cpu().squeeze().numpy()\n    return style_embedding",
            "def get_style_embedding(prompt, tokenizer, style_encoder):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    prompt = tokenizer([prompt], return_tensors='pt')\n    input_ids = prompt['input_ids']\n    token_type_ids = prompt['token_type_ids']\n    attention_mask = prompt['attention_mask']\n    with torch.no_grad():\n        output = style_encoder(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n    style_embedding = output['pooled_output'].cpu().squeeze().numpy()\n    return style_embedding",
            "def get_style_embedding(prompt, tokenizer, style_encoder):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    prompt = tokenizer([prompt], return_tensors='pt')\n    input_ids = prompt['input_ids']\n    token_type_ids = prompt['token_type_ids']\n    attention_mask = prompt['attention_mask']\n    with torch.no_grad():\n        output = style_encoder(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n    style_embedding = output['pooled_output'].cpu().squeeze().numpy()\n    return style_embedding",
            "def get_style_embedding(prompt, tokenizer, style_encoder):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    prompt = tokenizer([prompt], return_tensors='pt')\n    input_ids = prompt['input_ids']\n    token_type_ids = prompt['token_type_ids']\n    attention_mask = prompt['attention_mask']\n    with torch.no_grad():\n        output = style_encoder(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n    style_embedding = output['pooled_output'].cpu().squeeze().numpy()\n    return style_embedding"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(args, config):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    root_path = os.path.join(config.output_directory, args.logdir)\n    ckpt_path = os.path.join(root_path, 'ckpt')\n    files = os.listdir(ckpt_path)\n    for file in files:\n        if args.checkpoint:\n            if file != args.checkpoint:\n                continue\n        checkpoint_path = os.path.join(ckpt_path, file)\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(config.style_encoder_ckpt, map_location='cpu')\n        model_ckpt = {}\n        for (key, value) in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt)\n        generator = JETSGenerator(conf).to(device)\n        model_CKPT = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n        with open(config.token_list_path, 'r') as f:\n            token2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            speaker2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n        text_path = args.test_file\n        if os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n            r = glob.glob(root_path + '/test_audio/audio/' + f'{file}/*')\n            for j in r:\n                os.remove(j)\n        texts = []\n        prompts = []\n        speakers = []\n        contents = []\n        with open(text_path, 'r') as f:\n            for line in f:\n                line = line.strip().split('|')\n                speakers.append(line[0])\n                prompts.append(line[1])\n                texts.append(line[2].split())\n                contents.append(line[3])\n        for (i, (speaker, prompt, text, content)) in enumerate(tqdm(zip(speakers, prompts, texts, contents))):\n            style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n            content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n            if speaker not in speaker2id:\n                continue\n            speaker = speaker2id[speaker]\n            text_int = [token2id[ph] for ph in text]\n            sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n            sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n            style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n            content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n            speaker = torch.from_numpy(np.array([speaker])).to(device)\n            with torch.no_grad():\n                infer_output = generator(inputs_ling=sequence, inputs_style_embedding=style_embedding, input_lengths=sequence_len, inputs_content_embedding=content_embedding, inputs_speaker=speaker, alpha=1.0)\n                audio = infer_output['wav_predictions'].squeeze() * MAX_WAV_VALUE\n                audio = audio.cpu().numpy().astype('int16')\n                if not os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n                    os.makedirs(root_path + '/test_audio/audio/' + f'{file}/', exist_ok=True)\n                sf.write(file=root_path + '/test_audio/audio/' + f'{file}/{i + 1}.wav', data=audio, samplerate=config.sampling_rate)",
        "mutated": [
            "def main(args, config):\n    if False:\n        i = 10\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    root_path = os.path.join(config.output_directory, args.logdir)\n    ckpt_path = os.path.join(root_path, 'ckpt')\n    files = os.listdir(ckpt_path)\n    for file in files:\n        if args.checkpoint:\n            if file != args.checkpoint:\n                continue\n        checkpoint_path = os.path.join(ckpt_path, file)\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(config.style_encoder_ckpt, map_location='cpu')\n        model_ckpt = {}\n        for (key, value) in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt)\n        generator = JETSGenerator(conf).to(device)\n        model_CKPT = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n        with open(config.token_list_path, 'r') as f:\n            token2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            speaker2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n        text_path = args.test_file\n        if os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n            r = glob.glob(root_path + '/test_audio/audio/' + f'{file}/*')\n            for j in r:\n                os.remove(j)\n        texts = []\n        prompts = []\n        speakers = []\n        contents = []\n        with open(text_path, 'r') as f:\n            for line in f:\n                line = line.strip().split('|')\n                speakers.append(line[0])\n                prompts.append(line[1])\n                texts.append(line[2].split())\n                contents.append(line[3])\n        for (i, (speaker, prompt, text, content)) in enumerate(tqdm(zip(speakers, prompts, texts, contents))):\n            style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n            content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n            if speaker not in speaker2id:\n                continue\n            speaker = speaker2id[speaker]\n            text_int = [token2id[ph] for ph in text]\n            sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n            sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n            style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n            content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n            speaker = torch.from_numpy(np.array([speaker])).to(device)\n            with torch.no_grad():\n                infer_output = generator(inputs_ling=sequence, inputs_style_embedding=style_embedding, input_lengths=sequence_len, inputs_content_embedding=content_embedding, inputs_speaker=speaker, alpha=1.0)\n                audio = infer_output['wav_predictions'].squeeze() * MAX_WAV_VALUE\n                audio = audio.cpu().numpy().astype('int16')\n                if not os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n                    os.makedirs(root_path + '/test_audio/audio/' + f'{file}/', exist_ok=True)\n                sf.write(file=root_path + '/test_audio/audio/' + f'{file}/{i + 1}.wav', data=audio, samplerate=config.sampling_rate)",
            "def main(args, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    root_path = os.path.join(config.output_directory, args.logdir)\n    ckpt_path = os.path.join(root_path, 'ckpt')\n    files = os.listdir(ckpt_path)\n    for file in files:\n        if args.checkpoint:\n            if file != args.checkpoint:\n                continue\n        checkpoint_path = os.path.join(ckpt_path, file)\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(config.style_encoder_ckpt, map_location='cpu')\n        model_ckpt = {}\n        for (key, value) in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt)\n        generator = JETSGenerator(conf).to(device)\n        model_CKPT = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n        with open(config.token_list_path, 'r') as f:\n            token2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            speaker2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n        text_path = args.test_file\n        if os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n            r = glob.glob(root_path + '/test_audio/audio/' + f'{file}/*')\n            for j in r:\n                os.remove(j)\n        texts = []\n        prompts = []\n        speakers = []\n        contents = []\n        with open(text_path, 'r') as f:\n            for line in f:\n                line = line.strip().split('|')\n                speakers.append(line[0])\n                prompts.append(line[1])\n                texts.append(line[2].split())\n                contents.append(line[3])\n        for (i, (speaker, prompt, text, content)) in enumerate(tqdm(zip(speakers, prompts, texts, contents))):\n            style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n            content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n            if speaker not in speaker2id:\n                continue\n            speaker = speaker2id[speaker]\n            text_int = [token2id[ph] for ph in text]\n            sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n            sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n            style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n            content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n            speaker = torch.from_numpy(np.array([speaker])).to(device)\n            with torch.no_grad():\n                infer_output = generator(inputs_ling=sequence, inputs_style_embedding=style_embedding, input_lengths=sequence_len, inputs_content_embedding=content_embedding, inputs_speaker=speaker, alpha=1.0)\n                audio = infer_output['wav_predictions'].squeeze() * MAX_WAV_VALUE\n                audio = audio.cpu().numpy().astype('int16')\n                if not os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n                    os.makedirs(root_path + '/test_audio/audio/' + f'{file}/', exist_ok=True)\n                sf.write(file=root_path + '/test_audio/audio/' + f'{file}/{i + 1}.wav', data=audio, samplerate=config.sampling_rate)",
            "def main(args, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    root_path = os.path.join(config.output_directory, args.logdir)\n    ckpt_path = os.path.join(root_path, 'ckpt')\n    files = os.listdir(ckpt_path)\n    for file in files:\n        if args.checkpoint:\n            if file != args.checkpoint:\n                continue\n        checkpoint_path = os.path.join(ckpt_path, file)\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(config.style_encoder_ckpt, map_location='cpu')\n        model_ckpt = {}\n        for (key, value) in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt)\n        generator = JETSGenerator(conf).to(device)\n        model_CKPT = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n        with open(config.token_list_path, 'r') as f:\n            token2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            speaker2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n        text_path = args.test_file\n        if os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n            r = glob.glob(root_path + '/test_audio/audio/' + f'{file}/*')\n            for j in r:\n                os.remove(j)\n        texts = []\n        prompts = []\n        speakers = []\n        contents = []\n        with open(text_path, 'r') as f:\n            for line in f:\n                line = line.strip().split('|')\n                speakers.append(line[0])\n                prompts.append(line[1])\n                texts.append(line[2].split())\n                contents.append(line[3])\n        for (i, (speaker, prompt, text, content)) in enumerate(tqdm(zip(speakers, prompts, texts, contents))):\n            style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n            content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n            if speaker not in speaker2id:\n                continue\n            speaker = speaker2id[speaker]\n            text_int = [token2id[ph] for ph in text]\n            sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n            sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n            style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n            content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n            speaker = torch.from_numpy(np.array([speaker])).to(device)\n            with torch.no_grad():\n                infer_output = generator(inputs_ling=sequence, inputs_style_embedding=style_embedding, input_lengths=sequence_len, inputs_content_embedding=content_embedding, inputs_speaker=speaker, alpha=1.0)\n                audio = infer_output['wav_predictions'].squeeze() * MAX_WAV_VALUE\n                audio = audio.cpu().numpy().astype('int16')\n                if not os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n                    os.makedirs(root_path + '/test_audio/audio/' + f'{file}/', exist_ok=True)\n                sf.write(file=root_path + '/test_audio/audio/' + f'{file}/{i + 1}.wav', data=audio, samplerate=config.sampling_rate)",
            "def main(args, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    root_path = os.path.join(config.output_directory, args.logdir)\n    ckpt_path = os.path.join(root_path, 'ckpt')\n    files = os.listdir(ckpt_path)\n    for file in files:\n        if args.checkpoint:\n            if file != args.checkpoint:\n                continue\n        checkpoint_path = os.path.join(ckpt_path, file)\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(config.style_encoder_ckpt, map_location='cpu')\n        model_ckpt = {}\n        for (key, value) in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt)\n        generator = JETSGenerator(conf).to(device)\n        model_CKPT = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n        with open(config.token_list_path, 'r') as f:\n            token2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            speaker2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n        text_path = args.test_file\n        if os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n            r = glob.glob(root_path + '/test_audio/audio/' + f'{file}/*')\n            for j in r:\n                os.remove(j)\n        texts = []\n        prompts = []\n        speakers = []\n        contents = []\n        with open(text_path, 'r') as f:\n            for line in f:\n                line = line.strip().split('|')\n                speakers.append(line[0])\n                prompts.append(line[1])\n                texts.append(line[2].split())\n                contents.append(line[3])\n        for (i, (speaker, prompt, text, content)) in enumerate(tqdm(zip(speakers, prompts, texts, contents))):\n            style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n            content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n            if speaker not in speaker2id:\n                continue\n            speaker = speaker2id[speaker]\n            text_int = [token2id[ph] for ph in text]\n            sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n            sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n            style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n            content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n            speaker = torch.from_numpy(np.array([speaker])).to(device)\n            with torch.no_grad():\n                infer_output = generator(inputs_ling=sequence, inputs_style_embedding=style_embedding, input_lengths=sequence_len, inputs_content_embedding=content_embedding, inputs_speaker=speaker, alpha=1.0)\n                audio = infer_output['wav_predictions'].squeeze() * MAX_WAV_VALUE\n                audio = audio.cpu().numpy().astype('int16')\n                if not os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n                    os.makedirs(root_path + '/test_audio/audio/' + f'{file}/', exist_ok=True)\n                sf.write(file=root_path + '/test_audio/audio/' + f'{file}/{i + 1}.wav', data=audio, samplerate=config.sampling_rate)",
            "def main(args, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    root_path = os.path.join(config.output_directory, args.logdir)\n    ckpt_path = os.path.join(root_path, 'ckpt')\n    files = os.listdir(ckpt_path)\n    for file in files:\n        if args.checkpoint:\n            if file != args.checkpoint:\n                continue\n        checkpoint_path = os.path.join(ckpt_path, file)\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(config.style_encoder_ckpt, map_location='cpu')\n        model_ckpt = {}\n        for (key, value) in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt)\n        generator = JETSGenerator(conf).to(device)\n        model_CKPT = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n        with open(config.token_list_path, 'r') as f:\n            token2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            speaker2id = {t.strip(): idx for (idx, t) in enumerate(f.readlines())}\n        tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n        text_path = args.test_file\n        if os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n            r = glob.glob(root_path + '/test_audio/audio/' + f'{file}/*')\n            for j in r:\n                os.remove(j)\n        texts = []\n        prompts = []\n        speakers = []\n        contents = []\n        with open(text_path, 'r') as f:\n            for line in f:\n                line = line.strip().split('|')\n                speakers.append(line[0])\n                prompts.append(line[1])\n                texts.append(line[2].split())\n                contents.append(line[3])\n        for (i, (speaker, prompt, text, content)) in enumerate(tqdm(zip(speakers, prompts, texts, contents))):\n            style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n            content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n            if speaker not in speaker2id:\n                continue\n            speaker = speaker2id[speaker]\n            text_int = [token2id[ph] for ph in text]\n            sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n            sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n            style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n            content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n            speaker = torch.from_numpy(np.array([speaker])).to(device)\n            with torch.no_grad():\n                infer_output = generator(inputs_ling=sequence, inputs_style_embedding=style_embedding, input_lengths=sequence_len, inputs_content_embedding=content_embedding, inputs_speaker=speaker, alpha=1.0)\n                audio = infer_output['wav_predictions'].squeeze() * MAX_WAV_VALUE\n                audio = audio.cpu().numpy().astype('int16')\n                if not os.path.exists(root_path + '/test_audio/audio/' + f'{file}/'):\n                    os.makedirs(root_path + '/test_audio/audio/' + f'{file}/', exist_ok=True)\n                sf.write(file=root_path + '/test_audio/audio/' + f'{file}/{i + 1}.wav', data=audio, samplerate=config.sampling_rate)"
        ]
    }
]