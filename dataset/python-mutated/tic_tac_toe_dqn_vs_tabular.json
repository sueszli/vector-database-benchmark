[
    {
        "func_name": "pretty_board",
        "original": "def pretty_board(time_step):\n    \"\"\"Returns the board in `time_step` in a human readable format.\"\"\"\n    info_state = time_step.observations['info_state'][0]\n    x_locations = np.nonzero(info_state[9:18])[0]\n    o_locations = np.nonzero(info_state[18:])[0]\n    board = np.full(3 * 3, '.')\n    board[x_locations] = 'X'\n    board[o_locations] = '0'\n    board = np.reshape(board, (3, 3))\n    return board",
        "mutated": [
            "def pretty_board(time_step):\n    if False:\n        i = 10\n    'Returns the board in `time_step` in a human readable format.'\n    info_state = time_step.observations['info_state'][0]\n    x_locations = np.nonzero(info_state[9:18])[0]\n    o_locations = np.nonzero(info_state[18:])[0]\n    board = np.full(3 * 3, '.')\n    board[x_locations] = 'X'\n    board[o_locations] = '0'\n    board = np.reshape(board, (3, 3))\n    return board",
            "def pretty_board(time_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the board in `time_step` in a human readable format.'\n    info_state = time_step.observations['info_state'][0]\n    x_locations = np.nonzero(info_state[9:18])[0]\n    o_locations = np.nonzero(info_state[18:])[0]\n    board = np.full(3 * 3, '.')\n    board[x_locations] = 'X'\n    board[o_locations] = '0'\n    board = np.reshape(board, (3, 3))\n    return board",
            "def pretty_board(time_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the board in `time_step` in a human readable format.'\n    info_state = time_step.observations['info_state'][0]\n    x_locations = np.nonzero(info_state[9:18])[0]\n    o_locations = np.nonzero(info_state[18:])[0]\n    board = np.full(3 * 3, '.')\n    board[x_locations] = 'X'\n    board[o_locations] = '0'\n    board = np.reshape(board, (3, 3))\n    return board",
            "def pretty_board(time_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the board in `time_step` in a human readable format.'\n    info_state = time_step.observations['info_state'][0]\n    x_locations = np.nonzero(info_state[9:18])[0]\n    o_locations = np.nonzero(info_state[18:])[0]\n    board = np.full(3 * 3, '.')\n    board[x_locations] = 'X'\n    board[o_locations] = '0'\n    board = np.reshape(board, (3, 3))\n    return board",
            "def pretty_board(time_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the board in `time_step` in a human readable format.'\n    info_state = time_step.observations['info_state'][0]\n    x_locations = np.nonzero(info_state[9:18])[0]\n    o_locations = np.nonzero(info_state[18:])[0]\n    board = np.full(3 * 3, '.')\n    board[x_locations] = 'X'\n    board[o_locations] = '0'\n    board = np.reshape(board, (3, 3))\n    return board"
        ]
    },
    {
        "func_name": "command_line_action",
        "original": "def command_line_action(time_step):\n    \"\"\"Gets a valid action from the user on the command line.\"\"\"\n    current_player = time_step.observations['current_player']\n    legal_actions = time_step.observations['legal_actions'][current_player]\n    action = -1\n    while action not in legal_actions:\n        print('Choose an action from {}:'.format(legal_actions))\n        sys.stdout.flush()\n        action_str = input()\n        try:\n            action = int(action_str)\n        except ValueError:\n            continue\n    return action",
        "mutated": [
            "def command_line_action(time_step):\n    if False:\n        i = 10\n    'Gets a valid action from the user on the command line.'\n    current_player = time_step.observations['current_player']\n    legal_actions = time_step.observations['legal_actions'][current_player]\n    action = -1\n    while action not in legal_actions:\n        print('Choose an action from {}:'.format(legal_actions))\n        sys.stdout.flush()\n        action_str = input()\n        try:\n            action = int(action_str)\n        except ValueError:\n            continue\n    return action",
            "def command_line_action(time_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Gets a valid action from the user on the command line.'\n    current_player = time_step.observations['current_player']\n    legal_actions = time_step.observations['legal_actions'][current_player]\n    action = -1\n    while action not in legal_actions:\n        print('Choose an action from {}:'.format(legal_actions))\n        sys.stdout.flush()\n        action_str = input()\n        try:\n            action = int(action_str)\n        except ValueError:\n            continue\n    return action",
            "def command_line_action(time_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Gets a valid action from the user on the command line.'\n    current_player = time_step.observations['current_player']\n    legal_actions = time_step.observations['legal_actions'][current_player]\n    action = -1\n    while action not in legal_actions:\n        print('Choose an action from {}:'.format(legal_actions))\n        sys.stdout.flush()\n        action_str = input()\n        try:\n            action = int(action_str)\n        except ValueError:\n            continue\n    return action",
            "def command_line_action(time_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Gets a valid action from the user on the command line.'\n    current_player = time_step.observations['current_player']\n    legal_actions = time_step.observations['legal_actions'][current_player]\n    action = -1\n    while action not in legal_actions:\n        print('Choose an action from {}:'.format(legal_actions))\n        sys.stdout.flush()\n        action_str = input()\n        try:\n            action = int(action_str)\n        except ValueError:\n            continue\n    return action",
            "def command_line_action(time_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Gets a valid action from the user on the command line.'\n    current_player = time_step.observations['current_player']\n    legal_actions = time_step.observations['legal_actions'][current_player]\n    action = -1\n    while action not in legal_actions:\n        print('Choose an action from {}:'.format(legal_actions))\n        sys.stdout.flush()\n        action_str = input()\n        try:\n            action = int(action_str)\n        except ValueError:\n            continue\n    return action"
        ]
    },
    {
        "func_name": "eval_against_random_bots",
        "original": "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n    \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n    num_players = len(trained_agents)\n    sum_episode_rewards = np.zeros(num_players)\n    for player_pos in range(num_players):\n        cur_agents = random_agents[:]\n        cur_agents[player_pos] = trained_agents[player_pos]\n        for _ in range(num_episodes):\n            time_step = env.reset()\n            episode_rewards = 0\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n                episode_rewards += time_step.rewards[player_pos]\n            sum_episode_rewards[player_pos] += episode_rewards\n    return sum_episode_rewards / num_episodes",
        "mutated": [
            "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n    if False:\n        i = 10\n    'Evaluates `trained_agents` against `random_agents` for `num_episodes`.'\n    num_players = len(trained_agents)\n    sum_episode_rewards = np.zeros(num_players)\n    for player_pos in range(num_players):\n        cur_agents = random_agents[:]\n        cur_agents[player_pos] = trained_agents[player_pos]\n        for _ in range(num_episodes):\n            time_step = env.reset()\n            episode_rewards = 0\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n                episode_rewards += time_step.rewards[player_pos]\n            sum_episode_rewards[player_pos] += episode_rewards\n    return sum_episode_rewards / num_episodes",
            "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Evaluates `trained_agents` against `random_agents` for `num_episodes`.'\n    num_players = len(trained_agents)\n    sum_episode_rewards = np.zeros(num_players)\n    for player_pos in range(num_players):\n        cur_agents = random_agents[:]\n        cur_agents[player_pos] = trained_agents[player_pos]\n        for _ in range(num_episodes):\n            time_step = env.reset()\n            episode_rewards = 0\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n                episode_rewards += time_step.rewards[player_pos]\n            sum_episode_rewards[player_pos] += episode_rewards\n    return sum_episode_rewards / num_episodes",
            "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Evaluates `trained_agents` against `random_agents` for `num_episodes`.'\n    num_players = len(trained_agents)\n    sum_episode_rewards = np.zeros(num_players)\n    for player_pos in range(num_players):\n        cur_agents = random_agents[:]\n        cur_agents[player_pos] = trained_agents[player_pos]\n        for _ in range(num_episodes):\n            time_step = env.reset()\n            episode_rewards = 0\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n                episode_rewards += time_step.rewards[player_pos]\n            sum_episode_rewards[player_pos] += episode_rewards\n    return sum_episode_rewards / num_episodes",
            "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Evaluates `trained_agents` against `random_agents` for `num_episodes`.'\n    num_players = len(trained_agents)\n    sum_episode_rewards = np.zeros(num_players)\n    for player_pos in range(num_players):\n        cur_agents = random_agents[:]\n        cur_agents[player_pos] = trained_agents[player_pos]\n        for _ in range(num_episodes):\n            time_step = env.reset()\n            episode_rewards = 0\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n                episode_rewards += time_step.rewards[player_pos]\n            sum_episode_rewards[player_pos] += episode_rewards\n    return sum_episode_rewards / num_episodes",
            "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Evaluates `trained_agents` against `random_agents` for `num_episodes`.'\n    num_players = len(trained_agents)\n    sum_episode_rewards = np.zeros(num_players)\n    for player_pos in range(num_players):\n        cur_agents = random_agents[:]\n        cur_agents[player_pos] = trained_agents[player_pos]\n        for _ in range(num_episodes):\n            time_step = env.reset()\n            episode_rewards = 0\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n                episode_rewards += time_step.rewards[player_pos]\n            sum_episode_rewards[player_pos] += episode_rewards\n    return sum_episode_rewards / num_episodes"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(_):\n    game = 'tic_tac_toe'\n    num_players = 2\n    env = rl_environment.Environment(game)\n    state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [32, 32]\n    replay_buffer_capacity = int(10000.0)\n    train_episodes = FLAGS.num_episodes\n    loss_report_interval = 1000\n    with tf.Session() as sess:\n        dqn_agent = dqn.DQN(sess, player_id=0, state_representation_size=state_size, num_actions=num_actions, hidden_layers_sizes=hidden_layers_sizes, replay_buffer_capacity=replay_buffer_capacity)\n        tabular_q_agent = tabular_qlearner.QLearner(player_id=1, num_actions=num_actions)\n        agents = [dqn_agent, tabular_q_agent]\n        sess.run(tf.global_variables_initializer())\n        for ep in range(train_episodes):\n            if ep and ep % loss_report_interval == 0:\n                logging.info('[%s/%s] DQN loss: %s', ep, train_episodes, agents[0].loss)\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)\n        random_agents = [random_agent.RandomAgent(player_id=idx, num_actions=num_actions) for idx in range(num_players)]\n        r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n        logging.info('Mean episode rewards: %s', r_mean)\n        if not FLAGS.interactive_play:\n            return\n        human_player = 1\n        while True:\n            logging.info('You are playing as %s', 'X' if human_player else '0')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                if player_id == human_player:\n                    agent_out = agents[human_player].step(time_step, is_evaluation=True)\n                    logging.info('\\n%s', agent_out.probs.reshape((3, 3)))\n                    logging.info('\\n%s', pretty_board(time_step))\n                    action = command_line_action(time_step)\n                else:\n                    agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n                    action = agent_out.action\n                time_step = env.step([action])\n            logging.info('\\n%s', pretty_board(time_step))\n            logging.info('End of game!')\n            if time_step.rewards[human_player] > 0:\n                logging.info('You win')\n            elif time_step.rewards[human_player] < 0:\n                logging.info('You lose')\n            else:\n                logging.info('Draw')\n            human_player = 1 - human_player",
        "mutated": [
            "def main(_):\n    if False:\n        i = 10\n    game = 'tic_tac_toe'\n    num_players = 2\n    env = rl_environment.Environment(game)\n    state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [32, 32]\n    replay_buffer_capacity = int(10000.0)\n    train_episodes = FLAGS.num_episodes\n    loss_report_interval = 1000\n    with tf.Session() as sess:\n        dqn_agent = dqn.DQN(sess, player_id=0, state_representation_size=state_size, num_actions=num_actions, hidden_layers_sizes=hidden_layers_sizes, replay_buffer_capacity=replay_buffer_capacity)\n        tabular_q_agent = tabular_qlearner.QLearner(player_id=1, num_actions=num_actions)\n        agents = [dqn_agent, tabular_q_agent]\n        sess.run(tf.global_variables_initializer())\n        for ep in range(train_episodes):\n            if ep and ep % loss_report_interval == 0:\n                logging.info('[%s/%s] DQN loss: %s', ep, train_episodes, agents[0].loss)\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)\n        random_agents = [random_agent.RandomAgent(player_id=idx, num_actions=num_actions) for idx in range(num_players)]\n        r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n        logging.info('Mean episode rewards: %s', r_mean)\n        if not FLAGS.interactive_play:\n            return\n        human_player = 1\n        while True:\n            logging.info('You are playing as %s', 'X' if human_player else '0')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                if player_id == human_player:\n                    agent_out = agents[human_player].step(time_step, is_evaluation=True)\n                    logging.info('\\n%s', agent_out.probs.reshape((3, 3)))\n                    logging.info('\\n%s', pretty_board(time_step))\n                    action = command_line_action(time_step)\n                else:\n                    agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n                    action = agent_out.action\n                time_step = env.step([action])\n            logging.info('\\n%s', pretty_board(time_step))\n            logging.info('End of game!')\n            if time_step.rewards[human_player] > 0:\n                logging.info('You win')\n            elif time_step.rewards[human_player] < 0:\n                logging.info('You lose')\n            else:\n                logging.info('Draw')\n            human_player = 1 - human_player",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    game = 'tic_tac_toe'\n    num_players = 2\n    env = rl_environment.Environment(game)\n    state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [32, 32]\n    replay_buffer_capacity = int(10000.0)\n    train_episodes = FLAGS.num_episodes\n    loss_report_interval = 1000\n    with tf.Session() as sess:\n        dqn_agent = dqn.DQN(sess, player_id=0, state_representation_size=state_size, num_actions=num_actions, hidden_layers_sizes=hidden_layers_sizes, replay_buffer_capacity=replay_buffer_capacity)\n        tabular_q_agent = tabular_qlearner.QLearner(player_id=1, num_actions=num_actions)\n        agents = [dqn_agent, tabular_q_agent]\n        sess.run(tf.global_variables_initializer())\n        for ep in range(train_episodes):\n            if ep and ep % loss_report_interval == 0:\n                logging.info('[%s/%s] DQN loss: %s', ep, train_episodes, agents[0].loss)\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)\n        random_agents = [random_agent.RandomAgent(player_id=idx, num_actions=num_actions) for idx in range(num_players)]\n        r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n        logging.info('Mean episode rewards: %s', r_mean)\n        if not FLAGS.interactive_play:\n            return\n        human_player = 1\n        while True:\n            logging.info('You are playing as %s', 'X' if human_player else '0')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                if player_id == human_player:\n                    agent_out = agents[human_player].step(time_step, is_evaluation=True)\n                    logging.info('\\n%s', agent_out.probs.reshape((3, 3)))\n                    logging.info('\\n%s', pretty_board(time_step))\n                    action = command_line_action(time_step)\n                else:\n                    agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n                    action = agent_out.action\n                time_step = env.step([action])\n            logging.info('\\n%s', pretty_board(time_step))\n            logging.info('End of game!')\n            if time_step.rewards[human_player] > 0:\n                logging.info('You win')\n            elif time_step.rewards[human_player] < 0:\n                logging.info('You lose')\n            else:\n                logging.info('Draw')\n            human_player = 1 - human_player",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    game = 'tic_tac_toe'\n    num_players = 2\n    env = rl_environment.Environment(game)\n    state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [32, 32]\n    replay_buffer_capacity = int(10000.0)\n    train_episodes = FLAGS.num_episodes\n    loss_report_interval = 1000\n    with tf.Session() as sess:\n        dqn_agent = dqn.DQN(sess, player_id=0, state_representation_size=state_size, num_actions=num_actions, hidden_layers_sizes=hidden_layers_sizes, replay_buffer_capacity=replay_buffer_capacity)\n        tabular_q_agent = tabular_qlearner.QLearner(player_id=1, num_actions=num_actions)\n        agents = [dqn_agent, tabular_q_agent]\n        sess.run(tf.global_variables_initializer())\n        for ep in range(train_episodes):\n            if ep and ep % loss_report_interval == 0:\n                logging.info('[%s/%s] DQN loss: %s', ep, train_episodes, agents[0].loss)\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)\n        random_agents = [random_agent.RandomAgent(player_id=idx, num_actions=num_actions) for idx in range(num_players)]\n        r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n        logging.info('Mean episode rewards: %s', r_mean)\n        if not FLAGS.interactive_play:\n            return\n        human_player = 1\n        while True:\n            logging.info('You are playing as %s', 'X' if human_player else '0')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                if player_id == human_player:\n                    agent_out = agents[human_player].step(time_step, is_evaluation=True)\n                    logging.info('\\n%s', agent_out.probs.reshape((3, 3)))\n                    logging.info('\\n%s', pretty_board(time_step))\n                    action = command_line_action(time_step)\n                else:\n                    agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n                    action = agent_out.action\n                time_step = env.step([action])\n            logging.info('\\n%s', pretty_board(time_step))\n            logging.info('End of game!')\n            if time_step.rewards[human_player] > 0:\n                logging.info('You win')\n            elif time_step.rewards[human_player] < 0:\n                logging.info('You lose')\n            else:\n                logging.info('Draw')\n            human_player = 1 - human_player",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    game = 'tic_tac_toe'\n    num_players = 2\n    env = rl_environment.Environment(game)\n    state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [32, 32]\n    replay_buffer_capacity = int(10000.0)\n    train_episodes = FLAGS.num_episodes\n    loss_report_interval = 1000\n    with tf.Session() as sess:\n        dqn_agent = dqn.DQN(sess, player_id=0, state_representation_size=state_size, num_actions=num_actions, hidden_layers_sizes=hidden_layers_sizes, replay_buffer_capacity=replay_buffer_capacity)\n        tabular_q_agent = tabular_qlearner.QLearner(player_id=1, num_actions=num_actions)\n        agents = [dqn_agent, tabular_q_agent]\n        sess.run(tf.global_variables_initializer())\n        for ep in range(train_episodes):\n            if ep and ep % loss_report_interval == 0:\n                logging.info('[%s/%s] DQN loss: %s', ep, train_episodes, agents[0].loss)\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)\n        random_agents = [random_agent.RandomAgent(player_id=idx, num_actions=num_actions) for idx in range(num_players)]\n        r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n        logging.info('Mean episode rewards: %s', r_mean)\n        if not FLAGS.interactive_play:\n            return\n        human_player = 1\n        while True:\n            logging.info('You are playing as %s', 'X' if human_player else '0')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                if player_id == human_player:\n                    agent_out = agents[human_player].step(time_step, is_evaluation=True)\n                    logging.info('\\n%s', agent_out.probs.reshape((3, 3)))\n                    logging.info('\\n%s', pretty_board(time_step))\n                    action = command_line_action(time_step)\n                else:\n                    agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n                    action = agent_out.action\n                time_step = env.step([action])\n            logging.info('\\n%s', pretty_board(time_step))\n            logging.info('End of game!')\n            if time_step.rewards[human_player] > 0:\n                logging.info('You win')\n            elif time_step.rewards[human_player] < 0:\n                logging.info('You lose')\n            else:\n                logging.info('Draw')\n            human_player = 1 - human_player",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    game = 'tic_tac_toe'\n    num_players = 2\n    env = rl_environment.Environment(game)\n    state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [32, 32]\n    replay_buffer_capacity = int(10000.0)\n    train_episodes = FLAGS.num_episodes\n    loss_report_interval = 1000\n    with tf.Session() as sess:\n        dqn_agent = dqn.DQN(sess, player_id=0, state_representation_size=state_size, num_actions=num_actions, hidden_layers_sizes=hidden_layers_sizes, replay_buffer_capacity=replay_buffer_capacity)\n        tabular_q_agent = tabular_qlearner.QLearner(player_id=1, num_actions=num_actions)\n        agents = [dqn_agent, tabular_q_agent]\n        sess.run(tf.global_variables_initializer())\n        for ep in range(train_episodes):\n            if ep and ep % loss_report_interval == 0:\n                logging.info('[%s/%s] DQN loss: %s', ep, train_episodes, agents[0].loss)\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)\n        random_agents = [random_agent.RandomAgent(player_id=idx, num_actions=num_actions) for idx in range(num_players)]\n        r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n        logging.info('Mean episode rewards: %s', r_mean)\n        if not FLAGS.interactive_play:\n            return\n        human_player = 1\n        while True:\n            logging.info('You are playing as %s', 'X' if human_player else '0')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                if player_id == human_player:\n                    agent_out = agents[human_player].step(time_step, is_evaluation=True)\n                    logging.info('\\n%s', agent_out.probs.reshape((3, 3)))\n                    logging.info('\\n%s', pretty_board(time_step))\n                    action = command_line_action(time_step)\n                else:\n                    agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n                    action = agent_out.action\n                time_step = env.step([action])\n            logging.info('\\n%s', pretty_board(time_step))\n            logging.info('End of game!')\n            if time_step.rewards[human_player] > 0:\n                logging.info('You win')\n            elif time_step.rewards[human_player] < 0:\n                logging.info('You lose')\n            else:\n                logging.info('Draw')\n            human_player = 1 - human_player"
        ]
    }
]