[
    {
        "func_name": "__init__",
        "original": "def __init__(self, seed=0):\n    self.experiment_seed = seed\n    self.random = np.random.RandomState(self.experiment_seed)\n    self.results = None",
        "mutated": [
            "def __init__(self, seed=0):\n    if False:\n        i = 10\n    self.experiment_seed = seed\n    self.random = np.random.RandomState(self.experiment_seed)\n    self.results = None",
            "def __init__(self, seed=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.experiment_seed = seed\n    self.random = np.random.RandomState(self.experiment_seed)\n    self.results = None",
            "def __init__(self, seed=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.experiment_seed = seed\n    self.random = np.random.RandomState(self.experiment_seed)\n    self.results = None",
            "def __init__(self, seed=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.experiment_seed = seed\n    self.random = np.random.RandomState(self.experiment_seed)\n    self.results = None",
            "def __init__(self, seed=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.experiment_seed = seed\n    self.random = np.random.RandomState(self.experiment_seed)\n    self.results = None"
        ]
    },
    {
        "func_name": "estimate_exploitability_sym",
        "original": "def estimate_exploitability_sym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    \"\"\"Estimate exploitability via monte carlo.\n\n    Args:\n      dist: 1-d np.array, estimate of nash distribution\n      num_eval_samples: int, number of samples to estimate exploitability\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\n      num_players: int, number of players\n      game: game with minimal functionality (see games/small.py)\n      policies: list mapping checkpoints to policies\n    Returns:\n      list of exploitabilities computed using [index] monte carlo samples\n    \"\"\"\n    pg_mean = np.zeros_like(dist)\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        pg_s = np.zeros_like(dist)\n        for (query, payoffs) in game_results.items():\n            pg_s[query[0]] = payoffs[0]\n        pg_mean = (pg_mean * float(s) + pg_s) / float(s + 1)\n        exps_estimated.append(pg_mean.max() - pg_mean.dot(dist))\n    return exps_estimated",
        "mutated": [
            "def estimate_exploitability_sym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    if False:\n        i = 10\n    'Estimate exploitability via monte carlo.\\n\\n    Args:\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_eval_samples: int, number of samples to estimate exploitability\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n      num_players: int, number of players\\n      game: game with minimal functionality (see games/small.py)\\n      policies: list mapping checkpoints to policies\\n    Returns:\\n      list of exploitabilities computed using [index] monte carlo samples\\n    '\n    pg_mean = np.zeros_like(dist)\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        pg_s = np.zeros_like(dist)\n        for (query, payoffs) in game_results.items():\n            pg_s[query[0]] = payoffs[0]\n        pg_mean = (pg_mean * float(s) + pg_s) / float(s + 1)\n        exps_estimated.append(pg_mean.max() - pg_mean.dot(dist))\n    return exps_estimated",
            "def estimate_exploitability_sym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Estimate exploitability via monte carlo.\\n\\n    Args:\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_eval_samples: int, number of samples to estimate exploitability\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n      num_players: int, number of players\\n      game: game with minimal functionality (see games/small.py)\\n      policies: list mapping checkpoints to policies\\n    Returns:\\n      list of exploitabilities computed using [index] monte carlo samples\\n    '\n    pg_mean = np.zeros_like(dist)\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        pg_s = np.zeros_like(dist)\n        for (query, payoffs) in game_results.items():\n            pg_s[query[0]] = payoffs[0]\n        pg_mean = (pg_mean * float(s) + pg_s) / float(s + 1)\n        exps_estimated.append(pg_mean.max() - pg_mean.dot(dist))\n    return exps_estimated",
            "def estimate_exploitability_sym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Estimate exploitability via monte carlo.\\n\\n    Args:\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_eval_samples: int, number of samples to estimate exploitability\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n      num_players: int, number of players\\n      game: game with minimal functionality (see games/small.py)\\n      policies: list mapping checkpoints to policies\\n    Returns:\\n      list of exploitabilities computed using [index] monte carlo samples\\n    '\n    pg_mean = np.zeros_like(dist)\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        pg_s = np.zeros_like(dist)\n        for (query, payoffs) in game_results.items():\n            pg_s[query[0]] = payoffs[0]\n        pg_mean = (pg_mean * float(s) + pg_s) / float(s + 1)\n        exps_estimated.append(pg_mean.max() - pg_mean.dot(dist))\n    return exps_estimated",
            "def estimate_exploitability_sym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Estimate exploitability via monte carlo.\\n\\n    Args:\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_eval_samples: int, number of samples to estimate exploitability\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n      num_players: int, number of players\\n      game: game with minimal functionality (see games/small.py)\\n      policies: list mapping checkpoints to policies\\n    Returns:\\n      list of exploitabilities computed using [index] monte carlo samples\\n    '\n    pg_mean = np.zeros_like(dist)\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        pg_s = np.zeros_like(dist)\n        for (query, payoffs) in game_results.items():\n            pg_s[query[0]] = payoffs[0]\n        pg_mean = (pg_mean * float(s) + pg_s) / float(s + 1)\n        exps_estimated.append(pg_mean.max() - pg_mean.dot(dist))\n    return exps_estimated",
            "def estimate_exploitability_sym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Estimate exploitability via monte carlo.\\n\\n    Args:\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_eval_samples: int, number of samples to estimate exploitability\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n      num_players: int, number of players\\n      game: game with minimal functionality (see games/small.py)\\n      policies: list mapping checkpoints to policies\\n    Returns:\\n      list of exploitabilities computed using [index] monte carlo samples\\n    '\n    pg_mean = np.zeros_like(dist)\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        pg_s = np.zeros_like(dist)\n        for (query, payoffs) in game_results.items():\n            pg_s[query[0]] = payoffs[0]\n        pg_mean = (pg_mean * float(s) + pg_s) / float(s + 1)\n        exps_estimated.append(pg_mean.max() - pg_mean.dot(dist))\n    return exps_estimated"
        ]
    },
    {
        "func_name": "estimate_exploitability_nonsym",
        "original": "def estimate_exploitability_nonsym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    \"\"\"Estimate exploitability via monte carlo.\n\n    Args:\n      dist: list of 1-d np.arrays, estimate of nash distribution\n      num_eval_samples: int, number of samples to estimate exploitability\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\n      num_players: int, number of players\n      game: game with minimal functionality (see games/small.py)\n      policies: list mapping checkpoints to policies\n    Returns:\n      list of exploitabilities computed using [index] monte carlo samples\n    \"\"\"\n    pg_mean = [np.zeros_like(dist_i) for dist_i in dist]\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        for (pi_query, payoffs) in game_results.items():\n            (pi, query) = pi_query\n            ai = query[pi]\n            pg_mean[pi][ai] += (payoffs[pi] - pg_mean[pi][ai]) / float(s + 1)\n        exp_is = []\n        for i in range(num_players):\n            exp_is.append(pg_mean[i].max() - pg_mean[i].dot(dist[i]))\n        exps_estimated.append(np.mean(exp_is))\n    return exps_estimated",
        "mutated": [
            "def estimate_exploitability_nonsym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    if False:\n        i = 10\n    'Estimate exploitability via monte carlo.\\n\\n    Args:\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_eval_samples: int, number of samples to estimate exploitability\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n      num_players: int, number of players\\n      game: game with minimal functionality (see games/small.py)\\n      policies: list mapping checkpoints to policies\\n    Returns:\\n      list of exploitabilities computed using [index] monte carlo samples\\n    '\n    pg_mean = [np.zeros_like(dist_i) for dist_i in dist]\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        for (pi_query, payoffs) in game_results.items():\n            (pi, query) = pi_query\n            ai = query[pi]\n            pg_mean[pi][ai] += (payoffs[pi] - pg_mean[pi][ai]) / float(s + 1)\n        exp_is = []\n        for i in range(num_players):\n            exp_is.append(pg_mean[i].max() - pg_mean[i].dot(dist[i]))\n        exps_estimated.append(np.mean(exp_is))\n    return exps_estimated",
            "def estimate_exploitability_nonsym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Estimate exploitability via monte carlo.\\n\\n    Args:\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_eval_samples: int, number of samples to estimate exploitability\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n      num_players: int, number of players\\n      game: game with minimal functionality (see games/small.py)\\n      policies: list mapping checkpoints to policies\\n    Returns:\\n      list of exploitabilities computed using [index] monte carlo samples\\n    '\n    pg_mean = [np.zeros_like(dist_i) for dist_i in dist]\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        for (pi_query, payoffs) in game_results.items():\n            (pi, query) = pi_query\n            ai = query[pi]\n            pg_mean[pi][ai] += (payoffs[pi] - pg_mean[pi][ai]) / float(s + 1)\n        exp_is = []\n        for i in range(num_players):\n            exp_is.append(pg_mean[i].max() - pg_mean[i].dot(dist[i]))\n        exps_estimated.append(np.mean(exp_is))\n    return exps_estimated",
            "def estimate_exploitability_nonsym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Estimate exploitability via monte carlo.\\n\\n    Args:\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_eval_samples: int, number of samples to estimate exploitability\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n      num_players: int, number of players\\n      game: game with minimal functionality (see games/small.py)\\n      policies: list mapping checkpoints to policies\\n    Returns:\\n      list of exploitabilities computed using [index] monte carlo samples\\n    '\n    pg_mean = [np.zeros_like(dist_i) for dist_i in dist]\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        for (pi_query, payoffs) in game_results.items():\n            (pi, query) = pi_query\n            ai = query[pi]\n            pg_mean[pi][ai] += (payoffs[pi] - pg_mean[pi][ai]) / float(s + 1)\n        exp_is = []\n        for i in range(num_players):\n            exp_is.append(pg_mean[i].max() - pg_mean[i].dot(dist[i]))\n        exps_estimated.append(np.mean(exp_is))\n    return exps_estimated",
            "def estimate_exploitability_nonsym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Estimate exploitability via monte carlo.\\n\\n    Args:\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_eval_samples: int, number of samples to estimate exploitability\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n      num_players: int, number of players\\n      game: game with minimal functionality (see games/small.py)\\n      policies: list mapping checkpoints to policies\\n    Returns:\\n      list of exploitabilities computed using [index] monte carlo samples\\n    '\n    pg_mean = [np.zeros_like(dist_i) for dist_i in dist]\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        for (pi_query, payoffs) in game_results.items():\n            (pi, query) = pi_query\n            ai = query[pi]\n            pg_mean[pi][ai] += (payoffs[pi] - pg_mean[pi][ai]) / float(s + 1)\n        exp_is = []\n        for i in range(num_players):\n            exp_is.append(pg_mean[i].max() - pg_mean[i].dot(dist[i]))\n        exps_estimated.append(np.mean(exp_is))\n    return exps_estimated",
            "def estimate_exploitability_nonsym(self, dist, num_eval_samples, num_ckpts, num_players, game, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Estimate exploitability via monte carlo.\\n\\n    Args:\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_eval_samples: int, number of samples to estimate exploitability\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n      num_players: int, number of players\\n      game: game with minimal functionality (see games/small.py)\\n      policies: list mapping checkpoints to policies\\n    Returns:\\n      list of exploitabilities computed using [index] monte carlo samples\\n    '\n    pg_mean = [np.zeros_like(dist_i) for dist_i in dist]\n    exps_estimated = []\n    for s in range(num_eval_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries_for_exp(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        for (pi_query, payoffs) in game_results.items():\n            (pi, query) = pi_query\n            ai = query[pi]\n            pg_mean[pi][ai] += (payoffs[pi] - pg_mean[pi][ai]) / float(s + 1)\n        exp_is = []\n        for i in range(num_players):\n            exp_is.append(pg_mean[i].max() - pg_mean[i].dot(dist[i]))\n        exps_estimated.append(np.mean(exp_is))\n    return exps_estimated"
        ]
    },
    {
        "func_name": "update_payoff_matrices",
        "original": "def update_payoff_matrices(self, payoff_matrices, payoff_matrices_new, s):\n    \"\"\"Update mean of payoff matrices.\n\n    Args:\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\n          are sorted and arrays should be indexed in the same order\n          **current mean\n      payoff_matrices_new: **new sample\n      s: int, sample number\n    Returns:\n      payoff_matrices with updated means\n    \"\"\"\n    if payoff_matrices:\n        for key in payoff_matrices_new:\n            new = payoff_matrices_new[key]\n            old = payoff_matrices[key]\n            payoff_matrices[key] += (new - old) / float(s + 1)\n    else:\n        payoff_matrices = payoff_matrices_new\n    return payoff_matrices",
        "mutated": [
            "def update_payoff_matrices(self, payoff_matrices, payoff_matrices_new, s):\n    if False:\n        i = 10\n    'Update mean of payoff matrices.\\n\\n    Args:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n          **current mean\\n      payoff_matrices_new: **new sample\\n      s: int, sample number\\n    Returns:\\n      payoff_matrices with updated means\\n    '\n    if payoff_matrices:\n        for key in payoff_matrices_new:\n            new = payoff_matrices_new[key]\n            old = payoff_matrices[key]\n            payoff_matrices[key] += (new - old) / float(s + 1)\n    else:\n        payoff_matrices = payoff_matrices_new\n    return payoff_matrices",
            "def update_payoff_matrices(self, payoff_matrices, payoff_matrices_new, s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Update mean of payoff matrices.\\n\\n    Args:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n          **current mean\\n      payoff_matrices_new: **new sample\\n      s: int, sample number\\n    Returns:\\n      payoff_matrices with updated means\\n    '\n    if payoff_matrices:\n        for key in payoff_matrices_new:\n            new = payoff_matrices_new[key]\n            old = payoff_matrices[key]\n            payoff_matrices[key] += (new - old) / float(s + 1)\n    else:\n        payoff_matrices = payoff_matrices_new\n    return payoff_matrices",
            "def update_payoff_matrices(self, payoff_matrices, payoff_matrices_new, s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Update mean of payoff matrices.\\n\\n    Args:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n          **current mean\\n      payoff_matrices_new: **new sample\\n      s: int, sample number\\n    Returns:\\n      payoff_matrices with updated means\\n    '\n    if payoff_matrices:\n        for key in payoff_matrices_new:\n            new = payoff_matrices_new[key]\n            old = payoff_matrices[key]\n            payoff_matrices[key] += (new - old) / float(s + 1)\n    else:\n        payoff_matrices = payoff_matrices_new\n    return payoff_matrices",
            "def update_payoff_matrices(self, payoff_matrices, payoff_matrices_new, s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Update mean of payoff matrices.\\n\\n    Args:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n          **current mean\\n      payoff_matrices_new: **new sample\\n      s: int, sample number\\n    Returns:\\n      payoff_matrices with updated means\\n    '\n    if payoff_matrices:\n        for key in payoff_matrices_new:\n            new = payoff_matrices_new[key]\n            old = payoff_matrices[key]\n            payoff_matrices[key] += (new - old) / float(s + 1)\n    else:\n        payoff_matrices = payoff_matrices_new\n    return payoff_matrices",
            "def update_payoff_matrices(self, payoff_matrices, payoff_matrices_new, s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Update mean of payoff matrices.\\n\\n    Args:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n          **current mean\\n      payoff_matrices_new: **new sample\\n      s: int, sample number\\n    Returns:\\n      payoff_matrices with updated means\\n    '\n    if payoff_matrices:\n        for key in payoff_matrices_new:\n            new = payoff_matrices_new[key]\n            old = payoff_matrices[key]\n            payoff_matrices[key] += (new - old) / float(s + 1)\n    else:\n        payoff_matrices = payoff_matrices_new\n    return payoff_matrices"
        ]
    },
    {
        "func_name": "construct_payoff_matrices_from_samples_sym",
        "original": "def construct_payoff_matrices_from_samples_sym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    \"\"\"Construct payoff matrices (approx. sym. polymatrix game) from samples.\n\n    Args:\n      game: game with minimal functionality (see games/small.py)\n      dist: 1-d np.array, estimate of nash distribution\n      num_samples: int, `minibatch' size for stochastic gradient\n      policies: list mapping checkpoints to policies\n      num_players: int, number of players\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\n    Returns:\n      payoff_matrices (2 x num_ckpts x num_ckpts array) to compute adidas grad\n    \"\"\"\n    payoff_matrices = np.zeros((2, num_ckpts, num_ckpts))\n    for _ in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices += sym_game_runner.form_payoff_matrices(game_results, num_ckpts) / float(num_samples)\n    return payoff_matrices",
        "mutated": [
            "def construct_payoff_matrices_from_samples_sym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    if False:\n        i = 10\n    \"Construct payoff matrices (approx. sym. polymatrix game) from samples.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      policies: list mapping checkpoints to policies\\n      num_players: int, number of players\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n    Returns:\\n      payoff_matrices (2 x num_ckpts x num_ckpts array) to compute adidas grad\\n    \"\n    payoff_matrices = np.zeros((2, num_ckpts, num_ckpts))\n    for _ in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices += sym_game_runner.form_payoff_matrices(game_results, num_ckpts) / float(num_samples)\n    return payoff_matrices",
            "def construct_payoff_matrices_from_samples_sym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Construct payoff matrices (approx. sym. polymatrix game) from samples.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      policies: list mapping checkpoints to policies\\n      num_players: int, number of players\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n    Returns:\\n      payoff_matrices (2 x num_ckpts x num_ckpts array) to compute adidas grad\\n    \"\n    payoff_matrices = np.zeros((2, num_ckpts, num_ckpts))\n    for _ in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices += sym_game_runner.form_payoff_matrices(game_results, num_ckpts) / float(num_samples)\n    return payoff_matrices",
            "def construct_payoff_matrices_from_samples_sym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Construct payoff matrices (approx. sym. polymatrix game) from samples.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      policies: list mapping checkpoints to policies\\n      num_players: int, number of players\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n    Returns:\\n      payoff_matrices (2 x num_ckpts x num_ckpts array) to compute adidas grad\\n    \"\n    payoff_matrices = np.zeros((2, num_ckpts, num_ckpts))\n    for _ in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices += sym_game_runner.form_payoff_matrices(game_results, num_ckpts) / float(num_samples)\n    return payoff_matrices",
            "def construct_payoff_matrices_from_samples_sym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Construct payoff matrices (approx. sym. polymatrix game) from samples.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      policies: list mapping checkpoints to policies\\n      num_players: int, number of players\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n    Returns:\\n      payoff_matrices (2 x num_ckpts x num_ckpts array) to compute adidas grad\\n    \"\n    payoff_matrices = np.zeros((2, num_ckpts, num_ckpts))\n    for _ in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices += sym_game_runner.form_payoff_matrices(game_results, num_ckpts) / float(num_samples)\n    return payoff_matrices",
            "def construct_payoff_matrices_from_samples_sym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Construct payoff matrices (approx. sym. polymatrix game) from samples.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      policies: list mapping checkpoints to policies\\n      num_players: int, number of players\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n    Returns:\\n      payoff_matrices (2 x num_ckpts x num_ckpts array) to compute adidas grad\\n    \"\n    payoff_matrices = np.zeros((2, num_ckpts, num_ckpts))\n    for _ in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts, p=dist) for _ in range(num_players)])\n        game_queries = sym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = sym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices += sym_game_runner.form_payoff_matrices(game_results, num_ckpts) / float(num_samples)\n    return payoff_matrices"
        ]
    },
    {
        "func_name": "construct_payoff_matrices_exactly_sym",
        "original": "def construct_payoff_matrices_exactly_sym(self, game, dist, num_players):\n    \"\"\"Construct payoff matrices exactly (expected sym. polymatrix game).\n\n    Args:\n      game: game with minimal functionality (see games/small.py)\n      dist: 1-d np.array, estimate of nash distribution\n      num_players: int, number of players\n    Returns:\n      payoff_matrices (2 x A x A array) to compute adidas gradient\n    \"\"\"\n    sym_nash = [dist for _ in range(num_players)]\n    pt = game.payoff_tensor()\n    payoff_matrix_exp_0 = misc.pt_reduce(pt[0], sym_nash, [0, 1])\n    payoff_matrix_exp_1 = misc.pt_reduce(pt[1], sym_nash, [0, 1])\n    payoff_matrices = np.stack((payoff_matrix_exp_0, payoff_matrix_exp_1))\n    return payoff_matrices",
        "mutated": [
            "def construct_payoff_matrices_exactly_sym(self, game, dist, num_players):\n    if False:\n        i = 10\n    'Construct payoff matrices exactly (expected sym. polymatrix game).\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_players: int, number of players\\n    Returns:\\n      payoff_matrices (2 x A x A array) to compute adidas gradient\\n    '\n    sym_nash = [dist for _ in range(num_players)]\n    pt = game.payoff_tensor()\n    payoff_matrix_exp_0 = misc.pt_reduce(pt[0], sym_nash, [0, 1])\n    payoff_matrix_exp_1 = misc.pt_reduce(pt[1], sym_nash, [0, 1])\n    payoff_matrices = np.stack((payoff_matrix_exp_0, payoff_matrix_exp_1))\n    return payoff_matrices",
            "def construct_payoff_matrices_exactly_sym(self, game, dist, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Construct payoff matrices exactly (expected sym. polymatrix game).\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_players: int, number of players\\n    Returns:\\n      payoff_matrices (2 x A x A array) to compute adidas gradient\\n    '\n    sym_nash = [dist for _ in range(num_players)]\n    pt = game.payoff_tensor()\n    payoff_matrix_exp_0 = misc.pt_reduce(pt[0], sym_nash, [0, 1])\n    payoff_matrix_exp_1 = misc.pt_reduce(pt[1], sym_nash, [0, 1])\n    payoff_matrices = np.stack((payoff_matrix_exp_0, payoff_matrix_exp_1))\n    return payoff_matrices",
            "def construct_payoff_matrices_exactly_sym(self, game, dist, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Construct payoff matrices exactly (expected sym. polymatrix game).\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_players: int, number of players\\n    Returns:\\n      payoff_matrices (2 x A x A array) to compute adidas gradient\\n    '\n    sym_nash = [dist for _ in range(num_players)]\n    pt = game.payoff_tensor()\n    payoff_matrix_exp_0 = misc.pt_reduce(pt[0], sym_nash, [0, 1])\n    payoff_matrix_exp_1 = misc.pt_reduce(pt[1], sym_nash, [0, 1])\n    payoff_matrices = np.stack((payoff_matrix_exp_0, payoff_matrix_exp_1))\n    return payoff_matrices",
            "def construct_payoff_matrices_exactly_sym(self, game, dist, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Construct payoff matrices exactly (expected sym. polymatrix game).\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_players: int, number of players\\n    Returns:\\n      payoff_matrices (2 x A x A array) to compute adidas gradient\\n    '\n    sym_nash = [dist for _ in range(num_players)]\n    pt = game.payoff_tensor()\n    payoff_matrix_exp_0 = misc.pt_reduce(pt[0], sym_nash, [0, 1])\n    payoff_matrix_exp_1 = misc.pt_reduce(pt[1], sym_nash, [0, 1])\n    payoff_matrices = np.stack((payoff_matrix_exp_0, payoff_matrix_exp_1))\n    return payoff_matrices",
            "def construct_payoff_matrices_exactly_sym(self, game, dist, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Construct payoff matrices exactly (expected sym. polymatrix game).\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: 1-d np.array, estimate of nash distribution\\n      num_players: int, number of players\\n    Returns:\\n      payoff_matrices (2 x A x A array) to compute adidas gradient\\n    '\n    sym_nash = [dist for _ in range(num_players)]\n    pt = game.payoff_tensor()\n    payoff_matrix_exp_0 = misc.pt_reduce(pt[0], sym_nash, [0, 1])\n    payoff_matrix_exp_1 = misc.pt_reduce(pt[1], sym_nash, [0, 1])\n    payoff_matrices = np.stack((payoff_matrix_exp_0, payoff_matrix_exp_1))\n    return payoff_matrices"
        ]
    },
    {
        "func_name": "construct_payoff_matrices_from_samples_nonsym",
        "original": "def construct_payoff_matrices_from_samples_nonsym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    \"\"\"Construct payoff matrices (approx. nonsym. polymatrix) from samples.\n\n    Args:\n      game: game with minimal functionality (see games/small.py)\n      dist: list of 1-d np.arrays, estimate of nash distribution\n      num_samples: int, `minibatch' size for stochastic gradient\n      policies: list mapping checkpoints to policies\n      num_players: int, number of players\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\n    Returns:\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\n          are sorted and arrays should be indexed in the same order\n    \"\"\"\n    payoff_matrices = None\n    for s in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices_new = nonsym_game_runner.form_payoff_matrices(game_results, num_ckpts)\n        payoff_matrices = self.update_payoff_matrices(payoff_matrices, payoff_matrices_new, s)\n    return payoff_matrices",
        "mutated": [
            "def construct_payoff_matrices_from_samples_nonsym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    if False:\n        i = 10\n    \"Construct payoff matrices (approx. nonsym. polymatrix) from samples.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      policies: list mapping checkpoints to policies\\n      num_players: int, number of players\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n    Returns:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n    \"\n    payoff_matrices = None\n    for s in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices_new = nonsym_game_runner.form_payoff_matrices(game_results, num_ckpts)\n        payoff_matrices = self.update_payoff_matrices(payoff_matrices, payoff_matrices_new, s)\n    return payoff_matrices",
            "def construct_payoff_matrices_from_samples_nonsym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Construct payoff matrices (approx. nonsym. polymatrix) from samples.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      policies: list mapping checkpoints to policies\\n      num_players: int, number of players\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n    Returns:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n    \"\n    payoff_matrices = None\n    for s in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices_new = nonsym_game_runner.form_payoff_matrices(game_results, num_ckpts)\n        payoff_matrices = self.update_payoff_matrices(payoff_matrices, payoff_matrices_new, s)\n    return payoff_matrices",
            "def construct_payoff_matrices_from_samples_nonsym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Construct payoff matrices (approx. nonsym. polymatrix) from samples.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      policies: list mapping checkpoints to policies\\n      num_players: int, number of players\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n    Returns:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n    \"\n    payoff_matrices = None\n    for s in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices_new = nonsym_game_runner.form_payoff_matrices(game_results, num_ckpts)\n        payoff_matrices = self.update_payoff_matrices(payoff_matrices, payoff_matrices_new, s)\n    return payoff_matrices",
            "def construct_payoff_matrices_from_samples_nonsym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Construct payoff matrices (approx. nonsym. polymatrix) from samples.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      policies: list mapping checkpoints to policies\\n      num_players: int, number of players\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n    Returns:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n    \"\n    payoff_matrices = None\n    for s in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices_new = nonsym_game_runner.form_payoff_matrices(game_results, num_ckpts)\n        payoff_matrices = self.update_payoff_matrices(payoff_matrices, payoff_matrices_new, s)\n    return payoff_matrices",
            "def construct_payoff_matrices_from_samples_nonsym(self, game, dist, num_samples, policies, num_players, num_ckpts):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Construct payoff matrices (approx. nonsym. polymatrix) from samples.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      policies: list mapping checkpoints to policies\\n      num_players: int, number of players\\n      num_ckpts: int, number of checkpoints (actions, policies, ...)\\n    Returns:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n    \"\n    payoff_matrices = None\n    for s in range(num_samples):\n        base_profile = tuple([self.random.choice(num_ckpts[i], p=dist[i]) for i in range(num_players)])\n        game_queries = nonsym_game_runner.construct_game_queries(base_profile, num_ckpts)\n        game_results = nonsym_game_runner.run_games_and_record_payoffs(game_queries, game.get_payoffs_for_strategies, policies)\n        payoff_matrices_new = nonsym_game_runner.form_payoff_matrices(game_results, num_ckpts)\n        payoff_matrices = self.update_payoff_matrices(payoff_matrices, payoff_matrices_new, s)\n    return payoff_matrices"
        ]
    },
    {
        "func_name": "construct_payoff_matrices_exactly_nonsym",
        "original": "def construct_payoff_matrices_exactly_nonsym(self, game, dist, num_players):\n    \"\"\"Construct payoff matrices exactly (expected nonsym. polymatrix game).\n\n    Args:\n      game: game with minimal functionality (see games/small.py)\n      dist: list of 1-d np.arrays, estimate of nash distribution\n      num_players: int, number of players\n    Returns:\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\n          are sorted and arrays should be indexed in the same order\n    \"\"\"\n    pt = game.payoff_tensor()\n    payoff_matrices = {}\n    for (pi, pj) in itertools.combinations(range(num_players), 2):\n        key = (pi, pj)\n        pt_i = misc.pt_reduce(pt[pi], dist, [pi, pj])\n        pt_j = misc.pt_reduce(pt[pj], dist, [pi, pj])\n        payoff_matrices[key] = np.stack((pt_i, pt_j), axis=0)\n    return payoff_matrices",
        "mutated": [
            "def construct_payoff_matrices_exactly_nonsym(self, game, dist, num_players):\n    if False:\n        i = 10\n    'Construct payoff matrices exactly (expected nonsym. polymatrix game).\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_players: int, number of players\\n    Returns:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n    '\n    pt = game.payoff_tensor()\n    payoff_matrices = {}\n    for (pi, pj) in itertools.combinations(range(num_players), 2):\n        key = (pi, pj)\n        pt_i = misc.pt_reduce(pt[pi], dist, [pi, pj])\n        pt_j = misc.pt_reduce(pt[pj], dist, [pi, pj])\n        payoff_matrices[key] = np.stack((pt_i, pt_j), axis=0)\n    return payoff_matrices",
            "def construct_payoff_matrices_exactly_nonsym(self, game, dist, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Construct payoff matrices exactly (expected nonsym. polymatrix game).\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_players: int, number of players\\n    Returns:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n    '\n    pt = game.payoff_tensor()\n    payoff_matrices = {}\n    for (pi, pj) in itertools.combinations(range(num_players), 2):\n        key = (pi, pj)\n        pt_i = misc.pt_reduce(pt[pi], dist, [pi, pj])\n        pt_j = misc.pt_reduce(pt[pj], dist, [pi, pj])\n        payoff_matrices[key] = np.stack((pt_i, pt_j), axis=0)\n    return payoff_matrices",
            "def construct_payoff_matrices_exactly_nonsym(self, game, dist, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Construct payoff matrices exactly (expected nonsym. polymatrix game).\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_players: int, number of players\\n    Returns:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n    '\n    pt = game.payoff_tensor()\n    payoff_matrices = {}\n    for (pi, pj) in itertools.combinations(range(num_players), 2):\n        key = (pi, pj)\n        pt_i = misc.pt_reduce(pt[pi], dist, [pi, pj])\n        pt_j = misc.pt_reduce(pt[pj], dist, [pi, pj])\n        payoff_matrices[key] = np.stack((pt_i, pt_j), axis=0)\n    return payoff_matrices",
            "def construct_payoff_matrices_exactly_nonsym(self, game, dist, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Construct payoff matrices exactly (expected nonsym. polymatrix game).\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_players: int, number of players\\n    Returns:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n    '\n    pt = game.payoff_tensor()\n    payoff_matrices = {}\n    for (pi, pj) in itertools.combinations(range(num_players), 2):\n        key = (pi, pj)\n        pt_i = misc.pt_reduce(pt[pi], dist, [pi, pj])\n        pt_j = misc.pt_reduce(pt[pj], dist, [pi, pj])\n        payoff_matrices[key] = np.stack((pt_i, pt_j), axis=0)\n    return payoff_matrices",
            "def construct_payoff_matrices_exactly_nonsym(self, game, dist, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Construct payoff matrices exactly (expected nonsym. polymatrix game).\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      dist: list of 1-d np.arrays, estimate of nash distribution\\n      num_players: int, number of players\\n    Returns:\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n    '\n    pt = game.payoff_tensor()\n    payoff_matrices = {}\n    for (pi, pj) in itertools.combinations(range(num_players), 2):\n        key = (pi, pj)\n        pt_i = misc.pt_reduce(pt[pi], dist, [pi, pj])\n        pt_j = misc.pt_reduce(pt[pj], dist, [pi, pj])\n        payoff_matrices[key] = np.stack((pt_i, pt_j), axis=0)\n    return payoff_matrices"
        ]
    },
    {
        "func_name": "approximate_nash",
        "original": "def approximate_nash(self, game, solver, sym, num_iterations=10000, num_samples=1, num_eval_samples=int(100000.0), approx_eval=False, exact_eval=False, avg_trajectory=False, return_trajectory=False):\n    \"\"\"Runs solver on game.\n\n    Args:\n      game: game with minimal functionality (see games/small.py)\n      solver: gradient solver (see utils/updates.py)\n      sym: bool, true if the game is symmetric across players\n      num_iterations: int, number of incremental updates\n      num_samples: int, `minibatch' size for stochastic gradient\n      num_eval_samples: int, number of samples to estimate exploitability\n        default = # of samples for P[|sample_payoff-true| > C/100] < ~5e-7%\n        where C = pt.max() - pt.min();\n        P[|pt_grad|_inf <= C/100] > (1-5e-7)^num_actions\n      approx_eval: bool, whether to evaluate exploitability during\n        descent with stochastic samples\n      exact_eval: bool, whether to evaluate exploitability during\n        descent with exact expectation (req. full payoff tensor)\n      avg_trajectory: bool, whether to evaluate w.r.t. the average distribution\n        up to time t instead of the distribution at time t\n      return_trajectory: bool, whether to record all parameters (e.g., dist)\n        during learning and return them -- see solver code for details\n    Returns:\n      None -- dict of results stored in `results` attribute upon completion\n        (key=name of metric, value=[m_0, ..., m_{last_iter}])\n    \"\"\"\n    num_players = game.num_players()\n    num_strats = game.num_strategies()\n    if sym:\n        if len(set(num_strats)) != 1:\n            raise ValueError('Each player should have the same number of actions.')\n        num_strats = num_strats[0]\n    params = solver.init_vars(num_strats, num_players)\n    if sym:\n        dist_avg = np.zeros_like(params[0])\n        policies = list(range(num_strats))\n        num_ckpts = len(policies)\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_sym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_sym\n        exp = sym_exp\n        estimate_exploitability = self.estimate_exploitability_sym\n    else:\n        dist_avg = [np.zeros_like(dist_i) for dist_i in params[0]]\n        policies = [list(range(num_strats_i)) for num_strats_i in num_strats]\n        num_ckpts = [len(policy_i) for policy_i in policies]\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_nonsym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_nonsym\n        exp = nonsym_exp\n        estimate_exploitability = self.estimate_exploitability_nonsym\n    exps_exact = []\n    exps_solver_exact = []\n    exps_approx = []\n    exps_solver_approx = []\n    grad_norms = []\n    if return_trajectory:\n        params_traj = []\n    has_temp = False\n    if hasattr(solver, 'temperature') or hasattr(solver, 'p'):\n        has_temp = True\n        temperatures = []\n        if hasattr(solver, 'temperature'):\n            temp_attr = 'temperature'\n        else:\n            temp_attr = 'p'\n    early_exit = False\n    start = time.time()\n    for t in range(num_iterations + 1):\n        dist = params[0]\n        if return_trajectory:\n            params_traj.append(params)\n        if return_trajectory:\n            params_traj.append(params)\n        if has_temp:\n            temperatures.append(getattr(solver, temp_attr))\n        if num_samples < np.inf:\n            payoff_matrices = form_payoffs_appx(game, dist, num_samples, policies, num_players, num_ckpts)\n        else:\n            payoff_matrices = form_payoffs_exact(game, dist, num_players)\n        (grads, exp_sto, exp_solver_sto) = solver.compute_gradients(params, payoff_matrices)\n        if sym:\n            grads_dist = grads[0]\n            grad_norms.append(simplex.grad_norm(dist, grads_dist))\n        else:\n            grad_norm = 0.0\n            grads_dist = grads[0]\n            for (dist_i, grads_i) in zip(dist, grads_dist[0]):\n                grad_norm += simplex.grad_norm(dist_i, grads_i) ** 2.0\n            grad_norm = np.sqrt(grad_norm)\n            grad_norms.append(grad_norm)\n        if solver.has_aux:\n            solver.record_aux_errors(grads)\n        if sym:\n            dist_avg += (dist - dist_avg) / float(t + 1)\n        else:\n            for (i, dist_i) in enumerate(dist):\n                dist_avg[i] += (dist_i - dist_avg[i]) / float(t + 1)\n        if avg_trajectory:\n            dist_eval = dist_avg\n        else:\n            dist_eval = dist\n        if approx_eval:\n            exps_approx.append(exp_sto)\n            exps_solver_approx.append(exp_solver_sto)\n        if exact_eval:\n            pt = game.payoff_tensor()\n            exps_exact.append(exp.unreg_exploitability(dist_eval, pt))\n            exps_solver_exact.append(solver.exploitability(dist_eval, pt))\n        if t < num_iterations:\n            params = solver.update(params, grads, t)\n            if misc.isnan(params):\n                print('Warning: NaN detected in params post-update. Exiting loop.')\n                early_exit = True\n                break\n    end = time.time()\n    solve_runtime = end - start\n    start = end\n    exp_estimated = estimate_exploitability(dist_eval, num_eval_samples, num_ckpts, num_players, game, policies)\n    eval_runtime = time.time() - start\n    results = {'exps_approx': exps_approx, 'exps_solver_approx': exps_solver_approx, 'exps_exact': exps_exact, 'exps_solver_exact': exps_solver_exact, 'exp_estimated': exp_estimated, 'grad_norms': grad_norms, 'dist': dist, 'dist_avg': dist_avg, 'solve_runtime': solve_runtime, 'eval_runtime': eval_runtime, 'early_exit': early_exit}\n    if solver.has_aux:\n        results.update({'aux_errors': solver.aux_errors})\n    if return_trajectory:\n        results.update({'params_trajectory': params_traj})\n    if has_temp:\n        results.update({'temperatures': temperatures})\n    self.results = results",
        "mutated": [
            "def approximate_nash(self, game, solver, sym, num_iterations=10000, num_samples=1, num_eval_samples=int(100000.0), approx_eval=False, exact_eval=False, avg_trajectory=False, return_trajectory=False):\n    if False:\n        i = 10\n    \"Runs solver on game.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      solver: gradient solver (see utils/updates.py)\\n      sym: bool, true if the game is symmetric across players\\n      num_iterations: int, number of incremental updates\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      num_eval_samples: int, number of samples to estimate exploitability\\n        default = # of samples for P[|sample_payoff-true| > C/100] < ~5e-7%\\n        where C = pt.max() - pt.min();\\n        P[|pt_grad|_inf <= C/100] > (1-5e-7)^num_actions\\n      approx_eval: bool, whether to evaluate exploitability during\\n        descent with stochastic samples\\n      exact_eval: bool, whether to evaluate exploitability during\\n        descent with exact expectation (req. full payoff tensor)\\n      avg_trajectory: bool, whether to evaluate w.r.t. the average distribution\\n        up to time t instead of the distribution at time t\\n      return_trajectory: bool, whether to record all parameters (e.g., dist)\\n        during learning and return them -- see solver code for details\\n    Returns:\\n      None -- dict of results stored in `results` attribute upon completion\\n        (key=name of metric, value=[m_0, ..., m_{last_iter}])\\n    \"\n    num_players = game.num_players()\n    num_strats = game.num_strategies()\n    if sym:\n        if len(set(num_strats)) != 1:\n            raise ValueError('Each player should have the same number of actions.')\n        num_strats = num_strats[0]\n    params = solver.init_vars(num_strats, num_players)\n    if sym:\n        dist_avg = np.zeros_like(params[0])\n        policies = list(range(num_strats))\n        num_ckpts = len(policies)\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_sym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_sym\n        exp = sym_exp\n        estimate_exploitability = self.estimate_exploitability_sym\n    else:\n        dist_avg = [np.zeros_like(dist_i) for dist_i in params[0]]\n        policies = [list(range(num_strats_i)) for num_strats_i in num_strats]\n        num_ckpts = [len(policy_i) for policy_i in policies]\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_nonsym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_nonsym\n        exp = nonsym_exp\n        estimate_exploitability = self.estimate_exploitability_nonsym\n    exps_exact = []\n    exps_solver_exact = []\n    exps_approx = []\n    exps_solver_approx = []\n    grad_norms = []\n    if return_trajectory:\n        params_traj = []\n    has_temp = False\n    if hasattr(solver, 'temperature') or hasattr(solver, 'p'):\n        has_temp = True\n        temperatures = []\n        if hasattr(solver, 'temperature'):\n            temp_attr = 'temperature'\n        else:\n            temp_attr = 'p'\n    early_exit = False\n    start = time.time()\n    for t in range(num_iterations + 1):\n        dist = params[0]\n        if return_trajectory:\n            params_traj.append(params)\n        if return_trajectory:\n            params_traj.append(params)\n        if has_temp:\n            temperatures.append(getattr(solver, temp_attr))\n        if num_samples < np.inf:\n            payoff_matrices = form_payoffs_appx(game, dist, num_samples, policies, num_players, num_ckpts)\n        else:\n            payoff_matrices = form_payoffs_exact(game, dist, num_players)\n        (grads, exp_sto, exp_solver_sto) = solver.compute_gradients(params, payoff_matrices)\n        if sym:\n            grads_dist = grads[0]\n            grad_norms.append(simplex.grad_norm(dist, grads_dist))\n        else:\n            grad_norm = 0.0\n            grads_dist = grads[0]\n            for (dist_i, grads_i) in zip(dist, grads_dist[0]):\n                grad_norm += simplex.grad_norm(dist_i, grads_i) ** 2.0\n            grad_norm = np.sqrt(grad_norm)\n            grad_norms.append(grad_norm)\n        if solver.has_aux:\n            solver.record_aux_errors(grads)\n        if sym:\n            dist_avg += (dist - dist_avg) / float(t + 1)\n        else:\n            for (i, dist_i) in enumerate(dist):\n                dist_avg[i] += (dist_i - dist_avg[i]) / float(t + 1)\n        if avg_trajectory:\n            dist_eval = dist_avg\n        else:\n            dist_eval = dist\n        if approx_eval:\n            exps_approx.append(exp_sto)\n            exps_solver_approx.append(exp_solver_sto)\n        if exact_eval:\n            pt = game.payoff_tensor()\n            exps_exact.append(exp.unreg_exploitability(dist_eval, pt))\n            exps_solver_exact.append(solver.exploitability(dist_eval, pt))\n        if t < num_iterations:\n            params = solver.update(params, grads, t)\n            if misc.isnan(params):\n                print('Warning: NaN detected in params post-update. Exiting loop.')\n                early_exit = True\n                break\n    end = time.time()\n    solve_runtime = end - start\n    start = end\n    exp_estimated = estimate_exploitability(dist_eval, num_eval_samples, num_ckpts, num_players, game, policies)\n    eval_runtime = time.time() - start\n    results = {'exps_approx': exps_approx, 'exps_solver_approx': exps_solver_approx, 'exps_exact': exps_exact, 'exps_solver_exact': exps_solver_exact, 'exp_estimated': exp_estimated, 'grad_norms': grad_norms, 'dist': dist, 'dist_avg': dist_avg, 'solve_runtime': solve_runtime, 'eval_runtime': eval_runtime, 'early_exit': early_exit}\n    if solver.has_aux:\n        results.update({'aux_errors': solver.aux_errors})\n    if return_trajectory:\n        results.update({'params_trajectory': params_traj})\n    if has_temp:\n        results.update({'temperatures': temperatures})\n    self.results = results",
            "def approximate_nash(self, game, solver, sym, num_iterations=10000, num_samples=1, num_eval_samples=int(100000.0), approx_eval=False, exact_eval=False, avg_trajectory=False, return_trajectory=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Runs solver on game.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      solver: gradient solver (see utils/updates.py)\\n      sym: bool, true if the game is symmetric across players\\n      num_iterations: int, number of incremental updates\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      num_eval_samples: int, number of samples to estimate exploitability\\n        default = # of samples for P[|sample_payoff-true| > C/100] < ~5e-7%\\n        where C = pt.max() - pt.min();\\n        P[|pt_grad|_inf <= C/100] > (1-5e-7)^num_actions\\n      approx_eval: bool, whether to evaluate exploitability during\\n        descent with stochastic samples\\n      exact_eval: bool, whether to evaluate exploitability during\\n        descent with exact expectation (req. full payoff tensor)\\n      avg_trajectory: bool, whether to evaluate w.r.t. the average distribution\\n        up to time t instead of the distribution at time t\\n      return_trajectory: bool, whether to record all parameters (e.g., dist)\\n        during learning and return them -- see solver code for details\\n    Returns:\\n      None -- dict of results stored in `results` attribute upon completion\\n        (key=name of metric, value=[m_0, ..., m_{last_iter}])\\n    \"\n    num_players = game.num_players()\n    num_strats = game.num_strategies()\n    if sym:\n        if len(set(num_strats)) != 1:\n            raise ValueError('Each player should have the same number of actions.')\n        num_strats = num_strats[0]\n    params = solver.init_vars(num_strats, num_players)\n    if sym:\n        dist_avg = np.zeros_like(params[0])\n        policies = list(range(num_strats))\n        num_ckpts = len(policies)\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_sym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_sym\n        exp = sym_exp\n        estimate_exploitability = self.estimate_exploitability_sym\n    else:\n        dist_avg = [np.zeros_like(dist_i) for dist_i in params[0]]\n        policies = [list(range(num_strats_i)) for num_strats_i in num_strats]\n        num_ckpts = [len(policy_i) for policy_i in policies]\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_nonsym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_nonsym\n        exp = nonsym_exp\n        estimate_exploitability = self.estimate_exploitability_nonsym\n    exps_exact = []\n    exps_solver_exact = []\n    exps_approx = []\n    exps_solver_approx = []\n    grad_norms = []\n    if return_trajectory:\n        params_traj = []\n    has_temp = False\n    if hasattr(solver, 'temperature') or hasattr(solver, 'p'):\n        has_temp = True\n        temperatures = []\n        if hasattr(solver, 'temperature'):\n            temp_attr = 'temperature'\n        else:\n            temp_attr = 'p'\n    early_exit = False\n    start = time.time()\n    for t in range(num_iterations + 1):\n        dist = params[0]\n        if return_trajectory:\n            params_traj.append(params)\n        if return_trajectory:\n            params_traj.append(params)\n        if has_temp:\n            temperatures.append(getattr(solver, temp_attr))\n        if num_samples < np.inf:\n            payoff_matrices = form_payoffs_appx(game, dist, num_samples, policies, num_players, num_ckpts)\n        else:\n            payoff_matrices = form_payoffs_exact(game, dist, num_players)\n        (grads, exp_sto, exp_solver_sto) = solver.compute_gradients(params, payoff_matrices)\n        if sym:\n            grads_dist = grads[0]\n            grad_norms.append(simplex.grad_norm(dist, grads_dist))\n        else:\n            grad_norm = 0.0\n            grads_dist = grads[0]\n            for (dist_i, grads_i) in zip(dist, grads_dist[0]):\n                grad_norm += simplex.grad_norm(dist_i, grads_i) ** 2.0\n            grad_norm = np.sqrt(grad_norm)\n            grad_norms.append(grad_norm)\n        if solver.has_aux:\n            solver.record_aux_errors(grads)\n        if sym:\n            dist_avg += (dist - dist_avg) / float(t + 1)\n        else:\n            for (i, dist_i) in enumerate(dist):\n                dist_avg[i] += (dist_i - dist_avg[i]) / float(t + 1)\n        if avg_trajectory:\n            dist_eval = dist_avg\n        else:\n            dist_eval = dist\n        if approx_eval:\n            exps_approx.append(exp_sto)\n            exps_solver_approx.append(exp_solver_sto)\n        if exact_eval:\n            pt = game.payoff_tensor()\n            exps_exact.append(exp.unreg_exploitability(dist_eval, pt))\n            exps_solver_exact.append(solver.exploitability(dist_eval, pt))\n        if t < num_iterations:\n            params = solver.update(params, grads, t)\n            if misc.isnan(params):\n                print('Warning: NaN detected in params post-update. Exiting loop.')\n                early_exit = True\n                break\n    end = time.time()\n    solve_runtime = end - start\n    start = end\n    exp_estimated = estimate_exploitability(dist_eval, num_eval_samples, num_ckpts, num_players, game, policies)\n    eval_runtime = time.time() - start\n    results = {'exps_approx': exps_approx, 'exps_solver_approx': exps_solver_approx, 'exps_exact': exps_exact, 'exps_solver_exact': exps_solver_exact, 'exp_estimated': exp_estimated, 'grad_norms': grad_norms, 'dist': dist, 'dist_avg': dist_avg, 'solve_runtime': solve_runtime, 'eval_runtime': eval_runtime, 'early_exit': early_exit}\n    if solver.has_aux:\n        results.update({'aux_errors': solver.aux_errors})\n    if return_trajectory:\n        results.update({'params_trajectory': params_traj})\n    if has_temp:\n        results.update({'temperatures': temperatures})\n    self.results = results",
            "def approximate_nash(self, game, solver, sym, num_iterations=10000, num_samples=1, num_eval_samples=int(100000.0), approx_eval=False, exact_eval=False, avg_trajectory=False, return_trajectory=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Runs solver on game.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      solver: gradient solver (see utils/updates.py)\\n      sym: bool, true if the game is symmetric across players\\n      num_iterations: int, number of incremental updates\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      num_eval_samples: int, number of samples to estimate exploitability\\n        default = # of samples for P[|sample_payoff-true| > C/100] < ~5e-7%\\n        where C = pt.max() - pt.min();\\n        P[|pt_grad|_inf <= C/100] > (1-5e-7)^num_actions\\n      approx_eval: bool, whether to evaluate exploitability during\\n        descent with stochastic samples\\n      exact_eval: bool, whether to evaluate exploitability during\\n        descent with exact expectation (req. full payoff tensor)\\n      avg_trajectory: bool, whether to evaluate w.r.t. the average distribution\\n        up to time t instead of the distribution at time t\\n      return_trajectory: bool, whether to record all parameters (e.g., dist)\\n        during learning and return them -- see solver code for details\\n    Returns:\\n      None -- dict of results stored in `results` attribute upon completion\\n        (key=name of metric, value=[m_0, ..., m_{last_iter}])\\n    \"\n    num_players = game.num_players()\n    num_strats = game.num_strategies()\n    if sym:\n        if len(set(num_strats)) != 1:\n            raise ValueError('Each player should have the same number of actions.')\n        num_strats = num_strats[0]\n    params = solver.init_vars(num_strats, num_players)\n    if sym:\n        dist_avg = np.zeros_like(params[0])\n        policies = list(range(num_strats))\n        num_ckpts = len(policies)\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_sym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_sym\n        exp = sym_exp\n        estimate_exploitability = self.estimate_exploitability_sym\n    else:\n        dist_avg = [np.zeros_like(dist_i) for dist_i in params[0]]\n        policies = [list(range(num_strats_i)) for num_strats_i in num_strats]\n        num_ckpts = [len(policy_i) for policy_i in policies]\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_nonsym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_nonsym\n        exp = nonsym_exp\n        estimate_exploitability = self.estimate_exploitability_nonsym\n    exps_exact = []\n    exps_solver_exact = []\n    exps_approx = []\n    exps_solver_approx = []\n    grad_norms = []\n    if return_trajectory:\n        params_traj = []\n    has_temp = False\n    if hasattr(solver, 'temperature') or hasattr(solver, 'p'):\n        has_temp = True\n        temperatures = []\n        if hasattr(solver, 'temperature'):\n            temp_attr = 'temperature'\n        else:\n            temp_attr = 'p'\n    early_exit = False\n    start = time.time()\n    for t in range(num_iterations + 1):\n        dist = params[0]\n        if return_trajectory:\n            params_traj.append(params)\n        if return_trajectory:\n            params_traj.append(params)\n        if has_temp:\n            temperatures.append(getattr(solver, temp_attr))\n        if num_samples < np.inf:\n            payoff_matrices = form_payoffs_appx(game, dist, num_samples, policies, num_players, num_ckpts)\n        else:\n            payoff_matrices = form_payoffs_exact(game, dist, num_players)\n        (grads, exp_sto, exp_solver_sto) = solver.compute_gradients(params, payoff_matrices)\n        if sym:\n            grads_dist = grads[0]\n            grad_norms.append(simplex.grad_norm(dist, grads_dist))\n        else:\n            grad_norm = 0.0\n            grads_dist = grads[0]\n            for (dist_i, grads_i) in zip(dist, grads_dist[0]):\n                grad_norm += simplex.grad_norm(dist_i, grads_i) ** 2.0\n            grad_norm = np.sqrt(grad_norm)\n            grad_norms.append(grad_norm)\n        if solver.has_aux:\n            solver.record_aux_errors(grads)\n        if sym:\n            dist_avg += (dist - dist_avg) / float(t + 1)\n        else:\n            for (i, dist_i) in enumerate(dist):\n                dist_avg[i] += (dist_i - dist_avg[i]) / float(t + 1)\n        if avg_trajectory:\n            dist_eval = dist_avg\n        else:\n            dist_eval = dist\n        if approx_eval:\n            exps_approx.append(exp_sto)\n            exps_solver_approx.append(exp_solver_sto)\n        if exact_eval:\n            pt = game.payoff_tensor()\n            exps_exact.append(exp.unreg_exploitability(dist_eval, pt))\n            exps_solver_exact.append(solver.exploitability(dist_eval, pt))\n        if t < num_iterations:\n            params = solver.update(params, grads, t)\n            if misc.isnan(params):\n                print('Warning: NaN detected in params post-update. Exiting loop.')\n                early_exit = True\n                break\n    end = time.time()\n    solve_runtime = end - start\n    start = end\n    exp_estimated = estimate_exploitability(dist_eval, num_eval_samples, num_ckpts, num_players, game, policies)\n    eval_runtime = time.time() - start\n    results = {'exps_approx': exps_approx, 'exps_solver_approx': exps_solver_approx, 'exps_exact': exps_exact, 'exps_solver_exact': exps_solver_exact, 'exp_estimated': exp_estimated, 'grad_norms': grad_norms, 'dist': dist, 'dist_avg': dist_avg, 'solve_runtime': solve_runtime, 'eval_runtime': eval_runtime, 'early_exit': early_exit}\n    if solver.has_aux:\n        results.update({'aux_errors': solver.aux_errors})\n    if return_trajectory:\n        results.update({'params_trajectory': params_traj})\n    if has_temp:\n        results.update({'temperatures': temperatures})\n    self.results = results",
            "def approximate_nash(self, game, solver, sym, num_iterations=10000, num_samples=1, num_eval_samples=int(100000.0), approx_eval=False, exact_eval=False, avg_trajectory=False, return_trajectory=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Runs solver on game.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      solver: gradient solver (see utils/updates.py)\\n      sym: bool, true if the game is symmetric across players\\n      num_iterations: int, number of incremental updates\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      num_eval_samples: int, number of samples to estimate exploitability\\n        default = # of samples for P[|sample_payoff-true| > C/100] < ~5e-7%\\n        where C = pt.max() - pt.min();\\n        P[|pt_grad|_inf <= C/100] > (1-5e-7)^num_actions\\n      approx_eval: bool, whether to evaluate exploitability during\\n        descent with stochastic samples\\n      exact_eval: bool, whether to evaluate exploitability during\\n        descent with exact expectation (req. full payoff tensor)\\n      avg_trajectory: bool, whether to evaluate w.r.t. the average distribution\\n        up to time t instead of the distribution at time t\\n      return_trajectory: bool, whether to record all parameters (e.g., dist)\\n        during learning and return them -- see solver code for details\\n    Returns:\\n      None -- dict of results stored in `results` attribute upon completion\\n        (key=name of metric, value=[m_0, ..., m_{last_iter}])\\n    \"\n    num_players = game.num_players()\n    num_strats = game.num_strategies()\n    if sym:\n        if len(set(num_strats)) != 1:\n            raise ValueError('Each player should have the same number of actions.')\n        num_strats = num_strats[0]\n    params = solver.init_vars(num_strats, num_players)\n    if sym:\n        dist_avg = np.zeros_like(params[0])\n        policies = list(range(num_strats))\n        num_ckpts = len(policies)\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_sym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_sym\n        exp = sym_exp\n        estimate_exploitability = self.estimate_exploitability_sym\n    else:\n        dist_avg = [np.zeros_like(dist_i) for dist_i in params[0]]\n        policies = [list(range(num_strats_i)) for num_strats_i in num_strats]\n        num_ckpts = [len(policy_i) for policy_i in policies]\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_nonsym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_nonsym\n        exp = nonsym_exp\n        estimate_exploitability = self.estimate_exploitability_nonsym\n    exps_exact = []\n    exps_solver_exact = []\n    exps_approx = []\n    exps_solver_approx = []\n    grad_norms = []\n    if return_trajectory:\n        params_traj = []\n    has_temp = False\n    if hasattr(solver, 'temperature') or hasattr(solver, 'p'):\n        has_temp = True\n        temperatures = []\n        if hasattr(solver, 'temperature'):\n            temp_attr = 'temperature'\n        else:\n            temp_attr = 'p'\n    early_exit = False\n    start = time.time()\n    for t in range(num_iterations + 1):\n        dist = params[0]\n        if return_trajectory:\n            params_traj.append(params)\n        if return_trajectory:\n            params_traj.append(params)\n        if has_temp:\n            temperatures.append(getattr(solver, temp_attr))\n        if num_samples < np.inf:\n            payoff_matrices = form_payoffs_appx(game, dist, num_samples, policies, num_players, num_ckpts)\n        else:\n            payoff_matrices = form_payoffs_exact(game, dist, num_players)\n        (grads, exp_sto, exp_solver_sto) = solver.compute_gradients(params, payoff_matrices)\n        if sym:\n            grads_dist = grads[0]\n            grad_norms.append(simplex.grad_norm(dist, grads_dist))\n        else:\n            grad_norm = 0.0\n            grads_dist = grads[0]\n            for (dist_i, grads_i) in zip(dist, grads_dist[0]):\n                grad_norm += simplex.grad_norm(dist_i, grads_i) ** 2.0\n            grad_norm = np.sqrt(grad_norm)\n            grad_norms.append(grad_norm)\n        if solver.has_aux:\n            solver.record_aux_errors(grads)\n        if sym:\n            dist_avg += (dist - dist_avg) / float(t + 1)\n        else:\n            for (i, dist_i) in enumerate(dist):\n                dist_avg[i] += (dist_i - dist_avg[i]) / float(t + 1)\n        if avg_trajectory:\n            dist_eval = dist_avg\n        else:\n            dist_eval = dist\n        if approx_eval:\n            exps_approx.append(exp_sto)\n            exps_solver_approx.append(exp_solver_sto)\n        if exact_eval:\n            pt = game.payoff_tensor()\n            exps_exact.append(exp.unreg_exploitability(dist_eval, pt))\n            exps_solver_exact.append(solver.exploitability(dist_eval, pt))\n        if t < num_iterations:\n            params = solver.update(params, grads, t)\n            if misc.isnan(params):\n                print('Warning: NaN detected in params post-update. Exiting loop.')\n                early_exit = True\n                break\n    end = time.time()\n    solve_runtime = end - start\n    start = end\n    exp_estimated = estimate_exploitability(dist_eval, num_eval_samples, num_ckpts, num_players, game, policies)\n    eval_runtime = time.time() - start\n    results = {'exps_approx': exps_approx, 'exps_solver_approx': exps_solver_approx, 'exps_exact': exps_exact, 'exps_solver_exact': exps_solver_exact, 'exp_estimated': exp_estimated, 'grad_norms': grad_norms, 'dist': dist, 'dist_avg': dist_avg, 'solve_runtime': solve_runtime, 'eval_runtime': eval_runtime, 'early_exit': early_exit}\n    if solver.has_aux:\n        results.update({'aux_errors': solver.aux_errors})\n    if return_trajectory:\n        results.update({'params_trajectory': params_traj})\n    if has_temp:\n        results.update({'temperatures': temperatures})\n    self.results = results",
            "def approximate_nash(self, game, solver, sym, num_iterations=10000, num_samples=1, num_eval_samples=int(100000.0), approx_eval=False, exact_eval=False, avg_trajectory=False, return_trajectory=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Runs solver on game.\\n\\n    Args:\\n      game: game with minimal functionality (see games/small.py)\\n      solver: gradient solver (see utils/updates.py)\\n      sym: bool, true if the game is symmetric across players\\n      num_iterations: int, number of incremental updates\\n      num_samples: int, `minibatch' size for stochastic gradient\\n      num_eval_samples: int, number of samples to estimate exploitability\\n        default = # of samples for P[|sample_payoff-true| > C/100] < ~5e-7%\\n        where C = pt.max() - pt.min();\\n        P[|pt_grad|_inf <= C/100] > (1-5e-7)^num_actions\\n      approx_eval: bool, whether to evaluate exploitability during\\n        descent with stochastic samples\\n      exact_eval: bool, whether to evaluate exploitability during\\n        descent with exact expectation (req. full payoff tensor)\\n      avg_trajectory: bool, whether to evaluate w.r.t. the average distribution\\n        up to time t instead of the distribution at time t\\n      return_trajectory: bool, whether to record all parameters (e.g., dist)\\n        during learning and return them -- see solver code for details\\n    Returns:\\n      None -- dict of results stored in `results` attribute upon completion\\n        (key=name of metric, value=[m_0, ..., m_{last_iter}])\\n    \"\n    num_players = game.num_players()\n    num_strats = game.num_strategies()\n    if sym:\n        if len(set(num_strats)) != 1:\n            raise ValueError('Each player should have the same number of actions.')\n        num_strats = num_strats[0]\n    params = solver.init_vars(num_strats, num_players)\n    if sym:\n        dist_avg = np.zeros_like(params[0])\n        policies = list(range(num_strats))\n        num_ckpts = len(policies)\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_sym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_sym\n        exp = sym_exp\n        estimate_exploitability = self.estimate_exploitability_sym\n    else:\n        dist_avg = [np.zeros_like(dist_i) for dist_i in params[0]]\n        policies = [list(range(num_strats_i)) for num_strats_i in num_strats]\n        num_ckpts = [len(policy_i) for policy_i in policies]\n        form_payoffs_appx = self.construct_payoff_matrices_from_samples_nonsym\n        form_payoffs_exact = self.construct_payoff_matrices_exactly_nonsym\n        exp = nonsym_exp\n        estimate_exploitability = self.estimate_exploitability_nonsym\n    exps_exact = []\n    exps_solver_exact = []\n    exps_approx = []\n    exps_solver_approx = []\n    grad_norms = []\n    if return_trajectory:\n        params_traj = []\n    has_temp = False\n    if hasattr(solver, 'temperature') or hasattr(solver, 'p'):\n        has_temp = True\n        temperatures = []\n        if hasattr(solver, 'temperature'):\n            temp_attr = 'temperature'\n        else:\n            temp_attr = 'p'\n    early_exit = False\n    start = time.time()\n    for t in range(num_iterations + 1):\n        dist = params[0]\n        if return_trajectory:\n            params_traj.append(params)\n        if return_trajectory:\n            params_traj.append(params)\n        if has_temp:\n            temperatures.append(getattr(solver, temp_attr))\n        if num_samples < np.inf:\n            payoff_matrices = form_payoffs_appx(game, dist, num_samples, policies, num_players, num_ckpts)\n        else:\n            payoff_matrices = form_payoffs_exact(game, dist, num_players)\n        (grads, exp_sto, exp_solver_sto) = solver.compute_gradients(params, payoff_matrices)\n        if sym:\n            grads_dist = grads[0]\n            grad_norms.append(simplex.grad_norm(dist, grads_dist))\n        else:\n            grad_norm = 0.0\n            grads_dist = grads[0]\n            for (dist_i, grads_i) in zip(dist, grads_dist[0]):\n                grad_norm += simplex.grad_norm(dist_i, grads_i) ** 2.0\n            grad_norm = np.sqrt(grad_norm)\n            grad_norms.append(grad_norm)\n        if solver.has_aux:\n            solver.record_aux_errors(grads)\n        if sym:\n            dist_avg += (dist - dist_avg) / float(t + 1)\n        else:\n            for (i, dist_i) in enumerate(dist):\n                dist_avg[i] += (dist_i - dist_avg[i]) / float(t + 1)\n        if avg_trajectory:\n            dist_eval = dist_avg\n        else:\n            dist_eval = dist\n        if approx_eval:\n            exps_approx.append(exp_sto)\n            exps_solver_approx.append(exp_solver_sto)\n        if exact_eval:\n            pt = game.payoff_tensor()\n            exps_exact.append(exp.unreg_exploitability(dist_eval, pt))\n            exps_solver_exact.append(solver.exploitability(dist_eval, pt))\n        if t < num_iterations:\n            params = solver.update(params, grads, t)\n            if misc.isnan(params):\n                print('Warning: NaN detected in params post-update. Exiting loop.')\n                early_exit = True\n                break\n    end = time.time()\n    solve_runtime = end - start\n    start = end\n    exp_estimated = estimate_exploitability(dist_eval, num_eval_samples, num_ckpts, num_players, game, policies)\n    eval_runtime = time.time() - start\n    results = {'exps_approx': exps_approx, 'exps_solver_approx': exps_solver_approx, 'exps_exact': exps_exact, 'exps_solver_exact': exps_solver_exact, 'exp_estimated': exp_estimated, 'grad_norms': grad_norms, 'dist': dist, 'dist_avg': dist_avg, 'solve_runtime': solve_runtime, 'eval_runtime': eval_runtime, 'early_exit': early_exit}\n    if solver.has_aux:\n        results.update({'aux_errors': solver.aux_errors})\n    if return_trajectory:\n        results.update({'params_trajectory': params_traj})\n    if has_temp:\n        results.update({'temperatures': temperatures})\n    self.results = results"
        ]
    }
]