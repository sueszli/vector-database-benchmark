[
    {
        "func_name": "discount_cumsum",
        "original": "def discount_cumsum(x, gamma: float=1.0) -> np.ndarray:\n    assert abs(gamma - 1.0) < 1e-05, 'gamma equals to 1.0 in original decision transformer paper'\n    disc_cumsum = np.zeros_like(x)\n    disc_cumsum[-1] = x[-1]\n    for t in reversed(range(x.shape[0] - 1)):\n        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t + 1]\n    return disc_cumsum",
        "mutated": [
            "def discount_cumsum(x, gamma: float=1.0) -> np.ndarray:\n    if False:\n        i = 10\n    assert abs(gamma - 1.0) < 1e-05, 'gamma equals to 1.0 in original decision transformer paper'\n    disc_cumsum = np.zeros_like(x)\n    disc_cumsum[-1] = x[-1]\n    for t in reversed(range(x.shape[0] - 1)):\n        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t + 1]\n    return disc_cumsum",
            "def discount_cumsum(x, gamma: float=1.0) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert abs(gamma - 1.0) < 1e-05, 'gamma equals to 1.0 in original decision transformer paper'\n    disc_cumsum = np.zeros_like(x)\n    disc_cumsum[-1] = x[-1]\n    for t in reversed(range(x.shape[0] - 1)):\n        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t + 1]\n    return disc_cumsum",
            "def discount_cumsum(x, gamma: float=1.0) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert abs(gamma - 1.0) < 1e-05, 'gamma equals to 1.0 in original decision transformer paper'\n    disc_cumsum = np.zeros_like(x)\n    disc_cumsum[-1] = x[-1]\n    for t in reversed(range(x.shape[0] - 1)):\n        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t + 1]\n    return disc_cumsum",
            "def discount_cumsum(x, gamma: float=1.0) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert abs(gamma - 1.0) < 1e-05, 'gamma equals to 1.0 in original decision transformer paper'\n    disc_cumsum = np.zeros_like(x)\n    disc_cumsum[-1] = x[-1]\n    for t in reversed(range(x.shape[0] - 1)):\n        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t + 1]\n    return disc_cumsum",
            "def discount_cumsum(x, gamma: float=1.0) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert abs(gamma - 1.0) < 1e-05, 'gamma equals to 1.0 in original decision transformer paper'\n    disc_cumsum = np.zeros_like(x)\n    disc_cumsum[-1] = x[-1]\n    for t in reversed(range(x.shape[0] - 1)):\n        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t + 1]\n    return disc_cumsum"
        ]
    },
    {
        "func_name": "q_1step_td_error",
        "original": "def q_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        1 step td_error, support single agent case and multi agent case.\n    Arguments:\n        - data (:obj:`q_1step_td_data`): The input data, q_1step_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n    Returns:\n        - loss (:obj:`torch.Tensor`): 1step td error\n    Shapes:\n        - data (:obj:`q_1step_td_data`): the q_1step_td_data containing             ['q', 'next_q', 'act', 'next_act', 'reward', 'done', 'weight']\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\n    Examples:\n        >>> action_dim = 4\n        >>> data = q_1step_td_data(\n        >>>     q=torch.randn(3, action_dim),\n        >>>     next_q=torch.randn(3, action_dim),\n        >>>     act=torch.randint(0, action_dim, (3,)),\n        >>>     next_act=torch.randint(0, action_dim, (3,)),\n        >>>     reward=torch.randn(3),\n        >>>     done=torch.randint(0, 2, (3,)).bool(),\n        >>>     weight=torch.ones(3),\n        >>> )\n        >>> loss = q_1step_td_error(data, 0.99)\n    \"\"\"\n    (q, next_q, act, next_act, reward, done, weight) = data\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_q_s_a = next_q[batch_range, next_act]\n    target_q_s_a = gamma * (1 - done) * target_q_s_a + reward\n    return (criterion(q_s_a, target_q_s_a.detach()) * weight).mean()",
        "mutated": [
            "def q_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        1 step td_error, support single agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`q_1step_td_data`): The input data, q_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error\\n    Shapes:\\n        - data (:obj:`q_1step_td_data`): the q_1step_td_data containing             ['q', 'next_q', 'act', 'next_act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = q_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     next_q=torch.randn(3, action_dim),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     next_act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)).bool(),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = q_1step_td_error(data, 0.99)\\n    \"\n    (q, next_q, act, next_act, reward, done, weight) = data\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_q_s_a = next_q[batch_range, next_act]\n    target_q_s_a = gamma * (1 - done) * target_q_s_a + reward\n    return (criterion(q_s_a, target_q_s_a.detach()) * weight).mean()",
            "def q_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        1 step td_error, support single agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`q_1step_td_data`): The input data, q_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error\\n    Shapes:\\n        - data (:obj:`q_1step_td_data`): the q_1step_td_data containing             ['q', 'next_q', 'act', 'next_act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = q_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     next_q=torch.randn(3, action_dim),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     next_act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)).bool(),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = q_1step_td_error(data, 0.99)\\n    \"\n    (q, next_q, act, next_act, reward, done, weight) = data\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_q_s_a = next_q[batch_range, next_act]\n    target_q_s_a = gamma * (1 - done) * target_q_s_a + reward\n    return (criterion(q_s_a, target_q_s_a.detach()) * weight).mean()",
            "def q_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        1 step td_error, support single agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`q_1step_td_data`): The input data, q_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error\\n    Shapes:\\n        - data (:obj:`q_1step_td_data`): the q_1step_td_data containing             ['q', 'next_q', 'act', 'next_act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = q_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     next_q=torch.randn(3, action_dim),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     next_act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)).bool(),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = q_1step_td_error(data, 0.99)\\n    \"\n    (q, next_q, act, next_act, reward, done, weight) = data\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_q_s_a = next_q[batch_range, next_act]\n    target_q_s_a = gamma * (1 - done) * target_q_s_a + reward\n    return (criterion(q_s_a, target_q_s_a.detach()) * weight).mean()",
            "def q_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        1 step td_error, support single agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`q_1step_td_data`): The input data, q_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error\\n    Shapes:\\n        - data (:obj:`q_1step_td_data`): the q_1step_td_data containing             ['q', 'next_q', 'act', 'next_act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = q_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     next_q=torch.randn(3, action_dim),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     next_act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)).bool(),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = q_1step_td_error(data, 0.99)\\n    \"\n    (q, next_q, act, next_act, reward, done, weight) = data\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_q_s_a = next_q[batch_range, next_act]\n    target_q_s_a = gamma * (1 - done) * target_q_s_a + reward\n    return (criterion(q_s_a, target_q_s_a.detach()) * weight).mean()",
            "def q_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        1 step td_error, support single agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`q_1step_td_data`): The input data, q_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error\\n    Shapes:\\n        - data (:obj:`q_1step_td_data`): the q_1step_td_data containing             ['q', 'next_q', 'act', 'next_act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = q_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     next_q=torch.randn(3, action_dim),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     next_act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)).bool(),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = q_1step_td_error(data, 0.99)\\n    \"\n    (q, next_q, act, next_act, reward, done, weight) = data\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_q_s_a = next_q[batch_range, next_act]\n    target_q_s_a = gamma * (1 - done) * target_q_s_a + reward\n    return (criterion(q_s_a, target_q_s_a.detach()) * weight).mean()"
        ]
    },
    {
        "func_name": "m_q_1step_td_error",
        "original": "def m_q_1step_td_error(data: namedtuple, gamma: float, tau: float, alpha: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Munchausen td_error for DQN algorithm, support 1 step td error.\n    Arguments:\n        - data (:obj:`m_q_1step_td_data`): The input data, m_q_1step_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - tau (:obj:`float`): Entropy factor for Munchausen DQN\n        - alpha (:obj:`float`): Discount factor for Munchausen term\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n    Returns:\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\n    Shapes:\n        - data (:obj:`m_q_1step_td_data`): the m_q_1step_td_data containing             ['q', 'target_q', 'next_q', 'act', 'reward', 'done', 'weight']\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - target_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\n    Examples:\n        >>> action_dim = 4\n        >>> data = m_q_1step_td_data(\n        >>>     q=torch.randn(3, action_dim),\n        >>>     target_q=torch.randn(3, action_dim),\n        >>>     next_q=torch.randn(3, action_dim),\n        >>>     act=torch.randint(0, action_dim, (3,)),\n        >>>     reward=torch.randn(3),\n        >>>     done=torch.randint(0, 2, (3,)),\n        >>>     weight=torch.ones(3),\n        >>> )\n        >>> loss = m_q_1step_td_error(data, 0.99, 0.01, 0.01)\n    \"\"\"\n    (q, target_q, next_q, act, reward, done, weight) = data\n    lower_bound = -1\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_v_s = target_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum = torch.logsumexp((target_q - target_v_s) / tau, 1).unsqueeze(-1)\n    log_pi = target_q - target_v_s - tau * logsum\n    act_get = act.unsqueeze(-1)\n    munchausen_addon = log_pi.gather(1, act_get)\n    muchausen_term = alpha * torch.clamp(munchausen_addon, min=lower_bound, max=1)\n    target_v_s_next = next_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum_next = torch.logsumexp((next_q - target_v_s_next) / tau, 1).unsqueeze(-1)\n    tau_log_pi_next = next_q - target_v_s_next - tau * logsum_next\n    pi_target = F.softmax((next_q - target_v_s_next) / tau)\n    target_q_s_a = (gamma * (pi_target * (next_q - tau_log_pi_next) * (1 - done.unsqueeze(-1))).sum(1)).unsqueeze(-1)\n    target_q_s_a = reward.unsqueeze(-1) + muchausen_term + target_q_s_a\n    td_error_per_sample = criterion(q_s_a.unsqueeze(-1), target_q_s_a.detach()).squeeze(-1)\n    with torch.no_grad():\n        top2_q_s = target_q[batch_range].topk(2, dim=1, largest=True, sorted=True)[0]\n        action_gap = (top2_q_s[:, 0] - top2_q_s[:, 1]).mean()\n        clipped = munchausen_addon.gt(1) | munchausen_addon.lt(lower_bound)\n        clipfrac = torch.as_tensor(clipped).float()\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, action_gap, clipfrac)",
        "mutated": [
            "def m_q_1step_td_error(data: namedtuple, gamma: float, tau: float, alpha: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Munchausen td_error for DQN algorithm, support 1 step td error.\\n    Arguments:\\n        - data (:obj:`m_q_1step_td_data`): The input data, m_q_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - tau (:obj:`float`): Entropy factor for Munchausen DQN\\n        - alpha (:obj:`float`): Discount factor for Munchausen term\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`m_q_1step_td_data`): the m_q_1step_td_data containing             ['q', 'target_q', 'next_q', 'act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - target_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = m_q_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     target_q=torch.randn(3, action_dim),\\n        >>>     next_q=torch.randn(3, action_dim),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = m_q_1step_td_error(data, 0.99, 0.01, 0.01)\\n    \"\n    (q, target_q, next_q, act, reward, done, weight) = data\n    lower_bound = -1\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_v_s = target_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum = torch.logsumexp((target_q - target_v_s) / tau, 1).unsqueeze(-1)\n    log_pi = target_q - target_v_s - tau * logsum\n    act_get = act.unsqueeze(-1)\n    munchausen_addon = log_pi.gather(1, act_get)\n    muchausen_term = alpha * torch.clamp(munchausen_addon, min=lower_bound, max=1)\n    target_v_s_next = next_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum_next = torch.logsumexp((next_q - target_v_s_next) / tau, 1).unsqueeze(-1)\n    tau_log_pi_next = next_q - target_v_s_next - tau * logsum_next\n    pi_target = F.softmax((next_q - target_v_s_next) / tau)\n    target_q_s_a = (gamma * (pi_target * (next_q - tau_log_pi_next) * (1 - done.unsqueeze(-1))).sum(1)).unsqueeze(-1)\n    target_q_s_a = reward.unsqueeze(-1) + muchausen_term + target_q_s_a\n    td_error_per_sample = criterion(q_s_a.unsqueeze(-1), target_q_s_a.detach()).squeeze(-1)\n    with torch.no_grad():\n        top2_q_s = target_q[batch_range].topk(2, dim=1, largest=True, sorted=True)[0]\n        action_gap = (top2_q_s[:, 0] - top2_q_s[:, 1]).mean()\n        clipped = munchausen_addon.gt(1) | munchausen_addon.lt(lower_bound)\n        clipfrac = torch.as_tensor(clipped).float()\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, action_gap, clipfrac)",
            "def m_q_1step_td_error(data: namedtuple, gamma: float, tau: float, alpha: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Munchausen td_error for DQN algorithm, support 1 step td error.\\n    Arguments:\\n        - data (:obj:`m_q_1step_td_data`): The input data, m_q_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - tau (:obj:`float`): Entropy factor for Munchausen DQN\\n        - alpha (:obj:`float`): Discount factor for Munchausen term\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`m_q_1step_td_data`): the m_q_1step_td_data containing             ['q', 'target_q', 'next_q', 'act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - target_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = m_q_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     target_q=torch.randn(3, action_dim),\\n        >>>     next_q=torch.randn(3, action_dim),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = m_q_1step_td_error(data, 0.99, 0.01, 0.01)\\n    \"\n    (q, target_q, next_q, act, reward, done, weight) = data\n    lower_bound = -1\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_v_s = target_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum = torch.logsumexp((target_q - target_v_s) / tau, 1).unsqueeze(-1)\n    log_pi = target_q - target_v_s - tau * logsum\n    act_get = act.unsqueeze(-1)\n    munchausen_addon = log_pi.gather(1, act_get)\n    muchausen_term = alpha * torch.clamp(munchausen_addon, min=lower_bound, max=1)\n    target_v_s_next = next_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum_next = torch.logsumexp((next_q - target_v_s_next) / tau, 1).unsqueeze(-1)\n    tau_log_pi_next = next_q - target_v_s_next - tau * logsum_next\n    pi_target = F.softmax((next_q - target_v_s_next) / tau)\n    target_q_s_a = (gamma * (pi_target * (next_q - tau_log_pi_next) * (1 - done.unsqueeze(-1))).sum(1)).unsqueeze(-1)\n    target_q_s_a = reward.unsqueeze(-1) + muchausen_term + target_q_s_a\n    td_error_per_sample = criterion(q_s_a.unsqueeze(-1), target_q_s_a.detach()).squeeze(-1)\n    with torch.no_grad():\n        top2_q_s = target_q[batch_range].topk(2, dim=1, largest=True, sorted=True)[0]\n        action_gap = (top2_q_s[:, 0] - top2_q_s[:, 1]).mean()\n        clipped = munchausen_addon.gt(1) | munchausen_addon.lt(lower_bound)\n        clipfrac = torch.as_tensor(clipped).float()\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, action_gap, clipfrac)",
            "def m_q_1step_td_error(data: namedtuple, gamma: float, tau: float, alpha: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Munchausen td_error for DQN algorithm, support 1 step td error.\\n    Arguments:\\n        - data (:obj:`m_q_1step_td_data`): The input data, m_q_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - tau (:obj:`float`): Entropy factor for Munchausen DQN\\n        - alpha (:obj:`float`): Discount factor for Munchausen term\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`m_q_1step_td_data`): the m_q_1step_td_data containing             ['q', 'target_q', 'next_q', 'act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - target_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = m_q_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     target_q=torch.randn(3, action_dim),\\n        >>>     next_q=torch.randn(3, action_dim),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = m_q_1step_td_error(data, 0.99, 0.01, 0.01)\\n    \"\n    (q, target_q, next_q, act, reward, done, weight) = data\n    lower_bound = -1\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_v_s = target_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum = torch.logsumexp((target_q - target_v_s) / tau, 1).unsqueeze(-1)\n    log_pi = target_q - target_v_s - tau * logsum\n    act_get = act.unsqueeze(-1)\n    munchausen_addon = log_pi.gather(1, act_get)\n    muchausen_term = alpha * torch.clamp(munchausen_addon, min=lower_bound, max=1)\n    target_v_s_next = next_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum_next = torch.logsumexp((next_q - target_v_s_next) / tau, 1).unsqueeze(-1)\n    tau_log_pi_next = next_q - target_v_s_next - tau * logsum_next\n    pi_target = F.softmax((next_q - target_v_s_next) / tau)\n    target_q_s_a = (gamma * (pi_target * (next_q - tau_log_pi_next) * (1 - done.unsqueeze(-1))).sum(1)).unsqueeze(-1)\n    target_q_s_a = reward.unsqueeze(-1) + muchausen_term + target_q_s_a\n    td_error_per_sample = criterion(q_s_a.unsqueeze(-1), target_q_s_a.detach()).squeeze(-1)\n    with torch.no_grad():\n        top2_q_s = target_q[batch_range].topk(2, dim=1, largest=True, sorted=True)[0]\n        action_gap = (top2_q_s[:, 0] - top2_q_s[:, 1]).mean()\n        clipped = munchausen_addon.gt(1) | munchausen_addon.lt(lower_bound)\n        clipfrac = torch.as_tensor(clipped).float()\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, action_gap, clipfrac)",
            "def m_q_1step_td_error(data: namedtuple, gamma: float, tau: float, alpha: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Munchausen td_error for DQN algorithm, support 1 step td error.\\n    Arguments:\\n        - data (:obj:`m_q_1step_td_data`): The input data, m_q_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - tau (:obj:`float`): Entropy factor for Munchausen DQN\\n        - alpha (:obj:`float`): Discount factor for Munchausen term\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`m_q_1step_td_data`): the m_q_1step_td_data containing             ['q', 'target_q', 'next_q', 'act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - target_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = m_q_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     target_q=torch.randn(3, action_dim),\\n        >>>     next_q=torch.randn(3, action_dim),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = m_q_1step_td_error(data, 0.99, 0.01, 0.01)\\n    \"\n    (q, target_q, next_q, act, reward, done, weight) = data\n    lower_bound = -1\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_v_s = target_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum = torch.logsumexp((target_q - target_v_s) / tau, 1).unsqueeze(-1)\n    log_pi = target_q - target_v_s - tau * logsum\n    act_get = act.unsqueeze(-1)\n    munchausen_addon = log_pi.gather(1, act_get)\n    muchausen_term = alpha * torch.clamp(munchausen_addon, min=lower_bound, max=1)\n    target_v_s_next = next_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum_next = torch.logsumexp((next_q - target_v_s_next) / tau, 1).unsqueeze(-1)\n    tau_log_pi_next = next_q - target_v_s_next - tau * logsum_next\n    pi_target = F.softmax((next_q - target_v_s_next) / tau)\n    target_q_s_a = (gamma * (pi_target * (next_q - tau_log_pi_next) * (1 - done.unsqueeze(-1))).sum(1)).unsqueeze(-1)\n    target_q_s_a = reward.unsqueeze(-1) + muchausen_term + target_q_s_a\n    td_error_per_sample = criterion(q_s_a.unsqueeze(-1), target_q_s_a.detach()).squeeze(-1)\n    with torch.no_grad():\n        top2_q_s = target_q[batch_range].topk(2, dim=1, largest=True, sorted=True)[0]\n        action_gap = (top2_q_s[:, 0] - top2_q_s[:, 1]).mean()\n        clipped = munchausen_addon.gt(1) | munchausen_addon.lt(lower_bound)\n        clipfrac = torch.as_tensor(clipped).float()\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, action_gap, clipfrac)",
            "def m_q_1step_td_error(data: namedtuple, gamma: float, tau: float, alpha: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Munchausen td_error for DQN algorithm, support 1 step td error.\\n    Arguments:\\n        - data (:obj:`m_q_1step_td_data`): The input data, m_q_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - tau (:obj:`float`): Entropy factor for Munchausen DQN\\n        - alpha (:obj:`float`): Discount factor for Munchausen term\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`m_q_1step_td_data`): the m_q_1step_td_data containing             ['q', 'target_q', 'next_q', 'act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - target_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = m_q_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     target_q=torch.randn(3, action_dim),\\n        >>>     next_q=torch.randn(3, action_dim),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = m_q_1step_td_error(data, 0.99, 0.01, 0.01)\\n    \"\n    (q, target_q, next_q, act, reward, done, weight) = data\n    lower_bound = -1\n    assert len(act.shape) == 1, act.shape\n    assert len(reward.shape) == 1, reward.shape\n    batch_range = torch.arange(act.shape[0])\n    if weight is None:\n        weight = torch.ones_like(reward)\n    q_s_a = q[batch_range, act]\n    target_v_s = target_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum = torch.logsumexp((target_q - target_v_s) / tau, 1).unsqueeze(-1)\n    log_pi = target_q - target_v_s - tau * logsum\n    act_get = act.unsqueeze(-1)\n    munchausen_addon = log_pi.gather(1, act_get)\n    muchausen_term = alpha * torch.clamp(munchausen_addon, min=lower_bound, max=1)\n    target_v_s_next = next_q[batch_range].max(1)[0].unsqueeze(-1)\n    logsum_next = torch.logsumexp((next_q - target_v_s_next) / tau, 1).unsqueeze(-1)\n    tau_log_pi_next = next_q - target_v_s_next - tau * logsum_next\n    pi_target = F.softmax((next_q - target_v_s_next) / tau)\n    target_q_s_a = (gamma * (pi_target * (next_q - tau_log_pi_next) * (1 - done.unsqueeze(-1))).sum(1)).unsqueeze(-1)\n    target_q_s_a = reward.unsqueeze(-1) + muchausen_term + target_q_s_a\n    td_error_per_sample = criterion(q_s_a.unsqueeze(-1), target_q_s_a.detach()).squeeze(-1)\n    with torch.no_grad():\n        top2_q_s = target_q[batch_range].topk(2, dim=1, largest=True, sorted=True)[0]\n        action_gap = (top2_q_s[:, 0] - top2_q_s[:, 1]).mean()\n        clipped = munchausen_addon.gt(1) | munchausen_addon.lt(lower_bound)\n        clipfrac = torch.as_tensor(clipped).float()\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, action_gap, clipfrac)"
        ]
    },
    {
        "func_name": "q_v_1step_td_error",
        "original": "def q_v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        td_error between q and v value for SAC algorithm, support 1 step td error.\n    Arguments:\n        - data (:obj:`q_v_1step_td_data`): The input data, q_v_1step_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n    Returns:\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\n    Shapes:\n        - data (:obj:`q_v_1step_td_data`): the q_v_1step_td_data containing             ['q', 'v', 'act', 'reward', 'done', 'weight']\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )`\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\n    Examples:\n        >>> action_dim = 4\n        >>> data = q_v_1step_td_data(\n        >>>     q=torch.randn(3, action_dim),\n        >>>     v=torch.randn(3),\n        >>>     act=torch.randint(0, action_dim, (3,)),\n        >>>     reward=torch.randn(3),\n        >>>     done=torch.randint(0, 2, (3,)),\n        >>>     weight=torch.ones(3),\n        >>> )\n        >>> loss = q_v_1step_td_error(data, 0.99)\n    \"\"\"\n    (q, v, act, reward, done, weight) = data\n    if len(act.shape) == 1:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        q_s_a = q[batch_range, act]\n        target_q_s_a = gamma * (1 - done) * v + reward\n    else:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        actor_range = torch.arange(act.shape[1])\n        batch_actor_range = torch.arange(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(act)\n        temp_q = q.reshape(act.shape[0] * act.shape[1], -1)\n        temp_act = act.reshape(act.shape[0] * act.shape[1])\n        q_s_a = temp_q[batch_actor_range, temp_act]\n        q_s_a = q_s_a.reshape(act.shape[0], act.shape[1])\n        target_q_s_a = gamma * (1 - done).unsqueeze(1) * v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
        "mutated": [
            "def q_v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        td_error between q and v value for SAC algorithm, support 1 step td error.\\n    Arguments:\\n        - data (:obj:`q_v_1step_td_data`): The input data, q_v_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_v_1step_td_data`): the q_v_1step_td_data containing             ['q', 'v', 'act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = q_v_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     v=torch.randn(3),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = q_v_1step_td_error(data, 0.99)\\n    \"\n    (q, v, act, reward, done, weight) = data\n    if len(act.shape) == 1:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        q_s_a = q[batch_range, act]\n        target_q_s_a = gamma * (1 - done) * v + reward\n    else:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        actor_range = torch.arange(act.shape[1])\n        batch_actor_range = torch.arange(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(act)\n        temp_q = q.reshape(act.shape[0] * act.shape[1], -1)\n        temp_act = act.reshape(act.shape[0] * act.shape[1])\n        q_s_a = temp_q[batch_actor_range, temp_act]\n        q_s_a = q_s_a.reshape(act.shape[0], act.shape[1])\n        target_q_s_a = gamma * (1 - done).unsqueeze(1) * v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def q_v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        td_error between q and v value for SAC algorithm, support 1 step td error.\\n    Arguments:\\n        - data (:obj:`q_v_1step_td_data`): The input data, q_v_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_v_1step_td_data`): the q_v_1step_td_data containing             ['q', 'v', 'act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = q_v_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     v=torch.randn(3),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = q_v_1step_td_error(data, 0.99)\\n    \"\n    (q, v, act, reward, done, weight) = data\n    if len(act.shape) == 1:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        q_s_a = q[batch_range, act]\n        target_q_s_a = gamma * (1 - done) * v + reward\n    else:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        actor_range = torch.arange(act.shape[1])\n        batch_actor_range = torch.arange(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(act)\n        temp_q = q.reshape(act.shape[0] * act.shape[1], -1)\n        temp_act = act.reshape(act.shape[0] * act.shape[1])\n        q_s_a = temp_q[batch_actor_range, temp_act]\n        q_s_a = q_s_a.reshape(act.shape[0], act.shape[1])\n        target_q_s_a = gamma * (1 - done).unsqueeze(1) * v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def q_v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        td_error between q and v value for SAC algorithm, support 1 step td error.\\n    Arguments:\\n        - data (:obj:`q_v_1step_td_data`): The input data, q_v_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_v_1step_td_data`): the q_v_1step_td_data containing             ['q', 'v', 'act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = q_v_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     v=torch.randn(3),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = q_v_1step_td_error(data, 0.99)\\n    \"\n    (q, v, act, reward, done, weight) = data\n    if len(act.shape) == 1:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        q_s_a = q[batch_range, act]\n        target_q_s_a = gamma * (1 - done) * v + reward\n    else:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        actor_range = torch.arange(act.shape[1])\n        batch_actor_range = torch.arange(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(act)\n        temp_q = q.reshape(act.shape[0] * act.shape[1], -1)\n        temp_act = act.reshape(act.shape[0] * act.shape[1])\n        q_s_a = temp_q[batch_actor_range, temp_act]\n        q_s_a = q_s_a.reshape(act.shape[0], act.shape[1])\n        target_q_s_a = gamma * (1 - done).unsqueeze(1) * v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def q_v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        td_error between q and v value for SAC algorithm, support 1 step td error.\\n    Arguments:\\n        - data (:obj:`q_v_1step_td_data`): The input data, q_v_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_v_1step_td_data`): the q_v_1step_td_data containing             ['q', 'v', 'act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = q_v_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     v=torch.randn(3),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = q_v_1step_td_error(data, 0.99)\\n    \"\n    (q, v, act, reward, done, weight) = data\n    if len(act.shape) == 1:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        q_s_a = q[batch_range, act]\n        target_q_s_a = gamma * (1 - done) * v + reward\n    else:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        actor_range = torch.arange(act.shape[1])\n        batch_actor_range = torch.arange(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(act)\n        temp_q = q.reshape(act.shape[0] * act.shape[1], -1)\n        temp_act = act.reshape(act.shape[0] * act.shape[1])\n        q_s_a = temp_q[batch_actor_range, temp_act]\n        q_s_a = q_s_a.reshape(act.shape[0], act.shape[1])\n        target_q_s_a = gamma * (1 - done).unsqueeze(1) * v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def q_v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        td_error between q and v value for SAC algorithm, support 1 step td error.\\n    Arguments:\\n        - data (:obj:`q_v_1step_td_data`): The input data, q_v_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_v_1step_td_data`): the q_v_1step_td_data containing             ['q', 'v', 'act', 'reward', 'done', 'weight']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> action_dim = 4\\n        >>> data = q_v_1step_td_data(\\n        >>>     q=torch.randn(3, action_dim),\\n        >>>     v=torch.randn(3),\\n        >>>     act=torch.randint(0, action_dim, (3,)),\\n        >>>     reward=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>>     weight=torch.ones(3),\\n        >>> )\\n        >>> loss = q_v_1step_td_error(data, 0.99)\\n    \"\n    (q, v, act, reward, done, weight) = data\n    if len(act.shape) == 1:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        q_s_a = q[batch_range, act]\n        target_q_s_a = gamma * (1 - done) * v + reward\n    else:\n        assert len(reward.shape) == 1, reward.shape\n        batch_range = torch.arange(act.shape[0])\n        actor_range = torch.arange(act.shape[1])\n        batch_actor_range = torch.arange(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(act)\n        temp_q = q.reshape(act.shape[0] * act.shape[1], -1)\n        temp_act = act.reshape(act.shape[0] * act.shape[1])\n        q_s_a = temp_q[batch_actor_range, temp_act]\n        q_s_a = q_s_a.reshape(act.shape[0], act.shape[1])\n        target_q_s_a = gamma * (1 - done).unsqueeze(1) * v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)"
        ]
    },
    {
        "func_name": "view_similar",
        "original": "def view_similar(x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n    size = list(x.shape) + [1 for _ in range(len(target.shape) - len(x.shape))]\n    return x.view(*size)",
        "mutated": [
            "def view_similar(x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n    size = list(x.shape) + [1 for _ in range(len(target.shape) - len(x.shape))]\n    return x.view(*size)",
            "def view_similar(x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    size = list(x.shape) + [1 for _ in range(len(target.shape) - len(x.shape))]\n    return x.view(*size)",
            "def view_similar(x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    size = list(x.shape) + [1 for _ in range(len(target.shape) - len(x.shape))]\n    return x.view(*size)",
            "def view_similar(x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    size = list(x.shape) + [1 for _ in range(len(target.shape) - len(x.shape))]\n    return x.view(*size)",
            "def view_similar(x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    size = list(x.shape) + [1 for _ in range(len(target.shape) - len(x.shape))]\n    return x.view(*size)"
        ]
    },
    {
        "func_name": "nstep_return",
        "original": "def nstep_return(data: namedtuple, gamma: Union[float, list], nstep: int, value_gamma: Optional[torch.Tensor]=None):\n    \"\"\"\n    Overview:\n        Calculate nstep return for DQN algorithm, support single agent case and multi agent case.\n    Arguments:\n        - data (:obj:`nstep_return_data`): The input data, nstep_return_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - nstep (:obj:`int`): nstep num\n        - value_gamma (:obj:`torch.Tensor`): Discount factor for value\n    Returns:\n        - return (:obj:`torch.Tensor`): nstep return\n    Shapes:\n        - data (:obj:`nstep_return_data`): the nstep_return_data containing             ['reward', 'next_value', 'done']\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - next_value (:obj:`torch.FloatTensor`): :math:`(, B)`\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n    Examples:\n        >>> data = nstep_return_data(\n        >>>     reward=torch.randn(3, 3),\n        >>>     next_value=torch.randn(3),\n        >>>     done=torch.randint(0, 2, (3,)),\n        >>> )\n        >>> loss = nstep_return(data, 0.99, 3)\n    \"\"\"\n    (reward, next_value, done) = data\n    assert reward.shape[0] == nstep\n    device = reward.device\n    if isinstance(gamma, float):\n        reward_factor = torch.ones(nstep).to(device)\n        for i in range(1, nstep):\n            reward_factor[i] = gamma * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor).sum(0)\n        if value_gamma is None:\n            return_ = return_tmp + gamma ** nstep * next_value * (1 - done)\n        else:\n            return_ = return_tmp + value_gamma * next_value * (1 - done)\n    elif isinstance(gamma, list):\n        reward_factor = torch.ones([nstep + 1, done.shape[0]]).to(device)\n        for i in range(1, nstep + 1):\n            reward_factor[i] = torch.stack(gamma, dim=0).to(device) * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor[:nstep]).sum(0)\n        return_ = return_tmp + reward_factor[nstep] * next_value * (1 - done)\n    else:\n        raise TypeError('The type of gamma should be float or list')\n    return return_",
        "mutated": [
            "def nstep_return(data: namedtuple, gamma: Union[float, list], nstep: int, value_gamma: Optional[torch.Tensor]=None):\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Calculate nstep return for DQN algorithm, support single agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`nstep_return_data`): The input data, nstep_return_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num\\n        - value_gamma (:obj:`torch.Tensor`): Discount factor for value\\n    Returns:\\n        - return (:obj:`torch.Tensor`): nstep return\\n    Shapes:\\n        - data (:obj:`nstep_return_data`): the nstep_return_data containing             ['reward', 'next_value', 'done']\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - next_value (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> data = nstep_return_data(\\n        >>>     reward=torch.randn(3, 3),\\n        >>>     next_value=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>> )\\n        >>> loss = nstep_return(data, 0.99, 3)\\n    \"\n    (reward, next_value, done) = data\n    assert reward.shape[0] == nstep\n    device = reward.device\n    if isinstance(gamma, float):\n        reward_factor = torch.ones(nstep).to(device)\n        for i in range(1, nstep):\n            reward_factor[i] = gamma * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor).sum(0)\n        if value_gamma is None:\n            return_ = return_tmp + gamma ** nstep * next_value * (1 - done)\n        else:\n            return_ = return_tmp + value_gamma * next_value * (1 - done)\n    elif isinstance(gamma, list):\n        reward_factor = torch.ones([nstep + 1, done.shape[0]]).to(device)\n        for i in range(1, nstep + 1):\n            reward_factor[i] = torch.stack(gamma, dim=0).to(device) * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor[:nstep]).sum(0)\n        return_ = return_tmp + reward_factor[nstep] * next_value * (1 - done)\n    else:\n        raise TypeError('The type of gamma should be float or list')\n    return return_",
            "def nstep_return(data: namedtuple, gamma: Union[float, list], nstep: int, value_gamma: Optional[torch.Tensor]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Calculate nstep return for DQN algorithm, support single agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`nstep_return_data`): The input data, nstep_return_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num\\n        - value_gamma (:obj:`torch.Tensor`): Discount factor for value\\n    Returns:\\n        - return (:obj:`torch.Tensor`): nstep return\\n    Shapes:\\n        - data (:obj:`nstep_return_data`): the nstep_return_data containing             ['reward', 'next_value', 'done']\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - next_value (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> data = nstep_return_data(\\n        >>>     reward=torch.randn(3, 3),\\n        >>>     next_value=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>> )\\n        >>> loss = nstep_return(data, 0.99, 3)\\n    \"\n    (reward, next_value, done) = data\n    assert reward.shape[0] == nstep\n    device = reward.device\n    if isinstance(gamma, float):\n        reward_factor = torch.ones(nstep).to(device)\n        for i in range(1, nstep):\n            reward_factor[i] = gamma * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor).sum(0)\n        if value_gamma is None:\n            return_ = return_tmp + gamma ** nstep * next_value * (1 - done)\n        else:\n            return_ = return_tmp + value_gamma * next_value * (1 - done)\n    elif isinstance(gamma, list):\n        reward_factor = torch.ones([nstep + 1, done.shape[0]]).to(device)\n        for i in range(1, nstep + 1):\n            reward_factor[i] = torch.stack(gamma, dim=0).to(device) * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor[:nstep]).sum(0)\n        return_ = return_tmp + reward_factor[nstep] * next_value * (1 - done)\n    else:\n        raise TypeError('The type of gamma should be float or list')\n    return return_",
            "def nstep_return(data: namedtuple, gamma: Union[float, list], nstep: int, value_gamma: Optional[torch.Tensor]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Calculate nstep return for DQN algorithm, support single agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`nstep_return_data`): The input data, nstep_return_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num\\n        - value_gamma (:obj:`torch.Tensor`): Discount factor for value\\n    Returns:\\n        - return (:obj:`torch.Tensor`): nstep return\\n    Shapes:\\n        - data (:obj:`nstep_return_data`): the nstep_return_data containing             ['reward', 'next_value', 'done']\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - next_value (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> data = nstep_return_data(\\n        >>>     reward=torch.randn(3, 3),\\n        >>>     next_value=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>> )\\n        >>> loss = nstep_return(data, 0.99, 3)\\n    \"\n    (reward, next_value, done) = data\n    assert reward.shape[0] == nstep\n    device = reward.device\n    if isinstance(gamma, float):\n        reward_factor = torch.ones(nstep).to(device)\n        for i in range(1, nstep):\n            reward_factor[i] = gamma * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor).sum(0)\n        if value_gamma is None:\n            return_ = return_tmp + gamma ** nstep * next_value * (1 - done)\n        else:\n            return_ = return_tmp + value_gamma * next_value * (1 - done)\n    elif isinstance(gamma, list):\n        reward_factor = torch.ones([nstep + 1, done.shape[0]]).to(device)\n        for i in range(1, nstep + 1):\n            reward_factor[i] = torch.stack(gamma, dim=0).to(device) * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor[:nstep]).sum(0)\n        return_ = return_tmp + reward_factor[nstep] * next_value * (1 - done)\n    else:\n        raise TypeError('The type of gamma should be float or list')\n    return return_",
            "def nstep_return(data: namedtuple, gamma: Union[float, list], nstep: int, value_gamma: Optional[torch.Tensor]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Calculate nstep return for DQN algorithm, support single agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`nstep_return_data`): The input data, nstep_return_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num\\n        - value_gamma (:obj:`torch.Tensor`): Discount factor for value\\n    Returns:\\n        - return (:obj:`torch.Tensor`): nstep return\\n    Shapes:\\n        - data (:obj:`nstep_return_data`): the nstep_return_data containing             ['reward', 'next_value', 'done']\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - next_value (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> data = nstep_return_data(\\n        >>>     reward=torch.randn(3, 3),\\n        >>>     next_value=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>> )\\n        >>> loss = nstep_return(data, 0.99, 3)\\n    \"\n    (reward, next_value, done) = data\n    assert reward.shape[0] == nstep\n    device = reward.device\n    if isinstance(gamma, float):\n        reward_factor = torch.ones(nstep).to(device)\n        for i in range(1, nstep):\n            reward_factor[i] = gamma * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor).sum(0)\n        if value_gamma is None:\n            return_ = return_tmp + gamma ** nstep * next_value * (1 - done)\n        else:\n            return_ = return_tmp + value_gamma * next_value * (1 - done)\n    elif isinstance(gamma, list):\n        reward_factor = torch.ones([nstep + 1, done.shape[0]]).to(device)\n        for i in range(1, nstep + 1):\n            reward_factor[i] = torch.stack(gamma, dim=0).to(device) * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor[:nstep]).sum(0)\n        return_ = return_tmp + reward_factor[nstep] * next_value * (1 - done)\n    else:\n        raise TypeError('The type of gamma should be float or list')\n    return return_",
            "def nstep_return(data: namedtuple, gamma: Union[float, list], nstep: int, value_gamma: Optional[torch.Tensor]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Calculate nstep return for DQN algorithm, support single agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`nstep_return_data`): The input data, nstep_return_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num\\n        - value_gamma (:obj:`torch.Tensor`): Discount factor for value\\n    Returns:\\n        - return (:obj:`torch.Tensor`): nstep return\\n    Shapes:\\n        - data (:obj:`nstep_return_data`): the nstep_return_data containing             ['reward', 'next_value', 'done']\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - next_value (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> data = nstep_return_data(\\n        >>>     reward=torch.randn(3, 3),\\n        >>>     next_value=torch.randn(3),\\n        >>>     done=torch.randint(0, 2, (3,)),\\n        >>> )\\n        >>> loss = nstep_return(data, 0.99, 3)\\n    \"\n    (reward, next_value, done) = data\n    assert reward.shape[0] == nstep\n    device = reward.device\n    if isinstance(gamma, float):\n        reward_factor = torch.ones(nstep).to(device)\n        for i in range(1, nstep):\n            reward_factor[i] = gamma * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor).sum(0)\n        if value_gamma is None:\n            return_ = return_tmp + gamma ** nstep * next_value * (1 - done)\n        else:\n            return_ = return_tmp + value_gamma * next_value * (1 - done)\n    elif isinstance(gamma, list):\n        reward_factor = torch.ones([nstep + 1, done.shape[0]]).to(device)\n        for i in range(1, nstep + 1):\n            reward_factor[i] = torch.stack(gamma, dim=0).to(device) * reward_factor[i - 1]\n        reward_factor = view_similar(reward_factor, reward)\n        return_tmp = reward.mul(reward_factor[:nstep]).sum(0)\n        return_ = return_tmp + reward_factor[nstep] * next_value * (1 - done)\n    else:\n        raise TypeError('The type of gamma should be float or list')\n    return return_"
        ]
    },
    {
        "func_name": "dist_1step_td_error",
        "original": "def dist_1step_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        1 step td_error for distributed q-learning based algorithm\n    Arguments:\n        - data (:obj:`dist_1step_td_data`): The input data, dist_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - v_min (:obj:`float`): The min value of support\n        - v_max (:obj:`float`): The max value of support\n        - n_atom (:obj:`int`): The num of atom\n    Returns:\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\n    Shapes:\n        - data (:obj:`dist_1step_td_data`): the dist_1step_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\n        - next_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\n    Examples:\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\n        >>> next_dist = torch.randn(4, 3, 51).abs()\n        >>> act = torch.randint(0, 3, (4,))\n        >>> next_act = torch.randint(0, 3, (4,))\n        >>> reward = torch.randn(4)\n        >>> done = torch.randint(0, 2, (4,))\n        >>> data = dist_1step_td_data(dist, next_dist, act, next_act, reward, done, None)\n        >>> loss = dist_1step_td_error(data, 0.99, -10.0, 10.0, 51)\n    \"\"\"\n    (dist, next_dist, act, next_act, reward, done, weight) = data\n    device = reward.device\n    assert len(reward.shape) == 1, reward.shape\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        next_dist = next_dist[batch_range, next_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_act = next_act.reshape(act.shape[0] * act.shape[1])\n        next_dist = next_dist[batch_range, next_act].detach()\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n    target_z = reward + (1 - done) * gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n    log_p = torch.log(dist[batch_range, act])\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return loss",
        "mutated": [
            "def dist_1step_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        1 step td_error for distributed q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`dist_1step_td_data`): The input data, dist_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - v_min (:obj:`float`): The min value of support\\n        - v_max (:obj:`float`): The max value of support\\n        - n_atom (:obj:`int`): The num of atom\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_1step_td_data`): the dist_1step_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\\n        - next_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\\n        >>> next_dist = torch.randn(4, 3, 51).abs()\\n        >>> act = torch.randint(0, 3, (4,))\\n        >>> next_act = torch.randint(0, 3, (4,))\\n        >>> reward = torch.randn(4)\\n        >>> done = torch.randint(0, 2, (4,))\\n        >>> data = dist_1step_td_data(dist, next_dist, act, next_act, reward, done, None)\\n        >>> loss = dist_1step_td_error(data, 0.99, -10.0, 10.0, 51)\\n    \"\n    (dist, next_dist, act, next_act, reward, done, weight) = data\n    device = reward.device\n    assert len(reward.shape) == 1, reward.shape\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        next_dist = next_dist[batch_range, next_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_act = next_act.reshape(act.shape[0] * act.shape[1])\n        next_dist = next_dist[batch_range, next_act].detach()\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n    target_z = reward + (1 - done) * gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n    log_p = torch.log(dist[batch_range, act])\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return loss",
            "def dist_1step_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        1 step td_error for distributed q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`dist_1step_td_data`): The input data, dist_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - v_min (:obj:`float`): The min value of support\\n        - v_max (:obj:`float`): The max value of support\\n        - n_atom (:obj:`int`): The num of atom\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_1step_td_data`): the dist_1step_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\\n        - next_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\\n        >>> next_dist = torch.randn(4, 3, 51).abs()\\n        >>> act = torch.randint(0, 3, (4,))\\n        >>> next_act = torch.randint(0, 3, (4,))\\n        >>> reward = torch.randn(4)\\n        >>> done = torch.randint(0, 2, (4,))\\n        >>> data = dist_1step_td_data(dist, next_dist, act, next_act, reward, done, None)\\n        >>> loss = dist_1step_td_error(data, 0.99, -10.0, 10.0, 51)\\n    \"\n    (dist, next_dist, act, next_act, reward, done, weight) = data\n    device = reward.device\n    assert len(reward.shape) == 1, reward.shape\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        next_dist = next_dist[batch_range, next_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_act = next_act.reshape(act.shape[0] * act.shape[1])\n        next_dist = next_dist[batch_range, next_act].detach()\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n    target_z = reward + (1 - done) * gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n    log_p = torch.log(dist[batch_range, act])\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return loss",
            "def dist_1step_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        1 step td_error for distributed q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`dist_1step_td_data`): The input data, dist_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - v_min (:obj:`float`): The min value of support\\n        - v_max (:obj:`float`): The max value of support\\n        - n_atom (:obj:`int`): The num of atom\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_1step_td_data`): the dist_1step_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\\n        - next_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\\n        >>> next_dist = torch.randn(4, 3, 51).abs()\\n        >>> act = torch.randint(0, 3, (4,))\\n        >>> next_act = torch.randint(0, 3, (4,))\\n        >>> reward = torch.randn(4)\\n        >>> done = torch.randint(0, 2, (4,))\\n        >>> data = dist_1step_td_data(dist, next_dist, act, next_act, reward, done, None)\\n        >>> loss = dist_1step_td_error(data, 0.99, -10.0, 10.0, 51)\\n    \"\n    (dist, next_dist, act, next_act, reward, done, weight) = data\n    device = reward.device\n    assert len(reward.shape) == 1, reward.shape\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        next_dist = next_dist[batch_range, next_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_act = next_act.reshape(act.shape[0] * act.shape[1])\n        next_dist = next_dist[batch_range, next_act].detach()\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n    target_z = reward + (1 - done) * gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n    log_p = torch.log(dist[batch_range, act])\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return loss",
            "def dist_1step_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        1 step td_error for distributed q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`dist_1step_td_data`): The input data, dist_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - v_min (:obj:`float`): The min value of support\\n        - v_max (:obj:`float`): The max value of support\\n        - n_atom (:obj:`int`): The num of atom\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_1step_td_data`): the dist_1step_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\\n        - next_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\\n        >>> next_dist = torch.randn(4, 3, 51).abs()\\n        >>> act = torch.randint(0, 3, (4,))\\n        >>> next_act = torch.randint(0, 3, (4,))\\n        >>> reward = torch.randn(4)\\n        >>> done = torch.randint(0, 2, (4,))\\n        >>> data = dist_1step_td_data(dist, next_dist, act, next_act, reward, done, None)\\n        >>> loss = dist_1step_td_error(data, 0.99, -10.0, 10.0, 51)\\n    \"\n    (dist, next_dist, act, next_act, reward, done, weight) = data\n    device = reward.device\n    assert len(reward.shape) == 1, reward.shape\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        next_dist = next_dist[batch_range, next_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_act = next_act.reshape(act.shape[0] * act.shape[1])\n        next_dist = next_dist[batch_range, next_act].detach()\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n    target_z = reward + (1 - done) * gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n    log_p = torch.log(dist[batch_range, act])\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return loss",
            "def dist_1step_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        1 step td_error for distributed q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`dist_1step_td_data`): The input data, dist_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - v_min (:obj:`float`): The min value of support\\n        - v_max (:obj:`float`): The max value of support\\n        - n_atom (:obj:`int`): The num of atom\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_1step_td_data`): the dist_1step_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\\n        - next_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\\n        >>> next_dist = torch.randn(4, 3, 51).abs()\\n        >>> act = torch.randint(0, 3, (4,))\\n        >>> next_act = torch.randint(0, 3, (4,))\\n        >>> reward = torch.randn(4)\\n        >>> done = torch.randint(0, 2, (4,))\\n        >>> data = dist_1step_td_data(dist, next_dist, act, next_act, reward, done, None)\\n        >>> loss = dist_1step_td_error(data, 0.99, -10.0, 10.0, 51)\\n    \"\n    (dist, next_dist, act, next_act, reward, done, weight) = data\n    device = reward.device\n    assert len(reward.shape) == 1, reward.shape\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        next_dist = next_dist[batch_range, next_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_act = next_act.reshape(act.shape[0] * act.shape[1])\n        next_dist = next_dist[batch_range, next_act].detach()\n        next_dist = next_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n    target_z = reward + (1 - done) * gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n    log_p = torch.log(dist[batch_range, act])\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return loss"
        ]
    },
    {
        "func_name": "shape_fn_dntd",
        "original": "def shape_fn_dntd(args, kwargs):\n    \"\"\"\n    Overview:\n        Return dntd shape for hpc\n    Returns:\n        shape: [T, B, N, n_atom]\n    \"\"\"\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].dist.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].dist.shape))\n    return tmp",
        "mutated": [
            "def shape_fn_dntd(args, kwargs):\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Return dntd shape for hpc\\n    Returns:\\n        shape: [T, B, N, n_atom]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].dist.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].dist.shape))\n    return tmp",
            "def shape_fn_dntd(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Return dntd shape for hpc\\n    Returns:\\n        shape: [T, B, N, n_atom]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].dist.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].dist.shape))\n    return tmp",
            "def shape_fn_dntd(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Return dntd shape for hpc\\n    Returns:\\n        shape: [T, B, N, n_atom]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].dist.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].dist.shape))\n    return tmp",
            "def shape_fn_dntd(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Return dntd shape for hpc\\n    Returns:\\n        shape: [T, B, N, n_atom]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].dist.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].dist.shape))\n    return tmp",
            "def shape_fn_dntd(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Return dntd shape for hpc\\n    Returns:\\n        shape: [T, B, N, n_atom]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].dist.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].dist.shape))\n    return tmp"
        ]
    },
    {
        "func_name": "dist_nstep_td_error",
        "original": "@hpc_wrapper(shape_fn=shape_fn_dntd, namedtuple_data=True, include_args=[0, 1, 2, 3], include_kwargs=['data', 'gamma', 'v_min', 'v_max'])\ndef dist_nstep_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep (1 step or n step) td_error for distributed q-learning based algorithm, support single            agent case and multi agent case.\n    Arguments:\n        - data (:obj:`dist_nstep_td_data`): The input data, dist_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - nstep (:obj:`int`): nstep num, default set to 1\n    Returns:\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\n    Shapes:\n        - data (:obj:`dist_nstep_td_data`): the dist_nstep_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\n        - next_n_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_n_act (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n    Examples:\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\n        >>> next_n_dist = torch.randn(4, 3, 51).abs()\n        >>> done = torch.randn(4)\n        >>> action = torch.randint(0, 3, size=(4, ))\n        >>> next_action = torch.randint(0, 3, size=(4, ))\n        >>> reward = torch.randn(5, 4)\n        >>> data = dist_nstep_td_data(dist, next_n_dist, action, next_action, reward, done, None)\n        >>> loss, _ = dist_nstep_td_error(data, 0.95, -10.0, 10.0, 51, 5)\n    \"\"\"\n    (dist, next_n_dist, act, next_n_act, reward, done, weight) = data\n    device = reward.device\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_n_act = next_n_act.reshape(act.shape[0] * act.shape[1])\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n    if value_gamma is None:\n        target_z = reward + (1 - done) * gamma ** nstep * support\n    elif isinstance(value_gamma, float):\n        value_gamma = torch.tensor(value_gamma).unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    else:\n        value_gamma = value_gamma.unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_n_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_n_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_n_dist * (b - l.float())).view(-1))\n    assert (dist[batch_range, act] > 0.0).all(), ('dist act', dist[batch_range, act], 'dist:', dist)\n    log_p = torch.log(dist[batch_range, act])\n    if len(weight.shape) == 1:\n        weight = weight.unsqueeze(-1)\n    td_error_per_sample = -(log_p * proj_dist).sum(-1)\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return (loss, td_error_per_sample)",
        "mutated": [
            "@hpc_wrapper(shape_fn=shape_fn_dntd, namedtuple_data=True, include_args=[0, 1, 2, 3], include_kwargs=['data', 'gamma', 'v_min', 'v_max'])\ndef dist_nstep_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for distributed q-learning based algorithm, support single            agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`dist_nstep_td_data`): The input data, dist_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_nstep_td_data`): the dist_nstep_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\\n        - next_n_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\\n        >>> next_n_dist = torch.randn(4, 3, 51).abs()\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> reward = torch.randn(5, 4)\\n        >>> data = dist_nstep_td_data(dist, next_n_dist, action, next_action, reward, done, None)\\n        >>> loss, _ = dist_nstep_td_error(data, 0.95, -10.0, 10.0, 51, 5)\\n    \"\n    (dist, next_n_dist, act, next_n_act, reward, done, weight) = data\n    device = reward.device\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_n_act = next_n_act.reshape(act.shape[0] * act.shape[1])\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n    if value_gamma is None:\n        target_z = reward + (1 - done) * gamma ** nstep * support\n    elif isinstance(value_gamma, float):\n        value_gamma = torch.tensor(value_gamma).unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    else:\n        value_gamma = value_gamma.unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_n_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_n_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_n_dist * (b - l.float())).view(-1))\n    assert (dist[batch_range, act] > 0.0).all(), ('dist act', dist[batch_range, act], 'dist:', dist)\n    log_p = torch.log(dist[batch_range, act])\n    if len(weight.shape) == 1:\n        weight = weight.unsqueeze(-1)\n    td_error_per_sample = -(log_p * proj_dist).sum(-1)\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return (loss, td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_dntd, namedtuple_data=True, include_args=[0, 1, 2, 3], include_kwargs=['data', 'gamma', 'v_min', 'v_max'])\ndef dist_nstep_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for distributed q-learning based algorithm, support single            agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`dist_nstep_td_data`): The input data, dist_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_nstep_td_data`): the dist_nstep_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\\n        - next_n_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\\n        >>> next_n_dist = torch.randn(4, 3, 51).abs()\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> reward = torch.randn(5, 4)\\n        >>> data = dist_nstep_td_data(dist, next_n_dist, action, next_action, reward, done, None)\\n        >>> loss, _ = dist_nstep_td_error(data, 0.95, -10.0, 10.0, 51, 5)\\n    \"\n    (dist, next_n_dist, act, next_n_act, reward, done, weight) = data\n    device = reward.device\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_n_act = next_n_act.reshape(act.shape[0] * act.shape[1])\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n    if value_gamma is None:\n        target_z = reward + (1 - done) * gamma ** nstep * support\n    elif isinstance(value_gamma, float):\n        value_gamma = torch.tensor(value_gamma).unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    else:\n        value_gamma = value_gamma.unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_n_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_n_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_n_dist * (b - l.float())).view(-1))\n    assert (dist[batch_range, act] > 0.0).all(), ('dist act', dist[batch_range, act], 'dist:', dist)\n    log_p = torch.log(dist[batch_range, act])\n    if len(weight.shape) == 1:\n        weight = weight.unsqueeze(-1)\n    td_error_per_sample = -(log_p * proj_dist).sum(-1)\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return (loss, td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_dntd, namedtuple_data=True, include_args=[0, 1, 2, 3], include_kwargs=['data', 'gamma', 'v_min', 'v_max'])\ndef dist_nstep_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for distributed q-learning based algorithm, support single            agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`dist_nstep_td_data`): The input data, dist_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_nstep_td_data`): the dist_nstep_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\\n        - next_n_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\\n        >>> next_n_dist = torch.randn(4, 3, 51).abs()\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> reward = torch.randn(5, 4)\\n        >>> data = dist_nstep_td_data(dist, next_n_dist, action, next_action, reward, done, None)\\n        >>> loss, _ = dist_nstep_td_error(data, 0.95, -10.0, 10.0, 51, 5)\\n    \"\n    (dist, next_n_dist, act, next_n_act, reward, done, weight) = data\n    device = reward.device\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_n_act = next_n_act.reshape(act.shape[0] * act.shape[1])\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n    if value_gamma is None:\n        target_z = reward + (1 - done) * gamma ** nstep * support\n    elif isinstance(value_gamma, float):\n        value_gamma = torch.tensor(value_gamma).unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    else:\n        value_gamma = value_gamma.unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_n_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_n_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_n_dist * (b - l.float())).view(-1))\n    assert (dist[batch_range, act] > 0.0).all(), ('dist act', dist[batch_range, act], 'dist:', dist)\n    log_p = torch.log(dist[batch_range, act])\n    if len(weight.shape) == 1:\n        weight = weight.unsqueeze(-1)\n    td_error_per_sample = -(log_p * proj_dist).sum(-1)\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return (loss, td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_dntd, namedtuple_data=True, include_args=[0, 1, 2, 3], include_kwargs=['data', 'gamma', 'v_min', 'v_max'])\ndef dist_nstep_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for distributed q-learning based algorithm, support single            agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`dist_nstep_td_data`): The input data, dist_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_nstep_td_data`): the dist_nstep_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\\n        - next_n_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\\n        >>> next_n_dist = torch.randn(4, 3, 51).abs()\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> reward = torch.randn(5, 4)\\n        >>> data = dist_nstep_td_data(dist, next_n_dist, action, next_action, reward, done, None)\\n        >>> loss, _ = dist_nstep_td_error(data, 0.95, -10.0, 10.0, 51, 5)\\n    \"\n    (dist, next_n_dist, act, next_n_act, reward, done, weight) = data\n    device = reward.device\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_n_act = next_n_act.reshape(act.shape[0] * act.shape[1])\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n    if value_gamma is None:\n        target_z = reward + (1 - done) * gamma ** nstep * support\n    elif isinstance(value_gamma, float):\n        value_gamma = torch.tensor(value_gamma).unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    else:\n        value_gamma = value_gamma.unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_n_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_n_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_n_dist * (b - l.float())).view(-1))\n    assert (dist[batch_range, act] > 0.0).all(), ('dist act', dist[batch_range, act], 'dist:', dist)\n    log_p = torch.log(dist[batch_range, act])\n    if len(weight.shape) == 1:\n        weight = weight.unsqueeze(-1)\n    td_error_per_sample = -(log_p * proj_dist).sum(-1)\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return (loss, td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_dntd, namedtuple_data=True, include_args=[0, 1, 2, 3], include_kwargs=['data', 'gamma', 'v_min', 'v_max'])\ndef dist_nstep_td_error(data: namedtuple, gamma: float, v_min: float, v_max: float, n_atom: int, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for distributed q-learning based algorithm, support single            agent case and multi agent case.\\n    Arguments:\\n        - data (:obj:`dist_nstep_td_data`): The input data, dist_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_nstep_td_data`): the dist_nstep_td_data containing            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']\\n        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]\\n        - next_n_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`\\n        - act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_act (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> dist = torch.randn(4, 3, 51).abs().requires_grad_(True)\\n        >>> next_n_dist = torch.randn(4, 3, 51).abs()\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> reward = torch.randn(5, 4)\\n        >>> data = dist_nstep_td_data(dist, next_n_dist, action, next_action, reward, done, None)\\n        >>> loss, _ = dist_nstep_td_error(data, 0.95, -10.0, 10.0, 51, 5)\\n    \"\n    (dist, next_n_dist, act, next_n_act, reward, done, weight) = data\n    device = reward.device\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    support = torch.linspace(v_min, v_max, n_atom).to(device)\n    delta_z = (v_max - v_min) / (n_atom - 1)\n    if len(act.shape) == 1:\n        reward = reward.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        batch_size = act.shape[0]\n        batch_range = torch.arange(batch_size)\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n    else:\n        reward = reward.unsqueeze(-1).repeat(1, act.shape[1])\n        done = done.unsqueeze(-1).repeat(1, act.shape[1])\n        batch_size = act.shape[0] * act.shape[1]\n        batch_range = torch.arange(act.shape[0] * act.shape[1])\n        action_dim = dist.shape[2]\n        dist = dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        reward = reward.reshape(act.shape[0] * act.shape[1], -1)\n        done = done.reshape(act.shape[0] * act.shape[1], -1)\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], action_dim, -1)\n        next_n_act = next_n_act.reshape(act.shape[0] * act.shape[1])\n        next_n_dist = next_n_dist[batch_range, next_n_act].detach()\n        next_n_dist = next_n_dist.reshape(act.shape[0] * act.shape[1], -1)\n        act = act.reshape(act.shape[0] * act.shape[1])\n        if weight is None:\n            weight = torch.ones_like(reward)\n        elif isinstance(weight, float):\n            weight = torch.tensor(weight)\n    if value_gamma is None:\n        target_z = reward + (1 - done) * gamma ** nstep * support\n    elif isinstance(value_gamma, float):\n        value_gamma = torch.tensor(value_gamma).unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    else:\n        value_gamma = value_gamma.unsqueeze(-1)\n        target_z = reward + (1 - done) * value_gamma * support\n    target_z = target_z.clamp(min=v_min, max=v_max)\n    b = (target_z - v_min) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n    l[(u > 0) * (l == u)] -= 1\n    u[(l < n_atom - 1) * (l == u)] += 1\n    proj_dist = torch.zeros_like(next_n_dist)\n    offset = torch.linspace(0, (batch_size - 1) * n_atom, batch_size).unsqueeze(1).expand(batch_size, n_atom).long().to(device)\n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_n_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_n_dist * (b - l.float())).view(-1))\n    assert (dist[batch_range, act] > 0.0).all(), ('dist act', dist[batch_range, act], 'dist:', dist)\n    log_p = torch.log(dist[batch_range, act])\n    if len(weight.shape) == 1:\n        weight = weight.unsqueeze(-1)\n    td_error_per_sample = -(log_p * proj_dist).sum(-1)\n    loss = -(log_p * proj_dist * weight).sum(-1).mean()\n    return (loss, td_error_per_sample)"
        ]
    },
    {
        "func_name": "v_1step_td_error",
        "original": "def v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        1 step td_error for distributed value based algorithm\n    Arguments:\n        - data (:obj:`v_1step_td_data`): The input data, v_1step_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n    Returns:\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\n    Shapes:\n        - data (:obj:`v_1step_td_data`): the v_1step_td_data containing            ['v', 'next_v', 'reward', 'done', 'weight']\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\n    Examples:\n        >>> v = torch.randn(5).requires_grad_(True)\n        >>> next_v = torch.randn(5)\n        >>> reward = torch.rand(5)\n        >>> done = torch.zeros(5)\n        >>> data = v_1step_td_data(v, next_v, reward, done, None)\n        >>> loss, td_error_per_sample = v_1step_td_error(data, 0.99)\n    \"\"\"\n    (v, next_v, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    if len(v.shape) == len(reward.shape):\n        if done is not None:\n            target_v = gamma * (1 - done) * next_v + reward\n        else:\n            target_v = gamma * next_v + reward\n    elif done is not None:\n        target_v = gamma * (1 - done).unsqueeze(1) * next_v + reward.unsqueeze(1)\n    else:\n        target_v = gamma * next_v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
        "mutated": [
            "def v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        1 step td_error for distributed value based algorithm\\n    Arguments:\\n        - data (:obj:`v_1step_td_data`): The input data, v_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`v_1step_td_data`): the v_1step_td_data containing            ['v', 'next_v', 'reward', 'done', 'weight']\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> v = torch.randn(5).requires_grad_(True)\\n        >>> next_v = torch.randn(5)\\n        >>> reward = torch.rand(5)\\n        >>> done = torch.zeros(5)\\n        >>> data = v_1step_td_data(v, next_v, reward, done, None)\\n        >>> loss, td_error_per_sample = v_1step_td_error(data, 0.99)\\n    \"\n    (v, next_v, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    if len(v.shape) == len(reward.shape):\n        if done is not None:\n            target_v = gamma * (1 - done) * next_v + reward\n        else:\n            target_v = gamma * next_v + reward\n    elif done is not None:\n        target_v = gamma * (1 - done).unsqueeze(1) * next_v + reward.unsqueeze(1)\n    else:\n        target_v = gamma * next_v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        1 step td_error for distributed value based algorithm\\n    Arguments:\\n        - data (:obj:`v_1step_td_data`): The input data, v_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`v_1step_td_data`): the v_1step_td_data containing            ['v', 'next_v', 'reward', 'done', 'weight']\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> v = torch.randn(5).requires_grad_(True)\\n        >>> next_v = torch.randn(5)\\n        >>> reward = torch.rand(5)\\n        >>> done = torch.zeros(5)\\n        >>> data = v_1step_td_data(v, next_v, reward, done, None)\\n        >>> loss, td_error_per_sample = v_1step_td_error(data, 0.99)\\n    \"\n    (v, next_v, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    if len(v.shape) == len(reward.shape):\n        if done is not None:\n            target_v = gamma * (1 - done) * next_v + reward\n        else:\n            target_v = gamma * next_v + reward\n    elif done is not None:\n        target_v = gamma * (1 - done).unsqueeze(1) * next_v + reward.unsqueeze(1)\n    else:\n        target_v = gamma * next_v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        1 step td_error for distributed value based algorithm\\n    Arguments:\\n        - data (:obj:`v_1step_td_data`): The input data, v_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`v_1step_td_data`): the v_1step_td_data containing            ['v', 'next_v', 'reward', 'done', 'weight']\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> v = torch.randn(5).requires_grad_(True)\\n        >>> next_v = torch.randn(5)\\n        >>> reward = torch.rand(5)\\n        >>> done = torch.zeros(5)\\n        >>> data = v_1step_td_data(v, next_v, reward, done, None)\\n        >>> loss, td_error_per_sample = v_1step_td_error(data, 0.99)\\n    \"\n    (v, next_v, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    if len(v.shape) == len(reward.shape):\n        if done is not None:\n            target_v = gamma * (1 - done) * next_v + reward\n        else:\n            target_v = gamma * next_v + reward\n    elif done is not None:\n        target_v = gamma * (1 - done).unsqueeze(1) * next_v + reward.unsqueeze(1)\n    else:\n        target_v = gamma * next_v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        1 step td_error for distributed value based algorithm\\n    Arguments:\\n        - data (:obj:`v_1step_td_data`): The input data, v_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`v_1step_td_data`): the v_1step_td_data containing            ['v', 'next_v', 'reward', 'done', 'weight']\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> v = torch.randn(5).requires_grad_(True)\\n        >>> next_v = torch.randn(5)\\n        >>> reward = torch.rand(5)\\n        >>> done = torch.zeros(5)\\n        >>> data = v_1step_td_data(v, next_v, reward, done, None)\\n        >>> loss, td_error_per_sample = v_1step_td_error(data, 0.99)\\n    \"\n    (v, next_v, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    if len(v.shape) == len(reward.shape):\n        if done is not None:\n            target_v = gamma * (1 - done) * next_v + reward\n        else:\n            target_v = gamma * next_v + reward\n    elif done is not None:\n        target_v = gamma * (1 - done).unsqueeze(1) * next_v + reward.unsqueeze(1)\n    else:\n        target_v = gamma * next_v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def v_1step_td_error(data: namedtuple, gamma: float, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        1 step td_error for distributed value based algorithm\\n    Arguments:\\n        - data (:obj:`v_1step_td_data`): The input data, v_1step_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`v_1step_td_data`): the v_1step_td_data containing            ['v', 'next_v', 'reward', 'done', 'weight']\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n    Examples:\\n        >>> v = torch.randn(5).requires_grad_(True)\\n        >>> next_v = torch.randn(5)\\n        >>> reward = torch.rand(5)\\n        >>> done = torch.zeros(5)\\n        >>> data = v_1step_td_data(v, next_v, reward, done, None)\\n        >>> loss, td_error_per_sample = v_1step_td_error(data, 0.99)\\n    \"\n    (v, next_v, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    if len(v.shape) == len(reward.shape):\n        if done is not None:\n            target_v = gamma * (1 - done) * next_v + reward\n        else:\n            target_v = gamma * next_v + reward\n    elif done is not None:\n        target_v = gamma * (1 - done).unsqueeze(1) * next_v + reward.unsqueeze(1)\n    else:\n        target_v = gamma * next_v + reward.unsqueeze(1)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)"
        ]
    },
    {
        "func_name": "v_nstep_td_error",
        "original": "def v_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep (n step) td_error for distributed value based algorithm\n    Arguments:\n        - data (:obj:`dist_nstep_td_data`): The input data, v_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - nstep (:obj:`int`): nstep num, default set to 1\n    Returns:\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\n    Shapes:\n        - data (:obj:`dist_nstep_td_data`): The v_nstep_td_data containing\\\\\n            ['v', 'next_n_v', 'reward', 'done', 'weight', 'value_gamma']\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\n        - value_gamma (:obj:`torch.Tensor`): If the remaining data in the buffer is less than n_step\\\\\n            we use value_gamma as the gamma discount value for next_v rather than gamma**n_step\n    Examples:\n        >>> v = torch.randn(5).requires_grad_(True)\n        >>> next_v = torch.randn(5)\n        >>> reward = torch.rand(5, 5)\n        >>> done = torch.zeros(5)\n        >>> data = v_nstep_td_data(v, next_v, reward, done, 0.9, 0.99)\n        >>> loss, td_error_per_sample = v_nstep_td_error(data, 0.99, 5)\n    \"\"\"\n    (v, next_n_v, reward, done, weight, value_gamma) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    target_v = nstep_return(nstep_return_data(reward, next_n_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
        "mutated": [
            "def v_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Multistep (n step) td_error for distributed value based algorithm\\n    Arguments:\\n        - data (:obj:`dist_nstep_td_data`): The input data, v_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_nstep_td_data`): The v_nstep_td_data containing\\\\\\n            ['v', 'next_n_v', 'reward', 'done', 'weight', 'value_gamma']\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n        - value_gamma (:obj:`torch.Tensor`): If the remaining data in the buffer is less than n_step\\\\\\n            we use value_gamma as the gamma discount value for next_v rather than gamma**n_step\\n    Examples:\\n        >>> v = torch.randn(5).requires_grad_(True)\\n        >>> next_v = torch.randn(5)\\n        >>> reward = torch.rand(5, 5)\\n        >>> done = torch.zeros(5)\\n        >>> data = v_nstep_td_data(v, next_v, reward, done, 0.9, 0.99)\\n        >>> loss, td_error_per_sample = v_nstep_td_error(data, 0.99, 5)\\n    \"\n    (v, next_n_v, reward, done, weight, value_gamma) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    target_v = nstep_return(nstep_return_data(reward, next_n_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def v_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Multistep (n step) td_error for distributed value based algorithm\\n    Arguments:\\n        - data (:obj:`dist_nstep_td_data`): The input data, v_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_nstep_td_data`): The v_nstep_td_data containing\\\\\\n            ['v', 'next_n_v', 'reward', 'done', 'weight', 'value_gamma']\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n        - value_gamma (:obj:`torch.Tensor`): If the remaining data in the buffer is less than n_step\\\\\\n            we use value_gamma as the gamma discount value for next_v rather than gamma**n_step\\n    Examples:\\n        >>> v = torch.randn(5).requires_grad_(True)\\n        >>> next_v = torch.randn(5)\\n        >>> reward = torch.rand(5, 5)\\n        >>> done = torch.zeros(5)\\n        >>> data = v_nstep_td_data(v, next_v, reward, done, 0.9, 0.99)\\n        >>> loss, td_error_per_sample = v_nstep_td_error(data, 0.99, 5)\\n    \"\n    (v, next_n_v, reward, done, weight, value_gamma) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    target_v = nstep_return(nstep_return_data(reward, next_n_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def v_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Multistep (n step) td_error for distributed value based algorithm\\n    Arguments:\\n        - data (:obj:`dist_nstep_td_data`): The input data, v_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_nstep_td_data`): The v_nstep_td_data containing\\\\\\n            ['v', 'next_n_v', 'reward', 'done', 'weight', 'value_gamma']\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n        - value_gamma (:obj:`torch.Tensor`): If the remaining data in the buffer is less than n_step\\\\\\n            we use value_gamma as the gamma discount value for next_v rather than gamma**n_step\\n    Examples:\\n        >>> v = torch.randn(5).requires_grad_(True)\\n        >>> next_v = torch.randn(5)\\n        >>> reward = torch.rand(5, 5)\\n        >>> done = torch.zeros(5)\\n        >>> data = v_nstep_td_data(v, next_v, reward, done, 0.9, 0.99)\\n        >>> loss, td_error_per_sample = v_nstep_td_error(data, 0.99, 5)\\n    \"\n    (v, next_n_v, reward, done, weight, value_gamma) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    target_v = nstep_return(nstep_return_data(reward, next_n_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def v_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Multistep (n step) td_error for distributed value based algorithm\\n    Arguments:\\n        - data (:obj:`dist_nstep_td_data`): The input data, v_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_nstep_td_data`): The v_nstep_td_data containing\\\\\\n            ['v', 'next_n_v', 'reward', 'done', 'weight', 'value_gamma']\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n        - value_gamma (:obj:`torch.Tensor`): If the remaining data in the buffer is less than n_step\\\\\\n            we use value_gamma as the gamma discount value for next_v rather than gamma**n_step\\n    Examples:\\n        >>> v = torch.randn(5).requires_grad_(True)\\n        >>> next_v = torch.randn(5)\\n        >>> reward = torch.rand(5, 5)\\n        >>> done = torch.zeros(5)\\n        >>> data = v_nstep_td_data(v, next_v, reward, done, 0.9, 0.99)\\n        >>> loss, td_error_per_sample = v_nstep_td_error(data, 0.99, 5)\\n    \"\n    (v, next_n_v, reward, done, weight, value_gamma) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    target_v = nstep_return(nstep_return_data(reward, next_n_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def v_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Multistep (n step) td_error for distributed value based algorithm\\n    Arguments:\\n        - data (:obj:`dist_nstep_td_data`): The input data, v_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`dist_nstep_td_data`): The v_nstep_td_data containing\\\\\\n            ['v', 'next_n_v', 'reward', 'done', 'weight', 'value_gamma']\\n        - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]\\n        - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n        - value_gamma (:obj:`torch.Tensor`): If the remaining data in the buffer is less than n_step\\\\\\n            we use value_gamma as the gamma discount value for next_v rather than gamma**n_step\\n    Examples:\\n        >>> v = torch.randn(5).requires_grad_(True)\\n        >>> next_v = torch.randn(5)\\n        >>> reward = torch.rand(5, 5)\\n        >>> done = torch.zeros(5)\\n        >>> data = v_nstep_td_data(v, next_v, reward, done, 0.9, 0.99)\\n        >>> loss, td_error_per_sample = v_nstep_td_error(data, 0.99, 5)\\n    \"\n    (v, next_n_v, reward, done, weight, value_gamma) = data\n    if weight is None:\n        weight = torch.ones_like(v)\n    target_v = nstep_return(nstep_return_data(reward, next_n_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(v, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)"
        ]
    },
    {
        "func_name": "shape_fn_qntd",
        "original": "def shape_fn_qntd(args, kwargs):\n    \"\"\"\n    Overview:\n        Return qntd shape for hpc\n    Returns:\n        shape: [T, B, N]\n    \"\"\"\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp",
        "mutated": [
            "def shape_fn_qntd(args, kwargs):\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Return qntd shape for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp",
            "def shape_fn_qntd(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Return qntd shape for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp",
            "def shape_fn_qntd(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Return qntd shape for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp",
            "def shape_fn_qntd(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Return qntd shape for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp",
            "def shape_fn_qntd(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Return qntd shape for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp"
        ]
    },
    {
        "func_name": "q_nstep_td_error",
        "original": "@hpc_wrapper(shape_fn=shape_fn_qntd, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep (1 step or n step) td_error for q-learning based algorithm\n    Arguments:\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n        - nstep (:obj:`int`): nstep num, default set to 1\n    Returns:\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\n    Shapes:\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\n    Examples:\n        >>> next_q = torch.randn(4, 3)\n        >>> done = torch.randn(4)\n        >>> action = torch.randint(0, 3, size=(4, ))\n        >>> next_action = torch.randint(0, 3, size=(4, ))\n        >>> nstep =3\n        >>> q = torch.randn(4, 3).requires_grad_(True)\n        >>> reward = torch.rand(nstep, 4)\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\n        >>> loss, td_error_per_sample = q_nstep_td_error(data, 0.95, nstep=nstep)\n    \"\"\"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    if len(action.shape) > 1:\n        reward = reward.unsqueeze(-1)\n        weight = weight.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        if value_gamma is not None:\n            value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
        "mutated": [
            "@hpc_wrapper(shape_fn=shape_fn_qntd, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep =3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample = q_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    if len(action.shape) > 1:\n        reward = reward.unsqueeze(-1)\n        weight = weight.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        if value_gamma is not None:\n            value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_qntd, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep =3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample = q_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    if len(action.shape) > 1:\n        reward = reward.unsqueeze(-1)\n        weight = weight.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        if value_gamma is not None:\n            value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_qntd, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep =3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample = q_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    if len(action.shape) > 1:\n        reward = reward.unsqueeze(-1)\n        weight = weight.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        if value_gamma is not None:\n            value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_qntd, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep =3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample = q_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    if len(action.shape) > 1:\n        reward = reward.unsqueeze(-1)\n        weight = weight.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        if value_gamma is not None:\n            value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_qntd, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep =3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample = q_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    if len(action.shape) > 1:\n        reward = reward.unsqueeze(-1)\n        weight = weight.unsqueeze(-1)\n        done = done.unsqueeze(-1)\n        if value_gamma is not None:\n            value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)"
        ]
    },
    {
        "func_name": "bdq_nstep_td_error",
        "original": "def bdq_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep (1 step or n step) td_error for BDQ algorithm, referenced paper \"Action Branching Architectures for         Deep Reinforcement Learning\", link: https://arxiv.org/pdf/1711.08946.\n        In fact, the original paper only provides the 1-step TD-error calculation method, and here we extend the         calculation method of n-step, i.e., TD-error:\n    Arguments:\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n        - nstep (:obj:`int`): nstep num, default set to 1\n    Returns:\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\n    Shapes:\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing             ['q', 'next_n_q', 'action', 'reward', 'done']\n        - q (:obj:`torch.FloatTensor`): :math:`(B, D, N)` i.e. [batch_size, branch_num, action_bins_per_branch]\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, D, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(B, D)`\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, D)`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\n    Examples:\n        >>> action_per_branch = 3\n        >>> next_q = torch.randn(8, 6, action_per_branch)\n        >>> done = torch.randn(8)\n        >>> action = torch.randint(0, action_per_branch, size=(8, 6))\n        >>> next_action = torch.randint(0, action_per_branch, size=(8, 6))\n        >>> nstep =3\n        >>> q = torch.randn(8, 6, action_per_branch).requires_grad_(True)\n        >>> reward = torch.rand(nstep, 8)\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\n        >>> loss, td_error_per_sample = bdq_nstep_td_error(data, 0.95, nstep=nstep)\n    \"\"\"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    reward = reward.unsqueeze(-1)\n    done = done.unsqueeze(-1)\n    if value_gamma is not None:\n        value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    td_error_per_sample = td_error_per_sample.mean(-1)\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
        "mutated": [
            "def bdq_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Multistep (1 step or n step) td_error for BDQ algorithm, referenced paper \"Action Branching Architectures for         Deep Reinforcement Learning\", link: https://arxiv.org/pdf/1711.08946.\\n        In fact, the original paper only provides the 1-step TD-error calculation method, and here we extend the         calculation method of n-step, i.e., TD-error:\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing             [\\'q\\', \\'next_n_q\\', \\'action\\', \\'reward\\', \\'done\\']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, D, N)` i.e. [batch_size, branch_num, action_bins_per_branch]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, D, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, D)`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, D)`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> action_per_branch = 3\\n        >>> next_q = torch.randn(8, 6, action_per_branch)\\n        >>> done = torch.randn(8)\\n        >>> action = torch.randint(0, action_per_branch, size=(8, 6))\\n        >>> next_action = torch.randint(0, action_per_branch, size=(8, 6))\\n        >>> nstep =3\\n        >>> q = torch.randn(8, 6, action_per_branch).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 8)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample = bdq_nstep_td_error(data, 0.95, nstep=nstep)\\n    '\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    reward = reward.unsqueeze(-1)\n    done = done.unsqueeze(-1)\n    if value_gamma is not None:\n        value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    td_error_per_sample = td_error_per_sample.mean(-1)\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def bdq_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Multistep (1 step or n step) td_error for BDQ algorithm, referenced paper \"Action Branching Architectures for         Deep Reinforcement Learning\", link: https://arxiv.org/pdf/1711.08946.\\n        In fact, the original paper only provides the 1-step TD-error calculation method, and here we extend the         calculation method of n-step, i.e., TD-error:\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing             [\\'q\\', \\'next_n_q\\', \\'action\\', \\'reward\\', \\'done\\']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, D, N)` i.e. [batch_size, branch_num, action_bins_per_branch]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, D, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, D)`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, D)`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> action_per_branch = 3\\n        >>> next_q = torch.randn(8, 6, action_per_branch)\\n        >>> done = torch.randn(8)\\n        >>> action = torch.randint(0, action_per_branch, size=(8, 6))\\n        >>> next_action = torch.randint(0, action_per_branch, size=(8, 6))\\n        >>> nstep =3\\n        >>> q = torch.randn(8, 6, action_per_branch).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 8)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample = bdq_nstep_td_error(data, 0.95, nstep=nstep)\\n    '\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    reward = reward.unsqueeze(-1)\n    done = done.unsqueeze(-1)\n    if value_gamma is not None:\n        value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    td_error_per_sample = td_error_per_sample.mean(-1)\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def bdq_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Multistep (1 step or n step) td_error for BDQ algorithm, referenced paper \"Action Branching Architectures for         Deep Reinforcement Learning\", link: https://arxiv.org/pdf/1711.08946.\\n        In fact, the original paper only provides the 1-step TD-error calculation method, and here we extend the         calculation method of n-step, i.e., TD-error:\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing             [\\'q\\', \\'next_n_q\\', \\'action\\', \\'reward\\', \\'done\\']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, D, N)` i.e. [batch_size, branch_num, action_bins_per_branch]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, D, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, D)`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, D)`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> action_per_branch = 3\\n        >>> next_q = torch.randn(8, 6, action_per_branch)\\n        >>> done = torch.randn(8)\\n        >>> action = torch.randint(0, action_per_branch, size=(8, 6))\\n        >>> next_action = torch.randint(0, action_per_branch, size=(8, 6))\\n        >>> nstep =3\\n        >>> q = torch.randn(8, 6, action_per_branch).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 8)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample = bdq_nstep_td_error(data, 0.95, nstep=nstep)\\n    '\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    reward = reward.unsqueeze(-1)\n    done = done.unsqueeze(-1)\n    if value_gamma is not None:\n        value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    td_error_per_sample = td_error_per_sample.mean(-1)\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def bdq_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Multistep (1 step or n step) td_error for BDQ algorithm, referenced paper \"Action Branching Architectures for         Deep Reinforcement Learning\", link: https://arxiv.org/pdf/1711.08946.\\n        In fact, the original paper only provides the 1-step TD-error calculation method, and here we extend the         calculation method of n-step, i.e., TD-error:\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing             [\\'q\\', \\'next_n_q\\', \\'action\\', \\'reward\\', \\'done\\']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, D, N)` i.e. [batch_size, branch_num, action_bins_per_branch]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, D, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, D)`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, D)`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> action_per_branch = 3\\n        >>> next_q = torch.randn(8, 6, action_per_branch)\\n        >>> done = torch.randn(8)\\n        >>> action = torch.randint(0, action_per_branch, size=(8, 6))\\n        >>> next_action = torch.randint(0, action_per_branch, size=(8, 6))\\n        >>> nstep =3\\n        >>> q = torch.randn(8, 6, action_per_branch).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 8)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample = bdq_nstep_td_error(data, 0.95, nstep=nstep)\\n    '\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    reward = reward.unsqueeze(-1)\n    done = done.unsqueeze(-1)\n    if value_gamma is not None:\n        value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    td_error_per_sample = td_error_per_sample.mean(-1)\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "def bdq_nstep_td_error(data: namedtuple, gamma: Union[float, list], nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Multistep (1 step or n step) td_error for BDQ algorithm, referenced paper \"Action Branching Architectures for         Deep Reinforcement Learning\", link: https://arxiv.org/pdf/1711.08946.\\n        In fact, the original paper only provides the 1-step TD-error calculation method, and here we extend the         calculation method of n-step, i.e., TD-error:\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing             [\\'q\\', \\'next_n_q\\', \\'action\\', \\'reward\\', \\'done\\']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, D, N)` i.e. [batch_size, branch_num, action_bins_per_branch]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, D, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, D)`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, D)`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> action_per_branch = 3\\n        >>> next_q = torch.randn(8, 6, action_per_branch)\\n        >>> done = torch.randn(8)\\n        >>> action = torch.randint(0, action_per_branch, size=(8, 6))\\n        >>> next_action = torch.randint(0, action_per_branch, size=(8, 6))\\n        >>> nstep =3\\n        >>> q = torch.randn(8, 6, action_per_branch).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 8)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample = bdq_nstep_td_error(data, 0.95, nstep=nstep)\\n    '\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    reward = reward.unsqueeze(-1)\n    done = done.unsqueeze(-1)\n    if value_gamma is not None:\n        value_gamma = value_gamma.unsqueeze(-1)\n    q_s_a = q.gather(-1, action.unsqueeze(-1)).squeeze(-1)\n    target_q_s_a = next_n_q.gather(-1, next_n_action.unsqueeze(-1)).squeeze(-1)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    td_error_per_sample = td_error_per_sample.mean(-1)\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)"
        ]
    },
    {
        "func_name": "shape_fn_qntd_rescale",
        "original": "def shape_fn_qntd_rescale(args, kwargs):\n    \"\"\"\n    Overview:\n        Return qntd_rescale shape for hpc\n    Returns:\n        shape: [T, B, N]\n    \"\"\"\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp",
        "mutated": [
            "def shape_fn_qntd_rescale(args, kwargs):\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Return qntd_rescale shape for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp",
            "def shape_fn_qntd_rescale(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Return qntd_rescale shape for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp",
            "def shape_fn_qntd_rescale(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Return qntd_rescale shape for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp",
            "def shape_fn_qntd_rescale(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Return qntd_rescale shape for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp",
            "def shape_fn_qntd_rescale(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Return qntd_rescale shape for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = [kwargs['data'].reward.shape[0]]\n        tmp.extend(list(kwargs['data'].q.shape))\n    else:\n        tmp = [args[0].reward.shape[0]]\n        tmp.extend(list(args[0].q.shape))\n    return tmp"
        ]
    },
    {
        "func_name": "q_nstep_td_error_with_rescale",
        "original": "@hpc_wrapper(shape_fn=shape_fn_qntd_rescale, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error_with_rescale(data: namedtuple, gamma: Union[float, list], nstep: int=1, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep (1 step or n step) td_error with value rescaling\n    Arguments:\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - nstep (:obj:`int`): nstep num, default set to 1\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n        - trans_fn (:obj:`Callable`): Value transfrom function, default to value_transform            (refer to rl_utils/value_rescale.py)\n        - inv_trans_fn (:obj:`Callable`): Value inverse transfrom function, default to value_inv_transform            (refer to rl_utils/value_rescale.py)\n    Returns:\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\n    Shapes:\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n    Examples:\n        >>> next_q = torch.randn(4, 3)\n        >>> done = torch.randn(4)\n        >>> action = torch.randint(0, 3, size=(4, ))\n        >>> next_action = torch.randint(0, 3, size=(4, ))\n        >>> nstep =3\n        >>> q = torch.randn(4, 3).requires_grad_(True)\n        >>> reward = torch.rand(nstep, 4)\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\n        >>> loss, _ = q_nstep_td_error_with_rescale(data, 0.95, nstep=nstep)\n    \"\"\"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
        "mutated": [
            "@hpc_wrapper(shape_fn=shape_fn_qntd_rescale, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error_with_rescale(data: namedtuple, gamma: Union[float, list], nstep: int=1, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with value rescaling\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - trans_fn (:obj:`Callable`): Value transfrom function, default to value_transform            (refer to rl_utils/value_rescale.py)\\n        - inv_trans_fn (:obj:`Callable`): Value inverse transfrom function, default to value_inv_transform            (refer to rl_utils/value_rescale.py)\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep =3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, _ = q_nstep_td_error_with_rescale(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_qntd_rescale, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error_with_rescale(data: namedtuple, gamma: Union[float, list], nstep: int=1, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with value rescaling\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - trans_fn (:obj:`Callable`): Value transfrom function, default to value_transform            (refer to rl_utils/value_rescale.py)\\n        - inv_trans_fn (:obj:`Callable`): Value inverse transfrom function, default to value_inv_transform            (refer to rl_utils/value_rescale.py)\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep =3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, _ = q_nstep_td_error_with_rescale(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_qntd_rescale, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error_with_rescale(data: namedtuple, gamma: Union[float, list], nstep: int=1, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with value rescaling\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - trans_fn (:obj:`Callable`): Value transfrom function, default to value_transform            (refer to rl_utils/value_rescale.py)\\n        - inv_trans_fn (:obj:`Callable`): Value inverse transfrom function, default to value_inv_transform            (refer to rl_utils/value_rescale.py)\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep =3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, _ = q_nstep_td_error_with_rescale(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_qntd_rescale, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error_with_rescale(data: namedtuple, gamma: Union[float, list], nstep: int=1, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with value rescaling\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - trans_fn (:obj:`Callable`): Value transfrom function, default to value_transform            (refer to rl_utils/value_rescale.py)\\n        - inv_trans_fn (:obj:`Callable`): Value inverse transfrom function, default to value_inv_transform            (refer to rl_utils/value_rescale.py)\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep =3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, _ = q_nstep_td_error_with_rescale(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)",
            "@hpc_wrapper(shape_fn=shape_fn_qntd_rescale, namedtuple_data=True, include_args=[0, 1], include_kwargs=['data', 'gamma'])\ndef q_nstep_td_error_with_rescale(data: namedtuple, gamma: Union[float, list], nstep: int=1, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with value rescaling\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - trans_fn (:obj:`Callable`): Value transfrom function, default to value_transform            (refer to rl_utils/value_rescale.py)\\n        - inv_trans_fn (:obj:`Callable`): Value inverse transfrom function, default to value_inv_transform            (refer to rl_utils/value_rescale.py)\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep =3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, _ = q_nstep_td_error_with_rescale(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample)"
        ]
    },
    {
        "func_name": "dqfd_nstep_td_error",
        "original": "def dqfd_nstep_td_error(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, margin_function: float, lambda_one_step_td: float=1.0, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\n    Arguments:\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): discount factor\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n        - nstep (:obj:`int`): nstep num, default set to 10\n    Returns:\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\n    Shapes:\n        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\n        - is_expert (:obj:`int`) : 0 or 1\n    Examples:\n        >>> next_q = torch.randn(4, 3)\n        >>> done = torch.randn(4)\n        >>> done_1 = torch.randn(4)\n        >>> next_q_one_step = torch.randn(4, 3)\n        >>> action = torch.randint(0, 3, size=(4, ))\n        >>> next_action = torch.randint(0, 3, size=(4, ))\n        >>> next_action_one_step = torch.randint(0, 3, size=(4, ))\n        >>> is_expert = torch.ones((4))\n        >>> nstep = 3\n        >>> q = torch.randn(4, 3).requires_grad_(True)\n        >>> reward = torch.rand(nstep, 4)\n        >>> data = dqfd_nstep_td_data(\n        >>>     q, next_q, action, next_action, reward, done, done_1, None,\n        >>>     next_q_one_step, next_action_one_step, is_expert\n        >>> )\n        >>> loss, td_error_per_sample, loss_statistics = dqfd_nstep_td_error(\n        >>>     data, 0.95, lambda_n_step_td=1, lambda_supervised_loss=1,\n        >>>     margin_function=0.8, nstep=nstep\n        >>> )\n    \"\"\"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))",
        "mutated": [
            "def dqfd_nstep_td_error(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, margin_function: float, lambda_one_step_td: float=1.0, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\\n    Arguments:\\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 10\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - is_expert (:obj:`int`) : 0 or 1\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> done_1 = torch.randn(4)\\n        >>> next_q_one_step = torch.randn(4, 3)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action_one_step = torch.randint(0, 3, size=(4, ))\\n        >>> is_expert = torch.ones((4))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = dqfd_nstep_td_data(\\n        >>>     q, next_q, action, next_action, reward, done, done_1, None,\\n        >>>     next_q_one_step, next_action_one_step, is_expert\\n        >>> )\\n        >>> loss, td_error_per_sample, loss_statistics = dqfd_nstep_td_error(\\n        >>>     data, 0.95, lambda_n_step_td=1, lambda_supervised_loss=1,\\n        >>>     margin_function=0.8, nstep=nstep\\n        >>> )\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))",
            "def dqfd_nstep_td_error(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, margin_function: float, lambda_one_step_td: float=1.0, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\\n    Arguments:\\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 10\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - is_expert (:obj:`int`) : 0 or 1\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> done_1 = torch.randn(4)\\n        >>> next_q_one_step = torch.randn(4, 3)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action_one_step = torch.randint(0, 3, size=(4, ))\\n        >>> is_expert = torch.ones((4))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = dqfd_nstep_td_data(\\n        >>>     q, next_q, action, next_action, reward, done, done_1, None,\\n        >>>     next_q_one_step, next_action_one_step, is_expert\\n        >>> )\\n        >>> loss, td_error_per_sample, loss_statistics = dqfd_nstep_td_error(\\n        >>>     data, 0.95, lambda_n_step_td=1, lambda_supervised_loss=1,\\n        >>>     margin_function=0.8, nstep=nstep\\n        >>> )\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))",
            "def dqfd_nstep_td_error(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, margin_function: float, lambda_one_step_td: float=1.0, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\\n    Arguments:\\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 10\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - is_expert (:obj:`int`) : 0 or 1\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> done_1 = torch.randn(4)\\n        >>> next_q_one_step = torch.randn(4, 3)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action_one_step = torch.randint(0, 3, size=(4, ))\\n        >>> is_expert = torch.ones((4))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = dqfd_nstep_td_data(\\n        >>>     q, next_q, action, next_action, reward, done, done_1, None,\\n        >>>     next_q_one_step, next_action_one_step, is_expert\\n        >>> )\\n        >>> loss, td_error_per_sample, loss_statistics = dqfd_nstep_td_error(\\n        >>>     data, 0.95, lambda_n_step_td=1, lambda_supervised_loss=1,\\n        >>>     margin_function=0.8, nstep=nstep\\n        >>> )\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))",
            "def dqfd_nstep_td_error(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, margin_function: float, lambda_one_step_td: float=1.0, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\\n    Arguments:\\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 10\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - is_expert (:obj:`int`) : 0 or 1\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> done_1 = torch.randn(4)\\n        >>> next_q_one_step = torch.randn(4, 3)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action_one_step = torch.randint(0, 3, size=(4, ))\\n        >>> is_expert = torch.ones((4))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = dqfd_nstep_td_data(\\n        >>>     q, next_q, action, next_action, reward, done, done_1, None,\\n        >>>     next_q_one_step, next_action_one_step, is_expert\\n        >>> )\\n        >>> loss, td_error_per_sample, loss_statistics = dqfd_nstep_td_error(\\n        >>>     data, 0.95, lambda_n_step_td=1, lambda_supervised_loss=1,\\n        >>>     margin_function=0.8, nstep=nstep\\n        >>> )\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))",
            "def dqfd_nstep_td_error(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, margin_function: float, lambda_one_step_td: float=1.0, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\\n    Arguments:\\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 10\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - is_expert (:obj:`int`) : 0 or 1\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> done_1 = torch.randn(4)\\n        >>> next_q_one_step = torch.randn(4, 3)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action_one_step = torch.randint(0, 3, size=(4, ))\\n        >>> is_expert = torch.ones((4))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = dqfd_nstep_td_data(\\n        >>>     q, next_q, action, next_action, reward, done, done_1, None,\\n        >>>     next_q_one_step, next_action_one_step, is_expert\\n        >>> )\\n        >>> loss, td_error_per_sample, loss_statistics = dqfd_nstep_td_error(\\n        >>>     data, 0.95, lambda_n_step_td=1, lambda_supervised_loss=1,\\n        >>>     margin_function=0.8, nstep=nstep\\n        >>> )\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))"
        ]
    },
    {
        "func_name": "dqfd_nstep_td_error_with_rescale",
        "original": "def dqfd_nstep_td_error_with_rescale(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, lambda_one_step_td: float, margin_function: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\n    Arguments:\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n        - nstep (:obj:`int`): nstep num, default set to 10\n    Returns:\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\n    Shapes:\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\n        - is_expert (:obj:`int`) : 0 or 1\n    \"\"\"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    target_q_s_a_one_step = inv_trans_fn(target_q_s_a_one_step)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    target_q_s_a_one_step = trans_fn(target_q_s_a_one_step)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))",
        "mutated": [
            "def dqfd_nstep_td_error_with_rescale(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, lambda_one_step_td: float, margin_function: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\\n    Arguments:\\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 10\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - is_expert (:obj:`int`) : 0 or 1\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    target_q_s_a_one_step = inv_trans_fn(target_q_s_a_one_step)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    target_q_s_a_one_step = trans_fn(target_q_s_a_one_step)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))",
            "def dqfd_nstep_td_error_with_rescale(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, lambda_one_step_td: float, margin_function: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\\n    Arguments:\\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 10\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - is_expert (:obj:`int`) : 0 or 1\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    target_q_s_a_one_step = inv_trans_fn(target_q_s_a_one_step)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    target_q_s_a_one_step = trans_fn(target_q_s_a_one_step)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))",
            "def dqfd_nstep_td_error_with_rescale(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, lambda_one_step_td: float, margin_function: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\\n    Arguments:\\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 10\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - is_expert (:obj:`int`) : 0 or 1\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    target_q_s_a_one_step = inv_trans_fn(target_q_s_a_one_step)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    target_q_s_a_one_step = trans_fn(target_q_s_a_one_step)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))",
            "def dqfd_nstep_td_error_with_rescale(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, lambda_one_step_td: float, margin_function: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\\n    Arguments:\\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 10\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - is_expert (:obj:`int`) : 0 or 1\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    target_q_s_a_one_step = inv_trans_fn(target_q_s_a_one_step)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    target_q_s_a_one_step = trans_fn(target_q_s_a_one_step)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))",
            "def dqfd_nstep_td_error_with_rescale(data: namedtuple, gamma: float, lambda_n_step_td: float, lambda_supervised_loss: float, lambda_one_step_td: float, margin_function: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none'), trans_fn: Callable=value_transform, inv_trans_fn: Callable=value_inv_transform) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd\\n    Arguments:\\n        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 10\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'                , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n        - new_n_q_one_step (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - next_n_action_one_step (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - is_expert (:obj:`int`) : 0 or 1\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, done_one_step, weight, new_n_q_one_step, next_n_action_one_step, is_expert) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_q_s_a = next_n_q[batch_range, next_n_action]\n    target_q_s_a = inv_trans_fn(target_q_s_a)\n    target_q_s_a_one_step = new_n_q_one_step[batch_range, next_n_action_one_step]\n    target_q_s_a_one_step = inv_trans_fn(target_q_s_a_one_step)\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a = reward + gamma ** nstep * target_q_s_a * (1 - done)\n        else:\n            target_q_s_a = reward + value_gamma * target_q_s_a * (1 - done)\n    else:\n        target_q_s_a = nstep_return(nstep_return_data(reward, target_q_s_a, done), gamma, nstep, value_gamma)\n    target_q_s_a = trans_fn(target_q_s_a)\n    td_error_per_sample = criterion(q_s_a, target_q_s_a.detach())\n    nstep = 1\n    reward = reward[0].unsqueeze(0)\n    value_gamma = None\n    if cum_reward:\n        if value_gamma is None:\n            target_q_s_a_one_step = reward + gamma ** nstep * target_q_s_a_one_step * (1 - done_one_step)\n        else:\n            target_q_s_a_one_step = reward + value_gamma * target_q_s_a_one_step * (1 - done_one_step)\n    else:\n        target_q_s_a_one_step = nstep_return(nstep_return_data(reward, target_q_s_a_one_step, done_one_step), gamma, nstep, value_gamma)\n    target_q_s_a_one_step = trans_fn(target_q_s_a_one_step)\n    td_error_one_step_per_sample = criterion(q_s_a, target_q_s_a_one_step.detach())\n    device = q_s_a.device\n    device_cpu = torch.device('cpu')\n    l = margin_function * torch.ones_like(q).to(device_cpu)\n    l.scatter_(1, torch.LongTensor(action.unsqueeze(1).to(device_cpu)), torch.zeros_like(q, device=device_cpu))\n    JE = is_expert * (torch.max(q + l.to(device), dim=1)[0] - q_s_a)\n    return (((lambda_n_step_td * td_error_per_sample + lambda_one_step_td * td_error_one_step_per_sample + lambda_supervised_loss * JE) * weight).mean(), lambda_n_step_td * td_error_per_sample.abs() + lambda_one_step_td * td_error_one_step_per_sample.abs() + lambda_supervised_loss * JE.abs(), (td_error_per_sample.mean(), td_error_one_step_per_sample.mean(), JE.mean()))"
        ]
    },
    {
        "func_name": "qrdqn_nstep_td_error",
        "original": "def qrdqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep (1 step or n step) td_error with in QRDQN\n    Arguments:\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - nstep (:obj:`int`): nstep num, default set to 1\n    Returns:\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\n    Shapes:\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n    Examples:\n        >>> next_q = torch.randn(4, 3, 3)\n        >>> done = torch.randn(4)\n        >>> action = torch.randint(0, 3, size=(4, ))\n        >>> next_action = torch.randint(0, 3, size=(4, ))\n        >>> nstep = 3\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\n        >>> reward = torch.rand(nstep, 4)\n        >>> data = qrdqn_nstep_td_data(q, next_q, action, next_action, reward, done, 3, None)\n        >>> loss, td_error_per_sample = qrdqn_nstep_td_error(data, 0.95, nstep=nstep)\n    \"\"\"\n    (q, next_n_q, action, next_n_action, reward, done, tau, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action, :].unsqueeze(2)\n    target_q_s_a = next_n_q[batch_range, next_n_action, :].unsqueeze(1)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + gamma ** nstep * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + value_gamma.unsqueeze(-1).unsqueeze(-1) * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    u = F.smooth_l1_loss(target_q_s_a, q_s_a, reduction='none')\n    loss = (u * (tau - (target_q_s_a - q_s_a).detach().le(0.0).float()).abs()).sum(-1).mean(1)\n    return ((loss * weight).mean(), loss)",
        "mutated": [
            "def qrdqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in QRDQN\\n    Arguments:\\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(4, 3, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = qrdqn_nstep_td_data(q, next_q, action, next_action, reward, done, 3, None)\\n        >>> loss, td_error_per_sample = qrdqn_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, tau, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action, :].unsqueeze(2)\n    target_q_s_a = next_n_q[batch_range, next_n_action, :].unsqueeze(1)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + gamma ** nstep * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + value_gamma.unsqueeze(-1).unsqueeze(-1) * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    u = F.smooth_l1_loss(target_q_s_a, q_s_a, reduction='none')\n    loss = (u * (tau - (target_q_s_a - q_s_a).detach().le(0.0).float()).abs()).sum(-1).mean(1)\n    return ((loss * weight).mean(), loss)",
            "def qrdqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in QRDQN\\n    Arguments:\\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(4, 3, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = qrdqn_nstep_td_data(q, next_q, action, next_action, reward, done, 3, None)\\n        >>> loss, td_error_per_sample = qrdqn_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, tau, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action, :].unsqueeze(2)\n    target_q_s_a = next_n_q[batch_range, next_n_action, :].unsqueeze(1)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + gamma ** nstep * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + value_gamma.unsqueeze(-1).unsqueeze(-1) * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    u = F.smooth_l1_loss(target_q_s_a, q_s_a, reduction='none')\n    loss = (u * (tau - (target_q_s_a - q_s_a).detach().le(0.0).float()).abs()).sum(-1).mean(1)\n    return ((loss * weight).mean(), loss)",
            "def qrdqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in QRDQN\\n    Arguments:\\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(4, 3, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = qrdqn_nstep_td_data(q, next_q, action, next_action, reward, done, 3, None)\\n        >>> loss, td_error_per_sample = qrdqn_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, tau, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action, :].unsqueeze(2)\n    target_q_s_a = next_n_q[batch_range, next_n_action, :].unsqueeze(1)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + gamma ** nstep * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + value_gamma.unsqueeze(-1).unsqueeze(-1) * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    u = F.smooth_l1_loss(target_q_s_a, q_s_a, reduction='none')\n    loss = (u * (tau - (target_q_s_a - q_s_a).detach().le(0.0).float()).abs()).sum(-1).mean(1)\n    return ((loss * weight).mean(), loss)",
            "def qrdqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in QRDQN\\n    Arguments:\\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(4, 3, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = qrdqn_nstep_td_data(q, next_q, action, next_action, reward, done, 3, None)\\n        >>> loss, td_error_per_sample = qrdqn_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, tau, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action, :].unsqueeze(2)\n    target_q_s_a = next_n_q[batch_range, next_n_action, :].unsqueeze(1)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + gamma ** nstep * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + value_gamma.unsqueeze(-1).unsqueeze(-1) * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    u = F.smooth_l1_loss(target_q_s_a, q_s_a, reduction='none')\n    loss = (u * (tau - (target_q_s_a - q_s_a).detach().le(0.0).float()).abs()).sum(-1).mean(1)\n    return ((loss * weight).mean(), loss)",
            "def qrdqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in QRDQN\\n    Arguments:\\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(4, 3, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = qrdqn_nstep_td_data(q, next_q, action, next_action, reward, done, 3, None)\\n        >>> loss, td_error_per_sample = qrdqn_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, tau, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action, :].unsqueeze(2)\n    target_q_s_a = next_n_q[batch_range, next_n_action, :].unsqueeze(1)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + gamma ** nstep * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1).unsqueeze(-1) + value_gamma.unsqueeze(-1).unsqueeze(-1) * target_q_s_a * (1 - done).unsqueeze(-1).unsqueeze(-1)\n    u = F.smooth_l1_loss(target_q_s_a, q_s_a, reduction='none')\n    loss = (u * (tau - (target_q_s_a - q_s_a).detach().le(0.0).float()).abs()).sum(-1).mean(1)\n    return ((loss * weight).mean(), loss)"
        ]
    },
    {
        "func_name": "q_nstep_sql_td_error",
        "original": "def q_nstep_sql_td_error(data: namedtuple, gamma: float, alpha: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep (1 step or n step) td_error for q-learning based algorithm\n    Arguments:\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_sql_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - Alpha (:obj:\uff40float`): A parameter to weight entropy term in a policy equation\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target soft_q_value\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n        - nstep (:obj:`int`): nstep num, default set to 1\n    Returns:\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\n    Shapes:\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\n    Examples:\n        >>> next_q = torch.randn(4, 3)\n        >>> done = torch.randn(4)\n        >>> action = torch.randint(0, 3, size=(4, ))\n        >>> next_action = torch.randint(0, 3, size=(4, ))\n        >>> nstep = 3\n        >>> q = torch.randn(4, 3).requires_grad_(True)\n        >>> reward = torch.rand(nstep, 4)\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\n        >>> loss, td_error_per_sample, record_target_v = q_nstep_sql_td_error(data, 0.95, 1.0, nstep=nstep)\n    \"\"\"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_v = alpha * torch.logsumexp(next_n_q / alpha, 1)\n    target_v[target_v == float('Inf')] = 20\n    target_v[target_v == float('-Inf')] = -20\n    record_target_v = copy.deepcopy(target_v)\n    if cum_reward:\n        if value_gamma is None:\n            target_v = reward + gamma ** nstep * target_v * (1 - done)\n        else:\n            target_v = reward + value_gamma * target_v * (1 - done)\n    else:\n        target_v = nstep_return(nstep_return_data(reward, target_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, record_target_v)",
        "mutated": [
            "def q_nstep_sql_td_error(data: namedtuple, gamma: float, alpha: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_sql_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - Alpha (:obj:\uff40float`): A parameter to weight entropy term in a policy equation\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target soft_q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample, record_target_v = q_nstep_sql_td_error(data, 0.95, 1.0, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_v = alpha * torch.logsumexp(next_n_q / alpha, 1)\n    target_v[target_v == float('Inf')] = 20\n    target_v[target_v == float('-Inf')] = -20\n    record_target_v = copy.deepcopy(target_v)\n    if cum_reward:\n        if value_gamma is None:\n            target_v = reward + gamma ** nstep * target_v * (1 - done)\n        else:\n            target_v = reward + value_gamma * target_v * (1 - done)\n    else:\n        target_v = nstep_return(nstep_return_data(reward, target_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, record_target_v)",
            "def q_nstep_sql_td_error(data: namedtuple, gamma: float, alpha: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_sql_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - Alpha (:obj:\uff40float`): A parameter to weight entropy term in a policy equation\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target soft_q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample, record_target_v = q_nstep_sql_td_error(data, 0.95, 1.0, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_v = alpha * torch.logsumexp(next_n_q / alpha, 1)\n    target_v[target_v == float('Inf')] = 20\n    target_v[target_v == float('-Inf')] = -20\n    record_target_v = copy.deepcopy(target_v)\n    if cum_reward:\n        if value_gamma is None:\n            target_v = reward + gamma ** nstep * target_v * (1 - done)\n        else:\n            target_v = reward + value_gamma * target_v * (1 - done)\n    else:\n        target_v = nstep_return(nstep_return_data(reward, target_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, record_target_v)",
            "def q_nstep_sql_td_error(data: namedtuple, gamma: float, alpha: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_sql_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - Alpha (:obj:\uff40float`): A parameter to weight entropy term in a policy equation\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target soft_q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample, record_target_v = q_nstep_sql_td_error(data, 0.95, 1.0, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_v = alpha * torch.logsumexp(next_n_q / alpha, 1)\n    target_v[target_v == float('Inf')] = 20\n    target_v[target_v == float('-Inf')] = -20\n    record_target_v = copy.deepcopy(target_v)\n    if cum_reward:\n        if value_gamma is None:\n            target_v = reward + gamma ** nstep * target_v * (1 - done)\n        else:\n            target_v = reward + value_gamma * target_v * (1 - done)\n    else:\n        target_v = nstep_return(nstep_return_data(reward, target_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, record_target_v)",
            "def q_nstep_sql_td_error(data: namedtuple, gamma: float, alpha: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_sql_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - Alpha (:obj:\uff40float`): A parameter to weight entropy term in a policy equation\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target soft_q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample, record_target_v = q_nstep_sql_td_error(data, 0.95, 1.0, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_v = alpha * torch.logsumexp(next_n_q / alpha, 1)\n    target_v[target_v == float('Inf')] = 20\n    target_v[target_v == float('-Inf')] = -20\n    record_target_v = copy.deepcopy(target_v)\n    if cum_reward:\n        if value_gamma is None:\n            target_v = reward + gamma ** nstep * target_v * (1 - done)\n        else:\n            target_v = reward + value_gamma * target_v * (1 - done)\n    else:\n        target_v = nstep_return(nstep_return_data(reward, target_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, record_target_v)",
            "def q_nstep_sql_td_error(data: namedtuple, gamma: float, alpha: float, nstep: int=1, cum_reward: bool=False, value_gamma: Optional[torch.Tensor]=None, criterion: torch.nn.modules=nn.MSELoss(reduction='none')) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error for q-learning based algorithm\\n    Arguments:\\n        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_sql_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - Alpha (:obj:\uff40float`): A parameter to weight entropy term in a policy equation\\n        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data\\n        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target soft_q_value\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n        - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing            ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3).requires_grad_(True)\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = q_nstep_td_data(q, next_q, action, next_action, reward, done, None)\\n        >>> loss, td_error_per_sample, record_target_v = q_nstep_sql_td_error(data, 0.95, 1.0, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, weight) = data\n    assert len(action.shape) == 1, action.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_range = torch.arange(action.shape[0])\n    q_s_a = q[batch_range, action]\n    target_v = alpha * torch.logsumexp(next_n_q / alpha, 1)\n    target_v[target_v == float('Inf')] = 20\n    target_v[target_v == float('-Inf')] = -20\n    record_target_v = copy.deepcopy(target_v)\n    if cum_reward:\n        if value_gamma is None:\n            target_v = reward + gamma ** nstep * target_v * (1 - done)\n        else:\n            target_v = reward + value_gamma * target_v * (1 - done)\n    else:\n        target_v = nstep_return(nstep_return_data(reward, target_v, done), gamma, nstep, value_gamma)\n    td_error_per_sample = criterion(q_s_a, target_v.detach())\n    return ((td_error_per_sample * weight).mean(), td_error_per_sample, record_target_v)"
        ]
    },
    {
        "func_name": "iqn_nstep_td_error",
        "original": "def iqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep (1 step or n step) td_error with in IQN,             referenced paper Implicit Quantile Networks for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1806.06923.pdf>\n    Arguments:\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - nstep (:obj:`int`): nstep num, default set to 1\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n        - beta_function (:obj:`Callable`): The risk function\n    Returns:\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\n    Shapes:\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n    Examples:\n        >>> next_q = torch.randn(3, 4, 3)\n        >>> done = torch.randn(4)\n        >>> action = torch.randint(0, 3, size=(4, ))\n        >>> next_action = torch.randint(0, 3, size=(4, ))\n        >>> nstep = 3\n        >>> q = torch.randn(3, 4, 3).requires_grad_(True)\n        >>> replay_quantile = torch.randn([3, 4, 1])\n        >>> reward = torch.rand(nstep, 4)\n        >>> data = iqn_nstep_td_data(q, next_q, action, next_action, reward, done, replay_quantile, None)\n        >>> loss, td_error_per_sample = iqn_nstep_td_error(data, 0.95, nstep=nstep)\n    \"\"\"\n    (q, next_n_q, action, next_n_action, reward, done, replay_quantiles, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[0]\n    tau_prime = next_n_q.shape[0]\n    action = action.repeat([tau, 1]).unsqueeze(-1)\n    next_n_action = next_n_action.repeat([tau_prime, 1]).unsqueeze(-1)\n    q_s_a = torch.gather(q, -1, action).permute([1, 0, 2])\n    target_q_s_a = torch.gather(next_n_q, -1, next_n_action).permute([1, 0, 2])\n    assert reward.shape[0] == nstep\n    device = torch.device('cuda' if reward.is_cuda else 'cpu')\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a[:, :, None, :] - q_s_a[:, None, :, :]\n    huber_loss = torch.where(bellman_errors.abs() <= kappa, 0.5 * bellman_errors ** 2, kappa * (bellman_errors.abs() - 0.5 * kappa))\n    replay_quantiles = replay_quantiles.reshape([tau, batch_size, 1]).permute([1, 0, 2])\n    replay_quantiles = replay_quantiles[:, None, :, :].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(replay_quantiles - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)",
        "mutated": [
            "def iqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in IQN,             referenced paper Implicit Quantile Networks for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1806.06923.pdf>\\n    Arguments:\\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - beta_function (:obj:`Callable`): The risk function\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(3, 4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(3, 4, 3).requires_grad_(True)\\n        >>> replay_quantile = torch.randn([3, 4, 1])\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = iqn_nstep_td_data(q, next_q, action, next_action, reward, done, replay_quantile, None)\\n        >>> loss, td_error_per_sample = iqn_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, replay_quantiles, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[0]\n    tau_prime = next_n_q.shape[0]\n    action = action.repeat([tau, 1]).unsqueeze(-1)\n    next_n_action = next_n_action.repeat([tau_prime, 1]).unsqueeze(-1)\n    q_s_a = torch.gather(q, -1, action).permute([1, 0, 2])\n    target_q_s_a = torch.gather(next_n_q, -1, next_n_action).permute([1, 0, 2])\n    assert reward.shape[0] == nstep\n    device = torch.device('cuda' if reward.is_cuda else 'cpu')\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a[:, :, None, :] - q_s_a[:, None, :, :]\n    huber_loss = torch.where(bellman_errors.abs() <= kappa, 0.5 * bellman_errors ** 2, kappa * (bellman_errors.abs() - 0.5 * kappa))\n    replay_quantiles = replay_quantiles.reshape([tau, batch_size, 1]).permute([1, 0, 2])\n    replay_quantiles = replay_quantiles[:, None, :, :].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(replay_quantiles - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)",
            "def iqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in IQN,             referenced paper Implicit Quantile Networks for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1806.06923.pdf>\\n    Arguments:\\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - beta_function (:obj:`Callable`): The risk function\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(3, 4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(3, 4, 3).requires_grad_(True)\\n        >>> replay_quantile = torch.randn([3, 4, 1])\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = iqn_nstep_td_data(q, next_q, action, next_action, reward, done, replay_quantile, None)\\n        >>> loss, td_error_per_sample = iqn_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, replay_quantiles, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[0]\n    tau_prime = next_n_q.shape[0]\n    action = action.repeat([tau, 1]).unsqueeze(-1)\n    next_n_action = next_n_action.repeat([tau_prime, 1]).unsqueeze(-1)\n    q_s_a = torch.gather(q, -1, action).permute([1, 0, 2])\n    target_q_s_a = torch.gather(next_n_q, -1, next_n_action).permute([1, 0, 2])\n    assert reward.shape[0] == nstep\n    device = torch.device('cuda' if reward.is_cuda else 'cpu')\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a[:, :, None, :] - q_s_a[:, None, :, :]\n    huber_loss = torch.where(bellman_errors.abs() <= kappa, 0.5 * bellman_errors ** 2, kappa * (bellman_errors.abs() - 0.5 * kappa))\n    replay_quantiles = replay_quantiles.reshape([tau, batch_size, 1]).permute([1, 0, 2])\n    replay_quantiles = replay_quantiles[:, None, :, :].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(replay_quantiles - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)",
            "def iqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in IQN,             referenced paper Implicit Quantile Networks for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1806.06923.pdf>\\n    Arguments:\\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - beta_function (:obj:`Callable`): The risk function\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(3, 4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(3, 4, 3).requires_grad_(True)\\n        >>> replay_quantile = torch.randn([3, 4, 1])\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = iqn_nstep_td_data(q, next_q, action, next_action, reward, done, replay_quantile, None)\\n        >>> loss, td_error_per_sample = iqn_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, replay_quantiles, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[0]\n    tau_prime = next_n_q.shape[0]\n    action = action.repeat([tau, 1]).unsqueeze(-1)\n    next_n_action = next_n_action.repeat([tau_prime, 1]).unsqueeze(-1)\n    q_s_a = torch.gather(q, -1, action).permute([1, 0, 2])\n    target_q_s_a = torch.gather(next_n_q, -1, next_n_action).permute([1, 0, 2])\n    assert reward.shape[0] == nstep\n    device = torch.device('cuda' if reward.is_cuda else 'cpu')\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a[:, :, None, :] - q_s_a[:, None, :, :]\n    huber_loss = torch.where(bellman_errors.abs() <= kappa, 0.5 * bellman_errors ** 2, kappa * (bellman_errors.abs() - 0.5 * kappa))\n    replay_quantiles = replay_quantiles.reshape([tau, batch_size, 1]).permute([1, 0, 2])\n    replay_quantiles = replay_quantiles[:, None, :, :].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(replay_quantiles - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)",
            "def iqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in IQN,             referenced paper Implicit Quantile Networks for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1806.06923.pdf>\\n    Arguments:\\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - beta_function (:obj:`Callable`): The risk function\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(3, 4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(3, 4, 3).requires_grad_(True)\\n        >>> replay_quantile = torch.randn([3, 4, 1])\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = iqn_nstep_td_data(q, next_q, action, next_action, reward, done, replay_quantile, None)\\n        >>> loss, td_error_per_sample = iqn_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, replay_quantiles, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[0]\n    tau_prime = next_n_q.shape[0]\n    action = action.repeat([tau, 1]).unsqueeze(-1)\n    next_n_action = next_n_action.repeat([tau_prime, 1]).unsqueeze(-1)\n    q_s_a = torch.gather(q, -1, action).permute([1, 0, 2])\n    target_q_s_a = torch.gather(next_n_q, -1, next_n_action).permute([1, 0, 2])\n    assert reward.shape[0] == nstep\n    device = torch.device('cuda' if reward.is_cuda else 'cpu')\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a[:, :, None, :] - q_s_a[:, None, :, :]\n    huber_loss = torch.where(bellman_errors.abs() <= kappa, 0.5 * bellman_errors ** 2, kappa * (bellman_errors.abs() - 0.5 * kappa))\n    replay_quantiles = replay_quantiles.reshape([tau, batch_size, 1]).permute([1, 0, 2])\n    replay_quantiles = replay_quantiles[:, None, :, :].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(replay_quantiles - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)",
            "def iqn_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in IQN,             referenced paper Implicit Quantile Networks for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1806.06923.pdf>\\n    Arguments:\\n        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - beta_function (:obj:`Callable`): The risk function\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n    Examples:\\n        >>> next_q = torch.randn(3, 4, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(3, 4, 3).requires_grad_(True)\\n        >>> replay_quantile = torch.randn([3, 4, 1])\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = iqn_nstep_td_data(q, next_q, action, next_action, reward, done, replay_quantile, None)\\n        >>> loss, td_error_per_sample = iqn_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, replay_quantiles, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[0]\n    tau_prime = next_n_q.shape[0]\n    action = action.repeat([tau, 1]).unsqueeze(-1)\n    next_n_action = next_n_action.repeat([tau_prime, 1]).unsqueeze(-1)\n    q_s_a = torch.gather(q, -1, action).permute([1, 0, 2])\n    target_q_s_a = torch.gather(next_n_q, -1, next_n_action).permute([1, 0, 2])\n    assert reward.shape[0] == nstep\n    device = torch.device('cuda' if reward.is_cuda else 'cpu')\n    reward_factor = torch.ones(nstep).to(device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a[:, :, None, :] - q_s_a[:, None, :, :]\n    huber_loss = torch.where(bellman_errors.abs() <= kappa, 0.5 * bellman_errors ** 2, kappa * (bellman_errors.abs() - 0.5 * kappa))\n    replay_quantiles = replay_quantiles.reshape([tau, batch_size, 1]).permute([1, 0, 2])\n    replay_quantiles = replay_quantiles[:, None, :, :].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(replay_quantiles - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)"
        ]
    },
    {
        "func_name": "fqf_nstep_td_error",
        "original": "def fqf_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Multistep (1 step or n step) td_error with in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\n    Arguments:\n        - data (:obj:`fqf_nstep_td_data`): The input data, fqf_nstep_td_data to calculate loss\n        - gamma (:obj:`float`): Discount factor\n        - nstep (:obj:`int`): nstep num, default set to 1\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\n        - beta_function (:obj:`Callable`): The risk function\n    Returns:\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\n    Shapes:\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\n        - q (:obj:`torch.FloatTensor`): :math:`(B, tau, N)` i.e. [batch_size, tau, action_dim]\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, tau', N)`\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\n        - quantiles_hats (:obj:`torch.FloatTensor`): :math:`(B, tau)`\n    Examples:\n        >>> next_q = torch.randn(4, 3, 3)\n        >>> done = torch.randn(4)\n        >>> action = torch.randint(0, 3, size=(4, ))\n        >>> next_action = torch.randint(0, 3, size=(4, ))\n        >>> nstep = 3\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\n        >>> quantiles_hats = torch.randn([4, 3])\n        >>> reward = torch.rand(nstep, 4)\n        >>> data = fqf_nstep_td_data(q, next_q, action, next_action, reward, done, quantiles_hats, None)\n        >>> loss, td_error_per_sample = fqf_nstep_td_error(data, 0.95, nstep=nstep)\n    \"\"\"\n    (q, next_n_q, action, next_n_action, reward, done, quantiles_hats, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[1]\n    tau_prime = next_n_q.shape[1]\n    q_s_a = evaluate_quantile_at_action(q, action)\n    target_q_s_a = evaluate_quantile_at_action(next_n_q, next_n_action)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward.device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a.unsqueeze(2) - q_s_a.unsqueeze(1)\n    huber_loss = F.smooth_l1_loss(target_q_s_a.unsqueeze(2), q_s_a.unsqueeze(1), reduction='none')\n    quantiles_hats = quantiles_hats[:, None, :, None].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(quantiles_hats - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)",
        "mutated": [
            "def fqf_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\\n    Arguments:\\n        - data (:obj:`fqf_nstep_td_data`): The input data, fqf_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - beta_function (:obj:`Callable`): The risk function\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, tau, N)` i.e. [batch_size, tau, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, tau', N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - quantiles_hats (:obj:`torch.FloatTensor`): :math:`(B, tau)`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\\n        >>> quantiles_hats = torch.randn([4, 3])\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = fqf_nstep_td_data(q, next_q, action, next_action, reward, done, quantiles_hats, None)\\n        >>> loss, td_error_per_sample = fqf_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, quantiles_hats, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[1]\n    tau_prime = next_n_q.shape[1]\n    q_s_a = evaluate_quantile_at_action(q, action)\n    target_q_s_a = evaluate_quantile_at_action(next_n_q, next_n_action)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward.device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a.unsqueeze(2) - q_s_a.unsqueeze(1)\n    huber_loss = F.smooth_l1_loss(target_q_s_a.unsqueeze(2), q_s_a.unsqueeze(1), reduction='none')\n    quantiles_hats = quantiles_hats[:, None, :, None].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(quantiles_hats - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)",
            "def fqf_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\\n    Arguments:\\n        - data (:obj:`fqf_nstep_td_data`): The input data, fqf_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - beta_function (:obj:`Callable`): The risk function\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, tau, N)` i.e. [batch_size, tau, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, tau', N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - quantiles_hats (:obj:`torch.FloatTensor`): :math:`(B, tau)`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\\n        >>> quantiles_hats = torch.randn([4, 3])\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = fqf_nstep_td_data(q, next_q, action, next_action, reward, done, quantiles_hats, None)\\n        >>> loss, td_error_per_sample = fqf_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, quantiles_hats, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[1]\n    tau_prime = next_n_q.shape[1]\n    q_s_a = evaluate_quantile_at_action(q, action)\n    target_q_s_a = evaluate_quantile_at_action(next_n_q, next_n_action)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward.device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a.unsqueeze(2) - q_s_a.unsqueeze(1)\n    huber_loss = F.smooth_l1_loss(target_q_s_a.unsqueeze(2), q_s_a.unsqueeze(1), reduction='none')\n    quantiles_hats = quantiles_hats[:, None, :, None].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(quantiles_hats - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)",
            "def fqf_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\\n    Arguments:\\n        - data (:obj:`fqf_nstep_td_data`): The input data, fqf_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - beta_function (:obj:`Callable`): The risk function\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, tau, N)` i.e. [batch_size, tau, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, tau', N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - quantiles_hats (:obj:`torch.FloatTensor`): :math:`(B, tau)`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\\n        >>> quantiles_hats = torch.randn([4, 3])\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = fqf_nstep_td_data(q, next_q, action, next_action, reward, done, quantiles_hats, None)\\n        >>> loss, td_error_per_sample = fqf_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, quantiles_hats, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[1]\n    tau_prime = next_n_q.shape[1]\n    q_s_a = evaluate_quantile_at_action(q, action)\n    target_q_s_a = evaluate_quantile_at_action(next_n_q, next_n_action)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward.device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a.unsqueeze(2) - q_s_a.unsqueeze(1)\n    huber_loss = F.smooth_l1_loss(target_q_s_a.unsqueeze(2), q_s_a.unsqueeze(1), reduction='none')\n    quantiles_hats = quantiles_hats[:, None, :, None].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(quantiles_hats - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)",
            "def fqf_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\\n    Arguments:\\n        - data (:obj:`fqf_nstep_td_data`): The input data, fqf_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - beta_function (:obj:`Callable`): The risk function\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, tau, N)` i.e. [batch_size, tau, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, tau', N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - quantiles_hats (:obj:`torch.FloatTensor`): :math:`(B, tau)`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\\n        >>> quantiles_hats = torch.randn([4, 3])\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = fqf_nstep_td_data(q, next_q, action, next_action, reward, done, quantiles_hats, None)\\n        >>> loss, td_error_per_sample = fqf_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, quantiles_hats, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[1]\n    tau_prime = next_n_q.shape[1]\n    q_s_a = evaluate_quantile_at_action(q, action)\n    target_q_s_a = evaluate_quantile_at_action(next_n_q, next_n_action)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward.device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a.unsqueeze(2) - q_s_a.unsqueeze(1)\n    huber_loss = F.smooth_l1_loss(target_q_s_a.unsqueeze(2), q_s_a.unsqueeze(1), reduction='none')\n    quantiles_hats = quantiles_hats[:, None, :, None].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(quantiles_hats - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)",
            "def fqf_nstep_td_error(data: namedtuple, gamma: float, nstep: int=1, kappa: float=1.0, value_gamma: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Multistep (1 step or n step) td_error with in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\\n    Arguments:\\n        - data (:obj:`fqf_nstep_td_data`): The input data, fqf_nstep_td_data to calculate loss\\n        - gamma (:obj:`float`): Discount factor\\n        - nstep (:obj:`int`): nstep num, default set to 1\\n        - criterion (:obj:`torch.nn.modules`): Loss function criterion\\n        - beta_function (:obj:`Callable`): The risk function\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor\\n    Shapes:\\n        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing        ['q', 'next_n_q', 'action', 'reward', 'done']\\n        - q (:obj:`torch.FloatTensor`): :math:`(B, tau, N)` i.e. [batch_size, tau, action_dim]\\n        - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, tau', N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)\\n        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep\\n        - quantiles_hats (:obj:`torch.FloatTensor`): :math:`(B, tau)`\\n    Examples:\\n        >>> next_q = torch.randn(4, 3, 3)\\n        >>> done = torch.randn(4)\\n        >>> action = torch.randint(0, 3, size=(4, ))\\n        >>> next_action = torch.randint(0, 3, size=(4, ))\\n        >>> nstep = 3\\n        >>> q = torch.randn(4, 3, 3).requires_grad_(True)\\n        >>> quantiles_hats = torch.randn([4, 3])\\n        >>> reward = torch.rand(nstep, 4)\\n        >>> data = fqf_nstep_td_data(q, next_q, action, next_action, reward, done, quantiles_hats, None)\\n        >>> loss, td_error_per_sample = fqf_nstep_td_error(data, 0.95, nstep=nstep)\\n    \"\n    (q, next_n_q, action, next_n_action, reward, done, quantiles_hats, weight) = data\n    assert len(action.shape) == 1, action.shape\n    assert len(next_n_action.shape) == 1, next_n_action.shape\n    assert len(done.shape) == 1, done.shape\n    assert len(q.shape) == 3, q.shape\n    assert len(next_n_q.shape) == 3, next_n_q.shape\n    assert len(reward.shape) == 2, reward.shape\n    if weight is None:\n        weight = torch.ones_like(action)\n    batch_size = done.shape[0]\n    tau = q.shape[1]\n    tau_prime = next_n_q.shape[1]\n    q_s_a = evaluate_quantile_at_action(q, action)\n    target_q_s_a = evaluate_quantile_at_action(next_n_q, next_n_action)\n    assert reward.shape[0] == nstep\n    reward_factor = torch.ones(nstep).to(reward.device)\n    for i in range(1, nstep):\n        reward_factor[i] = gamma * reward_factor[i - 1]\n    reward = torch.matmul(reward_factor, reward)\n    if value_gamma is None:\n        target_q_s_a = reward.unsqueeze(-1) + gamma ** nstep * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    else:\n        target_q_s_a = reward.unsqueeze(-1) + value_gamma.unsqueeze(-1) * target_q_s_a.squeeze(-1) * (1 - done).unsqueeze(-1)\n    target_q_s_a = target_q_s_a.unsqueeze(-1)\n    bellman_errors = target_q_s_a.unsqueeze(2) - q_s_a.unsqueeze(1)\n    huber_loss = F.smooth_l1_loss(target_q_s_a.unsqueeze(2), q_s_a.unsqueeze(1), reduction='none')\n    quantiles_hats = quantiles_hats[:, None, :, None].repeat([1, tau_prime, 1, 1])\n    quantile_huber_loss = torch.abs(quantiles_hats - (bellman_errors < 0).float().detach()) * huber_loss / kappa\n    loss = quantile_huber_loss.sum(dim=2).mean(dim=1)[:, 0]\n    return ((loss * weight).mean(), loss)"
        ]
    },
    {
        "func_name": "evaluate_quantile_at_action",
        "original": "def evaluate_quantile_at_action(q_s, actions):\n    assert q_s.shape[0] == actions.shape[0]\n    (batch_size, num_quantiles) = q_s.shape[:2]\n    action_index = actions[:, None, None].expand(batch_size, num_quantiles, 1)\n    q_s_a = q_s.gather(dim=2, index=action_index)\n    return q_s_a",
        "mutated": [
            "def evaluate_quantile_at_action(q_s, actions):\n    if False:\n        i = 10\n    assert q_s.shape[0] == actions.shape[0]\n    (batch_size, num_quantiles) = q_s.shape[:2]\n    action_index = actions[:, None, None].expand(batch_size, num_quantiles, 1)\n    q_s_a = q_s.gather(dim=2, index=action_index)\n    return q_s_a",
            "def evaluate_quantile_at_action(q_s, actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert q_s.shape[0] == actions.shape[0]\n    (batch_size, num_quantiles) = q_s.shape[:2]\n    action_index = actions[:, None, None].expand(batch_size, num_quantiles, 1)\n    q_s_a = q_s.gather(dim=2, index=action_index)\n    return q_s_a",
            "def evaluate_quantile_at_action(q_s, actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert q_s.shape[0] == actions.shape[0]\n    (batch_size, num_quantiles) = q_s.shape[:2]\n    action_index = actions[:, None, None].expand(batch_size, num_quantiles, 1)\n    q_s_a = q_s.gather(dim=2, index=action_index)\n    return q_s_a",
            "def evaluate_quantile_at_action(q_s, actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert q_s.shape[0] == actions.shape[0]\n    (batch_size, num_quantiles) = q_s.shape[:2]\n    action_index = actions[:, None, None].expand(batch_size, num_quantiles, 1)\n    q_s_a = q_s.gather(dim=2, index=action_index)\n    return q_s_a",
            "def evaluate_quantile_at_action(q_s, actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert q_s.shape[0] == actions.shape[0]\n    (batch_size, num_quantiles) = q_s.shape[:2]\n    action_index = actions[:, None, None].expand(batch_size, num_quantiles, 1)\n    q_s_a = q_s.gather(dim=2, index=action_index)\n    return q_s_a"
        ]
    },
    {
        "func_name": "fqf_calculate_fraction_loss",
        "original": "def fqf_calculate_fraction_loss(q_tau_i, q_value, quantiles, actions):\n    \"\"\"\n    Overview:\n        Calculate the fraction loss in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\n    Arguments:\n        - q_tau_i (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles-1, action_dim)`\n        - q_value (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles, action_dim)`\n        - quantiles (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles+1)`\n        - actions (:obj:`torch.LongTensor`): :math:`(batch_size, )`\n    Returns:\n        - fraction_loss (:obj:`torch.Tensor`): fraction loss, 0-dim tensor\n    \"\"\"\n    assert q_value.requires_grad\n    batch_size = q_value.shape[0]\n    num_quantiles = q_value.shape[1]\n    with torch.no_grad():\n        sa_quantiles = evaluate_quantile_at_action(q_tau_i, actions)\n        assert sa_quantiles.shape == (batch_size, num_quantiles - 1, 1)\n        q_s_a_hats = evaluate_quantile_at_action(q_value, actions)\n        assert q_s_a_hats.shape == (batch_size, num_quantiles, 1)\n        assert not q_s_a_hats.requires_grad\n    values_1 = sa_quantiles - q_s_a_hats[:, :-1]\n    signs_1 = sa_quantiles > torch.cat([q_s_a_hats[:, :1], sa_quantiles[:, :-1]], dim=1)\n    assert values_1.shape == signs_1.shape\n    values_2 = sa_quantiles - q_s_a_hats[:, 1:]\n    signs_2 = sa_quantiles < torch.cat([sa_quantiles[:, 1:], q_s_a_hats[:, -1:]], dim=1)\n    assert values_2.shape == signs_2.shape\n    gradient_of_taus = (torch.where(signs_1, values_1, -values_1) + torch.where(signs_2, values_2, -values_2)).view(batch_size, num_quantiles - 1)\n    assert not gradient_of_taus.requires_grad\n    assert gradient_of_taus.shape == quantiles[:, 1:-1].shape\n    fraction_loss = (gradient_of_taus * quantiles[:, 1:-1]).sum(dim=1).mean()\n    return fraction_loss",
        "mutated": [
            "def fqf_calculate_fraction_loss(q_tau_i, q_value, quantiles, actions):\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Calculate the fraction loss in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\\n    Arguments:\\n        - q_tau_i (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles-1, action_dim)`\\n        - q_value (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles, action_dim)`\\n        - quantiles (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles+1)`\\n        - actions (:obj:`torch.LongTensor`): :math:`(batch_size, )`\\n    Returns:\\n        - fraction_loss (:obj:`torch.Tensor`): fraction loss, 0-dim tensor\\n    '\n    assert q_value.requires_grad\n    batch_size = q_value.shape[0]\n    num_quantiles = q_value.shape[1]\n    with torch.no_grad():\n        sa_quantiles = evaluate_quantile_at_action(q_tau_i, actions)\n        assert sa_quantiles.shape == (batch_size, num_quantiles - 1, 1)\n        q_s_a_hats = evaluate_quantile_at_action(q_value, actions)\n        assert q_s_a_hats.shape == (batch_size, num_quantiles, 1)\n        assert not q_s_a_hats.requires_grad\n    values_1 = sa_quantiles - q_s_a_hats[:, :-1]\n    signs_1 = sa_quantiles > torch.cat([q_s_a_hats[:, :1], sa_quantiles[:, :-1]], dim=1)\n    assert values_1.shape == signs_1.shape\n    values_2 = sa_quantiles - q_s_a_hats[:, 1:]\n    signs_2 = sa_quantiles < torch.cat([sa_quantiles[:, 1:], q_s_a_hats[:, -1:]], dim=1)\n    assert values_2.shape == signs_2.shape\n    gradient_of_taus = (torch.where(signs_1, values_1, -values_1) + torch.where(signs_2, values_2, -values_2)).view(batch_size, num_quantiles - 1)\n    assert not gradient_of_taus.requires_grad\n    assert gradient_of_taus.shape == quantiles[:, 1:-1].shape\n    fraction_loss = (gradient_of_taus * quantiles[:, 1:-1]).sum(dim=1).mean()\n    return fraction_loss",
            "def fqf_calculate_fraction_loss(q_tau_i, q_value, quantiles, actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Calculate the fraction loss in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\\n    Arguments:\\n        - q_tau_i (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles-1, action_dim)`\\n        - q_value (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles, action_dim)`\\n        - quantiles (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles+1)`\\n        - actions (:obj:`torch.LongTensor`): :math:`(batch_size, )`\\n    Returns:\\n        - fraction_loss (:obj:`torch.Tensor`): fraction loss, 0-dim tensor\\n    '\n    assert q_value.requires_grad\n    batch_size = q_value.shape[0]\n    num_quantiles = q_value.shape[1]\n    with torch.no_grad():\n        sa_quantiles = evaluate_quantile_at_action(q_tau_i, actions)\n        assert sa_quantiles.shape == (batch_size, num_quantiles - 1, 1)\n        q_s_a_hats = evaluate_quantile_at_action(q_value, actions)\n        assert q_s_a_hats.shape == (batch_size, num_quantiles, 1)\n        assert not q_s_a_hats.requires_grad\n    values_1 = sa_quantiles - q_s_a_hats[:, :-1]\n    signs_1 = sa_quantiles > torch.cat([q_s_a_hats[:, :1], sa_quantiles[:, :-1]], dim=1)\n    assert values_1.shape == signs_1.shape\n    values_2 = sa_quantiles - q_s_a_hats[:, 1:]\n    signs_2 = sa_quantiles < torch.cat([sa_quantiles[:, 1:], q_s_a_hats[:, -1:]], dim=1)\n    assert values_2.shape == signs_2.shape\n    gradient_of_taus = (torch.where(signs_1, values_1, -values_1) + torch.where(signs_2, values_2, -values_2)).view(batch_size, num_quantiles - 1)\n    assert not gradient_of_taus.requires_grad\n    assert gradient_of_taus.shape == quantiles[:, 1:-1].shape\n    fraction_loss = (gradient_of_taus * quantiles[:, 1:-1]).sum(dim=1).mean()\n    return fraction_loss",
            "def fqf_calculate_fraction_loss(q_tau_i, q_value, quantiles, actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Calculate the fraction loss in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\\n    Arguments:\\n        - q_tau_i (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles-1, action_dim)`\\n        - q_value (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles, action_dim)`\\n        - quantiles (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles+1)`\\n        - actions (:obj:`torch.LongTensor`): :math:`(batch_size, )`\\n    Returns:\\n        - fraction_loss (:obj:`torch.Tensor`): fraction loss, 0-dim tensor\\n    '\n    assert q_value.requires_grad\n    batch_size = q_value.shape[0]\n    num_quantiles = q_value.shape[1]\n    with torch.no_grad():\n        sa_quantiles = evaluate_quantile_at_action(q_tau_i, actions)\n        assert sa_quantiles.shape == (batch_size, num_quantiles - 1, 1)\n        q_s_a_hats = evaluate_quantile_at_action(q_value, actions)\n        assert q_s_a_hats.shape == (batch_size, num_quantiles, 1)\n        assert not q_s_a_hats.requires_grad\n    values_1 = sa_quantiles - q_s_a_hats[:, :-1]\n    signs_1 = sa_quantiles > torch.cat([q_s_a_hats[:, :1], sa_quantiles[:, :-1]], dim=1)\n    assert values_1.shape == signs_1.shape\n    values_2 = sa_quantiles - q_s_a_hats[:, 1:]\n    signs_2 = sa_quantiles < torch.cat([sa_quantiles[:, 1:], q_s_a_hats[:, -1:]], dim=1)\n    assert values_2.shape == signs_2.shape\n    gradient_of_taus = (torch.where(signs_1, values_1, -values_1) + torch.where(signs_2, values_2, -values_2)).view(batch_size, num_quantiles - 1)\n    assert not gradient_of_taus.requires_grad\n    assert gradient_of_taus.shape == quantiles[:, 1:-1].shape\n    fraction_loss = (gradient_of_taus * quantiles[:, 1:-1]).sum(dim=1).mean()\n    return fraction_loss",
            "def fqf_calculate_fraction_loss(q_tau_i, q_value, quantiles, actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Calculate the fraction loss in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\\n    Arguments:\\n        - q_tau_i (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles-1, action_dim)`\\n        - q_value (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles, action_dim)`\\n        - quantiles (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles+1)`\\n        - actions (:obj:`torch.LongTensor`): :math:`(batch_size, )`\\n    Returns:\\n        - fraction_loss (:obj:`torch.Tensor`): fraction loss, 0-dim tensor\\n    '\n    assert q_value.requires_grad\n    batch_size = q_value.shape[0]\n    num_quantiles = q_value.shape[1]\n    with torch.no_grad():\n        sa_quantiles = evaluate_quantile_at_action(q_tau_i, actions)\n        assert sa_quantiles.shape == (batch_size, num_quantiles - 1, 1)\n        q_s_a_hats = evaluate_quantile_at_action(q_value, actions)\n        assert q_s_a_hats.shape == (batch_size, num_quantiles, 1)\n        assert not q_s_a_hats.requires_grad\n    values_1 = sa_quantiles - q_s_a_hats[:, :-1]\n    signs_1 = sa_quantiles > torch.cat([q_s_a_hats[:, :1], sa_quantiles[:, :-1]], dim=1)\n    assert values_1.shape == signs_1.shape\n    values_2 = sa_quantiles - q_s_a_hats[:, 1:]\n    signs_2 = sa_quantiles < torch.cat([sa_quantiles[:, 1:], q_s_a_hats[:, -1:]], dim=1)\n    assert values_2.shape == signs_2.shape\n    gradient_of_taus = (torch.where(signs_1, values_1, -values_1) + torch.where(signs_2, values_2, -values_2)).view(batch_size, num_quantiles - 1)\n    assert not gradient_of_taus.requires_grad\n    assert gradient_of_taus.shape == quantiles[:, 1:-1].shape\n    fraction_loss = (gradient_of_taus * quantiles[:, 1:-1]).sum(dim=1).mean()\n    return fraction_loss",
            "def fqf_calculate_fraction_loss(q_tau_i, q_value, quantiles, actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Calculate the fraction loss in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             <https://arxiv.org/pdf/1911.02140.pdf>\\n    Arguments:\\n        - q_tau_i (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles-1, action_dim)`\\n        - q_value (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles, action_dim)`\\n        - quantiles (:obj:`torch.FloatTensor`): :math:`(batch_size, num_quantiles+1)`\\n        - actions (:obj:`torch.LongTensor`): :math:`(batch_size, )`\\n    Returns:\\n        - fraction_loss (:obj:`torch.Tensor`): fraction loss, 0-dim tensor\\n    '\n    assert q_value.requires_grad\n    batch_size = q_value.shape[0]\n    num_quantiles = q_value.shape[1]\n    with torch.no_grad():\n        sa_quantiles = evaluate_quantile_at_action(q_tau_i, actions)\n        assert sa_quantiles.shape == (batch_size, num_quantiles - 1, 1)\n        q_s_a_hats = evaluate_quantile_at_action(q_value, actions)\n        assert q_s_a_hats.shape == (batch_size, num_quantiles, 1)\n        assert not q_s_a_hats.requires_grad\n    values_1 = sa_quantiles - q_s_a_hats[:, :-1]\n    signs_1 = sa_quantiles > torch.cat([q_s_a_hats[:, :1], sa_quantiles[:, :-1]], dim=1)\n    assert values_1.shape == signs_1.shape\n    values_2 = sa_quantiles - q_s_a_hats[:, 1:]\n    signs_2 = sa_quantiles < torch.cat([sa_quantiles[:, 1:], q_s_a_hats[:, -1:]], dim=1)\n    assert values_2.shape == signs_2.shape\n    gradient_of_taus = (torch.where(signs_1, values_1, -values_1) + torch.where(signs_2, values_2, -values_2)).view(batch_size, num_quantiles - 1)\n    assert not gradient_of_taus.requires_grad\n    assert gradient_of_taus.shape == quantiles[:, 1:-1].shape\n    fraction_loss = (gradient_of_taus * quantiles[:, 1:-1]).sum(dim=1).mean()\n    return fraction_loss"
        ]
    },
    {
        "func_name": "shape_fn_td_lambda",
        "original": "def shape_fn_td_lambda(args, kwargs):\n    \"\"\"\n    Overview:\n        Return td_lambda shape for hpc\n    Returns:\n        shape: [T, B]\n    \"\"\"\n    if len(args) <= 0:\n        tmp = kwargs['data'].reward.shape[0]\n    else:\n        tmp = args[0].reward.shape\n    return tmp",
        "mutated": [
            "def shape_fn_td_lambda(args, kwargs):\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Return td_lambda shape for hpc\\n    Returns:\\n        shape: [T, B]\\n    '\n    if len(args) <= 0:\n        tmp = kwargs['data'].reward.shape[0]\n    else:\n        tmp = args[0].reward.shape\n    return tmp",
            "def shape_fn_td_lambda(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Return td_lambda shape for hpc\\n    Returns:\\n        shape: [T, B]\\n    '\n    if len(args) <= 0:\n        tmp = kwargs['data'].reward.shape[0]\n    else:\n        tmp = args[0].reward.shape\n    return tmp",
            "def shape_fn_td_lambda(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Return td_lambda shape for hpc\\n    Returns:\\n        shape: [T, B]\\n    '\n    if len(args) <= 0:\n        tmp = kwargs['data'].reward.shape[0]\n    else:\n        tmp = args[0].reward.shape\n    return tmp",
            "def shape_fn_td_lambda(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Return td_lambda shape for hpc\\n    Returns:\\n        shape: [T, B]\\n    '\n    if len(args) <= 0:\n        tmp = kwargs['data'].reward.shape[0]\n    else:\n        tmp = args[0].reward.shape\n    return tmp",
            "def shape_fn_td_lambda(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Return td_lambda shape for hpc\\n    Returns:\\n        shape: [T, B]\\n    '\n    if len(args) <= 0:\n        tmp = kwargs['data'].reward.shape[0]\n    else:\n        tmp = args[0].reward.shape\n    return tmp"
        ]
    },
    {
        "func_name": "td_lambda_error",
        "original": "@hpc_wrapper(shape_fn=shape_fn_td_lambda, namedtuple_data=True, include_args=[0, 1, 2], include_kwargs=['data', 'gamma', 'lambda_'])\ndef td_lambda_error(data: namedtuple, gamma: float=0.9, lambda_: float=0.8) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Computing TD(lambda) loss given constant gamma and lambda.\n        There is no special handling for terminal state value,\n        if some state has reached the terminal, just fill in zeros for values and rewards beyond terminal\n        (*including the terminal state*, values[terminal] should also be 0)\n    Arguments:\n        - data (:obj:`namedtuple`): td_lambda input data with fields ['value', 'reward', 'weight']\n        - gamma (:obj:`float`): Constant discount factor gamma, should be in [0, 1], defaults to 0.9\n        - lambda (:obj:`float`): Constant lambda, should be in [0, 1], defaults to 0.8\n    Returns:\n        - loss (:obj:`torch.Tensor`): Computed MSE loss, averaged over the batch\n    Shapes:\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`, where T is trajectory length and B is batch,            which is the estimation of the state value at step 0 to T\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, the returns from time step 0 to T-1\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\n        - loss (:obj:`torch.FloatTensor`): :math:`()`, 0-dim tensor\n    Examples:\n        >>> T, B = 8, 4\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\n        >>> reward = torch.rand(T, B)\n        >>> loss = td_lambda_error(td_lambda_data(value, reward, None))\n    \"\"\"\n    (value, reward, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    with torch.no_grad():\n        return_ = generalized_lambda_returns(value, reward, gamma, lambda_)\n    loss = 0.5 * (F.mse_loss(return_, value[:-1], reduction='none') * weight).mean()\n    return loss",
        "mutated": [
            "@hpc_wrapper(shape_fn=shape_fn_td_lambda, namedtuple_data=True, include_args=[0, 1, 2], include_kwargs=['data', 'gamma', 'lambda_'])\ndef td_lambda_error(data: namedtuple, gamma: float=0.9, lambda_: float=0.8) -> torch.Tensor:\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Computing TD(lambda) loss given constant gamma and lambda.\\n        There is no special handling for terminal state value,\\n        if some state has reached the terminal, just fill in zeros for values and rewards beyond terminal\\n        (*including the terminal state*, values[terminal] should also be 0)\\n    Arguments:\\n        - data (:obj:`namedtuple`): td_lambda input data with fields ['value', 'reward', 'weight']\\n        - gamma (:obj:`float`): Constant discount factor gamma, should be in [0, 1], defaults to 0.9\\n        - lambda (:obj:`float`): Constant lambda, should be in [0, 1], defaults to 0.8\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Computed MSE loss, averaged over the batch\\n    Shapes:\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`, where T is trajectory length and B is batch,            which is the estimation of the state value at step 0 to T\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, the returns from time step 0 to T-1\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n        - loss (:obj:`torch.FloatTensor`): :math:`()`, 0-dim tensor\\n    Examples:\\n        >>> T, B = 8, 4\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> loss = td_lambda_error(td_lambda_data(value, reward, None))\\n    \"\n    (value, reward, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    with torch.no_grad():\n        return_ = generalized_lambda_returns(value, reward, gamma, lambda_)\n    loss = 0.5 * (F.mse_loss(return_, value[:-1], reduction='none') * weight).mean()\n    return loss",
            "@hpc_wrapper(shape_fn=shape_fn_td_lambda, namedtuple_data=True, include_args=[0, 1, 2], include_kwargs=['data', 'gamma', 'lambda_'])\ndef td_lambda_error(data: namedtuple, gamma: float=0.9, lambda_: float=0.8) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Computing TD(lambda) loss given constant gamma and lambda.\\n        There is no special handling for terminal state value,\\n        if some state has reached the terminal, just fill in zeros for values and rewards beyond terminal\\n        (*including the terminal state*, values[terminal] should also be 0)\\n    Arguments:\\n        - data (:obj:`namedtuple`): td_lambda input data with fields ['value', 'reward', 'weight']\\n        - gamma (:obj:`float`): Constant discount factor gamma, should be in [0, 1], defaults to 0.9\\n        - lambda (:obj:`float`): Constant lambda, should be in [0, 1], defaults to 0.8\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Computed MSE loss, averaged over the batch\\n    Shapes:\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`, where T is trajectory length and B is batch,            which is the estimation of the state value at step 0 to T\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, the returns from time step 0 to T-1\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n        - loss (:obj:`torch.FloatTensor`): :math:`()`, 0-dim tensor\\n    Examples:\\n        >>> T, B = 8, 4\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> loss = td_lambda_error(td_lambda_data(value, reward, None))\\n    \"\n    (value, reward, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    with torch.no_grad():\n        return_ = generalized_lambda_returns(value, reward, gamma, lambda_)\n    loss = 0.5 * (F.mse_loss(return_, value[:-1], reduction='none') * weight).mean()\n    return loss",
            "@hpc_wrapper(shape_fn=shape_fn_td_lambda, namedtuple_data=True, include_args=[0, 1, 2], include_kwargs=['data', 'gamma', 'lambda_'])\ndef td_lambda_error(data: namedtuple, gamma: float=0.9, lambda_: float=0.8) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Computing TD(lambda) loss given constant gamma and lambda.\\n        There is no special handling for terminal state value,\\n        if some state has reached the terminal, just fill in zeros for values and rewards beyond terminal\\n        (*including the terminal state*, values[terminal] should also be 0)\\n    Arguments:\\n        - data (:obj:`namedtuple`): td_lambda input data with fields ['value', 'reward', 'weight']\\n        - gamma (:obj:`float`): Constant discount factor gamma, should be in [0, 1], defaults to 0.9\\n        - lambda (:obj:`float`): Constant lambda, should be in [0, 1], defaults to 0.8\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Computed MSE loss, averaged over the batch\\n    Shapes:\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`, where T is trajectory length and B is batch,            which is the estimation of the state value at step 0 to T\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, the returns from time step 0 to T-1\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n        - loss (:obj:`torch.FloatTensor`): :math:`()`, 0-dim tensor\\n    Examples:\\n        >>> T, B = 8, 4\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> loss = td_lambda_error(td_lambda_data(value, reward, None))\\n    \"\n    (value, reward, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    with torch.no_grad():\n        return_ = generalized_lambda_returns(value, reward, gamma, lambda_)\n    loss = 0.5 * (F.mse_loss(return_, value[:-1], reduction='none') * weight).mean()\n    return loss",
            "@hpc_wrapper(shape_fn=shape_fn_td_lambda, namedtuple_data=True, include_args=[0, 1, 2], include_kwargs=['data', 'gamma', 'lambda_'])\ndef td_lambda_error(data: namedtuple, gamma: float=0.9, lambda_: float=0.8) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Computing TD(lambda) loss given constant gamma and lambda.\\n        There is no special handling for terminal state value,\\n        if some state has reached the terminal, just fill in zeros for values and rewards beyond terminal\\n        (*including the terminal state*, values[terminal] should also be 0)\\n    Arguments:\\n        - data (:obj:`namedtuple`): td_lambda input data with fields ['value', 'reward', 'weight']\\n        - gamma (:obj:`float`): Constant discount factor gamma, should be in [0, 1], defaults to 0.9\\n        - lambda (:obj:`float`): Constant lambda, should be in [0, 1], defaults to 0.8\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Computed MSE loss, averaged over the batch\\n    Shapes:\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`, where T is trajectory length and B is batch,            which is the estimation of the state value at step 0 to T\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, the returns from time step 0 to T-1\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n        - loss (:obj:`torch.FloatTensor`): :math:`()`, 0-dim tensor\\n    Examples:\\n        >>> T, B = 8, 4\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> loss = td_lambda_error(td_lambda_data(value, reward, None))\\n    \"\n    (value, reward, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    with torch.no_grad():\n        return_ = generalized_lambda_returns(value, reward, gamma, lambda_)\n    loss = 0.5 * (F.mse_loss(return_, value[:-1], reduction='none') * weight).mean()\n    return loss",
            "@hpc_wrapper(shape_fn=shape_fn_td_lambda, namedtuple_data=True, include_args=[0, 1, 2], include_kwargs=['data', 'gamma', 'lambda_'])\ndef td_lambda_error(data: namedtuple, gamma: float=0.9, lambda_: float=0.8) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Computing TD(lambda) loss given constant gamma and lambda.\\n        There is no special handling for terminal state value,\\n        if some state has reached the terminal, just fill in zeros for values and rewards beyond terminal\\n        (*including the terminal state*, values[terminal] should also be 0)\\n    Arguments:\\n        - data (:obj:`namedtuple`): td_lambda input data with fields ['value', 'reward', 'weight']\\n        - gamma (:obj:`float`): Constant discount factor gamma, should be in [0, 1], defaults to 0.9\\n        - lambda (:obj:`float`): Constant lambda, should be in [0, 1], defaults to 0.8\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Computed MSE loss, averaged over the batch\\n    Shapes:\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`, where T is trajectory length and B is batch,            which is the estimation of the state value at step 0 to T\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, the returns from time step 0 to T-1\\n        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight\\n        - loss (:obj:`torch.FloatTensor`): :math:`()`, 0-dim tensor\\n    Examples:\\n        >>> T, B = 8, 4\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> loss = td_lambda_error(td_lambda_data(value, reward, None))\\n    \"\n    (value, reward, weight) = data\n    if weight is None:\n        weight = torch.ones_like(reward)\n    with torch.no_grad():\n        return_ = generalized_lambda_returns(value, reward, gamma, lambda_)\n    loss = 0.5 * (F.mse_loss(return_, value[:-1], reduction='none') * weight).mean()\n    return loss"
        ]
    },
    {
        "func_name": "generalized_lambda_returns",
        "original": "def generalized_lambda_returns(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Functional equivalent to trfl.value_ops.generalized_lambda_returns\n        https://github.com/deepmind/trfl/blob/2c07ac22512a16715cc759f0072be43a5d12ae45/trfl/value_ops.py#L74\n        Passing in a number instead of tensor to make the value constant for all samples in batch\n    Arguments:\n        - bootstrap_values (:obj:`torch.Tensor` or :obj:`float`):\n          estimation of the value at step 0 to *T*, of size [T_traj+1, batchsize]\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\n        - gammas (:obj:`torch.Tensor` or :obj:`float`):\n          Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\n        - lambda (:obj:`torch.Tensor` or :obj:`float`): Determining the mix of bootstrapping\n          vs further accumulation of multistep returns at each timestep, of size [T_traj, batchsize]\n        - done (:obj:`torch.Tensor` or :obj:`float`):\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\n    Returns:\n        - return (:obj:`torch.Tensor`): Computed lambda return value\n          for each state from 0 to T-1, of size [T_traj, batchsize]\n    \"\"\"\n    if not isinstance(gammas, torch.Tensor):\n        gammas = gammas * torch.ones_like(rewards)\n    if not isinstance(lambda_, torch.Tensor):\n        lambda_ = lambda_ * torch.ones_like(rewards)\n    bootstrap_values_tp1 = bootstrap_values[1:, :]\n    return multistep_forward_view(bootstrap_values_tp1, rewards, gammas, lambda_, done)",
        "mutated": [
            "def generalized_lambda_returns(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Functional equivalent to trfl.value_ops.generalized_lambda_returns\\n        https://github.com/deepmind/trfl/blob/2c07ac22512a16715cc759f0072be43a5d12ae45/trfl/value_ops.py#L74\\n        Passing in a number instead of tensor to make the value constant for all samples in batch\\n    Arguments:\\n        - bootstrap_values (:obj:`torch.Tensor` or :obj:`float`):\\n          estimation of the value at step 0 to *T*, of size [T_traj+1, batchsize]\\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\\n        - gammas (:obj:`torch.Tensor` or :obj:`float`):\\n          Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\\n        - lambda (:obj:`torch.Tensor` or :obj:`float`): Determining the mix of bootstrapping\\n          vs further accumulation of multistep returns at each timestep, of size [T_traj, batchsize]\\n        - done (:obj:`torch.Tensor` or :obj:`float`):\\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\\n    Returns:\\n        - return (:obj:`torch.Tensor`): Computed lambda return value\\n          for each state from 0 to T-1, of size [T_traj, batchsize]\\n    '\n    if not isinstance(gammas, torch.Tensor):\n        gammas = gammas * torch.ones_like(rewards)\n    if not isinstance(lambda_, torch.Tensor):\n        lambda_ = lambda_ * torch.ones_like(rewards)\n    bootstrap_values_tp1 = bootstrap_values[1:, :]\n    return multistep_forward_view(bootstrap_values_tp1, rewards, gammas, lambda_, done)",
            "def generalized_lambda_returns(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Functional equivalent to trfl.value_ops.generalized_lambda_returns\\n        https://github.com/deepmind/trfl/blob/2c07ac22512a16715cc759f0072be43a5d12ae45/trfl/value_ops.py#L74\\n        Passing in a number instead of tensor to make the value constant for all samples in batch\\n    Arguments:\\n        - bootstrap_values (:obj:`torch.Tensor` or :obj:`float`):\\n          estimation of the value at step 0 to *T*, of size [T_traj+1, batchsize]\\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\\n        - gammas (:obj:`torch.Tensor` or :obj:`float`):\\n          Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\\n        - lambda (:obj:`torch.Tensor` or :obj:`float`): Determining the mix of bootstrapping\\n          vs further accumulation of multistep returns at each timestep, of size [T_traj, batchsize]\\n        - done (:obj:`torch.Tensor` or :obj:`float`):\\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\\n    Returns:\\n        - return (:obj:`torch.Tensor`): Computed lambda return value\\n          for each state from 0 to T-1, of size [T_traj, batchsize]\\n    '\n    if not isinstance(gammas, torch.Tensor):\n        gammas = gammas * torch.ones_like(rewards)\n    if not isinstance(lambda_, torch.Tensor):\n        lambda_ = lambda_ * torch.ones_like(rewards)\n    bootstrap_values_tp1 = bootstrap_values[1:, :]\n    return multistep_forward_view(bootstrap_values_tp1, rewards, gammas, lambda_, done)",
            "def generalized_lambda_returns(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Functional equivalent to trfl.value_ops.generalized_lambda_returns\\n        https://github.com/deepmind/trfl/blob/2c07ac22512a16715cc759f0072be43a5d12ae45/trfl/value_ops.py#L74\\n        Passing in a number instead of tensor to make the value constant for all samples in batch\\n    Arguments:\\n        - bootstrap_values (:obj:`torch.Tensor` or :obj:`float`):\\n          estimation of the value at step 0 to *T*, of size [T_traj+1, batchsize]\\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\\n        - gammas (:obj:`torch.Tensor` or :obj:`float`):\\n          Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\\n        - lambda (:obj:`torch.Tensor` or :obj:`float`): Determining the mix of bootstrapping\\n          vs further accumulation of multistep returns at each timestep, of size [T_traj, batchsize]\\n        - done (:obj:`torch.Tensor` or :obj:`float`):\\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\\n    Returns:\\n        - return (:obj:`torch.Tensor`): Computed lambda return value\\n          for each state from 0 to T-1, of size [T_traj, batchsize]\\n    '\n    if not isinstance(gammas, torch.Tensor):\n        gammas = gammas * torch.ones_like(rewards)\n    if not isinstance(lambda_, torch.Tensor):\n        lambda_ = lambda_ * torch.ones_like(rewards)\n    bootstrap_values_tp1 = bootstrap_values[1:, :]\n    return multistep_forward_view(bootstrap_values_tp1, rewards, gammas, lambda_, done)",
            "def generalized_lambda_returns(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Functional equivalent to trfl.value_ops.generalized_lambda_returns\\n        https://github.com/deepmind/trfl/blob/2c07ac22512a16715cc759f0072be43a5d12ae45/trfl/value_ops.py#L74\\n        Passing in a number instead of tensor to make the value constant for all samples in batch\\n    Arguments:\\n        - bootstrap_values (:obj:`torch.Tensor` or :obj:`float`):\\n          estimation of the value at step 0 to *T*, of size [T_traj+1, batchsize]\\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\\n        - gammas (:obj:`torch.Tensor` or :obj:`float`):\\n          Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\\n        - lambda (:obj:`torch.Tensor` or :obj:`float`): Determining the mix of bootstrapping\\n          vs further accumulation of multistep returns at each timestep, of size [T_traj, batchsize]\\n        - done (:obj:`torch.Tensor` or :obj:`float`):\\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\\n    Returns:\\n        - return (:obj:`torch.Tensor`): Computed lambda return value\\n          for each state from 0 to T-1, of size [T_traj, batchsize]\\n    '\n    if not isinstance(gammas, torch.Tensor):\n        gammas = gammas * torch.ones_like(rewards)\n    if not isinstance(lambda_, torch.Tensor):\n        lambda_ = lambda_ * torch.ones_like(rewards)\n    bootstrap_values_tp1 = bootstrap_values[1:, :]\n    return multistep_forward_view(bootstrap_values_tp1, rewards, gammas, lambda_, done)",
            "def generalized_lambda_returns(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Functional equivalent to trfl.value_ops.generalized_lambda_returns\\n        https://github.com/deepmind/trfl/blob/2c07ac22512a16715cc759f0072be43a5d12ae45/trfl/value_ops.py#L74\\n        Passing in a number instead of tensor to make the value constant for all samples in batch\\n    Arguments:\\n        - bootstrap_values (:obj:`torch.Tensor` or :obj:`float`):\\n          estimation of the value at step 0 to *T*, of size [T_traj+1, batchsize]\\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\\n        - gammas (:obj:`torch.Tensor` or :obj:`float`):\\n          Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\\n        - lambda (:obj:`torch.Tensor` or :obj:`float`): Determining the mix of bootstrapping\\n          vs further accumulation of multistep returns at each timestep, of size [T_traj, batchsize]\\n        - done (:obj:`torch.Tensor` or :obj:`float`):\\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\\n    Returns:\\n        - return (:obj:`torch.Tensor`): Computed lambda return value\\n          for each state from 0 to T-1, of size [T_traj, batchsize]\\n    '\n    if not isinstance(gammas, torch.Tensor):\n        gammas = gammas * torch.ones_like(rewards)\n    if not isinstance(lambda_, torch.Tensor):\n        lambda_ = lambda_ * torch.ones_like(rewards)\n    bootstrap_values_tp1 = bootstrap_values[1:, :]\n    return multistep_forward_view(bootstrap_values_tp1, rewards, gammas, lambda_, done)"
        ]
    },
    {
        "func_name": "multistep_forward_view",
        "original": "def multistep_forward_view(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Same as trfl.sequence_ops.multistep_forward_view\n        Implementing (12.18) in Sutton & Barto\n\n        ```\n        result[T-1] = rewards[T-1] + gammas[T-1] * bootstrap_values[T]\n        for t in 0...T-2 :\n        result[t] = rewards[t] + gammas[t]*(lambdas[t]*result[t+1] + (1-lambdas[t])*bootstrap_values[t+1])\n        ```\n\n        Assuming the first dim of input tensors correspond to the index in batch\n    Arguments:\n        - bootstrap_values (:obj:`torch.Tensor`): Estimation of the value at *step 1 to T*, of size [T_traj, batchsize]\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\n        - gammas (:obj:`torch.Tensor`): Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\n        - lambda (:obj:`torch.Tensor`): Determining the mix of bootstrapping vs further accumulation of \\\\\n            multistep returns at each timestep of size [T_traj, batchsize], the element for T-1 is ignored \\\\\n            and effectively set to 0, as there is no information about future rewards.\n        - done (:obj:`torch.Tensor` or :obj:`float`):\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\n    Returns:\n        - ret (:obj:`torch.Tensor`): Computed lambda return value \\\\\n            for each state from 0 to T-1, of size [T_traj, batchsize]\n    \"\"\"\n    result = torch.empty_like(rewards)\n    if done is None:\n        done = torch.zeros_like(rewards)\n    result[-1, :] = rewards[-1, :] + (1 - done[-1, :]) * gammas[-1, :] * bootstrap_values[-1, :]\n    discounts = gammas * lambda_\n    for t in reversed(range(rewards.size()[0] - 1)):\n        result[t, :] = rewards[t, :] + (1 - done[t, :]) * (discounts[t, :] * result[t + 1, :] + (gammas[t, :] - discounts[t, :]) * bootstrap_values[t, :])\n    return result",
        "mutated": [
            "def multistep_forward_view(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Same as trfl.sequence_ops.multistep_forward_view\\n        Implementing (12.18) in Sutton & Barto\\n\\n        ```\\n        result[T-1] = rewards[T-1] + gammas[T-1] * bootstrap_values[T]\\n        for t in 0...T-2 :\\n        result[t] = rewards[t] + gammas[t]*(lambdas[t]*result[t+1] + (1-lambdas[t])*bootstrap_values[t+1])\\n        ```\\n\\n        Assuming the first dim of input tensors correspond to the index in batch\\n    Arguments:\\n        - bootstrap_values (:obj:`torch.Tensor`): Estimation of the value at *step 1 to T*, of size [T_traj, batchsize]\\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\\n        - gammas (:obj:`torch.Tensor`): Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\\n        - lambda (:obj:`torch.Tensor`): Determining the mix of bootstrapping vs further accumulation of \\\\\\n            multistep returns at each timestep of size [T_traj, batchsize], the element for T-1 is ignored \\\\\\n            and effectively set to 0, as there is no information about future rewards.\\n        - done (:obj:`torch.Tensor` or :obj:`float`):\\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\\n    Returns:\\n        - ret (:obj:`torch.Tensor`): Computed lambda return value \\\\\\n            for each state from 0 to T-1, of size [T_traj, batchsize]\\n    '\n    result = torch.empty_like(rewards)\n    if done is None:\n        done = torch.zeros_like(rewards)\n    result[-1, :] = rewards[-1, :] + (1 - done[-1, :]) * gammas[-1, :] * bootstrap_values[-1, :]\n    discounts = gammas * lambda_\n    for t in reversed(range(rewards.size()[0] - 1)):\n        result[t, :] = rewards[t, :] + (1 - done[t, :]) * (discounts[t, :] * result[t + 1, :] + (gammas[t, :] - discounts[t, :]) * bootstrap_values[t, :])\n    return result",
            "def multistep_forward_view(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Same as trfl.sequence_ops.multistep_forward_view\\n        Implementing (12.18) in Sutton & Barto\\n\\n        ```\\n        result[T-1] = rewards[T-1] + gammas[T-1] * bootstrap_values[T]\\n        for t in 0...T-2 :\\n        result[t] = rewards[t] + gammas[t]*(lambdas[t]*result[t+1] + (1-lambdas[t])*bootstrap_values[t+1])\\n        ```\\n\\n        Assuming the first dim of input tensors correspond to the index in batch\\n    Arguments:\\n        - bootstrap_values (:obj:`torch.Tensor`): Estimation of the value at *step 1 to T*, of size [T_traj, batchsize]\\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\\n        - gammas (:obj:`torch.Tensor`): Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\\n        - lambda (:obj:`torch.Tensor`): Determining the mix of bootstrapping vs further accumulation of \\\\\\n            multistep returns at each timestep of size [T_traj, batchsize], the element for T-1 is ignored \\\\\\n            and effectively set to 0, as there is no information about future rewards.\\n        - done (:obj:`torch.Tensor` or :obj:`float`):\\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\\n    Returns:\\n        - ret (:obj:`torch.Tensor`): Computed lambda return value \\\\\\n            for each state from 0 to T-1, of size [T_traj, batchsize]\\n    '\n    result = torch.empty_like(rewards)\n    if done is None:\n        done = torch.zeros_like(rewards)\n    result[-1, :] = rewards[-1, :] + (1 - done[-1, :]) * gammas[-1, :] * bootstrap_values[-1, :]\n    discounts = gammas * lambda_\n    for t in reversed(range(rewards.size()[0] - 1)):\n        result[t, :] = rewards[t, :] + (1 - done[t, :]) * (discounts[t, :] * result[t + 1, :] + (gammas[t, :] - discounts[t, :]) * bootstrap_values[t, :])\n    return result",
            "def multistep_forward_view(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Same as trfl.sequence_ops.multistep_forward_view\\n        Implementing (12.18) in Sutton & Barto\\n\\n        ```\\n        result[T-1] = rewards[T-1] + gammas[T-1] * bootstrap_values[T]\\n        for t in 0...T-2 :\\n        result[t] = rewards[t] + gammas[t]*(lambdas[t]*result[t+1] + (1-lambdas[t])*bootstrap_values[t+1])\\n        ```\\n\\n        Assuming the first dim of input tensors correspond to the index in batch\\n    Arguments:\\n        - bootstrap_values (:obj:`torch.Tensor`): Estimation of the value at *step 1 to T*, of size [T_traj, batchsize]\\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\\n        - gammas (:obj:`torch.Tensor`): Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\\n        - lambda (:obj:`torch.Tensor`): Determining the mix of bootstrapping vs further accumulation of \\\\\\n            multistep returns at each timestep of size [T_traj, batchsize], the element for T-1 is ignored \\\\\\n            and effectively set to 0, as there is no information about future rewards.\\n        - done (:obj:`torch.Tensor` or :obj:`float`):\\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\\n    Returns:\\n        - ret (:obj:`torch.Tensor`): Computed lambda return value \\\\\\n            for each state from 0 to T-1, of size [T_traj, batchsize]\\n    '\n    result = torch.empty_like(rewards)\n    if done is None:\n        done = torch.zeros_like(rewards)\n    result[-1, :] = rewards[-1, :] + (1 - done[-1, :]) * gammas[-1, :] * bootstrap_values[-1, :]\n    discounts = gammas * lambda_\n    for t in reversed(range(rewards.size()[0] - 1)):\n        result[t, :] = rewards[t, :] + (1 - done[t, :]) * (discounts[t, :] * result[t + 1, :] + (gammas[t, :] - discounts[t, :]) * bootstrap_values[t, :])\n    return result",
            "def multistep_forward_view(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Same as trfl.sequence_ops.multistep_forward_view\\n        Implementing (12.18) in Sutton & Barto\\n\\n        ```\\n        result[T-1] = rewards[T-1] + gammas[T-1] * bootstrap_values[T]\\n        for t in 0...T-2 :\\n        result[t] = rewards[t] + gammas[t]*(lambdas[t]*result[t+1] + (1-lambdas[t])*bootstrap_values[t+1])\\n        ```\\n\\n        Assuming the first dim of input tensors correspond to the index in batch\\n    Arguments:\\n        - bootstrap_values (:obj:`torch.Tensor`): Estimation of the value at *step 1 to T*, of size [T_traj, batchsize]\\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\\n        - gammas (:obj:`torch.Tensor`): Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\\n        - lambda (:obj:`torch.Tensor`): Determining the mix of bootstrapping vs further accumulation of \\\\\\n            multistep returns at each timestep of size [T_traj, batchsize], the element for T-1 is ignored \\\\\\n            and effectively set to 0, as there is no information about future rewards.\\n        - done (:obj:`torch.Tensor` or :obj:`float`):\\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\\n    Returns:\\n        - ret (:obj:`torch.Tensor`): Computed lambda return value \\\\\\n            for each state from 0 to T-1, of size [T_traj, batchsize]\\n    '\n    result = torch.empty_like(rewards)\n    if done is None:\n        done = torch.zeros_like(rewards)\n    result[-1, :] = rewards[-1, :] + (1 - done[-1, :]) * gammas[-1, :] * bootstrap_values[-1, :]\n    discounts = gammas * lambda_\n    for t in reversed(range(rewards.size()[0] - 1)):\n        result[t, :] = rewards[t, :] + (1 - done[t, :]) * (discounts[t, :] * result[t + 1, :] + (gammas[t, :] - discounts[t, :]) * bootstrap_values[t, :])\n    return result",
            "def multistep_forward_view(bootstrap_values: torch.Tensor, rewards: torch.Tensor, gammas: float, lambda_: float, done: Optional[torch.Tensor]=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Same as trfl.sequence_ops.multistep_forward_view\\n        Implementing (12.18) in Sutton & Barto\\n\\n        ```\\n        result[T-1] = rewards[T-1] + gammas[T-1] * bootstrap_values[T]\\n        for t in 0...T-2 :\\n        result[t] = rewards[t] + gammas[t]*(lambdas[t]*result[t+1] + (1-lambdas[t])*bootstrap_values[t+1])\\n        ```\\n\\n        Assuming the first dim of input tensors correspond to the index in batch\\n    Arguments:\\n        - bootstrap_values (:obj:`torch.Tensor`): Estimation of the value at *step 1 to T*, of size [T_traj, batchsize]\\n        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]\\n        - gammas (:obj:`torch.Tensor`): Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]\\n        - lambda (:obj:`torch.Tensor`): Determining the mix of bootstrapping vs further accumulation of \\\\\\n            multistep returns at each timestep of size [T_traj, batchsize], the element for T-1 is ignored \\\\\\n            and effectively set to 0, as there is no information about future rewards.\\n        - done (:obj:`torch.Tensor` or :obj:`float`):\\n          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]\\n    Returns:\\n        - ret (:obj:`torch.Tensor`): Computed lambda return value \\\\\\n            for each state from 0 to T-1, of size [T_traj, batchsize]\\n    '\n    result = torch.empty_like(rewards)\n    if done is None:\n        done = torch.zeros_like(rewards)\n    result[-1, :] = rewards[-1, :] + (1 - done[-1, :]) * gammas[-1, :] * bootstrap_values[-1, :]\n    discounts = gammas * lambda_\n    for t in reversed(range(rewards.size()[0] - 1)):\n        result[t, :] = rewards[t, :] + (1 - done[t, :]) * (discounts[t, :] * result[t + 1, :] + (gammas[t, :] - discounts[t, :]) * bootstrap_values[t, :])\n    return result"
        ]
    }
]