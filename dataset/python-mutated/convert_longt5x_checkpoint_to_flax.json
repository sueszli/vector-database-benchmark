[
    {
        "func_name": "convert_t5x_checkpoint_to_flax",
        "original": "def convert_t5x_checkpoint_to_flax(t5x_checkpoint_path, config_name, flax_dump_folder_path):\n    config = AutoConfig.from_pretrained(config_name)\n    flax_model = FlaxAutoModelForSeq2SeqLM.from_config(config=config)\n    t5x_model = checkpoints.load_t5x_checkpoint(t5x_checkpoint_path)\n    split_mlp_wi = 'wi_0' in t5x_model['target']['encoder']['layers_0']['mlp']\n    if config.model_type == 't5':\n        encoder_attn_name = 'SelfAttention'\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'local':\n        encoder_attn_name = 'LocalSelfAttention'\n    elif config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        encoder_attn_name = 'TransientGlobalSelfAttention'\n    else:\n        raise ValueError(\"Given config is expected to have `model_type='t5'`, or `model_type='longt5` with `encoder_attention_type` attribute with a value from ['local', 'transient-global].\")\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['encoder'][layer_name]['attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['encoder'][layer_name]['attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['encoder'][layer_name]['attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['encoder'][layer_name]['attention']['value']['kernel']\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            t5x_global_layer_norm = t5x_model['target']['encoder'][layer_name]['attention']['T5LayerNorm_0']['scale']\n        t5x_attention_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['encoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['encoder'][layer_name]['mlp']['wo']['kernel']\n        t5x_mlp_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_encoder_layer_block = flax_model.params['encoder']['block'][str(layer_index)]['layer']\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['k']['kernel'] = t5x_attention_key\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['o']['kernel'] = t5x_attention_out\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['q']['kernel'] = t5x_attention_query\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['v']['kernel'] = t5x_attention_value\n        flax_model_encoder_layer_block['0']['layer_norm']['weight'] = t5x_attention_layer_norm\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            flax_model_encoder_layer_block['0'][encoder_attn_name]['global_input_layer_norm']['weight'] = t5x_global_layer_norm\n        if split_mlp_wi:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_encoder_layer_block['1']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_encoder_layer_block['1']['layer_norm']['weight'] = t5x_mlp_layer_norm\n        flax_model.params['encoder']['block'][str(layer_index)]['layer'] = flax_model_encoder_layer_block\n    t5x_encoder_rel_embedding = t5x_model['target']['encoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['relative_attention_bias']['embedding'] = t5x_encoder_rel_embedding\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        t5x_encoder_global_rel_embedding = t5x_model['target']['encoder']['side_relpos_bias']['rel_embedding'].T\n        flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['global_relative_attention_bias']['embedding'] = t5x_encoder_global_rel_embedding\n    t5x_encoder_norm = t5x_model['target']['encoder']['encoder_norm']['scale']\n    flax_model.params['encoder']['final_layer_norm']['weight'] = t5x_encoder_norm\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['decoder'][layer_name]['self_attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['decoder'][layer_name]['self_attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['decoder'][layer_name]['self_attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['decoder'][layer_name]['self_attention']['value']['kernel']\n        t5x_pre_attention_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_self_attention_layer_norm']['scale']\n        t5x_enc_dec_attention_module = t5x_model['target']['decoder'][layer_name]['encoder_decoder_attention']\n        t5x_enc_dec_attention_key = t5x_enc_dec_attention_module['key']['kernel']\n        t5x_enc_dec_attention_out = t5x_enc_dec_attention_module['out']['kernel']\n        t5x_enc_dec_attention_query = t5x_enc_dec_attention_module['query']['kernel']\n        t5x_enc_dec_attention_value = t5x_enc_dec_attention_module['value']['kernel']\n        t5x_cross_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_cross_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['decoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['decoder'][layer_name]['mlp']['wo']['kernel']\n        tx5_mlp_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_decoder_layer_block = flax_model.params['decoder']['block'][str(layer_index)]['layer']\n        flax_model_decoder_layer_block['0']['SelfAttention']['k']['kernel'] = t5x_attention_key\n        flax_model_decoder_layer_block['0']['SelfAttention']['o']['kernel'] = t5x_attention_out\n        flax_model_decoder_layer_block['0']['SelfAttention']['q']['kernel'] = t5x_attention_query\n        flax_model_decoder_layer_block['0']['SelfAttention']['v']['kernel'] = t5x_attention_value\n        flax_model_decoder_layer_block['0']['layer_norm']['weight'] = t5x_pre_attention_layer_norm\n        flax_model_decoder_layer_block['1']['EncDecAttention']['k']['kernel'] = t5x_enc_dec_attention_key\n        flax_model_decoder_layer_block['1']['EncDecAttention']['o']['kernel'] = t5x_enc_dec_attention_out\n        flax_model_decoder_layer_block['1']['EncDecAttention']['q']['kernel'] = t5x_enc_dec_attention_query\n        flax_model_decoder_layer_block['1']['EncDecAttention']['v']['kernel'] = t5x_enc_dec_attention_value\n        flax_model_decoder_layer_block['1']['layer_norm']['weight'] = t5x_cross_layer_norm\n        if split_mlp_wi:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_decoder_layer_block['2']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_decoder_layer_block['2']['layer_norm']['weight'] = tx5_mlp_layer_norm\n        flax_model.params['decoder']['block'][str(layer_index)]['layer'] = flax_model_decoder_layer_block\n    tx5_decoder_norm = t5x_model['target']['decoder']['decoder_norm']['scale']\n    flax_model.params['decoder']['final_layer_norm']['weight'] = tx5_decoder_norm\n    t5x_decoder_rel_embedding = t5x_model['target']['decoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['decoder']['block']['0']['layer']['0']['SelfAttention']['relative_attention_bias']['embedding'] = t5x_decoder_rel_embedding\n    tx5_token_embeddings = t5x_model['target']['token_embedder']['embedding']\n    flax_model.params['shared']['embedding'] = tx5_token_embeddings\n    if 'logits_dense' in t5x_model['target']['decoder']:\n        flax_model.params['lm_head']['kernel'] = t5x_model['target']['decoder']['logits_dense']['kernel']\n    flax_model.save_pretrained(flax_dump_folder_path)\n    print('T5X Model was sucessfully converted!')",
        "mutated": [
            "def convert_t5x_checkpoint_to_flax(t5x_checkpoint_path, config_name, flax_dump_folder_path):\n    if False:\n        i = 10\n    config = AutoConfig.from_pretrained(config_name)\n    flax_model = FlaxAutoModelForSeq2SeqLM.from_config(config=config)\n    t5x_model = checkpoints.load_t5x_checkpoint(t5x_checkpoint_path)\n    split_mlp_wi = 'wi_0' in t5x_model['target']['encoder']['layers_0']['mlp']\n    if config.model_type == 't5':\n        encoder_attn_name = 'SelfAttention'\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'local':\n        encoder_attn_name = 'LocalSelfAttention'\n    elif config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        encoder_attn_name = 'TransientGlobalSelfAttention'\n    else:\n        raise ValueError(\"Given config is expected to have `model_type='t5'`, or `model_type='longt5` with `encoder_attention_type` attribute with a value from ['local', 'transient-global].\")\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['encoder'][layer_name]['attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['encoder'][layer_name]['attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['encoder'][layer_name]['attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['encoder'][layer_name]['attention']['value']['kernel']\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            t5x_global_layer_norm = t5x_model['target']['encoder'][layer_name]['attention']['T5LayerNorm_0']['scale']\n        t5x_attention_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['encoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['encoder'][layer_name]['mlp']['wo']['kernel']\n        t5x_mlp_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_encoder_layer_block = flax_model.params['encoder']['block'][str(layer_index)]['layer']\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['k']['kernel'] = t5x_attention_key\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['o']['kernel'] = t5x_attention_out\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['q']['kernel'] = t5x_attention_query\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['v']['kernel'] = t5x_attention_value\n        flax_model_encoder_layer_block['0']['layer_norm']['weight'] = t5x_attention_layer_norm\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            flax_model_encoder_layer_block['0'][encoder_attn_name]['global_input_layer_norm']['weight'] = t5x_global_layer_norm\n        if split_mlp_wi:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_encoder_layer_block['1']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_encoder_layer_block['1']['layer_norm']['weight'] = t5x_mlp_layer_norm\n        flax_model.params['encoder']['block'][str(layer_index)]['layer'] = flax_model_encoder_layer_block\n    t5x_encoder_rel_embedding = t5x_model['target']['encoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['relative_attention_bias']['embedding'] = t5x_encoder_rel_embedding\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        t5x_encoder_global_rel_embedding = t5x_model['target']['encoder']['side_relpos_bias']['rel_embedding'].T\n        flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['global_relative_attention_bias']['embedding'] = t5x_encoder_global_rel_embedding\n    t5x_encoder_norm = t5x_model['target']['encoder']['encoder_norm']['scale']\n    flax_model.params['encoder']['final_layer_norm']['weight'] = t5x_encoder_norm\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['decoder'][layer_name]['self_attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['decoder'][layer_name]['self_attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['decoder'][layer_name]['self_attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['decoder'][layer_name]['self_attention']['value']['kernel']\n        t5x_pre_attention_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_self_attention_layer_norm']['scale']\n        t5x_enc_dec_attention_module = t5x_model['target']['decoder'][layer_name]['encoder_decoder_attention']\n        t5x_enc_dec_attention_key = t5x_enc_dec_attention_module['key']['kernel']\n        t5x_enc_dec_attention_out = t5x_enc_dec_attention_module['out']['kernel']\n        t5x_enc_dec_attention_query = t5x_enc_dec_attention_module['query']['kernel']\n        t5x_enc_dec_attention_value = t5x_enc_dec_attention_module['value']['kernel']\n        t5x_cross_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_cross_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['decoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['decoder'][layer_name]['mlp']['wo']['kernel']\n        tx5_mlp_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_decoder_layer_block = flax_model.params['decoder']['block'][str(layer_index)]['layer']\n        flax_model_decoder_layer_block['0']['SelfAttention']['k']['kernel'] = t5x_attention_key\n        flax_model_decoder_layer_block['0']['SelfAttention']['o']['kernel'] = t5x_attention_out\n        flax_model_decoder_layer_block['0']['SelfAttention']['q']['kernel'] = t5x_attention_query\n        flax_model_decoder_layer_block['0']['SelfAttention']['v']['kernel'] = t5x_attention_value\n        flax_model_decoder_layer_block['0']['layer_norm']['weight'] = t5x_pre_attention_layer_norm\n        flax_model_decoder_layer_block['1']['EncDecAttention']['k']['kernel'] = t5x_enc_dec_attention_key\n        flax_model_decoder_layer_block['1']['EncDecAttention']['o']['kernel'] = t5x_enc_dec_attention_out\n        flax_model_decoder_layer_block['1']['EncDecAttention']['q']['kernel'] = t5x_enc_dec_attention_query\n        flax_model_decoder_layer_block['1']['EncDecAttention']['v']['kernel'] = t5x_enc_dec_attention_value\n        flax_model_decoder_layer_block['1']['layer_norm']['weight'] = t5x_cross_layer_norm\n        if split_mlp_wi:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_decoder_layer_block['2']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_decoder_layer_block['2']['layer_norm']['weight'] = tx5_mlp_layer_norm\n        flax_model.params['decoder']['block'][str(layer_index)]['layer'] = flax_model_decoder_layer_block\n    tx5_decoder_norm = t5x_model['target']['decoder']['decoder_norm']['scale']\n    flax_model.params['decoder']['final_layer_norm']['weight'] = tx5_decoder_norm\n    t5x_decoder_rel_embedding = t5x_model['target']['decoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['decoder']['block']['0']['layer']['0']['SelfAttention']['relative_attention_bias']['embedding'] = t5x_decoder_rel_embedding\n    tx5_token_embeddings = t5x_model['target']['token_embedder']['embedding']\n    flax_model.params['shared']['embedding'] = tx5_token_embeddings\n    if 'logits_dense' in t5x_model['target']['decoder']:\n        flax_model.params['lm_head']['kernel'] = t5x_model['target']['decoder']['logits_dense']['kernel']\n    flax_model.save_pretrained(flax_dump_folder_path)\n    print('T5X Model was sucessfully converted!')",
            "def convert_t5x_checkpoint_to_flax(t5x_checkpoint_path, config_name, flax_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    config = AutoConfig.from_pretrained(config_name)\n    flax_model = FlaxAutoModelForSeq2SeqLM.from_config(config=config)\n    t5x_model = checkpoints.load_t5x_checkpoint(t5x_checkpoint_path)\n    split_mlp_wi = 'wi_0' in t5x_model['target']['encoder']['layers_0']['mlp']\n    if config.model_type == 't5':\n        encoder_attn_name = 'SelfAttention'\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'local':\n        encoder_attn_name = 'LocalSelfAttention'\n    elif config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        encoder_attn_name = 'TransientGlobalSelfAttention'\n    else:\n        raise ValueError(\"Given config is expected to have `model_type='t5'`, or `model_type='longt5` with `encoder_attention_type` attribute with a value from ['local', 'transient-global].\")\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['encoder'][layer_name]['attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['encoder'][layer_name]['attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['encoder'][layer_name]['attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['encoder'][layer_name]['attention']['value']['kernel']\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            t5x_global_layer_norm = t5x_model['target']['encoder'][layer_name]['attention']['T5LayerNorm_0']['scale']\n        t5x_attention_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['encoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['encoder'][layer_name]['mlp']['wo']['kernel']\n        t5x_mlp_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_encoder_layer_block = flax_model.params['encoder']['block'][str(layer_index)]['layer']\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['k']['kernel'] = t5x_attention_key\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['o']['kernel'] = t5x_attention_out\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['q']['kernel'] = t5x_attention_query\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['v']['kernel'] = t5x_attention_value\n        flax_model_encoder_layer_block['0']['layer_norm']['weight'] = t5x_attention_layer_norm\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            flax_model_encoder_layer_block['0'][encoder_attn_name]['global_input_layer_norm']['weight'] = t5x_global_layer_norm\n        if split_mlp_wi:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_encoder_layer_block['1']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_encoder_layer_block['1']['layer_norm']['weight'] = t5x_mlp_layer_norm\n        flax_model.params['encoder']['block'][str(layer_index)]['layer'] = flax_model_encoder_layer_block\n    t5x_encoder_rel_embedding = t5x_model['target']['encoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['relative_attention_bias']['embedding'] = t5x_encoder_rel_embedding\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        t5x_encoder_global_rel_embedding = t5x_model['target']['encoder']['side_relpos_bias']['rel_embedding'].T\n        flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['global_relative_attention_bias']['embedding'] = t5x_encoder_global_rel_embedding\n    t5x_encoder_norm = t5x_model['target']['encoder']['encoder_norm']['scale']\n    flax_model.params['encoder']['final_layer_norm']['weight'] = t5x_encoder_norm\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['decoder'][layer_name]['self_attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['decoder'][layer_name]['self_attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['decoder'][layer_name]['self_attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['decoder'][layer_name]['self_attention']['value']['kernel']\n        t5x_pre_attention_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_self_attention_layer_norm']['scale']\n        t5x_enc_dec_attention_module = t5x_model['target']['decoder'][layer_name]['encoder_decoder_attention']\n        t5x_enc_dec_attention_key = t5x_enc_dec_attention_module['key']['kernel']\n        t5x_enc_dec_attention_out = t5x_enc_dec_attention_module['out']['kernel']\n        t5x_enc_dec_attention_query = t5x_enc_dec_attention_module['query']['kernel']\n        t5x_enc_dec_attention_value = t5x_enc_dec_attention_module['value']['kernel']\n        t5x_cross_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_cross_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['decoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['decoder'][layer_name]['mlp']['wo']['kernel']\n        tx5_mlp_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_decoder_layer_block = flax_model.params['decoder']['block'][str(layer_index)]['layer']\n        flax_model_decoder_layer_block['0']['SelfAttention']['k']['kernel'] = t5x_attention_key\n        flax_model_decoder_layer_block['0']['SelfAttention']['o']['kernel'] = t5x_attention_out\n        flax_model_decoder_layer_block['0']['SelfAttention']['q']['kernel'] = t5x_attention_query\n        flax_model_decoder_layer_block['0']['SelfAttention']['v']['kernel'] = t5x_attention_value\n        flax_model_decoder_layer_block['0']['layer_norm']['weight'] = t5x_pre_attention_layer_norm\n        flax_model_decoder_layer_block['1']['EncDecAttention']['k']['kernel'] = t5x_enc_dec_attention_key\n        flax_model_decoder_layer_block['1']['EncDecAttention']['o']['kernel'] = t5x_enc_dec_attention_out\n        flax_model_decoder_layer_block['1']['EncDecAttention']['q']['kernel'] = t5x_enc_dec_attention_query\n        flax_model_decoder_layer_block['1']['EncDecAttention']['v']['kernel'] = t5x_enc_dec_attention_value\n        flax_model_decoder_layer_block['1']['layer_norm']['weight'] = t5x_cross_layer_norm\n        if split_mlp_wi:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_decoder_layer_block['2']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_decoder_layer_block['2']['layer_norm']['weight'] = tx5_mlp_layer_norm\n        flax_model.params['decoder']['block'][str(layer_index)]['layer'] = flax_model_decoder_layer_block\n    tx5_decoder_norm = t5x_model['target']['decoder']['decoder_norm']['scale']\n    flax_model.params['decoder']['final_layer_norm']['weight'] = tx5_decoder_norm\n    t5x_decoder_rel_embedding = t5x_model['target']['decoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['decoder']['block']['0']['layer']['0']['SelfAttention']['relative_attention_bias']['embedding'] = t5x_decoder_rel_embedding\n    tx5_token_embeddings = t5x_model['target']['token_embedder']['embedding']\n    flax_model.params['shared']['embedding'] = tx5_token_embeddings\n    if 'logits_dense' in t5x_model['target']['decoder']:\n        flax_model.params['lm_head']['kernel'] = t5x_model['target']['decoder']['logits_dense']['kernel']\n    flax_model.save_pretrained(flax_dump_folder_path)\n    print('T5X Model was sucessfully converted!')",
            "def convert_t5x_checkpoint_to_flax(t5x_checkpoint_path, config_name, flax_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    config = AutoConfig.from_pretrained(config_name)\n    flax_model = FlaxAutoModelForSeq2SeqLM.from_config(config=config)\n    t5x_model = checkpoints.load_t5x_checkpoint(t5x_checkpoint_path)\n    split_mlp_wi = 'wi_0' in t5x_model['target']['encoder']['layers_0']['mlp']\n    if config.model_type == 't5':\n        encoder_attn_name = 'SelfAttention'\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'local':\n        encoder_attn_name = 'LocalSelfAttention'\n    elif config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        encoder_attn_name = 'TransientGlobalSelfAttention'\n    else:\n        raise ValueError(\"Given config is expected to have `model_type='t5'`, or `model_type='longt5` with `encoder_attention_type` attribute with a value from ['local', 'transient-global].\")\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['encoder'][layer_name]['attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['encoder'][layer_name]['attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['encoder'][layer_name]['attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['encoder'][layer_name]['attention']['value']['kernel']\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            t5x_global_layer_norm = t5x_model['target']['encoder'][layer_name]['attention']['T5LayerNorm_0']['scale']\n        t5x_attention_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['encoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['encoder'][layer_name]['mlp']['wo']['kernel']\n        t5x_mlp_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_encoder_layer_block = flax_model.params['encoder']['block'][str(layer_index)]['layer']\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['k']['kernel'] = t5x_attention_key\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['o']['kernel'] = t5x_attention_out\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['q']['kernel'] = t5x_attention_query\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['v']['kernel'] = t5x_attention_value\n        flax_model_encoder_layer_block['0']['layer_norm']['weight'] = t5x_attention_layer_norm\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            flax_model_encoder_layer_block['0'][encoder_attn_name]['global_input_layer_norm']['weight'] = t5x_global_layer_norm\n        if split_mlp_wi:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_encoder_layer_block['1']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_encoder_layer_block['1']['layer_norm']['weight'] = t5x_mlp_layer_norm\n        flax_model.params['encoder']['block'][str(layer_index)]['layer'] = flax_model_encoder_layer_block\n    t5x_encoder_rel_embedding = t5x_model['target']['encoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['relative_attention_bias']['embedding'] = t5x_encoder_rel_embedding\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        t5x_encoder_global_rel_embedding = t5x_model['target']['encoder']['side_relpos_bias']['rel_embedding'].T\n        flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['global_relative_attention_bias']['embedding'] = t5x_encoder_global_rel_embedding\n    t5x_encoder_norm = t5x_model['target']['encoder']['encoder_norm']['scale']\n    flax_model.params['encoder']['final_layer_norm']['weight'] = t5x_encoder_norm\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['decoder'][layer_name]['self_attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['decoder'][layer_name]['self_attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['decoder'][layer_name]['self_attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['decoder'][layer_name]['self_attention']['value']['kernel']\n        t5x_pre_attention_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_self_attention_layer_norm']['scale']\n        t5x_enc_dec_attention_module = t5x_model['target']['decoder'][layer_name]['encoder_decoder_attention']\n        t5x_enc_dec_attention_key = t5x_enc_dec_attention_module['key']['kernel']\n        t5x_enc_dec_attention_out = t5x_enc_dec_attention_module['out']['kernel']\n        t5x_enc_dec_attention_query = t5x_enc_dec_attention_module['query']['kernel']\n        t5x_enc_dec_attention_value = t5x_enc_dec_attention_module['value']['kernel']\n        t5x_cross_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_cross_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['decoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['decoder'][layer_name]['mlp']['wo']['kernel']\n        tx5_mlp_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_decoder_layer_block = flax_model.params['decoder']['block'][str(layer_index)]['layer']\n        flax_model_decoder_layer_block['0']['SelfAttention']['k']['kernel'] = t5x_attention_key\n        flax_model_decoder_layer_block['0']['SelfAttention']['o']['kernel'] = t5x_attention_out\n        flax_model_decoder_layer_block['0']['SelfAttention']['q']['kernel'] = t5x_attention_query\n        flax_model_decoder_layer_block['0']['SelfAttention']['v']['kernel'] = t5x_attention_value\n        flax_model_decoder_layer_block['0']['layer_norm']['weight'] = t5x_pre_attention_layer_norm\n        flax_model_decoder_layer_block['1']['EncDecAttention']['k']['kernel'] = t5x_enc_dec_attention_key\n        flax_model_decoder_layer_block['1']['EncDecAttention']['o']['kernel'] = t5x_enc_dec_attention_out\n        flax_model_decoder_layer_block['1']['EncDecAttention']['q']['kernel'] = t5x_enc_dec_attention_query\n        flax_model_decoder_layer_block['1']['EncDecAttention']['v']['kernel'] = t5x_enc_dec_attention_value\n        flax_model_decoder_layer_block['1']['layer_norm']['weight'] = t5x_cross_layer_norm\n        if split_mlp_wi:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_decoder_layer_block['2']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_decoder_layer_block['2']['layer_norm']['weight'] = tx5_mlp_layer_norm\n        flax_model.params['decoder']['block'][str(layer_index)]['layer'] = flax_model_decoder_layer_block\n    tx5_decoder_norm = t5x_model['target']['decoder']['decoder_norm']['scale']\n    flax_model.params['decoder']['final_layer_norm']['weight'] = tx5_decoder_norm\n    t5x_decoder_rel_embedding = t5x_model['target']['decoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['decoder']['block']['0']['layer']['0']['SelfAttention']['relative_attention_bias']['embedding'] = t5x_decoder_rel_embedding\n    tx5_token_embeddings = t5x_model['target']['token_embedder']['embedding']\n    flax_model.params['shared']['embedding'] = tx5_token_embeddings\n    if 'logits_dense' in t5x_model['target']['decoder']:\n        flax_model.params['lm_head']['kernel'] = t5x_model['target']['decoder']['logits_dense']['kernel']\n    flax_model.save_pretrained(flax_dump_folder_path)\n    print('T5X Model was sucessfully converted!')",
            "def convert_t5x_checkpoint_to_flax(t5x_checkpoint_path, config_name, flax_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    config = AutoConfig.from_pretrained(config_name)\n    flax_model = FlaxAutoModelForSeq2SeqLM.from_config(config=config)\n    t5x_model = checkpoints.load_t5x_checkpoint(t5x_checkpoint_path)\n    split_mlp_wi = 'wi_0' in t5x_model['target']['encoder']['layers_0']['mlp']\n    if config.model_type == 't5':\n        encoder_attn_name = 'SelfAttention'\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'local':\n        encoder_attn_name = 'LocalSelfAttention'\n    elif config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        encoder_attn_name = 'TransientGlobalSelfAttention'\n    else:\n        raise ValueError(\"Given config is expected to have `model_type='t5'`, or `model_type='longt5` with `encoder_attention_type` attribute with a value from ['local', 'transient-global].\")\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['encoder'][layer_name]['attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['encoder'][layer_name]['attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['encoder'][layer_name]['attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['encoder'][layer_name]['attention']['value']['kernel']\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            t5x_global_layer_norm = t5x_model['target']['encoder'][layer_name]['attention']['T5LayerNorm_0']['scale']\n        t5x_attention_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['encoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['encoder'][layer_name]['mlp']['wo']['kernel']\n        t5x_mlp_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_encoder_layer_block = flax_model.params['encoder']['block'][str(layer_index)]['layer']\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['k']['kernel'] = t5x_attention_key\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['o']['kernel'] = t5x_attention_out\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['q']['kernel'] = t5x_attention_query\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['v']['kernel'] = t5x_attention_value\n        flax_model_encoder_layer_block['0']['layer_norm']['weight'] = t5x_attention_layer_norm\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            flax_model_encoder_layer_block['0'][encoder_attn_name]['global_input_layer_norm']['weight'] = t5x_global_layer_norm\n        if split_mlp_wi:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_encoder_layer_block['1']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_encoder_layer_block['1']['layer_norm']['weight'] = t5x_mlp_layer_norm\n        flax_model.params['encoder']['block'][str(layer_index)]['layer'] = flax_model_encoder_layer_block\n    t5x_encoder_rel_embedding = t5x_model['target']['encoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['relative_attention_bias']['embedding'] = t5x_encoder_rel_embedding\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        t5x_encoder_global_rel_embedding = t5x_model['target']['encoder']['side_relpos_bias']['rel_embedding'].T\n        flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['global_relative_attention_bias']['embedding'] = t5x_encoder_global_rel_embedding\n    t5x_encoder_norm = t5x_model['target']['encoder']['encoder_norm']['scale']\n    flax_model.params['encoder']['final_layer_norm']['weight'] = t5x_encoder_norm\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['decoder'][layer_name]['self_attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['decoder'][layer_name]['self_attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['decoder'][layer_name]['self_attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['decoder'][layer_name]['self_attention']['value']['kernel']\n        t5x_pre_attention_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_self_attention_layer_norm']['scale']\n        t5x_enc_dec_attention_module = t5x_model['target']['decoder'][layer_name]['encoder_decoder_attention']\n        t5x_enc_dec_attention_key = t5x_enc_dec_attention_module['key']['kernel']\n        t5x_enc_dec_attention_out = t5x_enc_dec_attention_module['out']['kernel']\n        t5x_enc_dec_attention_query = t5x_enc_dec_attention_module['query']['kernel']\n        t5x_enc_dec_attention_value = t5x_enc_dec_attention_module['value']['kernel']\n        t5x_cross_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_cross_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['decoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['decoder'][layer_name]['mlp']['wo']['kernel']\n        tx5_mlp_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_decoder_layer_block = flax_model.params['decoder']['block'][str(layer_index)]['layer']\n        flax_model_decoder_layer_block['0']['SelfAttention']['k']['kernel'] = t5x_attention_key\n        flax_model_decoder_layer_block['0']['SelfAttention']['o']['kernel'] = t5x_attention_out\n        flax_model_decoder_layer_block['0']['SelfAttention']['q']['kernel'] = t5x_attention_query\n        flax_model_decoder_layer_block['0']['SelfAttention']['v']['kernel'] = t5x_attention_value\n        flax_model_decoder_layer_block['0']['layer_norm']['weight'] = t5x_pre_attention_layer_norm\n        flax_model_decoder_layer_block['1']['EncDecAttention']['k']['kernel'] = t5x_enc_dec_attention_key\n        flax_model_decoder_layer_block['1']['EncDecAttention']['o']['kernel'] = t5x_enc_dec_attention_out\n        flax_model_decoder_layer_block['1']['EncDecAttention']['q']['kernel'] = t5x_enc_dec_attention_query\n        flax_model_decoder_layer_block['1']['EncDecAttention']['v']['kernel'] = t5x_enc_dec_attention_value\n        flax_model_decoder_layer_block['1']['layer_norm']['weight'] = t5x_cross_layer_norm\n        if split_mlp_wi:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_decoder_layer_block['2']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_decoder_layer_block['2']['layer_norm']['weight'] = tx5_mlp_layer_norm\n        flax_model.params['decoder']['block'][str(layer_index)]['layer'] = flax_model_decoder_layer_block\n    tx5_decoder_norm = t5x_model['target']['decoder']['decoder_norm']['scale']\n    flax_model.params['decoder']['final_layer_norm']['weight'] = tx5_decoder_norm\n    t5x_decoder_rel_embedding = t5x_model['target']['decoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['decoder']['block']['0']['layer']['0']['SelfAttention']['relative_attention_bias']['embedding'] = t5x_decoder_rel_embedding\n    tx5_token_embeddings = t5x_model['target']['token_embedder']['embedding']\n    flax_model.params['shared']['embedding'] = tx5_token_embeddings\n    if 'logits_dense' in t5x_model['target']['decoder']:\n        flax_model.params['lm_head']['kernel'] = t5x_model['target']['decoder']['logits_dense']['kernel']\n    flax_model.save_pretrained(flax_dump_folder_path)\n    print('T5X Model was sucessfully converted!')",
            "def convert_t5x_checkpoint_to_flax(t5x_checkpoint_path, config_name, flax_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    config = AutoConfig.from_pretrained(config_name)\n    flax_model = FlaxAutoModelForSeq2SeqLM.from_config(config=config)\n    t5x_model = checkpoints.load_t5x_checkpoint(t5x_checkpoint_path)\n    split_mlp_wi = 'wi_0' in t5x_model['target']['encoder']['layers_0']['mlp']\n    if config.model_type == 't5':\n        encoder_attn_name = 'SelfAttention'\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'local':\n        encoder_attn_name = 'LocalSelfAttention'\n    elif config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        encoder_attn_name = 'TransientGlobalSelfAttention'\n    else:\n        raise ValueError(\"Given config is expected to have `model_type='t5'`, or `model_type='longt5` with `encoder_attention_type` attribute with a value from ['local', 'transient-global].\")\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['encoder'][layer_name]['attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['encoder'][layer_name]['attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['encoder'][layer_name]['attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['encoder'][layer_name]['attention']['value']['kernel']\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            t5x_global_layer_norm = t5x_model['target']['encoder'][layer_name]['attention']['T5LayerNorm_0']['scale']\n        t5x_attention_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['encoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['encoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['encoder'][layer_name]['mlp']['wo']['kernel']\n        t5x_mlp_layer_norm = t5x_model['target']['encoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_encoder_layer_block = flax_model.params['encoder']['block'][str(layer_index)]['layer']\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['k']['kernel'] = t5x_attention_key\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['o']['kernel'] = t5x_attention_out\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['q']['kernel'] = t5x_attention_query\n        flax_model_encoder_layer_block['0'][encoder_attn_name]['v']['kernel'] = t5x_attention_value\n        flax_model_encoder_layer_block['0']['layer_norm']['weight'] = t5x_attention_layer_norm\n        if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n            flax_model_encoder_layer_block['0'][encoder_attn_name]['global_input_layer_norm']['weight'] = t5x_global_layer_norm\n        if split_mlp_wi:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_encoder_layer_block['1']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_encoder_layer_block['1']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_encoder_layer_block['1']['layer_norm']['weight'] = t5x_mlp_layer_norm\n        flax_model.params['encoder']['block'][str(layer_index)]['layer'] = flax_model_encoder_layer_block\n    t5x_encoder_rel_embedding = t5x_model['target']['encoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['relative_attention_bias']['embedding'] = t5x_encoder_rel_embedding\n    if config.model_type == 'longt5' and config.encoder_attention_type == 'transient-global':\n        t5x_encoder_global_rel_embedding = t5x_model['target']['encoder']['side_relpos_bias']['rel_embedding'].T\n        flax_model.params['encoder']['block']['0']['layer']['0'][encoder_attn_name]['global_relative_attention_bias']['embedding'] = t5x_encoder_global_rel_embedding\n    t5x_encoder_norm = t5x_model['target']['encoder']['encoder_norm']['scale']\n    flax_model.params['encoder']['final_layer_norm']['weight'] = t5x_encoder_norm\n    for layer_index in range(config.num_layers):\n        layer_name = f'layers_{str(layer_index)}'\n        t5x_attention_key = t5x_model['target']['decoder'][layer_name]['self_attention']['key']['kernel']\n        t5x_attention_out = t5x_model['target']['decoder'][layer_name]['self_attention']['out']['kernel']\n        t5x_attention_query = t5x_model['target']['decoder'][layer_name]['self_attention']['query']['kernel']\n        t5x_attention_value = t5x_model['target']['decoder'][layer_name]['self_attention']['value']['kernel']\n        t5x_pre_attention_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_self_attention_layer_norm']['scale']\n        t5x_enc_dec_attention_module = t5x_model['target']['decoder'][layer_name]['encoder_decoder_attention']\n        t5x_enc_dec_attention_key = t5x_enc_dec_attention_module['key']['kernel']\n        t5x_enc_dec_attention_out = t5x_enc_dec_attention_module['out']['kernel']\n        t5x_enc_dec_attention_query = t5x_enc_dec_attention_module['query']['kernel']\n        t5x_enc_dec_attention_value = t5x_enc_dec_attention_module['value']['kernel']\n        t5x_cross_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_cross_attention_layer_norm']['scale']\n        if split_mlp_wi:\n            t5x_mlp_wi_0 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_0']['kernel']\n            t5x_mlp_wi_1 = t5x_model['target']['decoder'][layer_name]['mlp']['wi_1']['kernel']\n        else:\n            t5x_mlp_wi = t5x_model['target']['decoder'][layer_name]['mlp']['wi']['kernel']\n        t5x_mlp_wo = t5x_model['target']['decoder'][layer_name]['mlp']['wo']['kernel']\n        tx5_mlp_layer_norm = t5x_model['target']['decoder'][layer_name]['pre_mlp_layer_norm']['scale']\n        flax_model_decoder_layer_block = flax_model.params['decoder']['block'][str(layer_index)]['layer']\n        flax_model_decoder_layer_block['0']['SelfAttention']['k']['kernel'] = t5x_attention_key\n        flax_model_decoder_layer_block['0']['SelfAttention']['o']['kernel'] = t5x_attention_out\n        flax_model_decoder_layer_block['0']['SelfAttention']['q']['kernel'] = t5x_attention_query\n        flax_model_decoder_layer_block['0']['SelfAttention']['v']['kernel'] = t5x_attention_value\n        flax_model_decoder_layer_block['0']['layer_norm']['weight'] = t5x_pre_attention_layer_norm\n        flax_model_decoder_layer_block['1']['EncDecAttention']['k']['kernel'] = t5x_enc_dec_attention_key\n        flax_model_decoder_layer_block['1']['EncDecAttention']['o']['kernel'] = t5x_enc_dec_attention_out\n        flax_model_decoder_layer_block['1']['EncDecAttention']['q']['kernel'] = t5x_enc_dec_attention_query\n        flax_model_decoder_layer_block['1']['EncDecAttention']['v']['kernel'] = t5x_enc_dec_attention_value\n        flax_model_decoder_layer_block['1']['layer_norm']['weight'] = t5x_cross_layer_norm\n        if split_mlp_wi:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_0']['kernel'] = t5x_mlp_wi_0\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi_1']['kernel'] = t5x_mlp_wi_1\n        else:\n            flax_model_decoder_layer_block['2']['DenseReluDense']['wi']['kernel'] = t5x_mlp_wi\n        flax_model_decoder_layer_block['2']['DenseReluDense']['wo']['kernel'] = t5x_mlp_wo\n        flax_model_decoder_layer_block['2']['layer_norm']['weight'] = tx5_mlp_layer_norm\n        flax_model.params['decoder']['block'][str(layer_index)]['layer'] = flax_model_decoder_layer_block\n    tx5_decoder_norm = t5x_model['target']['decoder']['decoder_norm']['scale']\n    flax_model.params['decoder']['final_layer_norm']['weight'] = tx5_decoder_norm\n    t5x_decoder_rel_embedding = t5x_model['target']['decoder']['relpos_bias']['rel_embedding'].T\n    flax_model.params['decoder']['block']['0']['layer']['0']['SelfAttention']['relative_attention_bias']['embedding'] = t5x_decoder_rel_embedding\n    tx5_token_embeddings = t5x_model['target']['token_embedder']['embedding']\n    flax_model.params['shared']['embedding'] = tx5_token_embeddings\n    if 'logits_dense' in t5x_model['target']['decoder']:\n        flax_model.params['lm_head']['kernel'] = t5x_model['target']['decoder']['logits_dense']['kernel']\n    flax_model.save_pretrained(flax_dump_folder_path)\n    print('T5X Model was sucessfully converted!')"
        ]
    }
]