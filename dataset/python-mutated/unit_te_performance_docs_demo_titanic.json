[
    {
        "func_name": "test_that_te_is_helpful_for_titanic_gbm_xval",
        "original": "def test_that_te_is_helpful_for_titanic_gbm_xval():\n    titanic = h2o.import_file(pyunit_utils.locate('smalldata/gbm_test/titanic.csv'))\n    titanic['survived'] = titanic['survived'].asfactor()\n    response = 'survived'\n    (train, test) = titanic.split_frame(ratios=[0.8], seed=1234)\n    encoded_columns = ['home.dest', 'cabin', 'embarked']\n    blended_avg = True\n    inflection_point = 3\n    smoothing = 10\n    noise = 0.15\n    data_leakage_handling = 'k_fold'\n    fold_column = 'kfold_column'\n    train[fold_column] = train.kfold_column(n_folds=5, seed=3456)\n    titanic_te = H2OTargetEncoderEstimator(fold_column=fold_column, data_leakage_handling=data_leakage_handling, blending=blended_avg, inflection_point=inflection_point, smoothing=smoothing, seed=1234)\n    titanic_te.train(x=encoded_columns, y=response, training_frame=train)\n    train_te = titanic_te.transform(frame=train, as_training=True)\n    test_te = titanic_te.transform(frame=test, noise=0.0)\n    gbm_with_te = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_with_te')\n    x_with_te = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin_te', 'embarked_te', 'home.dest_te']\n    gbm_with_te.train(x=x_with_te, y=response, training_frame=train_te)\n    my_gbm_metrics_train_auc = gbm_with_te.model_performance(train_te).auc()\n    print('TE train:' + str(my_gbm_metrics_train_auc))\n    my_gbm_metrics = gbm_with_te.model_performance(test_te)\n    auc_with_te = my_gbm_metrics.auc()\n    print('TE test:' + str(auc_with_te))\n    gbm_baseline = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_baseline')\n    x_baseline = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin', 'embarked', 'home.dest']\n    gbm_baseline.train(x=x_baseline, y=response, training_frame=train)\n    gbm_baseline_metrics = gbm_baseline.model_performance(test)\n    auc_baseline = gbm_baseline_metrics.auc()\n    print('Baseline test:' + str(auc_baseline))\n    assert auc_with_te > auc_baseline",
        "mutated": [
            "def test_that_te_is_helpful_for_titanic_gbm_xval():\n    if False:\n        i = 10\n    titanic = h2o.import_file(pyunit_utils.locate('smalldata/gbm_test/titanic.csv'))\n    titanic['survived'] = titanic['survived'].asfactor()\n    response = 'survived'\n    (train, test) = titanic.split_frame(ratios=[0.8], seed=1234)\n    encoded_columns = ['home.dest', 'cabin', 'embarked']\n    blended_avg = True\n    inflection_point = 3\n    smoothing = 10\n    noise = 0.15\n    data_leakage_handling = 'k_fold'\n    fold_column = 'kfold_column'\n    train[fold_column] = train.kfold_column(n_folds=5, seed=3456)\n    titanic_te = H2OTargetEncoderEstimator(fold_column=fold_column, data_leakage_handling=data_leakage_handling, blending=blended_avg, inflection_point=inflection_point, smoothing=smoothing, seed=1234)\n    titanic_te.train(x=encoded_columns, y=response, training_frame=train)\n    train_te = titanic_te.transform(frame=train, as_training=True)\n    test_te = titanic_te.transform(frame=test, noise=0.0)\n    gbm_with_te = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_with_te')\n    x_with_te = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin_te', 'embarked_te', 'home.dest_te']\n    gbm_with_te.train(x=x_with_te, y=response, training_frame=train_te)\n    my_gbm_metrics_train_auc = gbm_with_te.model_performance(train_te).auc()\n    print('TE train:' + str(my_gbm_metrics_train_auc))\n    my_gbm_metrics = gbm_with_te.model_performance(test_te)\n    auc_with_te = my_gbm_metrics.auc()\n    print('TE test:' + str(auc_with_te))\n    gbm_baseline = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_baseline')\n    x_baseline = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin', 'embarked', 'home.dest']\n    gbm_baseline.train(x=x_baseline, y=response, training_frame=train)\n    gbm_baseline_metrics = gbm_baseline.model_performance(test)\n    auc_baseline = gbm_baseline_metrics.auc()\n    print('Baseline test:' + str(auc_baseline))\n    assert auc_with_te > auc_baseline",
            "def test_that_te_is_helpful_for_titanic_gbm_xval():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    titanic = h2o.import_file(pyunit_utils.locate('smalldata/gbm_test/titanic.csv'))\n    titanic['survived'] = titanic['survived'].asfactor()\n    response = 'survived'\n    (train, test) = titanic.split_frame(ratios=[0.8], seed=1234)\n    encoded_columns = ['home.dest', 'cabin', 'embarked']\n    blended_avg = True\n    inflection_point = 3\n    smoothing = 10\n    noise = 0.15\n    data_leakage_handling = 'k_fold'\n    fold_column = 'kfold_column'\n    train[fold_column] = train.kfold_column(n_folds=5, seed=3456)\n    titanic_te = H2OTargetEncoderEstimator(fold_column=fold_column, data_leakage_handling=data_leakage_handling, blending=blended_avg, inflection_point=inflection_point, smoothing=smoothing, seed=1234)\n    titanic_te.train(x=encoded_columns, y=response, training_frame=train)\n    train_te = titanic_te.transform(frame=train, as_training=True)\n    test_te = titanic_te.transform(frame=test, noise=0.0)\n    gbm_with_te = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_with_te')\n    x_with_te = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin_te', 'embarked_te', 'home.dest_te']\n    gbm_with_te.train(x=x_with_te, y=response, training_frame=train_te)\n    my_gbm_metrics_train_auc = gbm_with_te.model_performance(train_te).auc()\n    print('TE train:' + str(my_gbm_metrics_train_auc))\n    my_gbm_metrics = gbm_with_te.model_performance(test_te)\n    auc_with_te = my_gbm_metrics.auc()\n    print('TE test:' + str(auc_with_te))\n    gbm_baseline = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_baseline')\n    x_baseline = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin', 'embarked', 'home.dest']\n    gbm_baseline.train(x=x_baseline, y=response, training_frame=train)\n    gbm_baseline_metrics = gbm_baseline.model_performance(test)\n    auc_baseline = gbm_baseline_metrics.auc()\n    print('Baseline test:' + str(auc_baseline))\n    assert auc_with_te > auc_baseline",
            "def test_that_te_is_helpful_for_titanic_gbm_xval():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    titanic = h2o.import_file(pyunit_utils.locate('smalldata/gbm_test/titanic.csv'))\n    titanic['survived'] = titanic['survived'].asfactor()\n    response = 'survived'\n    (train, test) = titanic.split_frame(ratios=[0.8], seed=1234)\n    encoded_columns = ['home.dest', 'cabin', 'embarked']\n    blended_avg = True\n    inflection_point = 3\n    smoothing = 10\n    noise = 0.15\n    data_leakage_handling = 'k_fold'\n    fold_column = 'kfold_column'\n    train[fold_column] = train.kfold_column(n_folds=5, seed=3456)\n    titanic_te = H2OTargetEncoderEstimator(fold_column=fold_column, data_leakage_handling=data_leakage_handling, blending=blended_avg, inflection_point=inflection_point, smoothing=smoothing, seed=1234)\n    titanic_te.train(x=encoded_columns, y=response, training_frame=train)\n    train_te = titanic_te.transform(frame=train, as_training=True)\n    test_te = titanic_te.transform(frame=test, noise=0.0)\n    gbm_with_te = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_with_te')\n    x_with_te = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin_te', 'embarked_te', 'home.dest_te']\n    gbm_with_te.train(x=x_with_te, y=response, training_frame=train_te)\n    my_gbm_metrics_train_auc = gbm_with_te.model_performance(train_te).auc()\n    print('TE train:' + str(my_gbm_metrics_train_auc))\n    my_gbm_metrics = gbm_with_te.model_performance(test_te)\n    auc_with_te = my_gbm_metrics.auc()\n    print('TE test:' + str(auc_with_te))\n    gbm_baseline = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_baseline')\n    x_baseline = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin', 'embarked', 'home.dest']\n    gbm_baseline.train(x=x_baseline, y=response, training_frame=train)\n    gbm_baseline_metrics = gbm_baseline.model_performance(test)\n    auc_baseline = gbm_baseline_metrics.auc()\n    print('Baseline test:' + str(auc_baseline))\n    assert auc_with_te > auc_baseline",
            "def test_that_te_is_helpful_for_titanic_gbm_xval():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    titanic = h2o.import_file(pyunit_utils.locate('smalldata/gbm_test/titanic.csv'))\n    titanic['survived'] = titanic['survived'].asfactor()\n    response = 'survived'\n    (train, test) = titanic.split_frame(ratios=[0.8], seed=1234)\n    encoded_columns = ['home.dest', 'cabin', 'embarked']\n    blended_avg = True\n    inflection_point = 3\n    smoothing = 10\n    noise = 0.15\n    data_leakage_handling = 'k_fold'\n    fold_column = 'kfold_column'\n    train[fold_column] = train.kfold_column(n_folds=5, seed=3456)\n    titanic_te = H2OTargetEncoderEstimator(fold_column=fold_column, data_leakage_handling=data_leakage_handling, blending=blended_avg, inflection_point=inflection_point, smoothing=smoothing, seed=1234)\n    titanic_te.train(x=encoded_columns, y=response, training_frame=train)\n    train_te = titanic_te.transform(frame=train, as_training=True)\n    test_te = titanic_te.transform(frame=test, noise=0.0)\n    gbm_with_te = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_with_te')\n    x_with_te = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin_te', 'embarked_te', 'home.dest_te']\n    gbm_with_te.train(x=x_with_te, y=response, training_frame=train_te)\n    my_gbm_metrics_train_auc = gbm_with_te.model_performance(train_te).auc()\n    print('TE train:' + str(my_gbm_metrics_train_auc))\n    my_gbm_metrics = gbm_with_te.model_performance(test_te)\n    auc_with_te = my_gbm_metrics.auc()\n    print('TE test:' + str(auc_with_te))\n    gbm_baseline = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_baseline')\n    x_baseline = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin', 'embarked', 'home.dest']\n    gbm_baseline.train(x=x_baseline, y=response, training_frame=train)\n    gbm_baseline_metrics = gbm_baseline.model_performance(test)\n    auc_baseline = gbm_baseline_metrics.auc()\n    print('Baseline test:' + str(auc_baseline))\n    assert auc_with_te > auc_baseline",
            "def test_that_te_is_helpful_for_titanic_gbm_xval():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    titanic = h2o.import_file(pyunit_utils.locate('smalldata/gbm_test/titanic.csv'))\n    titanic['survived'] = titanic['survived'].asfactor()\n    response = 'survived'\n    (train, test) = titanic.split_frame(ratios=[0.8], seed=1234)\n    encoded_columns = ['home.dest', 'cabin', 'embarked']\n    blended_avg = True\n    inflection_point = 3\n    smoothing = 10\n    noise = 0.15\n    data_leakage_handling = 'k_fold'\n    fold_column = 'kfold_column'\n    train[fold_column] = train.kfold_column(n_folds=5, seed=3456)\n    titanic_te = H2OTargetEncoderEstimator(fold_column=fold_column, data_leakage_handling=data_leakage_handling, blending=blended_avg, inflection_point=inflection_point, smoothing=smoothing, seed=1234)\n    titanic_te.train(x=encoded_columns, y=response, training_frame=train)\n    train_te = titanic_te.transform(frame=train, as_training=True)\n    test_te = titanic_te.transform(frame=test, noise=0.0)\n    gbm_with_te = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_with_te')\n    x_with_te = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin_te', 'embarked_te', 'home.dest_te']\n    gbm_with_te.train(x=x_with_te, y=response, training_frame=train_te)\n    my_gbm_metrics_train_auc = gbm_with_te.model_performance(train_te).auc()\n    print('TE train:' + str(my_gbm_metrics_train_auc))\n    my_gbm_metrics = gbm_with_te.model_performance(test_te)\n    auc_with_te = my_gbm_metrics.auc()\n    print('TE test:' + str(auc_with_te))\n    gbm_baseline = H2OGradientBoostingEstimator(max_depth=6, min_rows=1, fold_column=fold_column, score_tree_interval=5, ntrees=10000, sample_rate=0.8, col_sample_rate=0.8, seed=1234, stopping_rounds=5, stopping_metric='auto', stopping_tolerance=0.001, model_id='gbm_baseline')\n    x_baseline = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin', 'embarked', 'home.dest']\n    gbm_baseline.train(x=x_baseline, y=response, training_frame=train)\n    gbm_baseline_metrics = gbm_baseline.model_performance(test)\n    auc_baseline = gbm_baseline_metrics.auc()\n    print('Baseline test:' + str(auc_baseline))\n    assert auc_with_te > auc_baseline"
        ]
    }
]