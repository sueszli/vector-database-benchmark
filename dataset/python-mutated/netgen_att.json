[
    {
        "func_name": "_get_lstm_cell",
        "original": "def _get_lstm_cell(num_layers, lstm_dim):\n    cell_list = [tf.contrib.rnn.BasicLSTMCell(lstm_dim, state_is_tuple=True) for _ in range(num_layers)]\n    cell = tf.contrib.rnn.MultiRNNCell(cell_list, state_is_tuple=True)\n    return cell",
        "mutated": [
            "def _get_lstm_cell(num_layers, lstm_dim):\n    if False:\n        i = 10\n    cell_list = [tf.contrib.rnn.BasicLSTMCell(lstm_dim, state_is_tuple=True) for _ in range(num_layers)]\n    cell = tf.contrib.rnn.MultiRNNCell(cell_list, state_is_tuple=True)\n    return cell",
            "def _get_lstm_cell(num_layers, lstm_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cell_list = [tf.contrib.rnn.BasicLSTMCell(lstm_dim, state_is_tuple=True) for _ in range(num_layers)]\n    cell = tf.contrib.rnn.MultiRNNCell(cell_list, state_is_tuple=True)\n    return cell",
            "def _get_lstm_cell(num_layers, lstm_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cell_list = [tf.contrib.rnn.BasicLSTMCell(lstm_dim, state_is_tuple=True) for _ in range(num_layers)]\n    cell = tf.contrib.rnn.MultiRNNCell(cell_list, state_is_tuple=True)\n    return cell",
            "def _get_lstm_cell(num_layers, lstm_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cell_list = [tf.contrib.rnn.BasicLSTMCell(lstm_dim, state_is_tuple=True) for _ in range(num_layers)]\n    cell = tf.contrib.rnn.MultiRNNCell(cell_list, state_is_tuple=True)\n    return cell",
            "def _get_lstm_cell(num_layers, lstm_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cell_list = [tf.contrib.rnn.BasicLSTMCell(lstm_dim, state_is_tuple=True) for _ in range(num_layers)]\n    cell = tf.contrib.rnn.MultiRNNCell(cell_list, state_is_tuple=True)\n    return cell"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, config, text_seq_batch, seq_length_batch, num_vocab_txt, num_vocab_nmn, EOS_token, decoder_sampling, embedding_mat, use_gt_layout=None, gt_layout_batch=None, scope='encoder_decoder', reuse=None):\n    self.T_decoder = config.T_decoder\n    self.encoder_num_vocab = num_vocab_txt\n    self.encoder_embed_dim = config.embed_dim_txt\n    self.decoder_num_vocab = num_vocab_nmn\n    self.decoder_embed_dim = config.embed_dim_nmn\n    self.lstm_dim = config.lstm_dim\n    self.num_layers = config.num_layers\n    self.EOS_token = EOS_token\n    self.decoder_sampling = decoder_sampling\n    self.embedding_mat = embedding_mat\n    with tf.variable_scope(scope, reuse=reuse):\n        self._build_encoder(text_seq_batch, seq_length_batch)\n        self._build_decoder(use_gt_layout, gt_layout_batch)",
        "mutated": [
            "def __init__(self, config, text_seq_batch, seq_length_batch, num_vocab_txt, num_vocab_nmn, EOS_token, decoder_sampling, embedding_mat, use_gt_layout=None, gt_layout_batch=None, scope='encoder_decoder', reuse=None):\n    if False:\n        i = 10\n    self.T_decoder = config.T_decoder\n    self.encoder_num_vocab = num_vocab_txt\n    self.encoder_embed_dim = config.embed_dim_txt\n    self.decoder_num_vocab = num_vocab_nmn\n    self.decoder_embed_dim = config.embed_dim_nmn\n    self.lstm_dim = config.lstm_dim\n    self.num_layers = config.num_layers\n    self.EOS_token = EOS_token\n    self.decoder_sampling = decoder_sampling\n    self.embedding_mat = embedding_mat\n    with tf.variable_scope(scope, reuse=reuse):\n        self._build_encoder(text_seq_batch, seq_length_batch)\n        self._build_decoder(use_gt_layout, gt_layout_batch)",
            "def __init__(self, config, text_seq_batch, seq_length_batch, num_vocab_txt, num_vocab_nmn, EOS_token, decoder_sampling, embedding_mat, use_gt_layout=None, gt_layout_batch=None, scope='encoder_decoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.T_decoder = config.T_decoder\n    self.encoder_num_vocab = num_vocab_txt\n    self.encoder_embed_dim = config.embed_dim_txt\n    self.decoder_num_vocab = num_vocab_nmn\n    self.decoder_embed_dim = config.embed_dim_nmn\n    self.lstm_dim = config.lstm_dim\n    self.num_layers = config.num_layers\n    self.EOS_token = EOS_token\n    self.decoder_sampling = decoder_sampling\n    self.embedding_mat = embedding_mat\n    with tf.variable_scope(scope, reuse=reuse):\n        self._build_encoder(text_seq_batch, seq_length_batch)\n        self._build_decoder(use_gt_layout, gt_layout_batch)",
            "def __init__(self, config, text_seq_batch, seq_length_batch, num_vocab_txt, num_vocab_nmn, EOS_token, decoder_sampling, embedding_mat, use_gt_layout=None, gt_layout_batch=None, scope='encoder_decoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.T_decoder = config.T_decoder\n    self.encoder_num_vocab = num_vocab_txt\n    self.encoder_embed_dim = config.embed_dim_txt\n    self.decoder_num_vocab = num_vocab_nmn\n    self.decoder_embed_dim = config.embed_dim_nmn\n    self.lstm_dim = config.lstm_dim\n    self.num_layers = config.num_layers\n    self.EOS_token = EOS_token\n    self.decoder_sampling = decoder_sampling\n    self.embedding_mat = embedding_mat\n    with tf.variable_scope(scope, reuse=reuse):\n        self._build_encoder(text_seq_batch, seq_length_batch)\n        self._build_decoder(use_gt_layout, gt_layout_batch)",
            "def __init__(self, config, text_seq_batch, seq_length_batch, num_vocab_txt, num_vocab_nmn, EOS_token, decoder_sampling, embedding_mat, use_gt_layout=None, gt_layout_batch=None, scope='encoder_decoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.T_decoder = config.T_decoder\n    self.encoder_num_vocab = num_vocab_txt\n    self.encoder_embed_dim = config.embed_dim_txt\n    self.decoder_num_vocab = num_vocab_nmn\n    self.decoder_embed_dim = config.embed_dim_nmn\n    self.lstm_dim = config.lstm_dim\n    self.num_layers = config.num_layers\n    self.EOS_token = EOS_token\n    self.decoder_sampling = decoder_sampling\n    self.embedding_mat = embedding_mat\n    with tf.variable_scope(scope, reuse=reuse):\n        self._build_encoder(text_seq_batch, seq_length_batch)\n        self._build_decoder(use_gt_layout, gt_layout_batch)",
            "def __init__(self, config, text_seq_batch, seq_length_batch, num_vocab_txt, num_vocab_nmn, EOS_token, decoder_sampling, embedding_mat, use_gt_layout=None, gt_layout_batch=None, scope='encoder_decoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.T_decoder = config.T_decoder\n    self.encoder_num_vocab = num_vocab_txt\n    self.encoder_embed_dim = config.embed_dim_txt\n    self.decoder_num_vocab = num_vocab_nmn\n    self.decoder_embed_dim = config.embed_dim_nmn\n    self.lstm_dim = config.lstm_dim\n    self.num_layers = config.num_layers\n    self.EOS_token = EOS_token\n    self.decoder_sampling = decoder_sampling\n    self.embedding_mat = embedding_mat\n    with tf.variable_scope(scope, reuse=reuse):\n        self._build_encoder(text_seq_batch, seq_length_batch)\n        self._build_decoder(use_gt_layout, gt_layout_batch)"
        ]
    },
    {
        "func_name": "_build_encoder",
        "original": "def _build_encoder(self, text_seq_batch, seq_length_batch, scope='encoder', reuse=None):\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    with tf.variable_scope(scope, reuse=reuse):\n        T = tf.shape(text_seq_batch)[0]\n        N = tf.shape(text_seq_batch)[1]\n        self.T_encoder = T\n        self.N = N\n        embedded_seq = tf.nn.embedding_lookup(self.embedding_mat, text_seq_batch)\n        self.embedded_input_seq = embedded_seq\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (encoder_outputs, encoder_states) = tf.nn.dynamic_rnn(cell, embedded_seq, seq_length_batch, dtype=tf.float32, time_major=True, scope='lstm')\n        self.encoder_outputs = encoder_outputs\n        self.encoder_states = encoder_states\n        encoder_h_transformed = fc('encoder_h_transform', tf.reshape(encoder_outputs, [-1, lstm_dim]), output_dim=lstm_dim)\n        encoder_h_transformed = tf.reshape(encoder_h_transformed, [T, N, lstm_dim])\n        self.encoder_h_transformed = encoder_h_transformed\n        seq_not_finished = tf.less(tf.range(T)[:, tf.newaxis, tf.newaxis], seq_length_batch[:, tf.newaxis])\n        seq_not_finished = tf.cast(seq_not_finished, tf.float32)\n        self.seq_not_finished = seq_not_finished",
        "mutated": [
            "def _build_encoder(self, text_seq_batch, seq_length_batch, scope='encoder', reuse=None):\n    if False:\n        i = 10\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    with tf.variable_scope(scope, reuse=reuse):\n        T = tf.shape(text_seq_batch)[0]\n        N = tf.shape(text_seq_batch)[1]\n        self.T_encoder = T\n        self.N = N\n        embedded_seq = tf.nn.embedding_lookup(self.embedding_mat, text_seq_batch)\n        self.embedded_input_seq = embedded_seq\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (encoder_outputs, encoder_states) = tf.nn.dynamic_rnn(cell, embedded_seq, seq_length_batch, dtype=tf.float32, time_major=True, scope='lstm')\n        self.encoder_outputs = encoder_outputs\n        self.encoder_states = encoder_states\n        encoder_h_transformed = fc('encoder_h_transform', tf.reshape(encoder_outputs, [-1, lstm_dim]), output_dim=lstm_dim)\n        encoder_h_transformed = tf.reshape(encoder_h_transformed, [T, N, lstm_dim])\n        self.encoder_h_transformed = encoder_h_transformed\n        seq_not_finished = tf.less(tf.range(T)[:, tf.newaxis, tf.newaxis], seq_length_batch[:, tf.newaxis])\n        seq_not_finished = tf.cast(seq_not_finished, tf.float32)\n        self.seq_not_finished = seq_not_finished",
            "def _build_encoder(self, text_seq_batch, seq_length_batch, scope='encoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    with tf.variable_scope(scope, reuse=reuse):\n        T = tf.shape(text_seq_batch)[0]\n        N = tf.shape(text_seq_batch)[1]\n        self.T_encoder = T\n        self.N = N\n        embedded_seq = tf.nn.embedding_lookup(self.embedding_mat, text_seq_batch)\n        self.embedded_input_seq = embedded_seq\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (encoder_outputs, encoder_states) = tf.nn.dynamic_rnn(cell, embedded_seq, seq_length_batch, dtype=tf.float32, time_major=True, scope='lstm')\n        self.encoder_outputs = encoder_outputs\n        self.encoder_states = encoder_states\n        encoder_h_transformed = fc('encoder_h_transform', tf.reshape(encoder_outputs, [-1, lstm_dim]), output_dim=lstm_dim)\n        encoder_h_transformed = tf.reshape(encoder_h_transformed, [T, N, lstm_dim])\n        self.encoder_h_transformed = encoder_h_transformed\n        seq_not_finished = tf.less(tf.range(T)[:, tf.newaxis, tf.newaxis], seq_length_batch[:, tf.newaxis])\n        seq_not_finished = tf.cast(seq_not_finished, tf.float32)\n        self.seq_not_finished = seq_not_finished",
            "def _build_encoder(self, text_seq_batch, seq_length_batch, scope='encoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    with tf.variable_scope(scope, reuse=reuse):\n        T = tf.shape(text_seq_batch)[0]\n        N = tf.shape(text_seq_batch)[1]\n        self.T_encoder = T\n        self.N = N\n        embedded_seq = tf.nn.embedding_lookup(self.embedding_mat, text_seq_batch)\n        self.embedded_input_seq = embedded_seq\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (encoder_outputs, encoder_states) = tf.nn.dynamic_rnn(cell, embedded_seq, seq_length_batch, dtype=tf.float32, time_major=True, scope='lstm')\n        self.encoder_outputs = encoder_outputs\n        self.encoder_states = encoder_states\n        encoder_h_transformed = fc('encoder_h_transform', tf.reshape(encoder_outputs, [-1, lstm_dim]), output_dim=lstm_dim)\n        encoder_h_transformed = tf.reshape(encoder_h_transformed, [T, N, lstm_dim])\n        self.encoder_h_transformed = encoder_h_transformed\n        seq_not_finished = tf.less(tf.range(T)[:, tf.newaxis, tf.newaxis], seq_length_batch[:, tf.newaxis])\n        seq_not_finished = tf.cast(seq_not_finished, tf.float32)\n        self.seq_not_finished = seq_not_finished",
            "def _build_encoder(self, text_seq_batch, seq_length_batch, scope='encoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    with tf.variable_scope(scope, reuse=reuse):\n        T = tf.shape(text_seq_batch)[0]\n        N = tf.shape(text_seq_batch)[1]\n        self.T_encoder = T\n        self.N = N\n        embedded_seq = tf.nn.embedding_lookup(self.embedding_mat, text_seq_batch)\n        self.embedded_input_seq = embedded_seq\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (encoder_outputs, encoder_states) = tf.nn.dynamic_rnn(cell, embedded_seq, seq_length_batch, dtype=tf.float32, time_major=True, scope='lstm')\n        self.encoder_outputs = encoder_outputs\n        self.encoder_states = encoder_states\n        encoder_h_transformed = fc('encoder_h_transform', tf.reshape(encoder_outputs, [-1, lstm_dim]), output_dim=lstm_dim)\n        encoder_h_transformed = tf.reshape(encoder_h_transformed, [T, N, lstm_dim])\n        self.encoder_h_transformed = encoder_h_transformed\n        seq_not_finished = tf.less(tf.range(T)[:, tf.newaxis, tf.newaxis], seq_length_batch[:, tf.newaxis])\n        seq_not_finished = tf.cast(seq_not_finished, tf.float32)\n        self.seq_not_finished = seq_not_finished",
            "def _build_encoder(self, text_seq_batch, seq_length_batch, scope='encoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    with tf.variable_scope(scope, reuse=reuse):\n        T = tf.shape(text_seq_batch)[0]\n        N = tf.shape(text_seq_batch)[1]\n        self.T_encoder = T\n        self.N = N\n        embedded_seq = tf.nn.embedding_lookup(self.embedding_mat, text_seq_batch)\n        self.embedded_input_seq = embedded_seq\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (encoder_outputs, encoder_states) = tf.nn.dynamic_rnn(cell, embedded_seq, seq_length_batch, dtype=tf.float32, time_major=True, scope='lstm')\n        self.encoder_outputs = encoder_outputs\n        self.encoder_states = encoder_states\n        encoder_h_transformed = fc('encoder_h_transform', tf.reshape(encoder_outputs, [-1, lstm_dim]), output_dim=lstm_dim)\n        encoder_h_transformed = tf.reshape(encoder_h_transformed, [T, N, lstm_dim])\n        self.encoder_h_transformed = encoder_h_transformed\n        seq_not_finished = tf.less(tf.range(T)[:, tf.newaxis, tf.newaxis], seq_length_batch[:, tf.newaxis])\n        seq_not_finished = tf.cast(seq_not_finished, tf.float32)\n        self.seq_not_finished = seq_not_finished"
        ]
    },
    {
        "func_name": "loop_fn",
        "original": "def loop_fn(time, cell_output, cell_state, loop_state):\n    if cell_output is None:\n        next_cell_state = encoder_states\n        next_input = tf.tile(go_embedding, [N, 1])\n    else:\n        next_cell_state = cell_state\n        att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n        att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n        att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n        d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n        token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n        if sampling:\n            logits = token_scores\n            predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n        else:\n            predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n        if use_gt_layout is not None:\n            predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n        mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n        all_token_probs = tf.nn.softmax(token_scores)\n        token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n        neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n        is_eos_predicted = loop_state[2]\n        predicted_token_old = predicted_token\n        predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n        token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n        neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n        is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n        next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n    elements_finished = tf.greater_equal(time, T_max)\n    if loop_state is None:\n        predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n        token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n    else:\n        t_write = time - 1\n        next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n    return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)",
        "mutated": [
            "def loop_fn(time, cell_output, cell_state, loop_state):\n    if False:\n        i = 10\n    if cell_output is None:\n        next_cell_state = encoder_states\n        next_input = tf.tile(go_embedding, [N, 1])\n    else:\n        next_cell_state = cell_state\n        att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n        att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n        att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n        d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n        token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n        if sampling:\n            logits = token_scores\n            predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n        else:\n            predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n        if use_gt_layout is not None:\n            predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n        mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n        all_token_probs = tf.nn.softmax(token_scores)\n        token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n        neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n        is_eos_predicted = loop_state[2]\n        predicted_token_old = predicted_token\n        predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n        token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n        neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n        is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n        next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n    elements_finished = tf.greater_equal(time, T_max)\n    if loop_state is None:\n        predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n        token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n    else:\n        t_write = time - 1\n        next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n    return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)",
            "def loop_fn(time, cell_output, cell_state, loop_state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if cell_output is None:\n        next_cell_state = encoder_states\n        next_input = tf.tile(go_embedding, [N, 1])\n    else:\n        next_cell_state = cell_state\n        att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n        att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n        att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n        d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n        token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n        if sampling:\n            logits = token_scores\n            predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n        else:\n            predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n        if use_gt_layout is not None:\n            predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n        mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n        all_token_probs = tf.nn.softmax(token_scores)\n        token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n        neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n        is_eos_predicted = loop_state[2]\n        predicted_token_old = predicted_token\n        predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n        token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n        neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n        is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n        next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n    elements_finished = tf.greater_equal(time, T_max)\n    if loop_state is None:\n        predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n        token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n    else:\n        t_write = time - 1\n        next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n    return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)",
            "def loop_fn(time, cell_output, cell_state, loop_state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if cell_output is None:\n        next_cell_state = encoder_states\n        next_input = tf.tile(go_embedding, [N, 1])\n    else:\n        next_cell_state = cell_state\n        att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n        att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n        att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n        d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n        token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n        if sampling:\n            logits = token_scores\n            predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n        else:\n            predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n        if use_gt_layout is not None:\n            predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n        mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n        all_token_probs = tf.nn.softmax(token_scores)\n        token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n        neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n        is_eos_predicted = loop_state[2]\n        predicted_token_old = predicted_token\n        predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n        token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n        neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n        is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n        next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n    elements_finished = tf.greater_equal(time, T_max)\n    if loop_state is None:\n        predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n        token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n    else:\n        t_write = time - 1\n        next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n    return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)",
            "def loop_fn(time, cell_output, cell_state, loop_state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if cell_output is None:\n        next_cell_state = encoder_states\n        next_input = tf.tile(go_embedding, [N, 1])\n    else:\n        next_cell_state = cell_state\n        att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n        att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n        att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n        d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n        token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n        if sampling:\n            logits = token_scores\n            predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n        else:\n            predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n        if use_gt_layout is not None:\n            predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n        mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n        all_token_probs = tf.nn.softmax(token_scores)\n        token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n        neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n        is_eos_predicted = loop_state[2]\n        predicted_token_old = predicted_token\n        predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n        token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n        neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n        is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n        next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n    elements_finished = tf.greater_equal(time, T_max)\n    if loop_state is None:\n        predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n        token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n    else:\n        t_write = time - 1\n        next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n    return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)",
            "def loop_fn(time, cell_output, cell_state, loop_state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if cell_output is None:\n        next_cell_state = encoder_states\n        next_input = tf.tile(go_embedding, [N, 1])\n    else:\n        next_cell_state = cell_state\n        att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n        att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n        att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n        d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n        token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n        if sampling:\n            logits = token_scores\n            predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n        else:\n            predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n        if use_gt_layout is not None:\n            predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n        mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n        all_token_probs = tf.nn.softmax(token_scores)\n        token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n        neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n        is_eos_predicted = loop_state[2]\n        predicted_token_old = predicted_token\n        predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n        token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n        neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n        is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n        next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n    elements_finished = tf.greater_equal(time, T_max)\n    if loop_state is None:\n        predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n        token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n        next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n    else:\n        t_write = time - 1\n        next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n    return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)"
        ]
    },
    {
        "func_name": "_build_decoder",
        "original": "def _build_decoder(self, use_gt_layout, gt_layout_batch, scope='decoder', reuse=None):\n    N = self.N\n    encoder_states = self.encoder_states\n    T_max = self.T_decoder\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    EOS_token = self.EOS_token\n    sampling = self.decoder_sampling\n    with tf.variable_scope(scope, reuse=reuse):\n        embedding_mat = tf.get_variable('embedding_mat', [self.decoder_num_vocab, self.decoder_embed_dim])\n        go_embedding = tf.get_variable('go_embedding', [1, self.decoder_embed_dim])\n        with tf.variable_scope('att_prediction'):\n            v = tf.get_variable('v', [lstm_dim])\n            W_a = tf.get_variable('weights', [lstm_dim, lstm_dim], initializer=tf.contrib.layers.xavier_initializer())\n            b_a = tf.get_variable('biases', lstm_dim, initializer=tf.constant_initializer(0.0))\n        with tf.variable_scope('token_prediction'):\n            W_y = tf.get_variable('weights', [lstm_dim * 2, self.decoder_num_vocab], initializer=tf.contrib.layers.xavier_initializer())\n            b_y = tf.get_variable('biases', self.decoder_num_vocab, initializer=tf.constant_initializer(0.0))\n        mask_range = tf.reshape(tf.range(self.decoder_num_vocab, dtype=tf.int32), [1, -1])\n        all_eos_pred = EOS_token * tf.ones([N], tf.int32)\n        all_one_prob = tf.ones([N], tf.float32)\n        all_zero_entropy = tf.zeros([N], tf.float32)\n        if use_gt_layout is not None:\n            gt_layout_mult = tf.cast(use_gt_layout, tf.int32)\n            pred_layout_mult = 1 - gt_layout_mult\n\n        def loop_fn(time, cell_output, cell_state, loop_state):\n            if cell_output is None:\n                next_cell_state = encoder_states\n                next_input = tf.tile(go_embedding, [N, 1])\n            else:\n                next_cell_state = cell_state\n                att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n                att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n                att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n                d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n                token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n                if sampling:\n                    logits = token_scores\n                    predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n                else:\n                    predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n                if use_gt_layout is not None:\n                    predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n                mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n                all_token_probs = tf.nn.softmax(token_scores)\n                token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n                neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n                is_eos_predicted = loop_state[2]\n                predicted_token_old = predicted_token\n                predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n                token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n                neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n                is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n                next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n            elements_finished = tf.greater_equal(time, T_max)\n            if loop_state is None:\n                predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n                token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n            else:\n                t_write = time - 1\n                next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n            return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (_, _, decodes_ta) = tf.nn.raw_rnn(cell, loop_fn, scope='lstm')\n        predicted_tokens = decodes_ta[0].stack()\n        token_probs = decodes_ta[1].stack()\n        neg_entropy = decodes_ta[3]\n        atts = decodes_ta[4].stack()\n        self.atts = atts\n        word_vecs = tf.reduce_sum(atts * self.embedded_input_seq, axis=1)\n        predicted_tokens.set_shape([None, None])\n        token_probs.set_shape([None, None])\n        neg_entropy.set_shape([None])\n        word_vecs.set_shape([None, None, self.encoder_embed_dim])\n        self.predicted_tokens = predicted_tokens\n        self.token_probs = token_probs\n        self.neg_entropy = neg_entropy\n        self.word_vecs = word_vecs",
        "mutated": [
            "def _build_decoder(self, use_gt_layout, gt_layout_batch, scope='decoder', reuse=None):\n    if False:\n        i = 10\n    N = self.N\n    encoder_states = self.encoder_states\n    T_max = self.T_decoder\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    EOS_token = self.EOS_token\n    sampling = self.decoder_sampling\n    with tf.variable_scope(scope, reuse=reuse):\n        embedding_mat = tf.get_variable('embedding_mat', [self.decoder_num_vocab, self.decoder_embed_dim])\n        go_embedding = tf.get_variable('go_embedding', [1, self.decoder_embed_dim])\n        with tf.variable_scope('att_prediction'):\n            v = tf.get_variable('v', [lstm_dim])\n            W_a = tf.get_variable('weights', [lstm_dim, lstm_dim], initializer=tf.contrib.layers.xavier_initializer())\n            b_a = tf.get_variable('biases', lstm_dim, initializer=tf.constant_initializer(0.0))\n        with tf.variable_scope('token_prediction'):\n            W_y = tf.get_variable('weights', [lstm_dim * 2, self.decoder_num_vocab], initializer=tf.contrib.layers.xavier_initializer())\n            b_y = tf.get_variable('biases', self.decoder_num_vocab, initializer=tf.constant_initializer(0.0))\n        mask_range = tf.reshape(tf.range(self.decoder_num_vocab, dtype=tf.int32), [1, -1])\n        all_eos_pred = EOS_token * tf.ones([N], tf.int32)\n        all_one_prob = tf.ones([N], tf.float32)\n        all_zero_entropy = tf.zeros([N], tf.float32)\n        if use_gt_layout is not None:\n            gt_layout_mult = tf.cast(use_gt_layout, tf.int32)\n            pred_layout_mult = 1 - gt_layout_mult\n\n        def loop_fn(time, cell_output, cell_state, loop_state):\n            if cell_output is None:\n                next_cell_state = encoder_states\n                next_input = tf.tile(go_embedding, [N, 1])\n            else:\n                next_cell_state = cell_state\n                att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n                att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n                att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n                d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n                token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n                if sampling:\n                    logits = token_scores\n                    predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n                else:\n                    predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n                if use_gt_layout is not None:\n                    predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n                mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n                all_token_probs = tf.nn.softmax(token_scores)\n                token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n                neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n                is_eos_predicted = loop_state[2]\n                predicted_token_old = predicted_token\n                predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n                token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n                neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n                is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n                next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n            elements_finished = tf.greater_equal(time, T_max)\n            if loop_state is None:\n                predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n                token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n            else:\n                t_write = time - 1\n                next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n            return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (_, _, decodes_ta) = tf.nn.raw_rnn(cell, loop_fn, scope='lstm')\n        predicted_tokens = decodes_ta[0].stack()\n        token_probs = decodes_ta[1].stack()\n        neg_entropy = decodes_ta[3]\n        atts = decodes_ta[4].stack()\n        self.atts = atts\n        word_vecs = tf.reduce_sum(atts * self.embedded_input_seq, axis=1)\n        predicted_tokens.set_shape([None, None])\n        token_probs.set_shape([None, None])\n        neg_entropy.set_shape([None])\n        word_vecs.set_shape([None, None, self.encoder_embed_dim])\n        self.predicted_tokens = predicted_tokens\n        self.token_probs = token_probs\n        self.neg_entropy = neg_entropy\n        self.word_vecs = word_vecs",
            "def _build_decoder(self, use_gt_layout, gt_layout_batch, scope='decoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    N = self.N\n    encoder_states = self.encoder_states\n    T_max = self.T_decoder\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    EOS_token = self.EOS_token\n    sampling = self.decoder_sampling\n    with tf.variable_scope(scope, reuse=reuse):\n        embedding_mat = tf.get_variable('embedding_mat', [self.decoder_num_vocab, self.decoder_embed_dim])\n        go_embedding = tf.get_variable('go_embedding', [1, self.decoder_embed_dim])\n        with tf.variable_scope('att_prediction'):\n            v = tf.get_variable('v', [lstm_dim])\n            W_a = tf.get_variable('weights', [lstm_dim, lstm_dim], initializer=tf.contrib.layers.xavier_initializer())\n            b_a = tf.get_variable('biases', lstm_dim, initializer=tf.constant_initializer(0.0))\n        with tf.variable_scope('token_prediction'):\n            W_y = tf.get_variable('weights', [lstm_dim * 2, self.decoder_num_vocab], initializer=tf.contrib.layers.xavier_initializer())\n            b_y = tf.get_variable('biases', self.decoder_num_vocab, initializer=tf.constant_initializer(0.0))\n        mask_range = tf.reshape(tf.range(self.decoder_num_vocab, dtype=tf.int32), [1, -1])\n        all_eos_pred = EOS_token * tf.ones([N], tf.int32)\n        all_one_prob = tf.ones([N], tf.float32)\n        all_zero_entropy = tf.zeros([N], tf.float32)\n        if use_gt_layout is not None:\n            gt_layout_mult = tf.cast(use_gt_layout, tf.int32)\n            pred_layout_mult = 1 - gt_layout_mult\n\n        def loop_fn(time, cell_output, cell_state, loop_state):\n            if cell_output is None:\n                next_cell_state = encoder_states\n                next_input = tf.tile(go_embedding, [N, 1])\n            else:\n                next_cell_state = cell_state\n                att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n                att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n                att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n                d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n                token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n                if sampling:\n                    logits = token_scores\n                    predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n                else:\n                    predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n                if use_gt_layout is not None:\n                    predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n                mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n                all_token_probs = tf.nn.softmax(token_scores)\n                token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n                neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n                is_eos_predicted = loop_state[2]\n                predicted_token_old = predicted_token\n                predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n                token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n                neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n                is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n                next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n            elements_finished = tf.greater_equal(time, T_max)\n            if loop_state is None:\n                predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n                token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n            else:\n                t_write = time - 1\n                next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n            return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (_, _, decodes_ta) = tf.nn.raw_rnn(cell, loop_fn, scope='lstm')\n        predicted_tokens = decodes_ta[0].stack()\n        token_probs = decodes_ta[1].stack()\n        neg_entropy = decodes_ta[3]\n        atts = decodes_ta[4].stack()\n        self.atts = atts\n        word_vecs = tf.reduce_sum(atts * self.embedded_input_seq, axis=1)\n        predicted_tokens.set_shape([None, None])\n        token_probs.set_shape([None, None])\n        neg_entropy.set_shape([None])\n        word_vecs.set_shape([None, None, self.encoder_embed_dim])\n        self.predicted_tokens = predicted_tokens\n        self.token_probs = token_probs\n        self.neg_entropy = neg_entropy\n        self.word_vecs = word_vecs",
            "def _build_decoder(self, use_gt_layout, gt_layout_batch, scope='decoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    N = self.N\n    encoder_states = self.encoder_states\n    T_max = self.T_decoder\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    EOS_token = self.EOS_token\n    sampling = self.decoder_sampling\n    with tf.variable_scope(scope, reuse=reuse):\n        embedding_mat = tf.get_variable('embedding_mat', [self.decoder_num_vocab, self.decoder_embed_dim])\n        go_embedding = tf.get_variable('go_embedding', [1, self.decoder_embed_dim])\n        with tf.variable_scope('att_prediction'):\n            v = tf.get_variable('v', [lstm_dim])\n            W_a = tf.get_variable('weights', [lstm_dim, lstm_dim], initializer=tf.contrib.layers.xavier_initializer())\n            b_a = tf.get_variable('biases', lstm_dim, initializer=tf.constant_initializer(0.0))\n        with tf.variable_scope('token_prediction'):\n            W_y = tf.get_variable('weights', [lstm_dim * 2, self.decoder_num_vocab], initializer=tf.contrib.layers.xavier_initializer())\n            b_y = tf.get_variable('biases', self.decoder_num_vocab, initializer=tf.constant_initializer(0.0))\n        mask_range = tf.reshape(tf.range(self.decoder_num_vocab, dtype=tf.int32), [1, -1])\n        all_eos_pred = EOS_token * tf.ones([N], tf.int32)\n        all_one_prob = tf.ones([N], tf.float32)\n        all_zero_entropy = tf.zeros([N], tf.float32)\n        if use_gt_layout is not None:\n            gt_layout_mult = tf.cast(use_gt_layout, tf.int32)\n            pred_layout_mult = 1 - gt_layout_mult\n\n        def loop_fn(time, cell_output, cell_state, loop_state):\n            if cell_output is None:\n                next_cell_state = encoder_states\n                next_input = tf.tile(go_embedding, [N, 1])\n            else:\n                next_cell_state = cell_state\n                att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n                att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n                att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n                d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n                token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n                if sampling:\n                    logits = token_scores\n                    predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n                else:\n                    predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n                if use_gt_layout is not None:\n                    predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n                mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n                all_token_probs = tf.nn.softmax(token_scores)\n                token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n                neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n                is_eos_predicted = loop_state[2]\n                predicted_token_old = predicted_token\n                predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n                token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n                neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n                is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n                next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n            elements_finished = tf.greater_equal(time, T_max)\n            if loop_state is None:\n                predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n                token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n            else:\n                t_write = time - 1\n                next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n            return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (_, _, decodes_ta) = tf.nn.raw_rnn(cell, loop_fn, scope='lstm')\n        predicted_tokens = decodes_ta[0].stack()\n        token_probs = decodes_ta[1].stack()\n        neg_entropy = decodes_ta[3]\n        atts = decodes_ta[4].stack()\n        self.atts = atts\n        word_vecs = tf.reduce_sum(atts * self.embedded_input_seq, axis=1)\n        predicted_tokens.set_shape([None, None])\n        token_probs.set_shape([None, None])\n        neg_entropy.set_shape([None])\n        word_vecs.set_shape([None, None, self.encoder_embed_dim])\n        self.predicted_tokens = predicted_tokens\n        self.token_probs = token_probs\n        self.neg_entropy = neg_entropy\n        self.word_vecs = word_vecs",
            "def _build_decoder(self, use_gt_layout, gt_layout_batch, scope='decoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    N = self.N\n    encoder_states = self.encoder_states\n    T_max = self.T_decoder\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    EOS_token = self.EOS_token\n    sampling = self.decoder_sampling\n    with tf.variable_scope(scope, reuse=reuse):\n        embedding_mat = tf.get_variable('embedding_mat', [self.decoder_num_vocab, self.decoder_embed_dim])\n        go_embedding = tf.get_variable('go_embedding', [1, self.decoder_embed_dim])\n        with tf.variable_scope('att_prediction'):\n            v = tf.get_variable('v', [lstm_dim])\n            W_a = tf.get_variable('weights', [lstm_dim, lstm_dim], initializer=tf.contrib.layers.xavier_initializer())\n            b_a = tf.get_variable('biases', lstm_dim, initializer=tf.constant_initializer(0.0))\n        with tf.variable_scope('token_prediction'):\n            W_y = tf.get_variable('weights', [lstm_dim * 2, self.decoder_num_vocab], initializer=tf.contrib.layers.xavier_initializer())\n            b_y = tf.get_variable('biases', self.decoder_num_vocab, initializer=tf.constant_initializer(0.0))\n        mask_range = tf.reshape(tf.range(self.decoder_num_vocab, dtype=tf.int32), [1, -1])\n        all_eos_pred = EOS_token * tf.ones([N], tf.int32)\n        all_one_prob = tf.ones([N], tf.float32)\n        all_zero_entropy = tf.zeros([N], tf.float32)\n        if use_gt_layout is not None:\n            gt_layout_mult = tf.cast(use_gt_layout, tf.int32)\n            pred_layout_mult = 1 - gt_layout_mult\n\n        def loop_fn(time, cell_output, cell_state, loop_state):\n            if cell_output is None:\n                next_cell_state = encoder_states\n                next_input = tf.tile(go_embedding, [N, 1])\n            else:\n                next_cell_state = cell_state\n                att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n                att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n                att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n                d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n                token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n                if sampling:\n                    logits = token_scores\n                    predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n                else:\n                    predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n                if use_gt_layout is not None:\n                    predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n                mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n                all_token_probs = tf.nn.softmax(token_scores)\n                token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n                neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n                is_eos_predicted = loop_state[2]\n                predicted_token_old = predicted_token\n                predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n                token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n                neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n                is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n                next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n            elements_finished = tf.greater_equal(time, T_max)\n            if loop_state is None:\n                predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n                token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n            else:\n                t_write = time - 1\n                next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n            return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (_, _, decodes_ta) = tf.nn.raw_rnn(cell, loop_fn, scope='lstm')\n        predicted_tokens = decodes_ta[0].stack()\n        token_probs = decodes_ta[1].stack()\n        neg_entropy = decodes_ta[3]\n        atts = decodes_ta[4].stack()\n        self.atts = atts\n        word_vecs = tf.reduce_sum(atts * self.embedded_input_seq, axis=1)\n        predicted_tokens.set_shape([None, None])\n        token_probs.set_shape([None, None])\n        neg_entropy.set_shape([None])\n        word_vecs.set_shape([None, None, self.encoder_embed_dim])\n        self.predicted_tokens = predicted_tokens\n        self.token_probs = token_probs\n        self.neg_entropy = neg_entropy\n        self.word_vecs = word_vecs",
            "def _build_decoder(self, use_gt_layout, gt_layout_batch, scope='decoder', reuse=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    N = self.N\n    encoder_states = self.encoder_states\n    T_max = self.T_decoder\n    lstm_dim = self.lstm_dim\n    num_layers = self.num_layers\n    EOS_token = self.EOS_token\n    sampling = self.decoder_sampling\n    with tf.variable_scope(scope, reuse=reuse):\n        embedding_mat = tf.get_variable('embedding_mat', [self.decoder_num_vocab, self.decoder_embed_dim])\n        go_embedding = tf.get_variable('go_embedding', [1, self.decoder_embed_dim])\n        with tf.variable_scope('att_prediction'):\n            v = tf.get_variable('v', [lstm_dim])\n            W_a = tf.get_variable('weights', [lstm_dim, lstm_dim], initializer=tf.contrib.layers.xavier_initializer())\n            b_a = tf.get_variable('biases', lstm_dim, initializer=tf.constant_initializer(0.0))\n        with tf.variable_scope('token_prediction'):\n            W_y = tf.get_variable('weights', [lstm_dim * 2, self.decoder_num_vocab], initializer=tf.contrib.layers.xavier_initializer())\n            b_y = tf.get_variable('biases', self.decoder_num_vocab, initializer=tf.constant_initializer(0.0))\n        mask_range = tf.reshape(tf.range(self.decoder_num_vocab, dtype=tf.int32), [1, -1])\n        all_eos_pred = EOS_token * tf.ones([N], tf.int32)\n        all_one_prob = tf.ones([N], tf.float32)\n        all_zero_entropy = tf.zeros([N], tf.float32)\n        if use_gt_layout is not None:\n            gt_layout_mult = tf.cast(use_gt_layout, tf.int32)\n            pred_layout_mult = 1 - gt_layout_mult\n\n        def loop_fn(time, cell_output, cell_state, loop_state):\n            if cell_output is None:\n                next_cell_state = encoder_states\n                next_input = tf.tile(go_embedding, [N, 1])\n            else:\n                next_cell_state = cell_state\n                att_raw = tf.reduce_sum(tf.tanh(tf.nn.xw_plus_b(cell_output, W_a, b_a) + self.encoder_h_transformed) * v, axis=2, keep_dims=True)\n                att = tf.nn.softmax(att_raw, dim=0) * self.seq_not_finished\n                att = att / tf.reduce_sum(att, axis=0, keep_dims=True)\n                d2 = tf.reduce_sum(att * self.encoder_outputs, axis=0)\n                token_scores = tf.nn.xw_plus_b(tf.concat([cell_output, d2], axis=1), W_y, b_y)\n                if sampling:\n                    logits = token_scores\n                    predicted_token = tf.cast(tf.reshape(tf.multinomial(token_scores, 1), [-1]), tf.int32)\n                else:\n                    predicted_token = tf.cast(tf.argmax(token_scores, 1), tf.int32)\n                if use_gt_layout is not None:\n                    predicted_token = gt_layout_batch[time - 1] * gt_layout_mult + predicted_token * pred_layout_mult\n                mask = tf.equal(mask_range, tf.reshape(predicted_token, [-1, 1]))\n                all_token_probs = tf.nn.softmax(token_scores)\n                token_prob = tf.reduce_sum(all_token_probs * tf.cast(mask, tf.float32), axis=1)\n                neg_entropy = tf.reduce_sum(all_token_probs * tf.log(all_token_probs), axis=1)\n                is_eos_predicted = loop_state[2]\n                predicted_token_old = predicted_token\n                predicted_token = tf.where(is_eos_predicted, all_eos_pred, predicted_token)\n                token_prob = tf.where(is_eos_predicted, all_one_prob, token_prob)\n                neg_entropy = tf.where(is_eos_predicted, all_zero_entropy, neg_entropy)\n                is_eos_predicted = tf.logical_or(is_eos_predicted, tf.equal(predicted_token_old, EOS_token))\n                next_input = tf.nn.embedding_lookup(embedding_mat, predicted_token)\n            elements_finished = tf.greater_equal(time, T_max)\n            if loop_state is None:\n                predicted_token_array = tf.TensorArray(dtype=tf.int32, size=T_max, infer_shape=False)\n                token_prob_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                att_array = tf.TensorArray(dtype=tf.float32, size=T_max, infer_shape=False)\n                next_loop_state = (predicted_token_array, token_prob_array, tf.zeros([N], dtype=tf.bool), tf.zeros([N], dtype=tf.float32), att_array)\n            else:\n                t_write = time - 1\n                next_loop_state = (loop_state[0].write(t_write, predicted_token), loop_state[1].write(t_write, token_prob), is_eos_predicted, loop_state[3] + neg_entropy, loop_state[4].write(t_write, att))\n            return (elements_finished, next_input, next_cell_state, cell_output, next_loop_state)\n        cell = _get_lstm_cell(num_layers, lstm_dim)\n        (_, _, decodes_ta) = tf.nn.raw_rnn(cell, loop_fn, scope='lstm')\n        predicted_tokens = decodes_ta[0].stack()\n        token_probs = decodes_ta[1].stack()\n        neg_entropy = decodes_ta[3]\n        atts = decodes_ta[4].stack()\n        self.atts = atts\n        word_vecs = tf.reduce_sum(atts * self.embedded_input_seq, axis=1)\n        predicted_tokens.set_shape([None, None])\n        token_probs.set_shape([None, None])\n        neg_entropy.set_shape([None])\n        word_vecs.set_shape([None, None, self.encoder_embed_dim])\n        self.predicted_tokens = predicted_tokens\n        self.token_probs = token_probs\n        self.neg_entropy = neg_entropy\n        self.word_vecs = word_vecs"
        ]
    }
]