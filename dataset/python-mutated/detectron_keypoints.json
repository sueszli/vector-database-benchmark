[
    {
        "func_name": "heatmaps_to_keypoints",
        "original": "def heatmaps_to_keypoints(maps, rois):\n    \"\"\"Extracts predicted keypoint locations from heatmaps. Output has shape\n    (#rois, 4, #keypoints) with the 4 rows corresponding to (x, y, logit, prob)\n    for each keypoint.\n    \"\"\"\n    offset_x = rois[:, 0]\n    offset_y = rois[:, 1]\n    widths = rois[:, 2] - rois[:, 0]\n    heights = rois[:, 3] - rois[:, 1]\n    widths = np.maximum(widths, 1)\n    heights = np.maximum(heights, 1)\n    widths_ceil = np.ceil(widths).astype(int)\n    heights_ceil = np.ceil(heights).astype(int)\n    num_keypoints = np.maximum(maps.shape[1], _NUM_KEYPOINTS)\n    maps = np.transpose(maps, [0, 2, 3, 1])\n    min_size = _INFERENCE_MIN_SIZE\n    xy_preds = np.zeros((len(rois), 4, num_keypoints), dtype=np.float32)\n    for i in range(len(rois)):\n        if min_size > 0:\n            roi_map_width = int(np.maximum(widths_ceil[i], min_size))\n            roi_map_height = int(np.maximum(heights_ceil[i], min_size))\n        else:\n            roi_map_width = widths_ceil[i]\n            roi_map_height = heights_ceil[i]\n        width_correction = widths[i] / roi_map_width\n        height_correction = heights[i] / roi_map_height\n        roi_map = cv2.resize(maps[i], (roi_map_width, roi_map_height), interpolation=cv2.INTER_CUBIC)\n        roi_map = np.transpose(roi_map, [2, 0, 1])\n        roi_map_probs = scores_to_probs(roi_map.copy())\n        w = roi_map.shape[2]\n        for k in range(num_keypoints):\n            pos = roi_map[k, :, :].argmax()\n            x_int = pos % w\n            y_int = (pos - x_int) // w\n            assert roi_map_probs[k, y_int, x_int] == roi_map_probs[k, :, :].max()\n            x = (x_int + 0.5) * width_correction\n            y = (y_int + 0.5) * height_correction\n            xy_preds[i, 0, k] = x + offset_x[i]\n            xy_preds[i, 1, k] = y + offset_y[i]\n            xy_preds[i, 2, k] = roi_map[k, y_int, x_int]\n            xy_preds[i, 3, k] = roi_map_probs[k, y_int, x_int]\n    return xy_preds",
        "mutated": [
            "def heatmaps_to_keypoints(maps, rois):\n    if False:\n        i = 10\n    'Extracts predicted keypoint locations from heatmaps. Output has shape\\n    (#rois, 4, #keypoints) with the 4 rows corresponding to (x, y, logit, prob)\\n    for each keypoint.\\n    '\n    offset_x = rois[:, 0]\n    offset_y = rois[:, 1]\n    widths = rois[:, 2] - rois[:, 0]\n    heights = rois[:, 3] - rois[:, 1]\n    widths = np.maximum(widths, 1)\n    heights = np.maximum(heights, 1)\n    widths_ceil = np.ceil(widths).astype(int)\n    heights_ceil = np.ceil(heights).astype(int)\n    num_keypoints = np.maximum(maps.shape[1], _NUM_KEYPOINTS)\n    maps = np.transpose(maps, [0, 2, 3, 1])\n    min_size = _INFERENCE_MIN_SIZE\n    xy_preds = np.zeros((len(rois), 4, num_keypoints), dtype=np.float32)\n    for i in range(len(rois)):\n        if min_size > 0:\n            roi_map_width = int(np.maximum(widths_ceil[i], min_size))\n            roi_map_height = int(np.maximum(heights_ceil[i], min_size))\n        else:\n            roi_map_width = widths_ceil[i]\n            roi_map_height = heights_ceil[i]\n        width_correction = widths[i] / roi_map_width\n        height_correction = heights[i] / roi_map_height\n        roi_map = cv2.resize(maps[i], (roi_map_width, roi_map_height), interpolation=cv2.INTER_CUBIC)\n        roi_map = np.transpose(roi_map, [2, 0, 1])\n        roi_map_probs = scores_to_probs(roi_map.copy())\n        w = roi_map.shape[2]\n        for k in range(num_keypoints):\n            pos = roi_map[k, :, :].argmax()\n            x_int = pos % w\n            y_int = (pos - x_int) // w\n            assert roi_map_probs[k, y_int, x_int] == roi_map_probs[k, :, :].max()\n            x = (x_int + 0.5) * width_correction\n            y = (y_int + 0.5) * height_correction\n            xy_preds[i, 0, k] = x + offset_x[i]\n            xy_preds[i, 1, k] = y + offset_y[i]\n            xy_preds[i, 2, k] = roi_map[k, y_int, x_int]\n            xy_preds[i, 3, k] = roi_map_probs[k, y_int, x_int]\n    return xy_preds",
            "def heatmaps_to_keypoints(maps, rois):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Extracts predicted keypoint locations from heatmaps. Output has shape\\n    (#rois, 4, #keypoints) with the 4 rows corresponding to (x, y, logit, prob)\\n    for each keypoint.\\n    '\n    offset_x = rois[:, 0]\n    offset_y = rois[:, 1]\n    widths = rois[:, 2] - rois[:, 0]\n    heights = rois[:, 3] - rois[:, 1]\n    widths = np.maximum(widths, 1)\n    heights = np.maximum(heights, 1)\n    widths_ceil = np.ceil(widths).astype(int)\n    heights_ceil = np.ceil(heights).astype(int)\n    num_keypoints = np.maximum(maps.shape[1], _NUM_KEYPOINTS)\n    maps = np.transpose(maps, [0, 2, 3, 1])\n    min_size = _INFERENCE_MIN_SIZE\n    xy_preds = np.zeros((len(rois), 4, num_keypoints), dtype=np.float32)\n    for i in range(len(rois)):\n        if min_size > 0:\n            roi_map_width = int(np.maximum(widths_ceil[i], min_size))\n            roi_map_height = int(np.maximum(heights_ceil[i], min_size))\n        else:\n            roi_map_width = widths_ceil[i]\n            roi_map_height = heights_ceil[i]\n        width_correction = widths[i] / roi_map_width\n        height_correction = heights[i] / roi_map_height\n        roi_map = cv2.resize(maps[i], (roi_map_width, roi_map_height), interpolation=cv2.INTER_CUBIC)\n        roi_map = np.transpose(roi_map, [2, 0, 1])\n        roi_map_probs = scores_to_probs(roi_map.copy())\n        w = roi_map.shape[2]\n        for k in range(num_keypoints):\n            pos = roi_map[k, :, :].argmax()\n            x_int = pos % w\n            y_int = (pos - x_int) // w\n            assert roi_map_probs[k, y_int, x_int] == roi_map_probs[k, :, :].max()\n            x = (x_int + 0.5) * width_correction\n            y = (y_int + 0.5) * height_correction\n            xy_preds[i, 0, k] = x + offset_x[i]\n            xy_preds[i, 1, k] = y + offset_y[i]\n            xy_preds[i, 2, k] = roi_map[k, y_int, x_int]\n            xy_preds[i, 3, k] = roi_map_probs[k, y_int, x_int]\n    return xy_preds",
            "def heatmaps_to_keypoints(maps, rois):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Extracts predicted keypoint locations from heatmaps. Output has shape\\n    (#rois, 4, #keypoints) with the 4 rows corresponding to (x, y, logit, prob)\\n    for each keypoint.\\n    '\n    offset_x = rois[:, 0]\n    offset_y = rois[:, 1]\n    widths = rois[:, 2] - rois[:, 0]\n    heights = rois[:, 3] - rois[:, 1]\n    widths = np.maximum(widths, 1)\n    heights = np.maximum(heights, 1)\n    widths_ceil = np.ceil(widths).astype(int)\n    heights_ceil = np.ceil(heights).astype(int)\n    num_keypoints = np.maximum(maps.shape[1], _NUM_KEYPOINTS)\n    maps = np.transpose(maps, [0, 2, 3, 1])\n    min_size = _INFERENCE_MIN_SIZE\n    xy_preds = np.zeros((len(rois), 4, num_keypoints), dtype=np.float32)\n    for i in range(len(rois)):\n        if min_size > 0:\n            roi_map_width = int(np.maximum(widths_ceil[i], min_size))\n            roi_map_height = int(np.maximum(heights_ceil[i], min_size))\n        else:\n            roi_map_width = widths_ceil[i]\n            roi_map_height = heights_ceil[i]\n        width_correction = widths[i] / roi_map_width\n        height_correction = heights[i] / roi_map_height\n        roi_map = cv2.resize(maps[i], (roi_map_width, roi_map_height), interpolation=cv2.INTER_CUBIC)\n        roi_map = np.transpose(roi_map, [2, 0, 1])\n        roi_map_probs = scores_to_probs(roi_map.copy())\n        w = roi_map.shape[2]\n        for k in range(num_keypoints):\n            pos = roi_map[k, :, :].argmax()\n            x_int = pos % w\n            y_int = (pos - x_int) // w\n            assert roi_map_probs[k, y_int, x_int] == roi_map_probs[k, :, :].max()\n            x = (x_int + 0.5) * width_correction\n            y = (y_int + 0.5) * height_correction\n            xy_preds[i, 0, k] = x + offset_x[i]\n            xy_preds[i, 1, k] = y + offset_y[i]\n            xy_preds[i, 2, k] = roi_map[k, y_int, x_int]\n            xy_preds[i, 3, k] = roi_map_probs[k, y_int, x_int]\n    return xy_preds",
            "def heatmaps_to_keypoints(maps, rois):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Extracts predicted keypoint locations from heatmaps. Output has shape\\n    (#rois, 4, #keypoints) with the 4 rows corresponding to (x, y, logit, prob)\\n    for each keypoint.\\n    '\n    offset_x = rois[:, 0]\n    offset_y = rois[:, 1]\n    widths = rois[:, 2] - rois[:, 0]\n    heights = rois[:, 3] - rois[:, 1]\n    widths = np.maximum(widths, 1)\n    heights = np.maximum(heights, 1)\n    widths_ceil = np.ceil(widths).astype(int)\n    heights_ceil = np.ceil(heights).astype(int)\n    num_keypoints = np.maximum(maps.shape[1], _NUM_KEYPOINTS)\n    maps = np.transpose(maps, [0, 2, 3, 1])\n    min_size = _INFERENCE_MIN_SIZE\n    xy_preds = np.zeros((len(rois), 4, num_keypoints), dtype=np.float32)\n    for i in range(len(rois)):\n        if min_size > 0:\n            roi_map_width = int(np.maximum(widths_ceil[i], min_size))\n            roi_map_height = int(np.maximum(heights_ceil[i], min_size))\n        else:\n            roi_map_width = widths_ceil[i]\n            roi_map_height = heights_ceil[i]\n        width_correction = widths[i] / roi_map_width\n        height_correction = heights[i] / roi_map_height\n        roi_map = cv2.resize(maps[i], (roi_map_width, roi_map_height), interpolation=cv2.INTER_CUBIC)\n        roi_map = np.transpose(roi_map, [2, 0, 1])\n        roi_map_probs = scores_to_probs(roi_map.copy())\n        w = roi_map.shape[2]\n        for k in range(num_keypoints):\n            pos = roi_map[k, :, :].argmax()\n            x_int = pos % w\n            y_int = (pos - x_int) // w\n            assert roi_map_probs[k, y_int, x_int] == roi_map_probs[k, :, :].max()\n            x = (x_int + 0.5) * width_correction\n            y = (y_int + 0.5) * height_correction\n            xy_preds[i, 0, k] = x + offset_x[i]\n            xy_preds[i, 1, k] = y + offset_y[i]\n            xy_preds[i, 2, k] = roi_map[k, y_int, x_int]\n            xy_preds[i, 3, k] = roi_map_probs[k, y_int, x_int]\n    return xy_preds",
            "def heatmaps_to_keypoints(maps, rois):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Extracts predicted keypoint locations from heatmaps. Output has shape\\n    (#rois, 4, #keypoints) with the 4 rows corresponding to (x, y, logit, prob)\\n    for each keypoint.\\n    '\n    offset_x = rois[:, 0]\n    offset_y = rois[:, 1]\n    widths = rois[:, 2] - rois[:, 0]\n    heights = rois[:, 3] - rois[:, 1]\n    widths = np.maximum(widths, 1)\n    heights = np.maximum(heights, 1)\n    widths_ceil = np.ceil(widths).astype(int)\n    heights_ceil = np.ceil(heights).astype(int)\n    num_keypoints = np.maximum(maps.shape[1], _NUM_KEYPOINTS)\n    maps = np.transpose(maps, [0, 2, 3, 1])\n    min_size = _INFERENCE_MIN_SIZE\n    xy_preds = np.zeros((len(rois), 4, num_keypoints), dtype=np.float32)\n    for i in range(len(rois)):\n        if min_size > 0:\n            roi_map_width = int(np.maximum(widths_ceil[i], min_size))\n            roi_map_height = int(np.maximum(heights_ceil[i], min_size))\n        else:\n            roi_map_width = widths_ceil[i]\n            roi_map_height = heights_ceil[i]\n        width_correction = widths[i] / roi_map_width\n        height_correction = heights[i] / roi_map_height\n        roi_map = cv2.resize(maps[i], (roi_map_width, roi_map_height), interpolation=cv2.INTER_CUBIC)\n        roi_map = np.transpose(roi_map, [2, 0, 1])\n        roi_map_probs = scores_to_probs(roi_map.copy())\n        w = roi_map.shape[2]\n        for k in range(num_keypoints):\n            pos = roi_map[k, :, :].argmax()\n            x_int = pos % w\n            y_int = (pos - x_int) // w\n            assert roi_map_probs[k, y_int, x_int] == roi_map_probs[k, :, :].max()\n            x = (x_int + 0.5) * width_correction\n            y = (y_int + 0.5) * height_correction\n            xy_preds[i, 0, k] = x + offset_x[i]\n            xy_preds[i, 1, k] = y + offset_y[i]\n            xy_preds[i, 2, k] = roi_map[k, y_int, x_int]\n            xy_preds[i, 3, k] = roi_map_probs[k, y_int, x_int]\n    return xy_preds"
        ]
    },
    {
        "func_name": "scores_to_probs",
        "original": "def scores_to_probs(scores):\n    \"\"\"Transforms CxHxW of scores to probabilities spatially.\"\"\"\n    channels = scores.shape[0]\n    for c in range(channels):\n        temp = scores[c, :, :]\n        max_score = temp.max()\n        temp = np.exp(temp - max_score) / np.sum(np.exp(temp - max_score))\n        scores[c, :, :] = temp\n    return scores",
        "mutated": [
            "def scores_to_probs(scores):\n    if False:\n        i = 10\n    'Transforms CxHxW of scores to probabilities spatially.'\n    channels = scores.shape[0]\n    for c in range(channels):\n        temp = scores[c, :, :]\n        max_score = temp.max()\n        temp = np.exp(temp - max_score) / np.sum(np.exp(temp - max_score))\n        scores[c, :, :] = temp\n    return scores",
            "def scores_to_probs(scores):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transforms CxHxW of scores to probabilities spatially.'\n    channels = scores.shape[0]\n    for c in range(channels):\n        temp = scores[c, :, :]\n        max_score = temp.max()\n        temp = np.exp(temp - max_score) / np.sum(np.exp(temp - max_score))\n        scores[c, :, :] = temp\n    return scores",
            "def scores_to_probs(scores):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transforms CxHxW of scores to probabilities spatially.'\n    channels = scores.shape[0]\n    for c in range(channels):\n        temp = scores[c, :, :]\n        max_score = temp.max()\n        temp = np.exp(temp - max_score) / np.sum(np.exp(temp - max_score))\n        scores[c, :, :] = temp\n    return scores",
            "def scores_to_probs(scores):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transforms CxHxW of scores to probabilities spatially.'\n    channels = scores.shape[0]\n    for c in range(channels):\n        temp = scores[c, :, :]\n        max_score = temp.max()\n        temp = np.exp(temp - max_score) / np.sum(np.exp(temp - max_score))\n        scores[c, :, :] = temp\n    return scores",
            "def scores_to_probs(scores):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transforms CxHxW of scores to probabilities spatially.'\n    channels = scores.shape[0]\n    for c in range(channels):\n        temp = scores[c, :, :]\n        max_score = temp.max()\n        temp = np.exp(temp - max_score) / np.sum(np.exp(temp - max_score))\n        scores[c, :, :] = temp\n    return scores"
        ]
    },
    {
        "func_name": "approx_heatmap_keypoint",
        "original": "def approx_heatmap_keypoint(heatmaps_in, bboxes_in):\n    \"\"\"\nMask R-CNN uses bicubic upscaling before taking the maximum of the heat map\nfor keypoints. We are using bilinear upscaling, which means we can approximate\nthe maximum coordinate with the low dimension maximum coordinates. We would like\nto avoid bicubic upscaling, because it is computationally expensive. Brown and\nLowe  (Invariant Features from Interest Point Groups, 2002) uses a method  for\nfitting a 3D quadratic function to the local sample points to determine the\ninterpolated location of the maximum of scale space, and his experiments showed\nthat this provides a substantial improvement to matching and stability for\nkeypoint extraction. This approach uses the Taylor expansion (up to the\nquadratic terms) of the scale-space function. It is equivalent with the Newton\nmethod. This efficient method were used in many keypoint estimation algorithms\nlike SIFT, SURF etc...\n\nThe implementation of Newton methods with numerical analysis is straight forward\nand super simple, though we need a linear solver.\n\n    \"\"\"\n    assert len(bboxes_in.shape) == 2\n    N = bboxes_in.shape[0]\n    assert bboxes_in.shape[1] == 4\n    assert len(heatmaps_in.shape) == 4\n    assert heatmaps_in.shape[0] == N\n    keypoint_count = heatmaps_in.shape[1]\n    heatmap_size = heatmaps_in.shape[2]\n    assert heatmap_size >= 2\n    assert heatmaps_in.shape[3] == heatmap_size\n    keypoints_out = np.zeros((N, keypoint_count, 4))\n    for k in range(N):\n        (x0, y0, x1, y1) = bboxes_in[k, :]\n        xLen = np.maximum(x1 - x0, 1)\n        yLen = np.maximum(y1 - y0, 1)\n        softmax_map = scores_to_probs(heatmaps_in[k, :, :, :].copy())\n        f = heatmaps_in[k]\n        for j in range(keypoint_count):\n            f = heatmaps_in[k][j]\n            maxX = -1\n            maxY = -1\n            maxScore = -100.0\n            maxProb = -100.0\n            for y in range(heatmap_size):\n                for x in range(heatmap_size):\n                    score = f[y, x]\n                    prob = softmax_map[j, y, x]\n                    if maxX < 0 or maxScore < score:\n                        maxScore = score\n                        maxProb = prob\n                        maxX = x\n                        maxY = y\n            fmax = [[0] * 3 for r in range(3)]\n            for x in range(3):\n                for y in range(3):\n                    hm_x = x + maxX - 1\n                    hm_y = y + maxY - 1\n                    hm_x = hm_x - 2 * (hm_x >= heatmap_size) + 2 * (hm_x < 0)\n                    hm_y = hm_y - 2 * (hm_y >= heatmap_size) + 2 * (hm_y < 0)\n                    assert hm_x < heatmap_size and hm_x >= 0\n                    assert hm_y < heatmap_size and hm_y >= 0\n                    fmax[y][x] = f[hm_y][hm_x]\n            b = [-(fmax[1][2] - fmax[1][0]) / 2, -(fmax[2][1] - fmax[0][1]) / 2]\n            A = [[fmax[1][0] - 2 * fmax[1][1] + fmax[1][2], (fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4], [(fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4, fmax[0][1] - 2 * fmax[1][1] + fmax[2][1]]]\n            div = A[1][1] * A[0][0] - A[0][1] * A[1][0]\n            if abs(div) < 0.0001:\n                deltaX = 0\n                deltaY = 0\n                deltaScore = maxScore\n            else:\n                deltaY = (b[1] * A[0][0] - b[0] * A[1][0]) / div\n                deltaX = (b[0] * A[1][1] - b[1] * A[0][1]) / div\n                if abs(deltaX) > 1.5 or abs(deltaY) > 1.5:\n                    scale = 1.5 / max(abs(deltaX), abs(deltaY))\n                    deltaX *= scale\n                    deltaY *= scale\n                deltaScore = fmax[1][1] - (b[0] * deltaX + b[1] * deltaY) + 0.5 * (deltaX * deltaX * A[0][0] + deltaX * deltaY * A[1][0] + deltaY * deltaX * A[0][1] + deltaY * deltaY * A[1][1])\n            assert abs(deltaX) <= 1.5\n            assert abs(deltaY) <= 1.5\n            keypoints_out[k, j, :] = (x0 + (maxX + deltaX + 0.5) * xLen / heatmap_size, y0 + (maxY + deltaY + 0.5) * yLen / heatmap_size, deltaScore, maxProb)\n    keypoints_out = np.transpose(keypoints_out, [0, 2, 1])\n    return keypoints_out",
        "mutated": [
            "def approx_heatmap_keypoint(heatmaps_in, bboxes_in):\n    if False:\n        i = 10\n    '\\nMask R-CNN uses bicubic upscaling before taking the maximum of the heat map\\nfor keypoints. We are using bilinear upscaling, which means we can approximate\\nthe maximum coordinate with the low dimension maximum coordinates. We would like\\nto avoid bicubic upscaling, because it is computationally expensive. Brown and\\nLowe  (Invariant Features from Interest Point Groups, 2002) uses a method  for\\nfitting a 3D quadratic function to the local sample points to determine the\\ninterpolated location of the maximum of scale space, and his experiments showed\\nthat this provides a substantial improvement to matching and stability for\\nkeypoint extraction. This approach uses the Taylor expansion (up to the\\nquadratic terms) of the scale-space function. It is equivalent with the Newton\\nmethod. This efficient method were used in many keypoint estimation algorithms\\nlike SIFT, SURF etc...\\n\\nThe implementation of Newton methods with numerical analysis is straight forward\\nand super simple, though we need a linear solver.\\n\\n    '\n    assert len(bboxes_in.shape) == 2\n    N = bboxes_in.shape[0]\n    assert bboxes_in.shape[1] == 4\n    assert len(heatmaps_in.shape) == 4\n    assert heatmaps_in.shape[0] == N\n    keypoint_count = heatmaps_in.shape[1]\n    heatmap_size = heatmaps_in.shape[2]\n    assert heatmap_size >= 2\n    assert heatmaps_in.shape[3] == heatmap_size\n    keypoints_out = np.zeros((N, keypoint_count, 4))\n    for k in range(N):\n        (x0, y0, x1, y1) = bboxes_in[k, :]\n        xLen = np.maximum(x1 - x0, 1)\n        yLen = np.maximum(y1 - y0, 1)\n        softmax_map = scores_to_probs(heatmaps_in[k, :, :, :].copy())\n        f = heatmaps_in[k]\n        for j in range(keypoint_count):\n            f = heatmaps_in[k][j]\n            maxX = -1\n            maxY = -1\n            maxScore = -100.0\n            maxProb = -100.0\n            for y in range(heatmap_size):\n                for x in range(heatmap_size):\n                    score = f[y, x]\n                    prob = softmax_map[j, y, x]\n                    if maxX < 0 or maxScore < score:\n                        maxScore = score\n                        maxProb = prob\n                        maxX = x\n                        maxY = y\n            fmax = [[0] * 3 for r in range(3)]\n            for x in range(3):\n                for y in range(3):\n                    hm_x = x + maxX - 1\n                    hm_y = y + maxY - 1\n                    hm_x = hm_x - 2 * (hm_x >= heatmap_size) + 2 * (hm_x < 0)\n                    hm_y = hm_y - 2 * (hm_y >= heatmap_size) + 2 * (hm_y < 0)\n                    assert hm_x < heatmap_size and hm_x >= 0\n                    assert hm_y < heatmap_size and hm_y >= 0\n                    fmax[y][x] = f[hm_y][hm_x]\n            b = [-(fmax[1][2] - fmax[1][0]) / 2, -(fmax[2][1] - fmax[0][1]) / 2]\n            A = [[fmax[1][0] - 2 * fmax[1][1] + fmax[1][2], (fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4], [(fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4, fmax[0][1] - 2 * fmax[1][1] + fmax[2][1]]]\n            div = A[1][1] * A[0][0] - A[0][1] * A[1][0]\n            if abs(div) < 0.0001:\n                deltaX = 0\n                deltaY = 0\n                deltaScore = maxScore\n            else:\n                deltaY = (b[1] * A[0][0] - b[0] * A[1][0]) / div\n                deltaX = (b[0] * A[1][1] - b[1] * A[0][1]) / div\n                if abs(deltaX) > 1.5 or abs(deltaY) > 1.5:\n                    scale = 1.5 / max(abs(deltaX), abs(deltaY))\n                    deltaX *= scale\n                    deltaY *= scale\n                deltaScore = fmax[1][1] - (b[0] * deltaX + b[1] * deltaY) + 0.5 * (deltaX * deltaX * A[0][0] + deltaX * deltaY * A[1][0] + deltaY * deltaX * A[0][1] + deltaY * deltaY * A[1][1])\n            assert abs(deltaX) <= 1.5\n            assert abs(deltaY) <= 1.5\n            keypoints_out[k, j, :] = (x0 + (maxX + deltaX + 0.5) * xLen / heatmap_size, y0 + (maxY + deltaY + 0.5) * yLen / heatmap_size, deltaScore, maxProb)\n    keypoints_out = np.transpose(keypoints_out, [0, 2, 1])\n    return keypoints_out",
            "def approx_heatmap_keypoint(heatmaps_in, bboxes_in):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\nMask R-CNN uses bicubic upscaling before taking the maximum of the heat map\\nfor keypoints. We are using bilinear upscaling, which means we can approximate\\nthe maximum coordinate with the low dimension maximum coordinates. We would like\\nto avoid bicubic upscaling, because it is computationally expensive. Brown and\\nLowe  (Invariant Features from Interest Point Groups, 2002) uses a method  for\\nfitting a 3D quadratic function to the local sample points to determine the\\ninterpolated location of the maximum of scale space, and his experiments showed\\nthat this provides a substantial improvement to matching and stability for\\nkeypoint extraction. This approach uses the Taylor expansion (up to the\\nquadratic terms) of the scale-space function. It is equivalent with the Newton\\nmethod. This efficient method were used in many keypoint estimation algorithms\\nlike SIFT, SURF etc...\\n\\nThe implementation of Newton methods with numerical analysis is straight forward\\nand super simple, though we need a linear solver.\\n\\n    '\n    assert len(bboxes_in.shape) == 2\n    N = bboxes_in.shape[0]\n    assert bboxes_in.shape[1] == 4\n    assert len(heatmaps_in.shape) == 4\n    assert heatmaps_in.shape[0] == N\n    keypoint_count = heatmaps_in.shape[1]\n    heatmap_size = heatmaps_in.shape[2]\n    assert heatmap_size >= 2\n    assert heatmaps_in.shape[3] == heatmap_size\n    keypoints_out = np.zeros((N, keypoint_count, 4))\n    for k in range(N):\n        (x0, y0, x1, y1) = bboxes_in[k, :]\n        xLen = np.maximum(x1 - x0, 1)\n        yLen = np.maximum(y1 - y0, 1)\n        softmax_map = scores_to_probs(heatmaps_in[k, :, :, :].copy())\n        f = heatmaps_in[k]\n        for j in range(keypoint_count):\n            f = heatmaps_in[k][j]\n            maxX = -1\n            maxY = -1\n            maxScore = -100.0\n            maxProb = -100.0\n            for y in range(heatmap_size):\n                for x in range(heatmap_size):\n                    score = f[y, x]\n                    prob = softmax_map[j, y, x]\n                    if maxX < 0 or maxScore < score:\n                        maxScore = score\n                        maxProb = prob\n                        maxX = x\n                        maxY = y\n            fmax = [[0] * 3 for r in range(3)]\n            for x in range(3):\n                for y in range(3):\n                    hm_x = x + maxX - 1\n                    hm_y = y + maxY - 1\n                    hm_x = hm_x - 2 * (hm_x >= heatmap_size) + 2 * (hm_x < 0)\n                    hm_y = hm_y - 2 * (hm_y >= heatmap_size) + 2 * (hm_y < 0)\n                    assert hm_x < heatmap_size and hm_x >= 0\n                    assert hm_y < heatmap_size and hm_y >= 0\n                    fmax[y][x] = f[hm_y][hm_x]\n            b = [-(fmax[1][2] - fmax[1][0]) / 2, -(fmax[2][1] - fmax[0][1]) / 2]\n            A = [[fmax[1][0] - 2 * fmax[1][1] + fmax[1][2], (fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4], [(fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4, fmax[0][1] - 2 * fmax[1][1] + fmax[2][1]]]\n            div = A[1][1] * A[0][0] - A[0][1] * A[1][0]\n            if abs(div) < 0.0001:\n                deltaX = 0\n                deltaY = 0\n                deltaScore = maxScore\n            else:\n                deltaY = (b[1] * A[0][0] - b[0] * A[1][0]) / div\n                deltaX = (b[0] * A[1][1] - b[1] * A[0][1]) / div\n                if abs(deltaX) > 1.5 or abs(deltaY) > 1.5:\n                    scale = 1.5 / max(abs(deltaX), abs(deltaY))\n                    deltaX *= scale\n                    deltaY *= scale\n                deltaScore = fmax[1][1] - (b[0] * deltaX + b[1] * deltaY) + 0.5 * (deltaX * deltaX * A[0][0] + deltaX * deltaY * A[1][0] + deltaY * deltaX * A[0][1] + deltaY * deltaY * A[1][1])\n            assert abs(deltaX) <= 1.5\n            assert abs(deltaY) <= 1.5\n            keypoints_out[k, j, :] = (x0 + (maxX + deltaX + 0.5) * xLen / heatmap_size, y0 + (maxY + deltaY + 0.5) * yLen / heatmap_size, deltaScore, maxProb)\n    keypoints_out = np.transpose(keypoints_out, [0, 2, 1])\n    return keypoints_out",
            "def approx_heatmap_keypoint(heatmaps_in, bboxes_in):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\nMask R-CNN uses bicubic upscaling before taking the maximum of the heat map\\nfor keypoints. We are using bilinear upscaling, which means we can approximate\\nthe maximum coordinate with the low dimension maximum coordinates. We would like\\nto avoid bicubic upscaling, because it is computationally expensive. Brown and\\nLowe  (Invariant Features from Interest Point Groups, 2002) uses a method  for\\nfitting a 3D quadratic function to the local sample points to determine the\\ninterpolated location of the maximum of scale space, and his experiments showed\\nthat this provides a substantial improvement to matching and stability for\\nkeypoint extraction. This approach uses the Taylor expansion (up to the\\nquadratic terms) of the scale-space function. It is equivalent with the Newton\\nmethod. This efficient method were used in many keypoint estimation algorithms\\nlike SIFT, SURF etc...\\n\\nThe implementation of Newton methods with numerical analysis is straight forward\\nand super simple, though we need a linear solver.\\n\\n    '\n    assert len(bboxes_in.shape) == 2\n    N = bboxes_in.shape[0]\n    assert bboxes_in.shape[1] == 4\n    assert len(heatmaps_in.shape) == 4\n    assert heatmaps_in.shape[0] == N\n    keypoint_count = heatmaps_in.shape[1]\n    heatmap_size = heatmaps_in.shape[2]\n    assert heatmap_size >= 2\n    assert heatmaps_in.shape[3] == heatmap_size\n    keypoints_out = np.zeros((N, keypoint_count, 4))\n    for k in range(N):\n        (x0, y0, x1, y1) = bboxes_in[k, :]\n        xLen = np.maximum(x1 - x0, 1)\n        yLen = np.maximum(y1 - y0, 1)\n        softmax_map = scores_to_probs(heatmaps_in[k, :, :, :].copy())\n        f = heatmaps_in[k]\n        for j in range(keypoint_count):\n            f = heatmaps_in[k][j]\n            maxX = -1\n            maxY = -1\n            maxScore = -100.0\n            maxProb = -100.0\n            for y in range(heatmap_size):\n                for x in range(heatmap_size):\n                    score = f[y, x]\n                    prob = softmax_map[j, y, x]\n                    if maxX < 0 or maxScore < score:\n                        maxScore = score\n                        maxProb = prob\n                        maxX = x\n                        maxY = y\n            fmax = [[0] * 3 for r in range(3)]\n            for x in range(3):\n                for y in range(3):\n                    hm_x = x + maxX - 1\n                    hm_y = y + maxY - 1\n                    hm_x = hm_x - 2 * (hm_x >= heatmap_size) + 2 * (hm_x < 0)\n                    hm_y = hm_y - 2 * (hm_y >= heatmap_size) + 2 * (hm_y < 0)\n                    assert hm_x < heatmap_size and hm_x >= 0\n                    assert hm_y < heatmap_size and hm_y >= 0\n                    fmax[y][x] = f[hm_y][hm_x]\n            b = [-(fmax[1][2] - fmax[1][0]) / 2, -(fmax[2][1] - fmax[0][1]) / 2]\n            A = [[fmax[1][0] - 2 * fmax[1][1] + fmax[1][2], (fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4], [(fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4, fmax[0][1] - 2 * fmax[1][1] + fmax[2][1]]]\n            div = A[1][1] * A[0][0] - A[0][1] * A[1][0]\n            if abs(div) < 0.0001:\n                deltaX = 0\n                deltaY = 0\n                deltaScore = maxScore\n            else:\n                deltaY = (b[1] * A[0][0] - b[0] * A[1][0]) / div\n                deltaX = (b[0] * A[1][1] - b[1] * A[0][1]) / div\n                if abs(deltaX) > 1.5 or abs(deltaY) > 1.5:\n                    scale = 1.5 / max(abs(deltaX), abs(deltaY))\n                    deltaX *= scale\n                    deltaY *= scale\n                deltaScore = fmax[1][1] - (b[0] * deltaX + b[1] * deltaY) + 0.5 * (deltaX * deltaX * A[0][0] + deltaX * deltaY * A[1][0] + deltaY * deltaX * A[0][1] + deltaY * deltaY * A[1][1])\n            assert abs(deltaX) <= 1.5\n            assert abs(deltaY) <= 1.5\n            keypoints_out[k, j, :] = (x0 + (maxX + deltaX + 0.5) * xLen / heatmap_size, y0 + (maxY + deltaY + 0.5) * yLen / heatmap_size, deltaScore, maxProb)\n    keypoints_out = np.transpose(keypoints_out, [0, 2, 1])\n    return keypoints_out",
            "def approx_heatmap_keypoint(heatmaps_in, bboxes_in):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\nMask R-CNN uses bicubic upscaling before taking the maximum of the heat map\\nfor keypoints. We are using bilinear upscaling, which means we can approximate\\nthe maximum coordinate with the low dimension maximum coordinates. We would like\\nto avoid bicubic upscaling, because it is computationally expensive. Brown and\\nLowe  (Invariant Features from Interest Point Groups, 2002) uses a method  for\\nfitting a 3D quadratic function to the local sample points to determine the\\ninterpolated location of the maximum of scale space, and his experiments showed\\nthat this provides a substantial improvement to matching and stability for\\nkeypoint extraction. This approach uses the Taylor expansion (up to the\\nquadratic terms) of the scale-space function. It is equivalent with the Newton\\nmethod. This efficient method were used in many keypoint estimation algorithms\\nlike SIFT, SURF etc...\\n\\nThe implementation of Newton methods with numerical analysis is straight forward\\nand super simple, though we need a linear solver.\\n\\n    '\n    assert len(bboxes_in.shape) == 2\n    N = bboxes_in.shape[0]\n    assert bboxes_in.shape[1] == 4\n    assert len(heatmaps_in.shape) == 4\n    assert heatmaps_in.shape[0] == N\n    keypoint_count = heatmaps_in.shape[1]\n    heatmap_size = heatmaps_in.shape[2]\n    assert heatmap_size >= 2\n    assert heatmaps_in.shape[3] == heatmap_size\n    keypoints_out = np.zeros((N, keypoint_count, 4))\n    for k in range(N):\n        (x0, y0, x1, y1) = bboxes_in[k, :]\n        xLen = np.maximum(x1 - x0, 1)\n        yLen = np.maximum(y1 - y0, 1)\n        softmax_map = scores_to_probs(heatmaps_in[k, :, :, :].copy())\n        f = heatmaps_in[k]\n        for j in range(keypoint_count):\n            f = heatmaps_in[k][j]\n            maxX = -1\n            maxY = -1\n            maxScore = -100.0\n            maxProb = -100.0\n            for y in range(heatmap_size):\n                for x in range(heatmap_size):\n                    score = f[y, x]\n                    prob = softmax_map[j, y, x]\n                    if maxX < 0 or maxScore < score:\n                        maxScore = score\n                        maxProb = prob\n                        maxX = x\n                        maxY = y\n            fmax = [[0] * 3 for r in range(3)]\n            for x in range(3):\n                for y in range(3):\n                    hm_x = x + maxX - 1\n                    hm_y = y + maxY - 1\n                    hm_x = hm_x - 2 * (hm_x >= heatmap_size) + 2 * (hm_x < 0)\n                    hm_y = hm_y - 2 * (hm_y >= heatmap_size) + 2 * (hm_y < 0)\n                    assert hm_x < heatmap_size and hm_x >= 0\n                    assert hm_y < heatmap_size and hm_y >= 0\n                    fmax[y][x] = f[hm_y][hm_x]\n            b = [-(fmax[1][2] - fmax[1][0]) / 2, -(fmax[2][1] - fmax[0][1]) / 2]\n            A = [[fmax[1][0] - 2 * fmax[1][1] + fmax[1][2], (fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4], [(fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4, fmax[0][1] - 2 * fmax[1][1] + fmax[2][1]]]\n            div = A[1][1] * A[0][0] - A[0][1] * A[1][0]\n            if abs(div) < 0.0001:\n                deltaX = 0\n                deltaY = 0\n                deltaScore = maxScore\n            else:\n                deltaY = (b[1] * A[0][0] - b[0] * A[1][0]) / div\n                deltaX = (b[0] * A[1][1] - b[1] * A[0][1]) / div\n                if abs(deltaX) > 1.5 or abs(deltaY) > 1.5:\n                    scale = 1.5 / max(abs(deltaX), abs(deltaY))\n                    deltaX *= scale\n                    deltaY *= scale\n                deltaScore = fmax[1][1] - (b[0] * deltaX + b[1] * deltaY) + 0.5 * (deltaX * deltaX * A[0][0] + deltaX * deltaY * A[1][0] + deltaY * deltaX * A[0][1] + deltaY * deltaY * A[1][1])\n            assert abs(deltaX) <= 1.5\n            assert abs(deltaY) <= 1.5\n            keypoints_out[k, j, :] = (x0 + (maxX + deltaX + 0.5) * xLen / heatmap_size, y0 + (maxY + deltaY + 0.5) * yLen / heatmap_size, deltaScore, maxProb)\n    keypoints_out = np.transpose(keypoints_out, [0, 2, 1])\n    return keypoints_out",
            "def approx_heatmap_keypoint(heatmaps_in, bboxes_in):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\nMask R-CNN uses bicubic upscaling before taking the maximum of the heat map\\nfor keypoints. We are using bilinear upscaling, which means we can approximate\\nthe maximum coordinate with the low dimension maximum coordinates. We would like\\nto avoid bicubic upscaling, because it is computationally expensive. Brown and\\nLowe  (Invariant Features from Interest Point Groups, 2002) uses a method  for\\nfitting a 3D quadratic function to the local sample points to determine the\\ninterpolated location of the maximum of scale space, and his experiments showed\\nthat this provides a substantial improvement to matching and stability for\\nkeypoint extraction. This approach uses the Taylor expansion (up to the\\nquadratic terms) of the scale-space function. It is equivalent with the Newton\\nmethod. This efficient method were used in many keypoint estimation algorithms\\nlike SIFT, SURF etc...\\n\\nThe implementation of Newton methods with numerical analysis is straight forward\\nand super simple, though we need a linear solver.\\n\\n    '\n    assert len(bboxes_in.shape) == 2\n    N = bboxes_in.shape[0]\n    assert bboxes_in.shape[1] == 4\n    assert len(heatmaps_in.shape) == 4\n    assert heatmaps_in.shape[0] == N\n    keypoint_count = heatmaps_in.shape[1]\n    heatmap_size = heatmaps_in.shape[2]\n    assert heatmap_size >= 2\n    assert heatmaps_in.shape[3] == heatmap_size\n    keypoints_out = np.zeros((N, keypoint_count, 4))\n    for k in range(N):\n        (x0, y0, x1, y1) = bboxes_in[k, :]\n        xLen = np.maximum(x1 - x0, 1)\n        yLen = np.maximum(y1 - y0, 1)\n        softmax_map = scores_to_probs(heatmaps_in[k, :, :, :].copy())\n        f = heatmaps_in[k]\n        for j in range(keypoint_count):\n            f = heatmaps_in[k][j]\n            maxX = -1\n            maxY = -1\n            maxScore = -100.0\n            maxProb = -100.0\n            for y in range(heatmap_size):\n                for x in range(heatmap_size):\n                    score = f[y, x]\n                    prob = softmax_map[j, y, x]\n                    if maxX < 0 or maxScore < score:\n                        maxScore = score\n                        maxProb = prob\n                        maxX = x\n                        maxY = y\n            fmax = [[0] * 3 for r in range(3)]\n            for x in range(3):\n                for y in range(3):\n                    hm_x = x + maxX - 1\n                    hm_y = y + maxY - 1\n                    hm_x = hm_x - 2 * (hm_x >= heatmap_size) + 2 * (hm_x < 0)\n                    hm_y = hm_y - 2 * (hm_y >= heatmap_size) + 2 * (hm_y < 0)\n                    assert hm_x < heatmap_size and hm_x >= 0\n                    assert hm_y < heatmap_size and hm_y >= 0\n                    fmax[y][x] = f[hm_y][hm_x]\n            b = [-(fmax[1][2] - fmax[1][0]) / 2, -(fmax[2][1] - fmax[0][1]) / 2]\n            A = [[fmax[1][0] - 2 * fmax[1][1] + fmax[1][2], (fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4], [(fmax[2][2] - fmax[2][0] - fmax[0][2] + fmax[0][0]) / 4, fmax[0][1] - 2 * fmax[1][1] + fmax[2][1]]]\n            div = A[1][1] * A[0][0] - A[0][1] * A[1][0]\n            if abs(div) < 0.0001:\n                deltaX = 0\n                deltaY = 0\n                deltaScore = maxScore\n            else:\n                deltaY = (b[1] * A[0][0] - b[0] * A[1][0]) / div\n                deltaX = (b[0] * A[1][1] - b[1] * A[0][1]) / div\n                if abs(deltaX) > 1.5 or abs(deltaY) > 1.5:\n                    scale = 1.5 / max(abs(deltaX), abs(deltaY))\n                    deltaX *= scale\n                    deltaY *= scale\n                deltaScore = fmax[1][1] - (b[0] * deltaX + b[1] * deltaY) + 0.5 * (deltaX * deltaX * A[0][0] + deltaX * deltaY * A[1][0] + deltaY * deltaX * A[0][1] + deltaY * deltaY * A[1][1])\n            assert abs(deltaX) <= 1.5\n            assert abs(deltaY) <= 1.5\n            keypoints_out[k, j, :] = (x0 + (maxX + deltaX + 0.5) * xLen / heatmap_size, y0 + (maxY + deltaY + 0.5) * yLen / heatmap_size, deltaScore, maxProb)\n    keypoints_out = np.transpose(keypoints_out, [0, 2, 1])\n    return keypoints_out"
        ]
    }
]