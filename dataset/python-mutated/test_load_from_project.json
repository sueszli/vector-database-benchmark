[
    {
        "func_name": "airbyte_instance_fixture",
        "original": "@pytest.fixture(name='airbyte_instance', params=[True, False], scope='module')\ndef airbyte_instance_fixture(request) -> AirbyteResource:\n    if request.param:\n        return AirbyteResource(host='some_host', port='8000', poll_interval=0)\n    else:\n        return airbyte_resource(build_init_resource_context({'host': 'some_host', 'port': '8000', 'poll_interval': 0}))",
        "mutated": [
            "@pytest.fixture(name='airbyte_instance', params=[True, False], scope='module')\ndef airbyte_instance_fixture(request) -> AirbyteResource:\n    if False:\n        i = 10\n    if request.param:\n        return AirbyteResource(host='some_host', port='8000', poll_interval=0)\n    else:\n        return airbyte_resource(build_init_resource_context({'host': 'some_host', 'port': '8000', 'poll_interval': 0}))",
            "@pytest.fixture(name='airbyte_instance', params=[True, False], scope='module')\ndef airbyte_instance_fixture(request) -> AirbyteResource:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if request.param:\n        return AirbyteResource(host='some_host', port='8000', poll_interval=0)\n    else:\n        return airbyte_resource(build_init_resource_context({'host': 'some_host', 'port': '8000', 'poll_interval': 0}))",
            "@pytest.fixture(name='airbyte_instance', params=[True, False], scope='module')\ndef airbyte_instance_fixture(request) -> AirbyteResource:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if request.param:\n        return AirbyteResource(host='some_host', port='8000', poll_interval=0)\n    else:\n        return airbyte_resource(build_init_resource_context({'host': 'some_host', 'port': '8000', 'poll_interval': 0}))",
            "@pytest.fixture(name='airbyte_instance', params=[True, False], scope='module')\ndef airbyte_instance_fixture(request) -> AirbyteResource:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if request.param:\n        return AirbyteResource(host='some_host', port='8000', poll_interval=0)\n    else:\n        return airbyte_resource(build_init_resource_context({'host': 'some_host', 'port': '8000', 'poll_interval': 0}))",
            "@pytest.fixture(name='airbyte_instance', params=[True, False], scope='module')\ndef airbyte_instance_fixture(request) -> AirbyteResource:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if request.param:\n        return AirbyteResource(host='some_host', port='8000', poll_interval=0)\n    else:\n        return airbyte_resource(build_init_resource_context({'host': 'some_host', 'port': '8000', 'poll_interval': 0}))"
        ]
    },
    {
        "func_name": "test_load_from_project",
        "original": "@responses.activate\n@pytest.mark.parametrize('use_normalization_tables', [True, False])\n@pytest.mark.parametrize('connection_to_group_fn', [None, lambda x: f'{x[0]}_group'])\n@pytest.mark.parametrize('filter_connection', [None, 'filter_fn', 'dirs'])\n@pytest.mark.parametrize('connection_to_asset_key_fn', [None, lambda conn, name: AssetKey([f'{conn.name[0]}_{name}'])])\ndef test_load_from_project(use_normalization_tables, connection_to_group_fn, filter_connection, connection_to_asset_key_fn, airbyte_instance):\n    if connection_to_group_fn:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_to_group_fn=connection_to_group_fn, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    else:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    ab_assets = ab_cacheable_assets.build_definitions(ab_cacheable_assets.compute_cacheable_data())\n    if filter_connection == 'filter_fn':\n        assert len(ab_assets) == 0\n        return\n    tables = {'dagster_releases', 'dagster_tags', 'dagster_teams', 'dagster_array_test', 'dagster_unknown_test'} | ({'dagster_releases_assets', 'dagster_releases_author', 'dagster_tags_commit', 'dagster_releases_foo', 'dagster_array_test_author'} if use_normalization_tables else set())\n    if connection_to_asset_key_fn:\n        tables = {connection_to_asset_key_fn(AirbyteConnectionMetadata('Github <> snowflake-ben', '', use_normalization_tables, []), t).path[0] for t in tables}\n    assert ab_assets[0].keys == {AssetKey(t) for t in tables}\n    assert all([ab_assets[0].group_names_by_key.get(AssetKey(t)) == (connection_to_group_fn('GitHub <> snowflake-ben') if connection_to_group_fn else 'github_snowflake_ben') for t in tables])\n    assert len(ab_assets[0].op.output_defs) == len(tables)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/get', json=get_project_connection_json(), status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/sync', json={'job': {'id': 1}}, status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/jobs/get', json=get_project_job_json(), status=200)\n    res = materialize(with_resources(ab_assets, resource_defs={'airbyte': airbyte_resource.configured({'host': 'some_host', 'port': '8000', 'poll_interval': 0})}))\n    materializations = [event.event_specific_data.materialization for event in res.events_for_node('airbyte_sync_87b7f') if event.event_type_value == 'ASSET_MATERIALIZATION']\n    assert len(materializations) == len(tables)\n    assert {m.asset_key for m in materializations} == {AssetKey(t) for t in tables}",
        "mutated": [
            "@responses.activate\n@pytest.mark.parametrize('use_normalization_tables', [True, False])\n@pytest.mark.parametrize('connection_to_group_fn', [None, lambda x: f'{x[0]}_group'])\n@pytest.mark.parametrize('filter_connection', [None, 'filter_fn', 'dirs'])\n@pytest.mark.parametrize('connection_to_asset_key_fn', [None, lambda conn, name: AssetKey([f'{conn.name[0]}_{name}'])])\ndef test_load_from_project(use_normalization_tables, connection_to_group_fn, filter_connection, connection_to_asset_key_fn, airbyte_instance):\n    if False:\n        i = 10\n    if connection_to_group_fn:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_to_group_fn=connection_to_group_fn, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    else:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    ab_assets = ab_cacheable_assets.build_definitions(ab_cacheable_assets.compute_cacheable_data())\n    if filter_connection == 'filter_fn':\n        assert len(ab_assets) == 0\n        return\n    tables = {'dagster_releases', 'dagster_tags', 'dagster_teams', 'dagster_array_test', 'dagster_unknown_test'} | ({'dagster_releases_assets', 'dagster_releases_author', 'dagster_tags_commit', 'dagster_releases_foo', 'dagster_array_test_author'} if use_normalization_tables else set())\n    if connection_to_asset_key_fn:\n        tables = {connection_to_asset_key_fn(AirbyteConnectionMetadata('Github <> snowflake-ben', '', use_normalization_tables, []), t).path[0] for t in tables}\n    assert ab_assets[0].keys == {AssetKey(t) for t in tables}\n    assert all([ab_assets[0].group_names_by_key.get(AssetKey(t)) == (connection_to_group_fn('GitHub <> snowflake-ben') if connection_to_group_fn else 'github_snowflake_ben') for t in tables])\n    assert len(ab_assets[0].op.output_defs) == len(tables)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/get', json=get_project_connection_json(), status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/sync', json={'job': {'id': 1}}, status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/jobs/get', json=get_project_job_json(), status=200)\n    res = materialize(with_resources(ab_assets, resource_defs={'airbyte': airbyte_resource.configured({'host': 'some_host', 'port': '8000', 'poll_interval': 0})}))\n    materializations = [event.event_specific_data.materialization for event in res.events_for_node('airbyte_sync_87b7f') if event.event_type_value == 'ASSET_MATERIALIZATION']\n    assert len(materializations) == len(tables)\n    assert {m.asset_key for m in materializations} == {AssetKey(t) for t in tables}",
            "@responses.activate\n@pytest.mark.parametrize('use_normalization_tables', [True, False])\n@pytest.mark.parametrize('connection_to_group_fn', [None, lambda x: f'{x[0]}_group'])\n@pytest.mark.parametrize('filter_connection', [None, 'filter_fn', 'dirs'])\n@pytest.mark.parametrize('connection_to_asset_key_fn', [None, lambda conn, name: AssetKey([f'{conn.name[0]}_{name}'])])\ndef test_load_from_project(use_normalization_tables, connection_to_group_fn, filter_connection, connection_to_asset_key_fn, airbyte_instance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if connection_to_group_fn:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_to_group_fn=connection_to_group_fn, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    else:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    ab_assets = ab_cacheable_assets.build_definitions(ab_cacheable_assets.compute_cacheable_data())\n    if filter_connection == 'filter_fn':\n        assert len(ab_assets) == 0\n        return\n    tables = {'dagster_releases', 'dagster_tags', 'dagster_teams', 'dagster_array_test', 'dagster_unknown_test'} | ({'dagster_releases_assets', 'dagster_releases_author', 'dagster_tags_commit', 'dagster_releases_foo', 'dagster_array_test_author'} if use_normalization_tables else set())\n    if connection_to_asset_key_fn:\n        tables = {connection_to_asset_key_fn(AirbyteConnectionMetadata('Github <> snowflake-ben', '', use_normalization_tables, []), t).path[0] for t in tables}\n    assert ab_assets[0].keys == {AssetKey(t) for t in tables}\n    assert all([ab_assets[0].group_names_by_key.get(AssetKey(t)) == (connection_to_group_fn('GitHub <> snowflake-ben') if connection_to_group_fn else 'github_snowflake_ben') for t in tables])\n    assert len(ab_assets[0].op.output_defs) == len(tables)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/get', json=get_project_connection_json(), status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/sync', json={'job': {'id': 1}}, status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/jobs/get', json=get_project_job_json(), status=200)\n    res = materialize(with_resources(ab_assets, resource_defs={'airbyte': airbyte_resource.configured({'host': 'some_host', 'port': '8000', 'poll_interval': 0})}))\n    materializations = [event.event_specific_data.materialization for event in res.events_for_node('airbyte_sync_87b7f') if event.event_type_value == 'ASSET_MATERIALIZATION']\n    assert len(materializations) == len(tables)\n    assert {m.asset_key for m in materializations} == {AssetKey(t) for t in tables}",
            "@responses.activate\n@pytest.mark.parametrize('use_normalization_tables', [True, False])\n@pytest.mark.parametrize('connection_to_group_fn', [None, lambda x: f'{x[0]}_group'])\n@pytest.mark.parametrize('filter_connection', [None, 'filter_fn', 'dirs'])\n@pytest.mark.parametrize('connection_to_asset_key_fn', [None, lambda conn, name: AssetKey([f'{conn.name[0]}_{name}'])])\ndef test_load_from_project(use_normalization_tables, connection_to_group_fn, filter_connection, connection_to_asset_key_fn, airbyte_instance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if connection_to_group_fn:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_to_group_fn=connection_to_group_fn, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    else:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    ab_assets = ab_cacheable_assets.build_definitions(ab_cacheable_assets.compute_cacheable_data())\n    if filter_connection == 'filter_fn':\n        assert len(ab_assets) == 0\n        return\n    tables = {'dagster_releases', 'dagster_tags', 'dagster_teams', 'dagster_array_test', 'dagster_unknown_test'} | ({'dagster_releases_assets', 'dagster_releases_author', 'dagster_tags_commit', 'dagster_releases_foo', 'dagster_array_test_author'} if use_normalization_tables else set())\n    if connection_to_asset_key_fn:\n        tables = {connection_to_asset_key_fn(AirbyteConnectionMetadata('Github <> snowflake-ben', '', use_normalization_tables, []), t).path[0] for t in tables}\n    assert ab_assets[0].keys == {AssetKey(t) for t in tables}\n    assert all([ab_assets[0].group_names_by_key.get(AssetKey(t)) == (connection_to_group_fn('GitHub <> snowflake-ben') if connection_to_group_fn else 'github_snowflake_ben') for t in tables])\n    assert len(ab_assets[0].op.output_defs) == len(tables)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/get', json=get_project_connection_json(), status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/sync', json={'job': {'id': 1}}, status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/jobs/get', json=get_project_job_json(), status=200)\n    res = materialize(with_resources(ab_assets, resource_defs={'airbyte': airbyte_resource.configured({'host': 'some_host', 'port': '8000', 'poll_interval': 0})}))\n    materializations = [event.event_specific_data.materialization for event in res.events_for_node('airbyte_sync_87b7f') if event.event_type_value == 'ASSET_MATERIALIZATION']\n    assert len(materializations) == len(tables)\n    assert {m.asset_key for m in materializations} == {AssetKey(t) for t in tables}",
            "@responses.activate\n@pytest.mark.parametrize('use_normalization_tables', [True, False])\n@pytest.mark.parametrize('connection_to_group_fn', [None, lambda x: f'{x[0]}_group'])\n@pytest.mark.parametrize('filter_connection', [None, 'filter_fn', 'dirs'])\n@pytest.mark.parametrize('connection_to_asset_key_fn', [None, lambda conn, name: AssetKey([f'{conn.name[0]}_{name}'])])\ndef test_load_from_project(use_normalization_tables, connection_to_group_fn, filter_connection, connection_to_asset_key_fn, airbyte_instance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if connection_to_group_fn:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_to_group_fn=connection_to_group_fn, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    else:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    ab_assets = ab_cacheable_assets.build_definitions(ab_cacheable_assets.compute_cacheable_data())\n    if filter_connection == 'filter_fn':\n        assert len(ab_assets) == 0\n        return\n    tables = {'dagster_releases', 'dagster_tags', 'dagster_teams', 'dagster_array_test', 'dagster_unknown_test'} | ({'dagster_releases_assets', 'dagster_releases_author', 'dagster_tags_commit', 'dagster_releases_foo', 'dagster_array_test_author'} if use_normalization_tables else set())\n    if connection_to_asset_key_fn:\n        tables = {connection_to_asset_key_fn(AirbyteConnectionMetadata('Github <> snowflake-ben', '', use_normalization_tables, []), t).path[0] for t in tables}\n    assert ab_assets[0].keys == {AssetKey(t) for t in tables}\n    assert all([ab_assets[0].group_names_by_key.get(AssetKey(t)) == (connection_to_group_fn('GitHub <> snowflake-ben') if connection_to_group_fn else 'github_snowflake_ben') for t in tables])\n    assert len(ab_assets[0].op.output_defs) == len(tables)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/get', json=get_project_connection_json(), status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/sync', json={'job': {'id': 1}}, status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/jobs/get', json=get_project_job_json(), status=200)\n    res = materialize(with_resources(ab_assets, resource_defs={'airbyte': airbyte_resource.configured({'host': 'some_host', 'port': '8000', 'poll_interval': 0})}))\n    materializations = [event.event_specific_data.materialization for event in res.events_for_node('airbyte_sync_87b7f') if event.event_type_value == 'ASSET_MATERIALIZATION']\n    assert len(materializations) == len(tables)\n    assert {m.asset_key for m in materializations} == {AssetKey(t) for t in tables}",
            "@responses.activate\n@pytest.mark.parametrize('use_normalization_tables', [True, False])\n@pytest.mark.parametrize('connection_to_group_fn', [None, lambda x: f'{x[0]}_group'])\n@pytest.mark.parametrize('filter_connection', [None, 'filter_fn', 'dirs'])\n@pytest.mark.parametrize('connection_to_asset_key_fn', [None, lambda conn, name: AssetKey([f'{conn.name[0]}_{name}'])])\ndef test_load_from_project(use_normalization_tables, connection_to_group_fn, filter_connection, connection_to_asset_key_fn, airbyte_instance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if connection_to_group_fn:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_to_group_fn=connection_to_group_fn, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    else:\n        ab_cacheable_assets = load_assets_from_airbyte_project(file_relative_path(__file__, './test_airbyte_project'), create_assets_for_normalization_tables=use_normalization_tables, connection_filter=(lambda _: False) if filter_connection == 'filter_fn' else None, connection_directories=['github_snowflake_ben'] if filter_connection == 'dirs' else None, connection_to_asset_key_fn=connection_to_asset_key_fn)\n    ab_assets = ab_cacheable_assets.build_definitions(ab_cacheable_assets.compute_cacheable_data())\n    if filter_connection == 'filter_fn':\n        assert len(ab_assets) == 0\n        return\n    tables = {'dagster_releases', 'dagster_tags', 'dagster_teams', 'dagster_array_test', 'dagster_unknown_test'} | ({'dagster_releases_assets', 'dagster_releases_author', 'dagster_tags_commit', 'dagster_releases_foo', 'dagster_array_test_author'} if use_normalization_tables else set())\n    if connection_to_asset_key_fn:\n        tables = {connection_to_asset_key_fn(AirbyteConnectionMetadata('Github <> snowflake-ben', '', use_normalization_tables, []), t).path[0] for t in tables}\n    assert ab_assets[0].keys == {AssetKey(t) for t in tables}\n    assert all([ab_assets[0].group_names_by_key.get(AssetKey(t)) == (connection_to_group_fn('GitHub <> snowflake-ben') if connection_to_group_fn else 'github_snowflake_ben') for t in tables])\n    assert len(ab_assets[0].op.output_defs) == len(tables)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/get', json=get_project_connection_json(), status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/connections/sync', json={'job': {'id': 1}}, status=200)\n    responses.add(method=responses.POST, url=airbyte_instance.api_base_url + '/jobs/get', json=get_project_job_json(), status=200)\n    res = materialize(with_resources(ab_assets, resource_defs={'airbyte': airbyte_resource.configured({'host': 'some_host', 'port': '8000', 'poll_interval': 0})}))\n    materializations = [event.event_specific_data.materialization for event in res.events_for_node('airbyte_sync_87b7f') if event.event_type_value == 'ASSET_MATERIALIZATION']\n    assert len(materializations) == len(tables)\n    assert {m.asset_key for m in materializations} == {AssetKey(t) for t in tables}"
        ]
    }
]