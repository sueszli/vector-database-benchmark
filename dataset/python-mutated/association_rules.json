[
    {
        "func_name": "conviction_helper",
        "original": "def conviction_helper(sAC, sA, sC):\n    confidence = sAC / sA\n    conviction = np.empty(confidence.shape, dtype=float)\n    if not len(conviction.shape):\n        conviction = conviction[np.newaxis]\n        confidence = confidence[np.newaxis]\n        sAC = sAC[np.newaxis]\n        sA = sA[np.newaxis]\n        sC = sC[np.newaxis]\n    conviction[:] = np.inf\n    conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n    return conviction",
        "mutated": [
            "def conviction_helper(sAC, sA, sC):\n    if False:\n        i = 10\n    confidence = sAC / sA\n    conviction = np.empty(confidence.shape, dtype=float)\n    if not len(conviction.shape):\n        conviction = conviction[np.newaxis]\n        confidence = confidence[np.newaxis]\n        sAC = sAC[np.newaxis]\n        sA = sA[np.newaxis]\n        sC = sC[np.newaxis]\n    conviction[:] = np.inf\n    conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n    return conviction",
            "def conviction_helper(sAC, sA, sC):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    confidence = sAC / sA\n    conviction = np.empty(confidence.shape, dtype=float)\n    if not len(conviction.shape):\n        conviction = conviction[np.newaxis]\n        confidence = confidence[np.newaxis]\n        sAC = sAC[np.newaxis]\n        sA = sA[np.newaxis]\n        sC = sC[np.newaxis]\n    conviction[:] = np.inf\n    conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n    return conviction",
            "def conviction_helper(sAC, sA, sC):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    confidence = sAC / sA\n    conviction = np.empty(confidence.shape, dtype=float)\n    if not len(conviction.shape):\n        conviction = conviction[np.newaxis]\n        confidence = confidence[np.newaxis]\n        sAC = sAC[np.newaxis]\n        sA = sA[np.newaxis]\n        sC = sC[np.newaxis]\n    conviction[:] = np.inf\n    conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n    return conviction",
            "def conviction_helper(sAC, sA, sC):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    confidence = sAC / sA\n    conviction = np.empty(confidence.shape, dtype=float)\n    if not len(conviction.shape):\n        conviction = conviction[np.newaxis]\n        confidence = confidence[np.newaxis]\n        sAC = sAC[np.newaxis]\n        sA = sA[np.newaxis]\n        sC = sC[np.newaxis]\n    conviction[:] = np.inf\n    conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n    return conviction",
            "def conviction_helper(sAC, sA, sC):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    confidence = sAC / sA\n    conviction = np.empty(confidence.shape, dtype=float)\n    if not len(conviction.shape):\n        conviction = conviction[np.newaxis]\n        confidence = confidence[np.newaxis]\n        sAC = sAC[np.newaxis]\n        sA = sA[np.newaxis]\n        sC = sC[np.newaxis]\n    conviction[:] = np.inf\n    conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n    return conviction"
        ]
    },
    {
        "func_name": "zhangs_metric_helper",
        "original": "def zhangs_metric_helper(sAC, sA, sC):\n    denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n    numerator = metric_dict['leverage'](sAC, sA, sC)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n    return zhangs_metric",
        "mutated": [
            "def zhangs_metric_helper(sAC, sA, sC):\n    if False:\n        i = 10\n    denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n    numerator = metric_dict['leverage'](sAC, sA, sC)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n    return zhangs_metric",
            "def zhangs_metric_helper(sAC, sA, sC):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n    numerator = metric_dict['leverage'](sAC, sA, sC)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n    return zhangs_metric",
            "def zhangs_metric_helper(sAC, sA, sC):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n    numerator = metric_dict['leverage'](sAC, sA, sC)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n    return zhangs_metric",
            "def zhangs_metric_helper(sAC, sA, sC):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n    numerator = metric_dict['leverage'](sAC, sA, sC)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n    return zhangs_metric",
            "def zhangs_metric_helper(sAC, sA, sC):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n    numerator = metric_dict['leverage'](sAC, sA, sC)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n    return zhangs_metric"
        ]
    },
    {
        "func_name": "association_rules",
        "original": "def association_rules(df, metric='confidence', min_threshold=0.8, support_only=False):\n    \"\"\"Generates a DataFrame of association rules including the\n    metrics 'score', 'confidence', and 'lift'\n\n    Parameters\n    -----------\n    df : pandas DataFrame\n      pandas DataFrame of frequent itemsets\n      with columns ['support', 'itemsets']\n\n    metric : string (default: 'confidence')\n      Metric to evaluate if a rule is of interest.\n      **Automatically set to 'support' if `support_only=True`.**\n      Otherwise, supported metrics are 'support', 'confidence', 'lift',\n      'leverage', 'conviction' and 'zhangs_metric'\n      These metrics are computed as follows:\n\n      - support(A->C) = support(A+C) [aka 'support'], range: [0, 1]\n\n      - confidence(A->C) = support(A+C) / support(A), range: [0, 1]\n\n      - lift(A->C) = confidence(A->C) / support(C), range: [0, inf]\n\n      - leverage(A->C) = support(A->C) - support(A)*support(C),\n        range: [-1, 1]\n\n      - conviction = [1 - support(C)] / [1 - confidence(A->C)],\n        range: [0, inf]\n\n      - zhangs_metric(A->C) =\n        leverage(A->C) / max(support(A->C)*(1-support(A)), support(A)*(support(C)-support(A->C)))\n        range: [-1,1]\n\n\n    min_threshold : float (default: 0.8)\n      Minimal threshold for the evaluation metric,\n      via the `metric` parameter,\n      to decide whether a candidate rule is of interest.\n\n    support_only : bool (default: False)\n      Only computes the rule support and fills the other\n      metric columns with NaNs. This is useful if:\n\n      a) the input DataFrame is incomplete, e.g., does\n      not contain support values for all rule antecedents\n      and consequents\n\n      b) you simply want to speed up the computation because\n      you don't need the other metrics.\n\n    Returns\n    ----------\n    pandas DataFrame with columns \"antecedents\" and \"consequents\"\n      that store itemsets, plus the scoring metric columns:\n      \"antecedent support\", \"consequent support\",\n      \"support\", \"confidence\", \"lift\",\n      \"leverage\", \"conviction\"\n      of all rules for which\n      metric(rule) >= min_threshold.\n      Each entry in the \"antecedents\" and \"consequents\" columns are\n      of type `frozenset`, which is a Python built-in type that\n      behaves similarly to sets except that it is immutable\n      (For more info, see\n      https://docs.python.org/3.6/library/stdtypes.html#frozenset).\n\n    Examples\n    -----------\n    For usage examples, please see\n    https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\n\n    \"\"\"\n    if not df.shape[0]:\n        raise ValueError('The input DataFrame `df` containing the frequent itemsets is empty.')\n    if not all((col in df.columns for col in ['support', 'itemsets'])):\n        raise ValueError(\"Dataframe needs to contain the                         columns 'support' and 'itemsets'\")\n\n    def conviction_helper(sAC, sA, sC):\n        confidence = sAC / sA\n        conviction = np.empty(confidence.shape, dtype=float)\n        if not len(conviction.shape):\n            conviction = conviction[np.newaxis]\n            confidence = confidence[np.newaxis]\n            sAC = sAC[np.newaxis]\n            sA = sA[np.newaxis]\n            sC = sC[np.newaxis]\n        conviction[:] = np.inf\n        conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n        return conviction\n\n    def zhangs_metric_helper(sAC, sA, sC):\n        denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n        numerator = metric_dict['leverage'](sAC, sA, sC)\n        with np.errstate(divide='ignore', invalid='ignore'):\n            zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n        return zhangs_metric\n    metric_dict = {'antecedent support': lambda _, sA, __: sA, 'consequent support': lambda _, __, sC: sC, 'support': lambda sAC, _, __: sAC, 'confidence': lambda sAC, sA, _: sAC / sA, 'lift': lambda sAC, sA, sC: metric_dict['confidence'](sAC, sA, sC) / sC, 'leverage': lambda sAC, sA, sC: metric_dict['support'](sAC, sA, sC) - sA * sC, 'conviction': lambda sAC, sA, sC: conviction_helper(sAC, sA, sC), 'zhangs_metric': lambda sAC, sA, sC: zhangs_metric_helper(sAC, sA, sC)}\n    columns_ordered = ['antecedent support', 'consequent support', 'support', 'confidence', 'lift', 'leverage', 'conviction', 'zhangs_metric']\n    if support_only:\n        metric = 'support'\n    elif metric not in metric_dict.keys():\n        raise ValueError(\"Metric must be 'confidence' or 'lift', got '{}'\".format(metric))\n    keys = df['itemsets'].values\n    values = df['support'].values\n    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n    frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n    rule_antecedents = []\n    rule_consequents = []\n    rule_supports = []\n    for k in frequent_items_dict.keys():\n        sAC = frequent_items_dict[k]\n        for idx in range(len(k) - 1, 0, -1):\n            for c in combinations(k, r=idx):\n                antecedent = frozenset(c)\n                consequent = k.difference(antecedent)\n                if support_only:\n                    sA = None\n                    sC = None\n                else:\n                    try:\n                        sA = frequent_items_dict[antecedent]\n                        sC = frequent_items_dict[consequent]\n                    except KeyError as e:\n                        s = str(e) + 'You are likely getting this error because the DataFrame is missing  antecedent and/or consequent  information. You can try using the  `support_only=True` option'\n                        raise KeyError(s)\n                score = metric_dict[metric](sAC, sA, sC)\n                if score >= min_threshold:\n                    rule_antecedents.append(antecedent)\n                    rule_consequents.append(consequent)\n                    rule_supports.append([sAC, sA, sC])\n    if not rule_supports:\n        return pd.DataFrame(columns=['antecedents', 'consequents'] + columns_ordered)\n    else:\n        rule_supports = np.array(rule_supports).T.astype(float)\n        df_res = pd.DataFrame(data=list(zip(rule_antecedents, rule_consequents)), columns=['antecedents', 'consequents'])\n        if support_only:\n            sAC = rule_supports[0]\n            for m in columns_ordered:\n                df_res[m] = np.nan\n            df_res['support'] = sAC\n        else:\n            sAC = rule_supports[0]\n            sA = rule_supports[1]\n            sC = rule_supports[2]\n            for m in columns_ordered:\n                df_res[m] = metric_dict[m](sAC, sA, sC)\n        return df_res",
        "mutated": [
            "def association_rules(df, metric='confidence', min_threshold=0.8, support_only=False):\n    if False:\n        i = 10\n    'Generates a DataFrame of association rules including the\\n    metrics \\'score\\', \\'confidence\\', and \\'lift\\'\\n\\n    Parameters\\n    -----------\\n    df : pandas DataFrame\\n      pandas DataFrame of frequent itemsets\\n      with columns [\\'support\\', \\'itemsets\\']\\n\\n    metric : string (default: \\'confidence\\')\\n      Metric to evaluate if a rule is of interest.\\n      **Automatically set to \\'support\\' if `support_only=True`.**\\n      Otherwise, supported metrics are \\'support\\', \\'confidence\\', \\'lift\\',\\n      \\'leverage\\', \\'conviction\\' and \\'zhangs_metric\\'\\n      These metrics are computed as follows:\\n\\n      - support(A->C) = support(A+C) [aka \\'support\\'], range: [0, 1]\\n\\n      - confidence(A->C) = support(A+C) / support(A), range: [0, 1]\\n\\n      - lift(A->C) = confidence(A->C) / support(C), range: [0, inf]\\n\\n      - leverage(A->C) = support(A->C) - support(A)*support(C),\\n        range: [-1, 1]\\n\\n      - conviction = [1 - support(C)] / [1 - confidence(A->C)],\\n        range: [0, inf]\\n\\n      - zhangs_metric(A->C) =\\n        leverage(A->C) / max(support(A->C)*(1-support(A)), support(A)*(support(C)-support(A->C)))\\n        range: [-1,1]\\n\\n\\n    min_threshold : float (default: 0.8)\\n      Minimal threshold for the evaluation metric,\\n      via the `metric` parameter,\\n      to decide whether a candidate rule is of interest.\\n\\n    support_only : bool (default: False)\\n      Only computes the rule support and fills the other\\n      metric columns with NaNs. This is useful if:\\n\\n      a) the input DataFrame is incomplete, e.g., does\\n      not contain support values for all rule antecedents\\n      and consequents\\n\\n      b) you simply want to speed up the computation because\\n      you don\\'t need the other metrics.\\n\\n    Returns\\n    ----------\\n    pandas DataFrame with columns \"antecedents\" and \"consequents\"\\n      that store itemsets, plus the scoring metric columns:\\n      \"antecedent support\", \"consequent support\",\\n      \"support\", \"confidence\", \"lift\",\\n      \"leverage\", \"conviction\"\\n      of all rules for which\\n      metric(rule) >= min_threshold.\\n      Each entry in the \"antecedents\" and \"consequents\" columns are\\n      of type `frozenset`, which is a Python built-in type that\\n      behaves similarly to sets except that it is immutable\\n      (For more info, see\\n      https://docs.python.org/3.6/library/stdtypes.html#frozenset).\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\\n\\n    '\n    if not df.shape[0]:\n        raise ValueError('The input DataFrame `df` containing the frequent itemsets is empty.')\n    if not all((col in df.columns for col in ['support', 'itemsets'])):\n        raise ValueError(\"Dataframe needs to contain the                         columns 'support' and 'itemsets'\")\n\n    def conviction_helper(sAC, sA, sC):\n        confidence = sAC / sA\n        conviction = np.empty(confidence.shape, dtype=float)\n        if not len(conviction.shape):\n            conviction = conviction[np.newaxis]\n            confidence = confidence[np.newaxis]\n            sAC = sAC[np.newaxis]\n            sA = sA[np.newaxis]\n            sC = sC[np.newaxis]\n        conviction[:] = np.inf\n        conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n        return conviction\n\n    def zhangs_metric_helper(sAC, sA, sC):\n        denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n        numerator = metric_dict['leverage'](sAC, sA, sC)\n        with np.errstate(divide='ignore', invalid='ignore'):\n            zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n        return zhangs_metric\n    metric_dict = {'antecedent support': lambda _, sA, __: sA, 'consequent support': lambda _, __, sC: sC, 'support': lambda sAC, _, __: sAC, 'confidence': lambda sAC, sA, _: sAC / sA, 'lift': lambda sAC, sA, sC: metric_dict['confidence'](sAC, sA, sC) / sC, 'leverage': lambda sAC, sA, sC: metric_dict['support'](sAC, sA, sC) - sA * sC, 'conviction': lambda sAC, sA, sC: conviction_helper(sAC, sA, sC), 'zhangs_metric': lambda sAC, sA, sC: zhangs_metric_helper(sAC, sA, sC)}\n    columns_ordered = ['antecedent support', 'consequent support', 'support', 'confidence', 'lift', 'leverage', 'conviction', 'zhangs_metric']\n    if support_only:\n        metric = 'support'\n    elif metric not in metric_dict.keys():\n        raise ValueError(\"Metric must be 'confidence' or 'lift', got '{}'\".format(metric))\n    keys = df['itemsets'].values\n    values = df['support'].values\n    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n    frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n    rule_antecedents = []\n    rule_consequents = []\n    rule_supports = []\n    for k in frequent_items_dict.keys():\n        sAC = frequent_items_dict[k]\n        for idx in range(len(k) - 1, 0, -1):\n            for c in combinations(k, r=idx):\n                antecedent = frozenset(c)\n                consequent = k.difference(antecedent)\n                if support_only:\n                    sA = None\n                    sC = None\n                else:\n                    try:\n                        sA = frequent_items_dict[antecedent]\n                        sC = frequent_items_dict[consequent]\n                    except KeyError as e:\n                        s = str(e) + 'You are likely getting this error because the DataFrame is missing  antecedent and/or consequent  information. You can try using the  `support_only=True` option'\n                        raise KeyError(s)\n                score = metric_dict[metric](sAC, sA, sC)\n                if score >= min_threshold:\n                    rule_antecedents.append(antecedent)\n                    rule_consequents.append(consequent)\n                    rule_supports.append([sAC, sA, sC])\n    if not rule_supports:\n        return pd.DataFrame(columns=['antecedents', 'consequents'] + columns_ordered)\n    else:\n        rule_supports = np.array(rule_supports).T.astype(float)\n        df_res = pd.DataFrame(data=list(zip(rule_antecedents, rule_consequents)), columns=['antecedents', 'consequents'])\n        if support_only:\n            sAC = rule_supports[0]\n            for m in columns_ordered:\n                df_res[m] = np.nan\n            df_res['support'] = sAC\n        else:\n            sAC = rule_supports[0]\n            sA = rule_supports[1]\n            sC = rule_supports[2]\n            for m in columns_ordered:\n                df_res[m] = metric_dict[m](sAC, sA, sC)\n        return df_res",
            "def association_rules(df, metric='confidence', min_threshold=0.8, support_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generates a DataFrame of association rules including the\\n    metrics \\'score\\', \\'confidence\\', and \\'lift\\'\\n\\n    Parameters\\n    -----------\\n    df : pandas DataFrame\\n      pandas DataFrame of frequent itemsets\\n      with columns [\\'support\\', \\'itemsets\\']\\n\\n    metric : string (default: \\'confidence\\')\\n      Metric to evaluate if a rule is of interest.\\n      **Automatically set to \\'support\\' if `support_only=True`.**\\n      Otherwise, supported metrics are \\'support\\', \\'confidence\\', \\'lift\\',\\n      \\'leverage\\', \\'conviction\\' and \\'zhangs_metric\\'\\n      These metrics are computed as follows:\\n\\n      - support(A->C) = support(A+C) [aka \\'support\\'], range: [0, 1]\\n\\n      - confidence(A->C) = support(A+C) / support(A), range: [0, 1]\\n\\n      - lift(A->C) = confidence(A->C) / support(C), range: [0, inf]\\n\\n      - leverage(A->C) = support(A->C) - support(A)*support(C),\\n        range: [-1, 1]\\n\\n      - conviction = [1 - support(C)] / [1 - confidence(A->C)],\\n        range: [0, inf]\\n\\n      - zhangs_metric(A->C) =\\n        leverage(A->C) / max(support(A->C)*(1-support(A)), support(A)*(support(C)-support(A->C)))\\n        range: [-1,1]\\n\\n\\n    min_threshold : float (default: 0.8)\\n      Minimal threshold for the evaluation metric,\\n      via the `metric` parameter,\\n      to decide whether a candidate rule is of interest.\\n\\n    support_only : bool (default: False)\\n      Only computes the rule support and fills the other\\n      metric columns with NaNs. This is useful if:\\n\\n      a) the input DataFrame is incomplete, e.g., does\\n      not contain support values for all rule antecedents\\n      and consequents\\n\\n      b) you simply want to speed up the computation because\\n      you don\\'t need the other metrics.\\n\\n    Returns\\n    ----------\\n    pandas DataFrame with columns \"antecedents\" and \"consequents\"\\n      that store itemsets, plus the scoring metric columns:\\n      \"antecedent support\", \"consequent support\",\\n      \"support\", \"confidence\", \"lift\",\\n      \"leverage\", \"conviction\"\\n      of all rules for which\\n      metric(rule) >= min_threshold.\\n      Each entry in the \"antecedents\" and \"consequents\" columns are\\n      of type `frozenset`, which is a Python built-in type that\\n      behaves similarly to sets except that it is immutable\\n      (For more info, see\\n      https://docs.python.org/3.6/library/stdtypes.html#frozenset).\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\\n\\n    '\n    if not df.shape[0]:\n        raise ValueError('The input DataFrame `df` containing the frequent itemsets is empty.')\n    if not all((col in df.columns for col in ['support', 'itemsets'])):\n        raise ValueError(\"Dataframe needs to contain the                         columns 'support' and 'itemsets'\")\n\n    def conviction_helper(sAC, sA, sC):\n        confidence = sAC / sA\n        conviction = np.empty(confidence.shape, dtype=float)\n        if not len(conviction.shape):\n            conviction = conviction[np.newaxis]\n            confidence = confidence[np.newaxis]\n            sAC = sAC[np.newaxis]\n            sA = sA[np.newaxis]\n            sC = sC[np.newaxis]\n        conviction[:] = np.inf\n        conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n        return conviction\n\n    def zhangs_metric_helper(sAC, sA, sC):\n        denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n        numerator = metric_dict['leverage'](sAC, sA, sC)\n        with np.errstate(divide='ignore', invalid='ignore'):\n            zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n        return zhangs_metric\n    metric_dict = {'antecedent support': lambda _, sA, __: sA, 'consequent support': lambda _, __, sC: sC, 'support': lambda sAC, _, __: sAC, 'confidence': lambda sAC, sA, _: sAC / sA, 'lift': lambda sAC, sA, sC: metric_dict['confidence'](sAC, sA, sC) / sC, 'leverage': lambda sAC, sA, sC: metric_dict['support'](sAC, sA, sC) - sA * sC, 'conviction': lambda sAC, sA, sC: conviction_helper(sAC, sA, sC), 'zhangs_metric': lambda sAC, sA, sC: zhangs_metric_helper(sAC, sA, sC)}\n    columns_ordered = ['antecedent support', 'consequent support', 'support', 'confidence', 'lift', 'leverage', 'conviction', 'zhangs_metric']\n    if support_only:\n        metric = 'support'\n    elif metric not in metric_dict.keys():\n        raise ValueError(\"Metric must be 'confidence' or 'lift', got '{}'\".format(metric))\n    keys = df['itemsets'].values\n    values = df['support'].values\n    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n    frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n    rule_antecedents = []\n    rule_consequents = []\n    rule_supports = []\n    for k in frequent_items_dict.keys():\n        sAC = frequent_items_dict[k]\n        for idx in range(len(k) - 1, 0, -1):\n            for c in combinations(k, r=idx):\n                antecedent = frozenset(c)\n                consequent = k.difference(antecedent)\n                if support_only:\n                    sA = None\n                    sC = None\n                else:\n                    try:\n                        sA = frequent_items_dict[antecedent]\n                        sC = frequent_items_dict[consequent]\n                    except KeyError as e:\n                        s = str(e) + 'You are likely getting this error because the DataFrame is missing  antecedent and/or consequent  information. You can try using the  `support_only=True` option'\n                        raise KeyError(s)\n                score = metric_dict[metric](sAC, sA, sC)\n                if score >= min_threshold:\n                    rule_antecedents.append(antecedent)\n                    rule_consequents.append(consequent)\n                    rule_supports.append([sAC, sA, sC])\n    if not rule_supports:\n        return pd.DataFrame(columns=['antecedents', 'consequents'] + columns_ordered)\n    else:\n        rule_supports = np.array(rule_supports).T.astype(float)\n        df_res = pd.DataFrame(data=list(zip(rule_antecedents, rule_consequents)), columns=['antecedents', 'consequents'])\n        if support_only:\n            sAC = rule_supports[0]\n            for m in columns_ordered:\n                df_res[m] = np.nan\n            df_res['support'] = sAC\n        else:\n            sAC = rule_supports[0]\n            sA = rule_supports[1]\n            sC = rule_supports[2]\n            for m in columns_ordered:\n                df_res[m] = metric_dict[m](sAC, sA, sC)\n        return df_res",
            "def association_rules(df, metric='confidence', min_threshold=0.8, support_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generates a DataFrame of association rules including the\\n    metrics \\'score\\', \\'confidence\\', and \\'lift\\'\\n\\n    Parameters\\n    -----------\\n    df : pandas DataFrame\\n      pandas DataFrame of frequent itemsets\\n      with columns [\\'support\\', \\'itemsets\\']\\n\\n    metric : string (default: \\'confidence\\')\\n      Metric to evaluate if a rule is of interest.\\n      **Automatically set to \\'support\\' if `support_only=True`.**\\n      Otherwise, supported metrics are \\'support\\', \\'confidence\\', \\'lift\\',\\n      \\'leverage\\', \\'conviction\\' and \\'zhangs_metric\\'\\n      These metrics are computed as follows:\\n\\n      - support(A->C) = support(A+C) [aka \\'support\\'], range: [0, 1]\\n\\n      - confidence(A->C) = support(A+C) / support(A), range: [0, 1]\\n\\n      - lift(A->C) = confidence(A->C) / support(C), range: [0, inf]\\n\\n      - leverage(A->C) = support(A->C) - support(A)*support(C),\\n        range: [-1, 1]\\n\\n      - conviction = [1 - support(C)] / [1 - confidence(A->C)],\\n        range: [0, inf]\\n\\n      - zhangs_metric(A->C) =\\n        leverage(A->C) / max(support(A->C)*(1-support(A)), support(A)*(support(C)-support(A->C)))\\n        range: [-1,1]\\n\\n\\n    min_threshold : float (default: 0.8)\\n      Minimal threshold for the evaluation metric,\\n      via the `metric` parameter,\\n      to decide whether a candidate rule is of interest.\\n\\n    support_only : bool (default: False)\\n      Only computes the rule support and fills the other\\n      metric columns with NaNs. This is useful if:\\n\\n      a) the input DataFrame is incomplete, e.g., does\\n      not contain support values for all rule antecedents\\n      and consequents\\n\\n      b) you simply want to speed up the computation because\\n      you don\\'t need the other metrics.\\n\\n    Returns\\n    ----------\\n    pandas DataFrame with columns \"antecedents\" and \"consequents\"\\n      that store itemsets, plus the scoring metric columns:\\n      \"antecedent support\", \"consequent support\",\\n      \"support\", \"confidence\", \"lift\",\\n      \"leverage\", \"conviction\"\\n      of all rules for which\\n      metric(rule) >= min_threshold.\\n      Each entry in the \"antecedents\" and \"consequents\" columns are\\n      of type `frozenset`, which is a Python built-in type that\\n      behaves similarly to sets except that it is immutable\\n      (For more info, see\\n      https://docs.python.org/3.6/library/stdtypes.html#frozenset).\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\\n\\n    '\n    if not df.shape[0]:\n        raise ValueError('The input DataFrame `df` containing the frequent itemsets is empty.')\n    if not all((col in df.columns for col in ['support', 'itemsets'])):\n        raise ValueError(\"Dataframe needs to contain the                         columns 'support' and 'itemsets'\")\n\n    def conviction_helper(sAC, sA, sC):\n        confidence = sAC / sA\n        conviction = np.empty(confidence.shape, dtype=float)\n        if not len(conviction.shape):\n            conviction = conviction[np.newaxis]\n            confidence = confidence[np.newaxis]\n            sAC = sAC[np.newaxis]\n            sA = sA[np.newaxis]\n            sC = sC[np.newaxis]\n        conviction[:] = np.inf\n        conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n        return conviction\n\n    def zhangs_metric_helper(sAC, sA, sC):\n        denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n        numerator = metric_dict['leverage'](sAC, sA, sC)\n        with np.errstate(divide='ignore', invalid='ignore'):\n            zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n        return zhangs_metric\n    metric_dict = {'antecedent support': lambda _, sA, __: sA, 'consequent support': lambda _, __, sC: sC, 'support': lambda sAC, _, __: sAC, 'confidence': lambda sAC, sA, _: sAC / sA, 'lift': lambda sAC, sA, sC: metric_dict['confidence'](sAC, sA, sC) / sC, 'leverage': lambda sAC, sA, sC: metric_dict['support'](sAC, sA, sC) - sA * sC, 'conviction': lambda sAC, sA, sC: conviction_helper(sAC, sA, sC), 'zhangs_metric': lambda sAC, sA, sC: zhangs_metric_helper(sAC, sA, sC)}\n    columns_ordered = ['antecedent support', 'consequent support', 'support', 'confidence', 'lift', 'leverage', 'conviction', 'zhangs_metric']\n    if support_only:\n        metric = 'support'\n    elif metric not in metric_dict.keys():\n        raise ValueError(\"Metric must be 'confidence' or 'lift', got '{}'\".format(metric))\n    keys = df['itemsets'].values\n    values = df['support'].values\n    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n    frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n    rule_antecedents = []\n    rule_consequents = []\n    rule_supports = []\n    for k in frequent_items_dict.keys():\n        sAC = frequent_items_dict[k]\n        for idx in range(len(k) - 1, 0, -1):\n            for c in combinations(k, r=idx):\n                antecedent = frozenset(c)\n                consequent = k.difference(antecedent)\n                if support_only:\n                    sA = None\n                    sC = None\n                else:\n                    try:\n                        sA = frequent_items_dict[antecedent]\n                        sC = frequent_items_dict[consequent]\n                    except KeyError as e:\n                        s = str(e) + 'You are likely getting this error because the DataFrame is missing  antecedent and/or consequent  information. You can try using the  `support_only=True` option'\n                        raise KeyError(s)\n                score = metric_dict[metric](sAC, sA, sC)\n                if score >= min_threshold:\n                    rule_antecedents.append(antecedent)\n                    rule_consequents.append(consequent)\n                    rule_supports.append([sAC, sA, sC])\n    if not rule_supports:\n        return pd.DataFrame(columns=['antecedents', 'consequents'] + columns_ordered)\n    else:\n        rule_supports = np.array(rule_supports).T.astype(float)\n        df_res = pd.DataFrame(data=list(zip(rule_antecedents, rule_consequents)), columns=['antecedents', 'consequents'])\n        if support_only:\n            sAC = rule_supports[0]\n            for m in columns_ordered:\n                df_res[m] = np.nan\n            df_res['support'] = sAC\n        else:\n            sAC = rule_supports[0]\n            sA = rule_supports[1]\n            sC = rule_supports[2]\n            for m in columns_ordered:\n                df_res[m] = metric_dict[m](sAC, sA, sC)\n        return df_res",
            "def association_rules(df, metric='confidence', min_threshold=0.8, support_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generates a DataFrame of association rules including the\\n    metrics \\'score\\', \\'confidence\\', and \\'lift\\'\\n\\n    Parameters\\n    -----------\\n    df : pandas DataFrame\\n      pandas DataFrame of frequent itemsets\\n      with columns [\\'support\\', \\'itemsets\\']\\n\\n    metric : string (default: \\'confidence\\')\\n      Metric to evaluate if a rule is of interest.\\n      **Automatically set to \\'support\\' if `support_only=True`.**\\n      Otherwise, supported metrics are \\'support\\', \\'confidence\\', \\'lift\\',\\n      \\'leverage\\', \\'conviction\\' and \\'zhangs_metric\\'\\n      These metrics are computed as follows:\\n\\n      - support(A->C) = support(A+C) [aka \\'support\\'], range: [0, 1]\\n\\n      - confidence(A->C) = support(A+C) / support(A), range: [0, 1]\\n\\n      - lift(A->C) = confidence(A->C) / support(C), range: [0, inf]\\n\\n      - leverage(A->C) = support(A->C) - support(A)*support(C),\\n        range: [-1, 1]\\n\\n      - conviction = [1 - support(C)] / [1 - confidence(A->C)],\\n        range: [0, inf]\\n\\n      - zhangs_metric(A->C) =\\n        leverage(A->C) / max(support(A->C)*(1-support(A)), support(A)*(support(C)-support(A->C)))\\n        range: [-1,1]\\n\\n\\n    min_threshold : float (default: 0.8)\\n      Minimal threshold for the evaluation metric,\\n      via the `metric` parameter,\\n      to decide whether a candidate rule is of interest.\\n\\n    support_only : bool (default: False)\\n      Only computes the rule support and fills the other\\n      metric columns with NaNs. This is useful if:\\n\\n      a) the input DataFrame is incomplete, e.g., does\\n      not contain support values for all rule antecedents\\n      and consequents\\n\\n      b) you simply want to speed up the computation because\\n      you don\\'t need the other metrics.\\n\\n    Returns\\n    ----------\\n    pandas DataFrame with columns \"antecedents\" and \"consequents\"\\n      that store itemsets, plus the scoring metric columns:\\n      \"antecedent support\", \"consequent support\",\\n      \"support\", \"confidence\", \"lift\",\\n      \"leverage\", \"conviction\"\\n      of all rules for which\\n      metric(rule) >= min_threshold.\\n      Each entry in the \"antecedents\" and \"consequents\" columns are\\n      of type `frozenset`, which is a Python built-in type that\\n      behaves similarly to sets except that it is immutable\\n      (For more info, see\\n      https://docs.python.org/3.6/library/stdtypes.html#frozenset).\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\\n\\n    '\n    if not df.shape[0]:\n        raise ValueError('The input DataFrame `df` containing the frequent itemsets is empty.')\n    if not all((col in df.columns for col in ['support', 'itemsets'])):\n        raise ValueError(\"Dataframe needs to contain the                         columns 'support' and 'itemsets'\")\n\n    def conviction_helper(sAC, sA, sC):\n        confidence = sAC / sA\n        conviction = np.empty(confidence.shape, dtype=float)\n        if not len(conviction.shape):\n            conviction = conviction[np.newaxis]\n            confidence = confidence[np.newaxis]\n            sAC = sAC[np.newaxis]\n            sA = sA[np.newaxis]\n            sC = sC[np.newaxis]\n        conviction[:] = np.inf\n        conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n        return conviction\n\n    def zhangs_metric_helper(sAC, sA, sC):\n        denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n        numerator = metric_dict['leverage'](sAC, sA, sC)\n        with np.errstate(divide='ignore', invalid='ignore'):\n            zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n        return zhangs_metric\n    metric_dict = {'antecedent support': lambda _, sA, __: sA, 'consequent support': lambda _, __, sC: sC, 'support': lambda sAC, _, __: sAC, 'confidence': lambda sAC, sA, _: sAC / sA, 'lift': lambda sAC, sA, sC: metric_dict['confidence'](sAC, sA, sC) / sC, 'leverage': lambda sAC, sA, sC: metric_dict['support'](sAC, sA, sC) - sA * sC, 'conviction': lambda sAC, sA, sC: conviction_helper(sAC, sA, sC), 'zhangs_metric': lambda sAC, sA, sC: zhangs_metric_helper(sAC, sA, sC)}\n    columns_ordered = ['antecedent support', 'consequent support', 'support', 'confidence', 'lift', 'leverage', 'conviction', 'zhangs_metric']\n    if support_only:\n        metric = 'support'\n    elif metric not in metric_dict.keys():\n        raise ValueError(\"Metric must be 'confidence' or 'lift', got '{}'\".format(metric))\n    keys = df['itemsets'].values\n    values = df['support'].values\n    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n    frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n    rule_antecedents = []\n    rule_consequents = []\n    rule_supports = []\n    for k in frequent_items_dict.keys():\n        sAC = frequent_items_dict[k]\n        for idx in range(len(k) - 1, 0, -1):\n            for c in combinations(k, r=idx):\n                antecedent = frozenset(c)\n                consequent = k.difference(antecedent)\n                if support_only:\n                    sA = None\n                    sC = None\n                else:\n                    try:\n                        sA = frequent_items_dict[antecedent]\n                        sC = frequent_items_dict[consequent]\n                    except KeyError as e:\n                        s = str(e) + 'You are likely getting this error because the DataFrame is missing  antecedent and/or consequent  information. You can try using the  `support_only=True` option'\n                        raise KeyError(s)\n                score = metric_dict[metric](sAC, sA, sC)\n                if score >= min_threshold:\n                    rule_antecedents.append(antecedent)\n                    rule_consequents.append(consequent)\n                    rule_supports.append([sAC, sA, sC])\n    if not rule_supports:\n        return pd.DataFrame(columns=['antecedents', 'consequents'] + columns_ordered)\n    else:\n        rule_supports = np.array(rule_supports).T.astype(float)\n        df_res = pd.DataFrame(data=list(zip(rule_antecedents, rule_consequents)), columns=['antecedents', 'consequents'])\n        if support_only:\n            sAC = rule_supports[0]\n            for m in columns_ordered:\n                df_res[m] = np.nan\n            df_res['support'] = sAC\n        else:\n            sAC = rule_supports[0]\n            sA = rule_supports[1]\n            sC = rule_supports[2]\n            for m in columns_ordered:\n                df_res[m] = metric_dict[m](sAC, sA, sC)\n        return df_res",
            "def association_rules(df, metric='confidence', min_threshold=0.8, support_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generates a DataFrame of association rules including the\\n    metrics \\'score\\', \\'confidence\\', and \\'lift\\'\\n\\n    Parameters\\n    -----------\\n    df : pandas DataFrame\\n      pandas DataFrame of frequent itemsets\\n      with columns [\\'support\\', \\'itemsets\\']\\n\\n    metric : string (default: \\'confidence\\')\\n      Metric to evaluate if a rule is of interest.\\n      **Automatically set to \\'support\\' if `support_only=True`.**\\n      Otherwise, supported metrics are \\'support\\', \\'confidence\\', \\'lift\\',\\n      \\'leverage\\', \\'conviction\\' and \\'zhangs_metric\\'\\n      These metrics are computed as follows:\\n\\n      - support(A->C) = support(A+C) [aka \\'support\\'], range: [0, 1]\\n\\n      - confidence(A->C) = support(A+C) / support(A), range: [0, 1]\\n\\n      - lift(A->C) = confidence(A->C) / support(C), range: [0, inf]\\n\\n      - leverage(A->C) = support(A->C) - support(A)*support(C),\\n        range: [-1, 1]\\n\\n      - conviction = [1 - support(C)] / [1 - confidence(A->C)],\\n        range: [0, inf]\\n\\n      - zhangs_metric(A->C) =\\n        leverage(A->C) / max(support(A->C)*(1-support(A)), support(A)*(support(C)-support(A->C)))\\n        range: [-1,1]\\n\\n\\n    min_threshold : float (default: 0.8)\\n      Minimal threshold for the evaluation metric,\\n      via the `metric` parameter,\\n      to decide whether a candidate rule is of interest.\\n\\n    support_only : bool (default: False)\\n      Only computes the rule support and fills the other\\n      metric columns with NaNs. This is useful if:\\n\\n      a) the input DataFrame is incomplete, e.g., does\\n      not contain support values for all rule antecedents\\n      and consequents\\n\\n      b) you simply want to speed up the computation because\\n      you don\\'t need the other metrics.\\n\\n    Returns\\n    ----------\\n    pandas DataFrame with columns \"antecedents\" and \"consequents\"\\n      that store itemsets, plus the scoring metric columns:\\n      \"antecedent support\", \"consequent support\",\\n      \"support\", \"confidence\", \"lift\",\\n      \"leverage\", \"conviction\"\\n      of all rules for which\\n      metric(rule) >= min_threshold.\\n      Each entry in the \"antecedents\" and \"consequents\" columns are\\n      of type `frozenset`, which is a Python built-in type that\\n      behaves similarly to sets except that it is immutable\\n      (For more info, see\\n      https://docs.python.org/3.6/library/stdtypes.html#frozenset).\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\\n\\n    '\n    if not df.shape[0]:\n        raise ValueError('The input DataFrame `df` containing the frequent itemsets is empty.')\n    if not all((col in df.columns for col in ['support', 'itemsets'])):\n        raise ValueError(\"Dataframe needs to contain the                         columns 'support' and 'itemsets'\")\n\n    def conviction_helper(sAC, sA, sC):\n        confidence = sAC / sA\n        conviction = np.empty(confidence.shape, dtype=float)\n        if not len(conviction.shape):\n            conviction = conviction[np.newaxis]\n            confidence = confidence[np.newaxis]\n            sAC = sAC[np.newaxis]\n            sA = sA[np.newaxis]\n            sC = sC[np.newaxis]\n        conviction[:] = np.inf\n        conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (1.0 - confidence[confidence < 1.0])\n        return conviction\n\n    def zhangs_metric_helper(sAC, sA, sC):\n        denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n        numerator = metric_dict['leverage'](sAC, sA, sC)\n        with np.errstate(divide='ignore', invalid='ignore'):\n            zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n        return zhangs_metric\n    metric_dict = {'antecedent support': lambda _, sA, __: sA, 'consequent support': lambda _, __, sC: sC, 'support': lambda sAC, _, __: sAC, 'confidence': lambda sAC, sA, _: sAC / sA, 'lift': lambda sAC, sA, sC: metric_dict['confidence'](sAC, sA, sC) / sC, 'leverage': lambda sAC, sA, sC: metric_dict['support'](sAC, sA, sC) - sA * sC, 'conviction': lambda sAC, sA, sC: conviction_helper(sAC, sA, sC), 'zhangs_metric': lambda sAC, sA, sC: zhangs_metric_helper(sAC, sA, sC)}\n    columns_ordered = ['antecedent support', 'consequent support', 'support', 'confidence', 'lift', 'leverage', 'conviction', 'zhangs_metric']\n    if support_only:\n        metric = 'support'\n    elif metric not in metric_dict.keys():\n        raise ValueError(\"Metric must be 'confidence' or 'lift', got '{}'\".format(metric))\n    keys = df['itemsets'].values\n    values = df['support'].values\n    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n    frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n    rule_antecedents = []\n    rule_consequents = []\n    rule_supports = []\n    for k in frequent_items_dict.keys():\n        sAC = frequent_items_dict[k]\n        for idx in range(len(k) - 1, 0, -1):\n            for c in combinations(k, r=idx):\n                antecedent = frozenset(c)\n                consequent = k.difference(antecedent)\n                if support_only:\n                    sA = None\n                    sC = None\n                else:\n                    try:\n                        sA = frequent_items_dict[antecedent]\n                        sC = frequent_items_dict[consequent]\n                    except KeyError as e:\n                        s = str(e) + 'You are likely getting this error because the DataFrame is missing  antecedent and/or consequent  information. You can try using the  `support_only=True` option'\n                        raise KeyError(s)\n                score = metric_dict[metric](sAC, sA, sC)\n                if score >= min_threshold:\n                    rule_antecedents.append(antecedent)\n                    rule_consequents.append(consequent)\n                    rule_supports.append([sAC, sA, sC])\n    if not rule_supports:\n        return pd.DataFrame(columns=['antecedents', 'consequents'] + columns_ordered)\n    else:\n        rule_supports = np.array(rule_supports).T.astype(float)\n        df_res = pd.DataFrame(data=list(zip(rule_antecedents, rule_consequents)), columns=['antecedents', 'consequents'])\n        if support_only:\n            sAC = rule_supports[0]\n            for m in columns_ordered:\n                df_res[m] = np.nan\n            df_res['support'] = sAC\n        else:\n            sAC = rule_supports[0]\n            sA = rule_supports[1]\n            sC = rule_supports[2]\n            for m in columns_ordered:\n                df_res[m] = metric_dict[m](sAC, sA, sC)\n        return df_res"
        ]
    }
]