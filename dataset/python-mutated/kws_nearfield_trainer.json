[
    {
        "func_name": "__init__",
        "original": "def __init__(self, model: str, work_dir: str, cfg_file: Optional[str]=None, arg_parse_fn: Optional[Callable]=None, model_revision: Optional[str]=DEFAULT_MODEL_REVISION, **kwargs):\n    \"\"\"\n        Args:\n            model (str): model id in modelscope\n            work_dir (str): main directory for training and evaluating\n            cfg_file (str): config file for training and evaluating\n            kwargs:\n                seed (int): random seed\n        \"\"\"\n    if isinstance(model, str):\n        self.model_dir = self.get_or_download_model_dir(model, model_revision)\n        if cfg_file is None:\n            cfg_file = os.path.join(self.model_dir, ModelFile.CONFIGURATION)\n    else:\n        assert cfg_file is not None, 'Config file should not be None if model is not from pretrained!'\n        self.model_dir = os.path.dirname(cfg_file)\n    super().__init__(cfg_file, arg_parse_fn)\n    configs = Config.from_file(cfg_file)\n    self.launcher = 'pytorch'\n    self.dist_backend = configs.train.get('dist_backend', 'nccl')\n    set_random_seed(kwargs.get('seed', 666))\n    self.init_dist()\n    self.work_dir = work_dir\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        logger.info(f'Current working dir is {work_dir}')\n    token_file = os.path.join(self.model_dir, 'train/tokens.txt')\n    assert os.path.exists(token_file), f'{token_file} is missing'\n    self.token_table = read_token(token_file)\n    lexicon_file = os.path.join(self.model_dir, 'train/lexicon.txt')\n    assert os.path.exists(lexicon_file), f'{lexicon_file} is missing'\n    self.lexicon_table = read_lexicon(lexicon_file)\n    feature_transform_file = os.path.join(self.model_dir, 'train/feature_transform.txt.80dim-l2r2')\n    assert os.path.exists(feature_transform_file), f'{feature_transform_file} is missing'\n    configs.model['cmvn_file'] = feature_transform_file\n    self.configs = configs\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(configs.to_dict())\n            fout.write(data)",
        "mutated": [
            "def __init__(self, model: str, work_dir: str, cfg_file: Optional[str]=None, arg_parse_fn: Optional[Callable]=None, model_revision: Optional[str]=DEFAULT_MODEL_REVISION, **kwargs):\n    if False:\n        i = 10\n    '\\n        Args:\\n            model (str): model id in modelscope\\n            work_dir (str): main directory for training and evaluating\\n            cfg_file (str): config file for training and evaluating\\n            kwargs:\\n                seed (int): random seed\\n        '\n    if isinstance(model, str):\n        self.model_dir = self.get_or_download_model_dir(model, model_revision)\n        if cfg_file is None:\n            cfg_file = os.path.join(self.model_dir, ModelFile.CONFIGURATION)\n    else:\n        assert cfg_file is not None, 'Config file should not be None if model is not from pretrained!'\n        self.model_dir = os.path.dirname(cfg_file)\n    super().__init__(cfg_file, arg_parse_fn)\n    configs = Config.from_file(cfg_file)\n    self.launcher = 'pytorch'\n    self.dist_backend = configs.train.get('dist_backend', 'nccl')\n    set_random_seed(kwargs.get('seed', 666))\n    self.init_dist()\n    self.work_dir = work_dir\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        logger.info(f'Current working dir is {work_dir}')\n    token_file = os.path.join(self.model_dir, 'train/tokens.txt')\n    assert os.path.exists(token_file), f'{token_file} is missing'\n    self.token_table = read_token(token_file)\n    lexicon_file = os.path.join(self.model_dir, 'train/lexicon.txt')\n    assert os.path.exists(lexicon_file), f'{lexicon_file} is missing'\n    self.lexicon_table = read_lexicon(lexicon_file)\n    feature_transform_file = os.path.join(self.model_dir, 'train/feature_transform.txt.80dim-l2r2')\n    assert os.path.exists(feature_transform_file), f'{feature_transform_file} is missing'\n    configs.model['cmvn_file'] = feature_transform_file\n    self.configs = configs\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(configs.to_dict())\n            fout.write(data)",
            "def __init__(self, model: str, work_dir: str, cfg_file: Optional[str]=None, arg_parse_fn: Optional[Callable]=None, model_revision: Optional[str]=DEFAULT_MODEL_REVISION, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Args:\\n            model (str): model id in modelscope\\n            work_dir (str): main directory for training and evaluating\\n            cfg_file (str): config file for training and evaluating\\n            kwargs:\\n                seed (int): random seed\\n        '\n    if isinstance(model, str):\n        self.model_dir = self.get_or_download_model_dir(model, model_revision)\n        if cfg_file is None:\n            cfg_file = os.path.join(self.model_dir, ModelFile.CONFIGURATION)\n    else:\n        assert cfg_file is not None, 'Config file should not be None if model is not from pretrained!'\n        self.model_dir = os.path.dirname(cfg_file)\n    super().__init__(cfg_file, arg_parse_fn)\n    configs = Config.from_file(cfg_file)\n    self.launcher = 'pytorch'\n    self.dist_backend = configs.train.get('dist_backend', 'nccl')\n    set_random_seed(kwargs.get('seed', 666))\n    self.init_dist()\n    self.work_dir = work_dir\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        logger.info(f'Current working dir is {work_dir}')\n    token_file = os.path.join(self.model_dir, 'train/tokens.txt')\n    assert os.path.exists(token_file), f'{token_file} is missing'\n    self.token_table = read_token(token_file)\n    lexicon_file = os.path.join(self.model_dir, 'train/lexicon.txt')\n    assert os.path.exists(lexicon_file), f'{lexicon_file} is missing'\n    self.lexicon_table = read_lexicon(lexicon_file)\n    feature_transform_file = os.path.join(self.model_dir, 'train/feature_transform.txt.80dim-l2r2')\n    assert os.path.exists(feature_transform_file), f'{feature_transform_file} is missing'\n    configs.model['cmvn_file'] = feature_transform_file\n    self.configs = configs\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(configs.to_dict())\n            fout.write(data)",
            "def __init__(self, model: str, work_dir: str, cfg_file: Optional[str]=None, arg_parse_fn: Optional[Callable]=None, model_revision: Optional[str]=DEFAULT_MODEL_REVISION, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Args:\\n            model (str): model id in modelscope\\n            work_dir (str): main directory for training and evaluating\\n            cfg_file (str): config file for training and evaluating\\n            kwargs:\\n                seed (int): random seed\\n        '\n    if isinstance(model, str):\n        self.model_dir = self.get_or_download_model_dir(model, model_revision)\n        if cfg_file is None:\n            cfg_file = os.path.join(self.model_dir, ModelFile.CONFIGURATION)\n    else:\n        assert cfg_file is not None, 'Config file should not be None if model is not from pretrained!'\n        self.model_dir = os.path.dirname(cfg_file)\n    super().__init__(cfg_file, arg_parse_fn)\n    configs = Config.from_file(cfg_file)\n    self.launcher = 'pytorch'\n    self.dist_backend = configs.train.get('dist_backend', 'nccl')\n    set_random_seed(kwargs.get('seed', 666))\n    self.init_dist()\n    self.work_dir = work_dir\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        logger.info(f'Current working dir is {work_dir}')\n    token_file = os.path.join(self.model_dir, 'train/tokens.txt')\n    assert os.path.exists(token_file), f'{token_file} is missing'\n    self.token_table = read_token(token_file)\n    lexicon_file = os.path.join(self.model_dir, 'train/lexicon.txt')\n    assert os.path.exists(lexicon_file), f'{lexicon_file} is missing'\n    self.lexicon_table = read_lexicon(lexicon_file)\n    feature_transform_file = os.path.join(self.model_dir, 'train/feature_transform.txt.80dim-l2r2')\n    assert os.path.exists(feature_transform_file), f'{feature_transform_file} is missing'\n    configs.model['cmvn_file'] = feature_transform_file\n    self.configs = configs\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(configs.to_dict())\n            fout.write(data)",
            "def __init__(self, model: str, work_dir: str, cfg_file: Optional[str]=None, arg_parse_fn: Optional[Callable]=None, model_revision: Optional[str]=DEFAULT_MODEL_REVISION, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Args:\\n            model (str): model id in modelscope\\n            work_dir (str): main directory for training and evaluating\\n            cfg_file (str): config file for training and evaluating\\n            kwargs:\\n                seed (int): random seed\\n        '\n    if isinstance(model, str):\n        self.model_dir = self.get_or_download_model_dir(model, model_revision)\n        if cfg_file is None:\n            cfg_file = os.path.join(self.model_dir, ModelFile.CONFIGURATION)\n    else:\n        assert cfg_file is not None, 'Config file should not be None if model is not from pretrained!'\n        self.model_dir = os.path.dirname(cfg_file)\n    super().__init__(cfg_file, arg_parse_fn)\n    configs = Config.from_file(cfg_file)\n    self.launcher = 'pytorch'\n    self.dist_backend = configs.train.get('dist_backend', 'nccl')\n    set_random_seed(kwargs.get('seed', 666))\n    self.init_dist()\n    self.work_dir = work_dir\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        logger.info(f'Current working dir is {work_dir}')\n    token_file = os.path.join(self.model_dir, 'train/tokens.txt')\n    assert os.path.exists(token_file), f'{token_file} is missing'\n    self.token_table = read_token(token_file)\n    lexicon_file = os.path.join(self.model_dir, 'train/lexicon.txt')\n    assert os.path.exists(lexicon_file), f'{lexicon_file} is missing'\n    self.lexicon_table = read_lexicon(lexicon_file)\n    feature_transform_file = os.path.join(self.model_dir, 'train/feature_transform.txt.80dim-l2r2')\n    assert os.path.exists(feature_transform_file), f'{feature_transform_file} is missing'\n    configs.model['cmvn_file'] = feature_transform_file\n    self.configs = configs\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(configs.to_dict())\n            fout.write(data)",
            "def __init__(self, model: str, work_dir: str, cfg_file: Optional[str]=None, arg_parse_fn: Optional[Callable]=None, model_revision: Optional[str]=DEFAULT_MODEL_REVISION, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Args:\\n            model (str): model id in modelscope\\n            work_dir (str): main directory for training and evaluating\\n            cfg_file (str): config file for training and evaluating\\n            kwargs:\\n                seed (int): random seed\\n        '\n    if isinstance(model, str):\n        self.model_dir = self.get_or_download_model_dir(model, model_revision)\n        if cfg_file is None:\n            cfg_file = os.path.join(self.model_dir, ModelFile.CONFIGURATION)\n    else:\n        assert cfg_file is not None, 'Config file should not be None if model is not from pretrained!'\n        self.model_dir = os.path.dirname(cfg_file)\n    super().__init__(cfg_file, arg_parse_fn)\n    configs = Config.from_file(cfg_file)\n    self.launcher = 'pytorch'\n    self.dist_backend = configs.train.get('dist_backend', 'nccl')\n    set_random_seed(kwargs.get('seed', 666))\n    self.init_dist()\n    self.work_dir = work_dir\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        logger.info(f'Current working dir is {work_dir}')\n    token_file = os.path.join(self.model_dir, 'train/tokens.txt')\n    assert os.path.exists(token_file), f'{token_file} is missing'\n    self.token_table = read_token(token_file)\n    lexicon_file = os.path.join(self.model_dir, 'train/lexicon.txt')\n    assert os.path.exists(lexicon_file), f'{lexicon_file} is missing'\n    self.lexicon_table = read_lexicon(lexicon_file)\n    feature_transform_file = os.path.join(self.model_dir, 'train/feature_transform.txt.80dim-l2r2')\n    assert os.path.exists(feature_transform_file), f'{feature_transform_file} is missing'\n    configs.model['cmvn_file'] = feature_transform_file\n    self.configs = configs\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(configs.to_dict())\n            fout.write(data)"
        ]
    },
    {
        "func_name": "train",
        "original": "def train(self, *args, **kwargs):\n    \"\"\"\n        Args:\n            kwargs:\n                train_data (int): wave list with kaldi style for training\n                cv_data (int): wave list with kaldi style for cross validation\n                trans_data (str): transcription list with kaldi style, merge train and cv\n                checkpoint (str): basemodel checkpoint, if None, default to use base.pt in model path\n                tensorboard_dir (str): path to save tensorboard results,\n                                       create 'tensorboard_dir' in work_dir by default\n                need_dump (bool): wether to dump data with mapping tokens or not\n        \"\"\"\n    train_checkpoint = kwargs.get('checkpoint', None)\n    if train_checkpoint is not None and os.path.exists(train_checkpoint):\n        self.checkpoint = train_checkpoint\n    else:\n        self.checkpoint = os.path.join(self.model_dir, 'train/base.pt')\n    self.tensorboard_dir = kwargs.get('tensorboard_dir', 'tensorboard')\n    assert kwargs['train_data'], 'please config train data in dict kwargs'\n    assert kwargs['cv_data'], 'please config cv data in dict kwargs'\n    assert kwargs['trans_data'], 'please config transcription data in dict kwargs'\n    self.train_data = kwargs['train_data']\n    self.cv_data = kwargs['cv_data']\n    self.trans_data = kwargs['trans_data']\n    self.need_dump = kwargs.get('need_dump', False) and (True if self.rank == 0 else False)\n    train_conf = self.configs['preprocessor']\n    cv_conf = copy.deepcopy(train_conf)\n    cv_conf['speed_perturb'] = False\n    cv_conf['spec_aug'] = False\n    cv_conf['shuffle'] = False\n    dump_train_file = os.path.join(self.work_dir, 'dump_train.txt')\n    dump_cv_file = os.path.join(self.work_dir, 'dump_cv.txt')\n    self.train_dataset = kws_nearfield_dataset(self.train_data, self.trans_data, train_conf, self.token_table, self.lexicon_table, self.need_dump, dump_train_file, True)\n    self.cv_dataset = kws_nearfield_dataset(self.cv_data, self.trans_data, cv_conf, self.token_table, self.lexicon_table, self.need_dump, dump_cv_file, True)\n    self.train_dataloader = DataLoader(self.train_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.train.dataloader.workers_per_gpu, prefetch_factor=self.configs.train.dataloader.get('prefetch', 2))\n    self.cv_dataloader = DataLoader(self.cv_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    self.model = self.build_model(self.configs)\n    num_params = count_parameters(self.model)\n    if self.rank == 0:\n        logger.warning('the number of model params: {}'.format(num_params))\n    if self.checkpoint is not None and os.path.exists(self.checkpoint):\n        load_checkpoint(self.checkpoint, self.model)\n        info_path = re.sub('.pt$', '.yaml', self.checkpoint)\n        infos = {}\n        if os.path.exists(info_path):\n            with open(info_path, 'r') as fin:\n                infos = yaml.load(fin, Loader=yaml.FullLoader)\n    else:\n        logger.warning('Training with random initialized params')\n        infos = {}\n    self.start_epoch = infos.get('epoch', -1) + 1\n    self.configs['train']['start_epoch'] = self.start_epoch\n    lr_last_epoch = infos.get('lr', self.configs['train']['optimizer']['lr'])\n    self.configs['train']['optimizer']['lr'] = lr_last_epoch\n    device_name = kwargs.get('device', 'gpu')\n    if self.world_size > 1:\n        device_name = f'cuda:{self.local_rank}'\n    self.train_device = create_device(device_name)\n    if self.world_size > 1:\n        assert torch.cuda.is_available()\n        self.model = self.model.to(self.train_device)\n        self.model = torch.nn.parallel.DistributedDataParallel(self.model)\n    else:\n        self.model = self.model.to(self.train_device)\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(self.configs.to_dict())\n            fout.write(data)\n    logger.info('Start training...')\n    writer = None\n    if self.rank == 0:\n        os.makedirs(self.work_dir, exist_ok=True)\n        writer = SummaryWriter(os.path.join(self.work_dir, self.tensorboard_dir))\n    log_interval = self.configs['train'].get('log_interval', 10)\n    optim_conf = self.configs['train']['optimizer']\n    optimizer = optim.Adam(self.model.parameters(), lr=optim_conf['lr'], weight_decay=optim_conf['weight_decay'])\n    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-06, threshold=0.01)\n    final_epoch = None\n    if self.start_epoch == 0 and self.rank == 0:\n        save_model_path = os.path.join(self.work_dir, 'init.pt')\n        save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n    logger.info('Start training...')\n    training_config = {}\n    training_config['grad_clip'] = optim_conf['grad_clip']\n    training_config['grad_accum'] = optim_conf.get('grad_accum', 1)\n    training_config['log_interval'] = log_interval\n    training_config['world_size'] = self.world_size\n    training_config['rank'] = self.rank\n    training_config['local_rank'] = self.local_rank\n    max_epoch = self.configs['train']['max_epochs']\n    totaltime = datetime.datetime.now()\n    for epoch in range(self.start_epoch, max_epoch):\n        self.train_dataset.set_epoch(epoch)\n        training_config['epoch'] = epoch\n        lr = optimizer.param_groups[0]['lr']\n        logger.info('Epoch {} TRAIN info lr {}'.format(epoch, lr))\n        executor_train(self.model, optimizer, self.train_dataloader, self.train_device, writer, training_config)\n        (cv_loss, cv_acc) = executor_cv(self.model, self.cv_dataloader, self.train_device, training_config)\n        logger.info('Epoch {} EVAL info cv_loss {:.6f}, cv_acc {:.2f}'.format(epoch, cv_loss, cv_acc))\n        if self.rank == 0:\n            save_model_path = os.path.join(self.work_dir, '{}.pt'.format(epoch))\n            save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n            info_path = re.sub('.pt$', '.yaml', save_model_path)\n            info_dict = dict(epoch=epoch, lr=lr, cv_loss=cv_loss)\n            with open(info_path, 'w') as fout:\n                data = yaml.dump(info_dict)\n                fout.write(data)\n            writer.add_scalar('epoch/cv_loss', cv_loss, epoch)\n            writer.add_scalar('epoch/lr', lr, epoch)\n        final_epoch = epoch\n        lr_scheduler.step(cv_loss)\n    if final_epoch is not None and self.rank == 0:\n        writer.close()\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))",
        "mutated": [
            "def train(self, *args, **kwargs):\n    if False:\n        i = 10\n    \"\\n        Args:\\n            kwargs:\\n                train_data (int): wave list with kaldi style for training\\n                cv_data (int): wave list with kaldi style for cross validation\\n                trans_data (str): transcription list with kaldi style, merge train and cv\\n                checkpoint (str): basemodel checkpoint, if None, default to use base.pt in model path\\n                tensorboard_dir (str): path to save tensorboard results,\\n                                       create 'tensorboard_dir' in work_dir by default\\n                need_dump (bool): wether to dump data with mapping tokens or not\\n        \"\n    train_checkpoint = kwargs.get('checkpoint', None)\n    if train_checkpoint is not None and os.path.exists(train_checkpoint):\n        self.checkpoint = train_checkpoint\n    else:\n        self.checkpoint = os.path.join(self.model_dir, 'train/base.pt')\n    self.tensorboard_dir = kwargs.get('tensorboard_dir', 'tensorboard')\n    assert kwargs['train_data'], 'please config train data in dict kwargs'\n    assert kwargs['cv_data'], 'please config cv data in dict kwargs'\n    assert kwargs['trans_data'], 'please config transcription data in dict kwargs'\n    self.train_data = kwargs['train_data']\n    self.cv_data = kwargs['cv_data']\n    self.trans_data = kwargs['trans_data']\n    self.need_dump = kwargs.get('need_dump', False) and (True if self.rank == 0 else False)\n    train_conf = self.configs['preprocessor']\n    cv_conf = copy.deepcopy(train_conf)\n    cv_conf['speed_perturb'] = False\n    cv_conf['spec_aug'] = False\n    cv_conf['shuffle'] = False\n    dump_train_file = os.path.join(self.work_dir, 'dump_train.txt')\n    dump_cv_file = os.path.join(self.work_dir, 'dump_cv.txt')\n    self.train_dataset = kws_nearfield_dataset(self.train_data, self.trans_data, train_conf, self.token_table, self.lexicon_table, self.need_dump, dump_train_file, True)\n    self.cv_dataset = kws_nearfield_dataset(self.cv_data, self.trans_data, cv_conf, self.token_table, self.lexicon_table, self.need_dump, dump_cv_file, True)\n    self.train_dataloader = DataLoader(self.train_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.train.dataloader.workers_per_gpu, prefetch_factor=self.configs.train.dataloader.get('prefetch', 2))\n    self.cv_dataloader = DataLoader(self.cv_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    self.model = self.build_model(self.configs)\n    num_params = count_parameters(self.model)\n    if self.rank == 0:\n        logger.warning('the number of model params: {}'.format(num_params))\n    if self.checkpoint is not None and os.path.exists(self.checkpoint):\n        load_checkpoint(self.checkpoint, self.model)\n        info_path = re.sub('.pt$', '.yaml', self.checkpoint)\n        infos = {}\n        if os.path.exists(info_path):\n            with open(info_path, 'r') as fin:\n                infos = yaml.load(fin, Loader=yaml.FullLoader)\n    else:\n        logger.warning('Training with random initialized params')\n        infos = {}\n    self.start_epoch = infos.get('epoch', -1) + 1\n    self.configs['train']['start_epoch'] = self.start_epoch\n    lr_last_epoch = infos.get('lr', self.configs['train']['optimizer']['lr'])\n    self.configs['train']['optimizer']['lr'] = lr_last_epoch\n    device_name = kwargs.get('device', 'gpu')\n    if self.world_size > 1:\n        device_name = f'cuda:{self.local_rank}'\n    self.train_device = create_device(device_name)\n    if self.world_size > 1:\n        assert torch.cuda.is_available()\n        self.model = self.model.to(self.train_device)\n        self.model = torch.nn.parallel.DistributedDataParallel(self.model)\n    else:\n        self.model = self.model.to(self.train_device)\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(self.configs.to_dict())\n            fout.write(data)\n    logger.info('Start training...')\n    writer = None\n    if self.rank == 0:\n        os.makedirs(self.work_dir, exist_ok=True)\n        writer = SummaryWriter(os.path.join(self.work_dir, self.tensorboard_dir))\n    log_interval = self.configs['train'].get('log_interval', 10)\n    optim_conf = self.configs['train']['optimizer']\n    optimizer = optim.Adam(self.model.parameters(), lr=optim_conf['lr'], weight_decay=optim_conf['weight_decay'])\n    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-06, threshold=0.01)\n    final_epoch = None\n    if self.start_epoch == 0 and self.rank == 0:\n        save_model_path = os.path.join(self.work_dir, 'init.pt')\n        save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n    logger.info('Start training...')\n    training_config = {}\n    training_config['grad_clip'] = optim_conf['grad_clip']\n    training_config['grad_accum'] = optim_conf.get('grad_accum', 1)\n    training_config['log_interval'] = log_interval\n    training_config['world_size'] = self.world_size\n    training_config['rank'] = self.rank\n    training_config['local_rank'] = self.local_rank\n    max_epoch = self.configs['train']['max_epochs']\n    totaltime = datetime.datetime.now()\n    for epoch in range(self.start_epoch, max_epoch):\n        self.train_dataset.set_epoch(epoch)\n        training_config['epoch'] = epoch\n        lr = optimizer.param_groups[0]['lr']\n        logger.info('Epoch {} TRAIN info lr {}'.format(epoch, lr))\n        executor_train(self.model, optimizer, self.train_dataloader, self.train_device, writer, training_config)\n        (cv_loss, cv_acc) = executor_cv(self.model, self.cv_dataloader, self.train_device, training_config)\n        logger.info('Epoch {} EVAL info cv_loss {:.6f}, cv_acc {:.2f}'.format(epoch, cv_loss, cv_acc))\n        if self.rank == 0:\n            save_model_path = os.path.join(self.work_dir, '{}.pt'.format(epoch))\n            save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n            info_path = re.sub('.pt$', '.yaml', save_model_path)\n            info_dict = dict(epoch=epoch, lr=lr, cv_loss=cv_loss)\n            with open(info_path, 'w') as fout:\n                data = yaml.dump(info_dict)\n                fout.write(data)\n            writer.add_scalar('epoch/cv_loss', cv_loss, epoch)\n            writer.add_scalar('epoch/lr', lr, epoch)\n        final_epoch = epoch\n        lr_scheduler.step(cv_loss)\n    if final_epoch is not None and self.rank == 0:\n        writer.close()\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))",
            "def train(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Args:\\n            kwargs:\\n                train_data (int): wave list with kaldi style for training\\n                cv_data (int): wave list with kaldi style for cross validation\\n                trans_data (str): transcription list with kaldi style, merge train and cv\\n                checkpoint (str): basemodel checkpoint, if None, default to use base.pt in model path\\n                tensorboard_dir (str): path to save tensorboard results,\\n                                       create 'tensorboard_dir' in work_dir by default\\n                need_dump (bool): wether to dump data with mapping tokens or not\\n        \"\n    train_checkpoint = kwargs.get('checkpoint', None)\n    if train_checkpoint is not None and os.path.exists(train_checkpoint):\n        self.checkpoint = train_checkpoint\n    else:\n        self.checkpoint = os.path.join(self.model_dir, 'train/base.pt')\n    self.tensorboard_dir = kwargs.get('tensorboard_dir', 'tensorboard')\n    assert kwargs['train_data'], 'please config train data in dict kwargs'\n    assert kwargs['cv_data'], 'please config cv data in dict kwargs'\n    assert kwargs['trans_data'], 'please config transcription data in dict kwargs'\n    self.train_data = kwargs['train_data']\n    self.cv_data = kwargs['cv_data']\n    self.trans_data = kwargs['trans_data']\n    self.need_dump = kwargs.get('need_dump', False) and (True if self.rank == 0 else False)\n    train_conf = self.configs['preprocessor']\n    cv_conf = copy.deepcopy(train_conf)\n    cv_conf['speed_perturb'] = False\n    cv_conf['spec_aug'] = False\n    cv_conf['shuffle'] = False\n    dump_train_file = os.path.join(self.work_dir, 'dump_train.txt')\n    dump_cv_file = os.path.join(self.work_dir, 'dump_cv.txt')\n    self.train_dataset = kws_nearfield_dataset(self.train_data, self.trans_data, train_conf, self.token_table, self.lexicon_table, self.need_dump, dump_train_file, True)\n    self.cv_dataset = kws_nearfield_dataset(self.cv_data, self.trans_data, cv_conf, self.token_table, self.lexicon_table, self.need_dump, dump_cv_file, True)\n    self.train_dataloader = DataLoader(self.train_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.train.dataloader.workers_per_gpu, prefetch_factor=self.configs.train.dataloader.get('prefetch', 2))\n    self.cv_dataloader = DataLoader(self.cv_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    self.model = self.build_model(self.configs)\n    num_params = count_parameters(self.model)\n    if self.rank == 0:\n        logger.warning('the number of model params: {}'.format(num_params))\n    if self.checkpoint is not None and os.path.exists(self.checkpoint):\n        load_checkpoint(self.checkpoint, self.model)\n        info_path = re.sub('.pt$', '.yaml', self.checkpoint)\n        infos = {}\n        if os.path.exists(info_path):\n            with open(info_path, 'r') as fin:\n                infos = yaml.load(fin, Loader=yaml.FullLoader)\n    else:\n        logger.warning('Training with random initialized params')\n        infos = {}\n    self.start_epoch = infos.get('epoch', -1) + 1\n    self.configs['train']['start_epoch'] = self.start_epoch\n    lr_last_epoch = infos.get('lr', self.configs['train']['optimizer']['lr'])\n    self.configs['train']['optimizer']['lr'] = lr_last_epoch\n    device_name = kwargs.get('device', 'gpu')\n    if self.world_size > 1:\n        device_name = f'cuda:{self.local_rank}'\n    self.train_device = create_device(device_name)\n    if self.world_size > 1:\n        assert torch.cuda.is_available()\n        self.model = self.model.to(self.train_device)\n        self.model = torch.nn.parallel.DistributedDataParallel(self.model)\n    else:\n        self.model = self.model.to(self.train_device)\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(self.configs.to_dict())\n            fout.write(data)\n    logger.info('Start training...')\n    writer = None\n    if self.rank == 0:\n        os.makedirs(self.work_dir, exist_ok=True)\n        writer = SummaryWriter(os.path.join(self.work_dir, self.tensorboard_dir))\n    log_interval = self.configs['train'].get('log_interval', 10)\n    optim_conf = self.configs['train']['optimizer']\n    optimizer = optim.Adam(self.model.parameters(), lr=optim_conf['lr'], weight_decay=optim_conf['weight_decay'])\n    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-06, threshold=0.01)\n    final_epoch = None\n    if self.start_epoch == 0 and self.rank == 0:\n        save_model_path = os.path.join(self.work_dir, 'init.pt')\n        save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n    logger.info('Start training...')\n    training_config = {}\n    training_config['grad_clip'] = optim_conf['grad_clip']\n    training_config['grad_accum'] = optim_conf.get('grad_accum', 1)\n    training_config['log_interval'] = log_interval\n    training_config['world_size'] = self.world_size\n    training_config['rank'] = self.rank\n    training_config['local_rank'] = self.local_rank\n    max_epoch = self.configs['train']['max_epochs']\n    totaltime = datetime.datetime.now()\n    for epoch in range(self.start_epoch, max_epoch):\n        self.train_dataset.set_epoch(epoch)\n        training_config['epoch'] = epoch\n        lr = optimizer.param_groups[0]['lr']\n        logger.info('Epoch {} TRAIN info lr {}'.format(epoch, lr))\n        executor_train(self.model, optimizer, self.train_dataloader, self.train_device, writer, training_config)\n        (cv_loss, cv_acc) = executor_cv(self.model, self.cv_dataloader, self.train_device, training_config)\n        logger.info('Epoch {} EVAL info cv_loss {:.6f}, cv_acc {:.2f}'.format(epoch, cv_loss, cv_acc))\n        if self.rank == 0:\n            save_model_path = os.path.join(self.work_dir, '{}.pt'.format(epoch))\n            save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n            info_path = re.sub('.pt$', '.yaml', save_model_path)\n            info_dict = dict(epoch=epoch, lr=lr, cv_loss=cv_loss)\n            with open(info_path, 'w') as fout:\n                data = yaml.dump(info_dict)\n                fout.write(data)\n            writer.add_scalar('epoch/cv_loss', cv_loss, epoch)\n            writer.add_scalar('epoch/lr', lr, epoch)\n        final_epoch = epoch\n        lr_scheduler.step(cv_loss)\n    if final_epoch is not None and self.rank == 0:\n        writer.close()\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))",
            "def train(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Args:\\n            kwargs:\\n                train_data (int): wave list with kaldi style for training\\n                cv_data (int): wave list with kaldi style for cross validation\\n                trans_data (str): transcription list with kaldi style, merge train and cv\\n                checkpoint (str): basemodel checkpoint, if None, default to use base.pt in model path\\n                tensorboard_dir (str): path to save tensorboard results,\\n                                       create 'tensorboard_dir' in work_dir by default\\n                need_dump (bool): wether to dump data with mapping tokens or not\\n        \"\n    train_checkpoint = kwargs.get('checkpoint', None)\n    if train_checkpoint is not None and os.path.exists(train_checkpoint):\n        self.checkpoint = train_checkpoint\n    else:\n        self.checkpoint = os.path.join(self.model_dir, 'train/base.pt')\n    self.tensorboard_dir = kwargs.get('tensorboard_dir', 'tensorboard')\n    assert kwargs['train_data'], 'please config train data in dict kwargs'\n    assert kwargs['cv_data'], 'please config cv data in dict kwargs'\n    assert kwargs['trans_data'], 'please config transcription data in dict kwargs'\n    self.train_data = kwargs['train_data']\n    self.cv_data = kwargs['cv_data']\n    self.trans_data = kwargs['trans_data']\n    self.need_dump = kwargs.get('need_dump', False) and (True if self.rank == 0 else False)\n    train_conf = self.configs['preprocessor']\n    cv_conf = copy.deepcopy(train_conf)\n    cv_conf['speed_perturb'] = False\n    cv_conf['spec_aug'] = False\n    cv_conf['shuffle'] = False\n    dump_train_file = os.path.join(self.work_dir, 'dump_train.txt')\n    dump_cv_file = os.path.join(self.work_dir, 'dump_cv.txt')\n    self.train_dataset = kws_nearfield_dataset(self.train_data, self.trans_data, train_conf, self.token_table, self.lexicon_table, self.need_dump, dump_train_file, True)\n    self.cv_dataset = kws_nearfield_dataset(self.cv_data, self.trans_data, cv_conf, self.token_table, self.lexicon_table, self.need_dump, dump_cv_file, True)\n    self.train_dataloader = DataLoader(self.train_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.train.dataloader.workers_per_gpu, prefetch_factor=self.configs.train.dataloader.get('prefetch', 2))\n    self.cv_dataloader = DataLoader(self.cv_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    self.model = self.build_model(self.configs)\n    num_params = count_parameters(self.model)\n    if self.rank == 0:\n        logger.warning('the number of model params: {}'.format(num_params))\n    if self.checkpoint is not None and os.path.exists(self.checkpoint):\n        load_checkpoint(self.checkpoint, self.model)\n        info_path = re.sub('.pt$', '.yaml', self.checkpoint)\n        infos = {}\n        if os.path.exists(info_path):\n            with open(info_path, 'r') as fin:\n                infos = yaml.load(fin, Loader=yaml.FullLoader)\n    else:\n        logger.warning('Training with random initialized params')\n        infos = {}\n    self.start_epoch = infos.get('epoch', -1) + 1\n    self.configs['train']['start_epoch'] = self.start_epoch\n    lr_last_epoch = infos.get('lr', self.configs['train']['optimizer']['lr'])\n    self.configs['train']['optimizer']['lr'] = lr_last_epoch\n    device_name = kwargs.get('device', 'gpu')\n    if self.world_size > 1:\n        device_name = f'cuda:{self.local_rank}'\n    self.train_device = create_device(device_name)\n    if self.world_size > 1:\n        assert torch.cuda.is_available()\n        self.model = self.model.to(self.train_device)\n        self.model = torch.nn.parallel.DistributedDataParallel(self.model)\n    else:\n        self.model = self.model.to(self.train_device)\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(self.configs.to_dict())\n            fout.write(data)\n    logger.info('Start training...')\n    writer = None\n    if self.rank == 0:\n        os.makedirs(self.work_dir, exist_ok=True)\n        writer = SummaryWriter(os.path.join(self.work_dir, self.tensorboard_dir))\n    log_interval = self.configs['train'].get('log_interval', 10)\n    optim_conf = self.configs['train']['optimizer']\n    optimizer = optim.Adam(self.model.parameters(), lr=optim_conf['lr'], weight_decay=optim_conf['weight_decay'])\n    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-06, threshold=0.01)\n    final_epoch = None\n    if self.start_epoch == 0 and self.rank == 0:\n        save_model_path = os.path.join(self.work_dir, 'init.pt')\n        save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n    logger.info('Start training...')\n    training_config = {}\n    training_config['grad_clip'] = optim_conf['grad_clip']\n    training_config['grad_accum'] = optim_conf.get('grad_accum', 1)\n    training_config['log_interval'] = log_interval\n    training_config['world_size'] = self.world_size\n    training_config['rank'] = self.rank\n    training_config['local_rank'] = self.local_rank\n    max_epoch = self.configs['train']['max_epochs']\n    totaltime = datetime.datetime.now()\n    for epoch in range(self.start_epoch, max_epoch):\n        self.train_dataset.set_epoch(epoch)\n        training_config['epoch'] = epoch\n        lr = optimizer.param_groups[0]['lr']\n        logger.info('Epoch {} TRAIN info lr {}'.format(epoch, lr))\n        executor_train(self.model, optimizer, self.train_dataloader, self.train_device, writer, training_config)\n        (cv_loss, cv_acc) = executor_cv(self.model, self.cv_dataloader, self.train_device, training_config)\n        logger.info('Epoch {} EVAL info cv_loss {:.6f}, cv_acc {:.2f}'.format(epoch, cv_loss, cv_acc))\n        if self.rank == 0:\n            save_model_path = os.path.join(self.work_dir, '{}.pt'.format(epoch))\n            save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n            info_path = re.sub('.pt$', '.yaml', save_model_path)\n            info_dict = dict(epoch=epoch, lr=lr, cv_loss=cv_loss)\n            with open(info_path, 'w') as fout:\n                data = yaml.dump(info_dict)\n                fout.write(data)\n            writer.add_scalar('epoch/cv_loss', cv_loss, epoch)\n            writer.add_scalar('epoch/lr', lr, epoch)\n        final_epoch = epoch\n        lr_scheduler.step(cv_loss)\n    if final_epoch is not None and self.rank == 0:\n        writer.close()\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))",
            "def train(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Args:\\n            kwargs:\\n                train_data (int): wave list with kaldi style for training\\n                cv_data (int): wave list with kaldi style for cross validation\\n                trans_data (str): transcription list with kaldi style, merge train and cv\\n                checkpoint (str): basemodel checkpoint, if None, default to use base.pt in model path\\n                tensorboard_dir (str): path to save tensorboard results,\\n                                       create 'tensorboard_dir' in work_dir by default\\n                need_dump (bool): wether to dump data with mapping tokens or not\\n        \"\n    train_checkpoint = kwargs.get('checkpoint', None)\n    if train_checkpoint is not None and os.path.exists(train_checkpoint):\n        self.checkpoint = train_checkpoint\n    else:\n        self.checkpoint = os.path.join(self.model_dir, 'train/base.pt')\n    self.tensorboard_dir = kwargs.get('tensorboard_dir', 'tensorboard')\n    assert kwargs['train_data'], 'please config train data in dict kwargs'\n    assert kwargs['cv_data'], 'please config cv data in dict kwargs'\n    assert kwargs['trans_data'], 'please config transcription data in dict kwargs'\n    self.train_data = kwargs['train_data']\n    self.cv_data = kwargs['cv_data']\n    self.trans_data = kwargs['trans_data']\n    self.need_dump = kwargs.get('need_dump', False) and (True if self.rank == 0 else False)\n    train_conf = self.configs['preprocessor']\n    cv_conf = copy.deepcopy(train_conf)\n    cv_conf['speed_perturb'] = False\n    cv_conf['spec_aug'] = False\n    cv_conf['shuffle'] = False\n    dump_train_file = os.path.join(self.work_dir, 'dump_train.txt')\n    dump_cv_file = os.path.join(self.work_dir, 'dump_cv.txt')\n    self.train_dataset = kws_nearfield_dataset(self.train_data, self.trans_data, train_conf, self.token_table, self.lexicon_table, self.need_dump, dump_train_file, True)\n    self.cv_dataset = kws_nearfield_dataset(self.cv_data, self.trans_data, cv_conf, self.token_table, self.lexicon_table, self.need_dump, dump_cv_file, True)\n    self.train_dataloader = DataLoader(self.train_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.train.dataloader.workers_per_gpu, prefetch_factor=self.configs.train.dataloader.get('prefetch', 2))\n    self.cv_dataloader = DataLoader(self.cv_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    self.model = self.build_model(self.configs)\n    num_params = count_parameters(self.model)\n    if self.rank == 0:\n        logger.warning('the number of model params: {}'.format(num_params))\n    if self.checkpoint is not None and os.path.exists(self.checkpoint):\n        load_checkpoint(self.checkpoint, self.model)\n        info_path = re.sub('.pt$', '.yaml', self.checkpoint)\n        infos = {}\n        if os.path.exists(info_path):\n            with open(info_path, 'r') as fin:\n                infos = yaml.load(fin, Loader=yaml.FullLoader)\n    else:\n        logger.warning('Training with random initialized params')\n        infos = {}\n    self.start_epoch = infos.get('epoch', -1) + 1\n    self.configs['train']['start_epoch'] = self.start_epoch\n    lr_last_epoch = infos.get('lr', self.configs['train']['optimizer']['lr'])\n    self.configs['train']['optimizer']['lr'] = lr_last_epoch\n    device_name = kwargs.get('device', 'gpu')\n    if self.world_size > 1:\n        device_name = f'cuda:{self.local_rank}'\n    self.train_device = create_device(device_name)\n    if self.world_size > 1:\n        assert torch.cuda.is_available()\n        self.model = self.model.to(self.train_device)\n        self.model = torch.nn.parallel.DistributedDataParallel(self.model)\n    else:\n        self.model = self.model.to(self.train_device)\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(self.configs.to_dict())\n            fout.write(data)\n    logger.info('Start training...')\n    writer = None\n    if self.rank == 0:\n        os.makedirs(self.work_dir, exist_ok=True)\n        writer = SummaryWriter(os.path.join(self.work_dir, self.tensorboard_dir))\n    log_interval = self.configs['train'].get('log_interval', 10)\n    optim_conf = self.configs['train']['optimizer']\n    optimizer = optim.Adam(self.model.parameters(), lr=optim_conf['lr'], weight_decay=optim_conf['weight_decay'])\n    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-06, threshold=0.01)\n    final_epoch = None\n    if self.start_epoch == 0 and self.rank == 0:\n        save_model_path = os.path.join(self.work_dir, 'init.pt')\n        save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n    logger.info('Start training...')\n    training_config = {}\n    training_config['grad_clip'] = optim_conf['grad_clip']\n    training_config['grad_accum'] = optim_conf.get('grad_accum', 1)\n    training_config['log_interval'] = log_interval\n    training_config['world_size'] = self.world_size\n    training_config['rank'] = self.rank\n    training_config['local_rank'] = self.local_rank\n    max_epoch = self.configs['train']['max_epochs']\n    totaltime = datetime.datetime.now()\n    for epoch in range(self.start_epoch, max_epoch):\n        self.train_dataset.set_epoch(epoch)\n        training_config['epoch'] = epoch\n        lr = optimizer.param_groups[0]['lr']\n        logger.info('Epoch {} TRAIN info lr {}'.format(epoch, lr))\n        executor_train(self.model, optimizer, self.train_dataloader, self.train_device, writer, training_config)\n        (cv_loss, cv_acc) = executor_cv(self.model, self.cv_dataloader, self.train_device, training_config)\n        logger.info('Epoch {} EVAL info cv_loss {:.6f}, cv_acc {:.2f}'.format(epoch, cv_loss, cv_acc))\n        if self.rank == 0:\n            save_model_path = os.path.join(self.work_dir, '{}.pt'.format(epoch))\n            save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n            info_path = re.sub('.pt$', '.yaml', save_model_path)\n            info_dict = dict(epoch=epoch, lr=lr, cv_loss=cv_loss)\n            with open(info_path, 'w') as fout:\n                data = yaml.dump(info_dict)\n                fout.write(data)\n            writer.add_scalar('epoch/cv_loss', cv_loss, epoch)\n            writer.add_scalar('epoch/lr', lr, epoch)\n        final_epoch = epoch\n        lr_scheduler.step(cv_loss)\n    if final_epoch is not None and self.rank == 0:\n        writer.close()\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))",
            "def train(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Args:\\n            kwargs:\\n                train_data (int): wave list with kaldi style for training\\n                cv_data (int): wave list with kaldi style for cross validation\\n                trans_data (str): transcription list with kaldi style, merge train and cv\\n                checkpoint (str): basemodel checkpoint, if None, default to use base.pt in model path\\n                tensorboard_dir (str): path to save tensorboard results,\\n                                       create 'tensorboard_dir' in work_dir by default\\n                need_dump (bool): wether to dump data with mapping tokens or not\\n        \"\n    train_checkpoint = kwargs.get('checkpoint', None)\n    if train_checkpoint is not None and os.path.exists(train_checkpoint):\n        self.checkpoint = train_checkpoint\n    else:\n        self.checkpoint = os.path.join(self.model_dir, 'train/base.pt')\n    self.tensorboard_dir = kwargs.get('tensorboard_dir', 'tensorboard')\n    assert kwargs['train_data'], 'please config train data in dict kwargs'\n    assert kwargs['cv_data'], 'please config cv data in dict kwargs'\n    assert kwargs['trans_data'], 'please config transcription data in dict kwargs'\n    self.train_data = kwargs['train_data']\n    self.cv_data = kwargs['cv_data']\n    self.trans_data = kwargs['trans_data']\n    self.need_dump = kwargs.get('need_dump', False) and (True if self.rank == 0 else False)\n    train_conf = self.configs['preprocessor']\n    cv_conf = copy.deepcopy(train_conf)\n    cv_conf['speed_perturb'] = False\n    cv_conf['spec_aug'] = False\n    cv_conf['shuffle'] = False\n    dump_train_file = os.path.join(self.work_dir, 'dump_train.txt')\n    dump_cv_file = os.path.join(self.work_dir, 'dump_cv.txt')\n    self.train_dataset = kws_nearfield_dataset(self.train_data, self.trans_data, train_conf, self.token_table, self.lexicon_table, self.need_dump, dump_train_file, True)\n    self.cv_dataset = kws_nearfield_dataset(self.cv_data, self.trans_data, cv_conf, self.token_table, self.lexicon_table, self.need_dump, dump_cv_file, True)\n    self.train_dataloader = DataLoader(self.train_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.train.dataloader.workers_per_gpu, prefetch_factor=self.configs.train.dataloader.get('prefetch', 2))\n    self.cv_dataloader = DataLoader(self.cv_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    self.model = self.build_model(self.configs)\n    num_params = count_parameters(self.model)\n    if self.rank == 0:\n        logger.warning('the number of model params: {}'.format(num_params))\n    if self.checkpoint is not None and os.path.exists(self.checkpoint):\n        load_checkpoint(self.checkpoint, self.model)\n        info_path = re.sub('.pt$', '.yaml', self.checkpoint)\n        infos = {}\n        if os.path.exists(info_path):\n            with open(info_path, 'r') as fin:\n                infos = yaml.load(fin, Loader=yaml.FullLoader)\n    else:\n        logger.warning('Training with random initialized params')\n        infos = {}\n    self.start_epoch = infos.get('epoch', -1) + 1\n    self.configs['train']['start_epoch'] = self.start_epoch\n    lr_last_epoch = infos.get('lr', self.configs['train']['optimizer']['lr'])\n    self.configs['train']['optimizer']['lr'] = lr_last_epoch\n    device_name = kwargs.get('device', 'gpu')\n    if self.world_size > 1:\n        device_name = f'cuda:{self.local_rank}'\n    self.train_device = create_device(device_name)\n    if self.world_size > 1:\n        assert torch.cuda.is_available()\n        self.model = self.model.to(self.train_device)\n        self.model = torch.nn.parallel.DistributedDataParallel(self.model)\n    else:\n        self.model = self.model.to(self.train_device)\n    if self.rank == 0:\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        saved_config_path = os.path.join(self.work_dir, 'config.yaml')\n        with open(saved_config_path, 'w') as fout:\n            data = yaml.dump(self.configs.to_dict())\n            fout.write(data)\n    logger.info('Start training...')\n    writer = None\n    if self.rank == 0:\n        os.makedirs(self.work_dir, exist_ok=True)\n        writer = SummaryWriter(os.path.join(self.work_dir, self.tensorboard_dir))\n    log_interval = self.configs['train'].get('log_interval', 10)\n    optim_conf = self.configs['train']['optimizer']\n    optimizer = optim.Adam(self.model.parameters(), lr=optim_conf['lr'], weight_decay=optim_conf['weight_decay'])\n    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-06, threshold=0.01)\n    final_epoch = None\n    if self.start_epoch == 0 and self.rank == 0:\n        save_model_path = os.path.join(self.work_dir, 'init.pt')\n        save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n    logger.info('Start training...')\n    training_config = {}\n    training_config['grad_clip'] = optim_conf['grad_clip']\n    training_config['grad_accum'] = optim_conf.get('grad_accum', 1)\n    training_config['log_interval'] = log_interval\n    training_config['world_size'] = self.world_size\n    training_config['rank'] = self.rank\n    training_config['local_rank'] = self.local_rank\n    max_epoch = self.configs['train']['max_epochs']\n    totaltime = datetime.datetime.now()\n    for epoch in range(self.start_epoch, max_epoch):\n        self.train_dataset.set_epoch(epoch)\n        training_config['epoch'] = epoch\n        lr = optimizer.param_groups[0]['lr']\n        logger.info('Epoch {} TRAIN info lr {}'.format(epoch, lr))\n        executor_train(self.model, optimizer, self.train_dataloader, self.train_device, writer, training_config)\n        (cv_loss, cv_acc) = executor_cv(self.model, self.cv_dataloader, self.train_device, training_config)\n        logger.info('Epoch {} EVAL info cv_loss {:.6f}, cv_acc {:.2f}'.format(epoch, cv_loss, cv_acc))\n        if self.rank == 0:\n            save_model_path = os.path.join(self.work_dir, '{}.pt'.format(epoch))\n            save_checkpoint(self.model, save_model_path, None, None, None, False, True)\n            info_path = re.sub('.pt$', '.yaml', save_model_path)\n            info_dict = dict(epoch=epoch, lr=lr, cv_loss=cv_loss)\n            with open(info_path, 'w') as fout:\n                data = yaml.dump(info_dict)\n                fout.write(data)\n            writer.add_scalar('epoch/cv_loss', cv_loss, epoch)\n            writer.add_scalar('epoch/lr', lr, epoch)\n        final_epoch = epoch\n        lr_scheduler.step(cv_loss)\n    if final_epoch is not None and self.rank == 0:\n        writer.close()\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))"
        ]
    },
    {
        "func_name": "evaluate",
        "original": "def evaluate(self, checkpoint_path: str, *args, **kwargs) -> Dict[str, float]:\n    \"\"\"\n        Args:\n            checkpoint_path (str): evaluating with ckpt or default average ckpt\n            kwargs:\n                test_dir (str): local path for saving test results\n                test_data (str): wave list with kaldi style\n                trans_data (str): transcription list with kaldi style\n                average_num (int): the NO. to do model averaging(checkpoint_path==None)\n                batch_size (int): batch size during evaluating\n                keywords (str): keyword string, split with ','\n                gpu (int): evaluating with cpu/gpu: -1 for cpu; >=0 for gpu\n        \"\"\"\n    self.avg_checkpoint = None\n    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n        logger.warning(f'evaluating with specific model: {checkpoint_path}')\n        eval_checkpoint = checkpoint_path\n    else:\n        if self.avg_checkpoint is None:\n            avg_num = kwargs.get('average_num', 10)\n            self.avg_checkpoint = os.path.join(self.work_dir, f'avg_{avg_num}.pt')\n            logger.warning(f'default average model not exist: {self.avg_checkpoint}')\n            avg_kwargs = dict(dst_model=self.avg_checkpoint, src_path=self.work_dir, val_best=True, avg_num=avg_num)\n            self.avg_checkpoint = average_model(**avg_kwargs)\n            model_cvt = self.build_model(self.configs)\n            kaldi_cvt = convert_to_kaldi(model_cvt, self.avg_checkpoint, self.work_dir)\n            logger.warning(f'average model convert to kaldi network: {kaldi_cvt}')\n        eval_checkpoint = self.avg_checkpoint\n        logger.warning(f'evaluating with average model: {self.avg_checkpoint}')\n    if kwargs.get('test_data', None) is not None and kwargs.get('trans_data', None) is not None:\n        logger.warning('evaluating with specific data and transcription')\n        test_data = kwargs['test_data']\n        trans_data = kwargs['trans_data']\n    else:\n        logger.warning('evaluating with cross validation data during training')\n        test_data = self.cv_data\n        trans_data = self.trans_data\n    logger.warning(f'test data: {test_data}')\n    logger.warning(f'trans data: {trans_data}')\n    test_conf = copy.deepcopy(self.configs['preprocessor'])\n    test_conf['filter_conf']['max_length'] = 102400\n    test_conf['filter_conf']['min_length'] = 0\n    test_conf['speed_perturb'] = False\n    test_conf['spec_aug'] = False\n    test_conf['shuffle'] = False\n    if kwargs.get('batch_size', None) is not None:\n        test_conf['batch_conf']['batch_size'] = kwargs['batch_size']\n    test_dataset = kws_nearfield_dataset(test_data, trans_data, test_conf, self.token_table, self.lexicon_table, False, '', False)\n    test_dataloader = DataLoader(test_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), persistent_workers=True, num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    assert kwargs.get('keywords', None) is not None, 'at least one keyword is needed'\n    keywords_str = kwargs['keywords']\n    keywords_list = keywords_str.strip().replace(' ', '').split(',')\n    keywords_token = {}\n    keywords_idxset = {0}\n    keywords_strset = {'<blk>'}\n    keywords_tokenmap = {'<blk>': 0}\n    for keyword in keywords_list:\n        (strs, indexes) = query_token_set(keyword, self.token_table, self.lexicon_table)\n        keywords_token[keyword] = {}\n        keywords_token[keyword]['token_id'] = indexes\n        keywords_token[keyword]['token_str'] = ''.join(('%s ' % str(i) for i in indexes))\n        [keywords_strset.add(i) for i in strs]\n        [keywords_idxset.add(i) for i in indexes]\n        for (txt, idx) in zip(strs, indexes):\n            if keywords_tokenmap.get(txt, None) is None:\n                keywords_tokenmap[txt] = idx\n    token_print = ''\n    for (txt, idx) in keywords_tokenmap.items():\n        token_print += f'{txt}({idx}) '\n    logger.warning(f'Token set is: {token_print}')\n    use_cuda = kwargs.get('gpu', -1) >= 0 and torch.cuda.is_available()\n    device_name = kwargs.get('device', 'cpu')\n    if self.world_size > 1 and use_cuda:\n        device_name = f'cuda:{self.local_rank}'\n    self.test_device = create_device(device_name)\n    self.test_model = self.build_model(self.configs)\n    load_checkpoint(eval_checkpoint, self.test_model)\n    self.test_model = self.test_model.to(self.test_device)\n    testing_config = {}\n    if kwargs.get('test_dir', None) is not None:\n        testing_config['test_dir'] = kwargs['test_dir']\n    else:\n        base_name = os.path.basename(eval_checkpoint)\n        testing_config['test_dir'] = os.path.join(self.work_dir, 'test_' + base_name)\n    self.test_dir = testing_config['test_dir']\n    if not os.path.exists(self.test_dir):\n        os.makedirs(self.test_dir)\n    logger.info('Start evaluating...')\n    totaltime = datetime.datetime.now()\n    score_file = executor_test(self.test_model, test_dataloader, self.test_device, keywords_token, keywords_idxset, testing_config)\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))\n    det_kwargs = dict(keywords=keywords_str, test_data=test_data, trans_data=trans_data, score_file=score_file)\n    det_results = compute_det(**det_kwargs)\n    print(det_results)",
        "mutated": [
            "def evaluate(self, checkpoint_path: str, *args, **kwargs) -> Dict[str, float]:\n    if False:\n        i = 10\n    \"\\n        Args:\\n            checkpoint_path (str): evaluating with ckpt or default average ckpt\\n            kwargs:\\n                test_dir (str): local path for saving test results\\n                test_data (str): wave list with kaldi style\\n                trans_data (str): transcription list with kaldi style\\n                average_num (int): the NO. to do model averaging(checkpoint_path==None)\\n                batch_size (int): batch size during evaluating\\n                keywords (str): keyword string, split with ','\\n                gpu (int): evaluating with cpu/gpu: -1 for cpu; >=0 for gpu\\n        \"\n    self.avg_checkpoint = None\n    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n        logger.warning(f'evaluating with specific model: {checkpoint_path}')\n        eval_checkpoint = checkpoint_path\n    else:\n        if self.avg_checkpoint is None:\n            avg_num = kwargs.get('average_num', 10)\n            self.avg_checkpoint = os.path.join(self.work_dir, f'avg_{avg_num}.pt')\n            logger.warning(f'default average model not exist: {self.avg_checkpoint}')\n            avg_kwargs = dict(dst_model=self.avg_checkpoint, src_path=self.work_dir, val_best=True, avg_num=avg_num)\n            self.avg_checkpoint = average_model(**avg_kwargs)\n            model_cvt = self.build_model(self.configs)\n            kaldi_cvt = convert_to_kaldi(model_cvt, self.avg_checkpoint, self.work_dir)\n            logger.warning(f'average model convert to kaldi network: {kaldi_cvt}')\n        eval_checkpoint = self.avg_checkpoint\n        logger.warning(f'evaluating with average model: {self.avg_checkpoint}')\n    if kwargs.get('test_data', None) is not None and kwargs.get('trans_data', None) is not None:\n        logger.warning('evaluating with specific data and transcription')\n        test_data = kwargs['test_data']\n        trans_data = kwargs['trans_data']\n    else:\n        logger.warning('evaluating with cross validation data during training')\n        test_data = self.cv_data\n        trans_data = self.trans_data\n    logger.warning(f'test data: {test_data}')\n    logger.warning(f'trans data: {trans_data}')\n    test_conf = copy.deepcopy(self.configs['preprocessor'])\n    test_conf['filter_conf']['max_length'] = 102400\n    test_conf['filter_conf']['min_length'] = 0\n    test_conf['speed_perturb'] = False\n    test_conf['spec_aug'] = False\n    test_conf['shuffle'] = False\n    if kwargs.get('batch_size', None) is not None:\n        test_conf['batch_conf']['batch_size'] = kwargs['batch_size']\n    test_dataset = kws_nearfield_dataset(test_data, trans_data, test_conf, self.token_table, self.lexicon_table, False, '', False)\n    test_dataloader = DataLoader(test_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), persistent_workers=True, num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    assert kwargs.get('keywords', None) is not None, 'at least one keyword is needed'\n    keywords_str = kwargs['keywords']\n    keywords_list = keywords_str.strip().replace(' ', '').split(',')\n    keywords_token = {}\n    keywords_idxset = {0}\n    keywords_strset = {'<blk>'}\n    keywords_tokenmap = {'<blk>': 0}\n    for keyword in keywords_list:\n        (strs, indexes) = query_token_set(keyword, self.token_table, self.lexicon_table)\n        keywords_token[keyword] = {}\n        keywords_token[keyword]['token_id'] = indexes\n        keywords_token[keyword]['token_str'] = ''.join(('%s ' % str(i) for i in indexes))\n        [keywords_strset.add(i) for i in strs]\n        [keywords_idxset.add(i) for i in indexes]\n        for (txt, idx) in zip(strs, indexes):\n            if keywords_tokenmap.get(txt, None) is None:\n                keywords_tokenmap[txt] = idx\n    token_print = ''\n    for (txt, idx) in keywords_tokenmap.items():\n        token_print += f'{txt}({idx}) '\n    logger.warning(f'Token set is: {token_print}')\n    use_cuda = kwargs.get('gpu', -1) >= 0 and torch.cuda.is_available()\n    device_name = kwargs.get('device', 'cpu')\n    if self.world_size > 1 and use_cuda:\n        device_name = f'cuda:{self.local_rank}'\n    self.test_device = create_device(device_name)\n    self.test_model = self.build_model(self.configs)\n    load_checkpoint(eval_checkpoint, self.test_model)\n    self.test_model = self.test_model.to(self.test_device)\n    testing_config = {}\n    if kwargs.get('test_dir', None) is not None:\n        testing_config['test_dir'] = kwargs['test_dir']\n    else:\n        base_name = os.path.basename(eval_checkpoint)\n        testing_config['test_dir'] = os.path.join(self.work_dir, 'test_' + base_name)\n    self.test_dir = testing_config['test_dir']\n    if not os.path.exists(self.test_dir):\n        os.makedirs(self.test_dir)\n    logger.info('Start evaluating...')\n    totaltime = datetime.datetime.now()\n    score_file = executor_test(self.test_model, test_dataloader, self.test_device, keywords_token, keywords_idxset, testing_config)\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))\n    det_kwargs = dict(keywords=keywords_str, test_data=test_data, trans_data=trans_data, score_file=score_file)\n    det_results = compute_det(**det_kwargs)\n    print(det_results)",
            "def evaluate(self, checkpoint_path: str, *args, **kwargs) -> Dict[str, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Args:\\n            checkpoint_path (str): evaluating with ckpt or default average ckpt\\n            kwargs:\\n                test_dir (str): local path for saving test results\\n                test_data (str): wave list with kaldi style\\n                trans_data (str): transcription list with kaldi style\\n                average_num (int): the NO. to do model averaging(checkpoint_path==None)\\n                batch_size (int): batch size during evaluating\\n                keywords (str): keyword string, split with ','\\n                gpu (int): evaluating with cpu/gpu: -1 for cpu; >=0 for gpu\\n        \"\n    self.avg_checkpoint = None\n    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n        logger.warning(f'evaluating with specific model: {checkpoint_path}')\n        eval_checkpoint = checkpoint_path\n    else:\n        if self.avg_checkpoint is None:\n            avg_num = kwargs.get('average_num', 10)\n            self.avg_checkpoint = os.path.join(self.work_dir, f'avg_{avg_num}.pt')\n            logger.warning(f'default average model not exist: {self.avg_checkpoint}')\n            avg_kwargs = dict(dst_model=self.avg_checkpoint, src_path=self.work_dir, val_best=True, avg_num=avg_num)\n            self.avg_checkpoint = average_model(**avg_kwargs)\n            model_cvt = self.build_model(self.configs)\n            kaldi_cvt = convert_to_kaldi(model_cvt, self.avg_checkpoint, self.work_dir)\n            logger.warning(f'average model convert to kaldi network: {kaldi_cvt}')\n        eval_checkpoint = self.avg_checkpoint\n        logger.warning(f'evaluating with average model: {self.avg_checkpoint}')\n    if kwargs.get('test_data', None) is not None and kwargs.get('trans_data', None) is not None:\n        logger.warning('evaluating with specific data and transcription')\n        test_data = kwargs['test_data']\n        trans_data = kwargs['trans_data']\n    else:\n        logger.warning('evaluating with cross validation data during training')\n        test_data = self.cv_data\n        trans_data = self.trans_data\n    logger.warning(f'test data: {test_data}')\n    logger.warning(f'trans data: {trans_data}')\n    test_conf = copy.deepcopy(self.configs['preprocessor'])\n    test_conf['filter_conf']['max_length'] = 102400\n    test_conf['filter_conf']['min_length'] = 0\n    test_conf['speed_perturb'] = False\n    test_conf['spec_aug'] = False\n    test_conf['shuffle'] = False\n    if kwargs.get('batch_size', None) is not None:\n        test_conf['batch_conf']['batch_size'] = kwargs['batch_size']\n    test_dataset = kws_nearfield_dataset(test_data, trans_data, test_conf, self.token_table, self.lexicon_table, False, '', False)\n    test_dataloader = DataLoader(test_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), persistent_workers=True, num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    assert kwargs.get('keywords', None) is not None, 'at least one keyword is needed'\n    keywords_str = kwargs['keywords']\n    keywords_list = keywords_str.strip().replace(' ', '').split(',')\n    keywords_token = {}\n    keywords_idxset = {0}\n    keywords_strset = {'<blk>'}\n    keywords_tokenmap = {'<blk>': 0}\n    for keyword in keywords_list:\n        (strs, indexes) = query_token_set(keyword, self.token_table, self.lexicon_table)\n        keywords_token[keyword] = {}\n        keywords_token[keyword]['token_id'] = indexes\n        keywords_token[keyword]['token_str'] = ''.join(('%s ' % str(i) for i in indexes))\n        [keywords_strset.add(i) for i in strs]\n        [keywords_idxset.add(i) for i in indexes]\n        for (txt, idx) in zip(strs, indexes):\n            if keywords_tokenmap.get(txt, None) is None:\n                keywords_tokenmap[txt] = idx\n    token_print = ''\n    for (txt, idx) in keywords_tokenmap.items():\n        token_print += f'{txt}({idx}) '\n    logger.warning(f'Token set is: {token_print}')\n    use_cuda = kwargs.get('gpu', -1) >= 0 and torch.cuda.is_available()\n    device_name = kwargs.get('device', 'cpu')\n    if self.world_size > 1 and use_cuda:\n        device_name = f'cuda:{self.local_rank}'\n    self.test_device = create_device(device_name)\n    self.test_model = self.build_model(self.configs)\n    load_checkpoint(eval_checkpoint, self.test_model)\n    self.test_model = self.test_model.to(self.test_device)\n    testing_config = {}\n    if kwargs.get('test_dir', None) is not None:\n        testing_config['test_dir'] = kwargs['test_dir']\n    else:\n        base_name = os.path.basename(eval_checkpoint)\n        testing_config['test_dir'] = os.path.join(self.work_dir, 'test_' + base_name)\n    self.test_dir = testing_config['test_dir']\n    if not os.path.exists(self.test_dir):\n        os.makedirs(self.test_dir)\n    logger.info('Start evaluating...')\n    totaltime = datetime.datetime.now()\n    score_file = executor_test(self.test_model, test_dataloader, self.test_device, keywords_token, keywords_idxset, testing_config)\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))\n    det_kwargs = dict(keywords=keywords_str, test_data=test_data, trans_data=trans_data, score_file=score_file)\n    det_results = compute_det(**det_kwargs)\n    print(det_results)",
            "def evaluate(self, checkpoint_path: str, *args, **kwargs) -> Dict[str, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Args:\\n            checkpoint_path (str): evaluating with ckpt or default average ckpt\\n            kwargs:\\n                test_dir (str): local path for saving test results\\n                test_data (str): wave list with kaldi style\\n                trans_data (str): transcription list with kaldi style\\n                average_num (int): the NO. to do model averaging(checkpoint_path==None)\\n                batch_size (int): batch size during evaluating\\n                keywords (str): keyword string, split with ','\\n                gpu (int): evaluating with cpu/gpu: -1 for cpu; >=0 for gpu\\n        \"\n    self.avg_checkpoint = None\n    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n        logger.warning(f'evaluating with specific model: {checkpoint_path}')\n        eval_checkpoint = checkpoint_path\n    else:\n        if self.avg_checkpoint is None:\n            avg_num = kwargs.get('average_num', 10)\n            self.avg_checkpoint = os.path.join(self.work_dir, f'avg_{avg_num}.pt')\n            logger.warning(f'default average model not exist: {self.avg_checkpoint}')\n            avg_kwargs = dict(dst_model=self.avg_checkpoint, src_path=self.work_dir, val_best=True, avg_num=avg_num)\n            self.avg_checkpoint = average_model(**avg_kwargs)\n            model_cvt = self.build_model(self.configs)\n            kaldi_cvt = convert_to_kaldi(model_cvt, self.avg_checkpoint, self.work_dir)\n            logger.warning(f'average model convert to kaldi network: {kaldi_cvt}')\n        eval_checkpoint = self.avg_checkpoint\n        logger.warning(f'evaluating with average model: {self.avg_checkpoint}')\n    if kwargs.get('test_data', None) is not None and kwargs.get('trans_data', None) is not None:\n        logger.warning('evaluating with specific data and transcription')\n        test_data = kwargs['test_data']\n        trans_data = kwargs['trans_data']\n    else:\n        logger.warning('evaluating with cross validation data during training')\n        test_data = self.cv_data\n        trans_data = self.trans_data\n    logger.warning(f'test data: {test_data}')\n    logger.warning(f'trans data: {trans_data}')\n    test_conf = copy.deepcopy(self.configs['preprocessor'])\n    test_conf['filter_conf']['max_length'] = 102400\n    test_conf['filter_conf']['min_length'] = 0\n    test_conf['speed_perturb'] = False\n    test_conf['spec_aug'] = False\n    test_conf['shuffle'] = False\n    if kwargs.get('batch_size', None) is not None:\n        test_conf['batch_conf']['batch_size'] = kwargs['batch_size']\n    test_dataset = kws_nearfield_dataset(test_data, trans_data, test_conf, self.token_table, self.lexicon_table, False, '', False)\n    test_dataloader = DataLoader(test_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), persistent_workers=True, num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    assert kwargs.get('keywords', None) is not None, 'at least one keyword is needed'\n    keywords_str = kwargs['keywords']\n    keywords_list = keywords_str.strip().replace(' ', '').split(',')\n    keywords_token = {}\n    keywords_idxset = {0}\n    keywords_strset = {'<blk>'}\n    keywords_tokenmap = {'<blk>': 0}\n    for keyword in keywords_list:\n        (strs, indexes) = query_token_set(keyword, self.token_table, self.lexicon_table)\n        keywords_token[keyword] = {}\n        keywords_token[keyword]['token_id'] = indexes\n        keywords_token[keyword]['token_str'] = ''.join(('%s ' % str(i) for i in indexes))\n        [keywords_strset.add(i) for i in strs]\n        [keywords_idxset.add(i) for i in indexes]\n        for (txt, idx) in zip(strs, indexes):\n            if keywords_tokenmap.get(txt, None) is None:\n                keywords_tokenmap[txt] = idx\n    token_print = ''\n    for (txt, idx) in keywords_tokenmap.items():\n        token_print += f'{txt}({idx}) '\n    logger.warning(f'Token set is: {token_print}')\n    use_cuda = kwargs.get('gpu', -1) >= 0 and torch.cuda.is_available()\n    device_name = kwargs.get('device', 'cpu')\n    if self.world_size > 1 and use_cuda:\n        device_name = f'cuda:{self.local_rank}'\n    self.test_device = create_device(device_name)\n    self.test_model = self.build_model(self.configs)\n    load_checkpoint(eval_checkpoint, self.test_model)\n    self.test_model = self.test_model.to(self.test_device)\n    testing_config = {}\n    if kwargs.get('test_dir', None) is not None:\n        testing_config['test_dir'] = kwargs['test_dir']\n    else:\n        base_name = os.path.basename(eval_checkpoint)\n        testing_config['test_dir'] = os.path.join(self.work_dir, 'test_' + base_name)\n    self.test_dir = testing_config['test_dir']\n    if not os.path.exists(self.test_dir):\n        os.makedirs(self.test_dir)\n    logger.info('Start evaluating...')\n    totaltime = datetime.datetime.now()\n    score_file = executor_test(self.test_model, test_dataloader, self.test_device, keywords_token, keywords_idxset, testing_config)\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))\n    det_kwargs = dict(keywords=keywords_str, test_data=test_data, trans_data=trans_data, score_file=score_file)\n    det_results = compute_det(**det_kwargs)\n    print(det_results)",
            "def evaluate(self, checkpoint_path: str, *args, **kwargs) -> Dict[str, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Args:\\n            checkpoint_path (str): evaluating with ckpt or default average ckpt\\n            kwargs:\\n                test_dir (str): local path for saving test results\\n                test_data (str): wave list with kaldi style\\n                trans_data (str): transcription list with kaldi style\\n                average_num (int): the NO. to do model averaging(checkpoint_path==None)\\n                batch_size (int): batch size during evaluating\\n                keywords (str): keyword string, split with ','\\n                gpu (int): evaluating with cpu/gpu: -1 for cpu; >=0 for gpu\\n        \"\n    self.avg_checkpoint = None\n    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n        logger.warning(f'evaluating with specific model: {checkpoint_path}')\n        eval_checkpoint = checkpoint_path\n    else:\n        if self.avg_checkpoint is None:\n            avg_num = kwargs.get('average_num', 10)\n            self.avg_checkpoint = os.path.join(self.work_dir, f'avg_{avg_num}.pt')\n            logger.warning(f'default average model not exist: {self.avg_checkpoint}')\n            avg_kwargs = dict(dst_model=self.avg_checkpoint, src_path=self.work_dir, val_best=True, avg_num=avg_num)\n            self.avg_checkpoint = average_model(**avg_kwargs)\n            model_cvt = self.build_model(self.configs)\n            kaldi_cvt = convert_to_kaldi(model_cvt, self.avg_checkpoint, self.work_dir)\n            logger.warning(f'average model convert to kaldi network: {kaldi_cvt}')\n        eval_checkpoint = self.avg_checkpoint\n        logger.warning(f'evaluating with average model: {self.avg_checkpoint}')\n    if kwargs.get('test_data', None) is not None and kwargs.get('trans_data', None) is not None:\n        logger.warning('evaluating with specific data and transcription')\n        test_data = kwargs['test_data']\n        trans_data = kwargs['trans_data']\n    else:\n        logger.warning('evaluating with cross validation data during training')\n        test_data = self.cv_data\n        trans_data = self.trans_data\n    logger.warning(f'test data: {test_data}')\n    logger.warning(f'trans data: {trans_data}')\n    test_conf = copy.deepcopy(self.configs['preprocessor'])\n    test_conf['filter_conf']['max_length'] = 102400\n    test_conf['filter_conf']['min_length'] = 0\n    test_conf['speed_perturb'] = False\n    test_conf['spec_aug'] = False\n    test_conf['shuffle'] = False\n    if kwargs.get('batch_size', None) is not None:\n        test_conf['batch_conf']['batch_size'] = kwargs['batch_size']\n    test_dataset = kws_nearfield_dataset(test_data, trans_data, test_conf, self.token_table, self.lexicon_table, False, '', False)\n    test_dataloader = DataLoader(test_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), persistent_workers=True, num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    assert kwargs.get('keywords', None) is not None, 'at least one keyword is needed'\n    keywords_str = kwargs['keywords']\n    keywords_list = keywords_str.strip().replace(' ', '').split(',')\n    keywords_token = {}\n    keywords_idxset = {0}\n    keywords_strset = {'<blk>'}\n    keywords_tokenmap = {'<blk>': 0}\n    for keyword in keywords_list:\n        (strs, indexes) = query_token_set(keyword, self.token_table, self.lexicon_table)\n        keywords_token[keyword] = {}\n        keywords_token[keyword]['token_id'] = indexes\n        keywords_token[keyword]['token_str'] = ''.join(('%s ' % str(i) for i in indexes))\n        [keywords_strset.add(i) for i in strs]\n        [keywords_idxset.add(i) for i in indexes]\n        for (txt, idx) in zip(strs, indexes):\n            if keywords_tokenmap.get(txt, None) is None:\n                keywords_tokenmap[txt] = idx\n    token_print = ''\n    for (txt, idx) in keywords_tokenmap.items():\n        token_print += f'{txt}({idx}) '\n    logger.warning(f'Token set is: {token_print}')\n    use_cuda = kwargs.get('gpu', -1) >= 0 and torch.cuda.is_available()\n    device_name = kwargs.get('device', 'cpu')\n    if self.world_size > 1 and use_cuda:\n        device_name = f'cuda:{self.local_rank}'\n    self.test_device = create_device(device_name)\n    self.test_model = self.build_model(self.configs)\n    load_checkpoint(eval_checkpoint, self.test_model)\n    self.test_model = self.test_model.to(self.test_device)\n    testing_config = {}\n    if kwargs.get('test_dir', None) is not None:\n        testing_config['test_dir'] = kwargs['test_dir']\n    else:\n        base_name = os.path.basename(eval_checkpoint)\n        testing_config['test_dir'] = os.path.join(self.work_dir, 'test_' + base_name)\n    self.test_dir = testing_config['test_dir']\n    if not os.path.exists(self.test_dir):\n        os.makedirs(self.test_dir)\n    logger.info('Start evaluating...')\n    totaltime = datetime.datetime.now()\n    score_file = executor_test(self.test_model, test_dataloader, self.test_device, keywords_token, keywords_idxset, testing_config)\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))\n    det_kwargs = dict(keywords=keywords_str, test_data=test_data, trans_data=trans_data, score_file=score_file)\n    det_results = compute_det(**det_kwargs)\n    print(det_results)",
            "def evaluate(self, checkpoint_path: str, *args, **kwargs) -> Dict[str, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Args:\\n            checkpoint_path (str): evaluating with ckpt or default average ckpt\\n            kwargs:\\n                test_dir (str): local path for saving test results\\n                test_data (str): wave list with kaldi style\\n                trans_data (str): transcription list with kaldi style\\n                average_num (int): the NO. to do model averaging(checkpoint_path==None)\\n                batch_size (int): batch size during evaluating\\n                keywords (str): keyword string, split with ','\\n                gpu (int): evaluating with cpu/gpu: -1 for cpu; >=0 for gpu\\n        \"\n    self.avg_checkpoint = None\n    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n        logger.warning(f'evaluating with specific model: {checkpoint_path}')\n        eval_checkpoint = checkpoint_path\n    else:\n        if self.avg_checkpoint is None:\n            avg_num = kwargs.get('average_num', 10)\n            self.avg_checkpoint = os.path.join(self.work_dir, f'avg_{avg_num}.pt')\n            logger.warning(f'default average model not exist: {self.avg_checkpoint}')\n            avg_kwargs = dict(dst_model=self.avg_checkpoint, src_path=self.work_dir, val_best=True, avg_num=avg_num)\n            self.avg_checkpoint = average_model(**avg_kwargs)\n            model_cvt = self.build_model(self.configs)\n            kaldi_cvt = convert_to_kaldi(model_cvt, self.avg_checkpoint, self.work_dir)\n            logger.warning(f'average model convert to kaldi network: {kaldi_cvt}')\n        eval_checkpoint = self.avg_checkpoint\n        logger.warning(f'evaluating with average model: {self.avg_checkpoint}')\n    if kwargs.get('test_data', None) is not None and kwargs.get('trans_data', None) is not None:\n        logger.warning('evaluating with specific data and transcription')\n        test_data = kwargs['test_data']\n        trans_data = kwargs['trans_data']\n    else:\n        logger.warning('evaluating with cross validation data during training')\n        test_data = self.cv_data\n        trans_data = self.trans_data\n    logger.warning(f'test data: {test_data}')\n    logger.warning(f'trans data: {trans_data}')\n    test_conf = copy.deepcopy(self.configs['preprocessor'])\n    test_conf['filter_conf']['max_length'] = 102400\n    test_conf['filter_conf']['min_length'] = 0\n    test_conf['speed_perturb'] = False\n    test_conf['spec_aug'] = False\n    test_conf['shuffle'] = False\n    if kwargs.get('batch_size', None) is not None:\n        test_conf['batch_conf']['batch_size'] = kwargs['batch_size']\n    test_dataset = kws_nearfield_dataset(test_data, trans_data, test_conf, self.token_table, self.lexicon_table, False, '', False)\n    test_dataloader = DataLoader(test_dataset, batch_size=None, pin_memory=kwargs.get('pin_memory', False), persistent_workers=True, num_workers=self.configs.evaluation.dataloader.workers_per_gpu, prefetch_factor=self.configs.evaluation.dataloader.get('prefetch', 2))\n    assert kwargs.get('keywords', None) is not None, 'at least one keyword is needed'\n    keywords_str = kwargs['keywords']\n    keywords_list = keywords_str.strip().replace(' ', '').split(',')\n    keywords_token = {}\n    keywords_idxset = {0}\n    keywords_strset = {'<blk>'}\n    keywords_tokenmap = {'<blk>': 0}\n    for keyword in keywords_list:\n        (strs, indexes) = query_token_set(keyword, self.token_table, self.lexicon_table)\n        keywords_token[keyword] = {}\n        keywords_token[keyword]['token_id'] = indexes\n        keywords_token[keyword]['token_str'] = ''.join(('%s ' % str(i) for i in indexes))\n        [keywords_strset.add(i) for i in strs]\n        [keywords_idxset.add(i) for i in indexes]\n        for (txt, idx) in zip(strs, indexes):\n            if keywords_tokenmap.get(txt, None) is None:\n                keywords_tokenmap[txt] = idx\n    token_print = ''\n    for (txt, idx) in keywords_tokenmap.items():\n        token_print += f'{txt}({idx}) '\n    logger.warning(f'Token set is: {token_print}')\n    use_cuda = kwargs.get('gpu', -1) >= 0 and torch.cuda.is_available()\n    device_name = kwargs.get('device', 'cpu')\n    if self.world_size > 1 and use_cuda:\n        device_name = f'cuda:{self.local_rank}'\n    self.test_device = create_device(device_name)\n    self.test_model = self.build_model(self.configs)\n    load_checkpoint(eval_checkpoint, self.test_model)\n    self.test_model = self.test_model.to(self.test_device)\n    testing_config = {}\n    if kwargs.get('test_dir', None) is not None:\n        testing_config['test_dir'] = kwargs['test_dir']\n    else:\n        base_name = os.path.basename(eval_checkpoint)\n        testing_config['test_dir'] = os.path.join(self.work_dir, 'test_' + base_name)\n    self.test_dir = testing_config['test_dir']\n    if not os.path.exists(self.test_dir):\n        os.makedirs(self.test_dir)\n    logger.info('Start evaluating...')\n    totaltime = datetime.datetime.now()\n    score_file = executor_test(self.test_model, test_dataloader, self.test_device, keywords_token, keywords_idxset, testing_config)\n    totaltime = datetime.datetime.now() - totaltime\n    logger.info('Total time spent: {:.2f} hours'.format(totaltime.total_seconds() / 3600.0))\n    det_kwargs = dict(keywords=keywords_str, test_data=test_data, trans_data=trans_data, score_file=score_file)\n    det_results = compute_det(**det_kwargs)\n    print(det_results)"
        ]
    },
    {
        "func_name": "build_model",
        "original": "def build_model(self, configs) -> nn.Module:\n    \"\"\" Instantiate a pytorch model and return.\n\n        By default, we will create a model using config from configuration file. You can\n        override this method in a subclass.\n\n        \"\"\"\n    model = Model.from_pretrained(self.model_dir, cfg_dict=configs, training=True)\n    if isinstance(model, TorchModel) and hasattr(model, 'model'):\n        return model.model\n    elif isinstance(model, nn.Module):\n        return model",
        "mutated": [
            "def build_model(self, configs) -> nn.Module:\n    if False:\n        i = 10\n    ' Instantiate a pytorch model and return.\\n\\n        By default, we will create a model using config from configuration file. You can\\n        override this method in a subclass.\\n\\n        '\n    model = Model.from_pretrained(self.model_dir, cfg_dict=configs, training=True)\n    if isinstance(model, TorchModel) and hasattr(model, 'model'):\n        return model.model\n    elif isinstance(model, nn.Module):\n        return model",
            "def build_model(self, configs) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Instantiate a pytorch model and return.\\n\\n        By default, we will create a model using config from configuration file. You can\\n        override this method in a subclass.\\n\\n        '\n    model = Model.from_pretrained(self.model_dir, cfg_dict=configs, training=True)\n    if isinstance(model, TorchModel) and hasattr(model, 'model'):\n        return model.model\n    elif isinstance(model, nn.Module):\n        return model",
            "def build_model(self, configs) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Instantiate a pytorch model and return.\\n\\n        By default, we will create a model using config from configuration file. You can\\n        override this method in a subclass.\\n\\n        '\n    model = Model.from_pretrained(self.model_dir, cfg_dict=configs, training=True)\n    if isinstance(model, TorchModel) and hasattr(model, 'model'):\n        return model.model\n    elif isinstance(model, nn.Module):\n        return model",
            "def build_model(self, configs) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Instantiate a pytorch model and return.\\n\\n        By default, we will create a model using config from configuration file. You can\\n        override this method in a subclass.\\n\\n        '\n    model = Model.from_pretrained(self.model_dir, cfg_dict=configs, training=True)\n    if isinstance(model, TorchModel) and hasattr(model, 'model'):\n        return model.model\n    elif isinstance(model, nn.Module):\n        return model",
            "def build_model(self, configs) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Instantiate a pytorch model and return.\\n\\n        By default, we will create a model using config from configuration file. You can\\n        override this method in a subclass.\\n\\n        '\n    model = Model.from_pretrained(self.model_dir, cfg_dict=configs, training=True)\n    if isinstance(model, TorchModel) and hasattr(model, 'model'):\n        return model.model\n    elif isinstance(model, nn.Module):\n        return model"
        ]
    },
    {
        "func_name": "init_dist",
        "original": "def init_dist(self, train_nodes=1):\n    if os.getenv('RANK', None) is None:\n        os.environ['RANK'] = '0'\n    if os.getenv('LOCAL_RANK', None) is None:\n        os.environ['LOCAL_RANK'] = '0'\n    if os.getenv('WORLD_SIZE', None) is None:\n        os.environ['WORLD_SIZE'] = '1'\n    if os.getenv('MASTER_ADDR', None) is None:\n        os.environ['MASTER_ADDR'] = 'localhost'\n    if os.getenv('MASTER_PORT', None) is None:\n        os.environ['MASTER_PORT'] = '29500'\n    self.rank = int(os.environ['RANK'])\n    self.local_rank = int(os.environ['LOCAL_RANK'])\n    self.world_size = int(os.environ['WORLD_SIZE'])\n    self.master_addr = os.environ['MASTER_ADDR']\n    self.master_port = os.environ['MASTER_PORT']\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('init dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.init_process_group(backend=self.dist_backend, init_method='env://')\n    elif train_nodes > 1:\n        dist.init_process_group(backend=self.dist_backend, init_method='env://')\n        dist.barrier()\n    logger.info('RANK {}/{}/{}, Master addr:{}, Master port:{}'.format(self.world_size, self.rank, self.local_rank, self.master_addr, self.master_port))",
        "mutated": [
            "def init_dist(self, train_nodes=1):\n    if False:\n        i = 10\n    if os.getenv('RANK', None) is None:\n        os.environ['RANK'] = '0'\n    if os.getenv('LOCAL_RANK', None) is None:\n        os.environ['LOCAL_RANK'] = '0'\n    if os.getenv('WORLD_SIZE', None) is None:\n        os.environ['WORLD_SIZE'] = '1'\n    if os.getenv('MASTER_ADDR', None) is None:\n        os.environ['MASTER_ADDR'] = 'localhost'\n    if os.getenv('MASTER_PORT', None) is None:\n        os.environ['MASTER_PORT'] = '29500'\n    self.rank = int(os.environ['RANK'])\n    self.local_rank = int(os.environ['LOCAL_RANK'])\n    self.world_size = int(os.environ['WORLD_SIZE'])\n    self.master_addr = os.environ['MASTER_ADDR']\n    self.master_port = os.environ['MASTER_PORT']\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('init dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.init_process_group(backend=self.dist_backend, init_method='env://')\n    elif train_nodes > 1:\n        dist.init_process_group(backend=self.dist_backend, init_method='env://')\n        dist.barrier()\n    logger.info('RANK {}/{}/{}, Master addr:{}, Master port:{}'.format(self.world_size, self.rank, self.local_rank, self.master_addr, self.master_port))",
            "def init_dist(self, train_nodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if os.getenv('RANK', None) is None:\n        os.environ['RANK'] = '0'\n    if os.getenv('LOCAL_RANK', None) is None:\n        os.environ['LOCAL_RANK'] = '0'\n    if os.getenv('WORLD_SIZE', None) is None:\n        os.environ['WORLD_SIZE'] = '1'\n    if os.getenv('MASTER_ADDR', None) is None:\n        os.environ['MASTER_ADDR'] = 'localhost'\n    if os.getenv('MASTER_PORT', None) is None:\n        os.environ['MASTER_PORT'] = '29500'\n    self.rank = int(os.environ['RANK'])\n    self.local_rank = int(os.environ['LOCAL_RANK'])\n    self.world_size = int(os.environ['WORLD_SIZE'])\n    self.master_addr = os.environ['MASTER_ADDR']\n    self.master_port = os.environ['MASTER_PORT']\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('init dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.init_process_group(backend=self.dist_backend, init_method='env://')\n    elif train_nodes > 1:\n        dist.init_process_group(backend=self.dist_backend, init_method='env://')\n        dist.barrier()\n    logger.info('RANK {}/{}/{}, Master addr:{}, Master port:{}'.format(self.world_size, self.rank, self.local_rank, self.master_addr, self.master_port))",
            "def init_dist(self, train_nodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if os.getenv('RANK', None) is None:\n        os.environ['RANK'] = '0'\n    if os.getenv('LOCAL_RANK', None) is None:\n        os.environ['LOCAL_RANK'] = '0'\n    if os.getenv('WORLD_SIZE', None) is None:\n        os.environ['WORLD_SIZE'] = '1'\n    if os.getenv('MASTER_ADDR', None) is None:\n        os.environ['MASTER_ADDR'] = 'localhost'\n    if os.getenv('MASTER_PORT', None) is None:\n        os.environ['MASTER_PORT'] = '29500'\n    self.rank = int(os.environ['RANK'])\n    self.local_rank = int(os.environ['LOCAL_RANK'])\n    self.world_size = int(os.environ['WORLD_SIZE'])\n    self.master_addr = os.environ['MASTER_ADDR']\n    self.master_port = os.environ['MASTER_PORT']\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('init dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.init_process_group(backend=self.dist_backend, init_method='env://')\n    elif train_nodes > 1:\n        dist.init_process_group(backend=self.dist_backend, init_method='env://')\n        dist.barrier()\n    logger.info('RANK {}/{}/{}, Master addr:{}, Master port:{}'.format(self.world_size, self.rank, self.local_rank, self.master_addr, self.master_port))",
            "def init_dist(self, train_nodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if os.getenv('RANK', None) is None:\n        os.environ['RANK'] = '0'\n    if os.getenv('LOCAL_RANK', None) is None:\n        os.environ['LOCAL_RANK'] = '0'\n    if os.getenv('WORLD_SIZE', None) is None:\n        os.environ['WORLD_SIZE'] = '1'\n    if os.getenv('MASTER_ADDR', None) is None:\n        os.environ['MASTER_ADDR'] = 'localhost'\n    if os.getenv('MASTER_PORT', None) is None:\n        os.environ['MASTER_PORT'] = '29500'\n    self.rank = int(os.environ['RANK'])\n    self.local_rank = int(os.environ['LOCAL_RANK'])\n    self.world_size = int(os.environ['WORLD_SIZE'])\n    self.master_addr = os.environ['MASTER_ADDR']\n    self.master_port = os.environ['MASTER_PORT']\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('init dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.init_process_group(backend=self.dist_backend, init_method='env://')\n    elif train_nodes > 1:\n        dist.init_process_group(backend=self.dist_backend, init_method='env://')\n        dist.barrier()\n    logger.info('RANK {}/{}/{}, Master addr:{}, Master port:{}'.format(self.world_size, self.rank, self.local_rank, self.master_addr, self.master_port))",
            "def init_dist(self, train_nodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if os.getenv('RANK', None) is None:\n        os.environ['RANK'] = '0'\n    if os.getenv('LOCAL_RANK', None) is None:\n        os.environ['LOCAL_RANK'] = '0'\n    if os.getenv('WORLD_SIZE', None) is None:\n        os.environ['WORLD_SIZE'] = '1'\n    if os.getenv('MASTER_ADDR', None) is None:\n        os.environ['MASTER_ADDR'] = 'localhost'\n    if os.getenv('MASTER_PORT', None) is None:\n        os.environ['MASTER_PORT'] = '29500'\n    self.rank = int(os.environ['RANK'])\n    self.local_rank = int(os.environ['LOCAL_RANK'])\n    self.world_size = int(os.environ['WORLD_SIZE'])\n    self.master_addr = os.environ['MASTER_ADDR']\n    self.master_port = os.environ['MASTER_PORT']\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('init dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.init_process_group(backend=self.dist_backend, init_method='env://')\n    elif train_nodes > 1:\n        dist.init_process_group(backend=self.dist_backend, init_method='env://')\n        dist.barrier()\n    logger.info('RANK {}/{}/{}, Master addr:{}, Master port:{}'.format(self.world_size, self.rank, self.local_rank, self.master_addr, self.master_port))"
        ]
    },
    {
        "func_name": "uninit_dist",
        "original": "def uninit_dist(self, train_nodes=1):\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('destory dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.destroy_process_group()\n    elif train_nodes > 1:\n        dist.barrier()\n        dist.destroy_process_group()",
        "mutated": [
            "def uninit_dist(self, train_nodes=1):\n    if False:\n        i = 10\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('destory dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.destroy_process_group()\n    elif train_nodes > 1:\n        dist.barrier()\n        dist.destroy_process_group()",
            "def uninit_dist(self, train_nodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('destory dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.destroy_process_group()\n    elif train_nodes > 1:\n        dist.barrier()\n        dist.destroy_process_group()",
            "def uninit_dist(self, train_nodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('destory dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.destroy_process_group()\n    elif train_nodes > 1:\n        dist.barrier()\n        dist.destroy_process_group()",
            "def uninit_dist(self, train_nodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('destory dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.destroy_process_group()\n    elif train_nodes > 1:\n        dist.barrier()\n        dist.destroy_process_group()",
            "def uninit_dist(self, train_nodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if train_nodes == 1:\n        if self.world_size > 1:\n            logger.info('destory dist on multiple gpus, this gpu {}'.format(self.local_rank))\n            dist.destroy_process_group()\n    elif train_nodes > 1:\n        dist.barrier()\n        dist.destroy_process_group()"
        ]
    }
]