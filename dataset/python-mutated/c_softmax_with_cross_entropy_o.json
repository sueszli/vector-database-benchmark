[
    {
        "func_name": "stable_softmax",
        "original": "def stable_softmax(x):\n    \"\"\"Compute the softmax of vector x in a numerically stable way.\"\"\"\n    shiftx = x - np.max(x)\n    exps = np.exp(shiftx)\n    return exps / np.sum(exps)",
        "mutated": [
            "def stable_softmax(x):\n    if False:\n        i = 10\n    'Compute the softmax of vector x in a numerically stable way.'\n    shiftx = x - np.max(x)\n    exps = np.exp(shiftx)\n    return exps / np.sum(exps)",
            "def stable_softmax(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the softmax of vector x in a numerically stable way.'\n    shiftx = x - np.max(x)\n    exps = np.exp(shiftx)\n    return exps / np.sum(exps)",
            "def stable_softmax(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the softmax of vector x in a numerically stable way.'\n    shiftx = x - np.max(x)\n    exps = np.exp(shiftx)\n    return exps / np.sum(exps)",
            "def stable_softmax(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the softmax of vector x in a numerically stable way.'\n    shiftx = x - np.max(x)\n    exps = np.exp(shiftx)\n    return exps / np.sum(exps)",
            "def stable_softmax(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the softmax of vector x in a numerically stable way.'\n    shiftx = x - np.max(x)\n    exps = np.exp(shiftx)\n    return exps / np.sum(exps)"
        ]
    },
    {
        "func_name": "cross_entropy",
        "original": "def cross_entropy(softmax, label, soft_label, axis, ignore_index=-1):\n    if soft_label:\n        return (-label * np.log(softmax)).sum(axis=axis, keepdims=True)\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    axis_dim = shape[axis]\n    remain = int(np.prod(shape[axis + 1:]))\n    softmax_reshape = softmax.reshape((n, axis_dim, remain))\n    label_reshape = label.reshape((n, 1, remain))\n    result = np.zeros_like(label_reshape, dtype=softmax.dtype)\n    for i in range(n):\n        for j in range(remain):\n            lbl = label_reshape[i, 0, j]\n            if lbl != ignore_index:\n                result[i, 0, j] -= np.log(softmax_reshape[i, lbl, j])\n    return result.reshape(label.shape)",
        "mutated": [
            "def cross_entropy(softmax, label, soft_label, axis, ignore_index=-1):\n    if False:\n        i = 10\n    if soft_label:\n        return (-label * np.log(softmax)).sum(axis=axis, keepdims=True)\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    axis_dim = shape[axis]\n    remain = int(np.prod(shape[axis + 1:]))\n    softmax_reshape = softmax.reshape((n, axis_dim, remain))\n    label_reshape = label.reshape((n, 1, remain))\n    result = np.zeros_like(label_reshape, dtype=softmax.dtype)\n    for i in range(n):\n        for j in range(remain):\n            lbl = label_reshape[i, 0, j]\n            if lbl != ignore_index:\n                result[i, 0, j] -= np.log(softmax_reshape[i, lbl, j])\n    return result.reshape(label.shape)",
            "def cross_entropy(softmax, label, soft_label, axis, ignore_index=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if soft_label:\n        return (-label * np.log(softmax)).sum(axis=axis, keepdims=True)\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    axis_dim = shape[axis]\n    remain = int(np.prod(shape[axis + 1:]))\n    softmax_reshape = softmax.reshape((n, axis_dim, remain))\n    label_reshape = label.reshape((n, 1, remain))\n    result = np.zeros_like(label_reshape, dtype=softmax.dtype)\n    for i in range(n):\n        for j in range(remain):\n            lbl = label_reshape[i, 0, j]\n            if lbl != ignore_index:\n                result[i, 0, j] -= np.log(softmax_reshape[i, lbl, j])\n    return result.reshape(label.shape)",
            "def cross_entropy(softmax, label, soft_label, axis, ignore_index=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if soft_label:\n        return (-label * np.log(softmax)).sum(axis=axis, keepdims=True)\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    axis_dim = shape[axis]\n    remain = int(np.prod(shape[axis + 1:]))\n    softmax_reshape = softmax.reshape((n, axis_dim, remain))\n    label_reshape = label.reshape((n, 1, remain))\n    result = np.zeros_like(label_reshape, dtype=softmax.dtype)\n    for i in range(n):\n        for j in range(remain):\n            lbl = label_reshape[i, 0, j]\n            if lbl != ignore_index:\n                result[i, 0, j] -= np.log(softmax_reshape[i, lbl, j])\n    return result.reshape(label.shape)",
            "def cross_entropy(softmax, label, soft_label, axis, ignore_index=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if soft_label:\n        return (-label * np.log(softmax)).sum(axis=axis, keepdims=True)\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    axis_dim = shape[axis]\n    remain = int(np.prod(shape[axis + 1:]))\n    softmax_reshape = softmax.reshape((n, axis_dim, remain))\n    label_reshape = label.reshape((n, 1, remain))\n    result = np.zeros_like(label_reshape, dtype=softmax.dtype)\n    for i in range(n):\n        for j in range(remain):\n            lbl = label_reshape[i, 0, j]\n            if lbl != ignore_index:\n                result[i, 0, j] -= np.log(softmax_reshape[i, lbl, j])\n    return result.reshape(label.shape)",
            "def cross_entropy(softmax, label, soft_label, axis, ignore_index=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if soft_label:\n        return (-label * np.log(softmax)).sum(axis=axis, keepdims=True)\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    axis_dim = shape[axis]\n    remain = int(np.prod(shape[axis + 1:]))\n    softmax_reshape = softmax.reshape((n, axis_dim, remain))\n    label_reshape = label.reshape((n, 1, remain))\n    result = np.zeros_like(label_reshape, dtype=softmax.dtype)\n    for i in range(n):\n        for j in range(remain):\n            lbl = label_reshape[i, 0, j]\n            if lbl != ignore_index:\n                result[i, 0, j] -= np.log(softmax_reshape[i, lbl, j])\n    return result.reshape(label.shape)"
        ]
    },
    {
        "func_name": "softmax_with_cross_entropy_grad",
        "original": "def softmax_with_cross_entropy_grad(softmax, label, loss_grad, axis):\n    logit_grad = softmax.copy()\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    d = int(np.prod(shape[axis:]))\n    for i in range(n * d):\n        row = int(i / d)\n        col = i % d\n        if col == label[row]:\n            logit_grad[row][col] = (logit_grad[row][col] - 1.0) * loss_grad[row]\n        else:\n            logit_grad[row][col] = logit_grad[row][col] * loss_grad[row]\n    return logit_grad",
        "mutated": [
            "def softmax_with_cross_entropy_grad(softmax, label, loss_grad, axis):\n    if False:\n        i = 10\n    logit_grad = softmax.copy()\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    d = int(np.prod(shape[axis:]))\n    for i in range(n * d):\n        row = int(i / d)\n        col = i % d\n        if col == label[row]:\n            logit_grad[row][col] = (logit_grad[row][col] - 1.0) * loss_grad[row]\n        else:\n            logit_grad[row][col] = logit_grad[row][col] * loss_grad[row]\n    return logit_grad",
            "def softmax_with_cross_entropy_grad(softmax, label, loss_grad, axis):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    logit_grad = softmax.copy()\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    d = int(np.prod(shape[axis:]))\n    for i in range(n * d):\n        row = int(i / d)\n        col = i % d\n        if col == label[row]:\n            logit_grad[row][col] = (logit_grad[row][col] - 1.0) * loss_grad[row]\n        else:\n            logit_grad[row][col] = logit_grad[row][col] * loss_grad[row]\n    return logit_grad",
            "def softmax_with_cross_entropy_grad(softmax, label, loss_grad, axis):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    logit_grad = softmax.copy()\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    d = int(np.prod(shape[axis:]))\n    for i in range(n * d):\n        row = int(i / d)\n        col = i % d\n        if col == label[row]:\n            logit_grad[row][col] = (logit_grad[row][col] - 1.0) * loss_grad[row]\n        else:\n            logit_grad[row][col] = logit_grad[row][col] * loss_grad[row]\n    return logit_grad",
            "def softmax_with_cross_entropy_grad(softmax, label, loss_grad, axis):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    logit_grad = softmax.copy()\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    d = int(np.prod(shape[axis:]))\n    for i in range(n * d):\n        row = int(i / d)\n        col = i % d\n        if col == label[row]:\n            logit_grad[row][col] = (logit_grad[row][col] - 1.0) * loss_grad[row]\n        else:\n            logit_grad[row][col] = logit_grad[row][col] * loss_grad[row]\n    return logit_grad",
            "def softmax_with_cross_entropy_grad(softmax, label, loss_grad, axis):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    logit_grad = softmax.copy()\n    shape = softmax.shape\n    axis %= len(shape)\n    n = int(np.prod(shape[:axis]))\n    d = int(np.prod(shape[axis:]))\n    for i in range(n * d):\n        row = int(i / d)\n        col = i % d\n        if col == label[row]:\n            logit_grad[row][col] = (logit_grad[row][col] - 1.0) * loss_grad[row]\n        else:\n            logit_grad[row][col] = logit_grad[row][col] * loss_grad[row]\n    return logit_grad"
        ]
    },
    {
        "func_name": "test_model",
        "original": "def test_model(self, data_type='float32'):\n    self.num_class = 1000\n    self.batch_size = 1024\n    fleet.init(is_collective=True)\n    strategy = fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    rank = fleet.worker_index()\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    ignore_index = label[0][0]\n    local_elements = int(self.num_class / 2)\n    np.random.seed(0)\n    input0 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    np.random.seed(1)\n    input1 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    inputs = np.concatenate((input0, input1), axis=1)\n    if rank == 0:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input0), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    else:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input1), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    paddle.device.cuda.synchronize()\n    softmax_list = []\n    paddle.distributed.all_gather(softmax_list, softmax)\n    need_softmax = np.apply_along_axis(stable_softmax, 1, inputs)\n    need_loss = cross_entropy(need_softmax, label, False, 1, ignore_index=ignore_index)\n    softmax = np.concatenate((softmax_list[0].numpy(), softmax_list[1].numpy()), axis=1)\n    rtol = 1e-06\n    np.testing.assert_allclose(loss.numpy(), need_loss, rtol=rtol)\n    np.testing.assert_allclose(softmax, need_softmax, rtol=rtol)",
        "mutated": [
            "def test_model(self, data_type='float32'):\n    if False:\n        i = 10\n    self.num_class = 1000\n    self.batch_size = 1024\n    fleet.init(is_collective=True)\n    strategy = fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    rank = fleet.worker_index()\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    ignore_index = label[0][0]\n    local_elements = int(self.num_class / 2)\n    np.random.seed(0)\n    input0 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    np.random.seed(1)\n    input1 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    inputs = np.concatenate((input0, input1), axis=1)\n    if rank == 0:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input0), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    else:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input1), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    paddle.device.cuda.synchronize()\n    softmax_list = []\n    paddle.distributed.all_gather(softmax_list, softmax)\n    need_softmax = np.apply_along_axis(stable_softmax, 1, inputs)\n    need_loss = cross_entropy(need_softmax, label, False, 1, ignore_index=ignore_index)\n    softmax = np.concatenate((softmax_list[0].numpy(), softmax_list[1].numpy()), axis=1)\n    rtol = 1e-06\n    np.testing.assert_allclose(loss.numpy(), need_loss, rtol=rtol)\n    np.testing.assert_allclose(softmax, need_softmax, rtol=rtol)",
            "def test_model(self, data_type='float32'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.num_class = 1000\n    self.batch_size = 1024\n    fleet.init(is_collective=True)\n    strategy = fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    rank = fleet.worker_index()\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    ignore_index = label[0][0]\n    local_elements = int(self.num_class / 2)\n    np.random.seed(0)\n    input0 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    np.random.seed(1)\n    input1 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    inputs = np.concatenate((input0, input1), axis=1)\n    if rank == 0:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input0), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    else:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input1), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    paddle.device.cuda.synchronize()\n    softmax_list = []\n    paddle.distributed.all_gather(softmax_list, softmax)\n    need_softmax = np.apply_along_axis(stable_softmax, 1, inputs)\n    need_loss = cross_entropy(need_softmax, label, False, 1, ignore_index=ignore_index)\n    softmax = np.concatenate((softmax_list[0].numpy(), softmax_list[1].numpy()), axis=1)\n    rtol = 1e-06\n    np.testing.assert_allclose(loss.numpy(), need_loss, rtol=rtol)\n    np.testing.assert_allclose(softmax, need_softmax, rtol=rtol)",
            "def test_model(self, data_type='float32'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.num_class = 1000\n    self.batch_size = 1024\n    fleet.init(is_collective=True)\n    strategy = fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    rank = fleet.worker_index()\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    ignore_index = label[0][0]\n    local_elements = int(self.num_class / 2)\n    np.random.seed(0)\n    input0 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    np.random.seed(1)\n    input1 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    inputs = np.concatenate((input0, input1), axis=1)\n    if rank == 0:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input0), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    else:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input1), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    paddle.device.cuda.synchronize()\n    softmax_list = []\n    paddle.distributed.all_gather(softmax_list, softmax)\n    need_softmax = np.apply_along_axis(stable_softmax, 1, inputs)\n    need_loss = cross_entropy(need_softmax, label, False, 1, ignore_index=ignore_index)\n    softmax = np.concatenate((softmax_list[0].numpy(), softmax_list[1].numpy()), axis=1)\n    rtol = 1e-06\n    np.testing.assert_allclose(loss.numpy(), need_loss, rtol=rtol)\n    np.testing.assert_allclose(softmax, need_softmax, rtol=rtol)",
            "def test_model(self, data_type='float32'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.num_class = 1000\n    self.batch_size = 1024\n    fleet.init(is_collective=True)\n    strategy = fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    rank = fleet.worker_index()\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    ignore_index = label[0][0]\n    local_elements = int(self.num_class / 2)\n    np.random.seed(0)\n    input0 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    np.random.seed(1)\n    input1 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    inputs = np.concatenate((input0, input1), axis=1)\n    if rank == 0:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input0), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    else:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input1), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    paddle.device.cuda.synchronize()\n    softmax_list = []\n    paddle.distributed.all_gather(softmax_list, softmax)\n    need_softmax = np.apply_along_axis(stable_softmax, 1, inputs)\n    need_loss = cross_entropy(need_softmax, label, False, 1, ignore_index=ignore_index)\n    softmax = np.concatenate((softmax_list[0].numpy(), softmax_list[1].numpy()), axis=1)\n    rtol = 1e-06\n    np.testing.assert_allclose(loss.numpy(), need_loss, rtol=rtol)\n    np.testing.assert_allclose(softmax, need_softmax, rtol=rtol)",
            "def test_model(self, data_type='float32'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.num_class = 1000\n    self.batch_size = 1024\n    fleet.init(is_collective=True)\n    strategy = fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    rank = fleet.worker_index()\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    ignore_index = label[0][0]\n    local_elements = int(self.num_class / 2)\n    np.random.seed(0)\n    input0 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    np.random.seed(1)\n    input1 = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, local_elements)).astype(data_type)\n    inputs = np.concatenate((input0, input1), axis=1)\n    if rank == 0:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input0), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    else:\n        (loss, softmax) = _c_softmax_with_cross_entropy(paddle.to_tensor(input1), paddle.to_tensor(label), ignore_index=ignore_index, return_softmax=True)\n    paddle.device.cuda.synchronize()\n    softmax_list = []\n    paddle.distributed.all_gather(softmax_list, softmax)\n    need_softmax = np.apply_along_axis(stable_softmax, 1, inputs)\n    need_loss = cross_entropy(need_softmax, label, False, 1, ignore_index=ignore_index)\n    softmax = np.concatenate((softmax_list[0].numpy(), softmax_list[1].numpy()), axis=1)\n    rtol = 1e-06\n    np.testing.assert_allclose(loss.numpy(), need_loss, rtol=rtol)\n    np.testing.assert_allclose(softmax, need_softmax, rtol=rtol)"
        ]
    }
]