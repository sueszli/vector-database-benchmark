[
    {
        "func_name": "predict",
        "original": "def predict(analogy):\n    analogy_a = analogy[:, 0]\n    analogy_b = analogy[:, 1]\n    analogy_c = analogy[:, 2]\n    a_emb = tf.gather(normalized_embeddings, analogy_a)\n    b_emb = tf.gather(normalized_embeddings, analogy_b)\n    c_emb = tf.gather(normalized_embeddings, analogy_c)\n    target = c_emb + (b_emb - a_emb)\n    dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n    'Predict the top 4 answers for analogy questions.'\n    (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n    return pred_idx",
        "mutated": [
            "def predict(analogy):\n    if False:\n        i = 10\n    analogy_a = analogy[:, 0]\n    analogy_b = analogy[:, 1]\n    analogy_c = analogy[:, 2]\n    a_emb = tf.gather(normalized_embeddings, analogy_a)\n    b_emb = tf.gather(normalized_embeddings, analogy_b)\n    c_emb = tf.gather(normalized_embeddings, analogy_c)\n    target = c_emb + (b_emb - a_emb)\n    dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n    'Predict the top 4 answers for analogy questions.'\n    (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n    return pred_idx",
            "def predict(analogy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    analogy_a = analogy[:, 0]\n    analogy_b = analogy[:, 1]\n    analogy_c = analogy[:, 2]\n    a_emb = tf.gather(normalized_embeddings, analogy_a)\n    b_emb = tf.gather(normalized_embeddings, analogy_b)\n    c_emb = tf.gather(normalized_embeddings, analogy_c)\n    target = c_emb + (b_emb - a_emb)\n    dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n    'Predict the top 4 answers for analogy questions.'\n    (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n    return pred_idx",
            "def predict(analogy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    analogy_a = analogy[:, 0]\n    analogy_b = analogy[:, 1]\n    analogy_c = analogy[:, 2]\n    a_emb = tf.gather(normalized_embeddings, analogy_a)\n    b_emb = tf.gather(normalized_embeddings, analogy_b)\n    c_emb = tf.gather(normalized_embeddings, analogy_c)\n    target = c_emb + (b_emb - a_emb)\n    dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n    'Predict the top 4 answers for analogy questions.'\n    (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n    return pred_idx",
            "def predict(analogy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    analogy_a = analogy[:, 0]\n    analogy_b = analogy[:, 1]\n    analogy_c = analogy[:, 2]\n    a_emb = tf.gather(normalized_embeddings, analogy_a)\n    b_emb = tf.gather(normalized_embeddings, analogy_b)\n    c_emb = tf.gather(normalized_embeddings, analogy_c)\n    target = c_emb + (b_emb - a_emb)\n    dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n    'Predict the top 4 answers for analogy questions.'\n    (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n    return pred_idx",
            "def predict(analogy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    analogy_a = analogy[:, 0]\n    analogy_b = analogy[:, 1]\n    analogy_c = analogy[:, 2]\n    a_emb = tf.gather(normalized_embeddings, analogy_a)\n    b_emb = tf.gather(normalized_embeddings, analogy_b)\n    c_emb = tf.gather(normalized_embeddings, analogy_c)\n    target = c_emb + (b_emb - a_emb)\n    dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n    'Predict the top 4 answers for analogy questions.'\n    (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n    return pred_idx"
        ]
    },
    {
        "func_name": "main_word2vec_basic",
        "original": "def main_word2vec_basic():\n    words = tl.files.load_matt_mahoney_text8_dataset()\n    data_size = len(words)\n    print(data_size)\n    print(words[0:10])\n    resume = False\n    _UNK = '_UNK'\n    if FLAGS.model == 'one':\n        vocabulary_size = 50000\n        batch_size = 128\n        embedding_size = 128\n        skip_window = 1\n        num_skips = 2\n        num_sampled = 64\n        learning_rate = 1.0\n        n_epoch = 20\n        model_file_name = 'model_word2vec_50k_128'\n    elif FLAGS.model == 'two':\n        vocabulary_size = 80000\n        batch_size = 20\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 100\n        learning_rate = 0.2\n        n_epoch = 15\n        model_file_name = 'model_word2vec_80k_200'\n    elif FLAGS.model == 'three':\n        vocabulary_size = 80000\n        batch_size = 500\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.025\n        n_epoch = 20\n        model_file_name = 'model_word2vec_80k_200_opt'\n    elif FLAGS.model == 'four':\n        vocabulary_size = 80000\n        batch_size = 100\n        embedding_size = 600\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.03\n        n_epoch = 200 * 10\n        model_file_name = 'model_word2vec_80k_600'\n    else:\n        raise Exception('Invalid model: %s' % FLAGS.model)\n    num_steps = int(data_size / batch_size * n_epoch)\n    print('%d Steps in a Epoch, total Epochs %d' % (int(data_size / batch_size), n_epoch))\n    print('   learning_rate: %f' % learning_rate)\n    print('   batch_size: %d' % batch_size)\n    print()\n    if resume:\n        print('Load existing data and dictionaries' + '!' * 10)\n        all_var = tl.files.load_npy_to_any(name=model_file_name + '.npy')\n        data = all_var['data']\n        count = all_var['count']\n        dictionary = all_var['dictionary']\n        reverse_dictionary = all_var['reverse_dictionary']\n    else:\n        (data, count, dictionary, reverse_dictionary) = tl.nlp.build_words_dataset(words, vocabulary_size, True, _UNK)\n    print('Most 5 common words (+UNK)', count[:5])\n    print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n    del words\n    print()\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=4, skip_window=2, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    print()\n    valid_size = 16\n    valid_window = 100\n    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n    labels = tl.layers.Input([batch_size, 1], dtype=tf.int32)\n    emb_net = tl.layers.Word2vecEmbedding(vocabulary_size=vocabulary_size, embedding_size=embedding_size, num_sampled=num_sampled, activate_nce_loss=True, nce_loss_args={}, E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0), nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))), nce_b_init=tl.initializers.constant(value=0.0), name='word2vec_layer')\n    (emb, nce) = emb_net([inputs, labels])\n    model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name='word2vec_model')\n    optimizer = tf.optimizers.Adagrad(learning_rate, initial_accumulator_value=0.1)\n    normalized_embeddings = emb_net.normalized_embeddings\n    model.train()\n    if resume:\n        print('Load existing model' + '!' * 10)\n        model.load_weights(filepath=model_file_name + '.hdf5')\n    tl.nlp.save_vocab(count, name='vocab_text8.txt')\n    average_loss = 0\n    step = 0\n    print_freq = 2000\n    while step < num_steps:\n        start_time = time.time()\n        (batch_inputs, batch_labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window, data_index=data_index)\n        with tf.GradientTape() as tape:\n            (outputs, nce_cost) = model([batch_inputs, batch_labels])\n        grad = tape.gradient(nce_cost, model.trainable_weights)\n        optimizer.apply_gradients(zip(grad, model.trainable_weights))\n        average_loss += nce_cost\n        if step % print_freq == 0:\n            if step > 0:\n                average_loss /= print_freq\n            print('Average loss at step %d/%d. loss: %f took: %fs/per step' % (step, num_steps, average_loss, time.time() - start_time))\n            average_loss = 0\n        if step % (print_freq * 5) == 0:\n            valid_embed = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n            sim = tf.matmul(valid_embed, normalized_embeddings, transpose_b=True)\n            sim = sim.numpy()\n            for i in xrange(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = 'Nearest to %s:' % valid_word\n                for k in xrange(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = '%s %s,' % (log_str, close_word)\n                print(log_str)\n        if step % (print_freq * 20) == 0 and step != 0:\n            print('Save model, data and dictionaries' + '!' * 10)\n            model.save_weights(filepath=model_file_name + '.hdf5')\n            tl.files.save_any_to_npy(save_dict={'data': data, 'count': count, 'dictionary': dictionary, 'reverse_dictionary': reverse_dictionary}, name=model_file_name + '.npy')\n        step += 1\n    print()\n    final_embeddings = normalized_embeddings\n    tl.visualize.tsne_embedding(final_embeddings, reverse_dictionary, plot_only=500, second=5, saveable=False, name='word2vec_basic')\n    print()\n    model.eval()\n    if not os.path.exists('questions-words.txt'):\n        print(\"Downloading file 'questions-words.txt'\")\n        wget.download('http://download.tensorflow.org/data/questions-words.txt')\n    analogy_questions = tl.nlp.read_analogies_file(eval_file='questions-words.txt', word2id=dictionary)\n    n_answer = 4\n\n    def predict(analogy):\n        analogy_a = analogy[:, 0]\n        analogy_b = analogy[:, 1]\n        analogy_c = analogy[:, 2]\n        a_emb = tf.gather(normalized_embeddings, analogy_a)\n        b_emb = tf.gather(normalized_embeddings, analogy_b)\n        c_emb = tf.gather(normalized_embeddings, analogy_c)\n        target = c_emb + (b_emb - a_emb)\n        dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n        'Predict the top 4 answers for analogy questions.'\n        (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n        return pred_idx\n    correct = 0\n    total = analogy_questions.shape[0]\n    start = 0\n    while start < total:\n        limit = start + 2500\n        sub = analogy_questions[start:limit, :]\n        idx = predict(sub)\n        start = limit\n        for question in xrange(sub.shape[0]):\n            for j in xrange(n_answer):\n                if idx[question, j] == sub[question, 3]:\n                    print(j + 1, tl.nlp.word_ids_to_words([idx[question, j]], reverse_dictionary), ':', tl.nlp.word_ids_to_words(sub[question, :], reverse_dictionary))\n                    correct += 1\n                    break\n                elif idx[question, j] in sub[question, :3]:\n                    continue\n                else:\n                    break\n    print('Eval %4d/%d accuracy = %4.1f%%' % (correct, total, correct * 100.0 / total))",
        "mutated": [
            "def main_word2vec_basic():\n    if False:\n        i = 10\n    words = tl.files.load_matt_mahoney_text8_dataset()\n    data_size = len(words)\n    print(data_size)\n    print(words[0:10])\n    resume = False\n    _UNK = '_UNK'\n    if FLAGS.model == 'one':\n        vocabulary_size = 50000\n        batch_size = 128\n        embedding_size = 128\n        skip_window = 1\n        num_skips = 2\n        num_sampled = 64\n        learning_rate = 1.0\n        n_epoch = 20\n        model_file_name = 'model_word2vec_50k_128'\n    elif FLAGS.model == 'two':\n        vocabulary_size = 80000\n        batch_size = 20\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 100\n        learning_rate = 0.2\n        n_epoch = 15\n        model_file_name = 'model_word2vec_80k_200'\n    elif FLAGS.model == 'three':\n        vocabulary_size = 80000\n        batch_size = 500\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.025\n        n_epoch = 20\n        model_file_name = 'model_word2vec_80k_200_opt'\n    elif FLAGS.model == 'four':\n        vocabulary_size = 80000\n        batch_size = 100\n        embedding_size = 600\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.03\n        n_epoch = 200 * 10\n        model_file_name = 'model_word2vec_80k_600'\n    else:\n        raise Exception('Invalid model: %s' % FLAGS.model)\n    num_steps = int(data_size / batch_size * n_epoch)\n    print('%d Steps in a Epoch, total Epochs %d' % (int(data_size / batch_size), n_epoch))\n    print('   learning_rate: %f' % learning_rate)\n    print('   batch_size: %d' % batch_size)\n    print()\n    if resume:\n        print('Load existing data and dictionaries' + '!' * 10)\n        all_var = tl.files.load_npy_to_any(name=model_file_name + '.npy')\n        data = all_var['data']\n        count = all_var['count']\n        dictionary = all_var['dictionary']\n        reverse_dictionary = all_var['reverse_dictionary']\n    else:\n        (data, count, dictionary, reverse_dictionary) = tl.nlp.build_words_dataset(words, vocabulary_size, True, _UNK)\n    print('Most 5 common words (+UNK)', count[:5])\n    print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n    del words\n    print()\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=4, skip_window=2, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    print()\n    valid_size = 16\n    valid_window = 100\n    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n    labels = tl.layers.Input([batch_size, 1], dtype=tf.int32)\n    emb_net = tl.layers.Word2vecEmbedding(vocabulary_size=vocabulary_size, embedding_size=embedding_size, num_sampled=num_sampled, activate_nce_loss=True, nce_loss_args={}, E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0), nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))), nce_b_init=tl.initializers.constant(value=0.0), name='word2vec_layer')\n    (emb, nce) = emb_net([inputs, labels])\n    model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name='word2vec_model')\n    optimizer = tf.optimizers.Adagrad(learning_rate, initial_accumulator_value=0.1)\n    normalized_embeddings = emb_net.normalized_embeddings\n    model.train()\n    if resume:\n        print('Load existing model' + '!' * 10)\n        model.load_weights(filepath=model_file_name + '.hdf5')\n    tl.nlp.save_vocab(count, name='vocab_text8.txt')\n    average_loss = 0\n    step = 0\n    print_freq = 2000\n    while step < num_steps:\n        start_time = time.time()\n        (batch_inputs, batch_labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window, data_index=data_index)\n        with tf.GradientTape() as tape:\n            (outputs, nce_cost) = model([batch_inputs, batch_labels])\n        grad = tape.gradient(nce_cost, model.trainable_weights)\n        optimizer.apply_gradients(zip(grad, model.trainable_weights))\n        average_loss += nce_cost\n        if step % print_freq == 0:\n            if step > 0:\n                average_loss /= print_freq\n            print('Average loss at step %d/%d. loss: %f took: %fs/per step' % (step, num_steps, average_loss, time.time() - start_time))\n            average_loss = 0\n        if step % (print_freq * 5) == 0:\n            valid_embed = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n            sim = tf.matmul(valid_embed, normalized_embeddings, transpose_b=True)\n            sim = sim.numpy()\n            for i in xrange(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = 'Nearest to %s:' % valid_word\n                for k in xrange(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = '%s %s,' % (log_str, close_word)\n                print(log_str)\n        if step % (print_freq * 20) == 0 and step != 0:\n            print('Save model, data and dictionaries' + '!' * 10)\n            model.save_weights(filepath=model_file_name + '.hdf5')\n            tl.files.save_any_to_npy(save_dict={'data': data, 'count': count, 'dictionary': dictionary, 'reverse_dictionary': reverse_dictionary}, name=model_file_name + '.npy')\n        step += 1\n    print()\n    final_embeddings = normalized_embeddings\n    tl.visualize.tsne_embedding(final_embeddings, reverse_dictionary, plot_only=500, second=5, saveable=False, name='word2vec_basic')\n    print()\n    model.eval()\n    if not os.path.exists('questions-words.txt'):\n        print(\"Downloading file 'questions-words.txt'\")\n        wget.download('http://download.tensorflow.org/data/questions-words.txt')\n    analogy_questions = tl.nlp.read_analogies_file(eval_file='questions-words.txt', word2id=dictionary)\n    n_answer = 4\n\n    def predict(analogy):\n        analogy_a = analogy[:, 0]\n        analogy_b = analogy[:, 1]\n        analogy_c = analogy[:, 2]\n        a_emb = tf.gather(normalized_embeddings, analogy_a)\n        b_emb = tf.gather(normalized_embeddings, analogy_b)\n        c_emb = tf.gather(normalized_embeddings, analogy_c)\n        target = c_emb + (b_emb - a_emb)\n        dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n        'Predict the top 4 answers for analogy questions.'\n        (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n        return pred_idx\n    correct = 0\n    total = analogy_questions.shape[0]\n    start = 0\n    while start < total:\n        limit = start + 2500\n        sub = analogy_questions[start:limit, :]\n        idx = predict(sub)\n        start = limit\n        for question in xrange(sub.shape[0]):\n            for j in xrange(n_answer):\n                if idx[question, j] == sub[question, 3]:\n                    print(j + 1, tl.nlp.word_ids_to_words([idx[question, j]], reverse_dictionary), ':', tl.nlp.word_ids_to_words(sub[question, :], reverse_dictionary))\n                    correct += 1\n                    break\n                elif idx[question, j] in sub[question, :3]:\n                    continue\n                else:\n                    break\n    print('Eval %4d/%d accuracy = %4.1f%%' % (correct, total, correct * 100.0 / total))",
            "def main_word2vec_basic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    words = tl.files.load_matt_mahoney_text8_dataset()\n    data_size = len(words)\n    print(data_size)\n    print(words[0:10])\n    resume = False\n    _UNK = '_UNK'\n    if FLAGS.model == 'one':\n        vocabulary_size = 50000\n        batch_size = 128\n        embedding_size = 128\n        skip_window = 1\n        num_skips = 2\n        num_sampled = 64\n        learning_rate = 1.0\n        n_epoch = 20\n        model_file_name = 'model_word2vec_50k_128'\n    elif FLAGS.model == 'two':\n        vocabulary_size = 80000\n        batch_size = 20\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 100\n        learning_rate = 0.2\n        n_epoch = 15\n        model_file_name = 'model_word2vec_80k_200'\n    elif FLAGS.model == 'three':\n        vocabulary_size = 80000\n        batch_size = 500\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.025\n        n_epoch = 20\n        model_file_name = 'model_word2vec_80k_200_opt'\n    elif FLAGS.model == 'four':\n        vocabulary_size = 80000\n        batch_size = 100\n        embedding_size = 600\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.03\n        n_epoch = 200 * 10\n        model_file_name = 'model_word2vec_80k_600'\n    else:\n        raise Exception('Invalid model: %s' % FLAGS.model)\n    num_steps = int(data_size / batch_size * n_epoch)\n    print('%d Steps in a Epoch, total Epochs %d' % (int(data_size / batch_size), n_epoch))\n    print('   learning_rate: %f' % learning_rate)\n    print('   batch_size: %d' % batch_size)\n    print()\n    if resume:\n        print('Load existing data and dictionaries' + '!' * 10)\n        all_var = tl.files.load_npy_to_any(name=model_file_name + '.npy')\n        data = all_var['data']\n        count = all_var['count']\n        dictionary = all_var['dictionary']\n        reverse_dictionary = all_var['reverse_dictionary']\n    else:\n        (data, count, dictionary, reverse_dictionary) = tl.nlp.build_words_dataset(words, vocabulary_size, True, _UNK)\n    print('Most 5 common words (+UNK)', count[:5])\n    print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n    del words\n    print()\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=4, skip_window=2, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    print()\n    valid_size = 16\n    valid_window = 100\n    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n    labels = tl.layers.Input([batch_size, 1], dtype=tf.int32)\n    emb_net = tl.layers.Word2vecEmbedding(vocabulary_size=vocabulary_size, embedding_size=embedding_size, num_sampled=num_sampled, activate_nce_loss=True, nce_loss_args={}, E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0), nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))), nce_b_init=tl.initializers.constant(value=0.0), name='word2vec_layer')\n    (emb, nce) = emb_net([inputs, labels])\n    model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name='word2vec_model')\n    optimizer = tf.optimizers.Adagrad(learning_rate, initial_accumulator_value=0.1)\n    normalized_embeddings = emb_net.normalized_embeddings\n    model.train()\n    if resume:\n        print('Load existing model' + '!' * 10)\n        model.load_weights(filepath=model_file_name + '.hdf5')\n    tl.nlp.save_vocab(count, name='vocab_text8.txt')\n    average_loss = 0\n    step = 0\n    print_freq = 2000\n    while step < num_steps:\n        start_time = time.time()\n        (batch_inputs, batch_labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window, data_index=data_index)\n        with tf.GradientTape() as tape:\n            (outputs, nce_cost) = model([batch_inputs, batch_labels])\n        grad = tape.gradient(nce_cost, model.trainable_weights)\n        optimizer.apply_gradients(zip(grad, model.trainable_weights))\n        average_loss += nce_cost\n        if step % print_freq == 0:\n            if step > 0:\n                average_loss /= print_freq\n            print('Average loss at step %d/%d. loss: %f took: %fs/per step' % (step, num_steps, average_loss, time.time() - start_time))\n            average_loss = 0\n        if step % (print_freq * 5) == 0:\n            valid_embed = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n            sim = tf.matmul(valid_embed, normalized_embeddings, transpose_b=True)\n            sim = sim.numpy()\n            for i in xrange(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = 'Nearest to %s:' % valid_word\n                for k in xrange(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = '%s %s,' % (log_str, close_word)\n                print(log_str)\n        if step % (print_freq * 20) == 0 and step != 0:\n            print('Save model, data and dictionaries' + '!' * 10)\n            model.save_weights(filepath=model_file_name + '.hdf5')\n            tl.files.save_any_to_npy(save_dict={'data': data, 'count': count, 'dictionary': dictionary, 'reverse_dictionary': reverse_dictionary}, name=model_file_name + '.npy')\n        step += 1\n    print()\n    final_embeddings = normalized_embeddings\n    tl.visualize.tsne_embedding(final_embeddings, reverse_dictionary, plot_only=500, second=5, saveable=False, name='word2vec_basic')\n    print()\n    model.eval()\n    if not os.path.exists('questions-words.txt'):\n        print(\"Downloading file 'questions-words.txt'\")\n        wget.download('http://download.tensorflow.org/data/questions-words.txt')\n    analogy_questions = tl.nlp.read_analogies_file(eval_file='questions-words.txt', word2id=dictionary)\n    n_answer = 4\n\n    def predict(analogy):\n        analogy_a = analogy[:, 0]\n        analogy_b = analogy[:, 1]\n        analogy_c = analogy[:, 2]\n        a_emb = tf.gather(normalized_embeddings, analogy_a)\n        b_emb = tf.gather(normalized_embeddings, analogy_b)\n        c_emb = tf.gather(normalized_embeddings, analogy_c)\n        target = c_emb + (b_emb - a_emb)\n        dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n        'Predict the top 4 answers for analogy questions.'\n        (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n        return pred_idx\n    correct = 0\n    total = analogy_questions.shape[0]\n    start = 0\n    while start < total:\n        limit = start + 2500\n        sub = analogy_questions[start:limit, :]\n        idx = predict(sub)\n        start = limit\n        for question in xrange(sub.shape[0]):\n            for j in xrange(n_answer):\n                if idx[question, j] == sub[question, 3]:\n                    print(j + 1, tl.nlp.word_ids_to_words([idx[question, j]], reverse_dictionary), ':', tl.nlp.word_ids_to_words(sub[question, :], reverse_dictionary))\n                    correct += 1\n                    break\n                elif idx[question, j] in sub[question, :3]:\n                    continue\n                else:\n                    break\n    print('Eval %4d/%d accuracy = %4.1f%%' % (correct, total, correct * 100.0 / total))",
            "def main_word2vec_basic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    words = tl.files.load_matt_mahoney_text8_dataset()\n    data_size = len(words)\n    print(data_size)\n    print(words[0:10])\n    resume = False\n    _UNK = '_UNK'\n    if FLAGS.model == 'one':\n        vocabulary_size = 50000\n        batch_size = 128\n        embedding_size = 128\n        skip_window = 1\n        num_skips = 2\n        num_sampled = 64\n        learning_rate = 1.0\n        n_epoch = 20\n        model_file_name = 'model_word2vec_50k_128'\n    elif FLAGS.model == 'two':\n        vocabulary_size = 80000\n        batch_size = 20\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 100\n        learning_rate = 0.2\n        n_epoch = 15\n        model_file_name = 'model_word2vec_80k_200'\n    elif FLAGS.model == 'three':\n        vocabulary_size = 80000\n        batch_size = 500\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.025\n        n_epoch = 20\n        model_file_name = 'model_word2vec_80k_200_opt'\n    elif FLAGS.model == 'four':\n        vocabulary_size = 80000\n        batch_size = 100\n        embedding_size = 600\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.03\n        n_epoch = 200 * 10\n        model_file_name = 'model_word2vec_80k_600'\n    else:\n        raise Exception('Invalid model: %s' % FLAGS.model)\n    num_steps = int(data_size / batch_size * n_epoch)\n    print('%d Steps in a Epoch, total Epochs %d' % (int(data_size / batch_size), n_epoch))\n    print('   learning_rate: %f' % learning_rate)\n    print('   batch_size: %d' % batch_size)\n    print()\n    if resume:\n        print('Load existing data and dictionaries' + '!' * 10)\n        all_var = tl.files.load_npy_to_any(name=model_file_name + '.npy')\n        data = all_var['data']\n        count = all_var['count']\n        dictionary = all_var['dictionary']\n        reverse_dictionary = all_var['reverse_dictionary']\n    else:\n        (data, count, dictionary, reverse_dictionary) = tl.nlp.build_words_dataset(words, vocabulary_size, True, _UNK)\n    print('Most 5 common words (+UNK)', count[:5])\n    print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n    del words\n    print()\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=4, skip_window=2, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    print()\n    valid_size = 16\n    valid_window = 100\n    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n    labels = tl.layers.Input([batch_size, 1], dtype=tf.int32)\n    emb_net = tl.layers.Word2vecEmbedding(vocabulary_size=vocabulary_size, embedding_size=embedding_size, num_sampled=num_sampled, activate_nce_loss=True, nce_loss_args={}, E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0), nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))), nce_b_init=tl.initializers.constant(value=0.0), name='word2vec_layer')\n    (emb, nce) = emb_net([inputs, labels])\n    model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name='word2vec_model')\n    optimizer = tf.optimizers.Adagrad(learning_rate, initial_accumulator_value=0.1)\n    normalized_embeddings = emb_net.normalized_embeddings\n    model.train()\n    if resume:\n        print('Load existing model' + '!' * 10)\n        model.load_weights(filepath=model_file_name + '.hdf5')\n    tl.nlp.save_vocab(count, name='vocab_text8.txt')\n    average_loss = 0\n    step = 0\n    print_freq = 2000\n    while step < num_steps:\n        start_time = time.time()\n        (batch_inputs, batch_labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window, data_index=data_index)\n        with tf.GradientTape() as tape:\n            (outputs, nce_cost) = model([batch_inputs, batch_labels])\n        grad = tape.gradient(nce_cost, model.trainable_weights)\n        optimizer.apply_gradients(zip(grad, model.trainable_weights))\n        average_loss += nce_cost\n        if step % print_freq == 0:\n            if step > 0:\n                average_loss /= print_freq\n            print('Average loss at step %d/%d. loss: %f took: %fs/per step' % (step, num_steps, average_loss, time.time() - start_time))\n            average_loss = 0\n        if step % (print_freq * 5) == 0:\n            valid_embed = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n            sim = tf.matmul(valid_embed, normalized_embeddings, transpose_b=True)\n            sim = sim.numpy()\n            for i in xrange(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = 'Nearest to %s:' % valid_word\n                for k in xrange(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = '%s %s,' % (log_str, close_word)\n                print(log_str)\n        if step % (print_freq * 20) == 0 and step != 0:\n            print('Save model, data and dictionaries' + '!' * 10)\n            model.save_weights(filepath=model_file_name + '.hdf5')\n            tl.files.save_any_to_npy(save_dict={'data': data, 'count': count, 'dictionary': dictionary, 'reverse_dictionary': reverse_dictionary}, name=model_file_name + '.npy')\n        step += 1\n    print()\n    final_embeddings = normalized_embeddings\n    tl.visualize.tsne_embedding(final_embeddings, reverse_dictionary, plot_only=500, second=5, saveable=False, name='word2vec_basic')\n    print()\n    model.eval()\n    if not os.path.exists('questions-words.txt'):\n        print(\"Downloading file 'questions-words.txt'\")\n        wget.download('http://download.tensorflow.org/data/questions-words.txt')\n    analogy_questions = tl.nlp.read_analogies_file(eval_file='questions-words.txt', word2id=dictionary)\n    n_answer = 4\n\n    def predict(analogy):\n        analogy_a = analogy[:, 0]\n        analogy_b = analogy[:, 1]\n        analogy_c = analogy[:, 2]\n        a_emb = tf.gather(normalized_embeddings, analogy_a)\n        b_emb = tf.gather(normalized_embeddings, analogy_b)\n        c_emb = tf.gather(normalized_embeddings, analogy_c)\n        target = c_emb + (b_emb - a_emb)\n        dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n        'Predict the top 4 answers for analogy questions.'\n        (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n        return pred_idx\n    correct = 0\n    total = analogy_questions.shape[0]\n    start = 0\n    while start < total:\n        limit = start + 2500\n        sub = analogy_questions[start:limit, :]\n        idx = predict(sub)\n        start = limit\n        for question in xrange(sub.shape[0]):\n            for j in xrange(n_answer):\n                if idx[question, j] == sub[question, 3]:\n                    print(j + 1, tl.nlp.word_ids_to_words([idx[question, j]], reverse_dictionary), ':', tl.nlp.word_ids_to_words(sub[question, :], reverse_dictionary))\n                    correct += 1\n                    break\n                elif idx[question, j] in sub[question, :3]:\n                    continue\n                else:\n                    break\n    print('Eval %4d/%d accuracy = %4.1f%%' % (correct, total, correct * 100.0 / total))",
            "def main_word2vec_basic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    words = tl.files.load_matt_mahoney_text8_dataset()\n    data_size = len(words)\n    print(data_size)\n    print(words[0:10])\n    resume = False\n    _UNK = '_UNK'\n    if FLAGS.model == 'one':\n        vocabulary_size = 50000\n        batch_size = 128\n        embedding_size = 128\n        skip_window = 1\n        num_skips = 2\n        num_sampled = 64\n        learning_rate = 1.0\n        n_epoch = 20\n        model_file_name = 'model_word2vec_50k_128'\n    elif FLAGS.model == 'two':\n        vocabulary_size = 80000\n        batch_size = 20\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 100\n        learning_rate = 0.2\n        n_epoch = 15\n        model_file_name = 'model_word2vec_80k_200'\n    elif FLAGS.model == 'three':\n        vocabulary_size = 80000\n        batch_size = 500\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.025\n        n_epoch = 20\n        model_file_name = 'model_word2vec_80k_200_opt'\n    elif FLAGS.model == 'four':\n        vocabulary_size = 80000\n        batch_size = 100\n        embedding_size = 600\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.03\n        n_epoch = 200 * 10\n        model_file_name = 'model_word2vec_80k_600'\n    else:\n        raise Exception('Invalid model: %s' % FLAGS.model)\n    num_steps = int(data_size / batch_size * n_epoch)\n    print('%d Steps in a Epoch, total Epochs %d' % (int(data_size / batch_size), n_epoch))\n    print('   learning_rate: %f' % learning_rate)\n    print('   batch_size: %d' % batch_size)\n    print()\n    if resume:\n        print('Load existing data and dictionaries' + '!' * 10)\n        all_var = tl.files.load_npy_to_any(name=model_file_name + '.npy')\n        data = all_var['data']\n        count = all_var['count']\n        dictionary = all_var['dictionary']\n        reverse_dictionary = all_var['reverse_dictionary']\n    else:\n        (data, count, dictionary, reverse_dictionary) = tl.nlp.build_words_dataset(words, vocabulary_size, True, _UNK)\n    print('Most 5 common words (+UNK)', count[:5])\n    print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n    del words\n    print()\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=4, skip_window=2, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    print()\n    valid_size = 16\n    valid_window = 100\n    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n    labels = tl.layers.Input([batch_size, 1], dtype=tf.int32)\n    emb_net = tl.layers.Word2vecEmbedding(vocabulary_size=vocabulary_size, embedding_size=embedding_size, num_sampled=num_sampled, activate_nce_loss=True, nce_loss_args={}, E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0), nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))), nce_b_init=tl.initializers.constant(value=0.0), name='word2vec_layer')\n    (emb, nce) = emb_net([inputs, labels])\n    model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name='word2vec_model')\n    optimizer = tf.optimizers.Adagrad(learning_rate, initial_accumulator_value=0.1)\n    normalized_embeddings = emb_net.normalized_embeddings\n    model.train()\n    if resume:\n        print('Load existing model' + '!' * 10)\n        model.load_weights(filepath=model_file_name + '.hdf5')\n    tl.nlp.save_vocab(count, name='vocab_text8.txt')\n    average_loss = 0\n    step = 0\n    print_freq = 2000\n    while step < num_steps:\n        start_time = time.time()\n        (batch_inputs, batch_labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window, data_index=data_index)\n        with tf.GradientTape() as tape:\n            (outputs, nce_cost) = model([batch_inputs, batch_labels])\n        grad = tape.gradient(nce_cost, model.trainable_weights)\n        optimizer.apply_gradients(zip(grad, model.trainable_weights))\n        average_loss += nce_cost\n        if step % print_freq == 0:\n            if step > 0:\n                average_loss /= print_freq\n            print('Average loss at step %d/%d. loss: %f took: %fs/per step' % (step, num_steps, average_loss, time.time() - start_time))\n            average_loss = 0\n        if step % (print_freq * 5) == 0:\n            valid_embed = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n            sim = tf.matmul(valid_embed, normalized_embeddings, transpose_b=True)\n            sim = sim.numpy()\n            for i in xrange(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = 'Nearest to %s:' % valid_word\n                for k in xrange(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = '%s %s,' % (log_str, close_word)\n                print(log_str)\n        if step % (print_freq * 20) == 0 and step != 0:\n            print('Save model, data and dictionaries' + '!' * 10)\n            model.save_weights(filepath=model_file_name + '.hdf5')\n            tl.files.save_any_to_npy(save_dict={'data': data, 'count': count, 'dictionary': dictionary, 'reverse_dictionary': reverse_dictionary}, name=model_file_name + '.npy')\n        step += 1\n    print()\n    final_embeddings = normalized_embeddings\n    tl.visualize.tsne_embedding(final_embeddings, reverse_dictionary, plot_only=500, second=5, saveable=False, name='word2vec_basic')\n    print()\n    model.eval()\n    if not os.path.exists('questions-words.txt'):\n        print(\"Downloading file 'questions-words.txt'\")\n        wget.download('http://download.tensorflow.org/data/questions-words.txt')\n    analogy_questions = tl.nlp.read_analogies_file(eval_file='questions-words.txt', word2id=dictionary)\n    n_answer = 4\n\n    def predict(analogy):\n        analogy_a = analogy[:, 0]\n        analogy_b = analogy[:, 1]\n        analogy_c = analogy[:, 2]\n        a_emb = tf.gather(normalized_embeddings, analogy_a)\n        b_emb = tf.gather(normalized_embeddings, analogy_b)\n        c_emb = tf.gather(normalized_embeddings, analogy_c)\n        target = c_emb + (b_emb - a_emb)\n        dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n        'Predict the top 4 answers for analogy questions.'\n        (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n        return pred_idx\n    correct = 0\n    total = analogy_questions.shape[0]\n    start = 0\n    while start < total:\n        limit = start + 2500\n        sub = analogy_questions[start:limit, :]\n        idx = predict(sub)\n        start = limit\n        for question in xrange(sub.shape[0]):\n            for j in xrange(n_answer):\n                if idx[question, j] == sub[question, 3]:\n                    print(j + 1, tl.nlp.word_ids_to_words([idx[question, j]], reverse_dictionary), ':', tl.nlp.word_ids_to_words(sub[question, :], reverse_dictionary))\n                    correct += 1\n                    break\n                elif idx[question, j] in sub[question, :3]:\n                    continue\n                else:\n                    break\n    print('Eval %4d/%d accuracy = %4.1f%%' % (correct, total, correct * 100.0 / total))",
            "def main_word2vec_basic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    words = tl.files.load_matt_mahoney_text8_dataset()\n    data_size = len(words)\n    print(data_size)\n    print(words[0:10])\n    resume = False\n    _UNK = '_UNK'\n    if FLAGS.model == 'one':\n        vocabulary_size = 50000\n        batch_size = 128\n        embedding_size = 128\n        skip_window = 1\n        num_skips = 2\n        num_sampled = 64\n        learning_rate = 1.0\n        n_epoch = 20\n        model_file_name = 'model_word2vec_50k_128'\n    elif FLAGS.model == 'two':\n        vocabulary_size = 80000\n        batch_size = 20\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 100\n        learning_rate = 0.2\n        n_epoch = 15\n        model_file_name = 'model_word2vec_80k_200'\n    elif FLAGS.model == 'three':\n        vocabulary_size = 80000\n        batch_size = 500\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.025\n        n_epoch = 20\n        model_file_name = 'model_word2vec_80k_200_opt'\n    elif FLAGS.model == 'four':\n        vocabulary_size = 80000\n        batch_size = 100\n        embedding_size = 600\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.03\n        n_epoch = 200 * 10\n        model_file_name = 'model_word2vec_80k_600'\n    else:\n        raise Exception('Invalid model: %s' % FLAGS.model)\n    num_steps = int(data_size / batch_size * n_epoch)\n    print('%d Steps in a Epoch, total Epochs %d' % (int(data_size / batch_size), n_epoch))\n    print('   learning_rate: %f' % learning_rate)\n    print('   batch_size: %d' % batch_size)\n    print()\n    if resume:\n        print('Load existing data and dictionaries' + '!' * 10)\n        all_var = tl.files.load_npy_to_any(name=model_file_name + '.npy')\n        data = all_var['data']\n        count = all_var['count']\n        dictionary = all_var['dictionary']\n        reverse_dictionary = all_var['reverse_dictionary']\n    else:\n        (data, count, dictionary, reverse_dictionary) = tl.nlp.build_words_dataset(words, vocabulary_size, True, _UNK)\n    print('Most 5 common words (+UNK)', count[:5])\n    print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n    del words\n    print()\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=4, skip_window=2, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    (batch, labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0)\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n    print()\n    valid_size = 16\n    valid_window = 100\n    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n    labels = tl.layers.Input([batch_size, 1], dtype=tf.int32)\n    emb_net = tl.layers.Word2vecEmbedding(vocabulary_size=vocabulary_size, embedding_size=embedding_size, num_sampled=num_sampled, activate_nce_loss=True, nce_loss_args={}, E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0), nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))), nce_b_init=tl.initializers.constant(value=0.0), name='word2vec_layer')\n    (emb, nce) = emb_net([inputs, labels])\n    model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name='word2vec_model')\n    optimizer = tf.optimizers.Adagrad(learning_rate, initial_accumulator_value=0.1)\n    normalized_embeddings = emb_net.normalized_embeddings\n    model.train()\n    if resume:\n        print('Load existing model' + '!' * 10)\n        model.load_weights(filepath=model_file_name + '.hdf5')\n    tl.nlp.save_vocab(count, name='vocab_text8.txt')\n    average_loss = 0\n    step = 0\n    print_freq = 2000\n    while step < num_steps:\n        start_time = time.time()\n        (batch_inputs, batch_labels, data_index) = tl.nlp.generate_skip_gram_batch(data=data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window, data_index=data_index)\n        with tf.GradientTape() as tape:\n            (outputs, nce_cost) = model([batch_inputs, batch_labels])\n        grad = tape.gradient(nce_cost, model.trainable_weights)\n        optimizer.apply_gradients(zip(grad, model.trainable_weights))\n        average_loss += nce_cost\n        if step % print_freq == 0:\n            if step > 0:\n                average_loss /= print_freq\n            print('Average loss at step %d/%d. loss: %f took: %fs/per step' % (step, num_steps, average_loss, time.time() - start_time))\n            average_loss = 0\n        if step % (print_freq * 5) == 0:\n            valid_embed = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n            sim = tf.matmul(valid_embed, normalized_embeddings, transpose_b=True)\n            sim = sim.numpy()\n            for i in xrange(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = 'Nearest to %s:' % valid_word\n                for k in xrange(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = '%s %s,' % (log_str, close_word)\n                print(log_str)\n        if step % (print_freq * 20) == 0 and step != 0:\n            print('Save model, data and dictionaries' + '!' * 10)\n            model.save_weights(filepath=model_file_name + '.hdf5')\n            tl.files.save_any_to_npy(save_dict={'data': data, 'count': count, 'dictionary': dictionary, 'reverse_dictionary': reverse_dictionary}, name=model_file_name + '.npy')\n        step += 1\n    print()\n    final_embeddings = normalized_embeddings\n    tl.visualize.tsne_embedding(final_embeddings, reverse_dictionary, plot_only=500, second=5, saveable=False, name='word2vec_basic')\n    print()\n    model.eval()\n    if not os.path.exists('questions-words.txt'):\n        print(\"Downloading file 'questions-words.txt'\")\n        wget.download('http://download.tensorflow.org/data/questions-words.txt')\n    analogy_questions = tl.nlp.read_analogies_file(eval_file='questions-words.txt', word2id=dictionary)\n    n_answer = 4\n\n    def predict(analogy):\n        analogy_a = analogy[:, 0]\n        analogy_b = analogy[:, 1]\n        analogy_c = analogy[:, 2]\n        a_emb = tf.gather(normalized_embeddings, analogy_a)\n        b_emb = tf.gather(normalized_embeddings, analogy_b)\n        c_emb = tf.gather(normalized_embeddings, analogy_c)\n        target = c_emb + (b_emb - a_emb)\n        dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n        'Predict the top 4 answers for analogy questions.'\n        (_, pred_idx) = tf.nn.top_k(dist, n_answer)\n        return pred_idx\n    correct = 0\n    total = analogy_questions.shape[0]\n    start = 0\n    while start < total:\n        limit = start + 2500\n        sub = analogy_questions[start:limit, :]\n        idx = predict(sub)\n        start = limit\n        for question in xrange(sub.shape[0]):\n            for j in xrange(n_answer):\n                if idx[question, j] == sub[question, 3]:\n                    print(j + 1, tl.nlp.word_ids_to_words([idx[question, j]], reverse_dictionary), ':', tl.nlp.word_ids_to_words(sub[question, :], reverse_dictionary))\n                    correct += 1\n                    break\n                elif idx[question, j] in sub[question, :3]:\n                    continue\n                else:\n                    break\n    print('Eval %4d/%d accuracy = %4.1f%%' % (correct, total, correct * 100.0 / total))"
        ]
    }
]