[
    {
        "func_name": "_get_eigen_vector",
        "original": "def _get_eigen_vector(dot_matrix, variance_thresh, num_features=None):\n    \"\"\"\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\n\n    Computation of Orthogonal Features\n\n    Gets eigen values and eigen vector from matrix which explain % variance_thresh of total variance.\n\n    :param dot_matrix: (np.array): Matrix for which eigen values/vectors should be computed.\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\n    :return: (pd.Series, pd.DataFrame): Eigenvalues, Eigenvectors.\n    \"\"\"\n    pass",
        "mutated": [
            "def _get_eigen_vector(dot_matrix, variance_thresh, num_features=None):\n    if False:\n        i = 10\n    '\\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\\n\\n    Computation of Orthogonal Features\\n\\n    Gets eigen values and eigen vector from matrix which explain % variance_thresh of total variance.\\n\\n    :param dot_matrix: (np.array): Matrix for which eigen values/vectors should be computed.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\\n    :return: (pd.Series, pd.DataFrame): Eigenvalues, Eigenvectors.\\n    '\n    pass",
            "def _get_eigen_vector(dot_matrix, variance_thresh, num_features=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\\n\\n    Computation of Orthogonal Features\\n\\n    Gets eigen values and eigen vector from matrix which explain % variance_thresh of total variance.\\n\\n    :param dot_matrix: (np.array): Matrix for which eigen values/vectors should be computed.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\\n    :return: (pd.Series, pd.DataFrame): Eigenvalues, Eigenvectors.\\n    '\n    pass",
            "def _get_eigen_vector(dot_matrix, variance_thresh, num_features=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\\n\\n    Computation of Orthogonal Features\\n\\n    Gets eigen values and eigen vector from matrix which explain % variance_thresh of total variance.\\n\\n    :param dot_matrix: (np.array): Matrix for which eigen values/vectors should be computed.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\\n    :return: (pd.Series, pd.DataFrame): Eigenvalues, Eigenvectors.\\n    '\n    pass",
            "def _get_eigen_vector(dot_matrix, variance_thresh, num_features=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\\n\\n    Computation of Orthogonal Features\\n\\n    Gets eigen values and eigen vector from matrix which explain % variance_thresh of total variance.\\n\\n    :param dot_matrix: (np.array): Matrix for which eigen values/vectors should be computed.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\\n    :return: (pd.Series, pd.DataFrame): Eigenvalues, Eigenvectors.\\n    '\n    pass",
            "def _get_eigen_vector(dot_matrix, variance_thresh, num_features=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\\n\\n    Computation of Orthogonal Features\\n\\n    Gets eigen values and eigen vector from matrix which explain % variance_thresh of total variance.\\n\\n    :param dot_matrix: (np.array): Matrix for which eigen values/vectors should be computed.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\\n    :return: (pd.Series, pd.DataFrame): Eigenvalues, Eigenvectors.\\n    '\n    pass"
        ]
    },
    {
        "func_name": "_standardize_df",
        "original": "def _standardize_df(data_frame):\n    \"\"\"\n    Helper function which divides df by std and extracts mean.\n\n    :param data_frame: (pd.DataFrame): Dataframe to standardize\n    :return: (pd.DataFrame): Standardized dataframe\n    \"\"\"\n    pass",
        "mutated": [
            "def _standardize_df(data_frame):\n    if False:\n        i = 10\n    '\\n    Helper function which divides df by std and extracts mean.\\n\\n    :param data_frame: (pd.DataFrame): Dataframe to standardize\\n    :return: (pd.DataFrame): Standardized dataframe\\n    '\n    pass",
            "def _standardize_df(data_frame):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Helper function which divides df by std and extracts mean.\\n\\n    :param data_frame: (pd.DataFrame): Dataframe to standardize\\n    :return: (pd.DataFrame): Standardized dataframe\\n    '\n    pass",
            "def _standardize_df(data_frame):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Helper function which divides df by std and extracts mean.\\n\\n    :param data_frame: (pd.DataFrame): Dataframe to standardize\\n    :return: (pd.DataFrame): Standardized dataframe\\n    '\n    pass",
            "def _standardize_df(data_frame):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Helper function which divides df by std and extracts mean.\\n\\n    :param data_frame: (pd.DataFrame): Dataframe to standardize\\n    :return: (pd.DataFrame): Standardized dataframe\\n    '\n    pass",
            "def _standardize_df(data_frame):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Helper function which divides df by std and extracts mean.\\n\\n    :param data_frame: (pd.DataFrame): Dataframe to standardize\\n    :return: (pd.DataFrame): Standardized dataframe\\n    '\n    pass"
        ]
    },
    {
        "func_name": "get_orthogonal_features",
        "original": "def get_orthogonal_features(feature_df, variance_thresh=0.95, num_features=None):\n    \"\"\"\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\n\n    Computation of Orthogonal Features.\n\n    Gets PCA orthogonal features.\n\n    :param feature_df: (pd.DataFrame): Dataframe of features.\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\n    :return: (pd.DataFrame): Compressed PCA features which explain %variance_thresh of variance.\n    \"\"\"\n    pass",
        "mutated": [
            "def get_orthogonal_features(feature_df, variance_thresh=0.95, num_features=None):\n    if False:\n        i = 10\n    '\\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\\n\\n    Computation of Orthogonal Features.\\n\\n    Gets PCA orthogonal features.\\n\\n    :param feature_df: (pd.DataFrame): Dataframe of features.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\\n    :return: (pd.DataFrame): Compressed PCA features which explain %variance_thresh of variance.\\n    '\n    pass",
            "def get_orthogonal_features(feature_df, variance_thresh=0.95, num_features=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\\n\\n    Computation of Orthogonal Features.\\n\\n    Gets PCA orthogonal features.\\n\\n    :param feature_df: (pd.DataFrame): Dataframe of features.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\\n    :return: (pd.DataFrame): Compressed PCA features which explain %variance_thresh of variance.\\n    '\n    pass",
            "def get_orthogonal_features(feature_df, variance_thresh=0.95, num_features=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\\n\\n    Computation of Orthogonal Features.\\n\\n    Gets PCA orthogonal features.\\n\\n    :param feature_df: (pd.DataFrame): Dataframe of features.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\\n    :return: (pd.DataFrame): Compressed PCA features which explain %variance_thresh of variance.\\n    '\n    pass",
            "def get_orthogonal_features(feature_df, variance_thresh=0.95, num_features=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\\n\\n    Computation of Orthogonal Features.\\n\\n    Gets PCA orthogonal features.\\n\\n    :param feature_df: (pd.DataFrame): Dataframe of features.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\\n    :return: (pd.DataFrame): Compressed PCA features which explain %variance_thresh of variance.\\n    '\n    pass",
            "def get_orthogonal_features(feature_df, variance_thresh=0.95, num_features=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Advances in Financial Machine Learning, Snippet 8.5, page 119.\\n\\n    Computation of Orthogonal Features.\\n\\n    Gets PCA orthogonal features.\\n\\n    :param feature_df: (pd.DataFrame): Dataframe of features.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain.\\n    :param num_features: (int) Manually set number of features, overrides variance_thresh. (None by default)\\n    :return: (pd.DataFrame): Compressed PCA features which explain %variance_thresh of variance.\\n    '\n    pass"
        ]
    },
    {
        "func_name": "get_pca_rank_weighted_kendall_tau",
        "original": "def get_pca_rank_weighted_kendall_tau(feature_imp, pca_rank):\n    \"\"\"\n    Advances in Financial Machine Learning, Snippet 8.6, page 121.\n\n    Computes Weighted Kendall's Tau Between Feature Importance and Inverse PCA Ranking.\n\n    :param feature_imp: (np.array): Feature mean importance.\n    :param pca_rank: (np.array): PCA based feature importance rank.\n    :return: (float): Weighted Kendall Tau of feature importance and inverse PCA rank with p_value.\n    \"\"\"\n    pass",
        "mutated": [
            "def get_pca_rank_weighted_kendall_tau(feature_imp, pca_rank):\n    if False:\n        i = 10\n    \"\\n    Advances in Financial Machine Learning, Snippet 8.6, page 121.\\n\\n    Computes Weighted Kendall's Tau Between Feature Importance and Inverse PCA Ranking.\\n\\n    :param feature_imp: (np.array): Feature mean importance.\\n    :param pca_rank: (np.array): PCA based feature importance rank.\\n    :return: (float): Weighted Kendall Tau of feature importance and inverse PCA rank with p_value.\\n    \"\n    pass",
            "def get_pca_rank_weighted_kendall_tau(feature_imp, pca_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Advances in Financial Machine Learning, Snippet 8.6, page 121.\\n\\n    Computes Weighted Kendall's Tau Between Feature Importance and Inverse PCA Ranking.\\n\\n    :param feature_imp: (np.array): Feature mean importance.\\n    :param pca_rank: (np.array): PCA based feature importance rank.\\n    :return: (float): Weighted Kendall Tau of feature importance and inverse PCA rank with p_value.\\n    \"\n    pass",
            "def get_pca_rank_weighted_kendall_tau(feature_imp, pca_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Advances in Financial Machine Learning, Snippet 8.6, page 121.\\n\\n    Computes Weighted Kendall's Tau Between Feature Importance and Inverse PCA Ranking.\\n\\n    :param feature_imp: (np.array): Feature mean importance.\\n    :param pca_rank: (np.array): PCA based feature importance rank.\\n    :return: (float): Weighted Kendall Tau of feature importance and inverse PCA rank with p_value.\\n    \"\n    pass",
            "def get_pca_rank_weighted_kendall_tau(feature_imp, pca_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Advances in Financial Machine Learning, Snippet 8.6, page 121.\\n\\n    Computes Weighted Kendall's Tau Between Feature Importance and Inverse PCA Ranking.\\n\\n    :param feature_imp: (np.array): Feature mean importance.\\n    :param pca_rank: (np.array): PCA based feature importance rank.\\n    :return: (float): Weighted Kendall Tau of feature importance and inverse PCA rank with p_value.\\n    \"\n    pass",
            "def get_pca_rank_weighted_kendall_tau(feature_imp, pca_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Advances in Financial Machine Learning, Snippet 8.6, page 121.\\n\\n    Computes Weighted Kendall's Tau Between Feature Importance and Inverse PCA Ranking.\\n\\n    :param feature_imp: (np.array): Feature mean importance.\\n    :param pca_rank: (np.array): PCA based feature importance rank.\\n    :return: (float): Weighted Kendall Tau of feature importance and inverse PCA rank with p_value.\\n    \"\n    pass"
        ]
    },
    {
        "func_name": "feature_pca_analysis",
        "original": "def feature_pca_analysis(feature_df, feature_importance, variance_thresh=0.95):\n    \"\"\"\n    Performs correlation analysis between feature importance (MDI for example, supervised) and PCA eigenvalues\n    (unsupervised).\n\n    High correlation means that probably the pattern identified by the ML algorithm is not entirely overfit.\n\n    :param feature_df: (pd.DataFrame): Features dataframe.\n    :param feature_importance: (pd.DataFrame): Individual MDI feature importance.\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain in PCA compression.\n    :return: (dict): Dictionary with kendall, spearman, pearson and weighted_kendall correlations and p_values.\n    \"\"\"\n    pass",
        "mutated": [
            "def feature_pca_analysis(feature_df, feature_importance, variance_thresh=0.95):\n    if False:\n        i = 10\n    '\\n    Performs correlation analysis between feature importance (MDI for example, supervised) and PCA eigenvalues\\n    (unsupervised).\\n\\n    High correlation means that probably the pattern identified by the ML algorithm is not entirely overfit.\\n\\n    :param feature_df: (pd.DataFrame): Features dataframe.\\n    :param feature_importance: (pd.DataFrame): Individual MDI feature importance.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain in PCA compression.\\n    :return: (dict): Dictionary with kendall, spearman, pearson and weighted_kendall correlations and p_values.\\n    '\n    pass",
            "def feature_pca_analysis(feature_df, feature_importance, variance_thresh=0.95):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Performs correlation analysis between feature importance (MDI for example, supervised) and PCA eigenvalues\\n    (unsupervised).\\n\\n    High correlation means that probably the pattern identified by the ML algorithm is not entirely overfit.\\n\\n    :param feature_df: (pd.DataFrame): Features dataframe.\\n    :param feature_importance: (pd.DataFrame): Individual MDI feature importance.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain in PCA compression.\\n    :return: (dict): Dictionary with kendall, spearman, pearson and weighted_kendall correlations and p_values.\\n    '\n    pass",
            "def feature_pca_analysis(feature_df, feature_importance, variance_thresh=0.95):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Performs correlation analysis between feature importance (MDI for example, supervised) and PCA eigenvalues\\n    (unsupervised).\\n\\n    High correlation means that probably the pattern identified by the ML algorithm is not entirely overfit.\\n\\n    :param feature_df: (pd.DataFrame): Features dataframe.\\n    :param feature_importance: (pd.DataFrame): Individual MDI feature importance.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain in PCA compression.\\n    :return: (dict): Dictionary with kendall, spearman, pearson and weighted_kendall correlations and p_values.\\n    '\n    pass",
            "def feature_pca_analysis(feature_df, feature_importance, variance_thresh=0.95):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Performs correlation analysis between feature importance (MDI for example, supervised) and PCA eigenvalues\\n    (unsupervised).\\n\\n    High correlation means that probably the pattern identified by the ML algorithm is not entirely overfit.\\n\\n    :param feature_df: (pd.DataFrame): Features dataframe.\\n    :param feature_importance: (pd.DataFrame): Individual MDI feature importance.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain in PCA compression.\\n    :return: (dict): Dictionary with kendall, spearman, pearson and weighted_kendall correlations and p_values.\\n    '\n    pass",
            "def feature_pca_analysis(feature_df, feature_importance, variance_thresh=0.95):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Performs correlation analysis between feature importance (MDI for example, supervised) and PCA eigenvalues\\n    (unsupervised).\\n\\n    High correlation means that probably the pattern identified by the ML algorithm is not entirely overfit.\\n\\n    :param feature_df: (pd.DataFrame): Features dataframe.\\n    :param feature_importance: (pd.DataFrame): Individual MDI feature importance.\\n    :param variance_thresh: (float): Percentage % of overall variance which compressed vectors should explain in PCA compression.\\n    :return: (dict): Dictionary with kendall, spearman, pearson and weighted_kendall correlations and p_values.\\n    '\n    pass"
        ]
    }
]