[
    {
        "func_name": "from_items",
        "original": "@PublicAPI\ndef from_items(items: List[Any], *, parallelism: int=-1) -> MaterializedDataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a list of local Python objects.\n\n    Use this method to create small datasets from data that fits in memory.\n\n    Examples:\n\n        >>> import ray\n        >>> ds = ray.data.from_items([1, 2, 3, 4, 5])\n        >>> ds\n        MaterializedDataset(num_blocks=..., num_rows=5, schema={item: int64})\n        >>> ds.schema()\n        Column  Type\n        ------  ----\n        item    int64\n\n    Args:\n        items: List of local Python objects.\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see\n            :ref:`Tuning read parallelism <read_parallelism>`.\n            Parallelism is upper bounded by ``len(items)``.\n\n    Returns:\n        A :class:`~ray.data.Dataset` holding the items.\n    \"\"\"\n    import builtins\n    if parallelism == 0:\n        raise ValueError(f'parallelism must be -1 or > 0, got: {parallelism}')\n    (detected_parallelism, _, _, _) = _autodetect_parallelism(parallelism, ray.util.get_current_placement_group(), DataContext.get_current())\n    detected_parallelism = min(len(items), detected_parallelism)\n    if detected_parallelism > 0:\n        (block_size, remainder) = divmod(len(items), detected_parallelism)\n    else:\n        (block_size, remainder) = (0, 0)\n    blocks: List[ObjectRef[Block]] = []\n    metadata: List[BlockMetadata] = []\n    for i in builtins.range(detected_parallelism):\n        stats = BlockExecStats.builder()\n        builder = DelegatingBlockBuilder()\n        block_start = i * block_size + min(i, remainder)\n        block_end = (i + 1) * block_size + min(i + 1, remainder)\n        for j in builtins.range(block_start, block_end):\n            item = items[j]\n            if not isinstance(item, collections.abc.Mapping):\n                item = {'item': item}\n            builder.add(item)\n        block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(BlockAccessor.for_block(block).get_metadata(input_files=None, exec_stats=stats.build()))\n    from_items_op = FromItems(blocks, metadata)\n    logical_plan = LogicalPlan(from_items_op)\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromItems': metadata}, parent=None), run_by_consumer=False), logical_plan)",
        "mutated": [
            "@PublicAPI\ndef from_items(items: List[Any], *, parallelism: int=-1) -> MaterializedDataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a list of local Python objects.\\n\\n    Use this method to create small datasets from data that fits in memory.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.from_items([1, 2, 3, 4, 5])\\n        >>> ds\\n        MaterializedDataset(num_blocks=..., num_rows=5, schema={item: int64})\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        item    int64\\n\\n    Args:\\n        items: List of local Python objects.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by ``len(items)``.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` holding the items.\\n    '\n    import builtins\n    if parallelism == 0:\n        raise ValueError(f'parallelism must be -1 or > 0, got: {parallelism}')\n    (detected_parallelism, _, _, _) = _autodetect_parallelism(parallelism, ray.util.get_current_placement_group(), DataContext.get_current())\n    detected_parallelism = min(len(items), detected_parallelism)\n    if detected_parallelism > 0:\n        (block_size, remainder) = divmod(len(items), detected_parallelism)\n    else:\n        (block_size, remainder) = (0, 0)\n    blocks: List[ObjectRef[Block]] = []\n    metadata: List[BlockMetadata] = []\n    for i in builtins.range(detected_parallelism):\n        stats = BlockExecStats.builder()\n        builder = DelegatingBlockBuilder()\n        block_start = i * block_size + min(i, remainder)\n        block_end = (i + 1) * block_size + min(i + 1, remainder)\n        for j in builtins.range(block_start, block_end):\n            item = items[j]\n            if not isinstance(item, collections.abc.Mapping):\n                item = {'item': item}\n            builder.add(item)\n        block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(BlockAccessor.for_block(block).get_metadata(input_files=None, exec_stats=stats.build()))\n    from_items_op = FromItems(blocks, metadata)\n    logical_plan = LogicalPlan(from_items_op)\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromItems': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@PublicAPI\ndef from_items(items: List[Any], *, parallelism: int=-1) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a list of local Python objects.\\n\\n    Use this method to create small datasets from data that fits in memory.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.from_items([1, 2, 3, 4, 5])\\n        >>> ds\\n        MaterializedDataset(num_blocks=..., num_rows=5, schema={item: int64})\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        item    int64\\n\\n    Args:\\n        items: List of local Python objects.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by ``len(items)``.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` holding the items.\\n    '\n    import builtins\n    if parallelism == 0:\n        raise ValueError(f'parallelism must be -1 or > 0, got: {parallelism}')\n    (detected_parallelism, _, _, _) = _autodetect_parallelism(parallelism, ray.util.get_current_placement_group(), DataContext.get_current())\n    detected_parallelism = min(len(items), detected_parallelism)\n    if detected_parallelism > 0:\n        (block_size, remainder) = divmod(len(items), detected_parallelism)\n    else:\n        (block_size, remainder) = (0, 0)\n    blocks: List[ObjectRef[Block]] = []\n    metadata: List[BlockMetadata] = []\n    for i in builtins.range(detected_parallelism):\n        stats = BlockExecStats.builder()\n        builder = DelegatingBlockBuilder()\n        block_start = i * block_size + min(i, remainder)\n        block_end = (i + 1) * block_size + min(i + 1, remainder)\n        for j in builtins.range(block_start, block_end):\n            item = items[j]\n            if not isinstance(item, collections.abc.Mapping):\n                item = {'item': item}\n            builder.add(item)\n        block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(BlockAccessor.for_block(block).get_metadata(input_files=None, exec_stats=stats.build()))\n    from_items_op = FromItems(blocks, metadata)\n    logical_plan = LogicalPlan(from_items_op)\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromItems': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@PublicAPI\ndef from_items(items: List[Any], *, parallelism: int=-1) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a list of local Python objects.\\n\\n    Use this method to create small datasets from data that fits in memory.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.from_items([1, 2, 3, 4, 5])\\n        >>> ds\\n        MaterializedDataset(num_blocks=..., num_rows=5, schema={item: int64})\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        item    int64\\n\\n    Args:\\n        items: List of local Python objects.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by ``len(items)``.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` holding the items.\\n    '\n    import builtins\n    if parallelism == 0:\n        raise ValueError(f'parallelism must be -1 or > 0, got: {parallelism}')\n    (detected_parallelism, _, _, _) = _autodetect_parallelism(parallelism, ray.util.get_current_placement_group(), DataContext.get_current())\n    detected_parallelism = min(len(items), detected_parallelism)\n    if detected_parallelism > 0:\n        (block_size, remainder) = divmod(len(items), detected_parallelism)\n    else:\n        (block_size, remainder) = (0, 0)\n    blocks: List[ObjectRef[Block]] = []\n    metadata: List[BlockMetadata] = []\n    for i in builtins.range(detected_parallelism):\n        stats = BlockExecStats.builder()\n        builder = DelegatingBlockBuilder()\n        block_start = i * block_size + min(i, remainder)\n        block_end = (i + 1) * block_size + min(i + 1, remainder)\n        for j in builtins.range(block_start, block_end):\n            item = items[j]\n            if not isinstance(item, collections.abc.Mapping):\n                item = {'item': item}\n            builder.add(item)\n        block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(BlockAccessor.for_block(block).get_metadata(input_files=None, exec_stats=stats.build()))\n    from_items_op = FromItems(blocks, metadata)\n    logical_plan = LogicalPlan(from_items_op)\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromItems': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@PublicAPI\ndef from_items(items: List[Any], *, parallelism: int=-1) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a list of local Python objects.\\n\\n    Use this method to create small datasets from data that fits in memory.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.from_items([1, 2, 3, 4, 5])\\n        >>> ds\\n        MaterializedDataset(num_blocks=..., num_rows=5, schema={item: int64})\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        item    int64\\n\\n    Args:\\n        items: List of local Python objects.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by ``len(items)``.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` holding the items.\\n    '\n    import builtins\n    if parallelism == 0:\n        raise ValueError(f'parallelism must be -1 or > 0, got: {parallelism}')\n    (detected_parallelism, _, _, _) = _autodetect_parallelism(parallelism, ray.util.get_current_placement_group(), DataContext.get_current())\n    detected_parallelism = min(len(items), detected_parallelism)\n    if detected_parallelism > 0:\n        (block_size, remainder) = divmod(len(items), detected_parallelism)\n    else:\n        (block_size, remainder) = (0, 0)\n    blocks: List[ObjectRef[Block]] = []\n    metadata: List[BlockMetadata] = []\n    for i in builtins.range(detected_parallelism):\n        stats = BlockExecStats.builder()\n        builder = DelegatingBlockBuilder()\n        block_start = i * block_size + min(i, remainder)\n        block_end = (i + 1) * block_size + min(i + 1, remainder)\n        for j in builtins.range(block_start, block_end):\n            item = items[j]\n            if not isinstance(item, collections.abc.Mapping):\n                item = {'item': item}\n            builder.add(item)\n        block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(BlockAccessor.for_block(block).get_metadata(input_files=None, exec_stats=stats.build()))\n    from_items_op = FromItems(blocks, metadata)\n    logical_plan = LogicalPlan(from_items_op)\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromItems': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@PublicAPI\ndef from_items(items: List[Any], *, parallelism: int=-1) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a list of local Python objects.\\n\\n    Use this method to create small datasets from data that fits in memory.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.from_items([1, 2, 3, 4, 5])\\n        >>> ds\\n        MaterializedDataset(num_blocks=..., num_rows=5, schema={item: int64})\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        item    int64\\n\\n    Args:\\n        items: List of local Python objects.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by ``len(items)``.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` holding the items.\\n    '\n    import builtins\n    if parallelism == 0:\n        raise ValueError(f'parallelism must be -1 or > 0, got: {parallelism}')\n    (detected_parallelism, _, _, _) = _autodetect_parallelism(parallelism, ray.util.get_current_placement_group(), DataContext.get_current())\n    detected_parallelism = min(len(items), detected_parallelism)\n    if detected_parallelism > 0:\n        (block_size, remainder) = divmod(len(items), detected_parallelism)\n    else:\n        (block_size, remainder) = (0, 0)\n    blocks: List[ObjectRef[Block]] = []\n    metadata: List[BlockMetadata] = []\n    for i in builtins.range(detected_parallelism):\n        stats = BlockExecStats.builder()\n        builder = DelegatingBlockBuilder()\n        block_start = i * block_size + min(i, remainder)\n        block_end = (i + 1) * block_size + min(i + 1, remainder)\n        for j in builtins.range(block_start, block_end):\n            item = items[j]\n            if not isinstance(item, collections.abc.Mapping):\n                item = {'item': item}\n            builder.add(item)\n        block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(BlockAccessor.for_block(block).get_metadata(input_files=None, exec_stats=stats.build()))\n    from_items_op = FromItems(blocks, metadata)\n    logical_plan = LogicalPlan(from_items_op)\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromItems': metadata}, parent=None), run_by_consumer=False), logical_plan)"
        ]
    },
    {
        "func_name": "range",
        "original": "@PublicAPI\ndef range(n: int, *, parallelism: int=-1) -> Dataset:\n    \"\"\"Creates a :class:`~ray.data.Dataset` from a range of integers [0..n).\n\n    This function allows for easy creation of synthetic datasets for testing or\n    benchmarking :ref:`Ray Data <data>`.\n\n    Examples:\n\n        >>> import ray\n        >>> ds = ray.data.range(10000)\n        >>> ds\n        Dataset(num_blocks=..., num_rows=10000, schema={id: int64})\n        >>> ds.map(lambda row: {\"id\": row[\"id\"] * 2}).take(4)\n        [{'id': 0}, {'id': 2}, {'id': 4}, {'id': 6}]\n\n    Args:\n        n: The upper bound of the range of integers.\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see\n            :ref:`Tuning read parallelism <read_parallelism>`.\n            Parallelism is upper bounded by n.\n\n    Returns:\n        A :class:`~ray.data.Dataset` producing the integers from the range 0 to n.\n\n    .. seealso::\n\n        :meth:`~ray.data.range_tensor`\n                    Call this method for creating synthetic datasets of tensor data.\n\n    \"\"\"\n    datasource = RangeDatasource(n=n, block_format='arrow', column_name='id')\n    return read_datasource(datasource, parallelism=parallelism)",
        "mutated": [
            "@PublicAPI\ndef range(n: int, *, parallelism: int=-1) -> Dataset:\n    if False:\n        i = 10\n    'Creates a :class:`~ray.data.Dataset` from a range of integers [0..n).\\n\\n    This function allows for easy creation of synthetic datasets for testing or\\n    benchmarking :ref:`Ray Data <data>`.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.range(10000)\\n        >>> ds\\n        Dataset(num_blocks=..., num_rows=10000, schema={id: int64})\\n        >>> ds.map(lambda row: {\"id\": row[\"id\"] * 2}).take(4)\\n        [{\\'id\\': 0}, {\\'id\\': 2}, {\\'id\\': 4}, {\\'id\\': 6}]\\n\\n    Args:\\n        n: The upper bound of the range of integers.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by n.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing the integers from the range 0 to n.\\n\\n    .. seealso::\\n\\n        :meth:`~ray.data.range_tensor`\\n                    Call this method for creating synthetic datasets of tensor data.\\n\\n    '\n    datasource = RangeDatasource(n=n, block_format='arrow', column_name='id')\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef range(n: int, *, parallelism: int=-1) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a :class:`~ray.data.Dataset` from a range of integers [0..n).\\n\\n    This function allows for easy creation of synthetic datasets for testing or\\n    benchmarking :ref:`Ray Data <data>`.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.range(10000)\\n        >>> ds\\n        Dataset(num_blocks=..., num_rows=10000, schema={id: int64})\\n        >>> ds.map(lambda row: {\"id\": row[\"id\"] * 2}).take(4)\\n        [{\\'id\\': 0}, {\\'id\\': 2}, {\\'id\\': 4}, {\\'id\\': 6}]\\n\\n    Args:\\n        n: The upper bound of the range of integers.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by n.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing the integers from the range 0 to n.\\n\\n    .. seealso::\\n\\n        :meth:`~ray.data.range_tensor`\\n                    Call this method for creating synthetic datasets of tensor data.\\n\\n    '\n    datasource = RangeDatasource(n=n, block_format='arrow', column_name='id')\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef range(n: int, *, parallelism: int=-1) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a :class:`~ray.data.Dataset` from a range of integers [0..n).\\n\\n    This function allows for easy creation of synthetic datasets for testing or\\n    benchmarking :ref:`Ray Data <data>`.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.range(10000)\\n        >>> ds\\n        Dataset(num_blocks=..., num_rows=10000, schema={id: int64})\\n        >>> ds.map(lambda row: {\"id\": row[\"id\"] * 2}).take(4)\\n        [{\\'id\\': 0}, {\\'id\\': 2}, {\\'id\\': 4}, {\\'id\\': 6}]\\n\\n    Args:\\n        n: The upper bound of the range of integers.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by n.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing the integers from the range 0 to n.\\n\\n    .. seealso::\\n\\n        :meth:`~ray.data.range_tensor`\\n                    Call this method for creating synthetic datasets of tensor data.\\n\\n    '\n    datasource = RangeDatasource(n=n, block_format='arrow', column_name='id')\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef range(n: int, *, parallelism: int=-1) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a :class:`~ray.data.Dataset` from a range of integers [0..n).\\n\\n    This function allows for easy creation of synthetic datasets for testing or\\n    benchmarking :ref:`Ray Data <data>`.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.range(10000)\\n        >>> ds\\n        Dataset(num_blocks=..., num_rows=10000, schema={id: int64})\\n        >>> ds.map(lambda row: {\"id\": row[\"id\"] * 2}).take(4)\\n        [{\\'id\\': 0}, {\\'id\\': 2}, {\\'id\\': 4}, {\\'id\\': 6}]\\n\\n    Args:\\n        n: The upper bound of the range of integers.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by n.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing the integers from the range 0 to n.\\n\\n    .. seealso::\\n\\n        :meth:`~ray.data.range_tensor`\\n                    Call this method for creating synthetic datasets of tensor data.\\n\\n    '\n    datasource = RangeDatasource(n=n, block_format='arrow', column_name='id')\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef range(n: int, *, parallelism: int=-1) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a :class:`~ray.data.Dataset` from a range of integers [0..n).\\n\\n    This function allows for easy creation of synthetic datasets for testing or\\n    benchmarking :ref:`Ray Data <data>`.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.range(10000)\\n        >>> ds\\n        Dataset(num_blocks=..., num_rows=10000, schema={id: int64})\\n        >>> ds.map(lambda row: {\"id\": row[\"id\"] * 2}).take(4)\\n        [{\\'id\\': 0}, {\\'id\\': 2}, {\\'id\\': 4}, {\\'id\\': 6}]\\n\\n    Args:\\n        n: The upper bound of the range of integers.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by n.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing the integers from the range 0 to n.\\n\\n    .. seealso::\\n\\n        :meth:`~ray.data.range_tensor`\\n                    Call this method for creating synthetic datasets of tensor data.\\n\\n    '\n    datasource = RangeDatasource(n=n, block_format='arrow', column_name='id')\n    return read_datasource(datasource, parallelism=parallelism)"
        ]
    },
    {
        "func_name": "range_tensor",
        "original": "@PublicAPI\ndef range_tensor(n: int, *, shape: Tuple=(1,), parallelism: int=-1) -> Dataset:\n    \"\"\"Creates a :class:`~ray.data.Dataset` tensors of the provided shape from range\n    [0...n].\n\n    This function allows for easy creation of synthetic tensor datasets for testing or\n    benchmarking :ref:`Ray Data <data>`.\n\n    Examples:\n\n        >>> import ray\n        >>> ds = ray.data.range_tensor(1000, shape=(2, 2))\n        >>> ds\n        Dataset(\n           num_blocks=...,\n           num_rows=1000,\n           schema={data: numpy.ndarray(shape=(2, 2), dtype=int64)}\n        )\n        >>> ds.map_batches(lambda row: {\"data\": row[\"data\"] * 2}).take(2)\n        [{'data': array([[0, 0],\n               [0, 0]])}, {'data': array([[2, 2],\n               [2, 2]])}]\n\n    Args:\n        n: The upper bound of the range of tensor records.\n        shape: The shape of each tensor in the dataset.\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see\n            :ref:`Tuning read parallelism <read_parallelism>`.\n            Parallelism is upper bounded by n.\n\n    Returns:\n        A :class:`~ray.data.Dataset` producing the tensor data from range 0 to n.\n\n    .. seealso::\n\n        :meth:`~ray.data.range`\n                    Call this method to create synthetic datasets of integer data.\n\n    \"\"\"\n    datasource = RangeDatasource(n=n, block_format='tensor', column_name='data', tensor_shape=tuple(shape))\n    return read_datasource(datasource, parallelism=parallelism)",
        "mutated": [
            "@PublicAPI\ndef range_tensor(n: int, *, shape: Tuple=(1,), parallelism: int=-1) -> Dataset:\n    if False:\n        i = 10\n    'Creates a :class:`~ray.data.Dataset` tensors of the provided shape from range\\n    [0...n].\\n\\n    This function allows for easy creation of synthetic tensor datasets for testing or\\n    benchmarking :ref:`Ray Data <data>`.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.range_tensor(1000, shape=(2, 2))\\n        >>> ds\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=1000,\\n           schema={data: numpy.ndarray(shape=(2, 2), dtype=int64)}\\n        )\\n        >>> ds.map_batches(lambda row: {\"data\": row[\"data\"] * 2}).take(2)\\n        [{\\'data\\': array([[0, 0],\\n               [0, 0]])}, {\\'data\\': array([[2, 2],\\n               [2, 2]])}]\\n\\n    Args:\\n        n: The upper bound of the range of tensor records.\\n        shape: The shape of each tensor in the dataset.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by n.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing the tensor data from range 0 to n.\\n\\n    .. seealso::\\n\\n        :meth:`~ray.data.range`\\n                    Call this method to create synthetic datasets of integer data.\\n\\n    '\n    datasource = RangeDatasource(n=n, block_format='tensor', column_name='data', tensor_shape=tuple(shape))\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef range_tensor(n: int, *, shape: Tuple=(1,), parallelism: int=-1) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a :class:`~ray.data.Dataset` tensors of the provided shape from range\\n    [0...n].\\n\\n    This function allows for easy creation of synthetic tensor datasets for testing or\\n    benchmarking :ref:`Ray Data <data>`.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.range_tensor(1000, shape=(2, 2))\\n        >>> ds\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=1000,\\n           schema={data: numpy.ndarray(shape=(2, 2), dtype=int64)}\\n        )\\n        >>> ds.map_batches(lambda row: {\"data\": row[\"data\"] * 2}).take(2)\\n        [{\\'data\\': array([[0, 0],\\n               [0, 0]])}, {\\'data\\': array([[2, 2],\\n               [2, 2]])}]\\n\\n    Args:\\n        n: The upper bound of the range of tensor records.\\n        shape: The shape of each tensor in the dataset.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by n.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing the tensor data from range 0 to n.\\n\\n    .. seealso::\\n\\n        :meth:`~ray.data.range`\\n                    Call this method to create synthetic datasets of integer data.\\n\\n    '\n    datasource = RangeDatasource(n=n, block_format='tensor', column_name='data', tensor_shape=tuple(shape))\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef range_tensor(n: int, *, shape: Tuple=(1,), parallelism: int=-1) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a :class:`~ray.data.Dataset` tensors of the provided shape from range\\n    [0...n].\\n\\n    This function allows for easy creation of synthetic tensor datasets for testing or\\n    benchmarking :ref:`Ray Data <data>`.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.range_tensor(1000, shape=(2, 2))\\n        >>> ds\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=1000,\\n           schema={data: numpy.ndarray(shape=(2, 2), dtype=int64)}\\n        )\\n        >>> ds.map_batches(lambda row: {\"data\": row[\"data\"] * 2}).take(2)\\n        [{\\'data\\': array([[0, 0],\\n               [0, 0]])}, {\\'data\\': array([[2, 2],\\n               [2, 2]])}]\\n\\n    Args:\\n        n: The upper bound of the range of tensor records.\\n        shape: The shape of each tensor in the dataset.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by n.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing the tensor data from range 0 to n.\\n\\n    .. seealso::\\n\\n        :meth:`~ray.data.range`\\n                    Call this method to create synthetic datasets of integer data.\\n\\n    '\n    datasource = RangeDatasource(n=n, block_format='tensor', column_name='data', tensor_shape=tuple(shape))\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef range_tensor(n: int, *, shape: Tuple=(1,), parallelism: int=-1) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a :class:`~ray.data.Dataset` tensors of the provided shape from range\\n    [0...n].\\n\\n    This function allows for easy creation of synthetic tensor datasets for testing or\\n    benchmarking :ref:`Ray Data <data>`.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.range_tensor(1000, shape=(2, 2))\\n        >>> ds\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=1000,\\n           schema={data: numpy.ndarray(shape=(2, 2), dtype=int64)}\\n        )\\n        >>> ds.map_batches(lambda row: {\"data\": row[\"data\"] * 2}).take(2)\\n        [{\\'data\\': array([[0, 0],\\n               [0, 0]])}, {\\'data\\': array([[2, 2],\\n               [2, 2]])}]\\n\\n    Args:\\n        n: The upper bound of the range of tensor records.\\n        shape: The shape of each tensor in the dataset.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by n.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing the tensor data from range 0 to n.\\n\\n    .. seealso::\\n\\n        :meth:`~ray.data.range`\\n                    Call this method to create synthetic datasets of integer data.\\n\\n    '\n    datasource = RangeDatasource(n=n, block_format='tensor', column_name='data', tensor_shape=tuple(shape))\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef range_tensor(n: int, *, shape: Tuple=(1,), parallelism: int=-1) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a :class:`~ray.data.Dataset` tensors of the provided shape from range\\n    [0...n].\\n\\n    This function allows for easy creation of synthetic tensor datasets for testing or\\n    benchmarking :ref:`Ray Data <data>`.\\n\\n    Examples:\\n\\n        >>> import ray\\n        >>> ds = ray.data.range_tensor(1000, shape=(2, 2))\\n        >>> ds\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=1000,\\n           schema={data: numpy.ndarray(shape=(2, 2), dtype=int64)}\\n        )\\n        >>> ds.map_batches(lambda row: {\"data\": row[\"data\"] * 2}).take(2)\\n        [{\\'data\\': array([[0, 0],\\n               [0, 0]])}, {\\'data\\': array([[2, 2],\\n               [2, 2]])}]\\n\\n    Args:\\n        n: The upper bound of the range of tensor records.\\n        shape: The shape of each tensor in the dataset.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`.\\n            Parallelism is upper bounded by n.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing the tensor data from range 0 to n.\\n\\n    .. seealso::\\n\\n        :meth:`~ray.data.range`\\n                    Call this method to create synthetic datasets of integer data.\\n\\n    '\n    datasource = RangeDatasource(n=n, block_format='tensor', column_name='data', tensor_shape=tuple(shape))\n    return read_datasource(datasource, parallelism=parallelism)"
        ]
    },
    {
        "func_name": "read_datasource",
        "original": "@PublicAPI\n@wrap_auto_init\ndef read_datasource(datasource: Datasource, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **read_args) -> Dataset:\n    \"\"\"Read a stream from a custom :class:`~ray.data.Datasource`.\n\n    Args:\n        datasource: The :class:`~ray.data.Datasource` to read data from.\n        parallelism: The requested parallelism of the read. Parallelism might be\n            limited by the available partitioning of the datasource. If set to -1,\n            parallelism is automatically chosen based on the available cluster\n            resources and estimated in-memory data size.\n        read_args: Additional kwargs to pass to the :class:`~ray.data.Datasource`\n            implementation.\n        ray_remote_args: kwargs passed to :meth:`ray.remote` in the read tasks.\n\n    Returns:\n        :class:`~ray.data.Dataset` that reads data from the :class:`~ray.data.Datasource`.\n    \"\"\"\n    ctx = DataContext.get_current()\n    if ray_remote_args is None:\n        ray_remote_args = {}\n    if not datasource.supports_distributed_reads:\n        ray_remote_args['scheduling_strategy'] = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n    if 'scheduling_strategy' not in ray_remote_args:\n        ray_remote_args['scheduling_strategy'] = ctx.scheduling_strategy\n    force_local = False\n    pa_ds = _lazy_import_pyarrow_dataset()\n    if pa_ds:\n        partitioning = read_args.get('dataset_kwargs', {}).get('partitioning', None)\n        if isinstance(partitioning, pa_ds.Partitioning):\n            logger.info(f'Forcing local metadata resolution since the provided partitioning {partitioning} is not serializable.')\n            force_local = True\n    if force_local:\n        datasource_or_legacy_reader = _get_datasource_or_legacy_reader(datasource, ctx, read_args)\n    else:\n        scheduling_strategy = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n        get_datasource_or_legacy_reader = cached_remote_fn(_get_datasource_or_legacy_reader, retry_exceptions=False, num_cpus=0).options(scheduling_strategy=scheduling_strategy)\n        datasource_or_legacy_reader = ray.get(get_datasource_or_legacy_reader.remote(datasource, ctx, _wrap_arrow_serialization_workaround(read_args)))\n    cur_pg = ray.util.get_current_placement_group()\n    (requested_parallelism, _, _, inmemory_size) = _autodetect_parallelism(parallelism, ctx.target_max_block_size, DataContext.get_current(), datasource_or_legacy_reader, placement_group=cur_pg)\n    read_tasks = datasource_or_legacy_reader.get_read_tasks(requested_parallelism)\n    if not ctx.use_streaming_executor:\n        _warn_on_high_parallelism(requested_parallelism, len(read_tasks))\n    read_stage_name = f'Read{datasource.get_name()}'\n    block_list = LazyBlockList(read_tasks, read_stage_name=read_stage_name, ray_remote_args=ray_remote_args, owned_by_consumer=False)\n    block_list._estimated_num_blocks = len(read_tasks) if read_tasks else 0\n    read_op = Read(datasource, datasource_or_legacy_reader, parallelism, inmemory_size, ray_remote_args)\n    logical_plan = LogicalPlan(read_op)\n    return Dataset(plan=ExecutionPlan(block_list, block_list.stats(), run_by_consumer=False), logical_plan=logical_plan)",
        "mutated": [
            "@PublicAPI\n@wrap_auto_init\ndef read_datasource(datasource: Datasource, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **read_args) -> Dataset:\n    if False:\n        i = 10\n    'Read a stream from a custom :class:`~ray.data.Datasource`.\\n\\n    Args:\\n        datasource: The :class:`~ray.data.Datasource` to read data from.\\n        parallelism: The requested parallelism of the read. Parallelism might be\\n            limited by the available partitioning of the datasource. If set to -1,\\n            parallelism is automatically chosen based on the available cluster\\n            resources and estimated in-memory data size.\\n        read_args: Additional kwargs to pass to the :class:`~ray.data.Datasource`\\n            implementation.\\n        ray_remote_args: kwargs passed to :meth:`ray.remote` in the read tasks.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` that reads data from the :class:`~ray.data.Datasource`.\\n    '\n    ctx = DataContext.get_current()\n    if ray_remote_args is None:\n        ray_remote_args = {}\n    if not datasource.supports_distributed_reads:\n        ray_remote_args['scheduling_strategy'] = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n    if 'scheduling_strategy' not in ray_remote_args:\n        ray_remote_args['scheduling_strategy'] = ctx.scheduling_strategy\n    force_local = False\n    pa_ds = _lazy_import_pyarrow_dataset()\n    if pa_ds:\n        partitioning = read_args.get('dataset_kwargs', {}).get('partitioning', None)\n        if isinstance(partitioning, pa_ds.Partitioning):\n            logger.info(f'Forcing local metadata resolution since the provided partitioning {partitioning} is not serializable.')\n            force_local = True\n    if force_local:\n        datasource_or_legacy_reader = _get_datasource_or_legacy_reader(datasource, ctx, read_args)\n    else:\n        scheduling_strategy = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n        get_datasource_or_legacy_reader = cached_remote_fn(_get_datasource_or_legacy_reader, retry_exceptions=False, num_cpus=0).options(scheduling_strategy=scheduling_strategy)\n        datasource_or_legacy_reader = ray.get(get_datasource_or_legacy_reader.remote(datasource, ctx, _wrap_arrow_serialization_workaround(read_args)))\n    cur_pg = ray.util.get_current_placement_group()\n    (requested_parallelism, _, _, inmemory_size) = _autodetect_parallelism(parallelism, ctx.target_max_block_size, DataContext.get_current(), datasource_or_legacy_reader, placement_group=cur_pg)\n    read_tasks = datasource_or_legacy_reader.get_read_tasks(requested_parallelism)\n    if not ctx.use_streaming_executor:\n        _warn_on_high_parallelism(requested_parallelism, len(read_tasks))\n    read_stage_name = f'Read{datasource.get_name()}'\n    block_list = LazyBlockList(read_tasks, read_stage_name=read_stage_name, ray_remote_args=ray_remote_args, owned_by_consumer=False)\n    block_list._estimated_num_blocks = len(read_tasks) if read_tasks else 0\n    read_op = Read(datasource, datasource_or_legacy_reader, parallelism, inmemory_size, ray_remote_args)\n    logical_plan = LogicalPlan(read_op)\n    return Dataset(plan=ExecutionPlan(block_list, block_list.stats(), run_by_consumer=False), logical_plan=logical_plan)",
            "@PublicAPI\n@wrap_auto_init\ndef read_datasource(datasource: Datasource, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **read_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Read a stream from a custom :class:`~ray.data.Datasource`.\\n\\n    Args:\\n        datasource: The :class:`~ray.data.Datasource` to read data from.\\n        parallelism: The requested parallelism of the read. Parallelism might be\\n            limited by the available partitioning of the datasource. If set to -1,\\n            parallelism is automatically chosen based on the available cluster\\n            resources and estimated in-memory data size.\\n        read_args: Additional kwargs to pass to the :class:`~ray.data.Datasource`\\n            implementation.\\n        ray_remote_args: kwargs passed to :meth:`ray.remote` in the read tasks.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` that reads data from the :class:`~ray.data.Datasource`.\\n    '\n    ctx = DataContext.get_current()\n    if ray_remote_args is None:\n        ray_remote_args = {}\n    if not datasource.supports_distributed_reads:\n        ray_remote_args['scheduling_strategy'] = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n    if 'scheduling_strategy' not in ray_remote_args:\n        ray_remote_args['scheduling_strategy'] = ctx.scheduling_strategy\n    force_local = False\n    pa_ds = _lazy_import_pyarrow_dataset()\n    if pa_ds:\n        partitioning = read_args.get('dataset_kwargs', {}).get('partitioning', None)\n        if isinstance(partitioning, pa_ds.Partitioning):\n            logger.info(f'Forcing local metadata resolution since the provided partitioning {partitioning} is not serializable.')\n            force_local = True\n    if force_local:\n        datasource_or_legacy_reader = _get_datasource_or_legacy_reader(datasource, ctx, read_args)\n    else:\n        scheduling_strategy = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n        get_datasource_or_legacy_reader = cached_remote_fn(_get_datasource_or_legacy_reader, retry_exceptions=False, num_cpus=0).options(scheduling_strategy=scheduling_strategy)\n        datasource_or_legacy_reader = ray.get(get_datasource_or_legacy_reader.remote(datasource, ctx, _wrap_arrow_serialization_workaround(read_args)))\n    cur_pg = ray.util.get_current_placement_group()\n    (requested_parallelism, _, _, inmemory_size) = _autodetect_parallelism(parallelism, ctx.target_max_block_size, DataContext.get_current(), datasource_or_legacy_reader, placement_group=cur_pg)\n    read_tasks = datasource_or_legacy_reader.get_read_tasks(requested_parallelism)\n    if not ctx.use_streaming_executor:\n        _warn_on_high_parallelism(requested_parallelism, len(read_tasks))\n    read_stage_name = f'Read{datasource.get_name()}'\n    block_list = LazyBlockList(read_tasks, read_stage_name=read_stage_name, ray_remote_args=ray_remote_args, owned_by_consumer=False)\n    block_list._estimated_num_blocks = len(read_tasks) if read_tasks else 0\n    read_op = Read(datasource, datasource_or_legacy_reader, parallelism, inmemory_size, ray_remote_args)\n    logical_plan = LogicalPlan(read_op)\n    return Dataset(plan=ExecutionPlan(block_list, block_list.stats(), run_by_consumer=False), logical_plan=logical_plan)",
            "@PublicAPI\n@wrap_auto_init\ndef read_datasource(datasource: Datasource, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **read_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Read a stream from a custom :class:`~ray.data.Datasource`.\\n\\n    Args:\\n        datasource: The :class:`~ray.data.Datasource` to read data from.\\n        parallelism: The requested parallelism of the read. Parallelism might be\\n            limited by the available partitioning of the datasource. If set to -1,\\n            parallelism is automatically chosen based on the available cluster\\n            resources and estimated in-memory data size.\\n        read_args: Additional kwargs to pass to the :class:`~ray.data.Datasource`\\n            implementation.\\n        ray_remote_args: kwargs passed to :meth:`ray.remote` in the read tasks.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` that reads data from the :class:`~ray.data.Datasource`.\\n    '\n    ctx = DataContext.get_current()\n    if ray_remote_args is None:\n        ray_remote_args = {}\n    if not datasource.supports_distributed_reads:\n        ray_remote_args['scheduling_strategy'] = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n    if 'scheduling_strategy' not in ray_remote_args:\n        ray_remote_args['scheduling_strategy'] = ctx.scheduling_strategy\n    force_local = False\n    pa_ds = _lazy_import_pyarrow_dataset()\n    if pa_ds:\n        partitioning = read_args.get('dataset_kwargs', {}).get('partitioning', None)\n        if isinstance(partitioning, pa_ds.Partitioning):\n            logger.info(f'Forcing local metadata resolution since the provided partitioning {partitioning} is not serializable.')\n            force_local = True\n    if force_local:\n        datasource_or_legacy_reader = _get_datasource_or_legacy_reader(datasource, ctx, read_args)\n    else:\n        scheduling_strategy = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n        get_datasource_or_legacy_reader = cached_remote_fn(_get_datasource_or_legacy_reader, retry_exceptions=False, num_cpus=0).options(scheduling_strategy=scheduling_strategy)\n        datasource_or_legacy_reader = ray.get(get_datasource_or_legacy_reader.remote(datasource, ctx, _wrap_arrow_serialization_workaround(read_args)))\n    cur_pg = ray.util.get_current_placement_group()\n    (requested_parallelism, _, _, inmemory_size) = _autodetect_parallelism(parallelism, ctx.target_max_block_size, DataContext.get_current(), datasource_or_legacy_reader, placement_group=cur_pg)\n    read_tasks = datasource_or_legacy_reader.get_read_tasks(requested_parallelism)\n    if not ctx.use_streaming_executor:\n        _warn_on_high_parallelism(requested_parallelism, len(read_tasks))\n    read_stage_name = f'Read{datasource.get_name()}'\n    block_list = LazyBlockList(read_tasks, read_stage_name=read_stage_name, ray_remote_args=ray_remote_args, owned_by_consumer=False)\n    block_list._estimated_num_blocks = len(read_tasks) if read_tasks else 0\n    read_op = Read(datasource, datasource_or_legacy_reader, parallelism, inmemory_size, ray_remote_args)\n    logical_plan = LogicalPlan(read_op)\n    return Dataset(plan=ExecutionPlan(block_list, block_list.stats(), run_by_consumer=False), logical_plan=logical_plan)",
            "@PublicAPI\n@wrap_auto_init\ndef read_datasource(datasource: Datasource, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **read_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Read a stream from a custom :class:`~ray.data.Datasource`.\\n\\n    Args:\\n        datasource: The :class:`~ray.data.Datasource` to read data from.\\n        parallelism: The requested parallelism of the read. Parallelism might be\\n            limited by the available partitioning of the datasource. If set to -1,\\n            parallelism is automatically chosen based on the available cluster\\n            resources and estimated in-memory data size.\\n        read_args: Additional kwargs to pass to the :class:`~ray.data.Datasource`\\n            implementation.\\n        ray_remote_args: kwargs passed to :meth:`ray.remote` in the read tasks.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` that reads data from the :class:`~ray.data.Datasource`.\\n    '\n    ctx = DataContext.get_current()\n    if ray_remote_args is None:\n        ray_remote_args = {}\n    if not datasource.supports_distributed_reads:\n        ray_remote_args['scheduling_strategy'] = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n    if 'scheduling_strategy' not in ray_remote_args:\n        ray_remote_args['scheduling_strategy'] = ctx.scheduling_strategy\n    force_local = False\n    pa_ds = _lazy_import_pyarrow_dataset()\n    if pa_ds:\n        partitioning = read_args.get('dataset_kwargs', {}).get('partitioning', None)\n        if isinstance(partitioning, pa_ds.Partitioning):\n            logger.info(f'Forcing local metadata resolution since the provided partitioning {partitioning} is not serializable.')\n            force_local = True\n    if force_local:\n        datasource_or_legacy_reader = _get_datasource_or_legacy_reader(datasource, ctx, read_args)\n    else:\n        scheduling_strategy = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n        get_datasource_or_legacy_reader = cached_remote_fn(_get_datasource_or_legacy_reader, retry_exceptions=False, num_cpus=0).options(scheduling_strategy=scheduling_strategy)\n        datasource_or_legacy_reader = ray.get(get_datasource_or_legacy_reader.remote(datasource, ctx, _wrap_arrow_serialization_workaround(read_args)))\n    cur_pg = ray.util.get_current_placement_group()\n    (requested_parallelism, _, _, inmemory_size) = _autodetect_parallelism(parallelism, ctx.target_max_block_size, DataContext.get_current(), datasource_or_legacy_reader, placement_group=cur_pg)\n    read_tasks = datasource_or_legacy_reader.get_read_tasks(requested_parallelism)\n    if not ctx.use_streaming_executor:\n        _warn_on_high_parallelism(requested_parallelism, len(read_tasks))\n    read_stage_name = f'Read{datasource.get_name()}'\n    block_list = LazyBlockList(read_tasks, read_stage_name=read_stage_name, ray_remote_args=ray_remote_args, owned_by_consumer=False)\n    block_list._estimated_num_blocks = len(read_tasks) if read_tasks else 0\n    read_op = Read(datasource, datasource_or_legacy_reader, parallelism, inmemory_size, ray_remote_args)\n    logical_plan = LogicalPlan(read_op)\n    return Dataset(plan=ExecutionPlan(block_list, block_list.stats(), run_by_consumer=False), logical_plan=logical_plan)",
            "@PublicAPI\n@wrap_auto_init\ndef read_datasource(datasource: Datasource, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **read_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Read a stream from a custom :class:`~ray.data.Datasource`.\\n\\n    Args:\\n        datasource: The :class:`~ray.data.Datasource` to read data from.\\n        parallelism: The requested parallelism of the read. Parallelism might be\\n            limited by the available partitioning of the datasource. If set to -1,\\n            parallelism is automatically chosen based on the available cluster\\n            resources and estimated in-memory data size.\\n        read_args: Additional kwargs to pass to the :class:`~ray.data.Datasource`\\n            implementation.\\n        ray_remote_args: kwargs passed to :meth:`ray.remote` in the read tasks.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` that reads data from the :class:`~ray.data.Datasource`.\\n    '\n    ctx = DataContext.get_current()\n    if ray_remote_args is None:\n        ray_remote_args = {}\n    if not datasource.supports_distributed_reads:\n        ray_remote_args['scheduling_strategy'] = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n    if 'scheduling_strategy' not in ray_remote_args:\n        ray_remote_args['scheduling_strategy'] = ctx.scheduling_strategy\n    force_local = False\n    pa_ds = _lazy_import_pyarrow_dataset()\n    if pa_ds:\n        partitioning = read_args.get('dataset_kwargs', {}).get('partitioning', None)\n        if isinstance(partitioning, pa_ds.Partitioning):\n            logger.info(f'Forcing local metadata resolution since the provided partitioning {partitioning} is not serializable.')\n            force_local = True\n    if force_local:\n        datasource_or_legacy_reader = _get_datasource_or_legacy_reader(datasource, ctx, read_args)\n    else:\n        scheduling_strategy = NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)\n        get_datasource_or_legacy_reader = cached_remote_fn(_get_datasource_or_legacy_reader, retry_exceptions=False, num_cpus=0).options(scheduling_strategy=scheduling_strategy)\n        datasource_or_legacy_reader = ray.get(get_datasource_or_legacy_reader.remote(datasource, ctx, _wrap_arrow_serialization_workaround(read_args)))\n    cur_pg = ray.util.get_current_placement_group()\n    (requested_parallelism, _, _, inmemory_size) = _autodetect_parallelism(parallelism, ctx.target_max_block_size, DataContext.get_current(), datasource_or_legacy_reader, placement_group=cur_pg)\n    read_tasks = datasource_or_legacy_reader.get_read_tasks(requested_parallelism)\n    if not ctx.use_streaming_executor:\n        _warn_on_high_parallelism(requested_parallelism, len(read_tasks))\n    read_stage_name = f'Read{datasource.get_name()}'\n    block_list = LazyBlockList(read_tasks, read_stage_name=read_stage_name, ray_remote_args=ray_remote_args, owned_by_consumer=False)\n    block_list._estimated_num_blocks = len(read_tasks) if read_tasks else 0\n    read_op = Read(datasource, datasource_or_legacy_reader, parallelism, inmemory_size, ray_remote_args)\n    logical_plan = LogicalPlan(read_op)\n    return Dataset(plan=ExecutionPlan(block_list, block_list.stats(), run_by_consumer=False), logical_plan=logical_plan)"
        ]
    },
    {
        "func_name": "read_mongo",
        "original": "@PublicAPI(stability='alpha')\ndef read_mongo(uri: str, database: str, collection: str, *, pipeline: Optional[List[Dict]]=None, schema: Optional['pymongoarrow.api.Schema']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **mongo_args) -> Dataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a MongoDB database.\n\n    The data to read from is specified via the ``uri``, ``database`` and ``collection``\n    of the MongoDB. The dataset is created from the results of executing\n    ``pipeline`` against the ``collection``. If ``pipeline`` is None, the entire\n    ``collection`` is read.\n\n    .. tip::\n\n        For more details about these MongoDB concepts, see the following:\n        - URI: https://www.mongodb.com/docs/manual/reference/connection-string/\n        - Database and Collection: https://www.mongodb.com/docs/manual/core/databases-and-collections/\n        - Pipeline: https://www.mongodb.com/docs/manual/core/aggregation-pipeline/\n\n    To read the MongoDB in parallel, the execution of the pipeline is run on partitions\n    of the collection, with a Ray read task to handle a partition. Partitions are\n    created in an attempt to evenly distribute the documents into the specified number\n    of partitions. The number of partitions is determined by ``parallelism`` which can\n    be requested from this interface or automatically chosen if unspecified (see the\n    ``parallelism`` arg below).\n\n    Examples:\n        >>> import ray\n        >>> from pymongoarrow.api import Schema # doctest: +SKIP\n        >>> ds = ray.data.read_mongo( # doctest: +SKIP\n        ...     uri=\"mongodb://username:password@mongodb0.example.com:27017/?authSource=admin\", # noqa: E501\n        ...     database=\"my_db\",\n        ...     collection=\"my_collection\",\n        ...     pipeline=[{\"$match\": {\"col2\": {\"$gte\": 0, \"$lt\": 100}}}, {\"$sort\": \"sort_field\"}], # noqa: E501\n        ...     schema=Schema({\"col1\": pa.string(), \"col2\": pa.int64()}),\n        ...     parallelism=10,\n        ... )\n\n    Args:\n        uri: The URI of the source MongoDB where the dataset is\n            read from. For the URI format, see details in the `MongoDB docs <https:/                /www.mongodb.com/docs/manual/reference/connection-string/>`_.\n        database: The name of the database hosted in the MongoDB. This database\n            must exist otherwise ValueError is raised.\n        collection: The name of the collection in the database. This collection\n            must exist otherwise ValueError is raised.\n        pipeline: A `MongoDB pipeline <https://www.mongodb.com/docs/manual/core            /aggregation-pipeline/>`_, which is executed on the given collection\n            with results used to create Dataset. If None, the entire collection will\n            be read.\n        schema: The schema used to read the collection. If None, it'll be inferred from\n            the results of pipeline.\n        parallelism: The requested parallelism of the read. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see :ref:`Tuning read parallelism\n            <read_parallelism>`.\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\n        mongo_args: kwargs passed to `aggregate_arrow_all() <https://mongo-arrow            .readthedocs.io/en/latest/api/api.html#pymongoarrow.api            aggregate_arrow_all>`_ in pymongoarrow in producing\n            Arrow-formatted results.\n\n    Returns:\n        :class:`~ray.data.Dataset` producing rows from the results of executing the pipeline on the specified MongoDB collection.\n\n    Raises:\n        ValueError: if ``database`` doesn't exist.\n        ValueError: if ``collection`` doesn't exist.\n    \"\"\"\n    datasource = MongoDatasource(uri=uri, database=database, collection=collection, pipeline=pipeline, schema=schema, **mongo_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI(stability='alpha')\ndef read_mongo(uri: str, database: str, collection: str, *, pipeline: Optional[List[Dict]]=None, schema: Optional['pymongoarrow.api.Schema']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **mongo_args) -> Dataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a MongoDB database.\\n\\n    The data to read from is specified via the ``uri``, ``database`` and ``collection``\\n    of the MongoDB. The dataset is created from the results of executing\\n    ``pipeline`` against the ``collection``. If ``pipeline`` is None, the entire\\n    ``collection`` is read.\\n\\n    .. tip::\\n\\n        For more details about these MongoDB concepts, see the following:\\n        - URI: https://www.mongodb.com/docs/manual/reference/connection-string/\\n        - Database and Collection: https://www.mongodb.com/docs/manual/core/databases-and-collections/\\n        - Pipeline: https://www.mongodb.com/docs/manual/core/aggregation-pipeline/\\n\\n    To read the MongoDB in parallel, the execution of the pipeline is run on partitions\\n    of the collection, with a Ray read task to handle a partition. Partitions are\\n    created in an attempt to evenly distribute the documents into the specified number\\n    of partitions. The number of partitions is determined by ``parallelism`` which can\\n    be requested from this interface or automatically chosen if unspecified (see the\\n    ``parallelism`` arg below).\\n\\n    Examples:\\n        >>> import ray\\n        >>> from pymongoarrow.api import Schema # doctest: +SKIP\\n        >>> ds = ray.data.read_mongo( # doctest: +SKIP\\n        ...     uri=\"mongodb://username:password@mongodb0.example.com:27017/?authSource=admin\", # noqa: E501\\n        ...     database=\"my_db\",\\n        ...     collection=\"my_collection\",\\n        ...     pipeline=[{\"$match\": {\"col2\": {\"$gte\": 0, \"$lt\": 100}}}, {\"$sort\": \"sort_field\"}], # noqa: E501\\n        ...     schema=Schema({\"col1\": pa.string(), \"col2\": pa.int64()}),\\n        ...     parallelism=10,\\n        ... )\\n\\n    Args:\\n        uri: The URI of the source MongoDB where the dataset is\\n            read from. For the URI format, see details in the `MongoDB docs <https:/                /www.mongodb.com/docs/manual/reference/connection-string/>`_.\\n        database: The name of the database hosted in the MongoDB. This database\\n            must exist otherwise ValueError is raised.\\n        collection: The name of the collection in the database. This collection\\n            must exist otherwise ValueError is raised.\\n        pipeline: A `MongoDB pipeline <https://www.mongodb.com/docs/manual/core            /aggregation-pipeline/>`_, which is executed on the given collection\\n            with results used to create Dataset. If None, the entire collection will\\n            be read.\\n        schema: The schema used to read the collection. If None, it\\'ll be inferred from\\n            the results of pipeline.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        mongo_args: kwargs passed to `aggregate_arrow_all() <https://mongo-arrow            .readthedocs.io/en/latest/api/api.html#pymongoarrow.api            aggregate_arrow_all>`_ in pymongoarrow in producing\\n            Arrow-formatted results.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing rows from the results of executing the pipeline on the specified MongoDB collection.\\n\\n    Raises:\\n        ValueError: if ``database`` doesn\\'t exist.\\n        ValueError: if ``collection`` doesn\\'t exist.\\n    '\n    datasource = MongoDatasource(uri=uri, database=database, collection=collection, pipeline=pipeline, schema=schema, **mongo_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_mongo(uri: str, database: str, collection: str, *, pipeline: Optional[List[Dict]]=None, schema: Optional['pymongoarrow.api.Schema']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **mongo_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a MongoDB database.\\n\\n    The data to read from is specified via the ``uri``, ``database`` and ``collection``\\n    of the MongoDB. The dataset is created from the results of executing\\n    ``pipeline`` against the ``collection``. If ``pipeline`` is None, the entire\\n    ``collection`` is read.\\n\\n    .. tip::\\n\\n        For more details about these MongoDB concepts, see the following:\\n        - URI: https://www.mongodb.com/docs/manual/reference/connection-string/\\n        - Database and Collection: https://www.mongodb.com/docs/manual/core/databases-and-collections/\\n        - Pipeline: https://www.mongodb.com/docs/manual/core/aggregation-pipeline/\\n\\n    To read the MongoDB in parallel, the execution of the pipeline is run on partitions\\n    of the collection, with a Ray read task to handle a partition. Partitions are\\n    created in an attempt to evenly distribute the documents into the specified number\\n    of partitions. The number of partitions is determined by ``parallelism`` which can\\n    be requested from this interface or automatically chosen if unspecified (see the\\n    ``parallelism`` arg below).\\n\\n    Examples:\\n        >>> import ray\\n        >>> from pymongoarrow.api import Schema # doctest: +SKIP\\n        >>> ds = ray.data.read_mongo( # doctest: +SKIP\\n        ...     uri=\"mongodb://username:password@mongodb0.example.com:27017/?authSource=admin\", # noqa: E501\\n        ...     database=\"my_db\",\\n        ...     collection=\"my_collection\",\\n        ...     pipeline=[{\"$match\": {\"col2\": {\"$gte\": 0, \"$lt\": 100}}}, {\"$sort\": \"sort_field\"}], # noqa: E501\\n        ...     schema=Schema({\"col1\": pa.string(), \"col2\": pa.int64()}),\\n        ...     parallelism=10,\\n        ... )\\n\\n    Args:\\n        uri: The URI of the source MongoDB where the dataset is\\n            read from. For the URI format, see details in the `MongoDB docs <https:/                /www.mongodb.com/docs/manual/reference/connection-string/>`_.\\n        database: The name of the database hosted in the MongoDB. This database\\n            must exist otherwise ValueError is raised.\\n        collection: The name of the collection in the database. This collection\\n            must exist otherwise ValueError is raised.\\n        pipeline: A `MongoDB pipeline <https://www.mongodb.com/docs/manual/core            /aggregation-pipeline/>`_, which is executed on the given collection\\n            with results used to create Dataset. If None, the entire collection will\\n            be read.\\n        schema: The schema used to read the collection. If None, it\\'ll be inferred from\\n            the results of pipeline.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        mongo_args: kwargs passed to `aggregate_arrow_all() <https://mongo-arrow            .readthedocs.io/en/latest/api/api.html#pymongoarrow.api            aggregate_arrow_all>`_ in pymongoarrow in producing\\n            Arrow-formatted results.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing rows from the results of executing the pipeline on the specified MongoDB collection.\\n\\n    Raises:\\n        ValueError: if ``database`` doesn\\'t exist.\\n        ValueError: if ``collection`` doesn\\'t exist.\\n    '\n    datasource = MongoDatasource(uri=uri, database=database, collection=collection, pipeline=pipeline, schema=schema, **mongo_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_mongo(uri: str, database: str, collection: str, *, pipeline: Optional[List[Dict]]=None, schema: Optional['pymongoarrow.api.Schema']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **mongo_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a MongoDB database.\\n\\n    The data to read from is specified via the ``uri``, ``database`` and ``collection``\\n    of the MongoDB. The dataset is created from the results of executing\\n    ``pipeline`` against the ``collection``. If ``pipeline`` is None, the entire\\n    ``collection`` is read.\\n\\n    .. tip::\\n\\n        For more details about these MongoDB concepts, see the following:\\n        - URI: https://www.mongodb.com/docs/manual/reference/connection-string/\\n        - Database and Collection: https://www.mongodb.com/docs/manual/core/databases-and-collections/\\n        - Pipeline: https://www.mongodb.com/docs/manual/core/aggregation-pipeline/\\n\\n    To read the MongoDB in parallel, the execution of the pipeline is run on partitions\\n    of the collection, with a Ray read task to handle a partition. Partitions are\\n    created in an attempt to evenly distribute the documents into the specified number\\n    of partitions. The number of partitions is determined by ``parallelism`` which can\\n    be requested from this interface or automatically chosen if unspecified (see the\\n    ``parallelism`` arg below).\\n\\n    Examples:\\n        >>> import ray\\n        >>> from pymongoarrow.api import Schema # doctest: +SKIP\\n        >>> ds = ray.data.read_mongo( # doctest: +SKIP\\n        ...     uri=\"mongodb://username:password@mongodb0.example.com:27017/?authSource=admin\", # noqa: E501\\n        ...     database=\"my_db\",\\n        ...     collection=\"my_collection\",\\n        ...     pipeline=[{\"$match\": {\"col2\": {\"$gte\": 0, \"$lt\": 100}}}, {\"$sort\": \"sort_field\"}], # noqa: E501\\n        ...     schema=Schema({\"col1\": pa.string(), \"col2\": pa.int64()}),\\n        ...     parallelism=10,\\n        ... )\\n\\n    Args:\\n        uri: The URI of the source MongoDB where the dataset is\\n            read from. For the URI format, see details in the `MongoDB docs <https:/                /www.mongodb.com/docs/manual/reference/connection-string/>`_.\\n        database: The name of the database hosted in the MongoDB. This database\\n            must exist otherwise ValueError is raised.\\n        collection: The name of the collection in the database. This collection\\n            must exist otherwise ValueError is raised.\\n        pipeline: A `MongoDB pipeline <https://www.mongodb.com/docs/manual/core            /aggregation-pipeline/>`_, which is executed on the given collection\\n            with results used to create Dataset. If None, the entire collection will\\n            be read.\\n        schema: The schema used to read the collection. If None, it\\'ll be inferred from\\n            the results of pipeline.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        mongo_args: kwargs passed to `aggregate_arrow_all() <https://mongo-arrow            .readthedocs.io/en/latest/api/api.html#pymongoarrow.api            aggregate_arrow_all>`_ in pymongoarrow in producing\\n            Arrow-formatted results.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing rows from the results of executing the pipeline on the specified MongoDB collection.\\n\\n    Raises:\\n        ValueError: if ``database`` doesn\\'t exist.\\n        ValueError: if ``collection`` doesn\\'t exist.\\n    '\n    datasource = MongoDatasource(uri=uri, database=database, collection=collection, pipeline=pipeline, schema=schema, **mongo_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_mongo(uri: str, database: str, collection: str, *, pipeline: Optional[List[Dict]]=None, schema: Optional['pymongoarrow.api.Schema']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **mongo_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a MongoDB database.\\n\\n    The data to read from is specified via the ``uri``, ``database`` and ``collection``\\n    of the MongoDB. The dataset is created from the results of executing\\n    ``pipeline`` against the ``collection``. If ``pipeline`` is None, the entire\\n    ``collection`` is read.\\n\\n    .. tip::\\n\\n        For more details about these MongoDB concepts, see the following:\\n        - URI: https://www.mongodb.com/docs/manual/reference/connection-string/\\n        - Database and Collection: https://www.mongodb.com/docs/manual/core/databases-and-collections/\\n        - Pipeline: https://www.mongodb.com/docs/manual/core/aggregation-pipeline/\\n\\n    To read the MongoDB in parallel, the execution of the pipeline is run on partitions\\n    of the collection, with a Ray read task to handle a partition. Partitions are\\n    created in an attempt to evenly distribute the documents into the specified number\\n    of partitions. The number of partitions is determined by ``parallelism`` which can\\n    be requested from this interface or automatically chosen if unspecified (see the\\n    ``parallelism`` arg below).\\n\\n    Examples:\\n        >>> import ray\\n        >>> from pymongoarrow.api import Schema # doctest: +SKIP\\n        >>> ds = ray.data.read_mongo( # doctest: +SKIP\\n        ...     uri=\"mongodb://username:password@mongodb0.example.com:27017/?authSource=admin\", # noqa: E501\\n        ...     database=\"my_db\",\\n        ...     collection=\"my_collection\",\\n        ...     pipeline=[{\"$match\": {\"col2\": {\"$gte\": 0, \"$lt\": 100}}}, {\"$sort\": \"sort_field\"}], # noqa: E501\\n        ...     schema=Schema({\"col1\": pa.string(), \"col2\": pa.int64()}),\\n        ...     parallelism=10,\\n        ... )\\n\\n    Args:\\n        uri: The URI of the source MongoDB where the dataset is\\n            read from. For the URI format, see details in the `MongoDB docs <https:/                /www.mongodb.com/docs/manual/reference/connection-string/>`_.\\n        database: The name of the database hosted in the MongoDB. This database\\n            must exist otherwise ValueError is raised.\\n        collection: The name of the collection in the database. This collection\\n            must exist otherwise ValueError is raised.\\n        pipeline: A `MongoDB pipeline <https://www.mongodb.com/docs/manual/core            /aggregation-pipeline/>`_, which is executed on the given collection\\n            with results used to create Dataset. If None, the entire collection will\\n            be read.\\n        schema: The schema used to read the collection. If None, it\\'ll be inferred from\\n            the results of pipeline.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        mongo_args: kwargs passed to `aggregate_arrow_all() <https://mongo-arrow            .readthedocs.io/en/latest/api/api.html#pymongoarrow.api            aggregate_arrow_all>`_ in pymongoarrow in producing\\n            Arrow-formatted results.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing rows from the results of executing the pipeline on the specified MongoDB collection.\\n\\n    Raises:\\n        ValueError: if ``database`` doesn\\'t exist.\\n        ValueError: if ``collection`` doesn\\'t exist.\\n    '\n    datasource = MongoDatasource(uri=uri, database=database, collection=collection, pipeline=pipeline, schema=schema, **mongo_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_mongo(uri: str, database: str, collection: str, *, pipeline: Optional[List[Dict]]=None, schema: Optional['pymongoarrow.api.Schema']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, **mongo_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a MongoDB database.\\n\\n    The data to read from is specified via the ``uri``, ``database`` and ``collection``\\n    of the MongoDB. The dataset is created from the results of executing\\n    ``pipeline`` against the ``collection``. If ``pipeline`` is None, the entire\\n    ``collection`` is read.\\n\\n    .. tip::\\n\\n        For more details about these MongoDB concepts, see the following:\\n        - URI: https://www.mongodb.com/docs/manual/reference/connection-string/\\n        - Database and Collection: https://www.mongodb.com/docs/manual/core/databases-and-collections/\\n        - Pipeline: https://www.mongodb.com/docs/manual/core/aggregation-pipeline/\\n\\n    To read the MongoDB in parallel, the execution of the pipeline is run on partitions\\n    of the collection, with a Ray read task to handle a partition. Partitions are\\n    created in an attempt to evenly distribute the documents into the specified number\\n    of partitions. The number of partitions is determined by ``parallelism`` which can\\n    be requested from this interface or automatically chosen if unspecified (see the\\n    ``parallelism`` arg below).\\n\\n    Examples:\\n        >>> import ray\\n        >>> from pymongoarrow.api import Schema # doctest: +SKIP\\n        >>> ds = ray.data.read_mongo( # doctest: +SKIP\\n        ...     uri=\"mongodb://username:password@mongodb0.example.com:27017/?authSource=admin\", # noqa: E501\\n        ...     database=\"my_db\",\\n        ...     collection=\"my_collection\",\\n        ...     pipeline=[{\"$match\": {\"col2\": {\"$gte\": 0, \"$lt\": 100}}}, {\"$sort\": \"sort_field\"}], # noqa: E501\\n        ...     schema=Schema({\"col1\": pa.string(), \"col2\": pa.int64()}),\\n        ...     parallelism=10,\\n        ... )\\n\\n    Args:\\n        uri: The URI of the source MongoDB where the dataset is\\n            read from. For the URI format, see details in the `MongoDB docs <https:/                /www.mongodb.com/docs/manual/reference/connection-string/>`_.\\n        database: The name of the database hosted in the MongoDB. This database\\n            must exist otherwise ValueError is raised.\\n        collection: The name of the collection in the database. This collection\\n            must exist otherwise ValueError is raised.\\n        pipeline: A `MongoDB pipeline <https://www.mongodb.com/docs/manual/core            /aggregation-pipeline/>`_, which is executed on the given collection\\n            with results used to create Dataset. If None, the entire collection will\\n            be read.\\n        schema: The schema used to read the collection. If None, it\\'ll be inferred from\\n            the results of pipeline.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        mongo_args: kwargs passed to `aggregate_arrow_all() <https://mongo-arrow            .readthedocs.io/en/latest/api/api.html#pymongoarrow.api            aggregate_arrow_all>`_ in pymongoarrow in producing\\n            Arrow-formatted results.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing rows from the results of executing the pipeline on the specified MongoDB collection.\\n\\n    Raises:\\n        ValueError: if ``database`` doesn\\'t exist.\\n        ValueError: if ``collection`` doesn\\'t exist.\\n    '\n    datasource = MongoDatasource(uri=uri, database=database, collection=collection, pipeline=pipeline, schema=schema, **mongo_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "read_bigquery",
        "original": "@PublicAPI(stability='alpha')\ndef read_bigquery(project_id: str, dataset: Optional[str]=None, query: Optional[str]=None, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None) -> Dataset:\n    \"\"\"Create a dataset from BigQuery.\n\n    The data to read from is specified via the ``project_id``, ``dataset``\n    and/or ``query`` parameters. The dataset is created from the results of\n    executing ``query`` if a query is provided. Otherwise, the entire\n    ``dataset`` is read.\n\n    For more information about BigQuery, see the following concepts:\n\n    - Project id: `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_\n\n    - Dataset: `Datasets Intro <https://cloud.google.com/bigquery/docs/datasets-intro>`_\n\n    - Query: `Query Syntax <https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax>`_\n\n    This method uses the BigQuery Storage Read API which reads in parallel,\n    with a Ray read task to handle each stream. The number of streams is\n    determined by ``parallelism`` which can be requested from this interface\n    or automatically chosen if unspecified (see the ``parallelism`` arg below).\n\n    .. warning::\n        The maximum query response size is 10GB. For more information, see `BigQuery response too large to return <https://cloud.google.com/knowledge/kb/bigquery-response-too-large-to-return-consider-setting-allowlargeresults-to-true-in-your-job-configuration-000004266>`_.\n\n    Examples:\n        .. testcode::\n            :skipif: True\n\n            import ray\n            # Users will need to authenticate beforehand (https://cloud.google.com/sdk/gcloud/reference/auth/login)\n            ds = ray.data.read_bigquery(\n                project_id=\"my_project\",\n                query=\"SELECT * FROM `bigquery-public-data.samples.gsod` LIMIT 1000\",\n            )\n\n    Args:\n        project_id: The name of the associated Google Cloud Project that hosts the dataset to read.\n            For more information, see `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_.\n        dataset: The name of the dataset hosted in BigQuery in the format of ``dataset_id.table_id``.\n            Both the dataset_id and table_id must exist otherwise an exception will be raised.\n        parallelism: The requested parallelism of the read. If -1, it will be\n            automatically chosen based on the available cluster resources and estimated\n            in-memory data size.\n        ray_remote_args: kwargs passed to ray.remote in the read tasks.\n\n    Returns:\n        Dataset producing rows from the results of executing the query (or reading the entire dataset)\n        on the specified BigQuery dataset.\n    \"\"\"\n    datasource = BigQueryDatasource(project_id=project_id, dataset=dataset, query=query)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI(stability='alpha')\ndef read_bigquery(project_id: str, dataset: Optional[str]=None, query: Optional[str]=None, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None) -> Dataset:\n    if False:\n        i = 10\n    'Create a dataset from BigQuery.\\n\\n    The data to read from is specified via the ``project_id``, ``dataset``\\n    and/or ``query`` parameters. The dataset is created from the results of\\n    executing ``query`` if a query is provided. Otherwise, the entire\\n    ``dataset`` is read.\\n\\n    For more information about BigQuery, see the following concepts:\\n\\n    - Project id: `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_\\n\\n    - Dataset: `Datasets Intro <https://cloud.google.com/bigquery/docs/datasets-intro>`_\\n\\n    - Query: `Query Syntax <https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax>`_\\n\\n    This method uses the BigQuery Storage Read API which reads in parallel,\\n    with a Ray read task to handle each stream. The number of streams is\\n    determined by ``parallelism`` which can be requested from this interface\\n    or automatically chosen if unspecified (see the ``parallelism`` arg below).\\n\\n    .. warning::\\n        The maximum query response size is 10GB. For more information, see `BigQuery response too large to return <https://cloud.google.com/knowledge/kb/bigquery-response-too-large-to-return-consider-setting-allowlargeresults-to-true-in-your-job-configuration-000004266>`_.\\n\\n    Examples:\\n        .. testcode::\\n            :skipif: True\\n\\n            import ray\\n            # Users will need to authenticate beforehand (https://cloud.google.com/sdk/gcloud/reference/auth/login)\\n            ds = ray.data.read_bigquery(\\n                project_id=\"my_project\",\\n                query=\"SELECT * FROM `bigquery-public-data.samples.gsod` LIMIT 1000\",\\n            )\\n\\n    Args:\\n        project_id: The name of the associated Google Cloud Project that hosts the dataset to read.\\n            For more information, see `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_.\\n        dataset: The name of the dataset hosted in BigQuery in the format of ``dataset_id.table_id``.\\n            Both the dataset_id and table_id must exist otherwise an exception will be raised.\\n        parallelism: The requested parallelism of the read. If -1, it will be\\n            automatically chosen based on the available cluster resources and estimated\\n            in-memory data size.\\n        ray_remote_args: kwargs passed to ray.remote in the read tasks.\\n\\n    Returns:\\n        Dataset producing rows from the results of executing the query (or reading the entire dataset)\\n        on the specified BigQuery dataset.\\n    '\n    datasource = BigQueryDatasource(project_id=project_id, dataset=dataset, query=query)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_bigquery(project_id: str, dataset: Optional[str]=None, query: Optional[str]=None, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a dataset from BigQuery.\\n\\n    The data to read from is specified via the ``project_id``, ``dataset``\\n    and/or ``query`` parameters. The dataset is created from the results of\\n    executing ``query`` if a query is provided. Otherwise, the entire\\n    ``dataset`` is read.\\n\\n    For more information about BigQuery, see the following concepts:\\n\\n    - Project id: `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_\\n\\n    - Dataset: `Datasets Intro <https://cloud.google.com/bigquery/docs/datasets-intro>`_\\n\\n    - Query: `Query Syntax <https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax>`_\\n\\n    This method uses the BigQuery Storage Read API which reads in parallel,\\n    with a Ray read task to handle each stream. The number of streams is\\n    determined by ``parallelism`` which can be requested from this interface\\n    or automatically chosen if unspecified (see the ``parallelism`` arg below).\\n\\n    .. warning::\\n        The maximum query response size is 10GB. For more information, see `BigQuery response too large to return <https://cloud.google.com/knowledge/kb/bigquery-response-too-large-to-return-consider-setting-allowlargeresults-to-true-in-your-job-configuration-000004266>`_.\\n\\n    Examples:\\n        .. testcode::\\n            :skipif: True\\n\\n            import ray\\n            # Users will need to authenticate beforehand (https://cloud.google.com/sdk/gcloud/reference/auth/login)\\n            ds = ray.data.read_bigquery(\\n                project_id=\"my_project\",\\n                query=\"SELECT * FROM `bigquery-public-data.samples.gsod` LIMIT 1000\",\\n            )\\n\\n    Args:\\n        project_id: The name of the associated Google Cloud Project that hosts the dataset to read.\\n            For more information, see `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_.\\n        dataset: The name of the dataset hosted in BigQuery in the format of ``dataset_id.table_id``.\\n            Both the dataset_id and table_id must exist otherwise an exception will be raised.\\n        parallelism: The requested parallelism of the read. If -1, it will be\\n            automatically chosen based on the available cluster resources and estimated\\n            in-memory data size.\\n        ray_remote_args: kwargs passed to ray.remote in the read tasks.\\n\\n    Returns:\\n        Dataset producing rows from the results of executing the query (or reading the entire dataset)\\n        on the specified BigQuery dataset.\\n    '\n    datasource = BigQueryDatasource(project_id=project_id, dataset=dataset, query=query)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_bigquery(project_id: str, dataset: Optional[str]=None, query: Optional[str]=None, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a dataset from BigQuery.\\n\\n    The data to read from is specified via the ``project_id``, ``dataset``\\n    and/or ``query`` parameters. The dataset is created from the results of\\n    executing ``query`` if a query is provided. Otherwise, the entire\\n    ``dataset`` is read.\\n\\n    For more information about BigQuery, see the following concepts:\\n\\n    - Project id: `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_\\n\\n    - Dataset: `Datasets Intro <https://cloud.google.com/bigquery/docs/datasets-intro>`_\\n\\n    - Query: `Query Syntax <https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax>`_\\n\\n    This method uses the BigQuery Storage Read API which reads in parallel,\\n    with a Ray read task to handle each stream. The number of streams is\\n    determined by ``parallelism`` which can be requested from this interface\\n    or automatically chosen if unspecified (see the ``parallelism`` arg below).\\n\\n    .. warning::\\n        The maximum query response size is 10GB. For more information, see `BigQuery response too large to return <https://cloud.google.com/knowledge/kb/bigquery-response-too-large-to-return-consider-setting-allowlargeresults-to-true-in-your-job-configuration-000004266>`_.\\n\\n    Examples:\\n        .. testcode::\\n            :skipif: True\\n\\n            import ray\\n            # Users will need to authenticate beforehand (https://cloud.google.com/sdk/gcloud/reference/auth/login)\\n            ds = ray.data.read_bigquery(\\n                project_id=\"my_project\",\\n                query=\"SELECT * FROM `bigquery-public-data.samples.gsod` LIMIT 1000\",\\n            )\\n\\n    Args:\\n        project_id: The name of the associated Google Cloud Project that hosts the dataset to read.\\n            For more information, see `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_.\\n        dataset: The name of the dataset hosted in BigQuery in the format of ``dataset_id.table_id``.\\n            Both the dataset_id and table_id must exist otherwise an exception will be raised.\\n        parallelism: The requested parallelism of the read. If -1, it will be\\n            automatically chosen based on the available cluster resources and estimated\\n            in-memory data size.\\n        ray_remote_args: kwargs passed to ray.remote in the read tasks.\\n\\n    Returns:\\n        Dataset producing rows from the results of executing the query (or reading the entire dataset)\\n        on the specified BigQuery dataset.\\n    '\n    datasource = BigQueryDatasource(project_id=project_id, dataset=dataset, query=query)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_bigquery(project_id: str, dataset: Optional[str]=None, query: Optional[str]=None, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a dataset from BigQuery.\\n\\n    The data to read from is specified via the ``project_id``, ``dataset``\\n    and/or ``query`` parameters. The dataset is created from the results of\\n    executing ``query`` if a query is provided. Otherwise, the entire\\n    ``dataset`` is read.\\n\\n    For more information about BigQuery, see the following concepts:\\n\\n    - Project id: `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_\\n\\n    - Dataset: `Datasets Intro <https://cloud.google.com/bigquery/docs/datasets-intro>`_\\n\\n    - Query: `Query Syntax <https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax>`_\\n\\n    This method uses the BigQuery Storage Read API which reads in parallel,\\n    with a Ray read task to handle each stream. The number of streams is\\n    determined by ``parallelism`` which can be requested from this interface\\n    or automatically chosen if unspecified (see the ``parallelism`` arg below).\\n\\n    .. warning::\\n        The maximum query response size is 10GB. For more information, see `BigQuery response too large to return <https://cloud.google.com/knowledge/kb/bigquery-response-too-large-to-return-consider-setting-allowlargeresults-to-true-in-your-job-configuration-000004266>`_.\\n\\n    Examples:\\n        .. testcode::\\n            :skipif: True\\n\\n            import ray\\n            # Users will need to authenticate beforehand (https://cloud.google.com/sdk/gcloud/reference/auth/login)\\n            ds = ray.data.read_bigquery(\\n                project_id=\"my_project\",\\n                query=\"SELECT * FROM `bigquery-public-data.samples.gsod` LIMIT 1000\",\\n            )\\n\\n    Args:\\n        project_id: The name of the associated Google Cloud Project that hosts the dataset to read.\\n            For more information, see `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_.\\n        dataset: The name of the dataset hosted in BigQuery in the format of ``dataset_id.table_id``.\\n            Both the dataset_id and table_id must exist otherwise an exception will be raised.\\n        parallelism: The requested parallelism of the read. If -1, it will be\\n            automatically chosen based on the available cluster resources and estimated\\n            in-memory data size.\\n        ray_remote_args: kwargs passed to ray.remote in the read tasks.\\n\\n    Returns:\\n        Dataset producing rows from the results of executing the query (or reading the entire dataset)\\n        on the specified BigQuery dataset.\\n    '\n    datasource = BigQueryDatasource(project_id=project_id, dataset=dataset, query=query)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_bigquery(project_id: str, dataset: Optional[str]=None, query: Optional[str]=None, *, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a dataset from BigQuery.\\n\\n    The data to read from is specified via the ``project_id``, ``dataset``\\n    and/or ``query`` parameters. The dataset is created from the results of\\n    executing ``query`` if a query is provided. Otherwise, the entire\\n    ``dataset`` is read.\\n\\n    For more information about BigQuery, see the following concepts:\\n\\n    - Project id: `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_\\n\\n    - Dataset: `Datasets Intro <https://cloud.google.com/bigquery/docs/datasets-intro>`_\\n\\n    - Query: `Query Syntax <https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax>`_\\n\\n    This method uses the BigQuery Storage Read API which reads in parallel,\\n    with a Ray read task to handle each stream. The number of streams is\\n    determined by ``parallelism`` which can be requested from this interface\\n    or automatically chosen if unspecified (see the ``parallelism`` arg below).\\n\\n    .. warning::\\n        The maximum query response size is 10GB. For more information, see `BigQuery response too large to return <https://cloud.google.com/knowledge/kb/bigquery-response-too-large-to-return-consider-setting-allowlargeresults-to-true-in-your-job-configuration-000004266>`_.\\n\\n    Examples:\\n        .. testcode::\\n            :skipif: True\\n\\n            import ray\\n            # Users will need to authenticate beforehand (https://cloud.google.com/sdk/gcloud/reference/auth/login)\\n            ds = ray.data.read_bigquery(\\n                project_id=\"my_project\",\\n                query=\"SELECT * FROM `bigquery-public-data.samples.gsod` LIMIT 1000\",\\n            )\\n\\n    Args:\\n        project_id: The name of the associated Google Cloud Project that hosts the dataset to read.\\n            For more information, see `Creating and Managing Projects <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_.\\n        dataset: The name of the dataset hosted in BigQuery in the format of ``dataset_id.table_id``.\\n            Both the dataset_id and table_id must exist otherwise an exception will be raised.\\n        parallelism: The requested parallelism of the read. If -1, it will be\\n            automatically chosen based on the available cluster resources and estimated\\n            in-memory data size.\\n        ray_remote_args: kwargs passed to ray.remote in the read tasks.\\n\\n    Returns:\\n        Dataset producing rows from the results of executing the query (or reading the entire dataset)\\n        on the specified BigQuery dataset.\\n    '\n    datasource = BigQueryDatasource(project_id=project_id, dataset=dataset, query=query)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "read_parquet",
        "original": "@PublicAPI\ndef read_parquet(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[ParquetMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_parquet_args) -> Dataset:\n    \"\"\"Creates a :class:`~ray.data.Dataset` from parquet files.\n\n\n    Examples:\n        Read a file in remote storage.\n\n        >>> import ray\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\n        >>> ds.schema()\n        Column        Type\n        ------        ----\n        sepal.length  double\n        sepal.width   double\n        petal.length  double\n        petal.width   double\n        variety       string\n\n        Read a directory in remote storage.\n\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris-parquet/\")\n\n        Read multiple local files.\n\n        >>> ray.data.read_parquet(\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"]) # doctest: +SKIP\n\n        Specify a schema for the parquet file.\n\n        >>> import pyarrow as pa\n        >>> fields = [(\"sepal.length\", pa.float32()),\n        ...           (\"sepal.width\", pa.float32()),\n        ...           (\"petal.length\", pa.float32()),\n        ...           (\"petal.width\", pa.float32()),\n        ...           (\"variety\", pa.string())]\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\",\n        ...     schema=pa.schema(fields))\n        >>> ds.schema()\n        Column        Type\n        ------        ----\n        sepal.length  float\n        sepal.width   float\n        petal.length  float\n        petal.width   float\n        variety       string\n\n        The Parquet reader also supports projection and filter pushdown, allowing column\n        selection and row filtering to be pushed down to the file scan.\n\n        .. testcode::\n\n            import pyarrow as pa\n\n            # Create a Dataset by reading a Parquet file, pushing column selection and\n            # row filtering down to the file scan.\n            ds = ray.data.read_parquet(\n                \"s3://anonymous@ray-example-data/iris.parquet\",\n                columns=[\"sepal.length\", \"variety\"],\n                filter=pa.dataset.field(\"sepal.length\") > 5.0,\n            )\n\n            ds.show(2)\n\n        .. testoutput::\n\n            {'sepal.length': 5.1, 'variety': 'Setosa'}\n            {'sepal.length': 5.4, 'variety': 'Setosa'}\n\n        For further arguments you can pass to PyArrow as a keyword argument, see the\n        `PyArrow API reference <https://arrow.apache.org/docs/python/generated/        pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_.\n\n    Args:\n        paths: A single file path or directory, or a list of file paths. Multiple\n            directories are not supported.\n        filesystem: The PyArrow filesystem\n            implementation to read from. These filesystems are specified in the\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\n            you need to provide specific configurations to the filesystem. By default,\n            the filesystem is automatically selected based on the scheme of the paths.\n            For example, if the path begins with ``s3://``, the ``S3FileSystem`` is\n            used. If ``None``, this function uses a system-chosen implementation.\n        columns: A list of column names to read. Only the specified columns are\n            read during the file scan.\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see :ref:`Tuning read parallelism\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\n            records in all the parquet files.\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\n            mappings for converting a Parquet column containing serialized\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\n            assumes that the tensors are serialized in the raw\n            NumPy array format in C-contiguous order (e.g., via\n            `arr.tobytes()`).\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\n            metadata providers may be able to resolve file metadata more quickly and/or\n            accurately. In most cases you do not need to set this parameter.\n        partition_filter: A\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\n            with a custom callback to read only selected partitions of a dataset.\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\n            Defaults to not shuffle with ``None``.\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\n            set of arguments, see the`PyArrow API <https://arrow.apache.org/docs/                python/generated/pyarrow.dataset.Scanner.html                    #pyarrow.dataset.Scanner.from_fragment>`_\n        file_extensions: A list of file extensions to filter files by.\n\n    Returns:\n        :class:`~ray.data.Dataset` producing records read from the specified parquet\n        files.\n    \"\"\"\n    if meta_provider is None:\n        meta_provider = get_parquet_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    dataset_kwargs = arrow_parquet_args.pop('dataset_kwargs', None)\n    _block_udf = arrow_parquet_args.pop('_block_udf', None)\n    schema = arrow_parquet_args.pop('schema', None)\n    datasource = ParquetDatasource(paths, columns=columns, dataset_kwargs=dataset_kwargs, to_batch_kwargs=arrow_parquet_args, _block_udf=_block_udf, filesystem=filesystem, schema=schema, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI\ndef read_parquet(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[ParquetMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_parquet_args) -> Dataset:\n    if False:\n        i = 10\n    'Creates a :class:`~ray.data.Dataset` from parquet files.\\n\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  double\\n        sepal.width   double\\n        petal.length  double\\n        petal.width   double\\n        variety       string\\n\\n        Read a directory in remote storage.\\n\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris-parquet/\")\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_parquet(\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"]) # doctest: +SKIP\\n\\n        Specify a schema for the parquet file.\\n\\n        >>> import pyarrow as pa\\n        >>> fields = [(\"sepal.length\", pa.float32()),\\n        ...           (\"sepal.width\", pa.float32()),\\n        ...           (\"petal.length\", pa.float32()),\\n        ...           (\"petal.width\", pa.float32()),\\n        ...           (\"variety\", pa.string())]\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\",\\n        ...     schema=pa.schema(fields))\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  float\\n        sepal.width   float\\n        petal.length  float\\n        petal.width   float\\n        variety       string\\n\\n        The Parquet reader also supports projection and filter pushdown, allowing column\\n        selection and row filtering to be pushed down to the file scan.\\n\\n        .. testcode::\\n\\n            import pyarrow as pa\\n\\n            # Create a Dataset by reading a Parquet file, pushing column selection and\\n            # row filtering down to the file scan.\\n            ds = ray.data.read_parquet(\\n                \"s3://anonymous@ray-example-data/iris.parquet\",\\n                columns=[\"sepal.length\", \"variety\"],\\n                filter=pa.dataset.field(\"sepal.length\") > 5.0,\\n            )\\n\\n            ds.show(2)\\n\\n        .. testoutput::\\n\\n            {\\'sepal.length\\': 5.1, \\'variety\\': \\'Setosa\\'}\\n            {\\'sepal.length\\': 5.4, \\'variety\\': \\'Setosa\\'}\\n\\n        For further arguments you can pass to PyArrow as a keyword argument, see the\\n        `PyArrow API reference <https://arrow.apache.org/docs/python/generated/        pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_.\\n\\n    Args:\\n        paths: A single file path or directory, or a list of file paths. Multiple\\n            directories are not supported.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the ``S3FileSystem`` is\\n            used. If ``None``, this function uses a system-chosen implementation.\\n        columns: A list of column names to read. Only the specified columns are\\n            read during the file scan.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the parquet files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\\n            mappings for converting a Parquet column containing serialized\\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\\n            assumes that the tensors are serialized in the raw\\n            NumPy array format in C-contiguous order (e.g., via\\n            `arr.tobytes()`).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases you do not need to set this parameter.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\\n            set of arguments, see the`PyArrow API <https://arrow.apache.org/docs/                python/generated/pyarrow.dataset.Scanner.html                    #pyarrow.dataset.Scanner.from_fragment>`_\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified parquet\\n        files.\\n    '\n    if meta_provider is None:\n        meta_provider = get_parquet_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    dataset_kwargs = arrow_parquet_args.pop('dataset_kwargs', None)\n    _block_udf = arrow_parquet_args.pop('_block_udf', None)\n    schema = arrow_parquet_args.pop('schema', None)\n    datasource = ParquetDatasource(paths, columns=columns, dataset_kwargs=dataset_kwargs, to_batch_kwargs=arrow_parquet_args, _block_udf=_block_udf, filesystem=filesystem, schema=schema, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_parquet(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[ParquetMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_parquet_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a :class:`~ray.data.Dataset` from parquet files.\\n\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  double\\n        sepal.width   double\\n        petal.length  double\\n        petal.width   double\\n        variety       string\\n\\n        Read a directory in remote storage.\\n\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris-parquet/\")\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_parquet(\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"]) # doctest: +SKIP\\n\\n        Specify a schema for the parquet file.\\n\\n        >>> import pyarrow as pa\\n        >>> fields = [(\"sepal.length\", pa.float32()),\\n        ...           (\"sepal.width\", pa.float32()),\\n        ...           (\"petal.length\", pa.float32()),\\n        ...           (\"petal.width\", pa.float32()),\\n        ...           (\"variety\", pa.string())]\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\",\\n        ...     schema=pa.schema(fields))\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  float\\n        sepal.width   float\\n        petal.length  float\\n        petal.width   float\\n        variety       string\\n\\n        The Parquet reader also supports projection and filter pushdown, allowing column\\n        selection and row filtering to be pushed down to the file scan.\\n\\n        .. testcode::\\n\\n            import pyarrow as pa\\n\\n            # Create a Dataset by reading a Parquet file, pushing column selection and\\n            # row filtering down to the file scan.\\n            ds = ray.data.read_parquet(\\n                \"s3://anonymous@ray-example-data/iris.parquet\",\\n                columns=[\"sepal.length\", \"variety\"],\\n                filter=pa.dataset.field(\"sepal.length\") > 5.0,\\n            )\\n\\n            ds.show(2)\\n\\n        .. testoutput::\\n\\n            {\\'sepal.length\\': 5.1, \\'variety\\': \\'Setosa\\'}\\n            {\\'sepal.length\\': 5.4, \\'variety\\': \\'Setosa\\'}\\n\\n        For further arguments you can pass to PyArrow as a keyword argument, see the\\n        `PyArrow API reference <https://arrow.apache.org/docs/python/generated/        pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_.\\n\\n    Args:\\n        paths: A single file path or directory, or a list of file paths. Multiple\\n            directories are not supported.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the ``S3FileSystem`` is\\n            used. If ``None``, this function uses a system-chosen implementation.\\n        columns: A list of column names to read. Only the specified columns are\\n            read during the file scan.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the parquet files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\\n            mappings for converting a Parquet column containing serialized\\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\\n            assumes that the tensors are serialized in the raw\\n            NumPy array format in C-contiguous order (e.g., via\\n            `arr.tobytes()`).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases you do not need to set this parameter.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\\n            set of arguments, see the`PyArrow API <https://arrow.apache.org/docs/                python/generated/pyarrow.dataset.Scanner.html                    #pyarrow.dataset.Scanner.from_fragment>`_\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified parquet\\n        files.\\n    '\n    if meta_provider is None:\n        meta_provider = get_parquet_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    dataset_kwargs = arrow_parquet_args.pop('dataset_kwargs', None)\n    _block_udf = arrow_parquet_args.pop('_block_udf', None)\n    schema = arrow_parquet_args.pop('schema', None)\n    datasource = ParquetDatasource(paths, columns=columns, dataset_kwargs=dataset_kwargs, to_batch_kwargs=arrow_parquet_args, _block_udf=_block_udf, filesystem=filesystem, schema=schema, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_parquet(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[ParquetMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_parquet_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a :class:`~ray.data.Dataset` from parquet files.\\n\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  double\\n        sepal.width   double\\n        petal.length  double\\n        petal.width   double\\n        variety       string\\n\\n        Read a directory in remote storage.\\n\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris-parquet/\")\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_parquet(\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"]) # doctest: +SKIP\\n\\n        Specify a schema for the parquet file.\\n\\n        >>> import pyarrow as pa\\n        >>> fields = [(\"sepal.length\", pa.float32()),\\n        ...           (\"sepal.width\", pa.float32()),\\n        ...           (\"petal.length\", pa.float32()),\\n        ...           (\"petal.width\", pa.float32()),\\n        ...           (\"variety\", pa.string())]\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\",\\n        ...     schema=pa.schema(fields))\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  float\\n        sepal.width   float\\n        petal.length  float\\n        petal.width   float\\n        variety       string\\n\\n        The Parquet reader also supports projection and filter pushdown, allowing column\\n        selection and row filtering to be pushed down to the file scan.\\n\\n        .. testcode::\\n\\n            import pyarrow as pa\\n\\n            # Create a Dataset by reading a Parquet file, pushing column selection and\\n            # row filtering down to the file scan.\\n            ds = ray.data.read_parquet(\\n                \"s3://anonymous@ray-example-data/iris.parquet\",\\n                columns=[\"sepal.length\", \"variety\"],\\n                filter=pa.dataset.field(\"sepal.length\") > 5.0,\\n            )\\n\\n            ds.show(2)\\n\\n        .. testoutput::\\n\\n            {\\'sepal.length\\': 5.1, \\'variety\\': \\'Setosa\\'}\\n            {\\'sepal.length\\': 5.4, \\'variety\\': \\'Setosa\\'}\\n\\n        For further arguments you can pass to PyArrow as a keyword argument, see the\\n        `PyArrow API reference <https://arrow.apache.org/docs/python/generated/        pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_.\\n\\n    Args:\\n        paths: A single file path or directory, or a list of file paths. Multiple\\n            directories are not supported.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the ``S3FileSystem`` is\\n            used. If ``None``, this function uses a system-chosen implementation.\\n        columns: A list of column names to read. Only the specified columns are\\n            read during the file scan.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the parquet files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\\n            mappings for converting a Parquet column containing serialized\\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\\n            assumes that the tensors are serialized in the raw\\n            NumPy array format in C-contiguous order (e.g., via\\n            `arr.tobytes()`).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases you do not need to set this parameter.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\\n            set of arguments, see the`PyArrow API <https://arrow.apache.org/docs/                python/generated/pyarrow.dataset.Scanner.html                    #pyarrow.dataset.Scanner.from_fragment>`_\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified parquet\\n        files.\\n    '\n    if meta_provider is None:\n        meta_provider = get_parquet_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    dataset_kwargs = arrow_parquet_args.pop('dataset_kwargs', None)\n    _block_udf = arrow_parquet_args.pop('_block_udf', None)\n    schema = arrow_parquet_args.pop('schema', None)\n    datasource = ParquetDatasource(paths, columns=columns, dataset_kwargs=dataset_kwargs, to_batch_kwargs=arrow_parquet_args, _block_udf=_block_udf, filesystem=filesystem, schema=schema, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_parquet(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[ParquetMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_parquet_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a :class:`~ray.data.Dataset` from parquet files.\\n\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  double\\n        sepal.width   double\\n        petal.length  double\\n        petal.width   double\\n        variety       string\\n\\n        Read a directory in remote storage.\\n\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris-parquet/\")\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_parquet(\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"]) # doctest: +SKIP\\n\\n        Specify a schema for the parquet file.\\n\\n        >>> import pyarrow as pa\\n        >>> fields = [(\"sepal.length\", pa.float32()),\\n        ...           (\"sepal.width\", pa.float32()),\\n        ...           (\"petal.length\", pa.float32()),\\n        ...           (\"petal.width\", pa.float32()),\\n        ...           (\"variety\", pa.string())]\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\",\\n        ...     schema=pa.schema(fields))\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  float\\n        sepal.width   float\\n        petal.length  float\\n        petal.width   float\\n        variety       string\\n\\n        The Parquet reader also supports projection and filter pushdown, allowing column\\n        selection and row filtering to be pushed down to the file scan.\\n\\n        .. testcode::\\n\\n            import pyarrow as pa\\n\\n            # Create a Dataset by reading a Parquet file, pushing column selection and\\n            # row filtering down to the file scan.\\n            ds = ray.data.read_parquet(\\n                \"s3://anonymous@ray-example-data/iris.parquet\",\\n                columns=[\"sepal.length\", \"variety\"],\\n                filter=pa.dataset.field(\"sepal.length\") > 5.0,\\n            )\\n\\n            ds.show(2)\\n\\n        .. testoutput::\\n\\n            {\\'sepal.length\\': 5.1, \\'variety\\': \\'Setosa\\'}\\n            {\\'sepal.length\\': 5.4, \\'variety\\': \\'Setosa\\'}\\n\\n        For further arguments you can pass to PyArrow as a keyword argument, see the\\n        `PyArrow API reference <https://arrow.apache.org/docs/python/generated/        pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_.\\n\\n    Args:\\n        paths: A single file path or directory, or a list of file paths. Multiple\\n            directories are not supported.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the ``S3FileSystem`` is\\n            used. If ``None``, this function uses a system-chosen implementation.\\n        columns: A list of column names to read. Only the specified columns are\\n            read during the file scan.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the parquet files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\\n            mappings for converting a Parquet column containing serialized\\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\\n            assumes that the tensors are serialized in the raw\\n            NumPy array format in C-contiguous order (e.g., via\\n            `arr.tobytes()`).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases you do not need to set this parameter.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\\n            set of arguments, see the`PyArrow API <https://arrow.apache.org/docs/                python/generated/pyarrow.dataset.Scanner.html                    #pyarrow.dataset.Scanner.from_fragment>`_\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified parquet\\n        files.\\n    '\n    if meta_provider is None:\n        meta_provider = get_parquet_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    dataset_kwargs = arrow_parquet_args.pop('dataset_kwargs', None)\n    _block_udf = arrow_parquet_args.pop('_block_udf', None)\n    schema = arrow_parquet_args.pop('schema', None)\n    datasource = ParquetDatasource(paths, columns=columns, dataset_kwargs=dataset_kwargs, to_batch_kwargs=arrow_parquet_args, _block_udf=_block_udf, filesystem=filesystem, schema=schema, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_parquet(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[ParquetMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_parquet_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a :class:`~ray.data.Dataset` from parquet files.\\n\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  double\\n        sepal.width   double\\n        petal.length  double\\n        petal.width   double\\n        variety       string\\n\\n        Read a directory in remote storage.\\n\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris-parquet/\")\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_parquet(\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"]) # doctest: +SKIP\\n\\n        Specify a schema for the parquet file.\\n\\n        >>> import pyarrow as pa\\n        >>> fields = [(\"sepal.length\", pa.float32()),\\n        ...           (\"sepal.width\", pa.float32()),\\n        ...           (\"petal.length\", pa.float32()),\\n        ...           (\"petal.width\", pa.float32()),\\n        ...           (\"variety\", pa.string())]\\n        >>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\",\\n        ...     schema=pa.schema(fields))\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  float\\n        sepal.width   float\\n        petal.length  float\\n        petal.width   float\\n        variety       string\\n\\n        The Parquet reader also supports projection and filter pushdown, allowing column\\n        selection and row filtering to be pushed down to the file scan.\\n\\n        .. testcode::\\n\\n            import pyarrow as pa\\n\\n            # Create a Dataset by reading a Parquet file, pushing column selection and\\n            # row filtering down to the file scan.\\n            ds = ray.data.read_parquet(\\n                \"s3://anonymous@ray-example-data/iris.parquet\",\\n                columns=[\"sepal.length\", \"variety\"],\\n                filter=pa.dataset.field(\"sepal.length\") > 5.0,\\n            )\\n\\n            ds.show(2)\\n\\n        .. testoutput::\\n\\n            {\\'sepal.length\\': 5.1, \\'variety\\': \\'Setosa\\'}\\n            {\\'sepal.length\\': 5.4, \\'variety\\': \\'Setosa\\'}\\n\\n        For further arguments you can pass to PyArrow as a keyword argument, see the\\n        `PyArrow API reference <https://arrow.apache.org/docs/python/generated/        pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_.\\n\\n    Args:\\n        paths: A single file path or directory, or a list of file paths. Multiple\\n            directories are not supported.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the ``S3FileSystem`` is\\n            used. If ``None``, this function uses a system-chosen implementation.\\n        columns: A list of column names to read. Only the specified columns are\\n            read during the file scan.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the parquet files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\\n            mappings for converting a Parquet column containing serialized\\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\\n            assumes that the tensors are serialized in the raw\\n            NumPy array format in C-contiguous order (e.g., via\\n            `arr.tobytes()`).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases you do not need to set this parameter.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\\n            set of arguments, see the`PyArrow API <https://arrow.apache.org/docs/                python/generated/pyarrow.dataset.Scanner.html                    #pyarrow.dataset.Scanner.from_fragment>`_\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified parquet\\n        files.\\n    '\n    if meta_provider is None:\n        meta_provider = get_parquet_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    dataset_kwargs = arrow_parquet_args.pop('dataset_kwargs', None)\n    _block_udf = arrow_parquet_args.pop('_block_udf', None)\n    schema = arrow_parquet_args.pop('schema', None)\n    datasource = ParquetDatasource(paths, columns=columns, dataset_kwargs=dataset_kwargs, to_batch_kwargs=arrow_parquet_args, _block_udf=_block_udf, filesystem=filesystem, schema=schema, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "read_images",
        "original": "@PublicAPI(stability='beta')\ndef read_images(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, meta_provider: Optional[BaseFileMetadataProvider]=None, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, size: Optional[Tuple[int, int]]=None, mode: Optional[str]=None, include_paths: bool=False, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ImageDatasource._FILE_EXTENSIONS) -> Dataset:\n    \"\"\"Creates a :class:`~ray.data.Dataset` from image files.\n\n    Examples:\n        >>> import ray\n        >>> path = \"s3://anonymous@ray-example-data/batoidea/JPEGImages/\"\n        >>> ds = ray.data.read_images(path)\n        >>> ds.schema()\n        Column  Type\n        ------  ----\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\n\n        If you need image file paths, set ``include_paths=True``.\n\n        >>> ds = ray.data.read_images(path, include_paths=True)\n        >>> ds.schema()\n        Column  Type\n        ------  ----\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\n        path    string\n        >>> ds.take(1)[0][\"path\"]\n        'ray-example-data/batoidea/JPEGImages/1.jpeg'\n\n        If your images are arranged like:\n\n        .. code::\n\n            root/dog/xxx.png\n            root/dog/xxy.png\n\n            root/cat/123.png\n            root/cat/nsdf3.png\n\n        Then you can include the labels by specifying a\n        :class:`~ray.data.datasource.partitioning.Partitioning`.\n\n        >>> import ray\n        >>> from ray.data.datasource.partitioning import Partitioning\n        >>> root = \"s3://anonymous@ray-example-data/image-datasets/dir-partitioned\"\n        >>> partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=root)\n        >>> ds = ray.data.read_images(root, size=(224, 224), partitioning=partitioning)\n        >>> ds.schema()\n        Column  Type\n        ------  ----\n        image   numpy.ndarray(shape=(224, 224, 3), dtype=uint8)\n        class   string\n\n    Args:\n        paths: A single file or directory, or a list of file or directory paths.\n            A list of paths can contain both files and directories.\n        filesystem: The pyarrow filesystem\n            implementation to read from. These filesystems are specified in the\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\n            you need to provide specific configurations to the filesystem. By default,\n            the filesystem is automatically selected based on the scheme of the paths.\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see :ref:`Tuning read parallelism\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\n            records in all the CSV files.\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\n            metadata providers may be able to resolve file metadata more quickly and/or\n            accurately. In most cases, you do not need to set this. If ``None``, this\n            function uses a system-chosen implementation.\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\n        arrow_open_file_args: kwargs passed to\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\n            when opening input files to read.\n        partition_filter:  A\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\n            with a custom callback to read only selected partitions of a dataset.\n            By default, this filters out any file paths whose file extension does not\n            match ``*.png``, ``*.jpg``, ``*.jpeg``, ``*.tiff``, ``*.bmp``, or ``*.gif``.\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\n            that describes how paths are organized. Defaults to ``None``.\n        size: The desired height and width of loaded images. If unspecified, images\n            retain their original shape.\n        mode: A `Pillow mode <https://pillow.readthedocs.io/en/stable/handbook/concepts            .html#modes>`_\n            describing the desired type and depth of pixels. If unspecified, image\n            modes are inferred by\n            `Pillow <https://pillow.readthedocs.io/en/stable/index.html>`_.\n        include_paths: If ``True``, include the path to each image. File paths are\n            stored in the ``'path'`` column.\n        ignore_missing_paths: If True, ignores any file/directory paths in ``paths``\n            that are not found. Defaults to False.\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\n            Defaults to not shuffle with ``None``.\n        file_extensions: A list of file extensions to filter files by.\n\n    Returns:\n        A :class:`~ray.data.Dataset` producing tensors that represent the images at\n        the specified paths. For information on working with tensors, read the\n        :ref:`tensor data guide <working_with_tensors>`.\n\n    Raises:\n        ValueError: if ``size`` contains non-positive numbers.\n        ValueError: if ``mode`` is unsupported.\n    \"\"\"\n    if meta_provider is None:\n        meta_provider = get_image_metadata_provider()\n    datasource = ImageDatasource(paths, size=size, mode=mode, include_paths=include_paths, filesystem=filesystem, meta_provider=meta_provider, open_stream_args=arrow_open_file_args, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI(stability='beta')\ndef read_images(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, meta_provider: Optional[BaseFileMetadataProvider]=None, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, size: Optional[Tuple[int, int]]=None, mode: Optional[str]=None, include_paths: bool=False, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ImageDatasource._FILE_EXTENSIONS) -> Dataset:\n    if False:\n        i = 10\n    'Creates a :class:`~ray.data.Dataset` from image files.\\n\\n    Examples:\\n        >>> import ray\\n        >>> path = \"s3://anonymous@ray-example-data/batoidea/JPEGImages/\"\\n        >>> ds = ray.data.read_images(path)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\\n\\n        If you need image file paths, set ``include_paths=True``.\\n\\n        >>> ds = ray.data.read_images(path, include_paths=True)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\\n        path    string\\n        >>> ds.take(1)[0][\"path\"]\\n        \\'ray-example-data/batoidea/JPEGImages/1.jpeg\\'\\n\\n        If your images are arranged like:\\n\\n        .. code::\\n\\n            root/dog/xxx.png\\n            root/dog/xxy.png\\n\\n            root/cat/123.png\\n            root/cat/nsdf3.png\\n\\n        Then you can include the labels by specifying a\\n        :class:`~ray.data.datasource.partitioning.Partitioning`.\\n\\n        >>> import ray\\n        >>> from ray.data.datasource.partitioning import Partitioning\\n        >>> root = \"s3://anonymous@ray-example-data/image-datasets/dir-partitioned\"\\n        >>> partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=root)\\n        >>> ds = ray.data.read_images(root, size=(224, 224), partitioning=partitioning)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(224, 224, 3), dtype=uint8)\\n        class   string\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The pyarrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_file_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\\n            when opening input files to read.\\n        partition_filter:  A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match ``*.png``, ``*.jpg``, ``*.jpeg``, ``*.tiff``, ``*.bmp``, or ``*.gif``.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        size: The desired height and width of loaded images. If unspecified, images\\n            retain their original shape.\\n        mode: A `Pillow mode <https://pillow.readthedocs.io/en/stable/handbook/concepts            .html#modes>`_\\n            describing the desired type and depth of pixels. If unspecified, image\\n            modes are inferred by\\n            `Pillow <https://pillow.readthedocs.io/en/stable/index.html>`_.\\n        include_paths: If ``True``, include the path to each image. File paths are\\n            stored in the ``\\'path\\'`` column.\\n        ignore_missing_paths: If True, ignores any file/directory paths in ``paths``\\n            that are not found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing tensors that represent the images at\\n        the specified paths. For information on working with tensors, read the\\n        :ref:`tensor data guide <working_with_tensors>`.\\n\\n    Raises:\\n        ValueError: if ``size`` contains non-positive numbers.\\n        ValueError: if ``mode`` is unsupported.\\n    '\n    if meta_provider is None:\n        meta_provider = get_image_metadata_provider()\n    datasource = ImageDatasource(paths, size=size, mode=mode, include_paths=include_paths, filesystem=filesystem, meta_provider=meta_provider, open_stream_args=arrow_open_file_args, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='beta')\ndef read_images(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, meta_provider: Optional[BaseFileMetadataProvider]=None, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, size: Optional[Tuple[int, int]]=None, mode: Optional[str]=None, include_paths: bool=False, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ImageDatasource._FILE_EXTENSIONS) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a :class:`~ray.data.Dataset` from image files.\\n\\n    Examples:\\n        >>> import ray\\n        >>> path = \"s3://anonymous@ray-example-data/batoidea/JPEGImages/\"\\n        >>> ds = ray.data.read_images(path)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\\n\\n        If you need image file paths, set ``include_paths=True``.\\n\\n        >>> ds = ray.data.read_images(path, include_paths=True)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\\n        path    string\\n        >>> ds.take(1)[0][\"path\"]\\n        \\'ray-example-data/batoidea/JPEGImages/1.jpeg\\'\\n\\n        If your images are arranged like:\\n\\n        .. code::\\n\\n            root/dog/xxx.png\\n            root/dog/xxy.png\\n\\n            root/cat/123.png\\n            root/cat/nsdf3.png\\n\\n        Then you can include the labels by specifying a\\n        :class:`~ray.data.datasource.partitioning.Partitioning`.\\n\\n        >>> import ray\\n        >>> from ray.data.datasource.partitioning import Partitioning\\n        >>> root = \"s3://anonymous@ray-example-data/image-datasets/dir-partitioned\"\\n        >>> partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=root)\\n        >>> ds = ray.data.read_images(root, size=(224, 224), partitioning=partitioning)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(224, 224, 3), dtype=uint8)\\n        class   string\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The pyarrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_file_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\\n            when opening input files to read.\\n        partition_filter:  A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match ``*.png``, ``*.jpg``, ``*.jpeg``, ``*.tiff``, ``*.bmp``, or ``*.gif``.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        size: The desired height and width of loaded images. If unspecified, images\\n            retain their original shape.\\n        mode: A `Pillow mode <https://pillow.readthedocs.io/en/stable/handbook/concepts            .html#modes>`_\\n            describing the desired type and depth of pixels. If unspecified, image\\n            modes are inferred by\\n            `Pillow <https://pillow.readthedocs.io/en/stable/index.html>`_.\\n        include_paths: If ``True``, include the path to each image. File paths are\\n            stored in the ``\\'path\\'`` column.\\n        ignore_missing_paths: If True, ignores any file/directory paths in ``paths``\\n            that are not found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing tensors that represent the images at\\n        the specified paths. For information on working with tensors, read the\\n        :ref:`tensor data guide <working_with_tensors>`.\\n\\n    Raises:\\n        ValueError: if ``size`` contains non-positive numbers.\\n        ValueError: if ``mode`` is unsupported.\\n    '\n    if meta_provider is None:\n        meta_provider = get_image_metadata_provider()\n    datasource = ImageDatasource(paths, size=size, mode=mode, include_paths=include_paths, filesystem=filesystem, meta_provider=meta_provider, open_stream_args=arrow_open_file_args, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='beta')\ndef read_images(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, meta_provider: Optional[BaseFileMetadataProvider]=None, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, size: Optional[Tuple[int, int]]=None, mode: Optional[str]=None, include_paths: bool=False, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ImageDatasource._FILE_EXTENSIONS) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a :class:`~ray.data.Dataset` from image files.\\n\\n    Examples:\\n        >>> import ray\\n        >>> path = \"s3://anonymous@ray-example-data/batoidea/JPEGImages/\"\\n        >>> ds = ray.data.read_images(path)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\\n\\n        If you need image file paths, set ``include_paths=True``.\\n\\n        >>> ds = ray.data.read_images(path, include_paths=True)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\\n        path    string\\n        >>> ds.take(1)[0][\"path\"]\\n        \\'ray-example-data/batoidea/JPEGImages/1.jpeg\\'\\n\\n        If your images are arranged like:\\n\\n        .. code::\\n\\n            root/dog/xxx.png\\n            root/dog/xxy.png\\n\\n            root/cat/123.png\\n            root/cat/nsdf3.png\\n\\n        Then you can include the labels by specifying a\\n        :class:`~ray.data.datasource.partitioning.Partitioning`.\\n\\n        >>> import ray\\n        >>> from ray.data.datasource.partitioning import Partitioning\\n        >>> root = \"s3://anonymous@ray-example-data/image-datasets/dir-partitioned\"\\n        >>> partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=root)\\n        >>> ds = ray.data.read_images(root, size=(224, 224), partitioning=partitioning)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(224, 224, 3), dtype=uint8)\\n        class   string\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The pyarrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_file_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\\n            when opening input files to read.\\n        partition_filter:  A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match ``*.png``, ``*.jpg``, ``*.jpeg``, ``*.tiff``, ``*.bmp``, or ``*.gif``.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        size: The desired height and width of loaded images. If unspecified, images\\n            retain their original shape.\\n        mode: A `Pillow mode <https://pillow.readthedocs.io/en/stable/handbook/concepts            .html#modes>`_\\n            describing the desired type and depth of pixels. If unspecified, image\\n            modes are inferred by\\n            `Pillow <https://pillow.readthedocs.io/en/stable/index.html>`_.\\n        include_paths: If ``True``, include the path to each image. File paths are\\n            stored in the ``\\'path\\'`` column.\\n        ignore_missing_paths: If True, ignores any file/directory paths in ``paths``\\n            that are not found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing tensors that represent the images at\\n        the specified paths. For information on working with tensors, read the\\n        :ref:`tensor data guide <working_with_tensors>`.\\n\\n    Raises:\\n        ValueError: if ``size`` contains non-positive numbers.\\n        ValueError: if ``mode`` is unsupported.\\n    '\n    if meta_provider is None:\n        meta_provider = get_image_metadata_provider()\n    datasource = ImageDatasource(paths, size=size, mode=mode, include_paths=include_paths, filesystem=filesystem, meta_provider=meta_provider, open_stream_args=arrow_open_file_args, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='beta')\ndef read_images(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, meta_provider: Optional[BaseFileMetadataProvider]=None, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, size: Optional[Tuple[int, int]]=None, mode: Optional[str]=None, include_paths: bool=False, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ImageDatasource._FILE_EXTENSIONS) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a :class:`~ray.data.Dataset` from image files.\\n\\n    Examples:\\n        >>> import ray\\n        >>> path = \"s3://anonymous@ray-example-data/batoidea/JPEGImages/\"\\n        >>> ds = ray.data.read_images(path)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\\n\\n        If you need image file paths, set ``include_paths=True``.\\n\\n        >>> ds = ray.data.read_images(path, include_paths=True)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\\n        path    string\\n        >>> ds.take(1)[0][\"path\"]\\n        \\'ray-example-data/batoidea/JPEGImages/1.jpeg\\'\\n\\n        If your images are arranged like:\\n\\n        .. code::\\n\\n            root/dog/xxx.png\\n            root/dog/xxy.png\\n\\n            root/cat/123.png\\n            root/cat/nsdf3.png\\n\\n        Then you can include the labels by specifying a\\n        :class:`~ray.data.datasource.partitioning.Partitioning`.\\n\\n        >>> import ray\\n        >>> from ray.data.datasource.partitioning import Partitioning\\n        >>> root = \"s3://anonymous@ray-example-data/image-datasets/dir-partitioned\"\\n        >>> partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=root)\\n        >>> ds = ray.data.read_images(root, size=(224, 224), partitioning=partitioning)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(224, 224, 3), dtype=uint8)\\n        class   string\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The pyarrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_file_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\\n            when opening input files to read.\\n        partition_filter:  A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match ``*.png``, ``*.jpg``, ``*.jpeg``, ``*.tiff``, ``*.bmp``, or ``*.gif``.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        size: The desired height and width of loaded images. If unspecified, images\\n            retain their original shape.\\n        mode: A `Pillow mode <https://pillow.readthedocs.io/en/stable/handbook/concepts            .html#modes>`_\\n            describing the desired type and depth of pixels. If unspecified, image\\n            modes are inferred by\\n            `Pillow <https://pillow.readthedocs.io/en/stable/index.html>`_.\\n        include_paths: If ``True``, include the path to each image. File paths are\\n            stored in the ``\\'path\\'`` column.\\n        ignore_missing_paths: If True, ignores any file/directory paths in ``paths``\\n            that are not found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing tensors that represent the images at\\n        the specified paths. For information on working with tensors, read the\\n        :ref:`tensor data guide <working_with_tensors>`.\\n\\n    Raises:\\n        ValueError: if ``size`` contains non-positive numbers.\\n        ValueError: if ``mode`` is unsupported.\\n    '\n    if meta_provider is None:\n        meta_provider = get_image_metadata_provider()\n    datasource = ImageDatasource(paths, size=size, mode=mode, include_paths=include_paths, filesystem=filesystem, meta_provider=meta_provider, open_stream_args=arrow_open_file_args, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='beta')\ndef read_images(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, meta_provider: Optional[BaseFileMetadataProvider]=None, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, size: Optional[Tuple[int, int]]=None, mode: Optional[str]=None, include_paths: bool=False, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ImageDatasource._FILE_EXTENSIONS) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a :class:`~ray.data.Dataset` from image files.\\n\\n    Examples:\\n        >>> import ray\\n        >>> path = \"s3://anonymous@ray-example-data/batoidea/JPEGImages/\"\\n        >>> ds = ray.data.read_images(path)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\\n\\n        If you need image file paths, set ``include_paths=True``.\\n\\n        >>> ds = ray.data.read_images(path, include_paths=True)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\\n        path    string\\n        >>> ds.take(1)[0][\"path\"]\\n        \\'ray-example-data/batoidea/JPEGImages/1.jpeg\\'\\n\\n        If your images are arranged like:\\n\\n        .. code::\\n\\n            root/dog/xxx.png\\n            root/dog/xxy.png\\n\\n            root/cat/123.png\\n            root/cat/nsdf3.png\\n\\n        Then you can include the labels by specifying a\\n        :class:`~ray.data.datasource.partitioning.Partitioning`.\\n\\n        >>> import ray\\n        >>> from ray.data.datasource.partitioning import Partitioning\\n        >>> root = \"s3://anonymous@ray-example-data/image-datasets/dir-partitioned\"\\n        >>> partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=root)\\n        >>> ds = ray.data.read_images(root, size=(224, 224), partitioning=partitioning)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        image   numpy.ndarray(shape=(224, 224, 3), dtype=uint8)\\n        class   string\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The pyarrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_file_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\\n            when opening input files to read.\\n        partition_filter:  A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match ``*.png``, ``*.jpg``, ``*.jpeg``, ``*.tiff``, ``*.bmp``, or ``*.gif``.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        size: The desired height and width of loaded images. If unspecified, images\\n            retain their original shape.\\n        mode: A `Pillow mode <https://pillow.readthedocs.io/en/stable/handbook/concepts            .html#modes>`_\\n            describing the desired type and depth of pixels. If unspecified, image\\n            modes are inferred by\\n            `Pillow <https://pillow.readthedocs.io/en/stable/index.html>`_.\\n        include_paths: If ``True``, include the path to each image. File paths are\\n            stored in the ``\\'path\\'`` column.\\n        ignore_missing_paths: If True, ignores any file/directory paths in ``paths``\\n            that are not found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` producing tensors that represent the images at\\n        the specified paths. For information on working with tensors, read the\\n        :ref:`tensor data guide <working_with_tensors>`.\\n\\n    Raises:\\n        ValueError: if ``size`` contains non-positive numbers.\\n        ValueError: if ``mode`` is unsupported.\\n    '\n    if meta_provider is None:\n        meta_provider = get_image_metadata_provider()\n    datasource = ImageDatasource(paths, size=size, mode=mode, include_paths=include_paths, filesystem=filesystem, meta_provider=meta_provider, open_stream_args=arrow_open_file_args, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "read_parquet_bulk",
        "original": "@PublicAPI\ndef read_parquet_bulk(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ParquetBaseDatasource._FILE_EXTENSIONS, **arrow_parquet_args) -> Dataset:\n    \"\"\"Create :class:`~ray.data.Dataset` from parquet files without reading metadata.\n\n    Use :meth:`~ray.data.read_parquet` for most cases.\n\n    Use :meth:`~ray.data.read_parquet_bulk` if all the provided paths point to files\n    and metadata fetching using :meth:`~ray.data.read_parquet` takes too long or the\n    parquet files do not all have a unified schema.\n\n    Performance slowdowns are possible when using this method with parquet files that\n    are very large.\n\n    .. warning::\n\n        Only provide file paths as input (i.e., no directory paths). An\n        OSError is raised if one or more paths point to directories. If your\n        use-case requires directory paths, use :meth:`~ray.data.read_parquet`\n        instead.\n\n    Examples:\n        Read multiple local files. You should always provide only input file paths\n        (i.e. no directory paths) when known to minimize read latency.\n\n        >>> ray.data.read_parquet_bulk( # doctest: +SKIP\n        ...     [\"/path/to/file1\", \"/path/to/file2\"])\n\n    Args:\n        paths: A single file path or a list of file paths.\n        filesystem: The PyArrow filesystem\n            implementation to read from. These filesystems are\n            specified in the\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/                filesystems.html#filesystem-implementations>`_.\n            Specify this parameter if you need to provide specific configurations to\n            the filesystem. By default, the filesystem is automatically selected based\n            on the scheme of the paths. For example, if the path begins with ``s3://``,\n            the `S3FileSystem` is used.\n        columns: A list of column names to read. Only the\n            specified columns are read during the file scan.\n        parallelism: The amount of parallelism to use for\n            the dataset. Defaults to -1, which automatically determines the optimal\n            parallelism for your configuration. You should not need to manually set\n            this value in most cases. For details on how the parallelism is\n            automatically determined and guidance on how to tune it, see\n            :ref:`Tuning read parallelism <read_parallelism>`. Parallelism is\n            upper bounded by the total number of records in all the parquet files.\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\n        arrow_open_file_args: kwargs passed to\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\n            when opening input files to read.\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\n            mappings for converting a Parquet column containing serialized\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\n            assumes that the tensors are serialized in the raw\n            NumPy array format in C-contiguous order (e.g. via\n            `arr.tobytes()`).\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\n            metadata providers may be able to resolve file metadata more quickly and/or\n            accurately. In most cases, you do not need to set this. If ``None``, this\n            function uses a system-chosen implementation.\n        partition_filter: A\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\n            with a custom callback to read only selected partitions of a dataset.\n            By default, this filters out any file paths whose file extension does not\n            match \"*.parquet*\".\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\n            Defaults to not shuffle with ``None``.\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\n            set of arguments, see\n            the `PyArrow API <https://arrow.apache.org/docs/python/generated/                pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_\n        file_extensions: A list of file extensions to filter files by.\n\n    Returns:\n       :class:`~ray.data.Dataset` producing records read from the specified paths.\n    \"\"\"\n    if meta_provider is None:\n        meta_provider = get_parquet_bulk_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    datasource = ParquetBaseDatasource(paths, read_table_args=arrow_parquet_args, filesystem=filesystem, open_stream_args=arrow_open_file_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions, **arrow_parquet_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI\ndef read_parquet_bulk(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ParquetBaseDatasource._FILE_EXTENSIONS, **arrow_parquet_args) -> Dataset:\n    if False:\n        i = 10\n    'Create :class:`~ray.data.Dataset` from parquet files without reading metadata.\\n\\n    Use :meth:`~ray.data.read_parquet` for most cases.\\n\\n    Use :meth:`~ray.data.read_parquet_bulk` if all the provided paths point to files\\n    and metadata fetching using :meth:`~ray.data.read_parquet` takes too long or the\\n    parquet files do not all have a unified schema.\\n\\n    Performance slowdowns are possible when using this method with parquet files that\\n    are very large.\\n\\n    .. warning::\\n\\n        Only provide file paths as input (i.e., no directory paths). An\\n        OSError is raised if one or more paths point to directories. If your\\n        use-case requires directory paths, use :meth:`~ray.data.read_parquet`\\n        instead.\\n\\n    Examples:\\n        Read multiple local files. You should always provide only input file paths\\n        (i.e. no directory paths) when known to minimize read latency.\\n\\n        >>> ray.data.read_parquet_bulk( # doctest: +SKIP\\n        ...     [\"/path/to/file1\", \"/path/to/file2\"])\\n\\n    Args:\\n        paths: A single file path or a list of file paths.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are\\n            specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/                filesystems.html#filesystem-implementations>`_.\\n            Specify this parameter if you need to provide specific configurations to\\n            the filesystem. By default, the filesystem is automatically selected based\\n            on the scheme of the paths. For example, if the path begins with ``s3://``,\\n            the `S3FileSystem` is used.\\n        columns: A list of column names to read. Only the\\n            specified columns are read during the file scan.\\n        parallelism: The amount of parallelism to use for\\n            the dataset. Defaults to -1, which automatically determines the optimal\\n            parallelism for your configuration. You should not need to manually set\\n            this value in most cases. For details on how the parallelism is\\n            automatically determined and guidance on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`. Parallelism is\\n            upper bounded by the total number of records in all the parquet files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_file_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\\n            when opening input files to read.\\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\\n            mappings for converting a Parquet column containing serialized\\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\\n            assumes that the tensors are serialized in the raw\\n            NumPy array format in C-contiguous order (e.g. via\\n            `arr.tobytes()`).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.parquet*\".\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\\n            set of arguments, see\\n            the `PyArrow API <https://arrow.apache.org/docs/python/generated/                pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n       :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_parquet_bulk_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    datasource = ParquetBaseDatasource(paths, read_table_args=arrow_parquet_args, filesystem=filesystem, open_stream_args=arrow_open_file_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions, **arrow_parquet_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_parquet_bulk(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ParquetBaseDatasource._FILE_EXTENSIONS, **arrow_parquet_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create :class:`~ray.data.Dataset` from parquet files without reading metadata.\\n\\n    Use :meth:`~ray.data.read_parquet` for most cases.\\n\\n    Use :meth:`~ray.data.read_parquet_bulk` if all the provided paths point to files\\n    and metadata fetching using :meth:`~ray.data.read_parquet` takes too long or the\\n    parquet files do not all have a unified schema.\\n\\n    Performance slowdowns are possible when using this method with parquet files that\\n    are very large.\\n\\n    .. warning::\\n\\n        Only provide file paths as input (i.e., no directory paths). An\\n        OSError is raised if one or more paths point to directories. If your\\n        use-case requires directory paths, use :meth:`~ray.data.read_parquet`\\n        instead.\\n\\n    Examples:\\n        Read multiple local files. You should always provide only input file paths\\n        (i.e. no directory paths) when known to minimize read latency.\\n\\n        >>> ray.data.read_parquet_bulk( # doctest: +SKIP\\n        ...     [\"/path/to/file1\", \"/path/to/file2\"])\\n\\n    Args:\\n        paths: A single file path or a list of file paths.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are\\n            specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/                filesystems.html#filesystem-implementations>`_.\\n            Specify this parameter if you need to provide specific configurations to\\n            the filesystem. By default, the filesystem is automatically selected based\\n            on the scheme of the paths. For example, if the path begins with ``s3://``,\\n            the `S3FileSystem` is used.\\n        columns: A list of column names to read. Only the\\n            specified columns are read during the file scan.\\n        parallelism: The amount of parallelism to use for\\n            the dataset. Defaults to -1, which automatically determines the optimal\\n            parallelism for your configuration. You should not need to manually set\\n            this value in most cases. For details on how the parallelism is\\n            automatically determined and guidance on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`. Parallelism is\\n            upper bounded by the total number of records in all the parquet files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_file_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\\n            when opening input files to read.\\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\\n            mappings for converting a Parquet column containing serialized\\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\\n            assumes that the tensors are serialized in the raw\\n            NumPy array format in C-contiguous order (e.g. via\\n            `arr.tobytes()`).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.parquet*\".\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\\n            set of arguments, see\\n            the `PyArrow API <https://arrow.apache.org/docs/python/generated/                pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n       :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_parquet_bulk_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    datasource = ParquetBaseDatasource(paths, read_table_args=arrow_parquet_args, filesystem=filesystem, open_stream_args=arrow_open_file_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions, **arrow_parquet_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_parquet_bulk(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ParquetBaseDatasource._FILE_EXTENSIONS, **arrow_parquet_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create :class:`~ray.data.Dataset` from parquet files without reading metadata.\\n\\n    Use :meth:`~ray.data.read_parquet` for most cases.\\n\\n    Use :meth:`~ray.data.read_parquet_bulk` if all the provided paths point to files\\n    and metadata fetching using :meth:`~ray.data.read_parquet` takes too long or the\\n    parquet files do not all have a unified schema.\\n\\n    Performance slowdowns are possible when using this method with parquet files that\\n    are very large.\\n\\n    .. warning::\\n\\n        Only provide file paths as input (i.e., no directory paths). An\\n        OSError is raised if one or more paths point to directories. If your\\n        use-case requires directory paths, use :meth:`~ray.data.read_parquet`\\n        instead.\\n\\n    Examples:\\n        Read multiple local files. You should always provide only input file paths\\n        (i.e. no directory paths) when known to minimize read latency.\\n\\n        >>> ray.data.read_parquet_bulk( # doctest: +SKIP\\n        ...     [\"/path/to/file1\", \"/path/to/file2\"])\\n\\n    Args:\\n        paths: A single file path or a list of file paths.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are\\n            specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/                filesystems.html#filesystem-implementations>`_.\\n            Specify this parameter if you need to provide specific configurations to\\n            the filesystem. By default, the filesystem is automatically selected based\\n            on the scheme of the paths. For example, if the path begins with ``s3://``,\\n            the `S3FileSystem` is used.\\n        columns: A list of column names to read. Only the\\n            specified columns are read during the file scan.\\n        parallelism: The amount of parallelism to use for\\n            the dataset. Defaults to -1, which automatically determines the optimal\\n            parallelism for your configuration. You should not need to manually set\\n            this value in most cases. For details on how the parallelism is\\n            automatically determined and guidance on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`. Parallelism is\\n            upper bounded by the total number of records in all the parquet files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_file_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\\n            when opening input files to read.\\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\\n            mappings for converting a Parquet column containing serialized\\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\\n            assumes that the tensors are serialized in the raw\\n            NumPy array format in C-contiguous order (e.g. via\\n            `arr.tobytes()`).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.parquet*\".\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\\n            set of arguments, see\\n            the `PyArrow API <https://arrow.apache.org/docs/python/generated/                pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n       :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_parquet_bulk_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    datasource = ParquetBaseDatasource(paths, read_table_args=arrow_parquet_args, filesystem=filesystem, open_stream_args=arrow_open_file_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions, **arrow_parquet_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_parquet_bulk(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ParquetBaseDatasource._FILE_EXTENSIONS, **arrow_parquet_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create :class:`~ray.data.Dataset` from parquet files without reading metadata.\\n\\n    Use :meth:`~ray.data.read_parquet` for most cases.\\n\\n    Use :meth:`~ray.data.read_parquet_bulk` if all the provided paths point to files\\n    and metadata fetching using :meth:`~ray.data.read_parquet` takes too long or the\\n    parquet files do not all have a unified schema.\\n\\n    Performance slowdowns are possible when using this method with parquet files that\\n    are very large.\\n\\n    .. warning::\\n\\n        Only provide file paths as input (i.e., no directory paths). An\\n        OSError is raised if one or more paths point to directories. If your\\n        use-case requires directory paths, use :meth:`~ray.data.read_parquet`\\n        instead.\\n\\n    Examples:\\n        Read multiple local files. You should always provide only input file paths\\n        (i.e. no directory paths) when known to minimize read latency.\\n\\n        >>> ray.data.read_parquet_bulk( # doctest: +SKIP\\n        ...     [\"/path/to/file1\", \"/path/to/file2\"])\\n\\n    Args:\\n        paths: A single file path or a list of file paths.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are\\n            specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/                filesystems.html#filesystem-implementations>`_.\\n            Specify this parameter if you need to provide specific configurations to\\n            the filesystem. By default, the filesystem is automatically selected based\\n            on the scheme of the paths. For example, if the path begins with ``s3://``,\\n            the `S3FileSystem` is used.\\n        columns: A list of column names to read. Only the\\n            specified columns are read during the file scan.\\n        parallelism: The amount of parallelism to use for\\n            the dataset. Defaults to -1, which automatically determines the optimal\\n            parallelism for your configuration. You should not need to manually set\\n            this value in most cases. For details on how the parallelism is\\n            automatically determined and guidance on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`. Parallelism is\\n            upper bounded by the total number of records in all the parquet files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_file_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\\n            when opening input files to read.\\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\\n            mappings for converting a Parquet column containing serialized\\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\\n            assumes that the tensors are serialized in the raw\\n            NumPy array format in C-contiguous order (e.g. via\\n            `arr.tobytes()`).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.parquet*\".\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\\n            set of arguments, see\\n            the `PyArrow API <https://arrow.apache.org/docs/python/generated/                pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n       :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_parquet_bulk_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    datasource = ParquetBaseDatasource(paths, read_table_args=arrow_parquet_args, filesystem=filesystem, open_stream_args=arrow_open_file_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions, **arrow_parquet_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_parquet_bulk(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, columns: Optional[List[str]]=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_file_args: Optional[Dict[str, Any]]=None, tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=ParquetBaseDatasource._FILE_EXTENSIONS, **arrow_parquet_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create :class:`~ray.data.Dataset` from parquet files without reading metadata.\\n\\n    Use :meth:`~ray.data.read_parquet` for most cases.\\n\\n    Use :meth:`~ray.data.read_parquet_bulk` if all the provided paths point to files\\n    and metadata fetching using :meth:`~ray.data.read_parquet` takes too long or the\\n    parquet files do not all have a unified schema.\\n\\n    Performance slowdowns are possible when using this method with parquet files that\\n    are very large.\\n\\n    .. warning::\\n\\n        Only provide file paths as input (i.e., no directory paths). An\\n        OSError is raised if one or more paths point to directories. If your\\n        use-case requires directory paths, use :meth:`~ray.data.read_parquet`\\n        instead.\\n\\n    Examples:\\n        Read multiple local files. You should always provide only input file paths\\n        (i.e. no directory paths) when known to minimize read latency.\\n\\n        >>> ray.data.read_parquet_bulk( # doctest: +SKIP\\n        ...     [\"/path/to/file1\", \"/path/to/file2\"])\\n\\n    Args:\\n        paths: A single file path or a list of file paths.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are\\n            specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/                filesystems.html#filesystem-implementations>`_.\\n            Specify this parameter if you need to provide specific configurations to\\n            the filesystem. By default, the filesystem is automatically selected based\\n            on the scheme of the paths. For example, if the path begins with ``s3://``,\\n            the `S3FileSystem` is used.\\n        columns: A list of column names to read. Only the\\n            specified columns are read during the file scan.\\n        parallelism: The amount of parallelism to use for\\n            the dataset. Defaults to -1, which automatically determines the optimal\\n            parallelism for your configuration. You should not need to manually set\\n            this value in most cases. For details on how the parallelism is\\n            automatically determined and guidance on how to tune it, see\\n            :ref:`Tuning read parallelism <read_parallelism>`. Parallelism is\\n            upper bounded by the total number of records in all the parquet files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_file_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_file>`_.\\n            when opening input files to read.\\n        tensor_column_schema: A dict of column name to PyArrow dtype and shape\\n            mappings for converting a Parquet column containing serialized\\n            tensors (ndarrays) as their elements to PyArrow tensors. This function\\n            assumes that the tensors are serialized in the raw\\n            NumPy array format in C-contiguous order (e.g. via\\n            `arr.tobytes()`).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.parquet*\".\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_parquet_args: Other parquet read options to pass to PyArrow. For the full\\n            set of arguments, see\\n            the `PyArrow API <https://arrow.apache.org/docs/python/generated/                pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n       :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_parquet_bulk_metadata_provider()\n    arrow_parquet_args = _resolve_parquet_args(tensor_column_schema, **arrow_parquet_args)\n    datasource = ParquetBaseDatasource(paths, read_table_args=arrow_parquet_args, filesystem=filesystem, open_stream_args=arrow_open_file_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions, **arrow_parquet_args)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "read_json",
        "original": "@PublicAPI\ndef read_json(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=JSONDatasource._FILE_EXTENSIONS, **arrow_json_args) -> Dataset:\n    \"\"\"Creates a :class:`~ray.data.Dataset` from JSON and JSONL files.\n\n    For JSON file, the whole file is read as one row.\n    For JSONL file, each line of file is read as separate row.\n\n    Examples:\n        Read a JSON file in remote storage.\n\n        >>> import ray\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/log.json\")\n        >>> ds.schema()\n        Column     Type\n        ------     ----\n        timestamp  timestamp[s]\n        size       int64\n\n        Read a JSONL file in remote storage.\n\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/train.jsonl\")\n        >>> ds.schema()\n        Column  Type\n        ------  ----\n        input   string\n\n        Read multiple local files.\n\n        >>> ray.data.read_json( # doctest: +SKIP\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\n\n        Read multiple directories.\n\n        >>> ray.data.read_json( # doctest: +SKIP\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\n\n        By default, :meth:`~ray.data.read_json` parses\n        `Hive-style partitions <https://athena.guide/articles/        hive-style-partitioning/>`_\n        from file paths. If your data adheres to a different partitioning scheme, set\n        the ``partitioning`` parameter.\n\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.json\")\n        >>> ds.take(1)\n        [{'order_number': 10107, 'quantity': 30, 'year': '2022', 'month': '09'}]\n\n        When reading large files, the default block size configured in PyArrow can be too small,\n        resulting in the following error:\n        ``pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries\n        (try to increase block size?)``.\n\n        To resolve this, use the ``read_options`` parameter to set a larger block size:\n\n        >>> import pyarrow.json as pajson\n        >>> block_size = 10 << 20 # Set block size to 10MB\n        >>> ray.data.read_json(\n        ...     \"s3://anonymous@ray-example-data/log.json\",\n        ...     read_options=pajson.ReadOptions(block_size=block_size)\n        ... )\n        Dataset(num_blocks=8, num_rows=1, schema={timestamp: timestamp[s], size: int64})\n\n    Args:\n        paths: A single file or directory, or a list of file or directory paths.\n            A list of paths can contain both files and directories.\n        filesystem: The PyArrow filesystem\n            implementation to read from. These filesystems are specified in the\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\n            you need to provide specific configurations to the filesystem. By default,\n            the filesystem is automatically selected based on the scheme of the paths.\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see :ref:`Tuning read parallelism\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\n            records in all the JSON files.\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\n        arrow_open_stream_args: kwargs passed to\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\n            when opening input files to read.\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\n            metadata providers may be able to resolve file metadata more quickly and/or\n            accurately. In most cases, you do not need to set this. If ``None``, this\n            function uses a system-chosen implementation.\n        partition_filter: A\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\n            Use with a custom callback to read only selected partitions of a\n            dataset.\n            By default, this filters out any file paths whose file extension does not\n            match \"*.json\" or \"*.jsonl\".\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\n            that describes how paths are organized. By default, this function parses\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\n            found. Defaults to False.\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\n            Defaults to not shuffle with ``None``.\n        arrow_json_args: JSON read options to pass to `pyarrow.json.read_json <https://            arrow.apache.org/docs/python/generated/pyarrow.json.read_json.html#pyarrow.            json.read_json>`_.\n        file_extensions: A list of file extensions to filter files by.\n\n    Returns:\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\n    \"\"\"\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(JSONDatasource._FILE_EXTENSIONS)\n    datasource = JSONDatasource(paths, arrow_json_args=arrow_json_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI\ndef read_json(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=JSONDatasource._FILE_EXTENSIONS, **arrow_json_args) -> Dataset:\n    if False:\n        i = 10\n    'Creates a :class:`~ray.data.Dataset` from JSON and JSONL files.\\n\\n    For JSON file, the whole file is read as one row.\\n    For JSONL file, each line of file is read as separate row.\\n\\n    Examples:\\n        Read a JSON file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/log.json\")\\n        >>> ds.schema()\\n        Column     Type\\n        ------     ----\\n        timestamp  timestamp[s]\\n        size       int64\\n\\n        Read a JSONL file in remote storage.\\n\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/train.jsonl\")\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        input   string\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_json( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read multiple directories.\\n\\n        >>> ray.data.read_json( # doctest: +SKIP\\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\\n\\n        By default, :meth:`~ray.data.read_json` parses\\n        `Hive-style partitions <https://athena.guide/articles/        hive-style-partitioning/>`_\\n        from file paths. If your data adheres to a different partitioning scheme, set\\n        the ``partitioning`` parameter.\\n\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.json\")\\n        >>> ds.take(1)\\n        [{\\'order_number\\': 10107, \\'quantity\\': 30, \\'year\\': \\'2022\\', \\'month\\': \\'09\\'}]\\n\\n        When reading large files, the default block size configured in PyArrow can be too small,\\n        resulting in the following error:\\n        ``pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries\\n        (try to increase block size?)``.\\n\\n        To resolve this, use the ``read_options`` parameter to set a larger block size:\\n\\n        >>> import pyarrow.json as pajson\\n        >>> block_size = 10 << 20 # Set block size to 10MB\\n        >>> ray.data.read_json(\\n        ...     \"s3://anonymous@ray-example-data/log.json\",\\n        ...     read_options=pajson.ReadOptions(block_size=block_size)\\n        ... )\\n        Dataset(num_blocks=8, num_rows=1, schema={timestamp: timestamp[s], size: int64})\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the JSON files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.json\" or \"*.jsonl\".\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. By default, this function parses\\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_json_args: JSON read options to pass to `pyarrow.json.read_json <https://            arrow.apache.org/docs/python/generated/pyarrow.json.read_json.html#pyarrow.            json.read_json>`_.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(JSONDatasource._FILE_EXTENSIONS)\n    datasource = JSONDatasource(paths, arrow_json_args=arrow_json_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_json(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=JSONDatasource._FILE_EXTENSIONS, **arrow_json_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a :class:`~ray.data.Dataset` from JSON and JSONL files.\\n\\n    For JSON file, the whole file is read as one row.\\n    For JSONL file, each line of file is read as separate row.\\n\\n    Examples:\\n        Read a JSON file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/log.json\")\\n        >>> ds.schema()\\n        Column     Type\\n        ------     ----\\n        timestamp  timestamp[s]\\n        size       int64\\n\\n        Read a JSONL file in remote storage.\\n\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/train.jsonl\")\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        input   string\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_json( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read multiple directories.\\n\\n        >>> ray.data.read_json( # doctest: +SKIP\\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\\n\\n        By default, :meth:`~ray.data.read_json` parses\\n        `Hive-style partitions <https://athena.guide/articles/        hive-style-partitioning/>`_\\n        from file paths. If your data adheres to a different partitioning scheme, set\\n        the ``partitioning`` parameter.\\n\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.json\")\\n        >>> ds.take(1)\\n        [{\\'order_number\\': 10107, \\'quantity\\': 30, \\'year\\': \\'2022\\', \\'month\\': \\'09\\'}]\\n\\n        When reading large files, the default block size configured in PyArrow can be too small,\\n        resulting in the following error:\\n        ``pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries\\n        (try to increase block size?)``.\\n\\n        To resolve this, use the ``read_options`` parameter to set a larger block size:\\n\\n        >>> import pyarrow.json as pajson\\n        >>> block_size = 10 << 20 # Set block size to 10MB\\n        >>> ray.data.read_json(\\n        ...     \"s3://anonymous@ray-example-data/log.json\",\\n        ...     read_options=pajson.ReadOptions(block_size=block_size)\\n        ... )\\n        Dataset(num_blocks=8, num_rows=1, schema={timestamp: timestamp[s], size: int64})\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the JSON files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.json\" or \"*.jsonl\".\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. By default, this function parses\\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_json_args: JSON read options to pass to `pyarrow.json.read_json <https://            arrow.apache.org/docs/python/generated/pyarrow.json.read_json.html#pyarrow.            json.read_json>`_.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(JSONDatasource._FILE_EXTENSIONS)\n    datasource = JSONDatasource(paths, arrow_json_args=arrow_json_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_json(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=JSONDatasource._FILE_EXTENSIONS, **arrow_json_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a :class:`~ray.data.Dataset` from JSON and JSONL files.\\n\\n    For JSON file, the whole file is read as one row.\\n    For JSONL file, each line of file is read as separate row.\\n\\n    Examples:\\n        Read a JSON file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/log.json\")\\n        >>> ds.schema()\\n        Column     Type\\n        ------     ----\\n        timestamp  timestamp[s]\\n        size       int64\\n\\n        Read a JSONL file in remote storage.\\n\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/train.jsonl\")\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        input   string\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_json( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read multiple directories.\\n\\n        >>> ray.data.read_json( # doctest: +SKIP\\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\\n\\n        By default, :meth:`~ray.data.read_json` parses\\n        `Hive-style partitions <https://athena.guide/articles/        hive-style-partitioning/>`_\\n        from file paths. If your data adheres to a different partitioning scheme, set\\n        the ``partitioning`` parameter.\\n\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.json\")\\n        >>> ds.take(1)\\n        [{\\'order_number\\': 10107, \\'quantity\\': 30, \\'year\\': \\'2022\\', \\'month\\': \\'09\\'}]\\n\\n        When reading large files, the default block size configured in PyArrow can be too small,\\n        resulting in the following error:\\n        ``pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries\\n        (try to increase block size?)``.\\n\\n        To resolve this, use the ``read_options`` parameter to set a larger block size:\\n\\n        >>> import pyarrow.json as pajson\\n        >>> block_size = 10 << 20 # Set block size to 10MB\\n        >>> ray.data.read_json(\\n        ...     \"s3://anonymous@ray-example-data/log.json\",\\n        ...     read_options=pajson.ReadOptions(block_size=block_size)\\n        ... )\\n        Dataset(num_blocks=8, num_rows=1, schema={timestamp: timestamp[s], size: int64})\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the JSON files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.json\" or \"*.jsonl\".\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. By default, this function parses\\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_json_args: JSON read options to pass to `pyarrow.json.read_json <https://            arrow.apache.org/docs/python/generated/pyarrow.json.read_json.html#pyarrow.            json.read_json>`_.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(JSONDatasource._FILE_EXTENSIONS)\n    datasource = JSONDatasource(paths, arrow_json_args=arrow_json_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_json(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=JSONDatasource._FILE_EXTENSIONS, **arrow_json_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a :class:`~ray.data.Dataset` from JSON and JSONL files.\\n\\n    For JSON file, the whole file is read as one row.\\n    For JSONL file, each line of file is read as separate row.\\n\\n    Examples:\\n        Read a JSON file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/log.json\")\\n        >>> ds.schema()\\n        Column     Type\\n        ------     ----\\n        timestamp  timestamp[s]\\n        size       int64\\n\\n        Read a JSONL file in remote storage.\\n\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/train.jsonl\")\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        input   string\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_json( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read multiple directories.\\n\\n        >>> ray.data.read_json( # doctest: +SKIP\\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\\n\\n        By default, :meth:`~ray.data.read_json` parses\\n        `Hive-style partitions <https://athena.guide/articles/        hive-style-partitioning/>`_\\n        from file paths. If your data adheres to a different partitioning scheme, set\\n        the ``partitioning`` parameter.\\n\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.json\")\\n        >>> ds.take(1)\\n        [{\\'order_number\\': 10107, \\'quantity\\': 30, \\'year\\': \\'2022\\', \\'month\\': \\'09\\'}]\\n\\n        When reading large files, the default block size configured in PyArrow can be too small,\\n        resulting in the following error:\\n        ``pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries\\n        (try to increase block size?)``.\\n\\n        To resolve this, use the ``read_options`` parameter to set a larger block size:\\n\\n        >>> import pyarrow.json as pajson\\n        >>> block_size = 10 << 20 # Set block size to 10MB\\n        >>> ray.data.read_json(\\n        ...     \"s3://anonymous@ray-example-data/log.json\",\\n        ...     read_options=pajson.ReadOptions(block_size=block_size)\\n        ... )\\n        Dataset(num_blocks=8, num_rows=1, schema={timestamp: timestamp[s], size: int64})\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the JSON files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.json\" or \"*.jsonl\".\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. By default, this function parses\\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_json_args: JSON read options to pass to `pyarrow.json.read_json <https://            arrow.apache.org/docs/python/generated/pyarrow.json.read_json.html#pyarrow.            json.read_json>`_.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(JSONDatasource._FILE_EXTENSIONS)\n    datasource = JSONDatasource(paths, arrow_json_args=arrow_json_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_json(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=JSONDatasource._FILE_EXTENSIONS, **arrow_json_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a :class:`~ray.data.Dataset` from JSON and JSONL files.\\n\\n    For JSON file, the whole file is read as one row.\\n    For JSONL file, each line of file is read as separate row.\\n\\n    Examples:\\n        Read a JSON file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/log.json\")\\n        >>> ds.schema()\\n        Column     Type\\n        ------     ----\\n        timestamp  timestamp[s]\\n        size       int64\\n\\n        Read a JSONL file in remote storage.\\n\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/train.jsonl\")\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        input   string\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_json( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read multiple directories.\\n\\n        >>> ray.data.read_json( # doctest: +SKIP\\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\\n\\n        By default, :meth:`~ray.data.read_json` parses\\n        `Hive-style partitions <https://athena.guide/articles/        hive-style-partitioning/>`_\\n        from file paths. If your data adheres to a different partitioning scheme, set\\n        the ``partitioning`` parameter.\\n\\n        >>> ds = ray.data.read_json(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.json\")\\n        >>> ds.take(1)\\n        [{\\'order_number\\': 10107, \\'quantity\\': 30, \\'year\\': \\'2022\\', \\'month\\': \\'09\\'}]\\n\\n        When reading large files, the default block size configured in PyArrow can be too small,\\n        resulting in the following error:\\n        ``pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries\\n        (try to increase block size?)``.\\n\\n        To resolve this, use the ``read_options`` parameter to set a larger block size:\\n\\n        >>> import pyarrow.json as pajson\\n        >>> block_size = 10 << 20 # Set block size to 10MB\\n        >>> ray.data.read_json(\\n        ...     \"s3://anonymous@ray-example-data/log.json\",\\n        ...     read_options=pajson.ReadOptions(block_size=block_size)\\n        ... )\\n        Dataset(num_blocks=8, num_rows=1, schema={timestamp: timestamp[s], size: int64})\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the JSON files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.json\" or \"*.jsonl\".\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. By default, this function parses\\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_json_args: JSON read options to pass to `pyarrow.json.read_json <https://            arrow.apache.org/docs/python/generated/pyarrow.json.read_json.html#pyarrow.            json.read_json>`_.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(JSONDatasource._FILE_EXTENSIONS)\n    datasource = JSONDatasource(paths, arrow_json_args=arrow_json_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "read_csv",
        "original": "@PublicAPI\ndef read_csv(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_csv_args) -> Dataset:\n    \"\"\"Creates a :class:`~ray.data.Dataset` from CSV files.\n\n    Examples:\n        Read a file in remote storage.\n\n        >>> import ray\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n        >>> ds.schema()\n        Column             Type\n        ------             ----\n        sepal length (cm)  double\n        sepal width (cm)   double\n        petal length (cm)  double\n        petal width (cm)   double\n        target             int64\n\n        Read multiple local files.\n\n        >>> ray.data.read_csv( # doctest: +SKIP\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\n\n        Read a directory from remote storage.\n\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris-csv/\")\n\n        Read files that use a different delimiter. For more uses of ParseOptions see\n        https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html  # noqa: #501\n\n        >>> from pyarrow import csv\n        >>> parse_options = csv.ParseOptions(delimiter=\"\\\\t\")\n        >>> ds = ray.data.read_csv(\n        ...     \"s3://anonymous@ray-example-data/iris.tsv\",\n        ...     parse_options=parse_options)\n        >>> ds.schema()\n        Column        Type\n        ------        ----\n        sepal.length  double\n        sepal.width   double\n        petal.length  double\n        petal.width   double\n        variety       string\n\n        Convert a date column with a custom format from a CSV file. For more uses of ConvertOptions see https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html  # noqa: #501\n\n        >>> from pyarrow import csv\n        >>> convert_options = csv.ConvertOptions(\n        ...     timestamp_parsers=[\"%m/%d/%Y\"])\n        >>> ds = ray.data.read_csv(\n        ...     \"s3://anonymous@ray-example-data/dow_jones.csv\",\n        ...     convert_options=convert_options)\n\n        By default, :meth:`~ray.data.read_csv` parses\n        `Hive-style partitions <https://athena.guide/        articles/hive-style-partitioning/>`_\n        from file paths. If your data adheres to a different partitioning scheme, set\n        the ``partitioning`` parameter.\n\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.csv\")\n        >>> ds.take(1)\n        [{'order_number': 10107, 'quantity': 30, 'year': '2022', 'month': '09'}]\n\n        By default, :meth:`~ray.data.read_csv` reads all files from file paths. If you want to filter\n        files by file extensions, set the ``partition_filter`` parameter.\n\n        Read only ``*.csv`` files from a directory.\n\n        >>> ray.data.read_csv(\"s3://anonymous@ray-example-data/different-extensions/\",\n        ...     file_extensions=[\"csv\"])\n        Dataset(num_blocks=..., num_rows=1, schema={a: int64, b: int64})\n\n    Args:\n        paths: A single file or directory, or a list of file or directory paths.\n            A list of paths can contain both files and directories.\n        filesystem: The PyArrow filesystem\n            implementation to read from. These filesystems are specified in the\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\n            you need to provide specific configurations to the filesystem. By default,\n            the filesystem is automatically selected based on the scheme of the paths.\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see :ref:`Tuning read parallelism\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\n            records in all the CSV files.\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\n        arrow_open_stream_args: kwargs passed to\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\n            when opening input files to read.\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\n            metadata providers may be able to resolve file metadata more quickly and/or\n            accurately. In most cases, you do not need to set this. If ``None``, this\n            function uses a system-chosen implementation.\n        partition_filter: A\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\n            Use with a custom callback to read only selected partitions of a\n            dataset. By default, no files are filtered.\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\n            that describes how paths are organized. By default, this function parses\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\n            found. Defaults to False.\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\n            Defaults to not shuffle with ``None``.\n        arrow_csv_args: CSV read options to pass to\n            `pyarrow.csv.open_csv <https://arrow.apache.org/docs/python/generated/            pyarrow.csv.open_csv.html#pyarrow.csv.open_csv>`_\n            when opening CSV files.\n        file_extensions: A list of file extensions to filter files by.\n\n    Returns:\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\n    \"\"\"\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(CSVDatasource._FILE_EXTENSIONS)\n    datasource = CSVDatasource(paths, arrow_csv_args=arrow_csv_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI\ndef read_csv(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_csv_args) -> Dataset:\n    if False:\n        i = 10\n    'Creates a :class:`~ray.data.Dataset` from CSV files.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\\n        >>> ds.schema()\\n        Column             Type\\n        ------             ----\\n        sepal length (cm)  double\\n        sepal width (cm)   double\\n        petal length (cm)  double\\n        petal width (cm)   double\\n        target             int64\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_csv( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read a directory from remote storage.\\n\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris-csv/\")\\n\\n        Read files that use a different delimiter. For more uses of ParseOptions see\\n        https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html  # noqa: #501\\n\\n        >>> from pyarrow import csv\\n        >>> parse_options = csv.ParseOptions(delimiter=\"\\\\t\")\\n        >>> ds = ray.data.read_csv(\\n        ...     \"s3://anonymous@ray-example-data/iris.tsv\",\\n        ...     parse_options=parse_options)\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  double\\n        sepal.width   double\\n        petal.length  double\\n        petal.width   double\\n        variety       string\\n\\n        Convert a date column with a custom format from a CSV file. For more uses of ConvertOptions see https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html  # noqa: #501\\n\\n        >>> from pyarrow import csv\\n        >>> convert_options = csv.ConvertOptions(\\n        ...     timestamp_parsers=[\"%m/%d/%Y\"])\\n        >>> ds = ray.data.read_csv(\\n        ...     \"s3://anonymous@ray-example-data/dow_jones.csv\",\\n        ...     convert_options=convert_options)\\n\\n        By default, :meth:`~ray.data.read_csv` parses\\n        `Hive-style partitions <https://athena.guide/        articles/hive-style-partitioning/>`_\\n        from file paths. If your data adheres to a different partitioning scheme, set\\n        the ``partitioning`` parameter.\\n\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.csv\")\\n        >>> ds.take(1)\\n        [{\\'order_number\\': 10107, \\'quantity\\': 30, \\'year\\': \\'2022\\', \\'month\\': \\'09\\'}]\\n\\n        By default, :meth:`~ray.data.read_csv` reads all files from file paths. If you want to filter\\n        files by file extensions, set the ``partition_filter`` parameter.\\n\\n        Read only ``*.csv`` files from a directory.\\n\\n        >>> ray.data.read_csv(\"s3://anonymous@ray-example-data/different-extensions/\",\\n        ...     file_extensions=[\"csv\"])\\n        Dataset(num_blocks=..., num_rows=1, schema={a: int64, b: int64})\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. By default, this function parses\\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_csv_args: CSV read options to pass to\\n            `pyarrow.csv.open_csv <https://arrow.apache.org/docs/python/generated/            pyarrow.csv.open_csv.html#pyarrow.csv.open_csv>`_\\n            when opening CSV files.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(CSVDatasource._FILE_EXTENSIONS)\n    datasource = CSVDatasource(paths, arrow_csv_args=arrow_csv_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_csv(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_csv_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a :class:`~ray.data.Dataset` from CSV files.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\\n        >>> ds.schema()\\n        Column             Type\\n        ------             ----\\n        sepal length (cm)  double\\n        sepal width (cm)   double\\n        petal length (cm)  double\\n        petal width (cm)   double\\n        target             int64\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_csv( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read a directory from remote storage.\\n\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris-csv/\")\\n\\n        Read files that use a different delimiter. For more uses of ParseOptions see\\n        https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html  # noqa: #501\\n\\n        >>> from pyarrow import csv\\n        >>> parse_options = csv.ParseOptions(delimiter=\"\\\\t\")\\n        >>> ds = ray.data.read_csv(\\n        ...     \"s3://anonymous@ray-example-data/iris.tsv\",\\n        ...     parse_options=parse_options)\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  double\\n        sepal.width   double\\n        petal.length  double\\n        petal.width   double\\n        variety       string\\n\\n        Convert a date column with a custom format from a CSV file. For more uses of ConvertOptions see https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html  # noqa: #501\\n\\n        >>> from pyarrow import csv\\n        >>> convert_options = csv.ConvertOptions(\\n        ...     timestamp_parsers=[\"%m/%d/%Y\"])\\n        >>> ds = ray.data.read_csv(\\n        ...     \"s3://anonymous@ray-example-data/dow_jones.csv\",\\n        ...     convert_options=convert_options)\\n\\n        By default, :meth:`~ray.data.read_csv` parses\\n        `Hive-style partitions <https://athena.guide/        articles/hive-style-partitioning/>`_\\n        from file paths. If your data adheres to a different partitioning scheme, set\\n        the ``partitioning`` parameter.\\n\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.csv\")\\n        >>> ds.take(1)\\n        [{\\'order_number\\': 10107, \\'quantity\\': 30, \\'year\\': \\'2022\\', \\'month\\': \\'09\\'}]\\n\\n        By default, :meth:`~ray.data.read_csv` reads all files from file paths. If you want to filter\\n        files by file extensions, set the ``partition_filter`` parameter.\\n\\n        Read only ``*.csv`` files from a directory.\\n\\n        >>> ray.data.read_csv(\"s3://anonymous@ray-example-data/different-extensions/\",\\n        ...     file_extensions=[\"csv\"])\\n        Dataset(num_blocks=..., num_rows=1, schema={a: int64, b: int64})\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. By default, this function parses\\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_csv_args: CSV read options to pass to\\n            `pyarrow.csv.open_csv <https://arrow.apache.org/docs/python/generated/            pyarrow.csv.open_csv.html#pyarrow.csv.open_csv>`_\\n            when opening CSV files.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(CSVDatasource._FILE_EXTENSIONS)\n    datasource = CSVDatasource(paths, arrow_csv_args=arrow_csv_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_csv(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_csv_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a :class:`~ray.data.Dataset` from CSV files.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\\n        >>> ds.schema()\\n        Column             Type\\n        ------             ----\\n        sepal length (cm)  double\\n        sepal width (cm)   double\\n        petal length (cm)  double\\n        petal width (cm)   double\\n        target             int64\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_csv( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read a directory from remote storage.\\n\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris-csv/\")\\n\\n        Read files that use a different delimiter. For more uses of ParseOptions see\\n        https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html  # noqa: #501\\n\\n        >>> from pyarrow import csv\\n        >>> parse_options = csv.ParseOptions(delimiter=\"\\\\t\")\\n        >>> ds = ray.data.read_csv(\\n        ...     \"s3://anonymous@ray-example-data/iris.tsv\",\\n        ...     parse_options=parse_options)\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  double\\n        sepal.width   double\\n        petal.length  double\\n        petal.width   double\\n        variety       string\\n\\n        Convert a date column with a custom format from a CSV file. For more uses of ConvertOptions see https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html  # noqa: #501\\n\\n        >>> from pyarrow import csv\\n        >>> convert_options = csv.ConvertOptions(\\n        ...     timestamp_parsers=[\"%m/%d/%Y\"])\\n        >>> ds = ray.data.read_csv(\\n        ...     \"s3://anonymous@ray-example-data/dow_jones.csv\",\\n        ...     convert_options=convert_options)\\n\\n        By default, :meth:`~ray.data.read_csv` parses\\n        `Hive-style partitions <https://athena.guide/        articles/hive-style-partitioning/>`_\\n        from file paths. If your data adheres to a different partitioning scheme, set\\n        the ``partitioning`` parameter.\\n\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.csv\")\\n        >>> ds.take(1)\\n        [{\\'order_number\\': 10107, \\'quantity\\': 30, \\'year\\': \\'2022\\', \\'month\\': \\'09\\'}]\\n\\n        By default, :meth:`~ray.data.read_csv` reads all files from file paths. If you want to filter\\n        files by file extensions, set the ``partition_filter`` parameter.\\n\\n        Read only ``*.csv`` files from a directory.\\n\\n        >>> ray.data.read_csv(\"s3://anonymous@ray-example-data/different-extensions/\",\\n        ...     file_extensions=[\"csv\"])\\n        Dataset(num_blocks=..., num_rows=1, schema={a: int64, b: int64})\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. By default, this function parses\\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_csv_args: CSV read options to pass to\\n            `pyarrow.csv.open_csv <https://arrow.apache.org/docs/python/generated/            pyarrow.csv.open_csv.html#pyarrow.csv.open_csv>`_\\n            when opening CSV files.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(CSVDatasource._FILE_EXTENSIONS)\n    datasource = CSVDatasource(paths, arrow_csv_args=arrow_csv_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_csv(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_csv_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a :class:`~ray.data.Dataset` from CSV files.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\\n        >>> ds.schema()\\n        Column             Type\\n        ------             ----\\n        sepal length (cm)  double\\n        sepal width (cm)   double\\n        petal length (cm)  double\\n        petal width (cm)   double\\n        target             int64\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_csv( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read a directory from remote storage.\\n\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris-csv/\")\\n\\n        Read files that use a different delimiter. For more uses of ParseOptions see\\n        https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html  # noqa: #501\\n\\n        >>> from pyarrow import csv\\n        >>> parse_options = csv.ParseOptions(delimiter=\"\\\\t\")\\n        >>> ds = ray.data.read_csv(\\n        ...     \"s3://anonymous@ray-example-data/iris.tsv\",\\n        ...     parse_options=parse_options)\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  double\\n        sepal.width   double\\n        petal.length  double\\n        petal.width   double\\n        variety       string\\n\\n        Convert a date column with a custom format from a CSV file. For more uses of ConvertOptions see https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html  # noqa: #501\\n\\n        >>> from pyarrow import csv\\n        >>> convert_options = csv.ConvertOptions(\\n        ...     timestamp_parsers=[\"%m/%d/%Y\"])\\n        >>> ds = ray.data.read_csv(\\n        ...     \"s3://anonymous@ray-example-data/dow_jones.csv\",\\n        ...     convert_options=convert_options)\\n\\n        By default, :meth:`~ray.data.read_csv` parses\\n        `Hive-style partitions <https://athena.guide/        articles/hive-style-partitioning/>`_\\n        from file paths. If your data adheres to a different partitioning scheme, set\\n        the ``partitioning`` parameter.\\n\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.csv\")\\n        >>> ds.take(1)\\n        [{\\'order_number\\': 10107, \\'quantity\\': 30, \\'year\\': \\'2022\\', \\'month\\': \\'09\\'}]\\n\\n        By default, :meth:`~ray.data.read_csv` reads all files from file paths. If you want to filter\\n        files by file extensions, set the ``partition_filter`` parameter.\\n\\n        Read only ``*.csv`` files from a directory.\\n\\n        >>> ray.data.read_csv(\"s3://anonymous@ray-example-data/different-extensions/\",\\n        ...     file_extensions=[\"csv\"])\\n        Dataset(num_blocks=..., num_rows=1, schema={a: int64, b: int64})\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. By default, this function parses\\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_csv_args: CSV read options to pass to\\n            `pyarrow.csv.open_csv <https://arrow.apache.org/docs/python/generated/            pyarrow.csv.open_csv.html#pyarrow.csv.open_csv>`_\\n            when opening CSV files.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(CSVDatasource._FILE_EXTENSIONS)\n    datasource = CSVDatasource(paths, arrow_csv_args=arrow_csv_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_csv(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=Partitioning('hive'), ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None, **arrow_csv_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a :class:`~ray.data.Dataset` from CSV files.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\\n        >>> ds.schema()\\n        Column             Type\\n        ------             ----\\n        sepal length (cm)  double\\n        sepal width (cm)   double\\n        petal length (cm)  double\\n        petal width (cm)   double\\n        target             int64\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_csv( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read a directory from remote storage.\\n\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris-csv/\")\\n\\n        Read files that use a different delimiter. For more uses of ParseOptions see\\n        https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html  # noqa: #501\\n\\n        >>> from pyarrow import csv\\n        >>> parse_options = csv.ParseOptions(delimiter=\"\\\\t\")\\n        >>> ds = ray.data.read_csv(\\n        ...     \"s3://anonymous@ray-example-data/iris.tsv\",\\n        ...     parse_options=parse_options)\\n        >>> ds.schema()\\n        Column        Type\\n        ------        ----\\n        sepal.length  double\\n        sepal.width   double\\n        petal.length  double\\n        petal.width   double\\n        variety       string\\n\\n        Convert a date column with a custom format from a CSV file. For more uses of ConvertOptions see https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html  # noqa: #501\\n\\n        >>> from pyarrow import csv\\n        >>> convert_options = csv.ConvertOptions(\\n        ...     timestamp_parsers=[\"%m/%d/%Y\"])\\n        >>> ds = ray.data.read_csv(\\n        ...     \"s3://anonymous@ray-example-data/dow_jones.csv\",\\n        ...     convert_options=convert_options)\\n\\n        By default, :meth:`~ray.data.read_csv` parses\\n        `Hive-style partitions <https://athena.guide/        articles/hive-style-partitioning/>`_\\n        from file paths. If your data adheres to a different partitioning scheme, set\\n        the ``partitioning`` parameter.\\n\\n        >>> ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/year=2022/month=09/sales.csv\")\\n        >>> ds.take(1)\\n        [{\\'order_number\\': 10107, \\'quantity\\': 30, \\'year\\': \\'2022\\', \\'month\\': \\'09\\'}]\\n\\n        By default, :meth:`~ray.data.read_csv` reads all files from file paths. If you want to filter\\n        files by file extensions, set the ``partition_filter`` parameter.\\n\\n        Read only ``*.csv`` files from a directory.\\n\\n        >>> ray.data.read_csv(\"s3://anonymous@ray-example-data/different-extensions/\",\\n        ...     file_extensions=[\"csv\"])\\n        Dataset(num_blocks=..., num_rows=1, schema={a: int64, b: int64})\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. By default, this function parses\\n            `Hive-style partitions <https://athena.guide/articles/                hive-style-partitioning/>`_.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        arrow_csv_args: CSV read options to pass to\\n            `pyarrow.csv.open_csv <https://arrow.apache.org/docs/python/generated/            pyarrow.csv.open_csv.html#pyarrow.csv.open_csv>`_\\n            when opening CSV files.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(CSVDatasource._FILE_EXTENSIONS)\n    datasource = CSVDatasource(paths, arrow_csv_args=arrow_csv_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "read_text",
        "original": "@PublicAPI\ndef read_text(paths: Union[str, List[str]], *, encoding: str='utf-8', drop_empty_lines: bool=True, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from lines stored in text files.\n\n    Examples:\n        Read a file in remote storage.\n\n        >>> import ray\n        >>> ds = ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\n        >>> ds.schema()\n        Column  Type\n        ------  ----\n        text    string\n\n        Read multiple local files.\n\n        >>> ray.data.read_text( # doctest: +SKIP\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\n\n    Args:\n        paths: A single file or directory, or a list of file or directory paths.\n            A list of paths can contain both files and directories.\n        encoding: The encoding of the files (e.g., \"utf-8\" or \"ascii\").\n        filesystem: The PyArrow filesystem\n            implementation to read from. These filesystems are specified in the\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\n            you need to provide specific configurations to the filesystem. By default,\n            the filesystem is automatically selected based on the scheme of the paths.\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see :ref:`Tuning read parallelism\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\n            lines in all the text files.\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks and\n            in the subsequent text decoding map task.\n        arrow_open_stream_args: kwargs passed to\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\n            when opening input files to read.\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\n            metadata providers may be able to resolve file metadata more quickly and/or\n            accurately. In most cases, you do not need to set this. If ``None``, this\n            function uses a system-chosen implementation.\n        partition_filter: A\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\n            Use with a custom callback to read only selected partitions of a\n            dataset. By default, no files are filtered.\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\n            that describes how paths are organized. Defaults to ``None``.\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\n            found. Defaults to False.\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\n            Defaults to not shuffle with ``None``.\n        file_extensions: A list of file extensions to filter files by.\n\n    Returns:\n        :class:`~ray.data.Dataset` producing lines of text read from the specified\n        paths.\n    \"\"\"\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TextDatasource._FILE_EXTENSIONS)\n    datasource = TextDatasource(paths, drop_empty_lines=drop_empty_lines, encoding=encoding, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI\ndef read_text(paths: Union[str, List[str]], *, encoding: str='utf-8', drop_empty_lines: bool=True, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from lines stored in text files.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        text    string\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_text( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        encoding: The encoding of the files (e.g., \"utf-8\" or \"ascii\").\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            lines in all the text files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks and\\n            in the subsequent text decoding map task.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing lines of text read from the specified\\n        paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TextDatasource._FILE_EXTENSIONS)\n    datasource = TextDatasource(paths, drop_empty_lines=drop_empty_lines, encoding=encoding, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_text(paths: Union[str, List[str]], *, encoding: str='utf-8', drop_empty_lines: bool=True, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from lines stored in text files.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        text    string\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_text( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        encoding: The encoding of the files (e.g., \"utf-8\" or \"ascii\").\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            lines in all the text files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks and\\n            in the subsequent text decoding map task.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing lines of text read from the specified\\n        paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TextDatasource._FILE_EXTENSIONS)\n    datasource = TextDatasource(paths, drop_empty_lines=drop_empty_lines, encoding=encoding, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_text(paths: Union[str, List[str]], *, encoding: str='utf-8', drop_empty_lines: bool=True, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from lines stored in text files.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        text    string\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_text( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        encoding: The encoding of the files (e.g., \"utf-8\" or \"ascii\").\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            lines in all the text files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks and\\n            in the subsequent text decoding map task.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing lines of text read from the specified\\n        paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TextDatasource._FILE_EXTENSIONS)\n    datasource = TextDatasource(paths, drop_empty_lines=drop_empty_lines, encoding=encoding, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_text(paths: Union[str, List[str]], *, encoding: str='utf-8', drop_empty_lines: bool=True, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from lines stored in text files.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        text    string\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_text( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        encoding: The encoding of the files (e.g., \"utf-8\" or \"ascii\").\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            lines in all the text files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks and\\n            in the subsequent text decoding map task.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing lines of text read from the specified\\n        paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TextDatasource._FILE_EXTENSIONS)\n    datasource = TextDatasource(paths, drop_empty_lines=drop_empty_lines, encoding=encoding, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_text(paths: Union[str, List[str]], *, encoding: str='utf-8', drop_empty_lines: bool=True, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from lines stored in text files.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> ds = ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        text    string\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_text( # doctest: +SKIP\\n        ...    [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        encoding: The encoding of the files (e.g., \"utf-8\" or \"ascii\").\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            lines in all the text files.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks and\\n            in the subsequent text decoding map task.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing lines of text read from the specified\\n        paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TextDatasource._FILE_EXTENSIONS)\n    datasource = TextDatasource(paths, drop_empty_lines=drop_empty_lines, encoding=encoding, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "read_numpy",
        "original": "@PublicAPI\ndef read_numpy(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=NumpyDatasource._FILE_EXTENSIONS, **numpy_load_args) -> Dataset:\n    \"\"\"Create an Arrow dataset from numpy files.\n\n    Examples:\n        Read a directory of files in remote storage.\n\n        >>> import ray\n        >>> ray.data.read_numpy(\"s3://bucket/path\") # doctest: +SKIP\n\n        Read multiple local files.\n\n        >>> ray.data.read_numpy([\"/path/to/file1\", \"/path/to/file2\"]) # doctest: +SKIP\n\n        Read multiple directories.\n\n        >>> ray.data.read_numpy( # doctest: +SKIP\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\n\n    Args:\n        paths: A single file/directory path or a list of file/directory paths.\n            A list of paths can contain both files and directories.\n        filesystem: The filesystem implementation to read from.\n        parallelism: The requested parallelism of the read. Parallelism may be\n            limited by the number of files of the dataset.\n        arrow_open_stream_args: kwargs passed to\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\n        numpy_load_args: Other options to pass to np.load.\n        meta_provider: File metadata provider. Custom metadata providers may\n            be able to resolve file metadata more quickly and/or accurately. If\n            ``None``, this function uses a system-chosen implementation.\n        partition_filter: Path-based partition filter, if any. Can be used\n            with a custom callback to read only selected partitions of a dataset.\n            By default, this filters out any file paths whose file extension does not\n            match \"*.npy*\".\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\n            that describes how paths are organized. Defaults to ``None``.\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\n            found. Defaults to False.\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\n            Defaults to not shuffle with ``None``.\n        file_extensions: A list of file extensions to filter files by.\n\n    Returns:\n        Dataset holding Tensor records read from the specified paths.\n    \"\"\"\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(NumpyDatasource._FILE_EXTENSIONS)\n    datasource = NumpyDatasource(paths, numpy_load_args=numpy_load_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
        "mutated": [
            "@PublicAPI\ndef read_numpy(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=NumpyDatasource._FILE_EXTENSIONS, **numpy_load_args) -> Dataset:\n    if False:\n        i = 10\n    'Create an Arrow dataset from numpy files.\\n\\n    Examples:\\n        Read a directory of files in remote storage.\\n\\n        >>> import ray\\n        >>> ray.data.read_numpy(\"s3://bucket/path\") # doctest: +SKIP\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_numpy([\"/path/to/file1\", \"/path/to/file2\"]) # doctest: +SKIP\\n\\n        Read multiple directories.\\n\\n        >>> ray.data.read_numpy( # doctest: +SKIP\\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\\n\\n    Args:\\n        paths: A single file/directory path or a list of file/directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The filesystem implementation to read from.\\n        parallelism: The requested parallelism of the read. Parallelism may be\\n            limited by the number of files of the dataset.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\\n        numpy_load_args: Other options to pass to np.load.\\n        meta_provider: File metadata provider. Custom metadata providers may\\n            be able to resolve file metadata more quickly and/or accurately. If\\n            ``None``, this function uses a system-chosen implementation.\\n        partition_filter: Path-based partition filter, if any. Can be used\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.npy*\".\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        Dataset holding Tensor records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(NumpyDatasource._FILE_EXTENSIONS)\n    datasource = NumpyDatasource(paths, numpy_load_args=numpy_load_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef read_numpy(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=NumpyDatasource._FILE_EXTENSIONS, **numpy_load_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create an Arrow dataset from numpy files.\\n\\n    Examples:\\n        Read a directory of files in remote storage.\\n\\n        >>> import ray\\n        >>> ray.data.read_numpy(\"s3://bucket/path\") # doctest: +SKIP\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_numpy([\"/path/to/file1\", \"/path/to/file2\"]) # doctest: +SKIP\\n\\n        Read multiple directories.\\n\\n        >>> ray.data.read_numpy( # doctest: +SKIP\\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\\n\\n    Args:\\n        paths: A single file/directory path or a list of file/directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The filesystem implementation to read from.\\n        parallelism: The requested parallelism of the read. Parallelism may be\\n            limited by the number of files of the dataset.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\\n        numpy_load_args: Other options to pass to np.load.\\n        meta_provider: File metadata provider. Custom metadata providers may\\n            be able to resolve file metadata more quickly and/or accurately. If\\n            ``None``, this function uses a system-chosen implementation.\\n        partition_filter: Path-based partition filter, if any. Can be used\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.npy*\".\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        Dataset holding Tensor records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(NumpyDatasource._FILE_EXTENSIONS)\n    datasource = NumpyDatasource(paths, numpy_load_args=numpy_load_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef read_numpy(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=NumpyDatasource._FILE_EXTENSIONS, **numpy_load_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create an Arrow dataset from numpy files.\\n\\n    Examples:\\n        Read a directory of files in remote storage.\\n\\n        >>> import ray\\n        >>> ray.data.read_numpy(\"s3://bucket/path\") # doctest: +SKIP\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_numpy([\"/path/to/file1\", \"/path/to/file2\"]) # doctest: +SKIP\\n\\n        Read multiple directories.\\n\\n        >>> ray.data.read_numpy( # doctest: +SKIP\\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\\n\\n    Args:\\n        paths: A single file/directory path or a list of file/directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The filesystem implementation to read from.\\n        parallelism: The requested parallelism of the read. Parallelism may be\\n            limited by the number of files of the dataset.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\\n        numpy_load_args: Other options to pass to np.load.\\n        meta_provider: File metadata provider. Custom metadata providers may\\n            be able to resolve file metadata more quickly and/or accurately. If\\n            ``None``, this function uses a system-chosen implementation.\\n        partition_filter: Path-based partition filter, if any. Can be used\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.npy*\".\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        Dataset holding Tensor records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(NumpyDatasource._FILE_EXTENSIONS)\n    datasource = NumpyDatasource(paths, numpy_load_args=numpy_load_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef read_numpy(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=NumpyDatasource._FILE_EXTENSIONS, **numpy_load_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create an Arrow dataset from numpy files.\\n\\n    Examples:\\n        Read a directory of files in remote storage.\\n\\n        >>> import ray\\n        >>> ray.data.read_numpy(\"s3://bucket/path\") # doctest: +SKIP\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_numpy([\"/path/to/file1\", \"/path/to/file2\"]) # doctest: +SKIP\\n\\n        Read multiple directories.\\n\\n        >>> ray.data.read_numpy( # doctest: +SKIP\\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\\n\\n    Args:\\n        paths: A single file/directory path or a list of file/directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The filesystem implementation to read from.\\n        parallelism: The requested parallelism of the read. Parallelism may be\\n            limited by the number of files of the dataset.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\\n        numpy_load_args: Other options to pass to np.load.\\n        meta_provider: File metadata provider. Custom metadata providers may\\n            be able to resolve file metadata more quickly and/or accurately. If\\n            ``None``, this function uses a system-chosen implementation.\\n        partition_filter: Path-based partition filter, if any. Can be used\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.npy*\".\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        Dataset holding Tensor records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(NumpyDatasource._FILE_EXTENSIONS)\n    datasource = NumpyDatasource(paths, numpy_load_args=numpy_load_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI\ndef read_numpy(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=NumpyDatasource._FILE_EXTENSIONS, **numpy_load_args) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create an Arrow dataset from numpy files.\\n\\n    Examples:\\n        Read a directory of files in remote storage.\\n\\n        >>> import ray\\n        >>> ray.data.read_numpy(\"s3://bucket/path\") # doctest: +SKIP\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_numpy([\"/path/to/file1\", \"/path/to/file2\"]) # doctest: +SKIP\\n\\n        Read multiple directories.\\n\\n        >>> ray.data.read_numpy( # doctest: +SKIP\\n        ...     [\"s3://bucket/path1\", \"s3://bucket/path2\"])\\n\\n    Args:\\n        paths: A single file/directory path or a list of file/directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The filesystem implementation to read from.\\n        parallelism: The requested parallelism of the read. Parallelism may be\\n            limited by the number of files of the dataset.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\\n        numpy_load_args: Other options to pass to np.load.\\n        meta_provider: File metadata provider. Custom metadata providers may\\n            be able to resolve file metadata more quickly and/or accurately. If\\n            ``None``, this function uses a system-chosen implementation.\\n        partition_filter: Path-based partition filter, if any. Can be used\\n            with a custom callback to read only selected partitions of a dataset.\\n            By default, this filters out any file paths whose file extension does not\\n            match \"*.npy*\".\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        Dataset holding Tensor records read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(NumpyDatasource._FILE_EXTENSIONS)\n    datasource = NumpyDatasource(paths, numpy_load_args=numpy_load_args, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)"
        ]
    },
    {
        "func_name": "read_tfrecords",
        "original": "@PublicAPI(stability='alpha')\ndef read_tfrecords(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, ignore_missing_paths: bool=False, tf_schema: Optional['schema_pb2.Schema']=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from TFRecord files that contain\n    `tf.train.Example <https://www.tensorflow.org/api_docs/python/tf/train/Example>`_\n    messages.\n\n    .. warning::\n        This function exclusively supports ``tf.train.Example`` messages. If a file\n        contains a message that isn't of type ``tf.train.Example``, then this function\n        fails.\n\n    Examples:\n        >>> import ray\n        >>> ray.data.read_tfrecords(\"s3://anonymous@ray-example-data/iris.tfrecords\")\n        Dataset(\n           num_blocks=...,\n           num_rows=150,\n           schema={...}\n        )\n\n        We can also read compressed TFRecord files, which use one of the\n        `compression types supported by Arrow <https://arrow.apache.org/docs/python/            generated/pyarrow.CompressedInputStream.html>`_:\n\n        >>> ray.data.read_tfrecords(\n        ...     \"s3://anonymous@ray-example-data/iris.tfrecords.gz\",\n        ...     arrow_open_stream_args={\"compression\": \"gzip\"},\n        ... )\n        Dataset(\n           num_blocks=...,\n           num_rows=150,\n           schema={...}\n        )\n\n    Args:\n        paths: A single file or directory, or a list of file or directory paths.\n            A list of paths can contain both files and directories.\n        filesystem: The PyArrow filesystem\n            implementation to read from. These filesystems are specified in the\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\n            you need to provide specific configurations to the filesystem. By default,\n            the filesystem is automatically selected based on the scheme of the paths.\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see :ref:`Tuning read parallelism\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\n            records in all the CSV files.\n        arrow_open_stream_args: kwargs passed to\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\n            when opening input files to read. To read a compressed TFRecord file,\n            pass the corresponding compression type (e.g., for ``GZIP`` or ``ZLIB``),\n            use ``arrow_open_stream_args={'compression_type': 'gzip'}``).\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\n            metadata providers may be able to resolve file metadata more quickly and/or\n            accurately. In most cases, you do not need to set this. If ``None``, this\n            function uses a system-chosen implementation.\n        partition_filter: A\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\n            Use with a custom callback to read only selected partitions of a\n            dataset.\n        ignore_missing_paths:  If True, ignores any file paths in ``paths`` that are not\n            found. Defaults to False.\n        tf_schema: Optional TensorFlow Schema which is used to explicitly set the schema\n            of the underlying Dataset.\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\n            Defaults to not shuffle with ``None``.\n        file_extensions: A list of file extensions to filter files by.\n\n    Returns:\n        A :class:`~ray.data.Dataset` that contains the example features.\n\n    Raises:\n        ValueError: If a file contains a message that isn't a ``tf.train.Example``.\n    \"\"\"\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TFRecordDatasource._FILE_EXTENSIONS)\n    datasource = TFRecordDatasource(paths, tf_schema=tf_schema, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
        "mutated": [
            "@PublicAPI(stability='alpha')\ndef read_tfrecords(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, ignore_missing_paths: bool=False, tf_schema: Optional['schema_pb2.Schema']=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from TFRecord files that contain\\n    `tf.train.Example <https://www.tensorflow.org/api_docs/python/tf/train/Example>`_\\n    messages.\\n\\n    .. warning::\\n        This function exclusively supports ``tf.train.Example`` messages. If a file\\n        contains a message that isn\\'t of type ``tf.train.Example``, then this function\\n        fails.\\n\\n    Examples:\\n        >>> import ray\\n        >>> ray.data.read_tfrecords(\"s3://anonymous@ray-example-data/iris.tfrecords\")\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=150,\\n           schema={...}\\n        )\\n\\n        We can also read compressed TFRecord files, which use one of the\\n        `compression types supported by Arrow <https://arrow.apache.org/docs/python/            generated/pyarrow.CompressedInputStream.html>`_:\\n\\n        >>> ray.data.read_tfrecords(\\n        ...     \"s3://anonymous@ray-example-data/iris.tfrecords.gz\",\\n        ...     arrow_open_stream_args={\"compression\": \"gzip\"},\\n        ... )\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=150,\\n           schema={...}\\n        )\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read. To read a compressed TFRecord file,\\n            pass the corresponding compression type (e.g., for ``GZIP`` or ``ZLIB``),\\n            use ``arrow_open_stream_args={\\'compression_type\\': \\'gzip\\'}``).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset.\\n        ignore_missing_paths:  If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        tf_schema: Optional TensorFlow Schema which is used to explicitly set the schema\\n            of the underlying Dataset.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` that contains the example features.\\n\\n    Raises:\\n        ValueError: If a file contains a message that isn\\'t a ``tf.train.Example``.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TFRecordDatasource._FILE_EXTENSIONS)\n    datasource = TFRecordDatasource(paths, tf_schema=tf_schema, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI(stability='alpha')\ndef read_tfrecords(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, ignore_missing_paths: bool=False, tf_schema: Optional['schema_pb2.Schema']=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from TFRecord files that contain\\n    `tf.train.Example <https://www.tensorflow.org/api_docs/python/tf/train/Example>`_\\n    messages.\\n\\n    .. warning::\\n        This function exclusively supports ``tf.train.Example`` messages. If a file\\n        contains a message that isn\\'t of type ``tf.train.Example``, then this function\\n        fails.\\n\\n    Examples:\\n        >>> import ray\\n        >>> ray.data.read_tfrecords(\"s3://anonymous@ray-example-data/iris.tfrecords\")\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=150,\\n           schema={...}\\n        )\\n\\n        We can also read compressed TFRecord files, which use one of the\\n        `compression types supported by Arrow <https://arrow.apache.org/docs/python/            generated/pyarrow.CompressedInputStream.html>`_:\\n\\n        >>> ray.data.read_tfrecords(\\n        ...     \"s3://anonymous@ray-example-data/iris.tfrecords.gz\",\\n        ...     arrow_open_stream_args={\"compression\": \"gzip\"},\\n        ... )\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=150,\\n           schema={...}\\n        )\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read. To read a compressed TFRecord file,\\n            pass the corresponding compression type (e.g., for ``GZIP`` or ``ZLIB``),\\n            use ``arrow_open_stream_args={\\'compression_type\\': \\'gzip\\'}``).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset.\\n        ignore_missing_paths:  If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        tf_schema: Optional TensorFlow Schema which is used to explicitly set the schema\\n            of the underlying Dataset.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` that contains the example features.\\n\\n    Raises:\\n        ValueError: If a file contains a message that isn\\'t a ``tf.train.Example``.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TFRecordDatasource._FILE_EXTENSIONS)\n    datasource = TFRecordDatasource(paths, tf_schema=tf_schema, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI(stability='alpha')\ndef read_tfrecords(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, ignore_missing_paths: bool=False, tf_schema: Optional['schema_pb2.Schema']=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from TFRecord files that contain\\n    `tf.train.Example <https://www.tensorflow.org/api_docs/python/tf/train/Example>`_\\n    messages.\\n\\n    .. warning::\\n        This function exclusively supports ``tf.train.Example`` messages. If a file\\n        contains a message that isn\\'t of type ``tf.train.Example``, then this function\\n        fails.\\n\\n    Examples:\\n        >>> import ray\\n        >>> ray.data.read_tfrecords(\"s3://anonymous@ray-example-data/iris.tfrecords\")\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=150,\\n           schema={...}\\n        )\\n\\n        We can also read compressed TFRecord files, which use one of the\\n        `compression types supported by Arrow <https://arrow.apache.org/docs/python/            generated/pyarrow.CompressedInputStream.html>`_:\\n\\n        >>> ray.data.read_tfrecords(\\n        ...     \"s3://anonymous@ray-example-data/iris.tfrecords.gz\",\\n        ...     arrow_open_stream_args={\"compression\": \"gzip\"},\\n        ... )\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=150,\\n           schema={...}\\n        )\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read. To read a compressed TFRecord file,\\n            pass the corresponding compression type (e.g., for ``GZIP`` or ``ZLIB``),\\n            use ``arrow_open_stream_args={\\'compression_type\\': \\'gzip\\'}``).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset.\\n        ignore_missing_paths:  If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        tf_schema: Optional TensorFlow Schema which is used to explicitly set the schema\\n            of the underlying Dataset.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` that contains the example features.\\n\\n    Raises:\\n        ValueError: If a file contains a message that isn\\'t a ``tf.train.Example``.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TFRecordDatasource._FILE_EXTENSIONS)\n    datasource = TFRecordDatasource(paths, tf_schema=tf_schema, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI(stability='alpha')\ndef read_tfrecords(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, ignore_missing_paths: bool=False, tf_schema: Optional['schema_pb2.Schema']=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from TFRecord files that contain\\n    `tf.train.Example <https://www.tensorflow.org/api_docs/python/tf/train/Example>`_\\n    messages.\\n\\n    .. warning::\\n        This function exclusively supports ``tf.train.Example`` messages. If a file\\n        contains a message that isn\\'t of type ``tf.train.Example``, then this function\\n        fails.\\n\\n    Examples:\\n        >>> import ray\\n        >>> ray.data.read_tfrecords(\"s3://anonymous@ray-example-data/iris.tfrecords\")\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=150,\\n           schema={...}\\n        )\\n\\n        We can also read compressed TFRecord files, which use one of the\\n        `compression types supported by Arrow <https://arrow.apache.org/docs/python/            generated/pyarrow.CompressedInputStream.html>`_:\\n\\n        >>> ray.data.read_tfrecords(\\n        ...     \"s3://anonymous@ray-example-data/iris.tfrecords.gz\",\\n        ...     arrow_open_stream_args={\"compression\": \"gzip\"},\\n        ... )\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=150,\\n           schema={...}\\n        )\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read. To read a compressed TFRecord file,\\n            pass the corresponding compression type (e.g., for ``GZIP`` or ``ZLIB``),\\n            use ``arrow_open_stream_args={\\'compression_type\\': \\'gzip\\'}``).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset.\\n        ignore_missing_paths:  If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        tf_schema: Optional TensorFlow Schema which is used to explicitly set the schema\\n            of the underlying Dataset.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` that contains the example features.\\n\\n    Raises:\\n        ValueError: If a file contains a message that isn\\'t a ``tf.train.Example``.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TFRecordDatasource._FILE_EXTENSIONS)\n    datasource = TFRecordDatasource(paths, tf_schema=tf_schema, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI(stability='alpha')\ndef read_tfrecords(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, ignore_missing_paths: bool=False, tf_schema: Optional['schema_pb2.Schema']=None, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from TFRecord files that contain\\n    `tf.train.Example <https://www.tensorflow.org/api_docs/python/tf/train/Example>`_\\n    messages.\\n\\n    .. warning::\\n        This function exclusively supports ``tf.train.Example`` messages. If a file\\n        contains a message that isn\\'t of type ``tf.train.Example``, then this function\\n        fails.\\n\\n    Examples:\\n        >>> import ray\\n        >>> ray.data.read_tfrecords(\"s3://anonymous@ray-example-data/iris.tfrecords\")\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=150,\\n           schema={...}\\n        )\\n\\n        We can also read compressed TFRecord files, which use one of the\\n        `compression types supported by Arrow <https://arrow.apache.org/docs/python/            generated/pyarrow.CompressedInputStream.html>`_:\\n\\n        >>> ray.data.read_tfrecords(\\n        ...     \"s3://anonymous@ray-example-data/iris.tfrecords.gz\",\\n        ...     arrow_open_stream_args={\"compression\": \"gzip\"},\\n        ... )\\n        Dataset(\\n           num_blocks=...,\\n           num_rows=150,\\n           schema={...}\\n        )\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            records in all the CSV files.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n            when opening input files to read. To read a compressed TFRecord file,\\n            pass the corresponding compression type (e.g., for ``GZIP`` or ``ZLIB``),\\n            use ``arrow_open_stream_args={\\'compression_type\\': \\'gzip\\'}``).\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset.\\n        ignore_missing_paths:  If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        tf_schema: Optional TensorFlow Schema which is used to explicitly set the schema\\n            of the underlying Dataset.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` that contains the example features.\\n\\n    Raises:\\n        ValueError: If a file contains a message that isn\\'t a ``tf.train.Example``.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(TFRecordDatasource._FILE_EXTENSIONS)\n    datasource = TFRecordDatasource(paths, tf_schema=tf_schema, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)"
        ]
    },
    {
        "func_name": "read_webdataset",
        "original": "@PublicAPI(stability='alpha')\ndef read_webdataset(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, decoder: Optional[Union[bool, str, callable, list]]=True, fileselect: Optional[Union[list, callable]]=None, filerename: Optional[Union[list, callable]]=None, suffixes: Optional[Union[list, callable]]=None, verbose_open: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from\n    `WebDataset <https://webdataset.github.io/webdataset/>`_ files.\n\n    Args:\n        paths: A single file/directory path or a list of file/directory paths.\n            A list of paths can contain both files and directories.\n        filesystem: The filesystem implementation to read from.\n        parallelism: The requested parallelism of the read. Parallelism may be\n            limited by the number of files in the dataset.\n        arrow_open_stream_args: Key-word arguments passed to\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\n            To read a compressed TFRecord file,\n            pass the corresponding compression type (e.g. for ``GZIP`` or ``ZLIB``, use\n            ``arrow_open_stream_args={'compression_type': 'gzip'}``).\n        meta_provider: File metadata provider. Custom metadata providers may\n            be able to resolve file metadata more quickly and/or accurately. If\n            ``None``, this function uses a system-chosen implementation.\n        partition_filter: Path-based partition filter, if any. Can be used\n            with a custom callback to read only selected partitions of a dataset.\n        decoder: A function or list of functions to decode the data.\n        fileselect: A callable or list of glob patterns to select files.\n        filerename: A function or list of tuples to rename files prior to grouping.\n        suffixes: A function or list of suffixes to select for creating samples.\n        verbose_open: Whether to print the file names as they are opened.\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\n            Defaults to not shuffle with ``None``.\n        file_extensions: A list of file extensions to filter files by.\n\n    Returns:\n        A :class:`~ray.data.Dataset` that contains the example features.\n\n    Raises:\n        ValueError: If a file contains a message that isn't a `tf.train.Example`_.\n\n    .. _tf.train.Example: https://www.tensorflow.org/api_docs/python/tf/train/Example\n    \"\"\"\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(WebDatasetDatasource._FILE_EXTENSIONS)\n    datasource = WebDatasetDatasource(paths, decoder=decoder, fileselect=fileselect, filerename=filerename, suffixes=suffixes, verbose_open=verbose_open, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
        "mutated": [
            "@PublicAPI(stability='alpha')\ndef read_webdataset(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, decoder: Optional[Union[bool, str, callable, list]]=True, fileselect: Optional[Union[list, callable]]=None, filerename: Optional[Union[list, callable]]=None, suffixes: Optional[Union[list, callable]]=None, verbose_open: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from\\n    `WebDataset <https://webdataset.github.io/webdataset/>`_ files.\\n\\n    Args:\\n        paths: A single file/directory path or a list of file/directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The filesystem implementation to read from.\\n        parallelism: The requested parallelism of the read. Parallelism may be\\n            limited by the number of files in the dataset.\\n        arrow_open_stream_args: Key-word arguments passed to\\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\\n            To read a compressed TFRecord file,\\n            pass the corresponding compression type (e.g. for ``GZIP`` or ``ZLIB``, use\\n            ``arrow_open_stream_args={\\'compression_type\\': \\'gzip\\'}``).\\n        meta_provider: File metadata provider. Custom metadata providers may\\n            be able to resolve file metadata more quickly and/or accurately. If\\n            ``None``, this function uses a system-chosen implementation.\\n        partition_filter: Path-based partition filter, if any. Can be used\\n            with a custom callback to read only selected partitions of a dataset.\\n        decoder: A function or list of functions to decode the data.\\n        fileselect: A callable or list of glob patterns to select files.\\n        filerename: A function or list of tuples to rename files prior to grouping.\\n        suffixes: A function or list of suffixes to select for creating samples.\\n        verbose_open: Whether to print the file names as they are opened.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` that contains the example features.\\n\\n    Raises:\\n        ValueError: If a file contains a message that isn\\'t a `tf.train.Example`_.\\n\\n    .. _tf.train.Example: https://www.tensorflow.org/api_docs/python/tf/train/Example\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(WebDatasetDatasource._FILE_EXTENSIONS)\n    datasource = WebDatasetDatasource(paths, decoder=decoder, fileselect=fileselect, filerename=filerename, suffixes=suffixes, verbose_open=verbose_open, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI(stability='alpha')\ndef read_webdataset(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, decoder: Optional[Union[bool, str, callable, list]]=True, fileselect: Optional[Union[list, callable]]=None, filerename: Optional[Union[list, callable]]=None, suffixes: Optional[Union[list, callable]]=None, verbose_open: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from\\n    `WebDataset <https://webdataset.github.io/webdataset/>`_ files.\\n\\n    Args:\\n        paths: A single file/directory path or a list of file/directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The filesystem implementation to read from.\\n        parallelism: The requested parallelism of the read. Parallelism may be\\n            limited by the number of files in the dataset.\\n        arrow_open_stream_args: Key-word arguments passed to\\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\\n            To read a compressed TFRecord file,\\n            pass the corresponding compression type (e.g. for ``GZIP`` or ``ZLIB``, use\\n            ``arrow_open_stream_args={\\'compression_type\\': \\'gzip\\'}``).\\n        meta_provider: File metadata provider. Custom metadata providers may\\n            be able to resolve file metadata more quickly and/or accurately. If\\n            ``None``, this function uses a system-chosen implementation.\\n        partition_filter: Path-based partition filter, if any. Can be used\\n            with a custom callback to read only selected partitions of a dataset.\\n        decoder: A function or list of functions to decode the data.\\n        fileselect: A callable or list of glob patterns to select files.\\n        filerename: A function or list of tuples to rename files prior to grouping.\\n        suffixes: A function or list of suffixes to select for creating samples.\\n        verbose_open: Whether to print the file names as they are opened.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` that contains the example features.\\n\\n    Raises:\\n        ValueError: If a file contains a message that isn\\'t a `tf.train.Example`_.\\n\\n    .. _tf.train.Example: https://www.tensorflow.org/api_docs/python/tf/train/Example\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(WebDatasetDatasource._FILE_EXTENSIONS)\n    datasource = WebDatasetDatasource(paths, decoder=decoder, fileselect=fileselect, filerename=filerename, suffixes=suffixes, verbose_open=verbose_open, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI(stability='alpha')\ndef read_webdataset(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, decoder: Optional[Union[bool, str, callable, list]]=True, fileselect: Optional[Union[list, callable]]=None, filerename: Optional[Union[list, callable]]=None, suffixes: Optional[Union[list, callable]]=None, verbose_open: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from\\n    `WebDataset <https://webdataset.github.io/webdataset/>`_ files.\\n\\n    Args:\\n        paths: A single file/directory path or a list of file/directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The filesystem implementation to read from.\\n        parallelism: The requested parallelism of the read. Parallelism may be\\n            limited by the number of files in the dataset.\\n        arrow_open_stream_args: Key-word arguments passed to\\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\\n            To read a compressed TFRecord file,\\n            pass the corresponding compression type (e.g. for ``GZIP`` or ``ZLIB``, use\\n            ``arrow_open_stream_args={\\'compression_type\\': \\'gzip\\'}``).\\n        meta_provider: File metadata provider. Custom metadata providers may\\n            be able to resolve file metadata more quickly and/or accurately. If\\n            ``None``, this function uses a system-chosen implementation.\\n        partition_filter: Path-based partition filter, if any. Can be used\\n            with a custom callback to read only selected partitions of a dataset.\\n        decoder: A function or list of functions to decode the data.\\n        fileselect: A callable or list of glob patterns to select files.\\n        filerename: A function or list of tuples to rename files prior to grouping.\\n        suffixes: A function or list of suffixes to select for creating samples.\\n        verbose_open: Whether to print the file names as they are opened.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` that contains the example features.\\n\\n    Raises:\\n        ValueError: If a file contains a message that isn\\'t a `tf.train.Example`_.\\n\\n    .. _tf.train.Example: https://www.tensorflow.org/api_docs/python/tf/train/Example\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(WebDatasetDatasource._FILE_EXTENSIONS)\n    datasource = WebDatasetDatasource(paths, decoder=decoder, fileselect=fileselect, filerename=filerename, suffixes=suffixes, verbose_open=verbose_open, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI(stability='alpha')\ndef read_webdataset(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, decoder: Optional[Union[bool, str, callable, list]]=True, fileselect: Optional[Union[list, callable]]=None, filerename: Optional[Union[list, callable]]=None, suffixes: Optional[Union[list, callable]]=None, verbose_open: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from\\n    `WebDataset <https://webdataset.github.io/webdataset/>`_ files.\\n\\n    Args:\\n        paths: A single file/directory path or a list of file/directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The filesystem implementation to read from.\\n        parallelism: The requested parallelism of the read. Parallelism may be\\n            limited by the number of files in the dataset.\\n        arrow_open_stream_args: Key-word arguments passed to\\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\\n            To read a compressed TFRecord file,\\n            pass the corresponding compression type (e.g. for ``GZIP`` or ``ZLIB``, use\\n            ``arrow_open_stream_args={\\'compression_type\\': \\'gzip\\'}``).\\n        meta_provider: File metadata provider. Custom metadata providers may\\n            be able to resolve file metadata more quickly and/or accurately. If\\n            ``None``, this function uses a system-chosen implementation.\\n        partition_filter: Path-based partition filter, if any. Can be used\\n            with a custom callback to read only selected partitions of a dataset.\\n        decoder: A function or list of functions to decode the data.\\n        fileselect: A callable or list of glob patterns to select files.\\n        filerename: A function or list of tuples to rename files prior to grouping.\\n        suffixes: A function or list of suffixes to select for creating samples.\\n        verbose_open: Whether to print the file names as they are opened.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` that contains the example features.\\n\\n    Raises:\\n        ValueError: If a file contains a message that isn\\'t a `tf.train.Example`_.\\n\\n    .. _tf.train.Example: https://www.tensorflow.org/api_docs/python/tf/train/Example\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(WebDatasetDatasource._FILE_EXTENSIONS)\n    datasource = WebDatasetDatasource(paths, decoder=decoder, fileselect=fileselect, filerename=filerename, suffixes=suffixes, verbose_open=verbose_open, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)",
            "@PublicAPI(stability='alpha')\ndef read_webdataset(paths: Union[str, List[str]], *, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, decoder: Optional[Union[bool, str, callable, list]]=True, fileselect: Optional[Union[list, callable]]=None, filerename: Optional[Union[list, callable]]=None, suffixes: Optional[Union[list, callable]]=None, verbose_open: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from\\n    `WebDataset <https://webdataset.github.io/webdataset/>`_ files.\\n\\n    Args:\\n        paths: A single file/directory path or a list of file/directory paths.\\n            A list of paths can contain both files and directories.\\n        filesystem: The filesystem implementation to read from.\\n        parallelism: The requested parallelism of the read. Parallelism may be\\n            limited by the number of files in the dataset.\\n        arrow_open_stream_args: Key-word arguments passed to\\n            `pyarrow.fs.FileSystem.open_input_stream <https://arrow.apache.org/docs/python/generated/pyarrow.fs.FileSystem.html>`_.\\n            To read a compressed TFRecord file,\\n            pass the corresponding compression type (e.g. for ``GZIP`` or ``ZLIB``, use\\n            ``arrow_open_stream_args={\\'compression_type\\': \\'gzip\\'}``).\\n        meta_provider: File metadata provider. Custom metadata providers may\\n            be able to resolve file metadata more quickly and/or accurately. If\\n            ``None``, this function uses a system-chosen implementation.\\n        partition_filter: Path-based partition filter, if any. Can be used\\n            with a custom callback to read only selected partitions of a dataset.\\n        decoder: A function or list of functions to decode the data.\\n        fileselect: A callable or list of glob patterns to select files.\\n        filerename: A function or list of tuples to rename files prior to grouping.\\n        suffixes: A function or list of suffixes to select for creating samples.\\n        verbose_open: Whether to print the file names as they are opened.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` that contains the example features.\\n\\n    Raises:\\n        ValueError: If a file contains a message that isn\\'t a `tf.train.Example`_.\\n\\n    .. _tf.train.Example: https://www.tensorflow.org/api_docs/python/tf/train/Example\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(WebDatasetDatasource._FILE_EXTENSIONS)\n    datasource = WebDatasetDatasource(paths, decoder=decoder, fileselect=fileselect, filerename=filerename, suffixes=suffixes, verbose_open=verbose_open, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism)"
        ]
    },
    {
        "func_name": "read_binary_files",
        "original": "@PublicAPI\ndef read_binary_files(paths: Union[str, List[str]], *, include_paths: bool=False, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from binary files of arbitrary contents.\n\n    Examples:\n        Read a file in remote storage.\n\n        >>> import ray\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\n        >>> ds = ray.data.read_binary_files(path)\n        >>> ds.schema()\n        Column  Type\n        ------  ----\n        bytes   binary\n\n        Read multiple local files.\n\n        >>> ray.data.read_binary_files( # doctest: +SKIP\n        ...     [\"local:///path/to/file1\", \"local:///path/to/file2\"])\n\n        Read a file with the filepaths included as a column in the dataset.\n\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\n        >>> ds = ray.data.read_binary_files(path, include_paths=True)\n        >>> ds.take(1)[0][\"path\"]\n        'ray-example-data/pdf-sample_0.pdf'\n\n\n    Args:\n        paths: A single file or directory, or a list of file or directory paths.\n            A list of paths can contain both files and directories.\n        include_paths: If ``True``, include the path to each file. File paths are\n            stored in the ``'path'`` column.\n        filesystem: The PyArrow filesystem\n            implementation to read from. These filesystems are specified in the\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\n            you need to provide specific configurations to the filesystem. By default,\n            the filesystem is automatically selected based on the scheme of the paths.\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see :ref:`Tuning read parallelism\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\n            files.\n        arrow_open_stream_args: kwargs passed to\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\n            metadata providers may be able to resolve file metadata more quickly and/or\n            accurately. In most cases, you do not need to set this. If ``None``, this\n            function uses a system-chosen implementation.\n        partition_filter: A\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\n            Use with a custom callback to read only selected partitions of a\n            dataset. By default, no files are filtered.\n            By default, this does not filter out any files.\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\n            that describes how paths are organized. Defaults to ``None``.\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\n            found. Defaults to False.\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\n            Defaults to not shuffle with ``None``.\n        file_extensions: A list of file extensions to filter files by.\n\n    Returns:\n        :class:`~ray.data.Dataset` producing rows read from the specified paths.\n    \"\"\"\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(BinaryDatasource._FILE_EXTENSIONS)\n    datasource = BinaryDatasource(paths, include_paths=include_paths, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI\ndef read_binary_files(paths: Union[str, List[str]], *, include_paths: bool=False, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from binary files of arbitrary contents.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\\n        >>> ds = ray.data.read_binary_files(path)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        bytes   binary\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_binary_files( # doctest: +SKIP\\n        ...     [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read a file with the filepaths included as a column in the dataset.\\n\\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\\n        >>> ds = ray.data.read_binary_files(path, include_paths=True)\\n        >>> ds.take(1)[0][\"path\"]\\n        \\'ray-example-data/pdf-sample_0.pdf\\'\\n\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        include_paths: If ``True``, include the path to each file. File paths are\\n            stored in the ``\\'path\\'`` column.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            files.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n            By default, this does not filter out any files.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing rows read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(BinaryDatasource._FILE_EXTENSIONS)\n    datasource = BinaryDatasource(paths, include_paths=include_paths, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_binary_files(paths: Union[str, List[str]], *, include_paths: bool=False, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from binary files of arbitrary contents.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\\n        >>> ds = ray.data.read_binary_files(path)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        bytes   binary\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_binary_files( # doctest: +SKIP\\n        ...     [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read a file with the filepaths included as a column in the dataset.\\n\\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\\n        >>> ds = ray.data.read_binary_files(path, include_paths=True)\\n        >>> ds.take(1)[0][\"path\"]\\n        \\'ray-example-data/pdf-sample_0.pdf\\'\\n\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        include_paths: If ``True``, include the path to each file. File paths are\\n            stored in the ``\\'path\\'`` column.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            files.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n            By default, this does not filter out any files.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing rows read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(BinaryDatasource._FILE_EXTENSIONS)\n    datasource = BinaryDatasource(paths, include_paths=include_paths, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_binary_files(paths: Union[str, List[str]], *, include_paths: bool=False, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from binary files of arbitrary contents.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\\n        >>> ds = ray.data.read_binary_files(path)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        bytes   binary\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_binary_files( # doctest: +SKIP\\n        ...     [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read a file with the filepaths included as a column in the dataset.\\n\\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\\n        >>> ds = ray.data.read_binary_files(path, include_paths=True)\\n        >>> ds.take(1)[0][\"path\"]\\n        \\'ray-example-data/pdf-sample_0.pdf\\'\\n\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        include_paths: If ``True``, include the path to each file. File paths are\\n            stored in the ``\\'path\\'`` column.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            files.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n            By default, this does not filter out any files.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing rows read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(BinaryDatasource._FILE_EXTENSIONS)\n    datasource = BinaryDatasource(paths, include_paths=include_paths, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_binary_files(paths: Union[str, List[str]], *, include_paths: bool=False, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from binary files of arbitrary contents.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\\n        >>> ds = ray.data.read_binary_files(path)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        bytes   binary\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_binary_files( # doctest: +SKIP\\n        ...     [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read a file with the filepaths included as a column in the dataset.\\n\\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\\n        >>> ds = ray.data.read_binary_files(path, include_paths=True)\\n        >>> ds.take(1)[0][\"path\"]\\n        \\'ray-example-data/pdf-sample_0.pdf\\'\\n\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        include_paths: If ``True``, include the path to each file. File paths are\\n            stored in the ``\\'path\\'`` column.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            files.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n            By default, this does not filter out any files.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing rows read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(BinaryDatasource._FILE_EXTENSIONS)\n    datasource = BinaryDatasource(paths, include_paths=include_paths, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef read_binary_files(paths: Union[str, List[str]], *, include_paths: bool=False, filesystem: Optional['pyarrow.fs.FileSystem']=None, parallelism: int=-1, ray_remote_args: Dict[str, Any]=None, arrow_open_stream_args: Optional[Dict[str, Any]]=None, meta_provider: Optional[BaseFileMetadataProvider]=None, partition_filter: Optional[PathPartitionFilter]=None, partitioning: Partitioning=None, ignore_missing_paths: bool=False, shuffle: Union[Literal['files'], None]=None, file_extensions: Optional[List[str]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from binary files of arbitrary contents.\\n\\n    Examples:\\n        Read a file in remote storage.\\n\\n        >>> import ray\\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\\n        >>> ds = ray.data.read_binary_files(path)\\n        >>> ds.schema()\\n        Column  Type\\n        ------  ----\\n        bytes   binary\\n\\n        Read multiple local files.\\n\\n        >>> ray.data.read_binary_files( # doctest: +SKIP\\n        ...     [\"local:///path/to/file1\", \"local:///path/to/file2\"])\\n\\n        Read a file with the filepaths included as a column in the dataset.\\n\\n        >>> path = \"s3://anonymous@ray-example-data/pdf-sample_0.pdf\"\\n        >>> ds = ray.data.read_binary_files(path, include_paths=True)\\n        >>> ds.take(1)[0][\"path\"]\\n        \\'ray-example-data/pdf-sample_0.pdf\\'\\n\\n\\n    Args:\\n        paths: A single file or directory, or a list of file or directory paths.\\n            A list of paths can contain both files and directories.\\n        include_paths: If ``True``, include the path to each file. File paths are\\n            stored in the ``\\'path\\'`` column.\\n        filesystem: The PyArrow filesystem\\n            implementation to read from. These filesystems are specified in the\\n            `PyArrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\\n            you need to provide specific configurations to the filesystem. By default,\\n            the filesystem is automatically selected based on the scheme of the paths.\\n            For example, if the path begins with ``s3://``, the `S3FileSystem` is used.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n        parallelism: The amount of parallelism to use for the dataset. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`. Parallelism is upper bounded by the total number of\\n            files.\\n        arrow_open_stream_args: kwargs passed to\\n            `pyarrow.fs.FileSystem.open_input_file <https://arrow.apache.org/docs/                python/generated/pyarrow.fs.FileSystem.html                    #pyarrow.fs.FileSystem.open_input_stream>`_.\\n        meta_provider: A :ref:`file metadata provider <metadata_provider>`. Custom\\n            metadata providers may be able to resolve file metadata more quickly and/or\\n            accurately. In most cases, you do not need to set this. If ``None``, this\\n            function uses a system-chosen implementation.\\n        partition_filter: A\\n            :class:`~ray.data.datasource.partitioning.PathPartitionFilter`.\\n            Use with a custom callback to read only selected partitions of a\\n            dataset. By default, no files are filtered.\\n            By default, this does not filter out any files.\\n        partitioning: A :class:`~ray.data.datasource.partitioning.Partitioning` object\\n            that describes how paths are organized. Defaults to ``None``.\\n        ignore_missing_paths: If True, ignores any file paths in ``paths`` that are not\\n            found. Defaults to False.\\n        shuffle: If setting to \"files\", randomly shuffle input files order before read.\\n            Defaults to not shuffle with ``None``.\\n        file_extensions: A list of file extensions to filter files by.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` producing rows read from the specified paths.\\n    '\n    if meta_provider is None:\n        meta_provider = get_generic_metadata_provider(BinaryDatasource._FILE_EXTENSIONS)\n    datasource = BinaryDatasource(paths, include_paths=include_paths, filesystem=filesystem, open_stream_args=arrow_open_stream_args, meta_provider=meta_provider, partition_filter=partition_filter, partitioning=partitioning, ignore_missing_paths=ignore_missing_paths, shuffle=shuffle, file_extensions=file_extensions)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "read_sql",
        "original": "@PublicAPI(stability='alpha')\ndef read_sql(sql: str, connection_factory: Callable[[], Connection], *, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    '''Read from a database that provides a\n    `Python DB API2-compliant <https://peps.python.org/pep-0249/>`_ connector.\n\n    .. note::\n\n        By default, ``read_sql`` launches multiple read tasks, and each task executes a\n        ``LIMIT`` and ``OFFSET`` to fetch a subset of the rows. However, for many\n        databases, ``OFFSET`` is slow.\n\n        As a workaround, set ``parallelism=1`` to directly fetch all rows in a single\n        task. Note that this approach requires all result rows to fit in the memory of\n        single task. If the rows don't fit, your program may raise an out of memory\n        error.\n\n    Examples:\n\n        For examples of reading from larger databases like MySQL and PostgreSQL, see\n        :ref:`Reading from SQL Databases <reading_sql>`.\n\n        .. testcode::\n\n            import sqlite3\n\n            import ray\n\n            # Create a simple database\n            connection = sqlite3.connect(\"example.db\")\n            connection.execute(\"CREATE TABLE movie(title, year, score)\")\n            connection.execute(\n                \"\"\"\n                INSERT INTO movie VALUES\n                    ('Monty Python and the Holy Grail', 1975, 8.2),\n                    (\"Monty Python Live at the Hollywood Bowl\", 1982, 7.9),\n                    (\"Monty Python's Life of Brian\", 1979, 8.0),\n                    (\"Rocky II\", 1979, 7.3)\n                \"\"\"\n            )\n            connection.commit()\n            connection.close()\n\n            def create_connection():\n                return sqlite3.connect(\"example.db\")\n\n            # Get all movies\n            ds = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\n            # Get movies after the year 1980\n            ds = ray.data.read_sql(\n                \"SELECT title, score FROM movie WHERE year >= 1980\", create_connection\n            )\n            # Get the number of movies per year\n            ds = ray.data.read_sql(\n                \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\n            )\n\n        .. testcode::\n            :hide:\n\n            import os\n            os.remove(\"example.db\")\n\n    Args:\n        sql: The SQL query to execute.\n        connection_factory: A function that takes no arguments and returns a\n            Python DB API2\n            `Connection object <https://peps.python.org/pep-0249/#connection-objects>`_.\n        parallelism: The requested parallelism of the read. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see :ref:`Tuning read parallelism\n            <read_parallelism>`.\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\n\n    Returns:\n        A :class:`Dataset` containing the queried data.\n    '''\n    datasource = SQLDatasource(sql=sql, connection_factory=connection_factory)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI(stability='alpha')\ndef read_sql(sql: str, connection_factory: Callable[[], Connection], *, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    if False:\n        i = 10\n    'Read from a database that provides a\\n    `Python DB API2-compliant <https://peps.python.org/pep-0249/>`_ connector.\\n\\n    .. note::\\n\\n        By default, ``read_sql`` launches multiple read tasks, and each task executes a\\n        ``LIMIT`` and ``OFFSET`` to fetch a subset of the rows. However, for many\\n        databases, ``OFFSET`` is slow.\\n\\n        As a workaround, set ``parallelism=1`` to directly fetch all rows in a single\\n        task. Note that this approach requires all result rows to fit in the memory of\\n        single task. If the rows don\\'t fit, your program may raise an out of memory\\n        error.\\n\\n    Examples:\\n\\n        For examples of reading from larger databases like MySQL and PostgreSQL, see\\n        :ref:`Reading from SQL Databases <reading_sql>`.\\n\\n        .. testcode::\\n\\n            import sqlite3\\n\\n            import ray\\n\\n            # Create a simple database\\n            connection = sqlite3.connect(\"example.db\")\\n            connection.execute(\"CREATE TABLE movie(title, year, score)\")\\n            connection.execute(\\n                \"\"\"\\n                INSERT INTO movie VALUES\\n                    (\\'Monty Python and the Holy Grail\\', 1975, 8.2),\\n                    (\"Monty Python Live at the Hollywood Bowl\", 1982, 7.9),\\n                    (\"Monty Python\\'s Life of Brian\", 1979, 8.0),\\n                    (\"Rocky II\", 1979, 7.3)\\n                \"\"\"\\n            )\\n            connection.commit()\\n            connection.close()\\n\\n            def create_connection():\\n                return sqlite3.connect(\"example.db\")\\n\\n            # Get all movies\\n            ds = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\\n            # Get movies after the year 1980\\n            ds = ray.data.read_sql(\\n                \"SELECT title, score FROM movie WHERE year >= 1980\", create_connection\\n            )\\n            # Get the number of movies per year\\n            ds = ray.data.read_sql(\\n                \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\\n            )\\n\\n        .. testcode::\\n            :hide:\\n\\n            import os\\n            os.remove(\"example.db\")\\n\\n    Args:\\n        sql: The SQL query to execute.\\n        connection_factory: A function that takes no arguments and returns a\\n            Python DB API2\\n            `Connection object <https://peps.python.org/pep-0249/#connection-objects>`_.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n\\n    Returns:\\n        A :class:`Dataset` containing the queried data.\\n    '\n    datasource = SQLDatasource(sql=sql, connection_factory=connection_factory)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_sql(sql: str, connection_factory: Callable[[], Connection], *, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Read from a database that provides a\\n    `Python DB API2-compliant <https://peps.python.org/pep-0249/>`_ connector.\\n\\n    .. note::\\n\\n        By default, ``read_sql`` launches multiple read tasks, and each task executes a\\n        ``LIMIT`` and ``OFFSET`` to fetch a subset of the rows. However, for many\\n        databases, ``OFFSET`` is slow.\\n\\n        As a workaround, set ``parallelism=1`` to directly fetch all rows in a single\\n        task. Note that this approach requires all result rows to fit in the memory of\\n        single task. If the rows don\\'t fit, your program may raise an out of memory\\n        error.\\n\\n    Examples:\\n\\n        For examples of reading from larger databases like MySQL and PostgreSQL, see\\n        :ref:`Reading from SQL Databases <reading_sql>`.\\n\\n        .. testcode::\\n\\n            import sqlite3\\n\\n            import ray\\n\\n            # Create a simple database\\n            connection = sqlite3.connect(\"example.db\")\\n            connection.execute(\"CREATE TABLE movie(title, year, score)\")\\n            connection.execute(\\n                \"\"\"\\n                INSERT INTO movie VALUES\\n                    (\\'Monty Python and the Holy Grail\\', 1975, 8.2),\\n                    (\"Monty Python Live at the Hollywood Bowl\", 1982, 7.9),\\n                    (\"Monty Python\\'s Life of Brian\", 1979, 8.0),\\n                    (\"Rocky II\", 1979, 7.3)\\n                \"\"\"\\n            )\\n            connection.commit()\\n            connection.close()\\n\\n            def create_connection():\\n                return sqlite3.connect(\"example.db\")\\n\\n            # Get all movies\\n            ds = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\\n            # Get movies after the year 1980\\n            ds = ray.data.read_sql(\\n                \"SELECT title, score FROM movie WHERE year >= 1980\", create_connection\\n            )\\n            # Get the number of movies per year\\n            ds = ray.data.read_sql(\\n                \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\\n            )\\n\\n        .. testcode::\\n            :hide:\\n\\n            import os\\n            os.remove(\"example.db\")\\n\\n    Args:\\n        sql: The SQL query to execute.\\n        connection_factory: A function that takes no arguments and returns a\\n            Python DB API2\\n            `Connection object <https://peps.python.org/pep-0249/#connection-objects>`_.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n\\n    Returns:\\n        A :class:`Dataset` containing the queried data.\\n    '\n    datasource = SQLDatasource(sql=sql, connection_factory=connection_factory)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_sql(sql: str, connection_factory: Callable[[], Connection], *, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Read from a database that provides a\\n    `Python DB API2-compliant <https://peps.python.org/pep-0249/>`_ connector.\\n\\n    .. note::\\n\\n        By default, ``read_sql`` launches multiple read tasks, and each task executes a\\n        ``LIMIT`` and ``OFFSET`` to fetch a subset of the rows. However, for many\\n        databases, ``OFFSET`` is slow.\\n\\n        As a workaround, set ``parallelism=1`` to directly fetch all rows in a single\\n        task. Note that this approach requires all result rows to fit in the memory of\\n        single task. If the rows don\\'t fit, your program may raise an out of memory\\n        error.\\n\\n    Examples:\\n\\n        For examples of reading from larger databases like MySQL and PostgreSQL, see\\n        :ref:`Reading from SQL Databases <reading_sql>`.\\n\\n        .. testcode::\\n\\n            import sqlite3\\n\\n            import ray\\n\\n            # Create a simple database\\n            connection = sqlite3.connect(\"example.db\")\\n            connection.execute(\"CREATE TABLE movie(title, year, score)\")\\n            connection.execute(\\n                \"\"\"\\n                INSERT INTO movie VALUES\\n                    (\\'Monty Python and the Holy Grail\\', 1975, 8.2),\\n                    (\"Monty Python Live at the Hollywood Bowl\", 1982, 7.9),\\n                    (\"Monty Python\\'s Life of Brian\", 1979, 8.0),\\n                    (\"Rocky II\", 1979, 7.3)\\n                \"\"\"\\n            )\\n            connection.commit()\\n            connection.close()\\n\\n            def create_connection():\\n                return sqlite3.connect(\"example.db\")\\n\\n            # Get all movies\\n            ds = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\\n            # Get movies after the year 1980\\n            ds = ray.data.read_sql(\\n                \"SELECT title, score FROM movie WHERE year >= 1980\", create_connection\\n            )\\n            # Get the number of movies per year\\n            ds = ray.data.read_sql(\\n                \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\\n            )\\n\\n        .. testcode::\\n            :hide:\\n\\n            import os\\n            os.remove(\"example.db\")\\n\\n    Args:\\n        sql: The SQL query to execute.\\n        connection_factory: A function that takes no arguments and returns a\\n            Python DB API2\\n            `Connection object <https://peps.python.org/pep-0249/#connection-objects>`_.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n\\n    Returns:\\n        A :class:`Dataset` containing the queried data.\\n    '\n    datasource = SQLDatasource(sql=sql, connection_factory=connection_factory)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_sql(sql: str, connection_factory: Callable[[], Connection], *, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Read from a database that provides a\\n    `Python DB API2-compliant <https://peps.python.org/pep-0249/>`_ connector.\\n\\n    .. note::\\n\\n        By default, ``read_sql`` launches multiple read tasks, and each task executes a\\n        ``LIMIT`` and ``OFFSET`` to fetch a subset of the rows. However, for many\\n        databases, ``OFFSET`` is slow.\\n\\n        As a workaround, set ``parallelism=1`` to directly fetch all rows in a single\\n        task. Note that this approach requires all result rows to fit in the memory of\\n        single task. If the rows don\\'t fit, your program may raise an out of memory\\n        error.\\n\\n    Examples:\\n\\n        For examples of reading from larger databases like MySQL and PostgreSQL, see\\n        :ref:`Reading from SQL Databases <reading_sql>`.\\n\\n        .. testcode::\\n\\n            import sqlite3\\n\\n            import ray\\n\\n            # Create a simple database\\n            connection = sqlite3.connect(\"example.db\")\\n            connection.execute(\"CREATE TABLE movie(title, year, score)\")\\n            connection.execute(\\n                \"\"\"\\n                INSERT INTO movie VALUES\\n                    (\\'Monty Python and the Holy Grail\\', 1975, 8.2),\\n                    (\"Monty Python Live at the Hollywood Bowl\", 1982, 7.9),\\n                    (\"Monty Python\\'s Life of Brian\", 1979, 8.0),\\n                    (\"Rocky II\", 1979, 7.3)\\n                \"\"\"\\n            )\\n            connection.commit()\\n            connection.close()\\n\\n            def create_connection():\\n                return sqlite3.connect(\"example.db\")\\n\\n            # Get all movies\\n            ds = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\\n            # Get movies after the year 1980\\n            ds = ray.data.read_sql(\\n                \"SELECT title, score FROM movie WHERE year >= 1980\", create_connection\\n            )\\n            # Get the number of movies per year\\n            ds = ray.data.read_sql(\\n                \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\\n            )\\n\\n        .. testcode::\\n            :hide:\\n\\n            import os\\n            os.remove(\"example.db\")\\n\\n    Args:\\n        sql: The SQL query to execute.\\n        connection_factory: A function that takes no arguments and returns a\\n            Python DB API2\\n            `Connection object <https://peps.python.org/pep-0249/#connection-objects>`_.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n\\n    Returns:\\n        A :class:`Dataset` containing the queried data.\\n    '\n    datasource = SQLDatasource(sql=sql, connection_factory=connection_factory)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_sql(sql: str, connection_factory: Callable[[], Connection], *, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Read from a database that provides a\\n    `Python DB API2-compliant <https://peps.python.org/pep-0249/>`_ connector.\\n\\n    .. note::\\n\\n        By default, ``read_sql`` launches multiple read tasks, and each task executes a\\n        ``LIMIT`` and ``OFFSET`` to fetch a subset of the rows. However, for many\\n        databases, ``OFFSET`` is slow.\\n\\n        As a workaround, set ``parallelism=1`` to directly fetch all rows in a single\\n        task. Note that this approach requires all result rows to fit in the memory of\\n        single task. If the rows don\\'t fit, your program may raise an out of memory\\n        error.\\n\\n    Examples:\\n\\n        For examples of reading from larger databases like MySQL and PostgreSQL, see\\n        :ref:`Reading from SQL Databases <reading_sql>`.\\n\\n        .. testcode::\\n\\n            import sqlite3\\n\\n            import ray\\n\\n            # Create a simple database\\n            connection = sqlite3.connect(\"example.db\")\\n            connection.execute(\"CREATE TABLE movie(title, year, score)\")\\n            connection.execute(\\n                \"\"\"\\n                INSERT INTO movie VALUES\\n                    (\\'Monty Python and the Holy Grail\\', 1975, 8.2),\\n                    (\"Monty Python Live at the Hollywood Bowl\", 1982, 7.9),\\n                    (\"Monty Python\\'s Life of Brian\", 1979, 8.0),\\n                    (\"Rocky II\", 1979, 7.3)\\n                \"\"\"\\n            )\\n            connection.commit()\\n            connection.close()\\n\\n            def create_connection():\\n                return sqlite3.connect(\"example.db\")\\n\\n            # Get all movies\\n            ds = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\\n            # Get movies after the year 1980\\n            ds = ray.data.read_sql(\\n                \"SELECT title, score FROM movie WHERE year >= 1980\", create_connection\\n            )\\n            # Get the number of movies per year\\n            ds = ray.data.read_sql(\\n                \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\\n            )\\n\\n        .. testcode::\\n            :hide:\\n\\n            import os\\n            os.remove(\"example.db\")\\n\\n    Args:\\n        sql: The SQL query to execute.\\n        connection_factory: A function that takes no arguments and returns a\\n            Python DB API2\\n            `Connection object <https://peps.python.org/pep-0249/#connection-objects>`_.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n\\n    Returns:\\n        A :class:`Dataset` containing the queried data.\\n    '\n    datasource = SQLDatasource(sql=sql, connection_factory=connection_factory)\n    return read_datasource(datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "read_databricks_tables",
        "original": "@PublicAPI(stability='alpha')\ndef read_databricks_tables(*, warehouse_id: str, table: Optional[str]=None, query: Optional[str]=None, catalog: Optional[str]=None, schema: Optional[str]=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    \"\"\"Read a Databricks unity catalog table or Databricks SQL execution result.\n\n    Before calling this API, set the ``DATABRICKS_HOST`` environment\n    variable to your Databricks warehouse access token.\n\n    .. code-block:: console\n\n        export DATABRICKS_TOKEN=...\n\n    If you're running your program on the Databricks runtime, also set the\n    ``DATABRICKS_HOST`` environment variable.\n\n    .. code-block:: console\n\n        export DATABRICKS_HOST=adb-<workspace-id>.<random-number>.azuredatabricks.net\n\n    .. note::\n\n        This function is built on the\n        `Databricks statement execution API <https://docs.databricks.com/api/workspace/statementexecution>`_.\n\n    Examples:\n\n        .. testcode::\n            :skipif: True\n\n            import ray\n\n            ds = ray.data.read_databricks_tables(\n                warehouse_id='a885ad08b64951ad',\n                catalog='catalog_1',\n                schema='db_1',\n                query='select id from table_1 limit 750000',\n            )\n\n    Args:\n        warehouse_id: The ID of the Databricks warehouse. The query statement is\n            executed on this warehouse.\n        table: The name of UC table you want to read. If this argument is set,\n            you can't set ``query`` argument, and the reader generates query\n            of ``select * from {table_name}`` under the hood.\n        query: The query you want to execute. If this argument is set,\n            you can't set ``table_name`` argument.\n        catalog: (Optional) The default catalog name used by the query.\n        schema: (Optional) The default schema used by the query.\n        parallelism: The requested parallelism of the read. Defaults to -1,\n            which automatically determines the optimal parallelism for your\n            configuration. You should not need to manually set this value in most cases.\n            For details on how the parallelism is automatically determined and guidance\n            on how to tune it, see :ref:`Tuning read parallelism\n            <read_parallelism>`.\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\n\n    Returns:\n        A :class:`Dataset` containing the queried data.\n    \"\"\"\n    from ray.data.datasource.databricks_uc_datasource import DatabricksUCDatasource\n    from ray.util.spark.databricks_hook import get_dbutils\n    from ray.util.spark.utils import get_spark_session, is_in_databricks_runtime\n    token = os.environ.get('DATABRICKS_TOKEN')\n    if not token:\n        raise ValueError(\"Please set environment variable 'DATABRICKS_TOKEN' to databricks workspace access token.\")\n    host = os.environ.get('DATABRICKS_HOST')\n    if not host:\n        if is_in_databricks_runtime():\n            ctx = get_dbutils().notebook.entry_point.getDbutils().notebook().getContext()\n            host = ctx.tags().get('browserHostName').get()\n        else:\n            raise ValueError('You are not in databricks runtime, please set environment variable \\'DATABRICKS_HOST\\' to databricks workspace URL(e.g. \"adb-<workspace-id>.<random-number>.azuredatabricks.net\").')\n    spark = get_spark_session()\n    if not catalog:\n        catalog = spark.sql('SELECT CURRENT_CATALOG()').collect()[0][0]\n    if not schema:\n        schema = spark.sql('SELECT CURRENT_DATABASE()').collect()[0][0]\n    if query is not None and table is not None:\n        raise ValueError(\"Only one of 'query' and 'table' arguments can be set.\")\n    if table:\n        query = f'select * from {table}'\n    if query is None:\n        raise ValueError(\"One of 'query' and 'table_name' arguments should be set.\")\n    datasource = DatabricksUCDatasource(host=host, token=token, warehouse_id=warehouse_id, catalog=catalog, schema=schema, query=query)\n    return read_datasource(datasource=datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI(stability='alpha')\ndef read_databricks_tables(*, warehouse_id: str, table: Optional[str]=None, query: Optional[str]=None, catalog: Optional[str]=None, schema: Optional[str]=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    if False:\n        i = 10\n    \"Read a Databricks unity catalog table or Databricks SQL execution result.\\n\\n    Before calling this API, set the ``DATABRICKS_HOST`` environment\\n    variable to your Databricks warehouse access token.\\n\\n    .. code-block:: console\\n\\n        export DATABRICKS_TOKEN=...\\n\\n    If you're running your program on the Databricks runtime, also set the\\n    ``DATABRICKS_HOST`` environment variable.\\n\\n    .. code-block:: console\\n\\n        export DATABRICKS_HOST=adb-<workspace-id>.<random-number>.azuredatabricks.net\\n\\n    .. note::\\n\\n        This function is built on the\\n        `Databricks statement execution API <https://docs.databricks.com/api/workspace/statementexecution>`_.\\n\\n    Examples:\\n\\n        .. testcode::\\n            :skipif: True\\n\\n            import ray\\n\\n            ds = ray.data.read_databricks_tables(\\n                warehouse_id='a885ad08b64951ad',\\n                catalog='catalog_1',\\n                schema='db_1',\\n                query='select id from table_1 limit 750000',\\n            )\\n\\n    Args:\\n        warehouse_id: The ID of the Databricks warehouse. The query statement is\\n            executed on this warehouse.\\n        table: The name of UC table you want to read. If this argument is set,\\n            you can't set ``query`` argument, and the reader generates query\\n            of ``select * from {table_name}`` under the hood.\\n        query: The query you want to execute. If this argument is set,\\n            you can't set ``table_name`` argument.\\n        catalog: (Optional) The default catalog name used by the query.\\n        schema: (Optional) The default schema used by the query.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n\\n    Returns:\\n        A :class:`Dataset` containing the queried data.\\n    \"\n    from ray.data.datasource.databricks_uc_datasource import DatabricksUCDatasource\n    from ray.util.spark.databricks_hook import get_dbutils\n    from ray.util.spark.utils import get_spark_session, is_in_databricks_runtime\n    token = os.environ.get('DATABRICKS_TOKEN')\n    if not token:\n        raise ValueError(\"Please set environment variable 'DATABRICKS_TOKEN' to databricks workspace access token.\")\n    host = os.environ.get('DATABRICKS_HOST')\n    if not host:\n        if is_in_databricks_runtime():\n            ctx = get_dbutils().notebook.entry_point.getDbutils().notebook().getContext()\n            host = ctx.tags().get('browserHostName').get()\n        else:\n            raise ValueError('You are not in databricks runtime, please set environment variable \\'DATABRICKS_HOST\\' to databricks workspace URL(e.g. \"adb-<workspace-id>.<random-number>.azuredatabricks.net\").')\n    spark = get_spark_session()\n    if not catalog:\n        catalog = spark.sql('SELECT CURRENT_CATALOG()').collect()[0][0]\n    if not schema:\n        schema = spark.sql('SELECT CURRENT_DATABASE()').collect()[0][0]\n    if query is not None and table is not None:\n        raise ValueError(\"Only one of 'query' and 'table' arguments can be set.\")\n    if table:\n        query = f'select * from {table}'\n    if query is None:\n        raise ValueError(\"One of 'query' and 'table_name' arguments should be set.\")\n    datasource = DatabricksUCDatasource(host=host, token=token, warehouse_id=warehouse_id, catalog=catalog, schema=schema, query=query)\n    return read_datasource(datasource=datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_databricks_tables(*, warehouse_id: str, table: Optional[str]=None, query: Optional[str]=None, catalog: Optional[str]=None, schema: Optional[str]=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Read a Databricks unity catalog table or Databricks SQL execution result.\\n\\n    Before calling this API, set the ``DATABRICKS_HOST`` environment\\n    variable to your Databricks warehouse access token.\\n\\n    .. code-block:: console\\n\\n        export DATABRICKS_TOKEN=...\\n\\n    If you're running your program on the Databricks runtime, also set the\\n    ``DATABRICKS_HOST`` environment variable.\\n\\n    .. code-block:: console\\n\\n        export DATABRICKS_HOST=adb-<workspace-id>.<random-number>.azuredatabricks.net\\n\\n    .. note::\\n\\n        This function is built on the\\n        `Databricks statement execution API <https://docs.databricks.com/api/workspace/statementexecution>`_.\\n\\n    Examples:\\n\\n        .. testcode::\\n            :skipif: True\\n\\n            import ray\\n\\n            ds = ray.data.read_databricks_tables(\\n                warehouse_id='a885ad08b64951ad',\\n                catalog='catalog_1',\\n                schema='db_1',\\n                query='select id from table_1 limit 750000',\\n            )\\n\\n    Args:\\n        warehouse_id: The ID of the Databricks warehouse. The query statement is\\n            executed on this warehouse.\\n        table: The name of UC table you want to read. If this argument is set,\\n            you can't set ``query`` argument, and the reader generates query\\n            of ``select * from {table_name}`` under the hood.\\n        query: The query you want to execute. If this argument is set,\\n            you can't set ``table_name`` argument.\\n        catalog: (Optional) The default catalog name used by the query.\\n        schema: (Optional) The default schema used by the query.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n\\n    Returns:\\n        A :class:`Dataset` containing the queried data.\\n    \"\n    from ray.data.datasource.databricks_uc_datasource import DatabricksUCDatasource\n    from ray.util.spark.databricks_hook import get_dbutils\n    from ray.util.spark.utils import get_spark_session, is_in_databricks_runtime\n    token = os.environ.get('DATABRICKS_TOKEN')\n    if not token:\n        raise ValueError(\"Please set environment variable 'DATABRICKS_TOKEN' to databricks workspace access token.\")\n    host = os.environ.get('DATABRICKS_HOST')\n    if not host:\n        if is_in_databricks_runtime():\n            ctx = get_dbutils().notebook.entry_point.getDbutils().notebook().getContext()\n            host = ctx.tags().get('browserHostName').get()\n        else:\n            raise ValueError('You are not in databricks runtime, please set environment variable \\'DATABRICKS_HOST\\' to databricks workspace URL(e.g. \"adb-<workspace-id>.<random-number>.azuredatabricks.net\").')\n    spark = get_spark_session()\n    if not catalog:\n        catalog = spark.sql('SELECT CURRENT_CATALOG()').collect()[0][0]\n    if not schema:\n        schema = spark.sql('SELECT CURRENT_DATABASE()').collect()[0][0]\n    if query is not None and table is not None:\n        raise ValueError(\"Only one of 'query' and 'table' arguments can be set.\")\n    if table:\n        query = f'select * from {table}'\n    if query is None:\n        raise ValueError(\"One of 'query' and 'table_name' arguments should be set.\")\n    datasource = DatabricksUCDatasource(host=host, token=token, warehouse_id=warehouse_id, catalog=catalog, schema=schema, query=query)\n    return read_datasource(datasource=datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_databricks_tables(*, warehouse_id: str, table: Optional[str]=None, query: Optional[str]=None, catalog: Optional[str]=None, schema: Optional[str]=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Read a Databricks unity catalog table or Databricks SQL execution result.\\n\\n    Before calling this API, set the ``DATABRICKS_HOST`` environment\\n    variable to your Databricks warehouse access token.\\n\\n    .. code-block:: console\\n\\n        export DATABRICKS_TOKEN=...\\n\\n    If you're running your program on the Databricks runtime, also set the\\n    ``DATABRICKS_HOST`` environment variable.\\n\\n    .. code-block:: console\\n\\n        export DATABRICKS_HOST=adb-<workspace-id>.<random-number>.azuredatabricks.net\\n\\n    .. note::\\n\\n        This function is built on the\\n        `Databricks statement execution API <https://docs.databricks.com/api/workspace/statementexecution>`_.\\n\\n    Examples:\\n\\n        .. testcode::\\n            :skipif: True\\n\\n            import ray\\n\\n            ds = ray.data.read_databricks_tables(\\n                warehouse_id='a885ad08b64951ad',\\n                catalog='catalog_1',\\n                schema='db_1',\\n                query='select id from table_1 limit 750000',\\n            )\\n\\n    Args:\\n        warehouse_id: The ID of the Databricks warehouse. The query statement is\\n            executed on this warehouse.\\n        table: The name of UC table you want to read. If this argument is set,\\n            you can't set ``query`` argument, and the reader generates query\\n            of ``select * from {table_name}`` under the hood.\\n        query: The query you want to execute. If this argument is set,\\n            you can't set ``table_name`` argument.\\n        catalog: (Optional) The default catalog name used by the query.\\n        schema: (Optional) The default schema used by the query.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n\\n    Returns:\\n        A :class:`Dataset` containing the queried data.\\n    \"\n    from ray.data.datasource.databricks_uc_datasource import DatabricksUCDatasource\n    from ray.util.spark.databricks_hook import get_dbutils\n    from ray.util.spark.utils import get_spark_session, is_in_databricks_runtime\n    token = os.environ.get('DATABRICKS_TOKEN')\n    if not token:\n        raise ValueError(\"Please set environment variable 'DATABRICKS_TOKEN' to databricks workspace access token.\")\n    host = os.environ.get('DATABRICKS_HOST')\n    if not host:\n        if is_in_databricks_runtime():\n            ctx = get_dbutils().notebook.entry_point.getDbutils().notebook().getContext()\n            host = ctx.tags().get('browserHostName').get()\n        else:\n            raise ValueError('You are not in databricks runtime, please set environment variable \\'DATABRICKS_HOST\\' to databricks workspace URL(e.g. \"adb-<workspace-id>.<random-number>.azuredatabricks.net\").')\n    spark = get_spark_session()\n    if not catalog:\n        catalog = spark.sql('SELECT CURRENT_CATALOG()').collect()[0][0]\n    if not schema:\n        schema = spark.sql('SELECT CURRENT_DATABASE()').collect()[0][0]\n    if query is not None and table is not None:\n        raise ValueError(\"Only one of 'query' and 'table' arguments can be set.\")\n    if table:\n        query = f'select * from {table}'\n    if query is None:\n        raise ValueError(\"One of 'query' and 'table_name' arguments should be set.\")\n    datasource = DatabricksUCDatasource(host=host, token=token, warehouse_id=warehouse_id, catalog=catalog, schema=schema, query=query)\n    return read_datasource(datasource=datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_databricks_tables(*, warehouse_id: str, table: Optional[str]=None, query: Optional[str]=None, catalog: Optional[str]=None, schema: Optional[str]=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Read a Databricks unity catalog table or Databricks SQL execution result.\\n\\n    Before calling this API, set the ``DATABRICKS_HOST`` environment\\n    variable to your Databricks warehouse access token.\\n\\n    .. code-block:: console\\n\\n        export DATABRICKS_TOKEN=...\\n\\n    If you're running your program on the Databricks runtime, also set the\\n    ``DATABRICKS_HOST`` environment variable.\\n\\n    .. code-block:: console\\n\\n        export DATABRICKS_HOST=adb-<workspace-id>.<random-number>.azuredatabricks.net\\n\\n    .. note::\\n\\n        This function is built on the\\n        `Databricks statement execution API <https://docs.databricks.com/api/workspace/statementexecution>`_.\\n\\n    Examples:\\n\\n        .. testcode::\\n            :skipif: True\\n\\n            import ray\\n\\n            ds = ray.data.read_databricks_tables(\\n                warehouse_id='a885ad08b64951ad',\\n                catalog='catalog_1',\\n                schema='db_1',\\n                query='select id from table_1 limit 750000',\\n            )\\n\\n    Args:\\n        warehouse_id: The ID of the Databricks warehouse. The query statement is\\n            executed on this warehouse.\\n        table: The name of UC table you want to read. If this argument is set,\\n            you can't set ``query`` argument, and the reader generates query\\n            of ``select * from {table_name}`` under the hood.\\n        query: The query you want to execute. If this argument is set,\\n            you can't set ``table_name`` argument.\\n        catalog: (Optional) The default catalog name used by the query.\\n        schema: (Optional) The default schema used by the query.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n\\n    Returns:\\n        A :class:`Dataset` containing the queried data.\\n    \"\n    from ray.data.datasource.databricks_uc_datasource import DatabricksUCDatasource\n    from ray.util.spark.databricks_hook import get_dbutils\n    from ray.util.spark.utils import get_spark_session, is_in_databricks_runtime\n    token = os.environ.get('DATABRICKS_TOKEN')\n    if not token:\n        raise ValueError(\"Please set environment variable 'DATABRICKS_TOKEN' to databricks workspace access token.\")\n    host = os.environ.get('DATABRICKS_HOST')\n    if not host:\n        if is_in_databricks_runtime():\n            ctx = get_dbutils().notebook.entry_point.getDbutils().notebook().getContext()\n            host = ctx.tags().get('browserHostName').get()\n        else:\n            raise ValueError('You are not in databricks runtime, please set environment variable \\'DATABRICKS_HOST\\' to databricks workspace URL(e.g. \"adb-<workspace-id>.<random-number>.azuredatabricks.net\").')\n    spark = get_spark_session()\n    if not catalog:\n        catalog = spark.sql('SELECT CURRENT_CATALOG()').collect()[0][0]\n    if not schema:\n        schema = spark.sql('SELECT CURRENT_DATABASE()').collect()[0][0]\n    if query is not None and table is not None:\n        raise ValueError(\"Only one of 'query' and 'table' arguments can be set.\")\n    if table:\n        query = f'select * from {table}'\n    if query is None:\n        raise ValueError(\"One of 'query' and 'table_name' arguments should be set.\")\n    datasource = DatabricksUCDatasource(host=host, token=token, warehouse_id=warehouse_id, catalog=catalog, schema=schema, query=query)\n    return read_datasource(datasource=datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)",
            "@PublicAPI(stability='alpha')\ndef read_databricks_tables(*, warehouse_id: str, table: Optional[str]=None, query: Optional[str]=None, catalog: Optional[str]=None, schema: Optional[str]=None, parallelism: int=-1, ray_remote_args: Optional[Dict[str, Any]]=None) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Read a Databricks unity catalog table or Databricks SQL execution result.\\n\\n    Before calling this API, set the ``DATABRICKS_HOST`` environment\\n    variable to your Databricks warehouse access token.\\n\\n    .. code-block:: console\\n\\n        export DATABRICKS_TOKEN=...\\n\\n    If you're running your program on the Databricks runtime, also set the\\n    ``DATABRICKS_HOST`` environment variable.\\n\\n    .. code-block:: console\\n\\n        export DATABRICKS_HOST=adb-<workspace-id>.<random-number>.azuredatabricks.net\\n\\n    .. note::\\n\\n        This function is built on the\\n        `Databricks statement execution API <https://docs.databricks.com/api/workspace/statementexecution>`_.\\n\\n    Examples:\\n\\n        .. testcode::\\n            :skipif: True\\n\\n            import ray\\n\\n            ds = ray.data.read_databricks_tables(\\n                warehouse_id='a885ad08b64951ad',\\n                catalog='catalog_1',\\n                schema='db_1',\\n                query='select id from table_1 limit 750000',\\n            )\\n\\n    Args:\\n        warehouse_id: The ID of the Databricks warehouse. The query statement is\\n            executed on this warehouse.\\n        table: The name of UC table you want to read. If this argument is set,\\n            you can't set ``query`` argument, and the reader generates query\\n            of ``select * from {table_name}`` under the hood.\\n        query: The query you want to execute. If this argument is set,\\n            you can't set ``table_name`` argument.\\n        catalog: (Optional) The default catalog name used by the query.\\n        schema: (Optional) The default schema used by the query.\\n        parallelism: The requested parallelism of the read. Defaults to -1,\\n            which automatically determines the optimal parallelism for your\\n            configuration. You should not need to manually set this value in most cases.\\n            For details on how the parallelism is automatically determined and guidance\\n            on how to tune it, see :ref:`Tuning read parallelism\\n            <read_parallelism>`.\\n        ray_remote_args: kwargs passed to :meth:`~ray.remote` in the read tasks.\\n\\n    Returns:\\n        A :class:`Dataset` containing the queried data.\\n    \"\n    from ray.data.datasource.databricks_uc_datasource import DatabricksUCDatasource\n    from ray.util.spark.databricks_hook import get_dbutils\n    from ray.util.spark.utils import get_spark_session, is_in_databricks_runtime\n    token = os.environ.get('DATABRICKS_TOKEN')\n    if not token:\n        raise ValueError(\"Please set environment variable 'DATABRICKS_TOKEN' to databricks workspace access token.\")\n    host = os.environ.get('DATABRICKS_HOST')\n    if not host:\n        if is_in_databricks_runtime():\n            ctx = get_dbutils().notebook.entry_point.getDbutils().notebook().getContext()\n            host = ctx.tags().get('browserHostName').get()\n        else:\n            raise ValueError('You are not in databricks runtime, please set environment variable \\'DATABRICKS_HOST\\' to databricks workspace URL(e.g. \"adb-<workspace-id>.<random-number>.azuredatabricks.net\").')\n    spark = get_spark_session()\n    if not catalog:\n        catalog = spark.sql('SELECT CURRENT_CATALOG()').collect()[0][0]\n    if not schema:\n        schema = spark.sql('SELECT CURRENT_DATABASE()').collect()[0][0]\n    if query is not None and table is not None:\n        raise ValueError(\"Only one of 'query' and 'table' arguments can be set.\")\n    if table:\n        query = f'select * from {table}'\n    if query is None:\n        raise ValueError(\"One of 'query' and 'table_name' arguments should be set.\")\n    datasource = DatabricksUCDatasource(host=host, token=token, warehouse_id=warehouse_id, catalog=catalog, schema=schema, query=query)\n    return read_datasource(datasource=datasource, parallelism=parallelism, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "to_ref",
        "original": "def to_ref(df):\n    if isinstance(df, pandas.DataFrame):\n        return ray.put(df)\n    elif isinstance(df, ray.ObjectRef):\n        return df\n    else:\n        raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')",
        "mutated": [
            "def to_ref(df):\n    if False:\n        i = 10\n    if isinstance(df, pandas.DataFrame):\n        return ray.put(df)\n    elif isinstance(df, ray.ObjectRef):\n        return df\n    else:\n        raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')",
            "def to_ref(df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if isinstance(df, pandas.DataFrame):\n        return ray.put(df)\n    elif isinstance(df, ray.ObjectRef):\n        return df\n    else:\n        raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')",
            "def to_ref(df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if isinstance(df, pandas.DataFrame):\n        return ray.put(df)\n    elif isinstance(df, ray.ObjectRef):\n        return df\n    else:\n        raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')",
            "def to_ref(df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if isinstance(df, pandas.DataFrame):\n        return ray.put(df)\n    elif isinstance(df, ray.ObjectRef):\n        return df\n    else:\n        raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')",
            "def to_ref(df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if isinstance(df, pandas.DataFrame):\n        return ray.put(df)\n    elif isinstance(df, ray.ObjectRef):\n        return df\n    else:\n        raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')"
        ]
    },
    {
        "func_name": "from_dask",
        "original": "@PublicAPI\ndef from_dask(df: 'dask.dataframe.DataFrame') -> MaterializedDataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a\n    `Dask DataFrame <https://docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.html#dask.dataframe.DataFrame>`_.\n\n    Args:\n        df: A `Dask DataFrame`_.\n\n    Returns:\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\n    \"\"\"\n    import dask\n    from ray.util.dask import ray_dask_get\n    partitions = df.to_delayed()\n    persisted_partitions = dask.persist(*partitions, scheduler=ray_dask_get)\n    import pandas\n\n    def to_ref(df):\n        if isinstance(df, pandas.DataFrame):\n            return ray.put(df)\n        elif isinstance(df, ray.ObjectRef):\n            return df\n        else:\n            raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')\n    ds = from_pandas_refs([to_ref(next(iter(part.dask.values()))) for part in persisted_partitions])\n    return ds",
        "mutated": [
            "@PublicAPI\ndef from_dask(df: 'dask.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Dask DataFrame <https://docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.html#dask.dataframe.DataFrame>`_.\\n\\n    Args:\\n        df: A `Dask DataFrame`_.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import dask\n    from ray.util.dask import ray_dask_get\n    partitions = df.to_delayed()\n    persisted_partitions = dask.persist(*partitions, scheduler=ray_dask_get)\n    import pandas\n\n    def to_ref(df):\n        if isinstance(df, pandas.DataFrame):\n            return ray.put(df)\n        elif isinstance(df, ray.ObjectRef):\n            return df\n        else:\n            raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')\n    ds = from_pandas_refs([to_ref(next(iter(part.dask.values()))) for part in persisted_partitions])\n    return ds",
            "@PublicAPI\ndef from_dask(df: 'dask.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Dask DataFrame <https://docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.html#dask.dataframe.DataFrame>`_.\\n\\n    Args:\\n        df: A `Dask DataFrame`_.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import dask\n    from ray.util.dask import ray_dask_get\n    partitions = df.to_delayed()\n    persisted_partitions = dask.persist(*partitions, scheduler=ray_dask_get)\n    import pandas\n\n    def to_ref(df):\n        if isinstance(df, pandas.DataFrame):\n            return ray.put(df)\n        elif isinstance(df, ray.ObjectRef):\n            return df\n        else:\n            raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')\n    ds = from_pandas_refs([to_ref(next(iter(part.dask.values()))) for part in persisted_partitions])\n    return ds",
            "@PublicAPI\ndef from_dask(df: 'dask.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Dask DataFrame <https://docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.html#dask.dataframe.DataFrame>`_.\\n\\n    Args:\\n        df: A `Dask DataFrame`_.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import dask\n    from ray.util.dask import ray_dask_get\n    partitions = df.to_delayed()\n    persisted_partitions = dask.persist(*partitions, scheduler=ray_dask_get)\n    import pandas\n\n    def to_ref(df):\n        if isinstance(df, pandas.DataFrame):\n            return ray.put(df)\n        elif isinstance(df, ray.ObjectRef):\n            return df\n        else:\n            raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')\n    ds = from_pandas_refs([to_ref(next(iter(part.dask.values()))) for part in persisted_partitions])\n    return ds",
            "@PublicAPI\ndef from_dask(df: 'dask.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Dask DataFrame <https://docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.html#dask.dataframe.DataFrame>`_.\\n\\n    Args:\\n        df: A `Dask DataFrame`_.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import dask\n    from ray.util.dask import ray_dask_get\n    partitions = df.to_delayed()\n    persisted_partitions = dask.persist(*partitions, scheduler=ray_dask_get)\n    import pandas\n\n    def to_ref(df):\n        if isinstance(df, pandas.DataFrame):\n            return ray.put(df)\n        elif isinstance(df, ray.ObjectRef):\n            return df\n        else:\n            raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')\n    ds = from_pandas_refs([to_ref(next(iter(part.dask.values()))) for part in persisted_partitions])\n    return ds",
            "@PublicAPI\ndef from_dask(df: 'dask.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Dask DataFrame <https://docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.html#dask.dataframe.DataFrame>`_.\\n\\n    Args:\\n        df: A `Dask DataFrame`_.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import dask\n    from ray.util.dask import ray_dask_get\n    partitions = df.to_delayed()\n    persisted_partitions = dask.persist(*partitions, scheduler=ray_dask_get)\n    import pandas\n\n    def to_ref(df):\n        if isinstance(df, pandas.DataFrame):\n            return ray.put(df)\n        elif isinstance(df, ray.ObjectRef):\n            return df\n        else:\n            raise ValueError(f'Expected a Ray object ref or a Pandas DataFrame, got {type(df)}')\n    ds = from_pandas_refs([to_ref(next(iter(part.dask.values()))) for part in persisted_partitions])\n    return ds"
        ]
    },
    {
        "func_name": "from_mars",
        "original": "@PublicAPI\ndef from_mars(df: 'mars.dataframe.DataFrame') -> MaterializedDataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a\n    `Mars DataFrame <https://mars-project.readthedocs.io/en/latest/reference/dataframe/index.html>`_.\n\n    Args:\n        df: A `Mars DataFrame`_, which must be executed by Mars-on-Ray.\n\n    Returns:\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\n    \"\"\"\n    import mars.dataframe as md\n    ds: Dataset = md.to_ray_dataset(df)\n    return ds",
        "mutated": [
            "@PublicAPI\ndef from_mars(df: 'mars.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Mars DataFrame <https://mars-project.readthedocs.io/en/latest/reference/dataframe/index.html>`_.\\n\\n    Args:\\n        df: A `Mars DataFrame`_, which must be executed by Mars-on-Ray.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import mars.dataframe as md\n    ds: Dataset = md.to_ray_dataset(df)\n    return ds",
            "@PublicAPI\ndef from_mars(df: 'mars.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Mars DataFrame <https://mars-project.readthedocs.io/en/latest/reference/dataframe/index.html>`_.\\n\\n    Args:\\n        df: A `Mars DataFrame`_, which must be executed by Mars-on-Ray.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import mars.dataframe as md\n    ds: Dataset = md.to_ray_dataset(df)\n    return ds",
            "@PublicAPI\ndef from_mars(df: 'mars.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Mars DataFrame <https://mars-project.readthedocs.io/en/latest/reference/dataframe/index.html>`_.\\n\\n    Args:\\n        df: A `Mars DataFrame`_, which must be executed by Mars-on-Ray.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import mars.dataframe as md\n    ds: Dataset = md.to_ray_dataset(df)\n    return ds",
            "@PublicAPI\ndef from_mars(df: 'mars.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Mars DataFrame <https://mars-project.readthedocs.io/en/latest/reference/dataframe/index.html>`_.\\n\\n    Args:\\n        df: A `Mars DataFrame`_, which must be executed by Mars-on-Ray.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import mars.dataframe as md\n    ds: Dataset = md.to_ray_dataset(df)\n    return ds",
            "@PublicAPI\ndef from_mars(df: 'mars.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Mars DataFrame <https://mars-project.readthedocs.io/en/latest/reference/dataframe/index.html>`_.\\n\\n    Args:\\n        df: A `Mars DataFrame`_, which must be executed by Mars-on-Ray.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import mars.dataframe as md\n    ds: Dataset = md.to_ray_dataset(df)\n    return ds"
        ]
    },
    {
        "func_name": "from_modin",
        "original": "@PublicAPI\ndef from_modin(df: 'modin.pandas.dataframe.DataFrame') -> MaterializedDataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a\n    `Modin DataFrame <https://modin.readthedocs.io/en/stable/flow/modin/pandas/dataframe.html>`_.\n\n    Args:\n        df: A `Modin DataFrame`_, which must be using the Ray backend.\n\n    Returns:\n        A :class:`~ray.data.MaterializedDataset` rows read from the DataFrame.\n    \"\"\"\n    from modin.distributed.dataframe.pandas.partitions import unwrap_partitions\n    parts = unwrap_partitions(df, axis=0)\n    ds = from_pandas_refs(parts)\n    return ds",
        "mutated": [
            "@PublicAPI\ndef from_modin(df: 'modin.pandas.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Modin DataFrame <https://modin.readthedocs.io/en/stable/flow/modin/pandas/dataframe.html>`_.\\n\\n    Args:\\n        df: A `Modin DataFrame`_, which must be using the Ray backend.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` rows read from the DataFrame.\\n    '\n    from modin.distributed.dataframe.pandas.partitions import unwrap_partitions\n    parts = unwrap_partitions(df, axis=0)\n    ds = from_pandas_refs(parts)\n    return ds",
            "@PublicAPI\ndef from_modin(df: 'modin.pandas.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Modin DataFrame <https://modin.readthedocs.io/en/stable/flow/modin/pandas/dataframe.html>`_.\\n\\n    Args:\\n        df: A `Modin DataFrame`_, which must be using the Ray backend.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` rows read from the DataFrame.\\n    '\n    from modin.distributed.dataframe.pandas.partitions import unwrap_partitions\n    parts = unwrap_partitions(df, axis=0)\n    ds = from_pandas_refs(parts)\n    return ds",
            "@PublicAPI\ndef from_modin(df: 'modin.pandas.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Modin DataFrame <https://modin.readthedocs.io/en/stable/flow/modin/pandas/dataframe.html>`_.\\n\\n    Args:\\n        df: A `Modin DataFrame`_, which must be using the Ray backend.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` rows read from the DataFrame.\\n    '\n    from modin.distributed.dataframe.pandas.partitions import unwrap_partitions\n    parts = unwrap_partitions(df, axis=0)\n    ds = from_pandas_refs(parts)\n    return ds",
            "@PublicAPI\ndef from_modin(df: 'modin.pandas.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Modin DataFrame <https://modin.readthedocs.io/en/stable/flow/modin/pandas/dataframe.html>`_.\\n\\n    Args:\\n        df: A `Modin DataFrame`_, which must be using the Ray backend.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` rows read from the DataFrame.\\n    '\n    from modin.distributed.dataframe.pandas.partitions import unwrap_partitions\n    parts = unwrap_partitions(df, axis=0)\n    ds = from_pandas_refs(parts)\n    return ds",
            "@PublicAPI\ndef from_modin(df: 'modin.pandas.dataframe.DataFrame') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Modin DataFrame <https://modin.readthedocs.io/en/stable/flow/modin/pandas/dataframe.html>`_.\\n\\n    Args:\\n        df: A `Modin DataFrame`_, which must be using the Ray backend.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` rows read from the DataFrame.\\n    '\n    from modin.distributed.dataframe.pandas.partitions import unwrap_partitions\n    parts = unwrap_partitions(df, axis=0)\n    ds = from_pandas_refs(parts)\n    return ds"
        ]
    },
    {
        "func_name": "from_pandas",
        "original": "@PublicAPI\ndef from_pandas(dfs: Union['pandas.DataFrame', List['pandas.DataFrame']]) -> MaterializedDataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a list of pandas dataframes.\n\n    Examples:\n        >>> import pandas as pd\n        >>> import ray\n        >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n        >>> ray.data.from_pandas(df)\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\n\n       Create a Ray Dataset from a list of Pandas DataFrames.\n\n        >>> ray.data.from_pandas([df, df])\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\n\n    Args:\n        dfs: A pandas dataframe or a list of pandas dataframes.\n\n    Returns:\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\n    \"\"\"\n    import pandas as pd\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    from ray.air.util.data_batch_conversion import _cast_ndarray_columns_to_tensor_extension\n    context = DataContext.get_current()\n    if context.enable_tensor_extension_casting:\n        dfs = [_cast_ndarray_columns_to_tensor_extension(df.copy()) for df in dfs]\n    return from_pandas_refs([ray.put(df) for df in dfs])",
        "mutated": [
            "@PublicAPI\ndef from_pandas(dfs: Union['pandas.DataFrame', List['pandas.DataFrame']]) -> MaterializedDataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a list of pandas dataframes.\\n\\n    Examples:\\n        >>> import pandas as pd\\n        >>> import ray\\n        >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\\n        >>> ray.data.from_pandas(df)\\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\\n\\n       Create a Ray Dataset from a list of Pandas DataFrames.\\n\\n        >>> ray.data.from_pandas([df, df])\\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\\n\\n    Args:\\n        dfs: A pandas dataframe or a list of pandas dataframes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\\n    '\n    import pandas as pd\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    from ray.air.util.data_batch_conversion import _cast_ndarray_columns_to_tensor_extension\n    context = DataContext.get_current()\n    if context.enable_tensor_extension_casting:\n        dfs = [_cast_ndarray_columns_to_tensor_extension(df.copy()) for df in dfs]\n    return from_pandas_refs([ray.put(df) for df in dfs])",
            "@PublicAPI\ndef from_pandas(dfs: Union['pandas.DataFrame', List['pandas.DataFrame']]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a list of pandas dataframes.\\n\\n    Examples:\\n        >>> import pandas as pd\\n        >>> import ray\\n        >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\\n        >>> ray.data.from_pandas(df)\\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\\n\\n       Create a Ray Dataset from a list of Pandas DataFrames.\\n\\n        >>> ray.data.from_pandas([df, df])\\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\\n\\n    Args:\\n        dfs: A pandas dataframe or a list of pandas dataframes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\\n    '\n    import pandas as pd\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    from ray.air.util.data_batch_conversion import _cast_ndarray_columns_to_tensor_extension\n    context = DataContext.get_current()\n    if context.enable_tensor_extension_casting:\n        dfs = [_cast_ndarray_columns_to_tensor_extension(df.copy()) for df in dfs]\n    return from_pandas_refs([ray.put(df) for df in dfs])",
            "@PublicAPI\ndef from_pandas(dfs: Union['pandas.DataFrame', List['pandas.DataFrame']]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a list of pandas dataframes.\\n\\n    Examples:\\n        >>> import pandas as pd\\n        >>> import ray\\n        >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\\n        >>> ray.data.from_pandas(df)\\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\\n\\n       Create a Ray Dataset from a list of Pandas DataFrames.\\n\\n        >>> ray.data.from_pandas([df, df])\\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\\n\\n    Args:\\n        dfs: A pandas dataframe or a list of pandas dataframes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\\n    '\n    import pandas as pd\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    from ray.air.util.data_batch_conversion import _cast_ndarray_columns_to_tensor_extension\n    context = DataContext.get_current()\n    if context.enable_tensor_extension_casting:\n        dfs = [_cast_ndarray_columns_to_tensor_extension(df.copy()) for df in dfs]\n    return from_pandas_refs([ray.put(df) for df in dfs])",
            "@PublicAPI\ndef from_pandas(dfs: Union['pandas.DataFrame', List['pandas.DataFrame']]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a list of pandas dataframes.\\n\\n    Examples:\\n        >>> import pandas as pd\\n        >>> import ray\\n        >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\\n        >>> ray.data.from_pandas(df)\\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\\n\\n       Create a Ray Dataset from a list of Pandas DataFrames.\\n\\n        >>> ray.data.from_pandas([df, df])\\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\\n\\n    Args:\\n        dfs: A pandas dataframe or a list of pandas dataframes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\\n    '\n    import pandas as pd\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    from ray.air.util.data_batch_conversion import _cast_ndarray_columns_to_tensor_extension\n    context = DataContext.get_current()\n    if context.enable_tensor_extension_casting:\n        dfs = [_cast_ndarray_columns_to_tensor_extension(df.copy()) for df in dfs]\n    return from_pandas_refs([ray.put(df) for df in dfs])",
            "@PublicAPI\ndef from_pandas(dfs: Union['pandas.DataFrame', List['pandas.DataFrame']]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a list of pandas dataframes.\\n\\n    Examples:\\n        >>> import pandas as pd\\n        >>> import ray\\n        >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\\n        >>> ray.data.from_pandas(df)\\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\\n\\n       Create a Ray Dataset from a list of Pandas DataFrames.\\n\\n        >>> ray.data.from_pandas([df, df])\\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\\n\\n    Args:\\n        dfs: A pandas dataframe or a list of pandas dataframes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\\n    '\n    import pandas as pd\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    from ray.air.util.data_batch_conversion import _cast_ndarray_columns_to_tensor_extension\n    context = DataContext.get_current()\n    if context.enable_tensor_extension_casting:\n        dfs = [_cast_ndarray_columns_to_tensor_extension(df.copy()) for df in dfs]\n    return from_pandas_refs([ray.put(df) for df in dfs])"
        ]
    },
    {
        "func_name": "from_pandas_refs",
        "original": "@DeveloperAPI\ndef from_pandas_refs(dfs: Union[ObjectRef['pandas.DataFrame'], List[ObjectRef['pandas.DataFrame']]]) -> MaterializedDataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a list of Ray object references to\n    pandas dataframes.\n\n    Examples:\n        >>> import pandas as pd\n        >>> import ray\n        >>> df_ref = ray.put(pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}))\n        >>> ray.data.from_pandas_refs(df_ref)\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\n\n        Create a Ray Dataset from a list of Pandas Dataframes references.\n\n        >>> ray.data.from_pandas_refs([df_ref, df_ref])\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\n\n    Args:\n        dfs: A Ray object reference to a pandas dataframe, or a list of\n             Ray object references to pandas dataframes.\n\n    Returns:\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\n    \"\"\"\n    if isinstance(dfs, ray.ObjectRef):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        for df in dfs:\n            if not isinstance(df, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(df)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(df)}')\n    context = DataContext.get_current()\n    if context.enable_pandas_block:\n        get_metadata = cached_remote_fn(get_table_block_metadata)\n        metadata = ray.get([get_metadata.remote(df) for df in dfs])\n        logical_plan = LogicalPlan(FromPandas(dfs, metadata))\n        return MaterializedDataset(ExecutionPlan(BlockList(dfs, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)\n    df_to_block = cached_remote_fn(pandas_df_to_arrow_block, num_returns=2)\n    res = [df_to_block.remote(df) for df in dfs]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromPandas(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)",
        "mutated": [
            "@DeveloperAPI\ndef from_pandas_refs(dfs: Union[ObjectRef['pandas.DataFrame'], List[ObjectRef['pandas.DataFrame']]]) -> MaterializedDataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    pandas dataframes.\\n\\n    Examples:\\n        >>> import pandas as pd\\n        >>> import ray\\n        >>> df_ref = ray.put(pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}))\\n        >>> ray.data.from_pandas_refs(df_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\\n\\n        Create a Ray Dataset from a list of Pandas Dataframes references.\\n\\n        >>> ray.data.from_pandas_refs([df_ref, df_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\\n\\n    Args:\\n        dfs: A Ray object reference to a pandas dataframe, or a list of\\n             Ray object references to pandas dataframes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\\n    '\n    if isinstance(dfs, ray.ObjectRef):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        for df in dfs:\n            if not isinstance(df, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(df)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(df)}')\n    context = DataContext.get_current()\n    if context.enable_pandas_block:\n        get_metadata = cached_remote_fn(get_table_block_metadata)\n        metadata = ray.get([get_metadata.remote(df) for df in dfs])\n        logical_plan = LogicalPlan(FromPandas(dfs, metadata))\n        return MaterializedDataset(ExecutionPlan(BlockList(dfs, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)\n    df_to_block = cached_remote_fn(pandas_df_to_arrow_block, num_returns=2)\n    res = [df_to_block.remote(df) for df in dfs]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromPandas(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_pandas_refs(dfs: Union[ObjectRef['pandas.DataFrame'], List[ObjectRef['pandas.DataFrame']]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    pandas dataframes.\\n\\n    Examples:\\n        >>> import pandas as pd\\n        >>> import ray\\n        >>> df_ref = ray.put(pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}))\\n        >>> ray.data.from_pandas_refs(df_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\\n\\n        Create a Ray Dataset from a list of Pandas Dataframes references.\\n\\n        >>> ray.data.from_pandas_refs([df_ref, df_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\\n\\n    Args:\\n        dfs: A Ray object reference to a pandas dataframe, or a list of\\n             Ray object references to pandas dataframes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\\n    '\n    if isinstance(dfs, ray.ObjectRef):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        for df in dfs:\n            if not isinstance(df, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(df)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(df)}')\n    context = DataContext.get_current()\n    if context.enable_pandas_block:\n        get_metadata = cached_remote_fn(get_table_block_metadata)\n        metadata = ray.get([get_metadata.remote(df) for df in dfs])\n        logical_plan = LogicalPlan(FromPandas(dfs, metadata))\n        return MaterializedDataset(ExecutionPlan(BlockList(dfs, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)\n    df_to_block = cached_remote_fn(pandas_df_to_arrow_block, num_returns=2)\n    res = [df_to_block.remote(df) for df in dfs]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromPandas(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_pandas_refs(dfs: Union[ObjectRef['pandas.DataFrame'], List[ObjectRef['pandas.DataFrame']]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    pandas dataframes.\\n\\n    Examples:\\n        >>> import pandas as pd\\n        >>> import ray\\n        >>> df_ref = ray.put(pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}))\\n        >>> ray.data.from_pandas_refs(df_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\\n\\n        Create a Ray Dataset from a list of Pandas Dataframes references.\\n\\n        >>> ray.data.from_pandas_refs([df_ref, df_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\\n\\n    Args:\\n        dfs: A Ray object reference to a pandas dataframe, or a list of\\n             Ray object references to pandas dataframes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\\n    '\n    if isinstance(dfs, ray.ObjectRef):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        for df in dfs:\n            if not isinstance(df, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(df)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(df)}')\n    context = DataContext.get_current()\n    if context.enable_pandas_block:\n        get_metadata = cached_remote_fn(get_table_block_metadata)\n        metadata = ray.get([get_metadata.remote(df) for df in dfs])\n        logical_plan = LogicalPlan(FromPandas(dfs, metadata))\n        return MaterializedDataset(ExecutionPlan(BlockList(dfs, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)\n    df_to_block = cached_remote_fn(pandas_df_to_arrow_block, num_returns=2)\n    res = [df_to_block.remote(df) for df in dfs]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromPandas(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_pandas_refs(dfs: Union[ObjectRef['pandas.DataFrame'], List[ObjectRef['pandas.DataFrame']]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    pandas dataframes.\\n\\n    Examples:\\n        >>> import pandas as pd\\n        >>> import ray\\n        >>> df_ref = ray.put(pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}))\\n        >>> ray.data.from_pandas_refs(df_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\\n\\n        Create a Ray Dataset from a list of Pandas Dataframes references.\\n\\n        >>> ray.data.from_pandas_refs([df_ref, df_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\\n\\n    Args:\\n        dfs: A Ray object reference to a pandas dataframe, or a list of\\n             Ray object references to pandas dataframes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\\n    '\n    if isinstance(dfs, ray.ObjectRef):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        for df in dfs:\n            if not isinstance(df, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(df)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(df)}')\n    context = DataContext.get_current()\n    if context.enable_pandas_block:\n        get_metadata = cached_remote_fn(get_table_block_metadata)\n        metadata = ray.get([get_metadata.remote(df) for df in dfs])\n        logical_plan = LogicalPlan(FromPandas(dfs, metadata))\n        return MaterializedDataset(ExecutionPlan(BlockList(dfs, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)\n    df_to_block = cached_remote_fn(pandas_df_to_arrow_block, num_returns=2)\n    res = [df_to_block.remote(df) for df in dfs]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromPandas(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_pandas_refs(dfs: Union[ObjectRef['pandas.DataFrame'], List[ObjectRef['pandas.DataFrame']]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    pandas dataframes.\\n\\n    Examples:\\n        >>> import pandas as pd\\n        >>> import ray\\n        >>> df_ref = ray.put(pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}))\\n        >>> ray.data.from_pandas_refs(df_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=3, schema={a: int64, b: int64})\\n\\n        Create a Ray Dataset from a list of Pandas Dataframes references.\\n\\n        >>> ray.data.from_pandas_refs([df_ref, df_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=6, schema={a: int64, b: int64})\\n\\n    Args:\\n        dfs: A Ray object reference to a pandas dataframe, or a list of\\n             Ray object references to pandas dataframes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data read from the dataframes.\\n    '\n    if isinstance(dfs, ray.ObjectRef):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        for df in dfs:\n            if not isinstance(df, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(df)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(df)}')\n    context = DataContext.get_current()\n    if context.enable_pandas_block:\n        get_metadata = cached_remote_fn(get_table_block_metadata)\n        metadata = ray.get([get_metadata.remote(df) for df in dfs])\n        logical_plan = LogicalPlan(FromPandas(dfs, metadata))\n        return MaterializedDataset(ExecutionPlan(BlockList(dfs, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)\n    df_to_block = cached_remote_fn(pandas_df_to_arrow_block, num_returns=2)\n    res = [df_to_block.remote(df) for df in dfs]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromPandas(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromPandas': metadata}, parent=None), run_by_consumer=False), logical_plan)"
        ]
    },
    {
        "func_name": "from_numpy",
        "original": "@PublicAPI\ndef from_numpy(ndarrays: Union[np.ndarray, List[np.ndarray]]) -> MaterializedDataset:\n    \"\"\"Creates a :class:`~ray.data.Dataset` from a list of NumPy ndarrays.\n\n    Examples:\n        >>> import numpy as np\n        >>> import ray\n        >>> arr = np.array([1])\n        >>> ray.data.from_numpy(arr)\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\n\n        Create a Ray Dataset from a list of NumPy arrays.\n\n        >>> ray.data.from_numpy([arr, arr])\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\n\n    Args:\n        ndarrays: A NumPy ndarray or a list of NumPy ndarrays.\n\n    Returns:\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\n    \"\"\"\n    if isinstance(ndarrays, np.ndarray):\n        ndarrays = [ndarrays]\n    return from_numpy_refs([ray.put(ndarray) for ndarray in ndarrays])",
        "mutated": [
            "@PublicAPI\ndef from_numpy(ndarrays: Union[np.ndarray, List[np.ndarray]]) -> MaterializedDataset:\n    if False:\n        i = 10\n    'Creates a :class:`~ray.data.Dataset` from a list of NumPy ndarrays.\\n\\n    Examples:\\n        >>> import numpy as np\\n        >>> import ray\\n        >>> arr = np.array([1])\\n        >>> ray.data.from_numpy(arr)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\\n\\n        Create a Ray Dataset from a list of NumPy arrays.\\n\\n        >>> ray.data.from_numpy([arr, arr])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\\n\\n    Args:\\n        ndarrays: A NumPy ndarray or a list of NumPy ndarrays.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\\n    '\n    if isinstance(ndarrays, np.ndarray):\n        ndarrays = [ndarrays]\n    return from_numpy_refs([ray.put(ndarray) for ndarray in ndarrays])",
            "@PublicAPI\ndef from_numpy(ndarrays: Union[np.ndarray, List[np.ndarray]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a :class:`~ray.data.Dataset` from a list of NumPy ndarrays.\\n\\n    Examples:\\n        >>> import numpy as np\\n        >>> import ray\\n        >>> arr = np.array([1])\\n        >>> ray.data.from_numpy(arr)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\\n\\n        Create a Ray Dataset from a list of NumPy arrays.\\n\\n        >>> ray.data.from_numpy([arr, arr])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\\n\\n    Args:\\n        ndarrays: A NumPy ndarray or a list of NumPy ndarrays.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\\n    '\n    if isinstance(ndarrays, np.ndarray):\n        ndarrays = [ndarrays]\n    return from_numpy_refs([ray.put(ndarray) for ndarray in ndarrays])",
            "@PublicAPI\ndef from_numpy(ndarrays: Union[np.ndarray, List[np.ndarray]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a :class:`~ray.data.Dataset` from a list of NumPy ndarrays.\\n\\n    Examples:\\n        >>> import numpy as np\\n        >>> import ray\\n        >>> arr = np.array([1])\\n        >>> ray.data.from_numpy(arr)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\\n\\n        Create a Ray Dataset from a list of NumPy arrays.\\n\\n        >>> ray.data.from_numpy([arr, arr])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\\n\\n    Args:\\n        ndarrays: A NumPy ndarray or a list of NumPy ndarrays.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\\n    '\n    if isinstance(ndarrays, np.ndarray):\n        ndarrays = [ndarrays]\n    return from_numpy_refs([ray.put(ndarray) for ndarray in ndarrays])",
            "@PublicAPI\ndef from_numpy(ndarrays: Union[np.ndarray, List[np.ndarray]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a :class:`~ray.data.Dataset` from a list of NumPy ndarrays.\\n\\n    Examples:\\n        >>> import numpy as np\\n        >>> import ray\\n        >>> arr = np.array([1])\\n        >>> ray.data.from_numpy(arr)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\\n\\n        Create a Ray Dataset from a list of NumPy arrays.\\n\\n        >>> ray.data.from_numpy([arr, arr])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\\n\\n    Args:\\n        ndarrays: A NumPy ndarray or a list of NumPy ndarrays.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\\n    '\n    if isinstance(ndarrays, np.ndarray):\n        ndarrays = [ndarrays]\n    return from_numpy_refs([ray.put(ndarray) for ndarray in ndarrays])",
            "@PublicAPI\ndef from_numpy(ndarrays: Union[np.ndarray, List[np.ndarray]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a :class:`~ray.data.Dataset` from a list of NumPy ndarrays.\\n\\n    Examples:\\n        >>> import numpy as np\\n        >>> import ray\\n        >>> arr = np.array([1])\\n        >>> ray.data.from_numpy(arr)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\\n\\n        Create a Ray Dataset from a list of NumPy arrays.\\n\\n        >>> ray.data.from_numpy([arr, arr])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\\n\\n    Args:\\n        ndarrays: A NumPy ndarray or a list of NumPy ndarrays.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\\n    '\n    if isinstance(ndarrays, np.ndarray):\n        ndarrays = [ndarrays]\n    return from_numpy_refs([ray.put(ndarray) for ndarray in ndarrays])"
        ]
    },
    {
        "func_name": "from_numpy_refs",
        "original": "@DeveloperAPI\ndef from_numpy_refs(ndarrays: Union[ObjectRef[np.ndarray], List[ObjectRef[np.ndarray]]]) -> MaterializedDataset:\n    \"\"\"Creates a :class:`~ray.data.Dataset` from a list of Ray object references to\n    NumPy ndarrays.\n\n    Examples:\n        >>> import numpy as np\n        >>> import ray\n        >>> arr_ref = ray.put(np.array([1]))\n        >>> ray.data.from_numpy_refs(arr_ref)\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\n\n        Create a Ray Dataset from a list of NumPy array references.\n\n        >>> ray.data.from_numpy_refs([arr_ref, arr_ref])\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\n\n    Args:\n        ndarrays: A Ray object reference to a NumPy ndarray or a list of Ray object\n            references to NumPy ndarrays.\n\n    Returns:\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\n    \"\"\"\n    if isinstance(ndarrays, ray.ObjectRef):\n        ndarrays = [ndarrays]\n    elif isinstance(ndarrays, list):\n        for ndarray in ndarrays:\n            if not isinstance(ndarray, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(ndarray)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(ndarray)}')\n    ctx = DataContext.get_current()\n    ndarray_to_block_remote = cached_remote_fn(ndarray_to_block, num_returns=2)\n    res = [ndarray_to_block_remote.remote(ndarray, ctx) for ndarray in ndarrays]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromNumpy(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromNumpy': metadata}, parent=None), run_by_consumer=False), logical_plan)",
        "mutated": [
            "@DeveloperAPI\ndef from_numpy_refs(ndarrays: Union[ObjectRef[np.ndarray], List[ObjectRef[np.ndarray]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n    'Creates a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    NumPy ndarrays.\\n\\n    Examples:\\n        >>> import numpy as np\\n        >>> import ray\\n        >>> arr_ref = ray.put(np.array([1]))\\n        >>> ray.data.from_numpy_refs(arr_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\\n\\n        Create a Ray Dataset from a list of NumPy array references.\\n\\n        >>> ray.data.from_numpy_refs([arr_ref, arr_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\\n\\n    Args:\\n        ndarrays: A Ray object reference to a NumPy ndarray or a list of Ray object\\n            references to NumPy ndarrays.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\\n    '\n    if isinstance(ndarrays, ray.ObjectRef):\n        ndarrays = [ndarrays]\n    elif isinstance(ndarrays, list):\n        for ndarray in ndarrays:\n            if not isinstance(ndarray, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(ndarray)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(ndarray)}')\n    ctx = DataContext.get_current()\n    ndarray_to_block_remote = cached_remote_fn(ndarray_to_block, num_returns=2)\n    res = [ndarray_to_block_remote.remote(ndarray, ctx) for ndarray in ndarrays]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromNumpy(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromNumpy': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_numpy_refs(ndarrays: Union[ObjectRef[np.ndarray], List[ObjectRef[np.ndarray]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    NumPy ndarrays.\\n\\n    Examples:\\n        >>> import numpy as np\\n        >>> import ray\\n        >>> arr_ref = ray.put(np.array([1]))\\n        >>> ray.data.from_numpy_refs(arr_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\\n\\n        Create a Ray Dataset from a list of NumPy array references.\\n\\n        >>> ray.data.from_numpy_refs([arr_ref, arr_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\\n\\n    Args:\\n        ndarrays: A Ray object reference to a NumPy ndarray or a list of Ray object\\n            references to NumPy ndarrays.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\\n    '\n    if isinstance(ndarrays, ray.ObjectRef):\n        ndarrays = [ndarrays]\n    elif isinstance(ndarrays, list):\n        for ndarray in ndarrays:\n            if not isinstance(ndarray, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(ndarray)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(ndarray)}')\n    ctx = DataContext.get_current()\n    ndarray_to_block_remote = cached_remote_fn(ndarray_to_block, num_returns=2)\n    res = [ndarray_to_block_remote.remote(ndarray, ctx) for ndarray in ndarrays]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromNumpy(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromNumpy': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_numpy_refs(ndarrays: Union[ObjectRef[np.ndarray], List[ObjectRef[np.ndarray]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    NumPy ndarrays.\\n\\n    Examples:\\n        >>> import numpy as np\\n        >>> import ray\\n        >>> arr_ref = ray.put(np.array([1]))\\n        >>> ray.data.from_numpy_refs(arr_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\\n\\n        Create a Ray Dataset from a list of NumPy array references.\\n\\n        >>> ray.data.from_numpy_refs([arr_ref, arr_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\\n\\n    Args:\\n        ndarrays: A Ray object reference to a NumPy ndarray or a list of Ray object\\n            references to NumPy ndarrays.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\\n    '\n    if isinstance(ndarrays, ray.ObjectRef):\n        ndarrays = [ndarrays]\n    elif isinstance(ndarrays, list):\n        for ndarray in ndarrays:\n            if not isinstance(ndarray, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(ndarray)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(ndarray)}')\n    ctx = DataContext.get_current()\n    ndarray_to_block_remote = cached_remote_fn(ndarray_to_block, num_returns=2)\n    res = [ndarray_to_block_remote.remote(ndarray, ctx) for ndarray in ndarrays]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromNumpy(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromNumpy': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_numpy_refs(ndarrays: Union[ObjectRef[np.ndarray], List[ObjectRef[np.ndarray]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    NumPy ndarrays.\\n\\n    Examples:\\n        >>> import numpy as np\\n        >>> import ray\\n        >>> arr_ref = ray.put(np.array([1]))\\n        >>> ray.data.from_numpy_refs(arr_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\\n\\n        Create a Ray Dataset from a list of NumPy array references.\\n\\n        >>> ray.data.from_numpy_refs([arr_ref, arr_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\\n\\n    Args:\\n        ndarrays: A Ray object reference to a NumPy ndarray or a list of Ray object\\n            references to NumPy ndarrays.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\\n    '\n    if isinstance(ndarrays, ray.ObjectRef):\n        ndarrays = [ndarrays]\n    elif isinstance(ndarrays, list):\n        for ndarray in ndarrays:\n            if not isinstance(ndarray, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(ndarray)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(ndarray)}')\n    ctx = DataContext.get_current()\n    ndarray_to_block_remote = cached_remote_fn(ndarray_to_block, num_returns=2)\n    res = [ndarray_to_block_remote.remote(ndarray, ctx) for ndarray in ndarrays]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromNumpy(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromNumpy': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_numpy_refs(ndarrays: Union[ObjectRef[np.ndarray], List[ObjectRef[np.ndarray]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    NumPy ndarrays.\\n\\n    Examples:\\n        >>> import numpy as np\\n        >>> import ray\\n        >>> arr_ref = ray.put(np.array([1]))\\n        >>> ray.data.from_numpy_refs(arr_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={data: int64})\\n\\n        Create a Ray Dataset from a list of NumPy array references.\\n\\n        >>> ray.data.from_numpy_refs([arr_ref, arr_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={data: int64})\\n\\n    Args:\\n        ndarrays: A Ray object reference to a NumPy ndarray or a list of Ray object\\n            references to NumPy ndarrays.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the given ndarrays.\\n    '\n    if isinstance(ndarrays, ray.ObjectRef):\n        ndarrays = [ndarrays]\n    elif isinstance(ndarrays, list):\n        for ndarray in ndarrays:\n            if not isinstance(ndarray, ray.ObjectRef):\n                raise ValueError(f'Expected list of Ray object refs, got list containing {type(ndarray)}')\n    else:\n        raise ValueError(f'Expected Ray object ref or list of Ray object refs, got {type(ndarray)}')\n    ctx = DataContext.get_current()\n    ndarray_to_block_remote = cached_remote_fn(ndarray_to_block, num_returns=2)\n    res = [ndarray_to_block_remote.remote(ndarray, ctx) for ndarray in ndarrays]\n    (blocks, metadata) = map(list, zip(*res))\n    metadata = ray.get(metadata)\n    logical_plan = LogicalPlan(FromNumpy(blocks, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(blocks, metadata, owned_by_consumer=False), DatasetStats(stages={'FromNumpy': metadata}, parent=None), run_by_consumer=False), logical_plan)"
        ]
    },
    {
        "func_name": "from_arrow",
        "original": "@PublicAPI\ndef from_arrow(tables: Union['pyarrow.Table', bytes, List[Union['pyarrow.Table', bytes]]]) -> MaterializedDataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a list of PyArrow tables.\n\n    Examples:\n        >>> import pyarrow as pa\n        >>> import ray\n        >>> table = pa.table({\"x\": [1]})\n        >>> ray.data.from_arrow(table)\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\n\n        Create a Ray Dataset from a list of PyArrow tables.\n\n        >>> ray.data.from_arrow([table, table])\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\n\n\n    Args:\n        tables: A PyArrow table, or a list of PyArrow tables,\n                or its streaming format in bytes.\n\n    Returns:\n        :class:`~ray.data.Dataset` holding data from the PyArrow tables.\n    \"\"\"\n    import pyarrow as pa\n    if isinstance(tables, (pa.Table, bytes)):\n        tables = [tables]\n    return from_arrow_refs([ray.put(t) for t in tables])",
        "mutated": [
            "@PublicAPI\ndef from_arrow(tables: Union['pyarrow.Table', bytes, List[Union['pyarrow.Table', bytes]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a list of PyArrow tables.\\n\\n    Examples:\\n        >>> import pyarrow as pa\\n        >>> import ray\\n        >>> table = pa.table({\"x\": [1]})\\n        >>> ray.data.from_arrow(table)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\\n\\n        Create a Ray Dataset from a list of PyArrow tables.\\n\\n        >>> ray.data.from_arrow([table, table])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\\n\\n\\n    Args:\\n        tables: A PyArrow table, or a list of PyArrow tables,\\n                or its streaming format in bytes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the PyArrow tables.\\n    '\n    import pyarrow as pa\n    if isinstance(tables, (pa.Table, bytes)):\n        tables = [tables]\n    return from_arrow_refs([ray.put(t) for t in tables])",
            "@PublicAPI\ndef from_arrow(tables: Union['pyarrow.Table', bytes, List[Union['pyarrow.Table', bytes]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a list of PyArrow tables.\\n\\n    Examples:\\n        >>> import pyarrow as pa\\n        >>> import ray\\n        >>> table = pa.table({\"x\": [1]})\\n        >>> ray.data.from_arrow(table)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\\n\\n        Create a Ray Dataset from a list of PyArrow tables.\\n\\n        >>> ray.data.from_arrow([table, table])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\\n\\n\\n    Args:\\n        tables: A PyArrow table, or a list of PyArrow tables,\\n                or its streaming format in bytes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the PyArrow tables.\\n    '\n    import pyarrow as pa\n    if isinstance(tables, (pa.Table, bytes)):\n        tables = [tables]\n    return from_arrow_refs([ray.put(t) for t in tables])",
            "@PublicAPI\ndef from_arrow(tables: Union['pyarrow.Table', bytes, List[Union['pyarrow.Table', bytes]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a list of PyArrow tables.\\n\\n    Examples:\\n        >>> import pyarrow as pa\\n        >>> import ray\\n        >>> table = pa.table({\"x\": [1]})\\n        >>> ray.data.from_arrow(table)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\\n\\n        Create a Ray Dataset from a list of PyArrow tables.\\n\\n        >>> ray.data.from_arrow([table, table])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\\n\\n\\n    Args:\\n        tables: A PyArrow table, or a list of PyArrow tables,\\n                or its streaming format in bytes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the PyArrow tables.\\n    '\n    import pyarrow as pa\n    if isinstance(tables, (pa.Table, bytes)):\n        tables = [tables]\n    return from_arrow_refs([ray.put(t) for t in tables])",
            "@PublicAPI\ndef from_arrow(tables: Union['pyarrow.Table', bytes, List[Union['pyarrow.Table', bytes]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a list of PyArrow tables.\\n\\n    Examples:\\n        >>> import pyarrow as pa\\n        >>> import ray\\n        >>> table = pa.table({\"x\": [1]})\\n        >>> ray.data.from_arrow(table)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\\n\\n        Create a Ray Dataset from a list of PyArrow tables.\\n\\n        >>> ray.data.from_arrow([table, table])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\\n\\n\\n    Args:\\n        tables: A PyArrow table, or a list of PyArrow tables,\\n                or its streaming format in bytes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the PyArrow tables.\\n    '\n    import pyarrow as pa\n    if isinstance(tables, (pa.Table, bytes)):\n        tables = [tables]\n    return from_arrow_refs([ray.put(t) for t in tables])",
            "@PublicAPI\ndef from_arrow(tables: Union['pyarrow.Table', bytes, List[Union['pyarrow.Table', bytes]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a list of PyArrow tables.\\n\\n    Examples:\\n        >>> import pyarrow as pa\\n        >>> import ray\\n        >>> table = pa.table({\"x\": [1]})\\n        >>> ray.data.from_arrow(table)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\\n\\n        Create a Ray Dataset from a list of PyArrow tables.\\n\\n        >>> ray.data.from_arrow([table, table])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\\n\\n\\n    Args:\\n        tables: A PyArrow table, or a list of PyArrow tables,\\n                or its streaming format in bytes.\\n\\n    Returns:\\n        :class:`~ray.data.Dataset` holding data from the PyArrow tables.\\n    '\n    import pyarrow as pa\n    if isinstance(tables, (pa.Table, bytes)):\n        tables = [tables]\n    return from_arrow_refs([ray.put(t) for t in tables])"
        ]
    },
    {
        "func_name": "from_arrow_refs",
        "original": "@DeveloperAPI\ndef from_arrow_refs(tables: Union[ObjectRef[Union['pyarrow.Table', bytes]], List[ObjectRef[Union['pyarrow.Table', bytes]]]]) -> MaterializedDataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a list of Ray object references to\n    PyArrow tables.\n\n    Examples:\n        >>> import pyarrow as pa\n        >>> import ray\n        >>> table_ref = ray.put(pa.table({\"x\": [1]}))\n        >>> ray.data.from_arrow_refs(table_ref)\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\n\n        Create a Ray Dataset from a list of PyArrow table references\n\n        >>> ray.data.from_arrow_refs([table_ref, table_ref])\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\n\n\n    Args:\n        tables: A Ray object reference to Arrow table, or list of Ray object\n                references to Arrow tables, or its streaming format in bytes.\n\n    Returns:\n         :class:`~ray.data.Dataset` holding data read from the tables.\n    \"\"\"\n    if isinstance(tables, ray.ObjectRef):\n        tables = [tables]\n    get_metadata = cached_remote_fn(get_table_block_metadata)\n    metadata = ray.get([get_metadata.remote(t) for t in tables])\n    logical_plan = LogicalPlan(FromArrow(tables, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(tables, metadata, owned_by_consumer=False), DatasetStats(stages={'FromArrow': metadata}, parent=None), run_by_consumer=False), logical_plan)",
        "mutated": [
            "@DeveloperAPI\ndef from_arrow_refs(tables: Union[ObjectRef[Union['pyarrow.Table', bytes]], List[ObjectRef[Union['pyarrow.Table', bytes]]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    PyArrow tables.\\n\\n    Examples:\\n        >>> import pyarrow as pa\\n        >>> import ray\\n        >>> table_ref = ray.put(pa.table({\"x\": [1]}))\\n        >>> ray.data.from_arrow_refs(table_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\\n\\n        Create a Ray Dataset from a list of PyArrow table references\\n\\n        >>> ray.data.from_arrow_refs([table_ref, table_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\\n\\n\\n    Args:\\n        tables: A Ray object reference to Arrow table, or list of Ray object\\n                references to Arrow tables, or its streaming format in bytes.\\n\\n    Returns:\\n         :class:`~ray.data.Dataset` holding data read from the tables.\\n    '\n    if isinstance(tables, ray.ObjectRef):\n        tables = [tables]\n    get_metadata = cached_remote_fn(get_table_block_metadata)\n    metadata = ray.get([get_metadata.remote(t) for t in tables])\n    logical_plan = LogicalPlan(FromArrow(tables, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(tables, metadata, owned_by_consumer=False), DatasetStats(stages={'FromArrow': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_arrow_refs(tables: Union[ObjectRef[Union['pyarrow.Table', bytes]], List[ObjectRef[Union['pyarrow.Table', bytes]]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    PyArrow tables.\\n\\n    Examples:\\n        >>> import pyarrow as pa\\n        >>> import ray\\n        >>> table_ref = ray.put(pa.table({\"x\": [1]}))\\n        >>> ray.data.from_arrow_refs(table_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\\n\\n        Create a Ray Dataset from a list of PyArrow table references\\n\\n        >>> ray.data.from_arrow_refs([table_ref, table_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\\n\\n\\n    Args:\\n        tables: A Ray object reference to Arrow table, or list of Ray object\\n                references to Arrow tables, or its streaming format in bytes.\\n\\n    Returns:\\n         :class:`~ray.data.Dataset` holding data read from the tables.\\n    '\n    if isinstance(tables, ray.ObjectRef):\n        tables = [tables]\n    get_metadata = cached_remote_fn(get_table_block_metadata)\n    metadata = ray.get([get_metadata.remote(t) for t in tables])\n    logical_plan = LogicalPlan(FromArrow(tables, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(tables, metadata, owned_by_consumer=False), DatasetStats(stages={'FromArrow': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_arrow_refs(tables: Union[ObjectRef[Union['pyarrow.Table', bytes]], List[ObjectRef[Union['pyarrow.Table', bytes]]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    PyArrow tables.\\n\\n    Examples:\\n        >>> import pyarrow as pa\\n        >>> import ray\\n        >>> table_ref = ray.put(pa.table({\"x\": [1]}))\\n        >>> ray.data.from_arrow_refs(table_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\\n\\n        Create a Ray Dataset from a list of PyArrow table references\\n\\n        >>> ray.data.from_arrow_refs([table_ref, table_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\\n\\n\\n    Args:\\n        tables: A Ray object reference to Arrow table, or list of Ray object\\n                references to Arrow tables, or its streaming format in bytes.\\n\\n    Returns:\\n         :class:`~ray.data.Dataset` holding data read from the tables.\\n    '\n    if isinstance(tables, ray.ObjectRef):\n        tables = [tables]\n    get_metadata = cached_remote_fn(get_table_block_metadata)\n    metadata = ray.get([get_metadata.remote(t) for t in tables])\n    logical_plan = LogicalPlan(FromArrow(tables, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(tables, metadata, owned_by_consumer=False), DatasetStats(stages={'FromArrow': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_arrow_refs(tables: Union[ObjectRef[Union['pyarrow.Table', bytes]], List[ObjectRef[Union['pyarrow.Table', bytes]]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    PyArrow tables.\\n\\n    Examples:\\n        >>> import pyarrow as pa\\n        >>> import ray\\n        >>> table_ref = ray.put(pa.table({\"x\": [1]}))\\n        >>> ray.data.from_arrow_refs(table_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\\n\\n        Create a Ray Dataset from a list of PyArrow table references\\n\\n        >>> ray.data.from_arrow_refs([table_ref, table_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\\n\\n\\n    Args:\\n        tables: A Ray object reference to Arrow table, or list of Ray object\\n                references to Arrow tables, or its streaming format in bytes.\\n\\n    Returns:\\n         :class:`~ray.data.Dataset` holding data read from the tables.\\n    '\n    if isinstance(tables, ray.ObjectRef):\n        tables = [tables]\n    get_metadata = cached_remote_fn(get_table_block_metadata)\n    metadata = ray.get([get_metadata.remote(t) for t in tables])\n    logical_plan = LogicalPlan(FromArrow(tables, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(tables, metadata, owned_by_consumer=False), DatasetStats(stages={'FromArrow': metadata}, parent=None), run_by_consumer=False), logical_plan)",
            "@DeveloperAPI\ndef from_arrow_refs(tables: Union[ObjectRef[Union['pyarrow.Table', bytes]], List[ObjectRef[Union['pyarrow.Table', bytes]]]]) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a list of Ray object references to\\n    PyArrow tables.\\n\\n    Examples:\\n        >>> import pyarrow as pa\\n        >>> import ray\\n        >>> table_ref = ray.put(pa.table({\"x\": [1]}))\\n        >>> ray.data.from_arrow_refs(table_ref)\\n        MaterializedDataset(num_blocks=1, num_rows=1, schema={x: int64})\\n\\n        Create a Ray Dataset from a list of PyArrow table references\\n\\n        >>> ray.data.from_arrow_refs([table_ref, table_ref])\\n        MaterializedDataset(num_blocks=2, num_rows=2, schema={x: int64})\\n\\n\\n    Args:\\n        tables: A Ray object reference to Arrow table, or list of Ray object\\n                references to Arrow tables, or its streaming format in bytes.\\n\\n    Returns:\\n         :class:`~ray.data.Dataset` holding data read from the tables.\\n    '\n    if isinstance(tables, ray.ObjectRef):\n        tables = [tables]\n    get_metadata = cached_remote_fn(get_table_block_metadata)\n    metadata = ray.get([get_metadata.remote(t) for t in tables])\n    logical_plan = LogicalPlan(FromArrow(tables, metadata))\n    return MaterializedDataset(ExecutionPlan(BlockList(tables, metadata, owned_by_consumer=False), DatasetStats(stages={'FromArrow': metadata}, parent=None), run_by_consumer=False), logical_plan)"
        ]
    },
    {
        "func_name": "from_spark",
        "original": "@PublicAPI\ndef from_spark(df: 'pyspark.sql.DataFrame', *, parallelism: Optional[int]=None) -> MaterializedDataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a\n    `Spark DataFrame <https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html>`_.\n\n    Args:\n        df: A `Spark DataFrame`_, which must be created by RayDP (Spark-on-Ray).\n        parallelism: The amount of parallelism to use for the dataset. If\n            not provided, the parallelism is equal to the number of partitions of\n            the original Spark DataFrame.\n\n    Returns:\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\n    \"\"\"\n    import raydp\n    return raydp.spark.spark_dataframe_to_ray_dataset(df, parallelism)",
        "mutated": [
            "@PublicAPI\ndef from_spark(df: 'pyspark.sql.DataFrame', *, parallelism: Optional[int]=None) -> MaterializedDataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Spark DataFrame <https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html>`_.\\n\\n    Args:\\n        df: A `Spark DataFrame`_, which must be created by RayDP (Spark-on-Ray).\\n        parallelism: The amount of parallelism to use for the dataset. If\\n            not provided, the parallelism is equal to the number of partitions of\\n            the original Spark DataFrame.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import raydp\n    return raydp.spark.spark_dataframe_to_ray_dataset(df, parallelism)",
            "@PublicAPI\ndef from_spark(df: 'pyspark.sql.DataFrame', *, parallelism: Optional[int]=None) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Spark DataFrame <https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html>`_.\\n\\n    Args:\\n        df: A `Spark DataFrame`_, which must be created by RayDP (Spark-on-Ray).\\n        parallelism: The amount of parallelism to use for the dataset. If\\n            not provided, the parallelism is equal to the number of partitions of\\n            the original Spark DataFrame.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import raydp\n    return raydp.spark.spark_dataframe_to_ray_dataset(df, parallelism)",
            "@PublicAPI\ndef from_spark(df: 'pyspark.sql.DataFrame', *, parallelism: Optional[int]=None) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Spark DataFrame <https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html>`_.\\n\\n    Args:\\n        df: A `Spark DataFrame`_, which must be created by RayDP (Spark-on-Ray).\\n        parallelism: The amount of parallelism to use for the dataset. If\\n            not provided, the parallelism is equal to the number of partitions of\\n            the original Spark DataFrame.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import raydp\n    return raydp.spark.spark_dataframe_to_ray_dataset(df, parallelism)",
            "@PublicAPI\ndef from_spark(df: 'pyspark.sql.DataFrame', *, parallelism: Optional[int]=None) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Spark DataFrame <https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html>`_.\\n\\n    Args:\\n        df: A `Spark DataFrame`_, which must be created by RayDP (Spark-on-Ray).\\n        parallelism: The amount of parallelism to use for the dataset. If\\n            not provided, the parallelism is equal to the number of partitions of\\n            the original Spark DataFrame.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import raydp\n    return raydp.spark.spark_dataframe_to_ray_dataset(df, parallelism)",
            "@PublicAPI\ndef from_spark(df: 'pyspark.sql.DataFrame', *, parallelism: Optional[int]=None) -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Spark DataFrame <https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html>`_.\\n\\n    Args:\\n        df: A `Spark DataFrame`_, which must be created by RayDP (Spark-on-Ray).\\n        parallelism: The amount of parallelism to use for the dataset. If\\n            not provided, the parallelism is equal to the number of partitions of\\n            the original Spark DataFrame.\\n\\n    Returns:\\n        A :class:`~ray.data.MaterializedDataset` holding rows read from the DataFrame.\\n    '\n    import raydp\n    return raydp.spark.spark_dataframe_to_ray_dataset(df, parallelism)"
        ]
    },
    {
        "func_name": "from_huggingface",
        "original": "@PublicAPI\ndef from_huggingface(dataset: Union['datasets.Dataset', 'datasets.IterableDataset']) -> Union[MaterializedDataset, Dataset]:\n    \"\"\"Create a :class:`~ray.data.MaterializedDataset` from a\n    `Hugging Face Datasets Dataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset/>`_\n    or a :class:`~ray.data.Dataset` from a `Hugging Face Datasets IterableDataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDataset/>`_.\n    For an `IterableDataset`, we use a streaming implementation to read data.\n\n    Example:\n\n        ..\n            The following `testoutput` is mocked to avoid illustrating download\n            logs like \"Downloading and preparing dataset 162.17 MiB\".\n\n        .. testcode::\n\n            import ray\n            import datasets\n\n            hf_dataset = datasets.load_dataset(\"tweet_eval\", \"emotion\")\n            ray_ds = ray.data.from_huggingface(hf_dataset[\"train\"])\n            print(ray_ds)\n\n            hf_dataset_stream = datasets.load_dataset(\"tweet_eval\", \"emotion\", streaming=True)\n            ray_ds_stream = ray.data.from_huggingface(hf_dataset_stream[\"train\"])\n            print(ray_ds_stream)\n\n        .. testoutput::\n            :options: +MOCK\n\n            MaterializedDataset(\n                num_blocks=...,\n                num_rows=3257,\n                schema={text: string, label: int64}\n            )\n            Dataset(\n                num_blocks=...,\n                num_rows=3257,\n                schema={text: string, label: int64}\n            )\n\n    Args:\n        dataset: A `Hugging Face Datasets Dataset`_ or `Hugging Face Datasets IterableDataset`_.\n            `DatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict/>`_\n            and `IterableDatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDatasetDict/>`_\n            are not supported.\n\n    Returns:\n        A :class:`~ray.data.Dataset` holding rows from the `Hugging Face Datasets Dataset`_.\n    \"\"\"\n    import datasets\n    if isinstance(dataset, datasets.IterableDataset):\n        from ray.data.datasource.huggingface_datasource import HuggingFaceDatasource\n        return read_datasource(HuggingFaceDatasource(dataset=dataset))\n    if isinstance(dataset, datasets.Dataset):\n        hf_ds_arrow = dataset.with_format('arrow')\n        ray_ds = from_arrow(hf_ds_arrow[:])\n        return ray_ds\n    elif isinstance(dataset, (datasets.DatasetDict, datasets.IterableDatasetDict)):\n        available_keys = list(dataset.keys())\n        raise DeprecationWarning(f\"You provided a Hugging Face DatasetDict or IterableDatasetDict, which contains multiple datasets, but `from_huggingface` now only accepts a single Hugging Face Dataset. To convert just a single Hugging Face Dataset to a Ray Dataset, specify a split. For example, `ray.data.from_huggingface(my_dataset_dictionary['{available_keys[0]}'])`. Available splits are {available_keys}.\")\n    else:\n        raise TypeError(f'`dataset` must be a `datasets.Dataset`, but got {type(dataset)}')",
        "mutated": [
            "@PublicAPI\ndef from_huggingface(dataset: Union['datasets.Dataset', 'datasets.IterableDataset']) -> Union[MaterializedDataset, Dataset]:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.MaterializedDataset` from a\\n    `Hugging Face Datasets Dataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset/>`_\\n    or a :class:`~ray.data.Dataset` from a `Hugging Face Datasets IterableDataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDataset/>`_.\\n    For an `IterableDataset`, we use a streaming implementation to read data.\\n\\n    Example:\\n\\n        ..\\n            The following `testoutput` is mocked to avoid illustrating download\\n            logs like \"Downloading and preparing dataset 162.17 MiB\".\\n\\n        .. testcode::\\n\\n            import ray\\n            import datasets\\n\\n            hf_dataset = datasets.load_dataset(\"tweet_eval\", \"emotion\")\\n            ray_ds = ray.data.from_huggingface(hf_dataset[\"train\"])\\n            print(ray_ds)\\n\\n            hf_dataset_stream = datasets.load_dataset(\"tweet_eval\", \"emotion\", streaming=True)\\n            ray_ds_stream = ray.data.from_huggingface(hf_dataset_stream[\"train\"])\\n            print(ray_ds_stream)\\n\\n        .. testoutput::\\n            :options: +MOCK\\n\\n            MaterializedDataset(\\n                num_blocks=...,\\n                num_rows=3257,\\n                schema={text: string, label: int64}\\n            )\\n            Dataset(\\n                num_blocks=...,\\n                num_rows=3257,\\n                schema={text: string, label: int64}\\n            )\\n\\n    Args:\\n        dataset: A `Hugging Face Datasets Dataset`_ or `Hugging Face Datasets IterableDataset`_.\\n            `DatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict/>`_\\n            and `IterableDatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDatasetDict/>`_\\n            are not supported.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` holding rows from the `Hugging Face Datasets Dataset`_.\\n    '\n    import datasets\n    if isinstance(dataset, datasets.IterableDataset):\n        from ray.data.datasource.huggingface_datasource import HuggingFaceDatasource\n        return read_datasource(HuggingFaceDatasource(dataset=dataset))\n    if isinstance(dataset, datasets.Dataset):\n        hf_ds_arrow = dataset.with_format('arrow')\n        ray_ds = from_arrow(hf_ds_arrow[:])\n        return ray_ds\n    elif isinstance(dataset, (datasets.DatasetDict, datasets.IterableDatasetDict)):\n        available_keys = list(dataset.keys())\n        raise DeprecationWarning(f\"You provided a Hugging Face DatasetDict or IterableDatasetDict, which contains multiple datasets, but `from_huggingface` now only accepts a single Hugging Face Dataset. To convert just a single Hugging Face Dataset to a Ray Dataset, specify a split. For example, `ray.data.from_huggingface(my_dataset_dictionary['{available_keys[0]}'])`. Available splits are {available_keys}.\")\n    else:\n        raise TypeError(f'`dataset` must be a `datasets.Dataset`, but got {type(dataset)}')",
            "@PublicAPI\ndef from_huggingface(dataset: Union['datasets.Dataset', 'datasets.IterableDataset']) -> Union[MaterializedDataset, Dataset]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.MaterializedDataset` from a\\n    `Hugging Face Datasets Dataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset/>`_\\n    or a :class:`~ray.data.Dataset` from a `Hugging Face Datasets IterableDataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDataset/>`_.\\n    For an `IterableDataset`, we use a streaming implementation to read data.\\n\\n    Example:\\n\\n        ..\\n            The following `testoutput` is mocked to avoid illustrating download\\n            logs like \"Downloading and preparing dataset 162.17 MiB\".\\n\\n        .. testcode::\\n\\n            import ray\\n            import datasets\\n\\n            hf_dataset = datasets.load_dataset(\"tweet_eval\", \"emotion\")\\n            ray_ds = ray.data.from_huggingface(hf_dataset[\"train\"])\\n            print(ray_ds)\\n\\n            hf_dataset_stream = datasets.load_dataset(\"tweet_eval\", \"emotion\", streaming=True)\\n            ray_ds_stream = ray.data.from_huggingface(hf_dataset_stream[\"train\"])\\n            print(ray_ds_stream)\\n\\n        .. testoutput::\\n            :options: +MOCK\\n\\n            MaterializedDataset(\\n                num_blocks=...,\\n                num_rows=3257,\\n                schema={text: string, label: int64}\\n            )\\n            Dataset(\\n                num_blocks=...,\\n                num_rows=3257,\\n                schema={text: string, label: int64}\\n            )\\n\\n    Args:\\n        dataset: A `Hugging Face Datasets Dataset`_ or `Hugging Face Datasets IterableDataset`_.\\n            `DatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict/>`_\\n            and `IterableDatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDatasetDict/>`_\\n            are not supported.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` holding rows from the `Hugging Face Datasets Dataset`_.\\n    '\n    import datasets\n    if isinstance(dataset, datasets.IterableDataset):\n        from ray.data.datasource.huggingface_datasource import HuggingFaceDatasource\n        return read_datasource(HuggingFaceDatasource(dataset=dataset))\n    if isinstance(dataset, datasets.Dataset):\n        hf_ds_arrow = dataset.with_format('arrow')\n        ray_ds = from_arrow(hf_ds_arrow[:])\n        return ray_ds\n    elif isinstance(dataset, (datasets.DatasetDict, datasets.IterableDatasetDict)):\n        available_keys = list(dataset.keys())\n        raise DeprecationWarning(f\"You provided a Hugging Face DatasetDict or IterableDatasetDict, which contains multiple datasets, but `from_huggingface` now only accepts a single Hugging Face Dataset. To convert just a single Hugging Face Dataset to a Ray Dataset, specify a split. For example, `ray.data.from_huggingface(my_dataset_dictionary['{available_keys[0]}'])`. Available splits are {available_keys}.\")\n    else:\n        raise TypeError(f'`dataset` must be a `datasets.Dataset`, but got {type(dataset)}')",
            "@PublicAPI\ndef from_huggingface(dataset: Union['datasets.Dataset', 'datasets.IterableDataset']) -> Union[MaterializedDataset, Dataset]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.MaterializedDataset` from a\\n    `Hugging Face Datasets Dataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset/>`_\\n    or a :class:`~ray.data.Dataset` from a `Hugging Face Datasets IterableDataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDataset/>`_.\\n    For an `IterableDataset`, we use a streaming implementation to read data.\\n\\n    Example:\\n\\n        ..\\n            The following `testoutput` is mocked to avoid illustrating download\\n            logs like \"Downloading and preparing dataset 162.17 MiB\".\\n\\n        .. testcode::\\n\\n            import ray\\n            import datasets\\n\\n            hf_dataset = datasets.load_dataset(\"tweet_eval\", \"emotion\")\\n            ray_ds = ray.data.from_huggingface(hf_dataset[\"train\"])\\n            print(ray_ds)\\n\\n            hf_dataset_stream = datasets.load_dataset(\"tweet_eval\", \"emotion\", streaming=True)\\n            ray_ds_stream = ray.data.from_huggingface(hf_dataset_stream[\"train\"])\\n            print(ray_ds_stream)\\n\\n        .. testoutput::\\n            :options: +MOCK\\n\\n            MaterializedDataset(\\n                num_blocks=...,\\n                num_rows=3257,\\n                schema={text: string, label: int64}\\n            )\\n            Dataset(\\n                num_blocks=...,\\n                num_rows=3257,\\n                schema={text: string, label: int64}\\n            )\\n\\n    Args:\\n        dataset: A `Hugging Face Datasets Dataset`_ or `Hugging Face Datasets IterableDataset`_.\\n            `DatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict/>`_\\n            and `IterableDatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDatasetDict/>`_\\n            are not supported.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` holding rows from the `Hugging Face Datasets Dataset`_.\\n    '\n    import datasets\n    if isinstance(dataset, datasets.IterableDataset):\n        from ray.data.datasource.huggingface_datasource import HuggingFaceDatasource\n        return read_datasource(HuggingFaceDatasource(dataset=dataset))\n    if isinstance(dataset, datasets.Dataset):\n        hf_ds_arrow = dataset.with_format('arrow')\n        ray_ds = from_arrow(hf_ds_arrow[:])\n        return ray_ds\n    elif isinstance(dataset, (datasets.DatasetDict, datasets.IterableDatasetDict)):\n        available_keys = list(dataset.keys())\n        raise DeprecationWarning(f\"You provided a Hugging Face DatasetDict or IterableDatasetDict, which contains multiple datasets, but `from_huggingface` now only accepts a single Hugging Face Dataset. To convert just a single Hugging Face Dataset to a Ray Dataset, specify a split. For example, `ray.data.from_huggingface(my_dataset_dictionary['{available_keys[0]}'])`. Available splits are {available_keys}.\")\n    else:\n        raise TypeError(f'`dataset` must be a `datasets.Dataset`, but got {type(dataset)}')",
            "@PublicAPI\ndef from_huggingface(dataset: Union['datasets.Dataset', 'datasets.IterableDataset']) -> Union[MaterializedDataset, Dataset]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.MaterializedDataset` from a\\n    `Hugging Face Datasets Dataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset/>`_\\n    or a :class:`~ray.data.Dataset` from a `Hugging Face Datasets IterableDataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDataset/>`_.\\n    For an `IterableDataset`, we use a streaming implementation to read data.\\n\\n    Example:\\n\\n        ..\\n            The following `testoutput` is mocked to avoid illustrating download\\n            logs like \"Downloading and preparing dataset 162.17 MiB\".\\n\\n        .. testcode::\\n\\n            import ray\\n            import datasets\\n\\n            hf_dataset = datasets.load_dataset(\"tweet_eval\", \"emotion\")\\n            ray_ds = ray.data.from_huggingface(hf_dataset[\"train\"])\\n            print(ray_ds)\\n\\n            hf_dataset_stream = datasets.load_dataset(\"tweet_eval\", \"emotion\", streaming=True)\\n            ray_ds_stream = ray.data.from_huggingface(hf_dataset_stream[\"train\"])\\n            print(ray_ds_stream)\\n\\n        .. testoutput::\\n            :options: +MOCK\\n\\n            MaterializedDataset(\\n                num_blocks=...,\\n                num_rows=3257,\\n                schema={text: string, label: int64}\\n            )\\n            Dataset(\\n                num_blocks=...,\\n                num_rows=3257,\\n                schema={text: string, label: int64}\\n            )\\n\\n    Args:\\n        dataset: A `Hugging Face Datasets Dataset`_ or `Hugging Face Datasets IterableDataset`_.\\n            `DatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict/>`_\\n            and `IterableDatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDatasetDict/>`_\\n            are not supported.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` holding rows from the `Hugging Face Datasets Dataset`_.\\n    '\n    import datasets\n    if isinstance(dataset, datasets.IterableDataset):\n        from ray.data.datasource.huggingface_datasource import HuggingFaceDatasource\n        return read_datasource(HuggingFaceDatasource(dataset=dataset))\n    if isinstance(dataset, datasets.Dataset):\n        hf_ds_arrow = dataset.with_format('arrow')\n        ray_ds = from_arrow(hf_ds_arrow[:])\n        return ray_ds\n    elif isinstance(dataset, (datasets.DatasetDict, datasets.IterableDatasetDict)):\n        available_keys = list(dataset.keys())\n        raise DeprecationWarning(f\"You provided a Hugging Face DatasetDict or IterableDatasetDict, which contains multiple datasets, but `from_huggingface` now only accepts a single Hugging Face Dataset. To convert just a single Hugging Face Dataset to a Ray Dataset, specify a split. For example, `ray.data.from_huggingface(my_dataset_dictionary['{available_keys[0]}'])`. Available splits are {available_keys}.\")\n    else:\n        raise TypeError(f'`dataset` must be a `datasets.Dataset`, but got {type(dataset)}')",
            "@PublicAPI\ndef from_huggingface(dataset: Union['datasets.Dataset', 'datasets.IterableDataset']) -> Union[MaterializedDataset, Dataset]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.MaterializedDataset` from a\\n    `Hugging Face Datasets Dataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset/>`_\\n    or a :class:`~ray.data.Dataset` from a `Hugging Face Datasets IterableDataset <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDataset/>`_.\\n    For an `IterableDataset`, we use a streaming implementation to read data.\\n\\n    Example:\\n\\n        ..\\n            The following `testoutput` is mocked to avoid illustrating download\\n            logs like \"Downloading and preparing dataset 162.17 MiB\".\\n\\n        .. testcode::\\n\\n            import ray\\n            import datasets\\n\\n            hf_dataset = datasets.load_dataset(\"tweet_eval\", \"emotion\")\\n            ray_ds = ray.data.from_huggingface(hf_dataset[\"train\"])\\n            print(ray_ds)\\n\\n            hf_dataset_stream = datasets.load_dataset(\"tweet_eval\", \"emotion\", streaming=True)\\n            ray_ds_stream = ray.data.from_huggingface(hf_dataset_stream[\"train\"])\\n            print(ray_ds_stream)\\n\\n        .. testoutput::\\n            :options: +MOCK\\n\\n            MaterializedDataset(\\n                num_blocks=...,\\n                num_rows=3257,\\n                schema={text: string, label: int64}\\n            )\\n            Dataset(\\n                num_blocks=...,\\n                num_rows=3257,\\n                schema={text: string, label: int64}\\n            )\\n\\n    Args:\\n        dataset: A `Hugging Face Datasets Dataset`_ or `Hugging Face Datasets IterableDataset`_.\\n            `DatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict/>`_\\n            and `IterableDatasetDict <https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.IterableDatasetDict/>`_\\n            are not supported.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` holding rows from the `Hugging Face Datasets Dataset`_.\\n    '\n    import datasets\n    if isinstance(dataset, datasets.IterableDataset):\n        from ray.data.datasource.huggingface_datasource import HuggingFaceDatasource\n        return read_datasource(HuggingFaceDatasource(dataset=dataset))\n    if isinstance(dataset, datasets.Dataset):\n        hf_ds_arrow = dataset.with_format('arrow')\n        ray_ds = from_arrow(hf_ds_arrow[:])\n        return ray_ds\n    elif isinstance(dataset, (datasets.DatasetDict, datasets.IterableDatasetDict)):\n        available_keys = list(dataset.keys())\n        raise DeprecationWarning(f\"You provided a Hugging Face DatasetDict or IterableDatasetDict, which contains multiple datasets, but `from_huggingface` now only accepts a single Hugging Face Dataset. To convert just a single Hugging Face Dataset to a Ray Dataset, specify a split. For example, `ray.data.from_huggingface(my_dataset_dictionary['{available_keys[0]}'])`. Available splits are {available_keys}.\")\n    else:\n        raise TypeError(f'`dataset` must be a `datasets.Dataset`, but got {type(dataset)}')"
        ]
    },
    {
        "func_name": "from_tf",
        "original": "@PublicAPI\ndef from_tf(dataset: 'tf.data.Dataset') -> MaterializedDataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a\n    `TensorFlow Dataset <https://www.tensorflow.org/api_docs/python/tf/data/Dataset/>`_.\n\n    This function is inefficient. Use it to read small datasets or prototype.\n\n    .. warning::\n        If your dataset is large, this function may execute slowly or raise an\n        out-of-memory error. To avoid issues, read the underyling data with a function\n        like :meth:`~ray.data.read_images`.\n\n    .. note::\n        This function isn't parallelized. It loads the entire dataset into the local\n        node's memory before moving the data to the distributed object store.\n\n    Examples:\n        >>> import ray\n        >>> import tensorflow_datasets as tfds\n        >>> dataset, _ = tfds.load('cifar10', split=[\"train\", \"test\"])  # doctest: +SKIP\n        >>> ds = ray.data.from_tf(dataset)  # doctest: +SKIP\n        >>> ds  # doctest: +SKIP\n        MaterializedDataset(\n            num_blocks=...,\n            num_rows=50000,\n            schema={\n                id: binary,\n                image: numpy.ndarray(shape=(32, 32, 3), dtype=uint8),\n                label: int64\n            }\n        )\n        >>> ds.take(1)  # doctest: +SKIP\n        [{'id': b'train_16399', 'image': array([[[143,  96,  70],\n        [141,  96,  72],\n        [135,  93,  72],\n        ...,\n        [ 96,  37,  19],\n        [105,  42,  18],\n        [104,  38,  20]],\n        ...,\n        [[195, 161, 126],\n        [187, 153, 123],\n        [186, 151, 128],\n        ...,\n        [212, 177, 147],\n        [219, 185, 155],\n        [221, 187, 157]]], dtype=uint8), 'label': 7}]\n\n    Args:\n        dataset: A `TensorFlow Dataset`_.\n\n    Returns:\n        A :class:`MaterializedDataset` that contains the samples stored in the `TensorFlow Dataset`_.\n    \"\"\"\n    return from_items(list(dataset.as_numpy_iterator()))",
        "mutated": [
            "@PublicAPI\ndef from_tf(dataset: 'tf.data.Dataset') -> MaterializedDataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a\\n    `TensorFlow Dataset <https://www.tensorflow.org/api_docs/python/tf/data/Dataset/>`_.\\n\\n    This function is inefficient. Use it to read small datasets or prototype.\\n\\n    .. warning::\\n        If your dataset is large, this function may execute slowly or raise an\\n        out-of-memory error. To avoid issues, read the underyling data with a function\\n        like :meth:`~ray.data.read_images`.\\n\\n    .. note::\\n        This function isn\\'t parallelized. It loads the entire dataset into the local\\n        node\\'s memory before moving the data to the distributed object store.\\n\\n    Examples:\\n        >>> import ray\\n        >>> import tensorflow_datasets as tfds\\n        >>> dataset, _ = tfds.load(\\'cifar10\\', split=[\"train\", \"test\"])  # doctest: +SKIP\\n        >>> ds = ray.data.from_tf(dataset)  # doctest: +SKIP\\n        >>> ds  # doctest: +SKIP\\n        MaterializedDataset(\\n            num_blocks=...,\\n            num_rows=50000,\\n            schema={\\n                id: binary,\\n                image: numpy.ndarray(shape=(32, 32, 3), dtype=uint8),\\n                label: int64\\n            }\\n        )\\n        >>> ds.take(1)  # doctest: +SKIP\\n        [{\\'id\\': b\\'train_16399\\', \\'image\\': array([[[143,  96,  70],\\n        [141,  96,  72],\\n        [135,  93,  72],\\n        ...,\\n        [ 96,  37,  19],\\n        [105,  42,  18],\\n        [104,  38,  20]],\\n        ...,\\n        [[195, 161, 126],\\n        [187, 153, 123],\\n        [186, 151, 128],\\n        ...,\\n        [212, 177, 147],\\n        [219, 185, 155],\\n        [221, 187, 157]]], dtype=uint8), \\'label\\': 7}]\\n\\n    Args:\\n        dataset: A `TensorFlow Dataset`_.\\n\\n    Returns:\\n        A :class:`MaterializedDataset` that contains the samples stored in the `TensorFlow Dataset`_.\\n    '\n    return from_items(list(dataset.as_numpy_iterator()))",
            "@PublicAPI\ndef from_tf(dataset: 'tf.data.Dataset') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a\\n    `TensorFlow Dataset <https://www.tensorflow.org/api_docs/python/tf/data/Dataset/>`_.\\n\\n    This function is inefficient. Use it to read small datasets or prototype.\\n\\n    .. warning::\\n        If your dataset is large, this function may execute slowly or raise an\\n        out-of-memory error. To avoid issues, read the underyling data with a function\\n        like :meth:`~ray.data.read_images`.\\n\\n    .. note::\\n        This function isn\\'t parallelized. It loads the entire dataset into the local\\n        node\\'s memory before moving the data to the distributed object store.\\n\\n    Examples:\\n        >>> import ray\\n        >>> import tensorflow_datasets as tfds\\n        >>> dataset, _ = tfds.load(\\'cifar10\\', split=[\"train\", \"test\"])  # doctest: +SKIP\\n        >>> ds = ray.data.from_tf(dataset)  # doctest: +SKIP\\n        >>> ds  # doctest: +SKIP\\n        MaterializedDataset(\\n            num_blocks=...,\\n            num_rows=50000,\\n            schema={\\n                id: binary,\\n                image: numpy.ndarray(shape=(32, 32, 3), dtype=uint8),\\n                label: int64\\n            }\\n        )\\n        >>> ds.take(1)  # doctest: +SKIP\\n        [{\\'id\\': b\\'train_16399\\', \\'image\\': array([[[143,  96,  70],\\n        [141,  96,  72],\\n        [135,  93,  72],\\n        ...,\\n        [ 96,  37,  19],\\n        [105,  42,  18],\\n        [104,  38,  20]],\\n        ...,\\n        [[195, 161, 126],\\n        [187, 153, 123],\\n        [186, 151, 128],\\n        ...,\\n        [212, 177, 147],\\n        [219, 185, 155],\\n        [221, 187, 157]]], dtype=uint8), \\'label\\': 7}]\\n\\n    Args:\\n        dataset: A `TensorFlow Dataset`_.\\n\\n    Returns:\\n        A :class:`MaterializedDataset` that contains the samples stored in the `TensorFlow Dataset`_.\\n    '\n    return from_items(list(dataset.as_numpy_iterator()))",
            "@PublicAPI\ndef from_tf(dataset: 'tf.data.Dataset') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a\\n    `TensorFlow Dataset <https://www.tensorflow.org/api_docs/python/tf/data/Dataset/>`_.\\n\\n    This function is inefficient. Use it to read small datasets or prototype.\\n\\n    .. warning::\\n        If your dataset is large, this function may execute slowly or raise an\\n        out-of-memory error. To avoid issues, read the underyling data with a function\\n        like :meth:`~ray.data.read_images`.\\n\\n    .. note::\\n        This function isn\\'t parallelized. It loads the entire dataset into the local\\n        node\\'s memory before moving the data to the distributed object store.\\n\\n    Examples:\\n        >>> import ray\\n        >>> import tensorflow_datasets as tfds\\n        >>> dataset, _ = tfds.load(\\'cifar10\\', split=[\"train\", \"test\"])  # doctest: +SKIP\\n        >>> ds = ray.data.from_tf(dataset)  # doctest: +SKIP\\n        >>> ds  # doctest: +SKIP\\n        MaterializedDataset(\\n            num_blocks=...,\\n            num_rows=50000,\\n            schema={\\n                id: binary,\\n                image: numpy.ndarray(shape=(32, 32, 3), dtype=uint8),\\n                label: int64\\n            }\\n        )\\n        >>> ds.take(1)  # doctest: +SKIP\\n        [{\\'id\\': b\\'train_16399\\', \\'image\\': array([[[143,  96,  70],\\n        [141,  96,  72],\\n        [135,  93,  72],\\n        ...,\\n        [ 96,  37,  19],\\n        [105,  42,  18],\\n        [104,  38,  20]],\\n        ...,\\n        [[195, 161, 126],\\n        [187, 153, 123],\\n        [186, 151, 128],\\n        ...,\\n        [212, 177, 147],\\n        [219, 185, 155],\\n        [221, 187, 157]]], dtype=uint8), \\'label\\': 7}]\\n\\n    Args:\\n        dataset: A `TensorFlow Dataset`_.\\n\\n    Returns:\\n        A :class:`MaterializedDataset` that contains the samples stored in the `TensorFlow Dataset`_.\\n    '\n    return from_items(list(dataset.as_numpy_iterator()))",
            "@PublicAPI\ndef from_tf(dataset: 'tf.data.Dataset') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a\\n    `TensorFlow Dataset <https://www.tensorflow.org/api_docs/python/tf/data/Dataset/>`_.\\n\\n    This function is inefficient. Use it to read small datasets or prototype.\\n\\n    .. warning::\\n        If your dataset is large, this function may execute slowly or raise an\\n        out-of-memory error. To avoid issues, read the underyling data with a function\\n        like :meth:`~ray.data.read_images`.\\n\\n    .. note::\\n        This function isn\\'t parallelized. It loads the entire dataset into the local\\n        node\\'s memory before moving the data to the distributed object store.\\n\\n    Examples:\\n        >>> import ray\\n        >>> import tensorflow_datasets as tfds\\n        >>> dataset, _ = tfds.load(\\'cifar10\\', split=[\"train\", \"test\"])  # doctest: +SKIP\\n        >>> ds = ray.data.from_tf(dataset)  # doctest: +SKIP\\n        >>> ds  # doctest: +SKIP\\n        MaterializedDataset(\\n            num_blocks=...,\\n            num_rows=50000,\\n            schema={\\n                id: binary,\\n                image: numpy.ndarray(shape=(32, 32, 3), dtype=uint8),\\n                label: int64\\n            }\\n        )\\n        >>> ds.take(1)  # doctest: +SKIP\\n        [{\\'id\\': b\\'train_16399\\', \\'image\\': array([[[143,  96,  70],\\n        [141,  96,  72],\\n        [135,  93,  72],\\n        ...,\\n        [ 96,  37,  19],\\n        [105,  42,  18],\\n        [104,  38,  20]],\\n        ...,\\n        [[195, 161, 126],\\n        [187, 153, 123],\\n        [186, 151, 128],\\n        ...,\\n        [212, 177, 147],\\n        [219, 185, 155],\\n        [221, 187, 157]]], dtype=uint8), \\'label\\': 7}]\\n\\n    Args:\\n        dataset: A `TensorFlow Dataset`_.\\n\\n    Returns:\\n        A :class:`MaterializedDataset` that contains the samples stored in the `TensorFlow Dataset`_.\\n    '\n    return from_items(list(dataset.as_numpy_iterator()))",
            "@PublicAPI\ndef from_tf(dataset: 'tf.data.Dataset') -> MaterializedDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a\\n    `TensorFlow Dataset <https://www.tensorflow.org/api_docs/python/tf/data/Dataset/>`_.\\n\\n    This function is inefficient. Use it to read small datasets or prototype.\\n\\n    .. warning::\\n        If your dataset is large, this function may execute slowly or raise an\\n        out-of-memory error. To avoid issues, read the underyling data with a function\\n        like :meth:`~ray.data.read_images`.\\n\\n    .. note::\\n        This function isn\\'t parallelized. It loads the entire dataset into the local\\n        node\\'s memory before moving the data to the distributed object store.\\n\\n    Examples:\\n        >>> import ray\\n        >>> import tensorflow_datasets as tfds\\n        >>> dataset, _ = tfds.load(\\'cifar10\\', split=[\"train\", \"test\"])  # doctest: +SKIP\\n        >>> ds = ray.data.from_tf(dataset)  # doctest: +SKIP\\n        >>> ds  # doctest: +SKIP\\n        MaterializedDataset(\\n            num_blocks=...,\\n            num_rows=50000,\\n            schema={\\n                id: binary,\\n                image: numpy.ndarray(shape=(32, 32, 3), dtype=uint8),\\n                label: int64\\n            }\\n        )\\n        >>> ds.take(1)  # doctest: +SKIP\\n        [{\\'id\\': b\\'train_16399\\', \\'image\\': array([[[143,  96,  70],\\n        [141,  96,  72],\\n        [135,  93,  72],\\n        ...,\\n        [ 96,  37,  19],\\n        [105,  42,  18],\\n        [104,  38,  20]],\\n        ...,\\n        [[195, 161, 126],\\n        [187, 153, 123],\\n        [186, 151, 128],\\n        ...,\\n        [212, 177, 147],\\n        [219, 185, 155],\\n        [221, 187, 157]]], dtype=uint8), \\'label\\': 7}]\\n\\n    Args:\\n        dataset: A `TensorFlow Dataset`_.\\n\\n    Returns:\\n        A :class:`MaterializedDataset` that contains the samples stored in the `TensorFlow Dataset`_.\\n    '\n    return from_items(list(dataset.as_numpy_iterator()))"
        ]
    },
    {
        "func_name": "from_torch",
        "original": "@PublicAPI\ndef from_torch(dataset: 'torch.utils.data.Dataset') -> Dataset:\n    \"\"\"Create a :class:`~ray.data.Dataset` from a\n    `Torch Dataset <https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset/>`_.\n\n    .. note::\n        The input dataset can either be map-style or iterable-style, and can have arbitrarily large amount of data.\n        The data will be sequentially streamed with one single read task.\n\n    Examples:\n        >>> import ray\n        >>> from torchvision import datasets\n        >>> dataset = datasets.MNIST(\"data\", download=True)  # doctest: +SKIP\n        >>> ds = ray.data.from_torch(dataset)  # doctest: +SKIP\n        >>> ds  # doctest: +SKIP\n        MaterializedDataset(num_blocks=..., num_rows=60000, schema={item: object})\n        >>> ds.take(1)  # doctest: +SKIP\n        {\"item\": (<PIL.Image.Image image mode=L size=28x28 at 0x...>, 5)}\n\n    Args:\n        dataset: A `Torch Dataset`_.\n\n    Returns:\n        A :class:`~ray.data.Dataset` containing the Torch dataset samples.\n    \"\"\"\n    ray_remote_args = {'scheduling_strategy': NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)}\n    return read_datasource(TorchDatasource(dataset=dataset), parallelism=1, ray_remote_args=ray_remote_args)",
        "mutated": [
            "@PublicAPI\ndef from_torch(dataset: 'torch.utils.data.Dataset') -> Dataset:\n    if False:\n        i = 10\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Torch Dataset <https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset/>`_.\\n\\n    .. note::\\n        The input dataset can either be map-style or iterable-style, and can have arbitrarily large amount of data.\\n        The data will be sequentially streamed with one single read task.\\n\\n    Examples:\\n        >>> import ray\\n        >>> from torchvision import datasets\\n        >>> dataset = datasets.MNIST(\"data\", download=True)  # doctest: +SKIP\\n        >>> ds = ray.data.from_torch(dataset)  # doctest: +SKIP\\n        >>> ds  # doctest: +SKIP\\n        MaterializedDataset(num_blocks=..., num_rows=60000, schema={item: object})\\n        >>> ds.take(1)  # doctest: +SKIP\\n        {\"item\": (<PIL.Image.Image image mode=L size=28x28 at 0x...>, 5)}\\n\\n    Args:\\n        dataset: A `Torch Dataset`_.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` containing the Torch dataset samples.\\n    '\n    ray_remote_args = {'scheduling_strategy': NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)}\n    return read_datasource(TorchDatasource(dataset=dataset), parallelism=1, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef from_torch(dataset: 'torch.utils.data.Dataset') -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Torch Dataset <https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset/>`_.\\n\\n    .. note::\\n        The input dataset can either be map-style or iterable-style, and can have arbitrarily large amount of data.\\n        The data will be sequentially streamed with one single read task.\\n\\n    Examples:\\n        >>> import ray\\n        >>> from torchvision import datasets\\n        >>> dataset = datasets.MNIST(\"data\", download=True)  # doctest: +SKIP\\n        >>> ds = ray.data.from_torch(dataset)  # doctest: +SKIP\\n        >>> ds  # doctest: +SKIP\\n        MaterializedDataset(num_blocks=..., num_rows=60000, schema={item: object})\\n        >>> ds.take(1)  # doctest: +SKIP\\n        {\"item\": (<PIL.Image.Image image mode=L size=28x28 at 0x...>, 5)}\\n\\n    Args:\\n        dataset: A `Torch Dataset`_.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` containing the Torch dataset samples.\\n    '\n    ray_remote_args = {'scheduling_strategy': NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)}\n    return read_datasource(TorchDatasource(dataset=dataset), parallelism=1, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef from_torch(dataset: 'torch.utils.data.Dataset') -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Torch Dataset <https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset/>`_.\\n\\n    .. note::\\n        The input dataset can either be map-style or iterable-style, and can have arbitrarily large amount of data.\\n        The data will be sequentially streamed with one single read task.\\n\\n    Examples:\\n        >>> import ray\\n        >>> from torchvision import datasets\\n        >>> dataset = datasets.MNIST(\"data\", download=True)  # doctest: +SKIP\\n        >>> ds = ray.data.from_torch(dataset)  # doctest: +SKIP\\n        >>> ds  # doctest: +SKIP\\n        MaterializedDataset(num_blocks=..., num_rows=60000, schema={item: object})\\n        >>> ds.take(1)  # doctest: +SKIP\\n        {\"item\": (<PIL.Image.Image image mode=L size=28x28 at 0x...>, 5)}\\n\\n    Args:\\n        dataset: A `Torch Dataset`_.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` containing the Torch dataset samples.\\n    '\n    ray_remote_args = {'scheduling_strategy': NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)}\n    return read_datasource(TorchDatasource(dataset=dataset), parallelism=1, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef from_torch(dataset: 'torch.utils.data.Dataset') -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Torch Dataset <https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset/>`_.\\n\\n    .. note::\\n        The input dataset can either be map-style or iterable-style, and can have arbitrarily large amount of data.\\n        The data will be sequentially streamed with one single read task.\\n\\n    Examples:\\n        >>> import ray\\n        >>> from torchvision import datasets\\n        >>> dataset = datasets.MNIST(\"data\", download=True)  # doctest: +SKIP\\n        >>> ds = ray.data.from_torch(dataset)  # doctest: +SKIP\\n        >>> ds  # doctest: +SKIP\\n        MaterializedDataset(num_blocks=..., num_rows=60000, schema={item: object})\\n        >>> ds.take(1)  # doctest: +SKIP\\n        {\"item\": (<PIL.Image.Image image mode=L size=28x28 at 0x...>, 5)}\\n\\n    Args:\\n        dataset: A `Torch Dataset`_.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` containing the Torch dataset samples.\\n    '\n    ray_remote_args = {'scheduling_strategy': NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)}\n    return read_datasource(TorchDatasource(dataset=dataset), parallelism=1, ray_remote_args=ray_remote_args)",
            "@PublicAPI\ndef from_torch(dataset: 'torch.utils.data.Dataset') -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a :class:`~ray.data.Dataset` from a\\n    `Torch Dataset <https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset/>`_.\\n\\n    .. note::\\n        The input dataset can either be map-style or iterable-style, and can have arbitrarily large amount of data.\\n        The data will be sequentially streamed with one single read task.\\n\\n    Examples:\\n        >>> import ray\\n        >>> from torchvision import datasets\\n        >>> dataset = datasets.MNIST(\"data\", download=True)  # doctest: +SKIP\\n        >>> ds = ray.data.from_torch(dataset)  # doctest: +SKIP\\n        >>> ds  # doctest: +SKIP\\n        MaterializedDataset(num_blocks=..., num_rows=60000, schema={item: object})\\n        >>> ds.take(1)  # doctest: +SKIP\\n        {\"item\": (<PIL.Image.Image image mode=L size=28x28 at 0x...>, 5)}\\n\\n    Args:\\n        dataset: A `Torch Dataset`_.\\n\\n    Returns:\\n        A :class:`~ray.data.Dataset` containing the Torch dataset samples.\\n    '\n    ray_remote_args = {'scheduling_strategy': NodeAffinitySchedulingStrategy(ray.get_runtime_context().get_node_id(), soft=False)}\n    return read_datasource(TorchDatasource(dataset=dataset), parallelism=1, ray_remote_args=ray_remote_args)"
        ]
    },
    {
        "func_name": "_get_datasource_or_legacy_reader",
        "original": "def _get_datasource_or_legacy_reader(ds: Datasource, ctx: DataContext, kwargs: dict) -> Union[Datasource, Reader]:\n    \"\"\"Generates reader.\n\n    Args:\n        ds: Datasource to read from.\n        ctx: Dataset config to use.\n        kwargs: Additional kwargs to pass to the legacy reader if\n            `Datasource.create_reader` is implemented.\n\n    Returns:\n        The datasource or a generated legacy reader.\n    \"\"\"\n    kwargs = _unwrap_arrow_serialization_workaround(kwargs)\n    DataContext._set_current(ctx)\n    if ds.should_create_reader:\n        datasource_or_legacy_reader = ds.create_reader(**kwargs)\n    else:\n        datasource_or_legacy_reader = ds\n    return datasource_or_legacy_reader",
        "mutated": [
            "def _get_datasource_or_legacy_reader(ds: Datasource, ctx: DataContext, kwargs: dict) -> Union[Datasource, Reader]:\n    if False:\n        i = 10\n    'Generates reader.\\n\\n    Args:\\n        ds: Datasource to read from.\\n        ctx: Dataset config to use.\\n        kwargs: Additional kwargs to pass to the legacy reader if\\n            `Datasource.create_reader` is implemented.\\n\\n    Returns:\\n        The datasource or a generated legacy reader.\\n    '\n    kwargs = _unwrap_arrow_serialization_workaround(kwargs)\n    DataContext._set_current(ctx)\n    if ds.should_create_reader:\n        datasource_or_legacy_reader = ds.create_reader(**kwargs)\n    else:\n        datasource_or_legacy_reader = ds\n    return datasource_or_legacy_reader",
            "def _get_datasource_or_legacy_reader(ds: Datasource, ctx: DataContext, kwargs: dict) -> Union[Datasource, Reader]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generates reader.\\n\\n    Args:\\n        ds: Datasource to read from.\\n        ctx: Dataset config to use.\\n        kwargs: Additional kwargs to pass to the legacy reader if\\n            `Datasource.create_reader` is implemented.\\n\\n    Returns:\\n        The datasource or a generated legacy reader.\\n    '\n    kwargs = _unwrap_arrow_serialization_workaround(kwargs)\n    DataContext._set_current(ctx)\n    if ds.should_create_reader:\n        datasource_or_legacy_reader = ds.create_reader(**kwargs)\n    else:\n        datasource_or_legacy_reader = ds\n    return datasource_or_legacy_reader",
            "def _get_datasource_or_legacy_reader(ds: Datasource, ctx: DataContext, kwargs: dict) -> Union[Datasource, Reader]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generates reader.\\n\\n    Args:\\n        ds: Datasource to read from.\\n        ctx: Dataset config to use.\\n        kwargs: Additional kwargs to pass to the legacy reader if\\n            `Datasource.create_reader` is implemented.\\n\\n    Returns:\\n        The datasource or a generated legacy reader.\\n    '\n    kwargs = _unwrap_arrow_serialization_workaround(kwargs)\n    DataContext._set_current(ctx)\n    if ds.should_create_reader:\n        datasource_or_legacy_reader = ds.create_reader(**kwargs)\n    else:\n        datasource_or_legacy_reader = ds\n    return datasource_or_legacy_reader",
            "def _get_datasource_or_legacy_reader(ds: Datasource, ctx: DataContext, kwargs: dict) -> Union[Datasource, Reader]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generates reader.\\n\\n    Args:\\n        ds: Datasource to read from.\\n        ctx: Dataset config to use.\\n        kwargs: Additional kwargs to pass to the legacy reader if\\n            `Datasource.create_reader` is implemented.\\n\\n    Returns:\\n        The datasource or a generated legacy reader.\\n    '\n    kwargs = _unwrap_arrow_serialization_workaround(kwargs)\n    DataContext._set_current(ctx)\n    if ds.should_create_reader:\n        datasource_or_legacy_reader = ds.create_reader(**kwargs)\n    else:\n        datasource_or_legacy_reader = ds\n    return datasource_or_legacy_reader",
            "def _get_datasource_or_legacy_reader(ds: Datasource, ctx: DataContext, kwargs: dict) -> Union[Datasource, Reader]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generates reader.\\n\\n    Args:\\n        ds: Datasource to read from.\\n        ctx: Dataset config to use.\\n        kwargs: Additional kwargs to pass to the legacy reader if\\n            `Datasource.create_reader` is implemented.\\n\\n    Returns:\\n        The datasource or a generated legacy reader.\\n    '\n    kwargs = _unwrap_arrow_serialization_workaround(kwargs)\n    DataContext._set_current(ctx)\n    if ds.should_create_reader:\n        datasource_or_legacy_reader = ds.create_reader(**kwargs)\n    else:\n        datasource_or_legacy_reader = ds\n    return datasource_or_legacy_reader"
        ]
    },
    {
        "func_name": "_block_udf",
        "original": "def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n    from ray.data.extensions import ArrowTensorArray\n    for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n        np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n        block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n    if existing_block_udf is not None:\n        block = existing_block_udf(block)\n    return block",
        "mutated": [
            "def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n    if False:\n        i = 10\n    from ray.data.extensions import ArrowTensorArray\n    for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n        np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n        block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n    if existing_block_udf is not None:\n        block = existing_block_udf(block)\n    return block",
            "def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from ray.data.extensions import ArrowTensorArray\n    for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n        np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n        block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n    if existing_block_udf is not None:\n        block = existing_block_udf(block)\n    return block",
            "def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from ray.data.extensions import ArrowTensorArray\n    for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n        np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n        block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n    if existing_block_udf is not None:\n        block = existing_block_udf(block)\n    return block",
            "def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from ray.data.extensions import ArrowTensorArray\n    for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n        np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n        block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n    if existing_block_udf is not None:\n        block = existing_block_udf(block)\n    return block",
            "def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from ray.data.extensions import ArrowTensorArray\n    for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n        np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n        block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n    if existing_block_udf is not None:\n        block = existing_block_udf(block)\n    return block"
        ]
    },
    {
        "func_name": "_resolve_parquet_args",
        "original": "def _resolve_parquet_args(tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, **arrow_parquet_args) -> Dict[str, Any]:\n    if tensor_column_schema is not None:\n        existing_block_udf = arrow_parquet_args.pop('_block_udf', None)\n\n        def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n            from ray.data.extensions import ArrowTensorArray\n            for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n                np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n                block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n            if existing_block_udf is not None:\n                block = existing_block_udf(block)\n            return block\n        arrow_parquet_args['_block_udf'] = _block_udf\n    return arrow_parquet_args",
        "mutated": [
            "def _resolve_parquet_args(tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, **arrow_parquet_args) -> Dict[str, Any]:\n    if False:\n        i = 10\n    if tensor_column_schema is not None:\n        existing_block_udf = arrow_parquet_args.pop('_block_udf', None)\n\n        def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n            from ray.data.extensions import ArrowTensorArray\n            for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n                np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n                block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n            if existing_block_udf is not None:\n                block = existing_block_udf(block)\n            return block\n        arrow_parquet_args['_block_udf'] = _block_udf\n    return arrow_parquet_args",
            "def _resolve_parquet_args(tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, **arrow_parquet_args) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if tensor_column_schema is not None:\n        existing_block_udf = arrow_parquet_args.pop('_block_udf', None)\n\n        def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n            from ray.data.extensions import ArrowTensorArray\n            for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n                np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n                block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n            if existing_block_udf is not None:\n                block = existing_block_udf(block)\n            return block\n        arrow_parquet_args['_block_udf'] = _block_udf\n    return arrow_parquet_args",
            "def _resolve_parquet_args(tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, **arrow_parquet_args) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if tensor_column_schema is not None:\n        existing_block_udf = arrow_parquet_args.pop('_block_udf', None)\n\n        def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n            from ray.data.extensions import ArrowTensorArray\n            for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n                np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n                block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n            if existing_block_udf is not None:\n                block = existing_block_udf(block)\n            return block\n        arrow_parquet_args['_block_udf'] = _block_udf\n    return arrow_parquet_args",
            "def _resolve_parquet_args(tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, **arrow_parquet_args) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if tensor_column_schema is not None:\n        existing_block_udf = arrow_parquet_args.pop('_block_udf', None)\n\n        def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n            from ray.data.extensions import ArrowTensorArray\n            for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n                np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n                block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n            if existing_block_udf is not None:\n                block = existing_block_udf(block)\n            return block\n        arrow_parquet_args['_block_udf'] = _block_udf\n    return arrow_parquet_args",
            "def _resolve_parquet_args(tensor_column_schema: Optional[Dict[str, Tuple[np.dtype, Tuple[int, ...]]]]=None, **arrow_parquet_args) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if tensor_column_schema is not None:\n        existing_block_udf = arrow_parquet_args.pop('_block_udf', None)\n\n        def _block_udf(block: 'pyarrow.Table') -> 'pyarrow.Table':\n            from ray.data.extensions import ArrowTensorArray\n            for (tensor_col_name, (dtype, shape)) in tensor_column_schema.items():\n                np_col = _create_possibly_ragged_ndarray([np.ndarray(shape, buffer=buf.as_buffer(), dtype=dtype) for buf in block.column(tensor_col_name)])\n                block = block.set_column(block._ensure_integer_index(tensor_col_name), tensor_col_name, ArrowTensorArray.from_numpy(np_col))\n            if existing_block_udf is not None:\n                block = existing_block_udf(block)\n            return block\n        arrow_parquet_args['_block_udf'] = _block_udf\n    return arrow_parquet_args"
        ]
    }
]