[
    {
        "func_name": "convert_state_dict",
        "original": "def convert_state_dict(state_dict):\n    state_dict_keys = list(state_dict.keys())\n    for name in state_dict_keys:\n        weight = state_dict.pop(name)\n        if name.startswith('emb.'):\n            name = name.replace('emb.', 'embeddings.')\n        if name.startswith('blocks.0.ln0'):\n            name = name.replace('blocks.0.ln0', 'blocks.0.pre_ln')\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.att', 'blocks.\\\\1.attention', name)\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.ffn', 'blocks.\\\\1.feed_forward', name)\n        if name.endswith('.time_mix_k'):\n            name = name.replace('.time_mix_k', '.time_mix_key')\n        if name.endswith('.time_mix_v'):\n            name = name.replace('.time_mix_v', '.time_mix_value')\n        if name.endswith('.time_mix_r'):\n            name = name.replace('.time_mix_r', '.time_mix_receptance')\n        if name != 'head.weight':\n            name = 'rwkv.' + name\n        state_dict[name] = weight\n    return state_dict",
        "mutated": [
            "def convert_state_dict(state_dict):\n    if False:\n        i = 10\n    state_dict_keys = list(state_dict.keys())\n    for name in state_dict_keys:\n        weight = state_dict.pop(name)\n        if name.startswith('emb.'):\n            name = name.replace('emb.', 'embeddings.')\n        if name.startswith('blocks.0.ln0'):\n            name = name.replace('blocks.0.ln0', 'blocks.0.pre_ln')\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.att', 'blocks.\\\\1.attention', name)\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.ffn', 'blocks.\\\\1.feed_forward', name)\n        if name.endswith('.time_mix_k'):\n            name = name.replace('.time_mix_k', '.time_mix_key')\n        if name.endswith('.time_mix_v'):\n            name = name.replace('.time_mix_v', '.time_mix_value')\n        if name.endswith('.time_mix_r'):\n            name = name.replace('.time_mix_r', '.time_mix_receptance')\n        if name != 'head.weight':\n            name = 'rwkv.' + name\n        state_dict[name] = weight\n    return state_dict",
            "def convert_state_dict(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    state_dict_keys = list(state_dict.keys())\n    for name in state_dict_keys:\n        weight = state_dict.pop(name)\n        if name.startswith('emb.'):\n            name = name.replace('emb.', 'embeddings.')\n        if name.startswith('blocks.0.ln0'):\n            name = name.replace('blocks.0.ln0', 'blocks.0.pre_ln')\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.att', 'blocks.\\\\1.attention', name)\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.ffn', 'blocks.\\\\1.feed_forward', name)\n        if name.endswith('.time_mix_k'):\n            name = name.replace('.time_mix_k', '.time_mix_key')\n        if name.endswith('.time_mix_v'):\n            name = name.replace('.time_mix_v', '.time_mix_value')\n        if name.endswith('.time_mix_r'):\n            name = name.replace('.time_mix_r', '.time_mix_receptance')\n        if name != 'head.weight':\n            name = 'rwkv.' + name\n        state_dict[name] = weight\n    return state_dict",
            "def convert_state_dict(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    state_dict_keys = list(state_dict.keys())\n    for name in state_dict_keys:\n        weight = state_dict.pop(name)\n        if name.startswith('emb.'):\n            name = name.replace('emb.', 'embeddings.')\n        if name.startswith('blocks.0.ln0'):\n            name = name.replace('blocks.0.ln0', 'blocks.0.pre_ln')\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.att', 'blocks.\\\\1.attention', name)\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.ffn', 'blocks.\\\\1.feed_forward', name)\n        if name.endswith('.time_mix_k'):\n            name = name.replace('.time_mix_k', '.time_mix_key')\n        if name.endswith('.time_mix_v'):\n            name = name.replace('.time_mix_v', '.time_mix_value')\n        if name.endswith('.time_mix_r'):\n            name = name.replace('.time_mix_r', '.time_mix_receptance')\n        if name != 'head.weight':\n            name = 'rwkv.' + name\n        state_dict[name] = weight\n    return state_dict",
            "def convert_state_dict(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    state_dict_keys = list(state_dict.keys())\n    for name in state_dict_keys:\n        weight = state_dict.pop(name)\n        if name.startswith('emb.'):\n            name = name.replace('emb.', 'embeddings.')\n        if name.startswith('blocks.0.ln0'):\n            name = name.replace('blocks.0.ln0', 'blocks.0.pre_ln')\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.att', 'blocks.\\\\1.attention', name)\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.ffn', 'blocks.\\\\1.feed_forward', name)\n        if name.endswith('.time_mix_k'):\n            name = name.replace('.time_mix_k', '.time_mix_key')\n        if name.endswith('.time_mix_v'):\n            name = name.replace('.time_mix_v', '.time_mix_value')\n        if name.endswith('.time_mix_r'):\n            name = name.replace('.time_mix_r', '.time_mix_receptance')\n        if name != 'head.weight':\n            name = 'rwkv.' + name\n        state_dict[name] = weight\n    return state_dict",
            "def convert_state_dict(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    state_dict_keys = list(state_dict.keys())\n    for name in state_dict_keys:\n        weight = state_dict.pop(name)\n        if name.startswith('emb.'):\n            name = name.replace('emb.', 'embeddings.')\n        if name.startswith('blocks.0.ln0'):\n            name = name.replace('blocks.0.ln0', 'blocks.0.pre_ln')\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.att', 'blocks.\\\\1.attention', name)\n        name = re.sub('blocks\\\\.(\\\\d+)\\\\.ffn', 'blocks.\\\\1.feed_forward', name)\n        if name.endswith('.time_mix_k'):\n            name = name.replace('.time_mix_k', '.time_mix_key')\n        if name.endswith('.time_mix_v'):\n            name = name.replace('.time_mix_v', '.time_mix_value')\n        if name.endswith('.time_mix_r'):\n            name = name.replace('.time_mix_r', '.time_mix_receptance')\n        if name != 'head.weight':\n            name = 'rwkv.' + name\n        state_dict[name] = weight\n    return state_dict"
        ]
    },
    {
        "func_name": "convert_rmkv_checkpoint_to_hf_format",
        "original": "def convert_rmkv_checkpoint_to_hf_format(repo_id, checkpoint_file, output_dir, size=None, tokenizer_file=None, push_to_hub=False, model_name=None):\n    if tokenizer_file is None:\n        print('No `--tokenizer_file` provided, we will use the default tokenizer.')\n        vocab_size = 50277\n        tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n    else:\n        tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n        vocab_size = len(tokenizer)\n    tokenizer.save_pretrained(output_dir)\n    possible_sizes = list(NUM_HIDDEN_LAYERS_MAPPING.keys())\n    if size is None:\n        for candidate in possible_sizes:\n            if candidate in checkpoint_file:\n                size = candidate\n                break\n        if size is None:\n            raise ValueError('Could not infer the size, please provide it with the `--size` argument.')\n    if size not in possible_sizes:\n        raise ValueError(f'`size` should be one of {possible_sizes}, got {size}.')\n    config = RwkvConfig(vocab_size=vocab_size, num_hidden_layers=NUM_HIDDEN_LAYERS_MAPPING[size], hidden_size=HIDEN_SIZE_MAPPING[size])\n    config.save_pretrained(output_dir)\n    model_file = hf_hub_download(repo_id, checkpoint_file)\n    state_dict = torch.load(model_file, map_location='cpu')\n    state_dict = convert_state_dict(state_dict)\n    (shards, index) = shard_checkpoint(state_dict)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(output_dir, shard_file))\n    if index is not None:\n        save_index_file = os.path.join(output_dir, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(\"Cleaning up shards. This may error with an OOM error, it this is the case don't worry you still have converted the model.\")\n        shard_files = list(shards.keys())\n        del state_dict\n        del shards\n        gc.collect()\n        for shard_file in shard_files:\n            state_dict = torch.load(os.path.join(output_dir, shard_file))\n            torch.save({k: v.cpu().clone() for (k, v) in state_dict.items()}, os.path.join(output_dir, shard_file))\n    del state_dict\n    gc.collect()\n    if push_to_hub:\n        if model_name is None:\n            raise ValueError('Please provide a `model_name` to push the model to the Hub.')\n        model = AutoModelForCausalLM.from_pretrained(output_dir)\n        model.push_to_hub(model_name, max_shard_size='2GB')\n        tokenizer.push_to_hub(model_name)",
        "mutated": [
            "def convert_rmkv_checkpoint_to_hf_format(repo_id, checkpoint_file, output_dir, size=None, tokenizer_file=None, push_to_hub=False, model_name=None):\n    if False:\n        i = 10\n    if tokenizer_file is None:\n        print('No `--tokenizer_file` provided, we will use the default tokenizer.')\n        vocab_size = 50277\n        tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n    else:\n        tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n        vocab_size = len(tokenizer)\n    tokenizer.save_pretrained(output_dir)\n    possible_sizes = list(NUM_HIDDEN_LAYERS_MAPPING.keys())\n    if size is None:\n        for candidate in possible_sizes:\n            if candidate in checkpoint_file:\n                size = candidate\n                break\n        if size is None:\n            raise ValueError('Could not infer the size, please provide it with the `--size` argument.')\n    if size not in possible_sizes:\n        raise ValueError(f'`size` should be one of {possible_sizes}, got {size}.')\n    config = RwkvConfig(vocab_size=vocab_size, num_hidden_layers=NUM_HIDDEN_LAYERS_MAPPING[size], hidden_size=HIDEN_SIZE_MAPPING[size])\n    config.save_pretrained(output_dir)\n    model_file = hf_hub_download(repo_id, checkpoint_file)\n    state_dict = torch.load(model_file, map_location='cpu')\n    state_dict = convert_state_dict(state_dict)\n    (shards, index) = shard_checkpoint(state_dict)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(output_dir, shard_file))\n    if index is not None:\n        save_index_file = os.path.join(output_dir, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(\"Cleaning up shards. This may error with an OOM error, it this is the case don't worry you still have converted the model.\")\n        shard_files = list(shards.keys())\n        del state_dict\n        del shards\n        gc.collect()\n        for shard_file in shard_files:\n            state_dict = torch.load(os.path.join(output_dir, shard_file))\n            torch.save({k: v.cpu().clone() for (k, v) in state_dict.items()}, os.path.join(output_dir, shard_file))\n    del state_dict\n    gc.collect()\n    if push_to_hub:\n        if model_name is None:\n            raise ValueError('Please provide a `model_name` to push the model to the Hub.')\n        model = AutoModelForCausalLM.from_pretrained(output_dir)\n        model.push_to_hub(model_name, max_shard_size='2GB')\n        tokenizer.push_to_hub(model_name)",
            "def convert_rmkv_checkpoint_to_hf_format(repo_id, checkpoint_file, output_dir, size=None, tokenizer_file=None, push_to_hub=False, model_name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if tokenizer_file is None:\n        print('No `--tokenizer_file` provided, we will use the default tokenizer.')\n        vocab_size = 50277\n        tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n    else:\n        tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n        vocab_size = len(tokenizer)\n    tokenizer.save_pretrained(output_dir)\n    possible_sizes = list(NUM_HIDDEN_LAYERS_MAPPING.keys())\n    if size is None:\n        for candidate in possible_sizes:\n            if candidate in checkpoint_file:\n                size = candidate\n                break\n        if size is None:\n            raise ValueError('Could not infer the size, please provide it with the `--size` argument.')\n    if size not in possible_sizes:\n        raise ValueError(f'`size` should be one of {possible_sizes}, got {size}.')\n    config = RwkvConfig(vocab_size=vocab_size, num_hidden_layers=NUM_HIDDEN_LAYERS_MAPPING[size], hidden_size=HIDEN_SIZE_MAPPING[size])\n    config.save_pretrained(output_dir)\n    model_file = hf_hub_download(repo_id, checkpoint_file)\n    state_dict = torch.load(model_file, map_location='cpu')\n    state_dict = convert_state_dict(state_dict)\n    (shards, index) = shard_checkpoint(state_dict)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(output_dir, shard_file))\n    if index is not None:\n        save_index_file = os.path.join(output_dir, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(\"Cleaning up shards. This may error with an OOM error, it this is the case don't worry you still have converted the model.\")\n        shard_files = list(shards.keys())\n        del state_dict\n        del shards\n        gc.collect()\n        for shard_file in shard_files:\n            state_dict = torch.load(os.path.join(output_dir, shard_file))\n            torch.save({k: v.cpu().clone() for (k, v) in state_dict.items()}, os.path.join(output_dir, shard_file))\n    del state_dict\n    gc.collect()\n    if push_to_hub:\n        if model_name is None:\n            raise ValueError('Please provide a `model_name` to push the model to the Hub.')\n        model = AutoModelForCausalLM.from_pretrained(output_dir)\n        model.push_to_hub(model_name, max_shard_size='2GB')\n        tokenizer.push_to_hub(model_name)",
            "def convert_rmkv_checkpoint_to_hf_format(repo_id, checkpoint_file, output_dir, size=None, tokenizer_file=None, push_to_hub=False, model_name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if tokenizer_file is None:\n        print('No `--tokenizer_file` provided, we will use the default tokenizer.')\n        vocab_size = 50277\n        tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n    else:\n        tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n        vocab_size = len(tokenizer)\n    tokenizer.save_pretrained(output_dir)\n    possible_sizes = list(NUM_HIDDEN_LAYERS_MAPPING.keys())\n    if size is None:\n        for candidate in possible_sizes:\n            if candidate in checkpoint_file:\n                size = candidate\n                break\n        if size is None:\n            raise ValueError('Could not infer the size, please provide it with the `--size` argument.')\n    if size not in possible_sizes:\n        raise ValueError(f'`size` should be one of {possible_sizes}, got {size}.')\n    config = RwkvConfig(vocab_size=vocab_size, num_hidden_layers=NUM_HIDDEN_LAYERS_MAPPING[size], hidden_size=HIDEN_SIZE_MAPPING[size])\n    config.save_pretrained(output_dir)\n    model_file = hf_hub_download(repo_id, checkpoint_file)\n    state_dict = torch.load(model_file, map_location='cpu')\n    state_dict = convert_state_dict(state_dict)\n    (shards, index) = shard_checkpoint(state_dict)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(output_dir, shard_file))\n    if index is not None:\n        save_index_file = os.path.join(output_dir, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(\"Cleaning up shards. This may error with an OOM error, it this is the case don't worry you still have converted the model.\")\n        shard_files = list(shards.keys())\n        del state_dict\n        del shards\n        gc.collect()\n        for shard_file in shard_files:\n            state_dict = torch.load(os.path.join(output_dir, shard_file))\n            torch.save({k: v.cpu().clone() for (k, v) in state_dict.items()}, os.path.join(output_dir, shard_file))\n    del state_dict\n    gc.collect()\n    if push_to_hub:\n        if model_name is None:\n            raise ValueError('Please provide a `model_name` to push the model to the Hub.')\n        model = AutoModelForCausalLM.from_pretrained(output_dir)\n        model.push_to_hub(model_name, max_shard_size='2GB')\n        tokenizer.push_to_hub(model_name)",
            "def convert_rmkv_checkpoint_to_hf_format(repo_id, checkpoint_file, output_dir, size=None, tokenizer_file=None, push_to_hub=False, model_name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if tokenizer_file is None:\n        print('No `--tokenizer_file` provided, we will use the default tokenizer.')\n        vocab_size = 50277\n        tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n    else:\n        tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n        vocab_size = len(tokenizer)\n    tokenizer.save_pretrained(output_dir)\n    possible_sizes = list(NUM_HIDDEN_LAYERS_MAPPING.keys())\n    if size is None:\n        for candidate in possible_sizes:\n            if candidate in checkpoint_file:\n                size = candidate\n                break\n        if size is None:\n            raise ValueError('Could not infer the size, please provide it with the `--size` argument.')\n    if size not in possible_sizes:\n        raise ValueError(f'`size` should be one of {possible_sizes}, got {size}.')\n    config = RwkvConfig(vocab_size=vocab_size, num_hidden_layers=NUM_HIDDEN_LAYERS_MAPPING[size], hidden_size=HIDEN_SIZE_MAPPING[size])\n    config.save_pretrained(output_dir)\n    model_file = hf_hub_download(repo_id, checkpoint_file)\n    state_dict = torch.load(model_file, map_location='cpu')\n    state_dict = convert_state_dict(state_dict)\n    (shards, index) = shard_checkpoint(state_dict)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(output_dir, shard_file))\n    if index is not None:\n        save_index_file = os.path.join(output_dir, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(\"Cleaning up shards. This may error with an OOM error, it this is the case don't worry you still have converted the model.\")\n        shard_files = list(shards.keys())\n        del state_dict\n        del shards\n        gc.collect()\n        for shard_file in shard_files:\n            state_dict = torch.load(os.path.join(output_dir, shard_file))\n            torch.save({k: v.cpu().clone() for (k, v) in state_dict.items()}, os.path.join(output_dir, shard_file))\n    del state_dict\n    gc.collect()\n    if push_to_hub:\n        if model_name is None:\n            raise ValueError('Please provide a `model_name` to push the model to the Hub.')\n        model = AutoModelForCausalLM.from_pretrained(output_dir)\n        model.push_to_hub(model_name, max_shard_size='2GB')\n        tokenizer.push_to_hub(model_name)",
            "def convert_rmkv_checkpoint_to_hf_format(repo_id, checkpoint_file, output_dir, size=None, tokenizer_file=None, push_to_hub=False, model_name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if tokenizer_file is None:\n        print('No `--tokenizer_file` provided, we will use the default tokenizer.')\n        vocab_size = 50277\n        tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n    else:\n        tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n        vocab_size = len(tokenizer)\n    tokenizer.save_pretrained(output_dir)\n    possible_sizes = list(NUM_HIDDEN_LAYERS_MAPPING.keys())\n    if size is None:\n        for candidate in possible_sizes:\n            if candidate in checkpoint_file:\n                size = candidate\n                break\n        if size is None:\n            raise ValueError('Could not infer the size, please provide it with the `--size` argument.')\n    if size not in possible_sizes:\n        raise ValueError(f'`size` should be one of {possible_sizes}, got {size}.')\n    config = RwkvConfig(vocab_size=vocab_size, num_hidden_layers=NUM_HIDDEN_LAYERS_MAPPING[size], hidden_size=HIDEN_SIZE_MAPPING[size])\n    config.save_pretrained(output_dir)\n    model_file = hf_hub_download(repo_id, checkpoint_file)\n    state_dict = torch.load(model_file, map_location='cpu')\n    state_dict = convert_state_dict(state_dict)\n    (shards, index) = shard_checkpoint(state_dict)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(output_dir, shard_file))\n    if index is not None:\n        save_index_file = os.path.join(output_dir, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(\"Cleaning up shards. This may error with an OOM error, it this is the case don't worry you still have converted the model.\")\n        shard_files = list(shards.keys())\n        del state_dict\n        del shards\n        gc.collect()\n        for shard_file in shard_files:\n            state_dict = torch.load(os.path.join(output_dir, shard_file))\n            torch.save({k: v.cpu().clone() for (k, v) in state_dict.items()}, os.path.join(output_dir, shard_file))\n    del state_dict\n    gc.collect()\n    if push_to_hub:\n        if model_name is None:\n            raise ValueError('Please provide a `model_name` to push the model to the Hub.')\n        model = AutoModelForCausalLM.from_pretrained(output_dir)\n        model.push_to_hub(model_name, max_shard_size='2GB')\n        tokenizer.push_to_hub(model_name)"
        ]
    }
]