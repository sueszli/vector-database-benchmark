[
    {
        "func_name": "backward",
        "original": "def backward(outputs, grad_outputs=None, **kwargs):\n    \"\"\"backward(outputs, grad_outputs=None, *, enable_double_backprop=False)\n\n    Runs backpropagation from variables simultaneously.\n\n    .. warning::\n\n        This feature is experimental. The interface can change in the future.\n\n    Args:\n        outputs (tuple or list of :class:`~chainer.Variable`):\n            A sequence of output variables from which backprop starts.\n        grad_outputs (None or tuple or list of :class:`~chainer.Variable`):\n            A sequence of variables that gives the initial value of each output\n            gradient.\n            If this argument is ``None``, backprop uses\n            :attr:`~chainer.Variable.grad_var` of ``outputs``.\n        enable_double_backprop (bool): If ``True``,\n            computational trace of the whole backpropagation procedure is\n            recorded to the computational graph so that one can further do\n            backpropagation from the resulting gradients. Note that\n            enabling it results in larger memory consumption needed to\n            store the gradients w.r.t intermediate variables that are\n            required for the second gradient computation.\n\n    .. seealso::\n       :meth:`chainer.Variable.backward`\n       :func:`chainer.grad`\n\n    \"\"\"\n    (enable_double_backprop,) = argument.parse_kwargs(kwargs, ('enable_double_backprop', False), retain_grad='semantics for retain_grad=True is under discussion', loss_scale='chainer.backward does not support loss_scale option')\n    if not isinstance(outputs, (tuple, list)):\n        raise TypeError('outputs must be a tuple or a list, not {}.'.format(type(outputs)))\n    for v in outputs:\n        if not isinstance(v, chainer.Variable):\n            raise TypeError('each output must be a Variable, not {}'.format(type(v)))\n    if grad_outputs is not None:\n        if not isinstance(grad_outputs, (tuple, list)):\n            raise TypeError('grad_outputs must be None, a tuple, or a list, not {}.'.format(type(grad_outputs)))\n        if len(outputs) != len(grad_outputs):\n            raise ValueError('grad_outputs must be of the same length as outputs.\\nlen(outputs) = {}, len(grad_outputs) = {}'.format(len(outputs), len(grad_outputs)))\n    is_chainerx = [v._has_chainerx_array for v in outputs]\n    if any(is_chainerx):\n        if not all(is_chainerx):\n            raise ValueError('cannot mix chainerx and other backends')\n        if grad_outputs is None:\n            grad_outputs = []\n            for y in outputs:\n                grad_outputs.append(y.grad_var)\n                y.grad_var = None\n        indices = [i for (i, gy) in enumerate(grad_outputs) if gy is not None]\n        outputs = [outputs[i] for i in indices]\n        grad_outputs = [grad_outputs[i] for i in indices]\n        outputs = chainer.functions.identity(*outputs)\n        if not isinstance(outputs, tuple):\n            outputs = (outputs,)\n        grad_outputs = chainer.functions.identity(*grad_outputs)\n        if not isinstance(grad_outputs, tuple):\n            grad_outputs = (grad_outputs,)\n        outputs_ = []\n        for (y, gy) in zip(outputs, grad_outputs):\n            if not y.requires_grad and gy is not None:\n                warnings.warn('Some of grads are ignored by chainer.backward.\\nbackend: ChainerX, output.dtype: {}, grad_output.dtype: {}'.format(y.dtype, gy.dtype), RuntimeWarning)\n                continue\n            y.grad_var = gy\n            outputs_.append(y)\n        outputs = outputs_\n        del outputs_\n        arrs = []\n        for y in outputs:\n            arr = y._data[0]\n            assert isinstance(arr, chainerx.ndarray)\n            arrs.append(arr)\n        chainerx.backward(arrs, enable_double_backprop=enable_double_backprop)\n        return\n    if grad_outputs is None:\n        grad_outputs = []\n        for y in outputs:\n            grad_var = y.grad_var\n            if grad_var is None:\n                warnings.warn('outputs contains a Variable without grad, or duplicate outputs. Note that chainer.backward does not set default grad.', RuntimeWarning)\n            y.grad_var = None\n            grad_outputs.append(grad_var)\n    outputs = [(y.node, gy) for (y, gy) in zip(outputs, grad_outputs) if gy is not None]\n    with chainer.using_config('enable_backprop', enable_double_backprop):\n        _backprop_to_all(outputs, False, None)",
        "mutated": [
            "def backward(outputs, grad_outputs=None, **kwargs):\n    if False:\n        i = 10\n    'backward(outputs, grad_outputs=None, *, enable_double_backprop=False)\\n\\n    Runs backpropagation from variables simultaneously.\\n\\n    .. warning::\\n\\n        This feature is experimental. The interface can change in the future.\\n\\n    Args:\\n        outputs (tuple or list of :class:`~chainer.Variable`):\\n            A sequence of output variables from which backprop starts.\\n        grad_outputs (None or tuple or list of :class:`~chainer.Variable`):\\n            A sequence of variables that gives the initial value of each output\\n            gradient.\\n            If this argument is ``None``, backprop uses\\n            :attr:`~chainer.Variable.grad_var` of ``outputs``.\\n        enable_double_backprop (bool): If ``True``,\\n            computational trace of the whole backpropagation procedure is\\n            recorded to the computational graph so that one can further do\\n            backpropagation from the resulting gradients. Note that\\n            enabling it results in larger memory consumption needed to\\n            store the gradients w.r.t intermediate variables that are\\n            required for the second gradient computation.\\n\\n    .. seealso::\\n       :meth:`chainer.Variable.backward`\\n       :func:`chainer.grad`\\n\\n    '\n    (enable_double_backprop,) = argument.parse_kwargs(kwargs, ('enable_double_backprop', False), retain_grad='semantics for retain_grad=True is under discussion', loss_scale='chainer.backward does not support loss_scale option')\n    if not isinstance(outputs, (tuple, list)):\n        raise TypeError('outputs must be a tuple or a list, not {}.'.format(type(outputs)))\n    for v in outputs:\n        if not isinstance(v, chainer.Variable):\n            raise TypeError('each output must be a Variable, not {}'.format(type(v)))\n    if grad_outputs is not None:\n        if not isinstance(grad_outputs, (tuple, list)):\n            raise TypeError('grad_outputs must be None, a tuple, or a list, not {}.'.format(type(grad_outputs)))\n        if len(outputs) != len(grad_outputs):\n            raise ValueError('grad_outputs must be of the same length as outputs.\\nlen(outputs) = {}, len(grad_outputs) = {}'.format(len(outputs), len(grad_outputs)))\n    is_chainerx = [v._has_chainerx_array for v in outputs]\n    if any(is_chainerx):\n        if not all(is_chainerx):\n            raise ValueError('cannot mix chainerx and other backends')\n        if grad_outputs is None:\n            grad_outputs = []\n            for y in outputs:\n                grad_outputs.append(y.grad_var)\n                y.grad_var = None\n        indices = [i for (i, gy) in enumerate(grad_outputs) if gy is not None]\n        outputs = [outputs[i] for i in indices]\n        grad_outputs = [grad_outputs[i] for i in indices]\n        outputs = chainer.functions.identity(*outputs)\n        if not isinstance(outputs, tuple):\n            outputs = (outputs,)\n        grad_outputs = chainer.functions.identity(*grad_outputs)\n        if not isinstance(grad_outputs, tuple):\n            grad_outputs = (grad_outputs,)\n        outputs_ = []\n        for (y, gy) in zip(outputs, grad_outputs):\n            if not y.requires_grad and gy is not None:\n                warnings.warn('Some of grads are ignored by chainer.backward.\\nbackend: ChainerX, output.dtype: {}, grad_output.dtype: {}'.format(y.dtype, gy.dtype), RuntimeWarning)\n                continue\n            y.grad_var = gy\n            outputs_.append(y)\n        outputs = outputs_\n        del outputs_\n        arrs = []\n        for y in outputs:\n            arr = y._data[0]\n            assert isinstance(arr, chainerx.ndarray)\n            arrs.append(arr)\n        chainerx.backward(arrs, enable_double_backprop=enable_double_backprop)\n        return\n    if grad_outputs is None:\n        grad_outputs = []\n        for y in outputs:\n            grad_var = y.grad_var\n            if grad_var is None:\n                warnings.warn('outputs contains a Variable without grad, or duplicate outputs. Note that chainer.backward does not set default grad.', RuntimeWarning)\n            y.grad_var = None\n            grad_outputs.append(grad_var)\n    outputs = [(y.node, gy) for (y, gy) in zip(outputs, grad_outputs) if gy is not None]\n    with chainer.using_config('enable_backprop', enable_double_backprop):\n        _backprop_to_all(outputs, False, None)",
            "def backward(outputs, grad_outputs=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'backward(outputs, grad_outputs=None, *, enable_double_backprop=False)\\n\\n    Runs backpropagation from variables simultaneously.\\n\\n    .. warning::\\n\\n        This feature is experimental. The interface can change in the future.\\n\\n    Args:\\n        outputs (tuple or list of :class:`~chainer.Variable`):\\n            A sequence of output variables from which backprop starts.\\n        grad_outputs (None or tuple or list of :class:`~chainer.Variable`):\\n            A sequence of variables that gives the initial value of each output\\n            gradient.\\n            If this argument is ``None``, backprop uses\\n            :attr:`~chainer.Variable.grad_var` of ``outputs``.\\n        enable_double_backprop (bool): If ``True``,\\n            computational trace of the whole backpropagation procedure is\\n            recorded to the computational graph so that one can further do\\n            backpropagation from the resulting gradients. Note that\\n            enabling it results in larger memory consumption needed to\\n            store the gradients w.r.t intermediate variables that are\\n            required for the second gradient computation.\\n\\n    .. seealso::\\n       :meth:`chainer.Variable.backward`\\n       :func:`chainer.grad`\\n\\n    '\n    (enable_double_backprop,) = argument.parse_kwargs(kwargs, ('enable_double_backprop', False), retain_grad='semantics for retain_grad=True is under discussion', loss_scale='chainer.backward does not support loss_scale option')\n    if not isinstance(outputs, (tuple, list)):\n        raise TypeError('outputs must be a tuple or a list, not {}.'.format(type(outputs)))\n    for v in outputs:\n        if not isinstance(v, chainer.Variable):\n            raise TypeError('each output must be a Variable, not {}'.format(type(v)))\n    if grad_outputs is not None:\n        if not isinstance(grad_outputs, (tuple, list)):\n            raise TypeError('grad_outputs must be None, a tuple, or a list, not {}.'.format(type(grad_outputs)))\n        if len(outputs) != len(grad_outputs):\n            raise ValueError('grad_outputs must be of the same length as outputs.\\nlen(outputs) = {}, len(grad_outputs) = {}'.format(len(outputs), len(grad_outputs)))\n    is_chainerx = [v._has_chainerx_array for v in outputs]\n    if any(is_chainerx):\n        if not all(is_chainerx):\n            raise ValueError('cannot mix chainerx and other backends')\n        if grad_outputs is None:\n            grad_outputs = []\n            for y in outputs:\n                grad_outputs.append(y.grad_var)\n                y.grad_var = None\n        indices = [i for (i, gy) in enumerate(grad_outputs) if gy is not None]\n        outputs = [outputs[i] for i in indices]\n        grad_outputs = [grad_outputs[i] for i in indices]\n        outputs = chainer.functions.identity(*outputs)\n        if not isinstance(outputs, tuple):\n            outputs = (outputs,)\n        grad_outputs = chainer.functions.identity(*grad_outputs)\n        if not isinstance(grad_outputs, tuple):\n            grad_outputs = (grad_outputs,)\n        outputs_ = []\n        for (y, gy) in zip(outputs, grad_outputs):\n            if not y.requires_grad and gy is not None:\n                warnings.warn('Some of grads are ignored by chainer.backward.\\nbackend: ChainerX, output.dtype: {}, grad_output.dtype: {}'.format(y.dtype, gy.dtype), RuntimeWarning)\n                continue\n            y.grad_var = gy\n            outputs_.append(y)\n        outputs = outputs_\n        del outputs_\n        arrs = []\n        for y in outputs:\n            arr = y._data[0]\n            assert isinstance(arr, chainerx.ndarray)\n            arrs.append(arr)\n        chainerx.backward(arrs, enable_double_backprop=enable_double_backprop)\n        return\n    if grad_outputs is None:\n        grad_outputs = []\n        for y in outputs:\n            grad_var = y.grad_var\n            if grad_var is None:\n                warnings.warn('outputs contains a Variable without grad, or duplicate outputs. Note that chainer.backward does not set default grad.', RuntimeWarning)\n            y.grad_var = None\n            grad_outputs.append(grad_var)\n    outputs = [(y.node, gy) for (y, gy) in zip(outputs, grad_outputs) if gy is not None]\n    with chainer.using_config('enable_backprop', enable_double_backprop):\n        _backprop_to_all(outputs, False, None)",
            "def backward(outputs, grad_outputs=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'backward(outputs, grad_outputs=None, *, enable_double_backprop=False)\\n\\n    Runs backpropagation from variables simultaneously.\\n\\n    .. warning::\\n\\n        This feature is experimental. The interface can change in the future.\\n\\n    Args:\\n        outputs (tuple or list of :class:`~chainer.Variable`):\\n            A sequence of output variables from which backprop starts.\\n        grad_outputs (None or tuple or list of :class:`~chainer.Variable`):\\n            A sequence of variables that gives the initial value of each output\\n            gradient.\\n            If this argument is ``None``, backprop uses\\n            :attr:`~chainer.Variable.grad_var` of ``outputs``.\\n        enable_double_backprop (bool): If ``True``,\\n            computational trace of the whole backpropagation procedure is\\n            recorded to the computational graph so that one can further do\\n            backpropagation from the resulting gradients. Note that\\n            enabling it results in larger memory consumption needed to\\n            store the gradients w.r.t intermediate variables that are\\n            required for the second gradient computation.\\n\\n    .. seealso::\\n       :meth:`chainer.Variable.backward`\\n       :func:`chainer.grad`\\n\\n    '\n    (enable_double_backprop,) = argument.parse_kwargs(kwargs, ('enable_double_backprop', False), retain_grad='semantics for retain_grad=True is under discussion', loss_scale='chainer.backward does not support loss_scale option')\n    if not isinstance(outputs, (tuple, list)):\n        raise TypeError('outputs must be a tuple or a list, not {}.'.format(type(outputs)))\n    for v in outputs:\n        if not isinstance(v, chainer.Variable):\n            raise TypeError('each output must be a Variable, not {}'.format(type(v)))\n    if grad_outputs is not None:\n        if not isinstance(grad_outputs, (tuple, list)):\n            raise TypeError('grad_outputs must be None, a tuple, or a list, not {}.'.format(type(grad_outputs)))\n        if len(outputs) != len(grad_outputs):\n            raise ValueError('grad_outputs must be of the same length as outputs.\\nlen(outputs) = {}, len(grad_outputs) = {}'.format(len(outputs), len(grad_outputs)))\n    is_chainerx = [v._has_chainerx_array for v in outputs]\n    if any(is_chainerx):\n        if not all(is_chainerx):\n            raise ValueError('cannot mix chainerx and other backends')\n        if grad_outputs is None:\n            grad_outputs = []\n            for y in outputs:\n                grad_outputs.append(y.grad_var)\n                y.grad_var = None\n        indices = [i for (i, gy) in enumerate(grad_outputs) if gy is not None]\n        outputs = [outputs[i] for i in indices]\n        grad_outputs = [grad_outputs[i] for i in indices]\n        outputs = chainer.functions.identity(*outputs)\n        if not isinstance(outputs, tuple):\n            outputs = (outputs,)\n        grad_outputs = chainer.functions.identity(*grad_outputs)\n        if not isinstance(grad_outputs, tuple):\n            grad_outputs = (grad_outputs,)\n        outputs_ = []\n        for (y, gy) in zip(outputs, grad_outputs):\n            if not y.requires_grad and gy is not None:\n                warnings.warn('Some of grads are ignored by chainer.backward.\\nbackend: ChainerX, output.dtype: {}, grad_output.dtype: {}'.format(y.dtype, gy.dtype), RuntimeWarning)\n                continue\n            y.grad_var = gy\n            outputs_.append(y)\n        outputs = outputs_\n        del outputs_\n        arrs = []\n        for y in outputs:\n            arr = y._data[0]\n            assert isinstance(arr, chainerx.ndarray)\n            arrs.append(arr)\n        chainerx.backward(arrs, enable_double_backprop=enable_double_backprop)\n        return\n    if grad_outputs is None:\n        grad_outputs = []\n        for y in outputs:\n            grad_var = y.grad_var\n            if grad_var is None:\n                warnings.warn('outputs contains a Variable without grad, or duplicate outputs. Note that chainer.backward does not set default grad.', RuntimeWarning)\n            y.grad_var = None\n            grad_outputs.append(grad_var)\n    outputs = [(y.node, gy) for (y, gy) in zip(outputs, grad_outputs) if gy is not None]\n    with chainer.using_config('enable_backprop', enable_double_backprop):\n        _backprop_to_all(outputs, False, None)",
            "def backward(outputs, grad_outputs=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'backward(outputs, grad_outputs=None, *, enable_double_backprop=False)\\n\\n    Runs backpropagation from variables simultaneously.\\n\\n    .. warning::\\n\\n        This feature is experimental. The interface can change in the future.\\n\\n    Args:\\n        outputs (tuple or list of :class:`~chainer.Variable`):\\n            A sequence of output variables from which backprop starts.\\n        grad_outputs (None or tuple or list of :class:`~chainer.Variable`):\\n            A sequence of variables that gives the initial value of each output\\n            gradient.\\n            If this argument is ``None``, backprop uses\\n            :attr:`~chainer.Variable.grad_var` of ``outputs``.\\n        enable_double_backprop (bool): If ``True``,\\n            computational trace of the whole backpropagation procedure is\\n            recorded to the computational graph so that one can further do\\n            backpropagation from the resulting gradients. Note that\\n            enabling it results in larger memory consumption needed to\\n            store the gradients w.r.t intermediate variables that are\\n            required for the second gradient computation.\\n\\n    .. seealso::\\n       :meth:`chainer.Variable.backward`\\n       :func:`chainer.grad`\\n\\n    '\n    (enable_double_backprop,) = argument.parse_kwargs(kwargs, ('enable_double_backprop', False), retain_grad='semantics for retain_grad=True is under discussion', loss_scale='chainer.backward does not support loss_scale option')\n    if not isinstance(outputs, (tuple, list)):\n        raise TypeError('outputs must be a tuple or a list, not {}.'.format(type(outputs)))\n    for v in outputs:\n        if not isinstance(v, chainer.Variable):\n            raise TypeError('each output must be a Variable, not {}'.format(type(v)))\n    if grad_outputs is not None:\n        if not isinstance(grad_outputs, (tuple, list)):\n            raise TypeError('grad_outputs must be None, a tuple, or a list, not {}.'.format(type(grad_outputs)))\n        if len(outputs) != len(grad_outputs):\n            raise ValueError('grad_outputs must be of the same length as outputs.\\nlen(outputs) = {}, len(grad_outputs) = {}'.format(len(outputs), len(grad_outputs)))\n    is_chainerx = [v._has_chainerx_array for v in outputs]\n    if any(is_chainerx):\n        if not all(is_chainerx):\n            raise ValueError('cannot mix chainerx and other backends')\n        if grad_outputs is None:\n            grad_outputs = []\n            for y in outputs:\n                grad_outputs.append(y.grad_var)\n                y.grad_var = None\n        indices = [i for (i, gy) in enumerate(grad_outputs) if gy is not None]\n        outputs = [outputs[i] for i in indices]\n        grad_outputs = [grad_outputs[i] for i in indices]\n        outputs = chainer.functions.identity(*outputs)\n        if not isinstance(outputs, tuple):\n            outputs = (outputs,)\n        grad_outputs = chainer.functions.identity(*grad_outputs)\n        if not isinstance(grad_outputs, tuple):\n            grad_outputs = (grad_outputs,)\n        outputs_ = []\n        for (y, gy) in zip(outputs, grad_outputs):\n            if not y.requires_grad and gy is not None:\n                warnings.warn('Some of grads are ignored by chainer.backward.\\nbackend: ChainerX, output.dtype: {}, grad_output.dtype: {}'.format(y.dtype, gy.dtype), RuntimeWarning)\n                continue\n            y.grad_var = gy\n            outputs_.append(y)\n        outputs = outputs_\n        del outputs_\n        arrs = []\n        for y in outputs:\n            arr = y._data[0]\n            assert isinstance(arr, chainerx.ndarray)\n            arrs.append(arr)\n        chainerx.backward(arrs, enable_double_backprop=enable_double_backprop)\n        return\n    if grad_outputs is None:\n        grad_outputs = []\n        for y in outputs:\n            grad_var = y.grad_var\n            if grad_var is None:\n                warnings.warn('outputs contains a Variable without grad, or duplicate outputs. Note that chainer.backward does not set default grad.', RuntimeWarning)\n            y.grad_var = None\n            grad_outputs.append(grad_var)\n    outputs = [(y.node, gy) for (y, gy) in zip(outputs, grad_outputs) if gy is not None]\n    with chainer.using_config('enable_backprop', enable_double_backprop):\n        _backprop_to_all(outputs, False, None)",
            "def backward(outputs, grad_outputs=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'backward(outputs, grad_outputs=None, *, enable_double_backprop=False)\\n\\n    Runs backpropagation from variables simultaneously.\\n\\n    .. warning::\\n\\n        This feature is experimental. The interface can change in the future.\\n\\n    Args:\\n        outputs (tuple or list of :class:`~chainer.Variable`):\\n            A sequence of output variables from which backprop starts.\\n        grad_outputs (None or tuple or list of :class:`~chainer.Variable`):\\n            A sequence of variables that gives the initial value of each output\\n            gradient.\\n            If this argument is ``None``, backprop uses\\n            :attr:`~chainer.Variable.grad_var` of ``outputs``.\\n        enable_double_backprop (bool): If ``True``,\\n            computational trace of the whole backpropagation procedure is\\n            recorded to the computational graph so that one can further do\\n            backpropagation from the resulting gradients. Note that\\n            enabling it results in larger memory consumption needed to\\n            store the gradients w.r.t intermediate variables that are\\n            required for the second gradient computation.\\n\\n    .. seealso::\\n       :meth:`chainer.Variable.backward`\\n       :func:`chainer.grad`\\n\\n    '\n    (enable_double_backprop,) = argument.parse_kwargs(kwargs, ('enable_double_backprop', False), retain_grad='semantics for retain_grad=True is under discussion', loss_scale='chainer.backward does not support loss_scale option')\n    if not isinstance(outputs, (tuple, list)):\n        raise TypeError('outputs must be a tuple or a list, not {}.'.format(type(outputs)))\n    for v in outputs:\n        if not isinstance(v, chainer.Variable):\n            raise TypeError('each output must be a Variable, not {}'.format(type(v)))\n    if grad_outputs is not None:\n        if not isinstance(grad_outputs, (tuple, list)):\n            raise TypeError('grad_outputs must be None, a tuple, or a list, not {}.'.format(type(grad_outputs)))\n        if len(outputs) != len(grad_outputs):\n            raise ValueError('grad_outputs must be of the same length as outputs.\\nlen(outputs) = {}, len(grad_outputs) = {}'.format(len(outputs), len(grad_outputs)))\n    is_chainerx = [v._has_chainerx_array for v in outputs]\n    if any(is_chainerx):\n        if not all(is_chainerx):\n            raise ValueError('cannot mix chainerx and other backends')\n        if grad_outputs is None:\n            grad_outputs = []\n            for y in outputs:\n                grad_outputs.append(y.grad_var)\n                y.grad_var = None\n        indices = [i for (i, gy) in enumerate(grad_outputs) if gy is not None]\n        outputs = [outputs[i] for i in indices]\n        grad_outputs = [grad_outputs[i] for i in indices]\n        outputs = chainer.functions.identity(*outputs)\n        if not isinstance(outputs, tuple):\n            outputs = (outputs,)\n        grad_outputs = chainer.functions.identity(*grad_outputs)\n        if not isinstance(grad_outputs, tuple):\n            grad_outputs = (grad_outputs,)\n        outputs_ = []\n        for (y, gy) in zip(outputs, grad_outputs):\n            if not y.requires_grad and gy is not None:\n                warnings.warn('Some of grads are ignored by chainer.backward.\\nbackend: ChainerX, output.dtype: {}, grad_output.dtype: {}'.format(y.dtype, gy.dtype), RuntimeWarning)\n                continue\n            y.grad_var = gy\n            outputs_.append(y)\n        outputs = outputs_\n        del outputs_\n        arrs = []\n        for y in outputs:\n            arr = y._data[0]\n            assert isinstance(arr, chainerx.ndarray)\n            arrs.append(arr)\n        chainerx.backward(arrs, enable_double_backprop=enable_double_backprop)\n        return\n    if grad_outputs is None:\n        grad_outputs = []\n        for y in outputs:\n            grad_var = y.grad_var\n            if grad_var is None:\n                warnings.warn('outputs contains a Variable without grad, or duplicate outputs. Note that chainer.backward does not set default grad.', RuntimeWarning)\n            y.grad_var = None\n            grad_outputs.append(grad_var)\n    outputs = [(y.node, gy) for (y, gy) in zip(outputs, grad_outputs) if gy is not None]\n    with chainer.using_config('enable_backprop', enable_double_backprop):\n        _backprop_to_all(outputs, False, None)"
        ]
    },
    {
        "func_name": "add_cand",
        "original": "def add_cand(cand):\n    if cand not in seen_set:\n        heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n        seen_set.add(cand)",
        "mutated": [
            "def add_cand(cand):\n    if False:\n        i = 10\n    if cand not in seen_set:\n        heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n        seen_set.add(cand)",
            "def add_cand(cand):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if cand not in seen_set:\n        heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n        seen_set.add(cand)",
            "def add_cand(cand):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if cand not in seen_set:\n        heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n        seen_set.add(cand)",
            "def add_cand(cand):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if cand not in seen_set:\n        heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n        seen_set.add(cand)",
            "def add_cand(cand):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if cand not in seen_set:\n        heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n        seen_set.add(cand)"
        ]
    },
    {
        "func_name": "_backprop_to_all",
        "original": "def _backprop_to_all(outputs, retain_grad, loss_scale):\n    \"\"\"Backprop to all input variables\n\n    Args:\n        outputs (list of tuple): each tuple is (y_node, y_grad_var).\n            y_grad_var should not be None.\n        retain_grad (bool): see docstring of Variable.backward\n        loss_scale (float): see docstring of Variable.backward\n\n    \"\"\"\n    OrderedDict = chainer.utils._collections.OrderedDict\n    cand_funcs = []\n    seen_set = set()\n\n    def add_cand(cand):\n        if cand not in seen_set:\n            heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n            seen_set.add(cand)\n    grads = _backprop_utils.GradTable(accumulate_grad_inputs=True)\n    leaf_nodes = set()\n    for (y, gy) in outputs:\n        grads.accumulate(y, gy)\n        func = y.creator_node\n        if func is None:\n            leaf_nodes.add(y)\n        else:\n            add_cand(func)\n    y = None\n    del y\n    is_debug = chainer.is_debug()\n    base_hooks = chainer.get_function_hooks().values()\n    while cand_funcs:\n        (_, _, func) = heapq.heappop(cand_funcs)\n        inputs = func.inputs\n        target_input_indexes = tuple([i for (i, x) in enumerate(inputs) if x.requires_grad])\n        outputs = [y() for y in func.outputs]\n        out_grad = tuple([grads.pop(y) if y is not None and y.creator_node is not None else None for y in outputs])\n        if not target_input_indexes:\n            continue\n        in_data = [x.data for x in inputs]\n        out_grad_array = [None if g is None else g.raw_array for g in out_grad]\n        if func._n_local_function_hooks != 0:\n            local_hooks = collections.OrderedDict(chainer.get_function_hooks())\n            local_hooks.update(func.local_function_hooks)\n            hooks = local_hooks.values()\n        else:\n            hooks = base_hooks\n        with chainer.using_device(backend.get_device_from_array(*in_data + out_grad_array)):\n            for hook in hooks:\n                hook.backward_preprocess(func, tuple(in_data), tuple(out_grad_array))\n            target_inputs = [inputs[i] for i in target_input_indexes]\n            in_grad = OrderedDict()\n            for x in target_inputs:\n                if x not in in_grad:\n                    in_grad[x] = grads.get_as_list(x)\n            _backprop_utils.backprop_step(func, target_input_indexes, out_grad, in_grad, is_debug)\n            for hook in hooks:\n                hook.backward_postprocess(func, tuple(in_data), tuple(out_grad_array))\n        if retain_grad:\n            for (y, gy) in six.moves.zip(outputs, out_grad):\n                if y is not None:\n                    y._set_grad_var_if_available(gy)\n            del gy\n        del out_grad\n        for (x, gx) in in_grad.items():\n            if not gx:\n                continue\n            for gx_elem in gx:\n                if gx_elem is not None:\n                    chainer.variable._check_grad_type(func, x, True, gx_elem.raw_array)\n            del gx_elem\n            if x.creator_node is None:\n                leaf_nodes.add(x)\n            else:\n                add_cand(x.creator_node)\n        del gx, in_grad\n    for x in leaf_nodes:\n        x_var = x.get_variable_or_none()\n        gx = grads.pop(x)\n        if x_var is not None:\n            x_var._set_grad_var_without_check(gx)\n            x_var._loss_scale = loss_scale\n    grads.assert_no_grads()",
        "mutated": [
            "def _backprop_to_all(outputs, retain_grad, loss_scale):\n    if False:\n        i = 10\n    'Backprop to all input variables\\n\\n    Args:\\n        outputs (list of tuple): each tuple is (y_node, y_grad_var).\\n            y_grad_var should not be None.\\n        retain_grad (bool): see docstring of Variable.backward\\n        loss_scale (float): see docstring of Variable.backward\\n\\n    '\n    OrderedDict = chainer.utils._collections.OrderedDict\n    cand_funcs = []\n    seen_set = set()\n\n    def add_cand(cand):\n        if cand not in seen_set:\n            heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n            seen_set.add(cand)\n    grads = _backprop_utils.GradTable(accumulate_grad_inputs=True)\n    leaf_nodes = set()\n    for (y, gy) in outputs:\n        grads.accumulate(y, gy)\n        func = y.creator_node\n        if func is None:\n            leaf_nodes.add(y)\n        else:\n            add_cand(func)\n    y = None\n    del y\n    is_debug = chainer.is_debug()\n    base_hooks = chainer.get_function_hooks().values()\n    while cand_funcs:\n        (_, _, func) = heapq.heappop(cand_funcs)\n        inputs = func.inputs\n        target_input_indexes = tuple([i for (i, x) in enumerate(inputs) if x.requires_grad])\n        outputs = [y() for y in func.outputs]\n        out_grad = tuple([grads.pop(y) if y is not None and y.creator_node is not None else None for y in outputs])\n        if not target_input_indexes:\n            continue\n        in_data = [x.data for x in inputs]\n        out_grad_array = [None if g is None else g.raw_array for g in out_grad]\n        if func._n_local_function_hooks != 0:\n            local_hooks = collections.OrderedDict(chainer.get_function_hooks())\n            local_hooks.update(func.local_function_hooks)\n            hooks = local_hooks.values()\n        else:\n            hooks = base_hooks\n        with chainer.using_device(backend.get_device_from_array(*in_data + out_grad_array)):\n            for hook in hooks:\n                hook.backward_preprocess(func, tuple(in_data), tuple(out_grad_array))\n            target_inputs = [inputs[i] for i in target_input_indexes]\n            in_grad = OrderedDict()\n            for x in target_inputs:\n                if x not in in_grad:\n                    in_grad[x] = grads.get_as_list(x)\n            _backprop_utils.backprop_step(func, target_input_indexes, out_grad, in_grad, is_debug)\n            for hook in hooks:\n                hook.backward_postprocess(func, tuple(in_data), tuple(out_grad_array))\n        if retain_grad:\n            for (y, gy) in six.moves.zip(outputs, out_grad):\n                if y is not None:\n                    y._set_grad_var_if_available(gy)\n            del gy\n        del out_grad\n        for (x, gx) in in_grad.items():\n            if not gx:\n                continue\n            for gx_elem in gx:\n                if gx_elem is not None:\n                    chainer.variable._check_grad_type(func, x, True, gx_elem.raw_array)\n            del gx_elem\n            if x.creator_node is None:\n                leaf_nodes.add(x)\n            else:\n                add_cand(x.creator_node)\n        del gx, in_grad\n    for x in leaf_nodes:\n        x_var = x.get_variable_or_none()\n        gx = grads.pop(x)\n        if x_var is not None:\n            x_var._set_grad_var_without_check(gx)\n            x_var._loss_scale = loss_scale\n    grads.assert_no_grads()",
            "def _backprop_to_all(outputs, retain_grad, loss_scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Backprop to all input variables\\n\\n    Args:\\n        outputs (list of tuple): each tuple is (y_node, y_grad_var).\\n            y_grad_var should not be None.\\n        retain_grad (bool): see docstring of Variable.backward\\n        loss_scale (float): see docstring of Variable.backward\\n\\n    '\n    OrderedDict = chainer.utils._collections.OrderedDict\n    cand_funcs = []\n    seen_set = set()\n\n    def add_cand(cand):\n        if cand not in seen_set:\n            heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n            seen_set.add(cand)\n    grads = _backprop_utils.GradTable(accumulate_grad_inputs=True)\n    leaf_nodes = set()\n    for (y, gy) in outputs:\n        grads.accumulate(y, gy)\n        func = y.creator_node\n        if func is None:\n            leaf_nodes.add(y)\n        else:\n            add_cand(func)\n    y = None\n    del y\n    is_debug = chainer.is_debug()\n    base_hooks = chainer.get_function_hooks().values()\n    while cand_funcs:\n        (_, _, func) = heapq.heappop(cand_funcs)\n        inputs = func.inputs\n        target_input_indexes = tuple([i for (i, x) in enumerate(inputs) if x.requires_grad])\n        outputs = [y() for y in func.outputs]\n        out_grad = tuple([grads.pop(y) if y is not None and y.creator_node is not None else None for y in outputs])\n        if not target_input_indexes:\n            continue\n        in_data = [x.data for x in inputs]\n        out_grad_array = [None if g is None else g.raw_array for g in out_grad]\n        if func._n_local_function_hooks != 0:\n            local_hooks = collections.OrderedDict(chainer.get_function_hooks())\n            local_hooks.update(func.local_function_hooks)\n            hooks = local_hooks.values()\n        else:\n            hooks = base_hooks\n        with chainer.using_device(backend.get_device_from_array(*in_data + out_grad_array)):\n            for hook in hooks:\n                hook.backward_preprocess(func, tuple(in_data), tuple(out_grad_array))\n            target_inputs = [inputs[i] for i in target_input_indexes]\n            in_grad = OrderedDict()\n            for x in target_inputs:\n                if x not in in_grad:\n                    in_grad[x] = grads.get_as_list(x)\n            _backprop_utils.backprop_step(func, target_input_indexes, out_grad, in_grad, is_debug)\n            for hook in hooks:\n                hook.backward_postprocess(func, tuple(in_data), tuple(out_grad_array))\n        if retain_grad:\n            for (y, gy) in six.moves.zip(outputs, out_grad):\n                if y is not None:\n                    y._set_grad_var_if_available(gy)\n            del gy\n        del out_grad\n        for (x, gx) in in_grad.items():\n            if not gx:\n                continue\n            for gx_elem in gx:\n                if gx_elem is not None:\n                    chainer.variable._check_grad_type(func, x, True, gx_elem.raw_array)\n            del gx_elem\n            if x.creator_node is None:\n                leaf_nodes.add(x)\n            else:\n                add_cand(x.creator_node)\n        del gx, in_grad\n    for x in leaf_nodes:\n        x_var = x.get_variable_or_none()\n        gx = grads.pop(x)\n        if x_var is not None:\n            x_var._set_grad_var_without_check(gx)\n            x_var._loss_scale = loss_scale\n    grads.assert_no_grads()",
            "def _backprop_to_all(outputs, retain_grad, loss_scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Backprop to all input variables\\n\\n    Args:\\n        outputs (list of tuple): each tuple is (y_node, y_grad_var).\\n            y_grad_var should not be None.\\n        retain_grad (bool): see docstring of Variable.backward\\n        loss_scale (float): see docstring of Variable.backward\\n\\n    '\n    OrderedDict = chainer.utils._collections.OrderedDict\n    cand_funcs = []\n    seen_set = set()\n\n    def add_cand(cand):\n        if cand not in seen_set:\n            heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n            seen_set.add(cand)\n    grads = _backprop_utils.GradTable(accumulate_grad_inputs=True)\n    leaf_nodes = set()\n    for (y, gy) in outputs:\n        grads.accumulate(y, gy)\n        func = y.creator_node\n        if func is None:\n            leaf_nodes.add(y)\n        else:\n            add_cand(func)\n    y = None\n    del y\n    is_debug = chainer.is_debug()\n    base_hooks = chainer.get_function_hooks().values()\n    while cand_funcs:\n        (_, _, func) = heapq.heappop(cand_funcs)\n        inputs = func.inputs\n        target_input_indexes = tuple([i for (i, x) in enumerate(inputs) if x.requires_grad])\n        outputs = [y() for y in func.outputs]\n        out_grad = tuple([grads.pop(y) if y is not None and y.creator_node is not None else None for y in outputs])\n        if not target_input_indexes:\n            continue\n        in_data = [x.data for x in inputs]\n        out_grad_array = [None if g is None else g.raw_array for g in out_grad]\n        if func._n_local_function_hooks != 0:\n            local_hooks = collections.OrderedDict(chainer.get_function_hooks())\n            local_hooks.update(func.local_function_hooks)\n            hooks = local_hooks.values()\n        else:\n            hooks = base_hooks\n        with chainer.using_device(backend.get_device_from_array(*in_data + out_grad_array)):\n            for hook in hooks:\n                hook.backward_preprocess(func, tuple(in_data), tuple(out_grad_array))\n            target_inputs = [inputs[i] for i in target_input_indexes]\n            in_grad = OrderedDict()\n            for x in target_inputs:\n                if x not in in_grad:\n                    in_grad[x] = grads.get_as_list(x)\n            _backprop_utils.backprop_step(func, target_input_indexes, out_grad, in_grad, is_debug)\n            for hook in hooks:\n                hook.backward_postprocess(func, tuple(in_data), tuple(out_grad_array))\n        if retain_grad:\n            for (y, gy) in six.moves.zip(outputs, out_grad):\n                if y is not None:\n                    y._set_grad_var_if_available(gy)\n            del gy\n        del out_grad\n        for (x, gx) in in_grad.items():\n            if not gx:\n                continue\n            for gx_elem in gx:\n                if gx_elem is not None:\n                    chainer.variable._check_grad_type(func, x, True, gx_elem.raw_array)\n            del gx_elem\n            if x.creator_node is None:\n                leaf_nodes.add(x)\n            else:\n                add_cand(x.creator_node)\n        del gx, in_grad\n    for x in leaf_nodes:\n        x_var = x.get_variable_or_none()\n        gx = grads.pop(x)\n        if x_var is not None:\n            x_var._set_grad_var_without_check(gx)\n            x_var._loss_scale = loss_scale\n    grads.assert_no_grads()",
            "def _backprop_to_all(outputs, retain_grad, loss_scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Backprop to all input variables\\n\\n    Args:\\n        outputs (list of tuple): each tuple is (y_node, y_grad_var).\\n            y_grad_var should not be None.\\n        retain_grad (bool): see docstring of Variable.backward\\n        loss_scale (float): see docstring of Variable.backward\\n\\n    '\n    OrderedDict = chainer.utils._collections.OrderedDict\n    cand_funcs = []\n    seen_set = set()\n\n    def add_cand(cand):\n        if cand not in seen_set:\n            heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n            seen_set.add(cand)\n    grads = _backprop_utils.GradTable(accumulate_grad_inputs=True)\n    leaf_nodes = set()\n    for (y, gy) in outputs:\n        grads.accumulate(y, gy)\n        func = y.creator_node\n        if func is None:\n            leaf_nodes.add(y)\n        else:\n            add_cand(func)\n    y = None\n    del y\n    is_debug = chainer.is_debug()\n    base_hooks = chainer.get_function_hooks().values()\n    while cand_funcs:\n        (_, _, func) = heapq.heappop(cand_funcs)\n        inputs = func.inputs\n        target_input_indexes = tuple([i for (i, x) in enumerate(inputs) if x.requires_grad])\n        outputs = [y() for y in func.outputs]\n        out_grad = tuple([grads.pop(y) if y is not None and y.creator_node is not None else None for y in outputs])\n        if not target_input_indexes:\n            continue\n        in_data = [x.data for x in inputs]\n        out_grad_array = [None if g is None else g.raw_array for g in out_grad]\n        if func._n_local_function_hooks != 0:\n            local_hooks = collections.OrderedDict(chainer.get_function_hooks())\n            local_hooks.update(func.local_function_hooks)\n            hooks = local_hooks.values()\n        else:\n            hooks = base_hooks\n        with chainer.using_device(backend.get_device_from_array(*in_data + out_grad_array)):\n            for hook in hooks:\n                hook.backward_preprocess(func, tuple(in_data), tuple(out_grad_array))\n            target_inputs = [inputs[i] for i in target_input_indexes]\n            in_grad = OrderedDict()\n            for x in target_inputs:\n                if x not in in_grad:\n                    in_grad[x] = grads.get_as_list(x)\n            _backprop_utils.backprop_step(func, target_input_indexes, out_grad, in_grad, is_debug)\n            for hook in hooks:\n                hook.backward_postprocess(func, tuple(in_data), tuple(out_grad_array))\n        if retain_grad:\n            for (y, gy) in six.moves.zip(outputs, out_grad):\n                if y is not None:\n                    y._set_grad_var_if_available(gy)\n            del gy\n        del out_grad\n        for (x, gx) in in_grad.items():\n            if not gx:\n                continue\n            for gx_elem in gx:\n                if gx_elem is not None:\n                    chainer.variable._check_grad_type(func, x, True, gx_elem.raw_array)\n            del gx_elem\n            if x.creator_node is None:\n                leaf_nodes.add(x)\n            else:\n                add_cand(x.creator_node)\n        del gx, in_grad\n    for x in leaf_nodes:\n        x_var = x.get_variable_or_none()\n        gx = grads.pop(x)\n        if x_var is not None:\n            x_var._set_grad_var_without_check(gx)\n            x_var._loss_scale = loss_scale\n    grads.assert_no_grads()",
            "def _backprop_to_all(outputs, retain_grad, loss_scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Backprop to all input variables\\n\\n    Args:\\n        outputs (list of tuple): each tuple is (y_node, y_grad_var).\\n            y_grad_var should not be None.\\n        retain_grad (bool): see docstring of Variable.backward\\n        loss_scale (float): see docstring of Variable.backward\\n\\n    '\n    OrderedDict = chainer.utils._collections.OrderedDict\n    cand_funcs = []\n    seen_set = set()\n\n    def add_cand(cand):\n        if cand not in seen_set:\n            heapq.heappush(cand_funcs, (-cand.rank, len(seen_set), cand))\n            seen_set.add(cand)\n    grads = _backprop_utils.GradTable(accumulate_grad_inputs=True)\n    leaf_nodes = set()\n    for (y, gy) in outputs:\n        grads.accumulate(y, gy)\n        func = y.creator_node\n        if func is None:\n            leaf_nodes.add(y)\n        else:\n            add_cand(func)\n    y = None\n    del y\n    is_debug = chainer.is_debug()\n    base_hooks = chainer.get_function_hooks().values()\n    while cand_funcs:\n        (_, _, func) = heapq.heappop(cand_funcs)\n        inputs = func.inputs\n        target_input_indexes = tuple([i for (i, x) in enumerate(inputs) if x.requires_grad])\n        outputs = [y() for y in func.outputs]\n        out_grad = tuple([grads.pop(y) if y is not None and y.creator_node is not None else None for y in outputs])\n        if not target_input_indexes:\n            continue\n        in_data = [x.data for x in inputs]\n        out_grad_array = [None if g is None else g.raw_array for g in out_grad]\n        if func._n_local_function_hooks != 0:\n            local_hooks = collections.OrderedDict(chainer.get_function_hooks())\n            local_hooks.update(func.local_function_hooks)\n            hooks = local_hooks.values()\n        else:\n            hooks = base_hooks\n        with chainer.using_device(backend.get_device_from_array(*in_data + out_grad_array)):\n            for hook in hooks:\n                hook.backward_preprocess(func, tuple(in_data), tuple(out_grad_array))\n            target_inputs = [inputs[i] for i in target_input_indexes]\n            in_grad = OrderedDict()\n            for x in target_inputs:\n                if x not in in_grad:\n                    in_grad[x] = grads.get_as_list(x)\n            _backprop_utils.backprop_step(func, target_input_indexes, out_grad, in_grad, is_debug)\n            for hook in hooks:\n                hook.backward_postprocess(func, tuple(in_data), tuple(out_grad_array))\n        if retain_grad:\n            for (y, gy) in six.moves.zip(outputs, out_grad):\n                if y is not None:\n                    y._set_grad_var_if_available(gy)\n            del gy\n        del out_grad\n        for (x, gx) in in_grad.items():\n            if not gx:\n                continue\n            for gx_elem in gx:\n                if gx_elem is not None:\n                    chainer.variable._check_grad_type(func, x, True, gx_elem.raw_array)\n            del gx_elem\n            if x.creator_node is None:\n                leaf_nodes.add(x)\n            else:\n                add_cand(x.creator_node)\n        del gx, in_grad\n    for x in leaf_nodes:\n        x_var = x.get_variable_or_none()\n        gx = grads.pop(x)\n        if x_var is not None:\n            x_var._set_grad_var_without_check(gx)\n            x_var._loss_scale = loss_scale\n    grads.assert_no_grads()"
        ]
    }
]