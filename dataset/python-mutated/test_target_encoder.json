[
    {
        "func_name": "_encode_target",
        "original": "def _encode_target(X_ordinal, y_numeric, n_categories, smooth):\n    \"\"\"Simple Python implementation of target encoding.\"\"\"\n    cur_encodings = np.zeros(n_categories, dtype=np.float64)\n    y_mean = np.mean(y_numeric)\n    if smooth == 'auto':\n        y_variance = np.var(y_numeric)\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            n_i = y_subset.shape[0]\n            if n_i == 0:\n                cur_encodings[c] = y_mean\n                continue\n            y_subset_variance = np.var(y_subset)\n            m = y_subset_variance / y_variance\n            lambda_ = n_i / (n_i + m)\n            cur_encodings[c] = lambda_ * np.mean(y_subset) + (1 - lambda_) * y_mean\n        return cur_encodings\n    else:\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            current_sum = np.sum(y_subset) + y_mean * smooth\n            current_cnt = y_subset.shape[0] + smooth\n            cur_encodings[c] = current_sum / current_cnt\n        return cur_encodings",
        "mutated": [
            "def _encode_target(X_ordinal, y_numeric, n_categories, smooth):\n    if False:\n        i = 10\n    'Simple Python implementation of target encoding.'\n    cur_encodings = np.zeros(n_categories, dtype=np.float64)\n    y_mean = np.mean(y_numeric)\n    if smooth == 'auto':\n        y_variance = np.var(y_numeric)\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            n_i = y_subset.shape[0]\n            if n_i == 0:\n                cur_encodings[c] = y_mean\n                continue\n            y_subset_variance = np.var(y_subset)\n            m = y_subset_variance / y_variance\n            lambda_ = n_i / (n_i + m)\n            cur_encodings[c] = lambda_ * np.mean(y_subset) + (1 - lambda_) * y_mean\n        return cur_encodings\n    else:\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            current_sum = np.sum(y_subset) + y_mean * smooth\n            current_cnt = y_subset.shape[0] + smooth\n            cur_encodings[c] = current_sum / current_cnt\n        return cur_encodings",
            "def _encode_target(X_ordinal, y_numeric, n_categories, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Simple Python implementation of target encoding.'\n    cur_encodings = np.zeros(n_categories, dtype=np.float64)\n    y_mean = np.mean(y_numeric)\n    if smooth == 'auto':\n        y_variance = np.var(y_numeric)\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            n_i = y_subset.shape[0]\n            if n_i == 0:\n                cur_encodings[c] = y_mean\n                continue\n            y_subset_variance = np.var(y_subset)\n            m = y_subset_variance / y_variance\n            lambda_ = n_i / (n_i + m)\n            cur_encodings[c] = lambda_ * np.mean(y_subset) + (1 - lambda_) * y_mean\n        return cur_encodings\n    else:\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            current_sum = np.sum(y_subset) + y_mean * smooth\n            current_cnt = y_subset.shape[0] + smooth\n            cur_encodings[c] = current_sum / current_cnt\n        return cur_encodings",
            "def _encode_target(X_ordinal, y_numeric, n_categories, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Simple Python implementation of target encoding.'\n    cur_encodings = np.zeros(n_categories, dtype=np.float64)\n    y_mean = np.mean(y_numeric)\n    if smooth == 'auto':\n        y_variance = np.var(y_numeric)\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            n_i = y_subset.shape[0]\n            if n_i == 0:\n                cur_encodings[c] = y_mean\n                continue\n            y_subset_variance = np.var(y_subset)\n            m = y_subset_variance / y_variance\n            lambda_ = n_i / (n_i + m)\n            cur_encodings[c] = lambda_ * np.mean(y_subset) + (1 - lambda_) * y_mean\n        return cur_encodings\n    else:\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            current_sum = np.sum(y_subset) + y_mean * smooth\n            current_cnt = y_subset.shape[0] + smooth\n            cur_encodings[c] = current_sum / current_cnt\n        return cur_encodings",
            "def _encode_target(X_ordinal, y_numeric, n_categories, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Simple Python implementation of target encoding.'\n    cur_encodings = np.zeros(n_categories, dtype=np.float64)\n    y_mean = np.mean(y_numeric)\n    if smooth == 'auto':\n        y_variance = np.var(y_numeric)\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            n_i = y_subset.shape[0]\n            if n_i == 0:\n                cur_encodings[c] = y_mean\n                continue\n            y_subset_variance = np.var(y_subset)\n            m = y_subset_variance / y_variance\n            lambda_ = n_i / (n_i + m)\n            cur_encodings[c] = lambda_ * np.mean(y_subset) + (1 - lambda_) * y_mean\n        return cur_encodings\n    else:\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            current_sum = np.sum(y_subset) + y_mean * smooth\n            current_cnt = y_subset.shape[0] + smooth\n            cur_encodings[c] = current_sum / current_cnt\n        return cur_encodings",
            "def _encode_target(X_ordinal, y_numeric, n_categories, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Simple Python implementation of target encoding.'\n    cur_encodings = np.zeros(n_categories, dtype=np.float64)\n    y_mean = np.mean(y_numeric)\n    if smooth == 'auto':\n        y_variance = np.var(y_numeric)\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            n_i = y_subset.shape[0]\n            if n_i == 0:\n                cur_encodings[c] = y_mean\n                continue\n            y_subset_variance = np.var(y_subset)\n            m = y_subset_variance / y_variance\n            lambda_ = n_i / (n_i + m)\n            cur_encodings[c] = lambda_ * np.mean(y_subset) + (1 - lambda_) * y_mean\n        return cur_encodings\n    else:\n        for c in range(n_categories):\n            y_subset = y_numeric[X_ordinal == c]\n            current_sum = np.sum(y_subset) + y_mean * smooth\n            current_cnt = y_subset.shape[0] + smooth\n            cur_encodings[c] = current_sum / current_cnt\n        return cur_encodings"
        ]
    },
    {
        "func_name": "test_encoding",
        "original": "@pytest.mark.parametrize('categories, unknown_value', [([np.array([0, 1, 2], dtype=np.int64)], 4), ([np.array([1.0, 3.0, np.nan], dtype=np.float64)], 6.0), ([np.array(['cat', 'dog', 'snake'], dtype=object)], 'bear'), ('auto', 3)])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary', 'continuous'])\ndef test_encoding(categories, unknown_value, global_random_seed, smooth, target_type):\n    \"\"\"Check encoding for binary and continuous targets.\n\n    Compare the values returned by `TargetEncoder.fit_transform` against the\n    expected encodings for cv splits from a naive reference Python\n    implementation in _encode_target.\n    \"\"\"\n    n_categories = 3\n    X_train_int_array = np.array([[0] * 20 + [1] * 30 + [2] * 40], dtype=np.int64).T\n    X_test_int_array = np.array([[0, 1, 2]], dtype=np.int64).T\n    n_samples = X_train_int_array.shape[0]\n    if categories == 'auto':\n        X_train = X_train_int_array\n        X_test = X_test_int_array\n    else:\n        X_train = categories[0][X_train_int_array]\n        X_test = categories[0][X_test_int_array]\n    X_test = np.concatenate((X_test, [[unknown_value]]))\n    data_rng = np.random.RandomState(global_random_seed)\n    n_splits = 3\n    if target_type == 'binary':\n        y_numeric = data_rng.randint(low=0, high=2, size=n_samples)\n        target_names = np.array(['cat', 'dog'], dtype=object)\n        y_train = target_names[y_numeric]\n    else:\n        assert target_type == 'continuous'\n        y_numeric = data_rng.uniform(low=-10, high=20, size=n_samples)\n        y_train = y_numeric\n    shuffled_idx = data_rng.permutation(n_samples)\n    X_train_int_array = X_train_int_array[shuffled_idx]\n    X_train = X_train[shuffled_idx]\n    y_train = y_train[shuffled_idx]\n    y_numeric = y_numeric[shuffled_idx]\n    if target_type == 'binary':\n        cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    else:\n        cv = KFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty_like(X_train_int_array, dtype=np.float64)\n    for (train_idx, test_idx) in cv.split(X_train_int_array, y_train):\n        (X_, y_) = (X_train_int_array[train_idx, 0], y_numeric[train_idx])\n        cur_encodings = _encode_target(X_, y_, n_categories, smooth)\n        expected_X_fit_transform[test_idx, 0] = cur_encodings[X_train_int_array[test_idx, 0]]\n    target_encoder = TargetEncoder(smooth=smooth, categories=categories, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == target_type\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(target_encoder.encodings_) == 1\n    if target_type == 'binary':\n        assert_array_equal(target_encoder.classes_, target_names)\n    else:\n        assert target_encoder.classes_ is None\n    y_mean = np.mean(y_numeric)\n    expected_encodings = _encode_target(X_train_int_array[:, 0], y_numeric, n_categories, smooth)\n    assert_allclose(target_encoder.encodings_[0], expected_encodings)\n    assert target_encoder.target_mean_ == pytest.approx(y_mean)\n    expected_X_test_transform = np.concatenate((expected_encodings, np.array([y_mean]))).reshape(-1, 1)\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
        "mutated": [
            "@pytest.mark.parametrize('categories, unknown_value', [([np.array([0, 1, 2], dtype=np.int64)], 4), ([np.array([1.0, 3.0, np.nan], dtype=np.float64)], 6.0), ([np.array(['cat', 'dog', 'snake'], dtype=object)], 'bear'), ('auto', 3)])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary', 'continuous'])\ndef test_encoding(categories, unknown_value, global_random_seed, smooth, target_type):\n    if False:\n        i = 10\n    'Check encoding for binary and continuous targets.\\n\\n    Compare the values returned by `TargetEncoder.fit_transform` against the\\n    expected encodings for cv splits from a naive reference Python\\n    implementation in _encode_target.\\n    '\n    n_categories = 3\n    X_train_int_array = np.array([[0] * 20 + [1] * 30 + [2] * 40], dtype=np.int64).T\n    X_test_int_array = np.array([[0, 1, 2]], dtype=np.int64).T\n    n_samples = X_train_int_array.shape[0]\n    if categories == 'auto':\n        X_train = X_train_int_array\n        X_test = X_test_int_array\n    else:\n        X_train = categories[0][X_train_int_array]\n        X_test = categories[0][X_test_int_array]\n    X_test = np.concatenate((X_test, [[unknown_value]]))\n    data_rng = np.random.RandomState(global_random_seed)\n    n_splits = 3\n    if target_type == 'binary':\n        y_numeric = data_rng.randint(low=0, high=2, size=n_samples)\n        target_names = np.array(['cat', 'dog'], dtype=object)\n        y_train = target_names[y_numeric]\n    else:\n        assert target_type == 'continuous'\n        y_numeric = data_rng.uniform(low=-10, high=20, size=n_samples)\n        y_train = y_numeric\n    shuffled_idx = data_rng.permutation(n_samples)\n    X_train_int_array = X_train_int_array[shuffled_idx]\n    X_train = X_train[shuffled_idx]\n    y_train = y_train[shuffled_idx]\n    y_numeric = y_numeric[shuffled_idx]\n    if target_type == 'binary':\n        cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    else:\n        cv = KFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty_like(X_train_int_array, dtype=np.float64)\n    for (train_idx, test_idx) in cv.split(X_train_int_array, y_train):\n        (X_, y_) = (X_train_int_array[train_idx, 0], y_numeric[train_idx])\n        cur_encodings = _encode_target(X_, y_, n_categories, smooth)\n        expected_X_fit_transform[test_idx, 0] = cur_encodings[X_train_int_array[test_idx, 0]]\n    target_encoder = TargetEncoder(smooth=smooth, categories=categories, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == target_type\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(target_encoder.encodings_) == 1\n    if target_type == 'binary':\n        assert_array_equal(target_encoder.classes_, target_names)\n    else:\n        assert target_encoder.classes_ is None\n    y_mean = np.mean(y_numeric)\n    expected_encodings = _encode_target(X_train_int_array[:, 0], y_numeric, n_categories, smooth)\n    assert_allclose(target_encoder.encodings_[0], expected_encodings)\n    assert target_encoder.target_mean_ == pytest.approx(y_mean)\n    expected_X_test_transform = np.concatenate((expected_encodings, np.array([y_mean]))).reshape(-1, 1)\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('categories, unknown_value', [([np.array([0, 1, 2], dtype=np.int64)], 4), ([np.array([1.0, 3.0, np.nan], dtype=np.float64)], 6.0), ([np.array(['cat', 'dog', 'snake'], dtype=object)], 'bear'), ('auto', 3)])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary', 'continuous'])\ndef test_encoding(categories, unknown_value, global_random_seed, smooth, target_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check encoding for binary and continuous targets.\\n\\n    Compare the values returned by `TargetEncoder.fit_transform` against the\\n    expected encodings for cv splits from a naive reference Python\\n    implementation in _encode_target.\\n    '\n    n_categories = 3\n    X_train_int_array = np.array([[0] * 20 + [1] * 30 + [2] * 40], dtype=np.int64).T\n    X_test_int_array = np.array([[0, 1, 2]], dtype=np.int64).T\n    n_samples = X_train_int_array.shape[0]\n    if categories == 'auto':\n        X_train = X_train_int_array\n        X_test = X_test_int_array\n    else:\n        X_train = categories[0][X_train_int_array]\n        X_test = categories[0][X_test_int_array]\n    X_test = np.concatenate((X_test, [[unknown_value]]))\n    data_rng = np.random.RandomState(global_random_seed)\n    n_splits = 3\n    if target_type == 'binary':\n        y_numeric = data_rng.randint(low=0, high=2, size=n_samples)\n        target_names = np.array(['cat', 'dog'], dtype=object)\n        y_train = target_names[y_numeric]\n    else:\n        assert target_type == 'continuous'\n        y_numeric = data_rng.uniform(low=-10, high=20, size=n_samples)\n        y_train = y_numeric\n    shuffled_idx = data_rng.permutation(n_samples)\n    X_train_int_array = X_train_int_array[shuffled_idx]\n    X_train = X_train[shuffled_idx]\n    y_train = y_train[shuffled_idx]\n    y_numeric = y_numeric[shuffled_idx]\n    if target_type == 'binary':\n        cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    else:\n        cv = KFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty_like(X_train_int_array, dtype=np.float64)\n    for (train_idx, test_idx) in cv.split(X_train_int_array, y_train):\n        (X_, y_) = (X_train_int_array[train_idx, 0], y_numeric[train_idx])\n        cur_encodings = _encode_target(X_, y_, n_categories, smooth)\n        expected_X_fit_transform[test_idx, 0] = cur_encodings[X_train_int_array[test_idx, 0]]\n    target_encoder = TargetEncoder(smooth=smooth, categories=categories, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == target_type\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(target_encoder.encodings_) == 1\n    if target_type == 'binary':\n        assert_array_equal(target_encoder.classes_, target_names)\n    else:\n        assert target_encoder.classes_ is None\n    y_mean = np.mean(y_numeric)\n    expected_encodings = _encode_target(X_train_int_array[:, 0], y_numeric, n_categories, smooth)\n    assert_allclose(target_encoder.encodings_[0], expected_encodings)\n    assert target_encoder.target_mean_ == pytest.approx(y_mean)\n    expected_X_test_transform = np.concatenate((expected_encodings, np.array([y_mean]))).reshape(-1, 1)\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('categories, unknown_value', [([np.array([0, 1, 2], dtype=np.int64)], 4), ([np.array([1.0, 3.0, np.nan], dtype=np.float64)], 6.0), ([np.array(['cat', 'dog', 'snake'], dtype=object)], 'bear'), ('auto', 3)])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary', 'continuous'])\ndef test_encoding(categories, unknown_value, global_random_seed, smooth, target_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check encoding for binary and continuous targets.\\n\\n    Compare the values returned by `TargetEncoder.fit_transform` against the\\n    expected encodings for cv splits from a naive reference Python\\n    implementation in _encode_target.\\n    '\n    n_categories = 3\n    X_train_int_array = np.array([[0] * 20 + [1] * 30 + [2] * 40], dtype=np.int64).T\n    X_test_int_array = np.array([[0, 1, 2]], dtype=np.int64).T\n    n_samples = X_train_int_array.shape[0]\n    if categories == 'auto':\n        X_train = X_train_int_array\n        X_test = X_test_int_array\n    else:\n        X_train = categories[0][X_train_int_array]\n        X_test = categories[0][X_test_int_array]\n    X_test = np.concatenate((X_test, [[unknown_value]]))\n    data_rng = np.random.RandomState(global_random_seed)\n    n_splits = 3\n    if target_type == 'binary':\n        y_numeric = data_rng.randint(low=0, high=2, size=n_samples)\n        target_names = np.array(['cat', 'dog'], dtype=object)\n        y_train = target_names[y_numeric]\n    else:\n        assert target_type == 'continuous'\n        y_numeric = data_rng.uniform(low=-10, high=20, size=n_samples)\n        y_train = y_numeric\n    shuffled_idx = data_rng.permutation(n_samples)\n    X_train_int_array = X_train_int_array[shuffled_idx]\n    X_train = X_train[shuffled_idx]\n    y_train = y_train[shuffled_idx]\n    y_numeric = y_numeric[shuffled_idx]\n    if target_type == 'binary':\n        cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    else:\n        cv = KFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty_like(X_train_int_array, dtype=np.float64)\n    for (train_idx, test_idx) in cv.split(X_train_int_array, y_train):\n        (X_, y_) = (X_train_int_array[train_idx, 0], y_numeric[train_idx])\n        cur_encodings = _encode_target(X_, y_, n_categories, smooth)\n        expected_X_fit_transform[test_idx, 0] = cur_encodings[X_train_int_array[test_idx, 0]]\n    target_encoder = TargetEncoder(smooth=smooth, categories=categories, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == target_type\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(target_encoder.encodings_) == 1\n    if target_type == 'binary':\n        assert_array_equal(target_encoder.classes_, target_names)\n    else:\n        assert target_encoder.classes_ is None\n    y_mean = np.mean(y_numeric)\n    expected_encodings = _encode_target(X_train_int_array[:, 0], y_numeric, n_categories, smooth)\n    assert_allclose(target_encoder.encodings_[0], expected_encodings)\n    assert target_encoder.target_mean_ == pytest.approx(y_mean)\n    expected_X_test_transform = np.concatenate((expected_encodings, np.array([y_mean]))).reshape(-1, 1)\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('categories, unknown_value', [([np.array([0, 1, 2], dtype=np.int64)], 4), ([np.array([1.0, 3.0, np.nan], dtype=np.float64)], 6.0), ([np.array(['cat', 'dog', 'snake'], dtype=object)], 'bear'), ('auto', 3)])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary', 'continuous'])\ndef test_encoding(categories, unknown_value, global_random_seed, smooth, target_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check encoding for binary and continuous targets.\\n\\n    Compare the values returned by `TargetEncoder.fit_transform` against the\\n    expected encodings for cv splits from a naive reference Python\\n    implementation in _encode_target.\\n    '\n    n_categories = 3\n    X_train_int_array = np.array([[0] * 20 + [1] * 30 + [2] * 40], dtype=np.int64).T\n    X_test_int_array = np.array([[0, 1, 2]], dtype=np.int64).T\n    n_samples = X_train_int_array.shape[0]\n    if categories == 'auto':\n        X_train = X_train_int_array\n        X_test = X_test_int_array\n    else:\n        X_train = categories[0][X_train_int_array]\n        X_test = categories[0][X_test_int_array]\n    X_test = np.concatenate((X_test, [[unknown_value]]))\n    data_rng = np.random.RandomState(global_random_seed)\n    n_splits = 3\n    if target_type == 'binary':\n        y_numeric = data_rng.randint(low=0, high=2, size=n_samples)\n        target_names = np.array(['cat', 'dog'], dtype=object)\n        y_train = target_names[y_numeric]\n    else:\n        assert target_type == 'continuous'\n        y_numeric = data_rng.uniform(low=-10, high=20, size=n_samples)\n        y_train = y_numeric\n    shuffled_idx = data_rng.permutation(n_samples)\n    X_train_int_array = X_train_int_array[shuffled_idx]\n    X_train = X_train[shuffled_idx]\n    y_train = y_train[shuffled_idx]\n    y_numeric = y_numeric[shuffled_idx]\n    if target_type == 'binary':\n        cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    else:\n        cv = KFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty_like(X_train_int_array, dtype=np.float64)\n    for (train_idx, test_idx) in cv.split(X_train_int_array, y_train):\n        (X_, y_) = (X_train_int_array[train_idx, 0], y_numeric[train_idx])\n        cur_encodings = _encode_target(X_, y_, n_categories, smooth)\n        expected_X_fit_transform[test_idx, 0] = cur_encodings[X_train_int_array[test_idx, 0]]\n    target_encoder = TargetEncoder(smooth=smooth, categories=categories, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == target_type\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(target_encoder.encodings_) == 1\n    if target_type == 'binary':\n        assert_array_equal(target_encoder.classes_, target_names)\n    else:\n        assert target_encoder.classes_ is None\n    y_mean = np.mean(y_numeric)\n    expected_encodings = _encode_target(X_train_int_array[:, 0], y_numeric, n_categories, smooth)\n    assert_allclose(target_encoder.encodings_[0], expected_encodings)\n    assert target_encoder.target_mean_ == pytest.approx(y_mean)\n    expected_X_test_transform = np.concatenate((expected_encodings, np.array([y_mean]))).reshape(-1, 1)\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('categories, unknown_value', [([np.array([0, 1, 2], dtype=np.int64)], 4), ([np.array([1.0, 3.0, np.nan], dtype=np.float64)], 6.0), ([np.array(['cat', 'dog', 'snake'], dtype=object)], 'bear'), ('auto', 3)])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary', 'continuous'])\ndef test_encoding(categories, unknown_value, global_random_seed, smooth, target_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check encoding for binary and continuous targets.\\n\\n    Compare the values returned by `TargetEncoder.fit_transform` against the\\n    expected encodings for cv splits from a naive reference Python\\n    implementation in _encode_target.\\n    '\n    n_categories = 3\n    X_train_int_array = np.array([[0] * 20 + [1] * 30 + [2] * 40], dtype=np.int64).T\n    X_test_int_array = np.array([[0, 1, 2]], dtype=np.int64).T\n    n_samples = X_train_int_array.shape[0]\n    if categories == 'auto':\n        X_train = X_train_int_array\n        X_test = X_test_int_array\n    else:\n        X_train = categories[0][X_train_int_array]\n        X_test = categories[0][X_test_int_array]\n    X_test = np.concatenate((X_test, [[unknown_value]]))\n    data_rng = np.random.RandomState(global_random_seed)\n    n_splits = 3\n    if target_type == 'binary':\n        y_numeric = data_rng.randint(low=0, high=2, size=n_samples)\n        target_names = np.array(['cat', 'dog'], dtype=object)\n        y_train = target_names[y_numeric]\n    else:\n        assert target_type == 'continuous'\n        y_numeric = data_rng.uniform(low=-10, high=20, size=n_samples)\n        y_train = y_numeric\n    shuffled_idx = data_rng.permutation(n_samples)\n    X_train_int_array = X_train_int_array[shuffled_idx]\n    X_train = X_train[shuffled_idx]\n    y_train = y_train[shuffled_idx]\n    y_numeric = y_numeric[shuffled_idx]\n    if target_type == 'binary':\n        cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    else:\n        cv = KFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty_like(X_train_int_array, dtype=np.float64)\n    for (train_idx, test_idx) in cv.split(X_train_int_array, y_train):\n        (X_, y_) = (X_train_int_array[train_idx, 0], y_numeric[train_idx])\n        cur_encodings = _encode_target(X_, y_, n_categories, smooth)\n        expected_X_fit_transform[test_idx, 0] = cur_encodings[X_train_int_array[test_idx, 0]]\n    target_encoder = TargetEncoder(smooth=smooth, categories=categories, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == target_type\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(target_encoder.encodings_) == 1\n    if target_type == 'binary':\n        assert_array_equal(target_encoder.classes_, target_names)\n    else:\n        assert target_encoder.classes_ is None\n    y_mean = np.mean(y_numeric)\n    expected_encodings = _encode_target(X_train_int_array[:, 0], y_numeric, n_categories, smooth)\n    assert_allclose(target_encoder.encodings_[0], expected_encodings)\n    assert target_encoder.target_mean_ == pytest.approx(y_mean)\n    expected_X_test_transform = np.concatenate((expected_encodings, np.array([y_mean]))).reshape(-1, 1)\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)"
        ]
    },
    {
        "func_name": "test_encoding_multiclass",
        "original": "@pytest.mark.parametrize('categories, unknown_values', [([np.array([0, 1, 2], dtype=np.int64)], 'auto'), ([np.array(['cat', 'dog', 'snake'], dtype=object)], ['bear', 'rabbit'])])\n@pytest.mark.parametrize('target_labels', [np.array([1, 2, 3]), np.array(['a', 'b', 'c'])])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\ndef test_encoding_multiclass(global_random_seed, categories, unknown_values, target_labels, smooth):\n    \"\"\"Check encoding for multiclass targets.\"\"\"\n    rng = np.random.RandomState(global_random_seed)\n    n_samples = 80\n    n_features = 2\n    feat_1_int = np.array(rng.randint(low=0, high=2, size=n_samples))\n    feat_2_int = np.array(rng.randint(low=0, high=3, size=n_samples))\n    feat_1 = categories[0][feat_1_int]\n    feat_2 = categories[0][feat_2_int]\n    X_train = np.column_stack((feat_1, feat_2))\n    X_train_int = np.column_stack((feat_1_int, feat_2_int))\n    categories_ = [[0, 1], [0, 1, 2]]\n    n_classes = 3\n    y_train_int = np.array(rng.randint(low=0, high=n_classes, size=n_samples))\n    y_train = target_labels[y_train_int]\n    y_train_enc = LabelBinarizer().fit_transform(y_train)\n    n_splits = 3\n    cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty((X_train_int.shape[0], X_train_int.shape[1] * n_classes), dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            for (train_idx, test_idx) in cv.split(X_train, y_train):\n                y_class = y_train_enc[:, c_idx]\n                (X_, y_) = (X_train_int[train_idx, f_idx], y_class[train_idx])\n                current_encoding = _encode_target(X_, y_, len(cats), smooth)\n                exp_idx = c_idx + f_idx * n_classes\n                expected_X_fit_transform[test_idx, exp_idx] = current_encoding[X_train_int[test_idx, f_idx]]\n    target_encoder = TargetEncoder(smooth=smooth, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == 'multiclass'\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            y_class = y_train_enc[:, c_idx]\n            current_encoding = _encode_target(X_train_int[:, f_idx], y_class, len(cats), smooth)\n            expected_encodings.append(current_encoding)\n    assert len(target_encoder.encodings_) == n_features * n_classes\n    for i in range(n_features * n_classes):\n        assert_allclose(target_encoder.encodings_[i], expected_encodings[i])\n    assert_array_equal(target_encoder.classes_, target_labels)\n    X_test_int = np.array([[0, 1], [1, 2], [4, 5]])\n    if unknown_values == 'auto':\n        X_test = X_test_int\n    else:\n        X_test = np.empty_like(X_test_int[:-1, :], dtype=object)\n        for column_idx in range(X_test_int.shape[1]):\n            X_test[:, column_idx] = categories[0][X_test_int[:-1, column_idx]]\n        X_test = np.vstack((X_test, unknown_values))\n    y_mean = np.mean(y_train_enc, axis=0)\n    expected_X_test_transform = np.empty((X_test_int.shape[0], X_test_int.shape[1] * n_classes), dtype=np.float64)\n    n_rows = X_test_int.shape[0]\n    f_idx = [0, 0, 0, 1, 1, 1]\n    for row_idx in range(n_rows - 1):\n        for (i, enc) in enumerate(expected_encodings):\n            expected_X_test_transform[row_idx, i] = enc[X_test_int[row_idx, f_idx[i]]]\n    mean_idx = [0, 1, 2, 0, 1, 2]\n    for i in range(n_classes * n_features):\n        expected_X_test_transform[n_rows - 1, i] = y_mean[mean_idx[i]]\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
        "mutated": [
            "@pytest.mark.parametrize('categories, unknown_values', [([np.array([0, 1, 2], dtype=np.int64)], 'auto'), ([np.array(['cat', 'dog', 'snake'], dtype=object)], ['bear', 'rabbit'])])\n@pytest.mark.parametrize('target_labels', [np.array([1, 2, 3]), np.array(['a', 'b', 'c'])])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\ndef test_encoding_multiclass(global_random_seed, categories, unknown_values, target_labels, smooth):\n    if False:\n        i = 10\n    'Check encoding for multiclass targets.'\n    rng = np.random.RandomState(global_random_seed)\n    n_samples = 80\n    n_features = 2\n    feat_1_int = np.array(rng.randint(low=0, high=2, size=n_samples))\n    feat_2_int = np.array(rng.randint(low=0, high=3, size=n_samples))\n    feat_1 = categories[0][feat_1_int]\n    feat_2 = categories[0][feat_2_int]\n    X_train = np.column_stack((feat_1, feat_2))\n    X_train_int = np.column_stack((feat_1_int, feat_2_int))\n    categories_ = [[0, 1], [0, 1, 2]]\n    n_classes = 3\n    y_train_int = np.array(rng.randint(low=0, high=n_classes, size=n_samples))\n    y_train = target_labels[y_train_int]\n    y_train_enc = LabelBinarizer().fit_transform(y_train)\n    n_splits = 3\n    cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty((X_train_int.shape[0], X_train_int.shape[1] * n_classes), dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            for (train_idx, test_idx) in cv.split(X_train, y_train):\n                y_class = y_train_enc[:, c_idx]\n                (X_, y_) = (X_train_int[train_idx, f_idx], y_class[train_idx])\n                current_encoding = _encode_target(X_, y_, len(cats), smooth)\n                exp_idx = c_idx + f_idx * n_classes\n                expected_X_fit_transform[test_idx, exp_idx] = current_encoding[X_train_int[test_idx, f_idx]]\n    target_encoder = TargetEncoder(smooth=smooth, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == 'multiclass'\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            y_class = y_train_enc[:, c_idx]\n            current_encoding = _encode_target(X_train_int[:, f_idx], y_class, len(cats), smooth)\n            expected_encodings.append(current_encoding)\n    assert len(target_encoder.encodings_) == n_features * n_classes\n    for i in range(n_features * n_classes):\n        assert_allclose(target_encoder.encodings_[i], expected_encodings[i])\n    assert_array_equal(target_encoder.classes_, target_labels)\n    X_test_int = np.array([[0, 1], [1, 2], [4, 5]])\n    if unknown_values == 'auto':\n        X_test = X_test_int\n    else:\n        X_test = np.empty_like(X_test_int[:-1, :], dtype=object)\n        for column_idx in range(X_test_int.shape[1]):\n            X_test[:, column_idx] = categories[0][X_test_int[:-1, column_idx]]\n        X_test = np.vstack((X_test, unknown_values))\n    y_mean = np.mean(y_train_enc, axis=0)\n    expected_X_test_transform = np.empty((X_test_int.shape[0], X_test_int.shape[1] * n_classes), dtype=np.float64)\n    n_rows = X_test_int.shape[0]\n    f_idx = [0, 0, 0, 1, 1, 1]\n    for row_idx in range(n_rows - 1):\n        for (i, enc) in enumerate(expected_encodings):\n            expected_X_test_transform[row_idx, i] = enc[X_test_int[row_idx, f_idx[i]]]\n    mean_idx = [0, 1, 2, 0, 1, 2]\n    for i in range(n_classes * n_features):\n        expected_X_test_transform[n_rows - 1, i] = y_mean[mean_idx[i]]\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('categories, unknown_values', [([np.array([0, 1, 2], dtype=np.int64)], 'auto'), ([np.array(['cat', 'dog', 'snake'], dtype=object)], ['bear', 'rabbit'])])\n@pytest.mark.parametrize('target_labels', [np.array([1, 2, 3]), np.array(['a', 'b', 'c'])])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\ndef test_encoding_multiclass(global_random_seed, categories, unknown_values, target_labels, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check encoding for multiclass targets.'\n    rng = np.random.RandomState(global_random_seed)\n    n_samples = 80\n    n_features = 2\n    feat_1_int = np.array(rng.randint(low=0, high=2, size=n_samples))\n    feat_2_int = np.array(rng.randint(low=0, high=3, size=n_samples))\n    feat_1 = categories[0][feat_1_int]\n    feat_2 = categories[0][feat_2_int]\n    X_train = np.column_stack((feat_1, feat_2))\n    X_train_int = np.column_stack((feat_1_int, feat_2_int))\n    categories_ = [[0, 1], [0, 1, 2]]\n    n_classes = 3\n    y_train_int = np.array(rng.randint(low=0, high=n_classes, size=n_samples))\n    y_train = target_labels[y_train_int]\n    y_train_enc = LabelBinarizer().fit_transform(y_train)\n    n_splits = 3\n    cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty((X_train_int.shape[0], X_train_int.shape[1] * n_classes), dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            for (train_idx, test_idx) in cv.split(X_train, y_train):\n                y_class = y_train_enc[:, c_idx]\n                (X_, y_) = (X_train_int[train_idx, f_idx], y_class[train_idx])\n                current_encoding = _encode_target(X_, y_, len(cats), smooth)\n                exp_idx = c_idx + f_idx * n_classes\n                expected_X_fit_transform[test_idx, exp_idx] = current_encoding[X_train_int[test_idx, f_idx]]\n    target_encoder = TargetEncoder(smooth=smooth, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == 'multiclass'\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            y_class = y_train_enc[:, c_idx]\n            current_encoding = _encode_target(X_train_int[:, f_idx], y_class, len(cats), smooth)\n            expected_encodings.append(current_encoding)\n    assert len(target_encoder.encodings_) == n_features * n_classes\n    for i in range(n_features * n_classes):\n        assert_allclose(target_encoder.encodings_[i], expected_encodings[i])\n    assert_array_equal(target_encoder.classes_, target_labels)\n    X_test_int = np.array([[0, 1], [1, 2], [4, 5]])\n    if unknown_values == 'auto':\n        X_test = X_test_int\n    else:\n        X_test = np.empty_like(X_test_int[:-1, :], dtype=object)\n        for column_idx in range(X_test_int.shape[1]):\n            X_test[:, column_idx] = categories[0][X_test_int[:-1, column_idx]]\n        X_test = np.vstack((X_test, unknown_values))\n    y_mean = np.mean(y_train_enc, axis=0)\n    expected_X_test_transform = np.empty((X_test_int.shape[0], X_test_int.shape[1] * n_classes), dtype=np.float64)\n    n_rows = X_test_int.shape[0]\n    f_idx = [0, 0, 0, 1, 1, 1]\n    for row_idx in range(n_rows - 1):\n        for (i, enc) in enumerate(expected_encodings):\n            expected_X_test_transform[row_idx, i] = enc[X_test_int[row_idx, f_idx[i]]]\n    mean_idx = [0, 1, 2, 0, 1, 2]\n    for i in range(n_classes * n_features):\n        expected_X_test_transform[n_rows - 1, i] = y_mean[mean_idx[i]]\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('categories, unknown_values', [([np.array([0, 1, 2], dtype=np.int64)], 'auto'), ([np.array(['cat', 'dog', 'snake'], dtype=object)], ['bear', 'rabbit'])])\n@pytest.mark.parametrize('target_labels', [np.array([1, 2, 3]), np.array(['a', 'b', 'c'])])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\ndef test_encoding_multiclass(global_random_seed, categories, unknown_values, target_labels, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check encoding for multiclass targets.'\n    rng = np.random.RandomState(global_random_seed)\n    n_samples = 80\n    n_features = 2\n    feat_1_int = np.array(rng.randint(low=0, high=2, size=n_samples))\n    feat_2_int = np.array(rng.randint(low=0, high=3, size=n_samples))\n    feat_1 = categories[0][feat_1_int]\n    feat_2 = categories[0][feat_2_int]\n    X_train = np.column_stack((feat_1, feat_2))\n    X_train_int = np.column_stack((feat_1_int, feat_2_int))\n    categories_ = [[0, 1], [0, 1, 2]]\n    n_classes = 3\n    y_train_int = np.array(rng.randint(low=0, high=n_classes, size=n_samples))\n    y_train = target_labels[y_train_int]\n    y_train_enc = LabelBinarizer().fit_transform(y_train)\n    n_splits = 3\n    cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty((X_train_int.shape[0], X_train_int.shape[1] * n_classes), dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            for (train_idx, test_idx) in cv.split(X_train, y_train):\n                y_class = y_train_enc[:, c_idx]\n                (X_, y_) = (X_train_int[train_idx, f_idx], y_class[train_idx])\n                current_encoding = _encode_target(X_, y_, len(cats), smooth)\n                exp_idx = c_idx + f_idx * n_classes\n                expected_X_fit_transform[test_idx, exp_idx] = current_encoding[X_train_int[test_idx, f_idx]]\n    target_encoder = TargetEncoder(smooth=smooth, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == 'multiclass'\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            y_class = y_train_enc[:, c_idx]\n            current_encoding = _encode_target(X_train_int[:, f_idx], y_class, len(cats), smooth)\n            expected_encodings.append(current_encoding)\n    assert len(target_encoder.encodings_) == n_features * n_classes\n    for i in range(n_features * n_classes):\n        assert_allclose(target_encoder.encodings_[i], expected_encodings[i])\n    assert_array_equal(target_encoder.classes_, target_labels)\n    X_test_int = np.array([[0, 1], [1, 2], [4, 5]])\n    if unknown_values == 'auto':\n        X_test = X_test_int\n    else:\n        X_test = np.empty_like(X_test_int[:-1, :], dtype=object)\n        for column_idx in range(X_test_int.shape[1]):\n            X_test[:, column_idx] = categories[0][X_test_int[:-1, column_idx]]\n        X_test = np.vstack((X_test, unknown_values))\n    y_mean = np.mean(y_train_enc, axis=0)\n    expected_X_test_transform = np.empty((X_test_int.shape[0], X_test_int.shape[1] * n_classes), dtype=np.float64)\n    n_rows = X_test_int.shape[0]\n    f_idx = [0, 0, 0, 1, 1, 1]\n    for row_idx in range(n_rows - 1):\n        for (i, enc) in enumerate(expected_encodings):\n            expected_X_test_transform[row_idx, i] = enc[X_test_int[row_idx, f_idx[i]]]\n    mean_idx = [0, 1, 2, 0, 1, 2]\n    for i in range(n_classes * n_features):\n        expected_X_test_transform[n_rows - 1, i] = y_mean[mean_idx[i]]\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('categories, unknown_values', [([np.array([0, 1, 2], dtype=np.int64)], 'auto'), ([np.array(['cat', 'dog', 'snake'], dtype=object)], ['bear', 'rabbit'])])\n@pytest.mark.parametrize('target_labels', [np.array([1, 2, 3]), np.array(['a', 'b', 'c'])])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\ndef test_encoding_multiclass(global_random_seed, categories, unknown_values, target_labels, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check encoding for multiclass targets.'\n    rng = np.random.RandomState(global_random_seed)\n    n_samples = 80\n    n_features = 2\n    feat_1_int = np.array(rng.randint(low=0, high=2, size=n_samples))\n    feat_2_int = np.array(rng.randint(low=0, high=3, size=n_samples))\n    feat_1 = categories[0][feat_1_int]\n    feat_2 = categories[0][feat_2_int]\n    X_train = np.column_stack((feat_1, feat_2))\n    X_train_int = np.column_stack((feat_1_int, feat_2_int))\n    categories_ = [[0, 1], [0, 1, 2]]\n    n_classes = 3\n    y_train_int = np.array(rng.randint(low=0, high=n_classes, size=n_samples))\n    y_train = target_labels[y_train_int]\n    y_train_enc = LabelBinarizer().fit_transform(y_train)\n    n_splits = 3\n    cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty((X_train_int.shape[0], X_train_int.shape[1] * n_classes), dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            for (train_idx, test_idx) in cv.split(X_train, y_train):\n                y_class = y_train_enc[:, c_idx]\n                (X_, y_) = (X_train_int[train_idx, f_idx], y_class[train_idx])\n                current_encoding = _encode_target(X_, y_, len(cats), smooth)\n                exp_idx = c_idx + f_idx * n_classes\n                expected_X_fit_transform[test_idx, exp_idx] = current_encoding[X_train_int[test_idx, f_idx]]\n    target_encoder = TargetEncoder(smooth=smooth, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == 'multiclass'\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            y_class = y_train_enc[:, c_idx]\n            current_encoding = _encode_target(X_train_int[:, f_idx], y_class, len(cats), smooth)\n            expected_encodings.append(current_encoding)\n    assert len(target_encoder.encodings_) == n_features * n_classes\n    for i in range(n_features * n_classes):\n        assert_allclose(target_encoder.encodings_[i], expected_encodings[i])\n    assert_array_equal(target_encoder.classes_, target_labels)\n    X_test_int = np.array([[0, 1], [1, 2], [4, 5]])\n    if unknown_values == 'auto':\n        X_test = X_test_int\n    else:\n        X_test = np.empty_like(X_test_int[:-1, :], dtype=object)\n        for column_idx in range(X_test_int.shape[1]):\n            X_test[:, column_idx] = categories[0][X_test_int[:-1, column_idx]]\n        X_test = np.vstack((X_test, unknown_values))\n    y_mean = np.mean(y_train_enc, axis=0)\n    expected_X_test_transform = np.empty((X_test_int.shape[0], X_test_int.shape[1] * n_classes), dtype=np.float64)\n    n_rows = X_test_int.shape[0]\n    f_idx = [0, 0, 0, 1, 1, 1]\n    for row_idx in range(n_rows - 1):\n        for (i, enc) in enumerate(expected_encodings):\n            expected_X_test_transform[row_idx, i] = enc[X_test_int[row_idx, f_idx[i]]]\n    mean_idx = [0, 1, 2, 0, 1, 2]\n    for i in range(n_classes * n_features):\n        expected_X_test_transform[n_rows - 1, i] = y_mean[mean_idx[i]]\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('categories, unknown_values', [([np.array([0, 1, 2], dtype=np.int64)], 'auto'), ([np.array(['cat', 'dog', 'snake'], dtype=object)], ['bear', 'rabbit'])])\n@pytest.mark.parametrize('target_labels', [np.array([1, 2, 3]), np.array(['a', 'b', 'c'])])\n@pytest.mark.parametrize('smooth', [5.0, 'auto'])\ndef test_encoding_multiclass(global_random_seed, categories, unknown_values, target_labels, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check encoding for multiclass targets.'\n    rng = np.random.RandomState(global_random_seed)\n    n_samples = 80\n    n_features = 2\n    feat_1_int = np.array(rng.randint(low=0, high=2, size=n_samples))\n    feat_2_int = np.array(rng.randint(low=0, high=3, size=n_samples))\n    feat_1 = categories[0][feat_1_int]\n    feat_2 = categories[0][feat_2_int]\n    X_train = np.column_stack((feat_1, feat_2))\n    X_train_int = np.column_stack((feat_1_int, feat_2_int))\n    categories_ = [[0, 1], [0, 1, 2]]\n    n_classes = 3\n    y_train_int = np.array(rng.randint(low=0, high=n_classes, size=n_samples))\n    y_train = target_labels[y_train_int]\n    y_train_enc = LabelBinarizer().fit_transform(y_train)\n    n_splits = 3\n    cv = StratifiedKFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n    expected_X_fit_transform = np.empty((X_train_int.shape[0], X_train_int.shape[1] * n_classes), dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            for (train_idx, test_idx) in cv.split(X_train, y_train):\n                y_class = y_train_enc[:, c_idx]\n                (X_, y_) = (X_train_int[train_idx, f_idx], y_class[train_idx])\n                current_encoding = _encode_target(X_, y_, len(cats), smooth)\n                exp_idx = c_idx + f_idx * n_classes\n                expected_X_fit_transform[test_idx, exp_idx] = current_encoding[X_train_int[test_idx, f_idx]]\n    target_encoder = TargetEncoder(smooth=smooth, cv=n_splits, random_state=global_random_seed)\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n    assert target_encoder.target_type_ == 'multiclass'\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories_):\n        for c_idx in range(n_classes):\n            y_class = y_train_enc[:, c_idx]\n            current_encoding = _encode_target(X_train_int[:, f_idx], y_class, len(cats), smooth)\n            expected_encodings.append(current_encoding)\n    assert len(target_encoder.encodings_) == n_features * n_classes\n    for i in range(n_features * n_classes):\n        assert_allclose(target_encoder.encodings_[i], expected_encodings[i])\n    assert_array_equal(target_encoder.classes_, target_labels)\n    X_test_int = np.array([[0, 1], [1, 2], [4, 5]])\n    if unknown_values == 'auto':\n        X_test = X_test_int\n    else:\n        X_test = np.empty_like(X_test_int[:-1, :], dtype=object)\n        for column_idx in range(X_test_int.shape[1]):\n            X_test[:, column_idx] = categories[0][X_test_int[:-1, column_idx]]\n        X_test = np.vstack((X_test, unknown_values))\n    y_mean = np.mean(y_train_enc, axis=0)\n    expected_X_test_transform = np.empty((X_test_int.shape[0], X_test_int.shape[1] * n_classes), dtype=np.float64)\n    n_rows = X_test_int.shape[0]\n    f_idx = [0, 0, 0, 1, 1, 1]\n    for row_idx in range(n_rows - 1):\n        for (i, enc) in enumerate(expected_encodings):\n            expected_X_test_transform[row_idx, i] = enc[X_test_int[row_idx, f_idx[i]]]\n    mean_idx = [0, 1, 2, 0, 1, 2]\n    for i in range(n_classes * n_features):\n        expected_X_test_transform[n_rows - 1, i] = y_mean[mean_idx[i]]\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)"
        ]
    },
    {
        "func_name": "test_custom_categories",
        "original": "@pytest.mark.parametrize('X, categories', [(np.array([[0] * 10 + [1] * 10 + [3]], dtype=np.int64).T, [[0, 1, 2]]), (np.array([['cat'] * 10 + ['dog'] * 10 + ['snake']], dtype=object).T, [['dog', 'cat', 'cow']])])\n@pytest.mark.parametrize('smooth', [4.0, 'auto'])\ndef test_custom_categories(X, categories, smooth):\n    \"\"\"Custom categories with unknown categories that are not in training data.\"\"\"\n    rng = np.random.RandomState(0)\n    y = rng.uniform(low=-10, high=20, size=X.shape[0])\n    enc = TargetEncoder(categories=categories, smooth=smooth, random_state=0).fit(X, y)\n    y_mean = y.mean()\n    X_trans = enc.transform(X[-1:])\n    assert X_trans[0, 0] == pytest.approx(y_mean)\n    assert len(enc.encodings_) == 1\n    assert enc.encodings_[0][-1] == pytest.approx(y_mean)",
        "mutated": [
            "@pytest.mark.parametrize('X, categories', [(np.array([[0] * 10 + [1] * 10 + [3]], dtype=np.int64).T, [[0, 1, 2]]), (np.array([['cat'] * 10 + ['dog'] * 10 + ['snake']], dtype=object).T, [['dog', 'cat', 'cow']])])\n@pytest.mark.parametrize('smooth', [4.0, 'auto'])\ndef test_custom_categories(X, categories, smooth):\n    if False:\n        i = 10\n    'Custom categories with unknown categories that are not in training data.'\n    rng = np.random.RandomState(0)\n    y = rng.uniform(low=-10, high=20, size=X.shape[0])\n    enc = TargetEncoder(categories=categories, smooth=smooth, random_state=0).fit(X, y)\n    y_mean = y.mean()\n    X_trans = enc.transform(X[-1:])\n    assert X_trans[0, 0] == pytest.approx(y_mean)\n    assert len(enc.encodings_) == 1\n    assert enc.encodings_[0][-1] == pytest.approx(y_mean)",
            "@pytest.mark.parametrize('X, categories', [(np.array([[0] * 10 + [1] * 10 + [3]], dtype=np.int64).T, [[0, 1, 2]]), (np.array([['cat'] * 10 + ['dog'] * 10 + ['snake']], dtype=object).T, [['dog', 'cat', 'cow']])])\n@pytest.mark.parametrize('smooth', [4.0, 'auto'])\ndef test_custom_categories(X, categories, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Custom categories with unknown categories that are not in training data.'\n    rng = np.random.RandomState(0)\n    y = rng.uniform(low=-10, high=20, size=X.shape[0])\n    enc = TargetEncoder(categories=categories, smooth=smooth, random_state=0).fit(X, y)\n    y_mean = y.mean()\n    X_trans = enc.transform(X[-1:])\n    assert X_trans[0, 0] == pytest.approx(y_mean)\n    assert len(enc.encodings_) == 1\n    assert enc.encodings_[0][-1] == pytest.approx(y_mean)",
            "@pytest.mark.parametrize('X, categories', [(np.array([[0] * 10 + [1] * 10 + [3]], dtype=np.int64).T, [[0, 1, 2]]), (np.array([['cat'] * 10 + ['dog'] * 10 + ['snake']], dtype=object).T, [['dog', 'cat', 'cow']])])\n@pytest.mark.parametrize('smooth', [4.0, 'auto'])\ndef test_custom_categories(X, categories, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Custom categories with unknown categories that are not in training data.'\n    rng = np.random.RandomState(0)\n    y = rng.uniform(low=-10, high=20, size=X.shape[0])\n    enc = TargetEncoder(categories=categories, smooth=smooth, random_state=0).fit(X, y)\n    y_mean = y.mean()\n    X_trans = enc.transform(X[-1:])\n    assert X_trans[0, 0] == pytest.approx(y_mean)\n    assert len(enc.encodings_) == 1\n    assert enc.encodings_[0][-1] == pytest.approx(y_mean)",
            "@pytest.mark.parametrize('X, categories', [(np.array([[0] * 10 + [1] * 10 + [3]], dtype=np.int64).T, [[0, 1, 2]]), (np.array([['cat'] * 10 + ['dog'] * 10 + ['snake']], dtype=object).T, [['dog', 'cat', 'cow']])])\n@pytest.mark.parametrize('smooth', [4.0, 'auto'])\ndef test_custom_categories(X, categories, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Custom categories with unknown categories that are not in training data.'\n    rng = np.random.RandomState(0)\n    y = rng.uniform(low=-10, high=20, size=X.shape[0])\n    enc = TargetEncoder(categories=categories, smooth=smooth, random_state=0).fit(X, y)\n    y_mean = y.mean()\n    X_trans = enc.transform(X[-1:])\n    assert X_trans[0, 0] == pytest.approx(y_mean)\n    assert len(enc.encodings_) == 1\n    assert enc.encodings_[0][-1] == pytest.approx(y_mean)",
            "@pytest.mark.parametrize('X, categories', [(np.array([[0] * 10 + [1] * 10 + [3]], dtype=np.int64).T, [[0, 1, 2]]), (np.array([['cat'] * 10 + ['dog'] * 10 + ['snake']], dtype=object).T, [['dog', 'cat', 'cow']])])\n@pytest.mark.parametrize('smooth', [4.0, 'auto'])\ndef test_custom_categories(X, categories, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Custom categories with unknown categories that are not in training data.'\n    rng = np.random.RandomState(0)\n    y = rng.uniform(low=-10, high=20, size=X.shape[0])\n    enc = TargetEncoder(categories=categories, smooth=smooth, random_state=0).fit(X, y)\n    y_mean = y.mean()\n    X_trans = enc.transform(X[-1:])\n    assert X_trans[0, 0] == pytest.approx(y_mean)\n    assert len(enc.encodings_) == 1\n    assert enc.encodings_[0][-1] == pytest.approx(y_mean)"
        ]
    },
    {
        "func_name": "test_errors",
        "original": "@pytest.mark.parametrize('y, msg', [([1, 2, 0, 1], 'Found input variables with inconsistent'), (np.array([[1, 2, 0], [1, 2, 3]]).T, \"Target type was inferred to be 'multiclass-multioutput'\")])\ndef test_errors(y, msg):\n    \"\"\"Check invalidate input.\"\"\"\n    X = np.array([[1, 0, 1]]).T\n    enc = TargetEncoder()\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X, y)",
        "mutated": [
            "@pytest.mark.parametrize('y, msg', [([1, 2, 0, 1], 'Found input variables with inconsistent'), (np.array([[1, 2, 0], [1, 2, 3]]).T, \"Target type was inferred to be 'multiclass-multioutput'\")])\ndef test_errors(y, msg):\n    if False:\n        i = 10\n    'Check invalidate input.'\n    X = np.array([[1, 0, 1]]).T\n    enc = TargetEncoder()\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X, y)",
            "@pytest.mark.parametrize('y, msg', [([1, 2, 0, 1], 'Found input variables with inconsistent'), (np.array([[1, 2, 0], [1, 2, 3]]).T, \"Target type was inferred to be 'multiclass-multioutput'\")])\ndef test_errors(y, msg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check invalidate input.'\n    X = np.array([[1, 0, 1]]).T\n    enc = TargetEncoder()\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X, y)",
            "@pytest.mark.parametrize('y, msg', [([1, 2, 0, 1], 'Found input variables with inconsistent'), (np.array([[1, 2, 0], [1, 2, 3]]).T, \"Target type was inferred to be 'multiclass-multioutput'\")])\ndef test_errors(y, msg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check invalidate input.'\n    X = np.array([[1, 0, 1]]).T\n    enc = TargetEncoder()\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X, y)",
            "@pytest.mark.parametrize('y, msg', [([1, 2, 0, 1], 'Found input variables with inconsistent'), (np.array([[1, 2, 0], [1, 2, 3]]).T, \"Target type was inferred to be 'multiclass-multioutput'\")])\ndef test_errors(y, msg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check invalidate input.'\n    X = np.array([[1, 0, 1]]).T\n    enc = TargetEncoder()\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X, y)",
            "@pytest.mark.parametrize('y, msg', [([1, 2, 0, 1], 'Found input variables with inconsistent'), (np.array([[1, 2, 0], [1, 2, 3]]).T, \"Target type was inferred to be 'multiclass-multioutput'\")])\ndef test_errors(y, msg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check invalidate input.'\n    X = np.array([[1, 0, 1]]).T\n    enc = TargetEncoder()\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X, y)"
        ]
    },
    {
        "func_name": "test_use_regression_target",
        "original": "def test_use_regression_target():\n    \"\"\"Check inferred and specified `target_type` on regression target.\"\"\"\n    X = np.array([[0, 1, 0, 1, 0, 1]]).T\n    y = np.array([1.0, 2.0, 3.0, 2.0, 3.0, 4.0])\n    enc = TargetEncoder(cv=2)\n    with pytest.warns(UserWarning, match=re.escape('The least populated class in y has only 1 members, which is less than n_splits=2.')):\n        enc.fit_transform(X, y)\n    assert enc.target_type_ == 'multiclass'\n    enc = TargetEncoder(cv=2, target_type='continuous')\n    enc.fit_transform(X, y)\n    assert enc.target_type_ == 'continuous'",
        "mutated": [
            "def test_use_regression_target():\n    if False:\n        i = 10\n    'Check inferred and specified `target_type` on regression target.'\n    X = np.array([[0, 1, 0, 1, 0, 1]]).T\n    y = np.array([1.0, 2.0, 3.0, 2.0, 3.0, 4.0])\n    enc = TargetEncoder(cv=2)\n    with pytest.warns(UserWarning, match=re.escape('The least populated class in y has only 1 members, which is less than n_splits=2.')):\n        enc.fit_transform(X, y)\n    assert enc.target_type_ == 'multiclass'\n    enc = TargetEncoder(cv=2, target_type='continuous')\n    enc.fit_transform(X, y)\n    assert enc.target_type_ == 'continuous'",
            "def test_use_regression_target():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check inferred and specified `target_type` on regression target.'\n    X = np.array([[0, 1, 0, 1, 0, 1]]).T\n    y = np.array([1.0, 2.0, 3.0, 2.0, 3.0, 4.0])\n    enc = TargetEncoder(cv=2)\n    with pytest.warns(UserWarning, match=re.escape('The least populated class in y has only 1 members, which is less than n_splits=2.')):\n        enc.fit_transform(X, y)\n    assert enc.target_type_ == 'multiclass'\n    enc = TargetEncoder(cv=2, target_type='continuous')\n    enc.fit_transform(X, y)\n    assert enc.target_type_ == 'continuous'",
            "def test_use_regression_target():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check inferred and specified `target_type` on regression target.'\n    X = np.array([[0, 1, 0, 1, 0, 1]]).T\n    y = np.array([1.0, 2.0, 3.0, 2.0, 3.0, 4.0])\n    enc = TargetEncoder(cv=2)\n    with pytest.warns(UserWarning, match=re.escape('The least populated class in y has only 1 members, which is less than n_splits=2.')):\n        enc.fit_transform(X, y)\n    assert enc.target_type_ == 'multiclass'\n    enc = TargetEncoder(cv=2, target_type='continuous')\n    enc.fit_transform(X, y)\n    assert enc.target_type_ == 'continuous'",
            "def test_use_regression_target():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check inferred and specified `target_type` on regression target.'\n    X = np.array([[0, 1, 0, 1, 0, 1]]).T\n    y = np.array([1.0, 2.0, 3.0, 2.0, 3.0, 4.0])\n    enc = TargetEncoder(cv=2)\n    with pytest.warns(UserWarning, match=re.escape('The least populated class in y has only 1 members, which is less than n_splits=2.')):\n        enc.fit_transform(X, y)\n    assert enc.target_type_ == 'multiclass'\n    enc = TargetEncoder(cv=2, target_type='continuous')\n    enc.fit_transform(X, y)\n    assert enc.target_type_ == 'continuous'",
            "def test_use_regression_target():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check inferred and specified `target_type` on regression target.'\n    X = np.array([[0, 1, 0, 1, 0, 1]]).T\n    y = np.array([1.0, 2.0, 3.0, 2.0, 3.0, 4.0])\n    enc = TargetEncoder(cv=2)\n    with pytest.warns(UserWarning, match=re.escape('The least populated class in y has only 1 members, which is less than n_splits=2.')):\n        enc.fit_transform(X, y)\n    assert enc.target_type_ == 'multiclass'\n    enc = TargetEncoder(cv=2, target_type='continuous')\n    enc.fit_transform(X, y)\n    assert enc.target_type_ == 'continuous'"
        ]
    },
    {
        "func_name": "test_feature_names_out_set_output",
        "original": "@pytest.mark.parametrize('y, feature_names', [([1, 2] * 10, ['A', 'B']), ([1, 2, 3] * 6 + [1, 2], ['A_1', 'A_2', 'A_3', 'B_1', 'B_2', 'B_3']), (['y1', 'y2', 'y3'] * 6 + ['y1', 'y2'], ['A_y1', 'A_y2', 'A_y3', 'B_y1', 'B_y2', 'B_y3'])])\ndef test_feature_names_out_set_output(y, feature_names):\n    \"\"\"Check TargetEncoder works with set_output.\"\"\"\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame({'A': ['a', 'b'] * 10, 'B': [1, 2] * 10})\n    enc_default = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_default.set_output(transform='default')\n    enc_pandas = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_pandas.set_output(transform='pandas')\n    X_default = enc_default.fit_transform(X_df, y)\n    X_pandas = enc_pandas.fit_transform(X_df, y)\n    assert_allclose(X_pandas.to_numpy(), X_default)\n    assert_array_equal(enc_pandas.get_feature_names_out(), feature_names)\n    assert_array_equal(enc_pandas.get_feature_names_out(), X_pandas.columns)",
        "mutated": [
            "@pytest.mark.parametrize('y, feature_names', [([1, 2] * 10, ['A', 'B']), ([1, 2, 3] * 6 + [1, 2], ['A_1', 'A_2', 'A_3', 'B_1', 'B_2', 'B_3']), (['y1', 'y2', 'y3'] * 6 + ['y1', 'y2'], ['A_y1', 'A_y2', 'A_y3', 'B_y1', 'B_y2', 'B_y3'])])\ndef test_feature_names_out_set_output(y, feature_names):\n    if False:\n        i = 10\n    'Check TargetEncoder works with set_output.'\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame({'A': ['a', 'b'] * 10, 'B': [1, 2] * 10})\n    enc_default = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_default.set_output(transform='default')\n    enc_pandas = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_pandas.set_output(transform='pandas')\n    X_default = enc_default.fit_transform(X_df, y)\n    X_pandas = enc_pandas.fit_transform(X_df, y)\n    assert_allclose(X_pandas.to_numpy(), X_default)\n    assert_array_equal(enc_pandas.get_feature_names_out(), feature_names)\n    assert_array_equal(enc_pandas.get_feature_names_out(), X_pandas.columns)",
            "@pytest.mark.parametrize('y, feature_names', [([1, 2] * 10, ['A', 'B']), ([1, 2, 3] * 6 + [1, 2], ['A_1', 'A_2', 'A_3', 'B_1', 'B_2', 'B_3']), (['y1', 'y2', 'y3'] * 6 + ['y1', 'y2'], ['A_y1', 'A_y2', 'A_y3', 'B_y1', 'B_y2', 'B_y3'])])\ndef test_feature_names_out_set_output(y, feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check TargetEncoder works with set_output.'\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame({'A': ['a', 'b'] * 10, 'B': [1, 2] * 10})\n    enc_default = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_default.set_output(transform='default')\n    enc_pandas = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_pandas.set_output(transform='pandas')\n    X_default = enc_default.fit_transform(X_df, y)\n    X_pandas = enc_pandas.fit_transform(X_df, y)\n    assert_allclose(X_pandas.to_numpy(), X_default)\n    assert_array_equal(enc_pandas.get_feature_names_out(), feature_names)\n    assert_array_equal(enc_pandas.get_feature_names_out(), X_pandas.columns)",
            "@pytest.mark.parametrize('y, feature_names', [([1, 2] * 10, ['A', 'B']), ([1, 2, 3] * 6 + [1, 2], ['A_1', 'A_2', 'A_3', 'B_1', 'B_2', 'B_3']), (['y1', 'y2', 'y3'] * 6 + ['y1', 'y2'], ['A_y1', 'A_y2', 'A_y3', 'B_y1', 'B_y2', 'B_y3'])])\ndef test_feature_names_out_set_output(y, feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check TargetEncoder works with set_output.'\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame({'A': ['a', 'b'] * 10, 'B': [1, 2] * 10})\n    enc_default = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_default.set_output(transform='default')\n    enc_pandas = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_pandas.set_output(transform='pandas')\n    X_default = enc_default.fit_transform(X_df, y)\n    X_pandas = enc_pandas.fit_transform(X_df, y)\n    assert_allclose(X_pandas.to_numpy(), X_default)\n    assert_array_equal(enc_pandas.get_feature_names_out(), feature_names)\n    assert_array_equal(enc_pandas.get_feature_names_out(), X_pandas.columns)",
            "@pytest.mark.parametrize('y, feature_names', [([1, 2] * 10, ['A', 'B']), ([1, 2, 3] * 6 + [1, 2], ['A_1', 'A_2', 'A_3', 'B_1', 'B_2', 'B_3']), (['y1', 'y2', 'y3'] * 6 + ['y1', 'y2'], ['A_y1', 'A_y2', 'A_y3', 'B_y1', 'B_y2', 'B_y3'])])\ndef test_feature_names_out_set_output(y, feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check TargetEncoder works with set_output.'\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame({'A': ['a', 'b'] * 10, 'B': [1, 2] * 10})\n    enc_default = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_default.set_output(transform='default')\n    enc_pandas = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_pandas.set_output(transform='pandas')\n    X_default = enc_default.fit_transform(X_df, y)\n    X_pandas = enc_pandas.fit_transform(X_df, y)\n    assert_allclose(X_pandas.to_numpy(), X_default)\n    assert_array_equal(enc_pandas.get_feature_names_out(), feature_names)\n    assert_array_equal(enc_pandas.get_feature_names_out(), X_pandas.columns)",
            "@pytest.mark.parametrize('y, feature_names', [([1, 2] * 10, ['A', 'B']), ([1, 2, 3] * 6 + [1, 2], ['A_1', 'A_2', 'A_3', 'B_1', 'B_2', 'B_3']), (['y1', 'y2', 'y3'] * 6 + ['y1', 'y2'], ['A_y1', 'A_y2', 'A_y3', 'B_y1', 'B_y2', 'B_y3'])])\ndef test_feature_names_out_set_output(y, feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check TargetEncoder works with set_output.'\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame({'A': ['a', 'b'] * 10, 'B': [1, 2] * 10})\n    enc_default = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_default.set_output(transform='default')\n    enc_pandas = TargetEncoder(cv=2, smooth=3.0, random_state=0)\n    enc_pandas.set_output(transform='pandas')\n    X_default = enc_default.fit_transform(X_df, y)\n    X_pandas = enc_pandas.fit_transform(X_df, y)\n    assert_allclose(X_pandas.to_numpy(), X_default)\n    assert_array_equal(enc_pandas.get_feature_names_out(), feature_names)\n    assert_array_equal(enc_pandas.get_feature_names_out(), X_pandas.columns)"
        ]
    },
    {
        "func_name": "test_multiple_features_quick",
        "original": "@pytest.mark.parametrize('to_pandas', [True, False])\n@pytest.mark.parametrize('smooth', [1.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary-ints', 'binary-str', 'continuous'])\ndef test_multiple_features_quick(to_pandas, smooth, target_type):\n    \"\"\"Check target encoder with multiple features.\"\"\"\n    X_ordinal = np.array([[1, 1], [0, 1], [1, 1], [2, 1], [1, 0], [0, 1], [1, 0], [0, 0]], dtype=np.int64)\n    if target_type == 'binary-str':\n        y_train = np.array(['a', 'b', 'a', 'a', 'b', 'b', 'a', 'b'])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    elif target_type == 'binary-ints':\n        y_train = np.array([3, 4, 3, 3, 3, 4, 4, 4])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    else:\n        y_train = np.array([3.0, 5.1, 2.4, 3.5, 4.1, 5.5, 10.3, 7.3], dtype=np.float32)\n        y_integer = y_train\n        cv = KFold(2, random_state=0, shuffle=True)\n    y_mean = np.mean(y_integer)\n    categories = [[0, 1, 2], [0, 1]]\n    X_test = np.array([[0, 1], [3, 0], [1, 10]], dtype=np.int64)\n    if to_pandas:\n        pd = pytest.importorskip('pandas')\n        X_train = pd.DataFrame({'feat0': X_ordinal[:, 0], 'feat1': np.array(['cat', 'dog'], dtype=object)[X_ordinal[:, 1]]})\n        X_test = pd.DataFrame({'feat0': X_test[:, 0], 'feat1': ['dog', 'cat', 'snake']})\n    else:\n        X_train = X_ordinal\n    expected_X_fit_transform = np.empty_like(X_ordinal, dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories):\n        for (train_idx, test_idx) in cv.split(X_ordinal, y_integer):\n            (X_, y_) = (X_ordinal[train_idx, f_idx], y_integer[train_idx])\n            current_encoding = _encode_target(X_, y_, len(cats), smooth)\n            expected_X_fit_transform[test_idx, f_idx] = current_encoding[X_ordinal[test_idx, f_idx]]\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories):\n        current_encoding = _encode_target(X_ordinal[:, f_idx], y_integer, len(cats), smooth)\n        expected_encodings.append(current_encoding)\n    expected_X_test_transform = np.array([[expected_encodings[0][0], expected_encodings[1][1]], [y_mean, expected_encodings[1][0]], [expected_encodings[0][1], y_mean]], dtype=np.float64)\n    enc = TargetEncoder(smooth=smooth, cv=2, random_state=0)\n    X_fit_transform = enc.fit_transform(X_train, y_train)\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(enc.encodings_) == 2\n    for i in range(2):\n        assert_allclose(enc.encodings_[i], expected_encodings[i])\n    X_test_transform = enc.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
        "mutated": [
            "@pytest.mark.parametrize('to_pandas', [True, False])\n@pytest.mark.parametrize('smooth', [1.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary-ints', 'binary-str', 'continuous'])\ndef test_multiple_features_quick(to_pandas, smooth, target_type):\n    if False:\n        i = 10\n    'Check target encoder with multiple features.'\n    X_ordinal = np.array([[1, 1], [0, 1], [1, 1], [2, 1], [1, 0], [0, 1], [1, 0], [0, 0]], dtype=np.int64)\n    if target_type == 'binary-str':\n        y_train = np.array(['a', 'b', 'a', 'a', 'b', 'b', 'a', 'b'])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    elif target_type == 'binary-ints':\n        y_train = np.array([3, 4, 3, 3, 3, 4, 4, 4])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    else:\n        y_train = np.array([3.0, 5.1, 2.4, 3.5, 4.1, 5.5, 10.3, 7.3], dtype=np.float32)\n        y_integer = y_train\n        cv = KFold(2, random_state=0, shuffle=True)\n    y_mean = np.mean(y_integer)\n    categories = [[0, 1, 2], [0, 1]]\n    X_test = np.array([[0, 1], [3, 0], [1, 10]], dtype=np.int64)\n    if to_pandas:\n        pd = pytest.importorskip('pandas')\n        X_train = pd.DataFrame({'feat0': X_ordinal[:, 0], 'feat1': np.array(['cat', 'dog'], dtype=object)[X_ordinal[:, 1]]})\n        X_test = pd.DataFrame({'feat0': X_test[:, 0], 'feat1': ['dog', 'cat', 'snake']})\n    else:\n        X_train = X_ordinal\n    expected_X_fit_transform = np.empty_like(X_ordinal, dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories):\n        for (train_idx, test_idx) in cv.split(X_ordinal, y_integer):\n            (X_, y_) = (X_ordinal[train_idx, f_idx], y_integer[train_idx])\n            current_encoding = _encode_target(X_, y_, len(cats), smooth)\n            expected_X_fit_transform[test_idx, f_idx] = current_encoding[X_ordinal[test_idx, f_idx]]\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories):\n        current_encoding = _encode_target(X_ordinal[:, f_idx], y_integer, len(cats), smooth)\n        expected_encodings.append(current_encoding)\n    expected_X_test_transform = np.array([[expected_encodings[0][0], expected_encodings[1][1]], [y_mean, expected_encodings[1][0]], [expected_encodings[0][1], y_mean]], dtype=np.float64)\n    enc = TargetEncoder(smooth=smooth, cv=2, random_state=0)\n    X_fit_transform = enc.fit_transform(X_train, y_train)\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(enc.encodings_) == 2\n    for i in range(2):\n        assert_allclose(enc.encodings_[i], expected_encodings[i])\n    X_test_transform = enc.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('to_pandas', [True, False])\n@pytest.mark.parametrize('smooth', [1.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary-ints', 'binary-str', 'continuous'])\ndef test_multiple_features_quick(to_pandas, smooth, target_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check target encoder with multiple features.'\n    X_ordinal = np.array([[1, 1], [0, 1], [1, 1], [2, 1], [1, 0], [0, 1], [1, 0], [0, 0]], dtype=np.int64)\n    if target_type == 'binary-str':\n        y_train = np.array(['a', 'b', 'a', 'a', 'b', 'b', 'a', 'b'])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    elif target_type == 'binary-ints':\n        y_train = np.array([3, 4, 3, 3, 3, 4, 4, 4])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    else:\n        y_train = np.array([3.0, 5.1, 2.4, 3.5, 4.1, 5.5, 10.3, 7.3], dtype=np.float32)\n        y_integer = y_train\n        cv = KFold(2, random_state=0, shuffle=True)\n    y_mean = np.mean(y_integer)\n    categories = [[0, 1, 2], [0, 1]]\n    X_test = np.array([[0, 1], [3, 0], [1, 10]], dtype=np.int64)\n    if to_pandas:\n        pd = pytest.importorskip('pandas')\n        X_train = pd.DataFrame({'feat0': X_ordinal[:, 0], 'feat1': np.array(['cat', 'dog'], dtype=object)[X_ordinal[:, 1]]})\n        X_test = pd.DataFrame({'feat0': X_test[:, 0], 'feat1': ['dog', 'cat', 'snake']})\n    else:\n        X_train = X_ordinal\n    expected_X_fit_transform = np.empty_like(X_ordinal, dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories):\n        for (train_idx, test_idx) in cv.split(X_ordinal, y_integer):\n            (X_, y_) = (X_ordinal[train_idx, f_idx], y_integer[train_idx])\n            current_encoding = _encode_target(X_, y_, len(cats), smooth)\n            expected_X_fit_transform[test_idx, f_idx] = current_encoding[X_ordinal[test_idx, f_idx]]\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories):\n        current_encoding = _encode_target(X_ordinal[:, f_idx], y_integer, len(cats), smooth)\n        expected_encodings.append(current_encoding)\n    expected_X_test_transform = np.array([[expected_encodings[0][0], expected_encodings[1][1]], [y_mean, expected_encodings[1][0]], [expected_encodings[0][1], y_mean]], dtype=np.float64)\n    enc = TargetEncoder(smooth=smooth, cv=2, random_state=0)\n    X_fit_transform = enc.fit_transform(X_train, y_train)\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(enc.encodings_) == 2\n    for i in range(2):\n        assert_allclose(enc.encodings_[i], expected_encodings[i])\n    X_test_transform = enc.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('to_pandas', [True, False])\n@pytest.mark.parametrize('smooth', [1.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary-ints', 'binary-str', 'continuous'])\ndef test_multiple_features_quick(to_pandas, smooth, target_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check target encoder with multiple features.'\n    X_ordinal = np.array([[1, 1], [0, 1], [1, 1], [2, 1], [1, 0], [0, 1], [1, 0], [0, 0]], dtype=np.int64)\n    if target_type == 'binary-str':\n        y_train = np.array(['a', 'b', 'a', 'a', 'b', 'b', 'a', 'b'])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    elif target_type == 'binary-ints':\n        y_train = np.array([3, 4, 3, 3, 3, 4, 4, 4])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    else:\n        y_train = np.array([3.0, 5.1, 2.4, 3.5, 4.1, 5.5, 10.3, 7.3], dtype=np.float32)\n        y_integer = y_train\n        cv = KFold(2, random_state=0, shuffle=True)\n    y_mean = np.mean(y_integer)\n    categories = [[0, 1, 2], [0, 1]]\n    X_test = np.array([[0, 1], [3, 0], [1, 10]], dtype=np.int64)\n    if to_pandas:\n        pd = pytest.importorskip('pandas')\n        X_train = pd.DataFrame({'feat0': X_ordinal[:, 0], 'feat1': np.array(['cat', 'dog'], dtype=object)[X_ordinal[:, 1]]})\n        X_test = pd.DataFrame({'feat0': X_test[:, 0], 'feat1': ['dog', 'cat', 'snake']})\n    else:\n        X_train = X_ordinal\n    expected_X_fit_transform = np.empty_like(X_ordinal, dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories):\n        for (train_idx, test_idx) in cv.split(X_ordinal, y_integer):\n            (X_, y_) = (X_ordinal[train_idx, f_idx], y_integer[train_idx])\n            current_encoding = _encode_target(X_, y_, len(cats), smooth)\n            expected_X_fit_transform[test_idx, f_idx] = current_encoding[X_ordinal[test_idx, f_idx]]\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories):\n        current_encoding = _encode_target(X_ordinal[:, f_idx], y_integer, len(cats), smooth)\n        expected_encodings.append(current_encoding)\n    expected_X_test_transform = np.array([[expected_encodings[0][0], expected_encodings[1][1]], [y_mean, expected_encodings[1][0]], [expected_encodings[0][1], y_mean]], dtype=np.float64)\n    enc = TargetEncoder(smooth=smooth, cv=2, random_state=0)\n    X_fit_transform = enc.fit_transform(X_train, y_train)\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(enc.encodings_) == 2\n    for i in range(2):\n        assert_allclose(enc.encodings_[i], expected_encodings[i])\n    X_test_transform = enc.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('to_pandas', [True, False])\n@pytest.mark.parametrize('smooth', [1.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary-ints', 'binary-str', 'continuous'])\ndef test_multiple_features_quick(to_pandas, smooth, target_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check target encoder with multiple features.'\n    X_ordinal = np.array([[1, 1], [0, 1], [1, 1], [2, 1], [1, 0], [0, 1], [1, 0], [0, 0]], dtype=np.int64)\n    if target_type == 'binary-str':\n        y_train = np.array(['a', 'b', 'a', 'a', 'b', 'b', 'a', 'b'])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    elif target_type == 'binary-ints':\n        y_train = np.array([3, 4, 3, 3, 3, 4, 4, 4])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    else:\n        y_train = np.array([3.0, 5.1, 2.4, 3.5, 4.1, 5.5, 10.3, 7.3], dtype=np.float32)\n        y_integer = y_train\n        cv = KFold(2, random_state=0, shuffle=True)\n    y_mean = np.mean(y_integer)\n    categories = [[0, 1, 2], [0, 1]]\n    X_test = np.array([[0, 1], [3, 0], [1, 10]], dtype=np.int64)\n    if to_pandas:\n        pd = pytest.importorskip('pandas')\n        X_train = pd.DataFrame({'feat0': X_ordinal[:, 0], 'feat1': np.array(['cat', 'dog'], dtype=object)[X_ordinal[:, 1]]})\n        X_test = pd.DataFrame({'feat0': X_test[:, 0], 'feat1': ['dog', 'cat', 'snake']})\n    else:\n        X_train = X_ordinal\n    expected_X_fit_transform = np.empty_like(X_ordinal, dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories):\n        for (train_idx, test_idx) in cv.split(X_ordinal, y_integer):\n            (X_, y_) = (X_ordinal[train_idx, f_idx], y_integer[train_idx])\n            current_encoding = _encode_target(X_, y_, len(cats), smooth)\n            expected_X_fit_transform[test_idx, f_idx] = current_encoding[X_ordinal[test_idx, f_idx]]\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories):\n        current_encoding = _encode_target(X_ordinal[:, f_idx], y_integer, len(cats), smooth)\n        expected_encodings.append(current_encoding)\n    expected_X_test_transform = np.array([[expected_encodings[0][0], expected_encodings[1][1]], [y_mean, expected_encodings[1][0]], [expected_encodings[0][1], y_mean]], dtype=np.float64)\n    enc = TargetEncoder(smooth=smooth, cv=2, random_state=0)\n    X_fit_transform = enc.fit_transform(X_train, y_train)\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(enc.encodings_) == 2\n    for i in range(2):\n        assert_allclose(enc.encodings_[i], expected_encodings[i])\n    X_test_transform = enc.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)",
            "@pytest.mark.parametrize('to_pandas', [True, False])\n@pytest.mark.parametrize('smooth', [1.0, 'auto'])\n@pytest.mark.parametrize('target_type', ['binary-ints', 'binary-str', 'continuous'])\ndef test_multiple_features_quick(to_pandas, smooth, target_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check target encoder with multiple features.'\n    X_ordinal = np.array([[1, 1], [0, 1], [1, 1], [2, 1], [1, 0], [0, 1], [1, 0], [0, 0]], dtype=np.int64)\n    if target_type == 'binary-str':\n        y_train = np.array(['a', 'b', 'a', 'a', 'b', 'b', 'a', 'b'])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    elif target_type == 'binary-ints':\n        y_train = np.array([3, 4, 3, 3, 3, 4, 4, 4])\n        y_integer = LabelEncoder().fit_transform(y_train)\n        cv = StratifiedKFold(2, random_state=0, shuffle=True)\n    else:\n        y_train = np.array([3.0, 5.1, 2.4, 3.5, 4.1, 5.5, 10.3, 7.3], dtype=np.float32)\n        y_integer = y_train\n        cv = KFold(2, random_state=0, shuffle=True)\n    y_mean = np.mean(y_integer)\n    categories = [[0, 1, 2], [0, 1]]\n    X_test = np.array([[0, 1], [3, 0], [1, 10]], dtype=np.int64)\n    if to_pandas:\n        pd = pytest.importorskip('pandas')\n        X_train = pd.DataFrame({'feat0': X_ordinal[:, 0], 'feat1': np.array(['cat', 'dog'], dtype=object)[X_ordinal[:, 1]]})\n        X_test = pd.DataFrame({'feat0': X_test[:, 0], 'feat1': ['dog', 'cat', 'snake']})\n    else:\n        X_train = X_ordinal\n    expected_X_fit_transform = np.empty_like(X_ordinal, dtype=np.float64)\n    for (f_idx, cats) in enumerate(categories):\n        for (train_idx, test_idx) in cv.split(X_ordinal, y_integer):\n            (X_, y_) = (X_ordinal[train_idx, f_idx], y_integer[train_idx])\n            current_encoding = _encode_target(X_, y_, len(cats), smooth)\n            expected_X_fit_transform[test_idx, f_idx] = current_encoding[X_ordinal[test_idx, f_idx]]\n    expected_encodings = []\n    for (f_idx, cats) in enumerate(categories):\n        current_encoding = _encode_target(X_ordinal[:, f_idx], y_integer, len(cats), smooth)\n        expected_encodings.append(current_encoding)\n    expected_X_test_transform = np.array([[expected_encodings[0][0], expected_encodings[1][1]], [y_mean, expected_encodings[1][0]], [expected_encodings[0][1], y_mean]], dtype=np.float64)\n    enc = TargetEncoder(smooth=smooth, cv=2, random_state=0)\n    X_fit_transform = enc.fit_transform(X_train, y_train)\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(enc.encodings_) == 2\n    for i in range(2):\n        assert_allclose(enc.encodings_[i], expected_encodings[i])\n    X_test_transform = enc.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)"
        ]
    },
    {
        "func_name": "test_constant_target_and_feature",
        "original": "@pytest.mark.parametrize('y, y_mean', [(np.array([3.4] * 20), 3.4), (np.array([0] * 20), 0), (np.array(['a'] * 20, dtype=object), 0)], ids=['continuous', 'binary', 'binary-string'])\n@pytest.mark.parametrize('smooth', ['auto', 4.0, 0.0])\ndef test_constant_target_and_feature(y, y_mean, smooth):\n    \"\"\"Check edge case where feature and target is constant.\"\"\"\n    X = np.array([[1] * 20]).T\n    n_samples = X.shape[0]\n    enc = TargetEncoder(cv=2, smooth=smooth, random_state=0)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans, np.repeat([[y_mean]], n_samples, axis=0))\n    assert enc.encodings_[0][0] == pytest.approx(y_mean)\n    assert enc.target_mean_ == pytest.approx(y_mean)\n    X_test = np.array([[1], [0]])\n    X_test_trans = enc.transform(X_test)\n    assert_allclose(X_test_trans, np.repeat([[y_mean]], 2, axis=0))",
        "mutated": [
            "@pytest.mark.parametrize('y, y_mean', [(np.array([3.4] * 20), 3.4), (np.array([0] * 20), 0), (np.array(['a'] * 20, dtype=object), 0)], ids=['continuous', 'binary', 'binary-string'])\n@pytest.mark.parametrize('smooth', ['auto', 4.0, 0.0])\ndef test_constant_target_and_feature(y, y_mean, smooth):\n    if False:\n        i = 10\n    'Check edge case where feature and target is constant.'\n    X = np.array([[1] * 20]).T\n    n_samples = X.shape[0]\n    enc = TargetEncoder(cv=2, smooth=smooth, random_state=0)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans, np.repeat([[y_mean]], n_samples, axis=0))\n    assert enc.encodings_[0][0] == pytest.approx(y_mean)\n    assert enc.target_mean_ == pytest.approx(y_mean)\n    X_test = np.array([[1], [0]])\n    X_test_trans = enc.transform(X_test)\n    assert_allclose(X_test_trans, np.repeat([[y_mean]], 2, axis=0))",
            "@pytest.mark.parametrize('y, y_mean', [(np.array([3.4] * 20), 3.4), (np.array([0] * 20), 0), (np.array(['a'] * 20, dtype=object), 0)], ids=['continuous', 'binary', 'binary-string'])\n@pytest.mark.parametrize('smooth', ['auto', 4.0, 0.0])\ndef test_constant_target_and_feature(y, y_mean, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check edge case where feature and target is constant.'\n    X = np.array([[1] * 20]).T\n    n_samples = X.shape[0]\n    enc = TargetEncoder(cv=2, smooth=smooth, random_state=0)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans, np.repeat([[y_mean]], n_samples, axis=0))\n    assert enc.encodings_[0][0] == pytest.approx(y_mean)\n    assert enc.target_mean_ == pytest.approx(y_mean)\n    X_test = np.array([[1], [0]])\n    X_test_trans = enc.transform(X_test)\n    assert_allclose(X_test_trans, np.repeat([[y_mean]], 2, axis=0))",
            "@pytest.mark.parametrize('y, y_mean', [(np.array([3.4] * 20), 3.4), (np.array([0] * 20), 0), (np.array(['a'] * 20, dtype=object), 0)], ids=['continuous', 'binary', 'binary-string'])\n@pytest.mark.parametrize('smooth', ['auto', 4.0, 0.0])\ndef test_constant_target_and_feature(y, y_mean, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check edge case where feature and target is constant.'\n    X = np.array([[1] * 20]).T\n    n_samples = X.shape[0]\n    enc = TargetEncoder(cv=2, smooth=smooth, random_state=0)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans, np.repeat([[y_mean]], n_samples, axis=0))\n    assert enc.encodings_[0][0] == pytest.approx(y_mean)\n    assert enc.target_mean_ == pytest.approx(y_mean)\n    X_test = np.array([[1], [0]])\n    X_test_trans = enc.transform(X_test)\n    assert_allclose(X_test_trans, np.repeat([[y_mean]], 2, axis=0))",
            "@pytest.mark.parametrize('y, y_mean', [(np.array([3.4] * 20), 3.4), (np.array([0] * 20), 0), (np.array(['a'] * 20, dtype=object), 0)], ids=['continuous', 'binary', 'binary-string'])\n@pytest.mark.parametrize('smooth', ['auto', 4.0, 0.0])\ndef test_constant_target_and_feature(y, y_mean, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check edge case where feature and target is constant.'\n    X = np.array([[1] * 20]).T\n    n_samples = X.shape[0]\n    enc = TargetEncoder(cv=2, smooth=smooth, random_state=0)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans, np.repeat([[y_mean]], n_samples, axis=0))\n    assert enc.encodings_[0][0] == pytest.approx(y_mean)\n    assert enc.target_mean_ == pytest.approx(y_mean)\n    X_test = np.array([[1], [0]])\n    X_test_trans = enc.transform(X_test)\n    assert_allclose(X_test_trans, np.repeat([[y_mean]], 2, axis=0))",
            "@pytest.mark.parametrize('y, y_mean', [(np.array([3.4] * 20), 3.4), (np.array([0] * 20), 0), (np.array(['a'] * 20, dtype=object), 0)], ids=['continuous', 'binary', 'binary-string'])\n@pytest.mark.parametrize('smooth', ['auto', 4.0, 0.0])\ndef test_constant_target_and_feature(y, y_mean, smooth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check edge case where feature and target is constant.'\n    X = np.array([[1] * 20]).T\n    n_samples = X.shape[0]\n    enc = TargetEncoder(cv=2, smooth=smooth, random_state=0)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans, np.repeat([[y_mean]], n_samples, axis=0))\n    assert enc.encodings_[0][0] == pytest.approx(y_mean)\n    assert enc.target_mean_ == pytest.approx(y_mean)\n    X_test = np.array([[1], [0]])\n    X_test_trans = enc.transform(X_test)\n    assert_allclose(X_test_trans, np.repeat([[y_mean]], 2, axis=0))"
        ]
    },
    {
        "func_name": "test_fit_transform_not_associated_with_y_if_ordinal_categorical_is_not",
        "original": "def test_fit_transform_not_associated_with_y_if_ordinal_categorical_is_not(global_random_seed):\n    cardinality = 30\n    n_samples = 3000\n    rng = np.random.RandomState(global_random_seed)\n    y_train = rng.normal(size=n_samples)\n    X_train = rng.randint(0, cardinality, size=n_samples).reshape(-1, 1)\n    y_sorted_indices = y_train.argsort()\n    y_train = y_train[y_sorted_indices]\n    X_train = X_train[y_sorted_indices]\n    target_encoder = TargetEncoder(shuffle=True, random_state=global_random_seed)\n    X_encoded_train_shuffled = target_encoder.fit_transform(X_train, y_train)\n    target_encoder = TargetEncoder(shuffle=False)\n    X_encoded_train_no_shuffled = target_encoder.fit_transform(X_train, y_train)\n    regressor = RandomForestRegressor(n_estimators=10, min_samples_leaf=20, random_state=global_random_seed)\n    cv = ShuffleSplit(n_splits=50, random_state=global_random_seed)\n    assert cross_val_score(regressor, X_train, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_shuffled, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_no_shuffled, y_train, cv=cv).mean() > 0.5",
        "mutated": [
            "def test_fit_transform_not_associated_with_y_if_ordinal_categorical_is_not(global_random_seed):\n    if False:\n        i = 10\n    cardinality = 30\n    n_samples = 3000\n    rng = np.random.RandomState(global_random_seed)\n    y_train = rng.normal(size=n_samples)\n    X_train = rng.randint(0, cardinality, size=n_samples).reshape(-1, 1)\n    y_sorted_indices = y_train.argsort()\n    y_train = y_train[y_sorted_indices]\n    X_train = X_train[y_sorted_indices]\n    target_encoder = TargetEncoder(shuffle=True, random_state=global_random_seed)\n    X_encoded_train_shuffled = target_encoder.fit_transform(X_train, y_train)\n    target_encoder = TargetEncoder(shuffle=False)\n    X_encoded_train_no_shuffled = target_encoder.fit_transform(X_train, y_train)\n    regressor = RandomForestRegressor(n_estimators=10, min_samples_leaf=20, random_state=global_random_seed)\n    cv = ShuffleSplit(n_splits=50, random_state=global_random_seed)\n    assert cross_val_score(regressor, X_train, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_shuffled, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_no_shuffled, y_train, cv=cv).mean() > 0.5",
            "def test_fit_transform_not_associated_with_y_if_ordinal_categorical_is_not(global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cardinality = 30\n    n_samples = 3000\n    rng = np.random.RandomState(global_random_seed)\n    y_train = rng.normal(size=n_samples)\n    X_train = rng.randint(0, cardinality, size=n_samples).reshape(-1, 1)\n    y_sorted_indices = y_train.argsort()\n    y_train = y_train[y_sorted_indices]\n    X_train = X_train[y_sorted_indices]\n    target_encoder = TargetEncoder(shuffle=True, random_state=global_random_seed)\n    X_encoded_train_shuffled = target_encoder.fit_transform(X_train, y_train)\n    target_encoder = TargetEncoder(shuffle=False)\n    X_encoded_train_no_shuffled = target_encoder.fit_transform(X_train, y_train)\n    regressor = RandomForestRegressor(n_estimators=10, min_samples_leaf=20, random_state=global_random_seed)\n    cv = ShuffleSplit(n_splits=50, random_state=global_random_seed)\n    assert cross_val_score(regressor, X_train, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_shuffled, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_no_shuffled, y_train, cv=cv).mean() > 0.5",
            "def test_fit_transform_not_associated_with_y_if_ordinal_categorical_is_not(global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cardinality = 30\n    n_samples = 3000\n    rng = np.random.RandomState(global_random_seed)\n    y_train = rng.normal(size=n_samples)\n    X_train = rng.randint(0, cardinality, size=n_samples).reshape(-1, 1)\n    y_sorted_indices = y_train.argsort()\n    y_train = y_train[y_sorted_indices]\n    X_train = X_train[y_sorted_indices]\n    target_encoder = TargetEncoder(shuffle=True, random_state=global_random_seed)\n    X_encoded_train_shuffled = target_encoder.fit_transform(X_train, y_train)\n    target_encoder = TargetEncoder(shuffle=False)\n    X_encoded_train_no_shuffled = target_encoder.fit_transform(X_train, y_train)\n    regressor = RandomForestRegressor(n_estimators=10, min_samples_leaf=20, random_state=global_random_seed)\n    cv = ShuffleSplit(n_splits=50, random_state=global_random_seed)\n    assert cross_val_score(regressor, X_train, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_shuffled, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_no_shuffled, y_train, cv=cv).mean() > 0.5",
            "def test_fit_transform_not_associated_with_y_if_ordinal_categorical_is_not(global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cardinality = 30\n    n_samples = 3000\n    rng = np.random.RandomState(global_random_seed)\n    y_train = rng.normal(size=n_samples)\n    X_train = rng.randint(0, cardinality, size=n_samples).reshape(-1, 1)\n    y_sorted_indices = y_train.argsort()\n    y_train = y_train[y_sorted_indices]\n    X_train = X_train[y_sorted_indices]\n    target_encoder = TargetEncoder(shuffle=True, random_state=global_random_seed)\n    X_encoded_train_shuffled = target_encoder.fit_transform(X_train, y_train)\n    target_encoder = TargetEncoder(shuffle=False)\n    X_encoded_train_no_shuffled = target_encoder.fit_transform(X_train, y_train)\n    regressor = RandomForestRegressor(n_estimators=10, min_samples_leaf=20, random_state=global_random_seed)\n    cv = ShuffleSplit(n_splits=50, random_state=global_random_seed)\n    assert cross_val_score(regressor, X_train, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_shuffled, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_no_shuffled, y_train, cv=cv).mean() > 0.5",
            "def test_fit_transform_not_associated_with_y_if_ordinal_categorical_is_not(global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cardinality = 30\n    n_samples = 3000\n    rng = np.random.RandomState(global_random_seed)\n    y_train = rng.normal(size=n_samples)\n    X_train = rng.randint(0, cardinality, size=n_samples).reshape(-1, 1)\n    y_sorted_indices = y_train.argsort()\n    y_train = y_train[y_sorted_indices]\n    X_train = X_train[y_sorted_indices]\n    target_encoder = TargetEncoder(shuffle=True, random_state=global_random_seed)\n    X_encoded_train_shuffled = target_encoder.fit_transform(X_train, y_train)\n    target_encoder = TargetEncoder(shuffle=False)\n    X_encoded_train_no_shuffled = target_encoder.fit_transform(X_train, y_train)\n    regressor = RandomForestRegressor(n_estimators=10, min_samples_leaf=20, random_state=global_random_seed)\n    cv = ShuffleSplit(n_splits=50, random_state=global_random_seed)\n    assert cross_val_score(regressor, X_train, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_shuffled, y_train, cv=cv).mean() < 0.1\n    assert cross_val_score(regressor, X_encoded_train_no_shuffled, y_train, cv=cv).mean() > 0.5"
        ]
    },
    {
        "func_name": "test_smooth_zero",
        "original": "def test_smooth_zero():\n    \"\"\"Check edge case with zero smoothing and cv does not contain category.\"\"\"\n    X = np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]).T\n    y = np.array([2.1, 4.3, 1.2, 3.1, 1.0, 9.0, 10.3, 14.2, 13.3, 15.0])\n    enc = TargetEncoder(smooth=0.0, shuffle=False, cv=2)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans[0], np.mean(y[5:]))\n    assert_allclose(X_trans[-1], np.mean(y[:5]))",
        "mutated": [
            "def test_smooth_zero():\n    if False:\n        i = 10\n    'Check edge case with zero smoothing and cv does not contain category.'\n    X = np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]).T\n    y = np.array([2.1, 4.3, 1.2, 3.1, 1.0, 9.0, 10.3, 14.2, 13.3, 15.0])\n    enc = TargetEncoder(smooth=0.0, shuffle=False, cv=2)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans[0], np.mean(y[5:]))\n    assert_allclose(X_trans[-1], np.mean(y[:5]))",
            "def test_smooth_zero():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check edge case with zero smoothing and cv does not contain category.'\n    X = np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]).T\n    y = np.array([2.1, 4.3, 1.2, 3.1, 1.0, 9.0, 10.3, 14.2, 13.3, 15.0])\n    enc = TargetEncoder(smooth=0.0, shuffle=False, cv=2)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans[0], np.mean(y[5:]))\n    assert_allclose(X_trans[-1], np.mean(y[:5]))",
            "def test_smooth_zero():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check edge case with zero smoothing and cv does not contain category.'\n    X = np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]).T\n    y = np.array([2.1, 4.3, 1.2, 3.1, 1.0, 9.0, 10.3, 14.2, 13.3, 15.0])\n    enc = TargetEncoder(smooth=0.0, shuffle=False, cv=2)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans[0], np.mean(y[5:]))\n    assert_allclose(X_trans[-1], np.mean(y[:5]))",
            "def test_smooth_zero():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check edge case with zero smoothing and cv does not contain category.'\n    X = np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]).T\n    y = np.array([2.1, 4.3, 1.2, 3.1, 1.0, 9.0, 10.3, 14.2, 13.3, 15.0])\n    enc = TargetEncoder(smooth=0.0, shuffle=False, cv=2)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans[0], np.mean(y[5:]))\n    assert_allclose(X_trans[-1], np.mean(y[:5]))",
            "def test_smooth_zero():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check edge case with zero smoothing and cv does not contain category.'\n    X = np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]).T\n    y = np.array([2.1, 4.3, 1.2, 3.1, 1.0, 9.0, 10.3, 14.2, 13.3, 15.0])\n    enc = TargetEncoder(smooth=0.0, shuffle=False, cv=2)\n    X_trans = enc.fit_transform(X, y)\n    assert_allclose(X_trans[0], np.mean(y[5:]))\n    assert_allclose(X_trans[-1], np.mean(y[:5]))"
        ]
    },
    {
        "func_name": "test_invariance_of_encoding_under_label_permutation",
        "original": "@pytest.mark.parametrize('smooth', [0.0, 1000.0, 'auto'])\ndef test_invariance_of_encoding_under_label_permutation(smooth, global_random_seed):\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.normal(size=1000)\n    n_categories = 30\n    X = KBinsDiscretizer(n_bins=n_categories, encode='ordinal').fit_transform(y.reshape(-1, 1))\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=global_random_seed)\n    permutated_labels = rng.permutation(n_categories)\n    X_train_permuted = permutated_labels[X_train.astype(np.int32)]\n    X_test_permuted = permutated_labels[X_test.astype(np.int32)]\n    target_encoder = TargetEncoder(smooth=smooth, random_state=global_random_seed)\n    X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n    X_test_encoded = target_encoder.transform(X_test)\n    X_train_permuted_encoded = target_encoder.fit_transform(X_train_permuted, y_train)\n    X_test_permuted_encoded = target_encoder.transform(X_test_permuted)\n    assert_allclose(X_train_encoded, X_train_permuted_encoded)\n    assert_allclose(X_test_encoded, X_test_permuted_encoded)",
        "mutated": [
            "@pytest.mark.parametrize('smooth', [0.0, 1000.0, 'auto'])\ndef test_invariance_of_encoding_under_label_permutation(smooth, global_random_seed):\n    if False:\n        i = 10\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.normal(size=1000)\n    n_categories = 30\n    X = KBinsDiscretizer(n_bins=n_categories, encode='ordinal').fit_transform(y.reshape(-1, 1))\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=global_random_seed)\n    permutated_labels = rng.permutation(n_categories)\n    X_train_permuted = permutated_labels[X_train.astype(np.int32)]\n    X_test_permuted = permutated_labels[X_test.astype(np.int32)]\n    target_encoder = TargetEncoder(smooth=smooth, random_state=global_random_seed)\n    X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n    X_test_encoded = target_encoder.transform(X_test)\n    X_train_permuted_encoded = target_encoder.fit_transform(X_train_permuted, y_train)\n    X_test_permuted_encoded = target_encoder.transform(X_test_permuted)\n    assert_allclose(X_train_encoded, X_train_permuted_encoded)\n    assert_allclose(X_test_encoded, X_test_permuted_encoded)",
            "@pytest.mark.parametrize('smooth', [0.0, 1000.0, 'auto'])\ndef test_invariance_of_encoding_under_label_permutation(smooth, global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.normal(size=1000)\n    n_categories = 30\n    X = KBinsDiscretizer(n_bins=n_categories, encode='ordinal').fit_transform(y.reshape(-1, 1))\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=global_random_seed)\n    permutated_labels = rng.permutation(n_categories)\n    X_train_permuted = permutated_labels[X_train.astype(np.int32)]\n    X_test_permuted = permutated_labels[X_test.astype(np.int32)]\n    target_encoder = TargetEncoder(smooth=smooth, random_state=global_random_seed)\n    X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n    X_test_encoded = target_encoder.transform(X_test)\n    X_train_permuted_encoded = target_encoder.fit_transform(X_train_permuted, y_train)\n    X_test_permuted_encoded = target_encoder.transform(X_test_permuted)\n    assert_allclose(X_train_encoded, X_train_permuted_encoded)\n    assert_allclose(X_test_encoded, X_test_permuted_encoded)",
            "@pytest.mark.parametrize('smooth', [0.0, 1000.0, 'auto'])\ndef test_invariance_of_encoding_under_label_permutation(smooth, global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.normal(size=1000)\n    n_categories = 30\n    X = KBinsDiscretizer(n_bins=n_categories, encode='ordinal').fit_transform(y.reshape(-1, 1))\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=global_random_seed)\n    permutated_labels = rng.permutation(n_categories)\n    X_train_permuted = permutated_labels[X_train.astype(np.int32)]\n    X_test_permuted = permutated_labels[X_test.astype(np.int32)]\n    target_encoder = TargetEncoder(smooth=smooth, random_state=global_random_seed)\n    X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n    X_test_encoded = target_encoder.transform(X_test)\n    X_train_permuted_encoded = target_encoder.fit_transform(X_train_permuted, y_train)\n    X_test_permuted_encoded = target_encoder.transform(X_test_permuted)\n    assert_allclose(X_train_encoded, X_train_permuted_encoded)\n    assert_allclose(X_test_encoded, X_test_permuted_encoded)",
            "@pytest.mark.parametrize('smooth', [0.0, 1000.0, 'auto'])\ndef test_invariance_of_encoding_under_label_permutation(smooth, global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.normal(size=1000)\n    n_categories = 30\n    X = KBinsDiscretizer(n_bins=n_categories, encode='ordinal').fit_transform(y.reshape(-1, 1))\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=global_random_seed)\n    permutated_labels = rng.permutation(n_categories)\n    X_train_permuted = permutated_labels[X_train.astype(np.int32)]\n    X_test_permuted = permutated_labels[X_test.astype(np.int32)]\n    target_encoder = TargetEncoder(smooth=smooth, random_state=global_random_seed)\n    X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n    X_test_encoded = target_encoder.transform(X_test)\n    X_train_permuted_encoded = target_encoder.fit_transform(X_train_permuted, y_train)\n    X_test_permuted_encoded = target_encoder.transform(X_test_permuted)\n    assert_allclose(X_train_encoded, X_train_permuted_encoded)\n    assert_allclose(X_test_encoded, X_test_permuted_encoded)",
            "@pytest.mark.parametrize('smooth', [0.0, 1000.0, 'auto'])\ndef test_invariance_of_encoding_under_label_permutation(smooth, global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.normal(size=1000)\n    n_categories = 30\n    X = KBinsDiscretizer(n_bins=n_categories, encode='ordinal').fit_transform(y.reshape(-1, 1))\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=global_random_seed)\n    permutated_labels = rng.permutation(n_categories)\n    X_train_permuted = permutated_labels[X_train.astype(np.int32)]\n    X_test_permuted = permutated_labels[X_test.astype(np.int32)]\n    target_encoder = TargetEncoder(smooth=smooth, random_state=global_random_seed)\n    X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n    X_test_encoded = target_encoder.transform(X_test)\n    X_train_permuted_encoded = target_encoder.fit_transform(X_train_permuted, y_train)\n    X_test_permuted_encoded = target_encoder.transform(X_test_permuted)\n    assert_allclose(X_train_encoded, X_train_permuted_encoded)\n    assert_allclose(X_test_encoded, X_test_permuted_encoded)"
        ]
    },
    {
        "func_name": "test_target_encoding_for_linear_regression",
        "original": "@pytest.mark.filterwarnings('ignore:In version 1.5 onwards, subsample=200_000')\n@pytest.mark.parametrize('smooth', [0.0, 'auto'])\ndef test_target_encoding_for_linear_regression(smooth, global_random_seed):\n    linear_regression = Ridge(alpha=1e-06, solver='lsqr', fit_intercept=False)\n    n_samples = 50000\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.randn(n_samples)\n    noise = 0.8 * rng.randn(n_samples)\n    n_categories = 100\n    X_informative = KBinsDiscretizer(n_bins=n_categories, encode='ordinal', strategy='uniform', random_state=rng).fit_transform((y + noise).reshape(-1, 1))\n    permutated_labels = rng.permutation(n_categories)\n    X_informative = permutated_labels[X_informative.astype(np.int32)]\n    X_shuffled = rng.permutation(X_informative)\n    X_near_unique_categories = rng.choice(int(0.9 * n_samples), size=n_samples, replace=True).reshape(-1, 1)\n    X = np.concatenate([X_informative, X_shuffled, X_near_unique_categories], axis=1)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=0)\n    raw_model = linear_regression.fit(X_train, y_train)\n    assert raw_model.score(X_train, y_train) < 0.1\n    assert raw_model.score(X_test, y_test) < 0.1\n    model_with_cv = make_pipeline(TargetEncoder(smooth=smooth, random_state=rng), linear_regression).fit(X_train, y_train)\n    coef = model_with_cv[-1].coef_\n    assert model_with_cv.score(X_train, y_train) > 0.5, coef\n    assert model_with_cv.score(X_test, y_test) > 0.5, coef\n    assert coef[0] == pytest.approx(1, abs=0.01)\n    assert (np.abs(coef[1:]) < 0.2).all()\n    target_encoder = TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train)\n    X_enc_no_cv_train = target_encoder.transform(X_train)\n    X_enc_no_cv_test = target_encoder.transform(X_test)\n    model_no_cv = linear_regression.fit(X_enc_no_cv_train, y_train)\n    coef = model_no_cv.coef_\n    assert model_no_cv.score(X_enc_no_cv_train, y_train) > 0.7, coef\n    assert model_no_cv.score(X_enc_no_cv_test, y_test) < 0.5, coef\n    assert abs(coef[0]) < abs(coef[2])",
        "mutated": [
            "@pytest.mark.filterwarnings('ignore:In version 1.5 onwards, subsample=200_000')\n@pytest.mark.parametrize('smooth', [0.0, 'auto'])\ndef test_target_encoding_for_linear_regression(smooth, global_random_seed):\n    if False:\n        i = 10\n    linear_regression = Ridge(alpha=1e-06, solver='lsqr', fit_intercept=False)\n    n_samples = 50000\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.randn(n_samples)\n    noise = 0.8 * rng.randn(n_samples)\n    n_categories = 100\n    X_informative = KBinsDiscretizer(n_bins=n_categories, encode='ordinal', strategy='uniform', random_state=rng).fit_transform((y + noise).reshape(-1, 1))\n    permutated_labels = rng.permutation(n_categories)\n    X_informative = permutated_labels[X_informative.astype(np.int32)]\n    X_shuffled = rng.permutation(X_informative)\n    X_near_unique_categories = rng.choice(int(0.9 * n_samples), size=n_samples, replace=True).reshape(-1, 1)\n    X = np.concatenate([X_informative, X_shuffled, X_near_unique_categories], axis=1)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=0)\n    raw_model = linear_regression.fit(X_train, y_train)\n    assert raw_model.score(X_train, y_train) < 0.1\n    assert raw_model.score(X_test, y_test) < 0.1\n    model_with_cv = make_pipeline(TargetEncoder(smooth=smooth, random_state=rng), linear_regression).fit(X_train, y_train)\n    coef = model_with_cv[-1].coef_\n    assert model_with_cv.score(X_train, y_train) > 0.5, coef\n    assert model_with_cv.score(X_test, y_test) > 0.5, coef\n    assert coef[0] == pytest.approx(1, abs=0.01)\n    assert (np.abs(coef[1:]) < 0.2).all()\n    target_encoder = TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train)\n    X_enc_no_cv_train = target_encoder.transform(X_train)\n    X_enc_no_cv_test = target_encoder.transform(X_test)\n    model_no_cv = linear_regression.fit(X_enc_no_cv_train, y_train)\n    coef = model_no_cv.coef_\n    assert model_no_cv.score(X_enc_no_cv_train, y_train) > 0.7, coef\n    assert model_no_cv.score(X_enc_no_cv_test, y_test) < 0.5, coef\n    assert abs(coef[0]) < abs(coef[2])",
            "@pytest.mark.filterwarnings('ignore:In version 1.5 onwards, subsample=200_000')\n@pytest.mark.parametrize('smooth', [0.0, 'auto'])\ndef test_target_encoding_for_linear_regression(smooth, global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    linear_regression = Ridge(alpha=1e-06, solver='lsqr', fit_intercept=False)\n    n_samples = 50000\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.randn(n_samples)\n    noise = 0.8 * rng.randn(n_samples)\n    n_categories = 100\n    X_informative = KBinsDiscretizer(n_bins=n_categories, encode='ordinal', strategy='uniform', random_state=rng).fit_transform((y + noise).reshape(-1, 1))\n    permutated_labels = rng.permutation(n_categories)\n    X_informative = permutated_labels[X_informative.astype(np.int32)]\n    X_shuffled = rng.permutation(X_informative)\n    X_near_unique_categories = rng.choice(int(0.9 * n_samples), size=n_samples, replace=True).reshape(-1, 1)\n    X = np.concatenate([X_informative, X_shuffled, X_near_unique_categories], axis=1)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=0)\n    raw_model = linear_regression.fit(X_train, y_train)\n    assert raw_model.score(X_train, y_train) < 0.1\n    assert raw_model.score(X_test, y_test) < 0.1\n    model_with_cv = make_pipeline(TargetEncoder(smooth=smooth, random_state=rng), linear_regression).fit(X_train, y_train)\n    coef = model_with_cv[-1].coef_\n    assert model_with_cv.score(X_train, y_train) > 0.5, coef\n    assert model_with_cv.score(X_test, y_test) > 0.5, coef\n    assert coef[0] == pytest.approx(1, abs=0.01)\n    assert (np.abs(coef[1:]) < 0.2).all()\n    target_encoder = TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train)\n    X_enc_no_cv_train = target_encoder.transform(X_train)\n    X_enc_no_cv_test = target_encoder.transform(X_test)\n    model_no_cv = linear_regression.fit(X_enc_no_cv_train, y_train)\n    coef = model_no_cv.coef_\n    assert model_no_cv.score(X_enc_no_cv_train, y_train) > 0.7, coef\n    assert model_no_cv.score(X_enc_no_cv_test, y_test) < 0.5, coef\n    assert abs(coef[0]) < abs(coef[2])",
            "@pytest.mark.filterwarnings('ignore:In version 1.5 onwards, subsample=200_000')\n@pytest.mark.parametrize('smooth', [0.0, 'auto'])\ndef test_target_encoding_for_linear_regression(smooth, global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    linear_regression = Ridge(alpha=1e-06, solver='lsqr', fit_intercept=False)\n    n_samples = 50000\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.randn(n_samples)\n    noise = 0.8 * rng.randn(n_samples)\n    n_categories = 100\n    X_informative = KBinsDiscretizer(n_bins=n_categories, encode='ordinal', strategy='uniform', random_state=rng).fit_transform((y + noise).reshape(-1, 1))\n    permutated_labels = rng.permutation(n_categories)\n    X_informative = permutated_labels[X_informative.astype(np.int32)]\n    X_shuffled = rng.permutation(X_informative)\n    X_near_unique_categories = rng.choice(int(0.9 * n_samples), size=n_samples, replace=True).reshape(-1, 1)\n    X = np.concatenate([X_informative, X_shuffled, X_near_unique_categories], axis=1)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=0)\n    raw_model = linear_regression.fit(X_train, y_train)\n    assert raw_model.score(X_train, y_train) < 0.1\n    assert raw_model.score(X_test, y_test) < 0.1\n    model_with_cv = make_pipeline(TargetEncoder(smooth=smooth, random_state=rng), linear_regression).fit(X_train, y_train)\n    coef = model_with_cv[-1].coef_\n    assert model_with_cv.score(X_train, y_train) > 0.5, coef\n    assert model_with_cv.score(X_test, y_test) > 0.5, coef\n    assert coef[0] == pytest.approx(1, abs=0.01)\n    assert (np.abs(coef[1:]) < 0.2).all()\n    target_encoder = TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train)\n    X_enc_no_cv_train = target_encoder.transform(X_train)\n    X_enc_no_cv_test = target_encoder.transform(X_test)\n    model_no_cv = linear_regression.fit(X_enc_no_cv_train, y_train)\n    coef = model_no_cv.coef_\n    assert model_no_cv.score(X_enc_no_cv_train, y_train) > 0.7, coef\n    assert model_no_cv.score(X_enc_no_cv_test, y_test) < 0.5, coef\n    assert abs(coef[0]) < abs(coef[2])",
            "@pytest.mark.filterwarnings('ignore:In version 1.5 onwards, subsample=200_000')\n@pytest.mark.parametrize('smooth', [0.0, 'auto'])\ndef test_target_encoding_for_linear_regression(smooth, global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    linear_regression = Ridge(alpha=1e-06, solver='lsqr', fit_intercept=False)\n    n_samples = 50000\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.randn(n_samples)\n    noise = 0.8 * rng.randn(n_samples)\n    n_categories = 100\n    X_informative = KBinsDiscretizer(n_bins=n_categories, encode='ordinal', strategy='uniform', random_state=rng).fit_transform((y + noise).reshape(-1, 1))\n    permutated_labels = rng.permutation(n_categories)\n    X_informative = permutated_labels[X_informative.astype(np.int32)]\n    X_shuffled = rng.permutation(X_informative)\n    X_near_unique_categories = rng.choice(int(0.9 * n_samples), size=n_samples, replace=True).reshape(-1, 1)\n    X = np.concatenate([X_informative, X_shuffled, X_near_unique_categories], axis=1)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=0)\n    raw_model = linear_regression.fit(X_train, y_train)\n    assert raw_model.score(X_train, y_train) < 0.1\n    assert raw_model.score(X_test, y_test) < 0.1\n    model_with_cv = make_pipeline(TargetEncoder(smooth=smooth, random_state=rng), linear_regression).fit(X_train, y_train)\n    coef = model_with_cv[-1].coef_\n    assert model_with_cv.score(X_train, y_train) > 0.5, coef\n    assert model_with_cv.score(X_test, y_test) > 0.5, coef\n    assert coef[0] == pytest.approx(1, abs=0.01)\n    assert (np.abs(coef[1:]) < 0.2).all()\n    target_encoder = TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train)\n    X_enc_no_cv_train = target_encoder.transform(X_train)\n    X_enc_no_cv_test = target_encoder.transform(X_test)\n    model_no_cv = linear_regression.fit(X_enc_no_cv_train, y_train)\n    coef = model_no_cv.coef_\n    assert model_no_cv.score(X_enc_no_cv_train, y_train) > 0.7, coef\n    assert model_no_cv.score(X_enc_no_cv_test, y_test) < 0.5, coef\n    assert abs(coef[0]) < abs(coef[2])",
            "@pytest.mark.filterwarnings('ignore:In version 1.5 onwards, subsample=200_000')\n@pytest.mark.parametrize('smooth', [0.0, 'auto'])\ndef test_target_encoding_for_linear_regression(smooth, global_random_seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    linear_regression = Ridge(alpha=1e-06, solver='lsqr', fit_intercept=False)\n    n_samples = 50000\n    rng = np.random.RandomState(global_random_seed)\n    y = rng.randn(n_samples)\n    noise = 0.8 * rng.randn(n_samples)\n    n_categories = 100\n    X_informative = KBinsDiscretizer(n_bins=n_categories, encode='ordinal', strategy='uniform', random_state=rng).fit_transform((y + noise).reshape(-1, 1))\n    permutated_labels = rng.permutation(n_categories)\n    X_informative = permutated_labels[X_informative.astype(np.int32)]\n    X_shuffled = rng.permutation(X_informative)\n    X_near_unique_categories = rng.choice(int(0.9 * n_samples), size=n_samples, replace=True).reshape(-1, 1)\n    X = np.concatenate([X_informative, X_shuffled, X_near_unique_categories], axis=1)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=0)\n    raw_model = linear_regression.fit(X_train, y_train)\n    assert raw_model.score(X_train, y_train) < 0.1\n    assert raw_model.score(X_test, y_test) < 0.1\n    model_with_cv = make_pipeline(TargetEncoder(smooth=smooth, random_state=rng), linear_regression).fit(X_train, y_train)\n    coef = model_with_cv[-1].coef_\n    assert model_with_cv.score(X_train, y_train) > 0.5, coef\n    assert model_with_cv.score(X_test, y_test) > 0.5, coef\n    assert coef[0] == pytest.approx(1, abs=0.01)\n    assert (np.abs(coef[1:]) < 0.2).all()\n    target_encoder = TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train)\n    X_enc_no_cv_train = target_encoder.transform(X_train)\n    X_enc_no_cv_test = target_encoder.transform(X_test)\n    model_no_cv = linear_regression.fit(X_enc_no_cv_train, y_train)\n    coef = model_no_cv.coef_\n    assert model_no_cv.score(X_enc_no_cv_train, y_train) > 0.7, coef\n    assert model_no_cv.score(X_enc_no_cv_test, y_test) < 0.5, coef\n    assert abs(coef[0]) < abs(coef[2])"
        ]
    }
]