[
    {
        "func_name": "__init__",
        "original": "def __init__(self, URM_train, verbose=True):\n    super(P3alphaRecommender, self).__init__(URM_train, verbose=verbose)",
        "mutated": [
            "def __init__(self, URM_train, verbose=True):\n    if False:\n        i = 10\n    super(P3alphaRecommender, self).__init__(URM_train, verbose=verbose)",
            "def __init__(self, URM_train, verbose=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(P3alphaRecommender, self).__init__(URM_train, verbose=verbose)",
            "def __init__(self, URM_train, verbose=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(P3alphaRecommender, self).__init__(URM_train, verbose=verbose)",
            "def __init__(self, URM_train, verbose=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(P3alphaRecommender, self).__init__(URM_train, verbose=verbose)",
            "def __init__(self, URM_train, verbose=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(P3alphaRecommender, self).__init__(URM_train, verbose=verbose)"
        ]
    },
    {
        "func_name": "__str__",
        "original": "def __str__(self):\n    return 'P3alpha(alpha={}, min_rating={}, topk={}, implicit={}, normalize_similarity={})'.format(self.alpha, self.min_rating, self.topK, self.implicit, self.normalize_similarity)",
        "mutated": [
            "def __str__(self):\n    if False:\n        i = 10\n    return 'P3alpha(alpha={}, min_rating={}, topk={}, implicit={}, normalize_similarity={})'.format(self.alpha, self.min_rating, self.topK, self.implicit, self.normalize_similarity)",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 'P3alpha(alpha={}, min_rating={}, topk={}, implicit={}, normalize_similarity={})'.format(self.alpha, self.min_rating, self.topK, self.implicit, self.normalize_similarity)",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 'P3alpha(alpha={}, min_rating={}, topk={}, implicit={}, normalize_similarity={})'.format(self.alpha, self.min_rating, self.topK, self.implicit, self.normalize_similarity)",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 'P3alpha(alpha={}, min_rating={}, topk={}, implicit={}, normalize_similarity={})'.format(self.alpha, self.min_rating, self.topK, self.implicit, self.normalize_similarity)",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 'P3alpha(alpha={}, min_rating={}, topk={}, implicit={}, normalize_similarity={})'.format(self.alpha, self.min_rating, self.topK, self.implicit, self.normalize_similarity)"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, topK=100, alpha=1.0, min_rating=0, implicit=False, normalize_similarity=False):\n    self.topK = topK\n    self.alpha = alpha\n    self.min_rating = min_rating\n    self.implicit = implicit\n    self.normalize_similarity = normalize_similarity\n    if self.min_rating > 0:\n        self.URM_train.data[self.URM_train.data < self.min_rating] = 0\n        self.URM_train.eliminate_zeros()\n        if self.implicit:\n            self.URM_train.data = np.ones(self.URM_train.data.size, dtype=np.float32)\n    Pui = normalize(self.URM_train, norm='l1', axis=1)\n    X_bool = self.URM_train.transpose(copy=True)\n    X_bool.data = np.ones(X_bool.data.size, np.float32)\n    Piu = normalize(X_bool, norm='l1', axis=1)\n    del X_bool\n    if self.alpha != 1.0:\n        Pui = Pui.power(self.alpha)\n        Piu = Piu.power(self.alpha)\n    block_dim = 200\n    d_t = Piu\n    dataBlock = 10000000\n    rows = np.zeros(dataBlock, dtype=np.int32)\n    cols = np.zeros(dataBlock, dtype=np.int32)\n    values = np.zeros(dataBlock, dtype=np.float32)\n    numCells = 0\n    start_time = time.time()\n    start_time_printBatch = start_time\n    for current_block_start_row in range(0, Pui.shape[1], block_dim):\n        if current_block_start_row + block_dim > Pui.shape[1]:\n            block_dim = Pui.shape[1] - current_block_start_row\n        similarity_block = d_t[current_block_start_row:current_block_start_row + block_dim, :] * Pui\n        similarity_block = similarity_block.toarray()\n        for row_in_block in range(block_dim):\n            row_data = similarity_block[row_in_block, :]\n            row_data[current_block_start_row + row_in_block] = 0\n            best = row_data.argsort()[::-1][:self.topK]\n            notZerosMask = row_data[best] != 0.0\n            values_to_add = row_data[best][notZerosMask]\n            cols_to_add = best[notZerosMask]\n            for index in range(len(values_to_add)):\n                if numCells == len(rows):\n                    rows = np.concatenate((rows, np.zeros(dataBlock, dtype=np.int32)))\n                    cols = np.concatenate((cols, np.zeros(dataBlock, dtype=np.int32)))\n                    values = np.concatenate((values, np.zeros(dataBlock, dtype=np.float32)))\n                rows[numCells] = current_block_start_row + row_in_block\n                cols[numCells] = cols_to_add[index]\n                values[numCells] = values_to_add[index]\n                numCells += 1\n        if time.time() - start_time_printBatch > 300:\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            self._print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(current_block_start_row + block_dim, 100.0 * float(current_block_start_row + block_dim) / Pui.shape[1], float(current_block_start_row + block_dim) / (time.time() - start_time), new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_printBatch = time.time()\n    self.W_sparse = sps.csr_matrix((values[:numCells], (rows[:numCells], cols[:numCells])), shape=(Pui.shape[1], Pui.shape[1]))\n    if self.normalize_similarity:\n        self.W_sparse = normalize(self.W_sparse, norm='l1', axis=1)\n    if self.topK != False:\n        self.W_sparse = similarityMatrixTopK(self.W_sparse, k=self.topK)\n    self.W_sparse = check_matrix(self.W_sparse, format='csr')",
        "mutated": [
            "def fit(self, topK=100, alpha=1.0, min_rating=0, implicit=False, normalize_similarity=False):\n    if False:\n        i = 10\n    self.topK = topK\n    self.alpha = alpha\n    self.min_rating = min_rating\n    self.implicit = implicit\n    self.normalize_similarity = normalize_similarity\n    if self.min_rating > 0:\n        self.URM_train.data[self.URM_train.data < self.min_rating] = 0\n        self.URM_train.eliminate_zeros()\n        if self.implicit:\n            self.URM_train.data = np.ones(self.URM_train.data.size, dtype=np.float32)\n    Pui = normalize(self.URM_train, norm='l1', axis=1)\n    X_bool = self.URM_train.transpose(copy=True)\n    X_bool.data = np.ones(X_bool.data.size, np.float32)\n    Piu = normalize(X_bool, norm='l1', axis=1)\n    del X_bool\n    if self.alpha != 1.0:\n        Pui = Pui.power(self.alpha)\n        Piu = Piu.power(self.alpha)\n    block_dim = 200\n    d_t = Piu\n    dataBlock = 10000000\n    rows = np.zeros(dataBlock, dtype=np.int32)\n    cols = np.zeros(dataBlock, dtype=np.int32)\n    values = np.zeros(dataBlock, dtype=np.float32)\n    numCells = 0\n    start_time = time.time()\n    start_time_printBatch = start_time\n    for current_block_start_row in range(0, Pui.shape[1], block_dim):\n        if current_block_start_row + block_dim > Pui.shape[1]:\n            block_dim = Pui.shape[1] - current_block_start_row\n        similarity_block = d_t[current_block_start_row:current_block_start_row + block_dim, :] * Pui\n        similarity_block = similarity_block.toarray()\n        for row_in_block in range(block_dim):\n            row_data = similarity_block[row_in_block, :]\n            row_data[current_block_start_row + row_in_block] = 0\n            best = row_data.argsort()[::-1][:self.topK]\n            notZerosMask = row_data[best] != 0.0\n            values_to_add = row_data[best][notZerosMask]\n            cols_to_add = best[notZerosMask]\n            for index in range(len(values_to_add)):\n                if numCells == len(rows):\n                    rows = np.concatenate((rows, np.zeros(dataBlock, dtype=np.int32)))\n                    cols = np.concatenate((cols, np.zeros(dataBlock, dtype=np.int32)))\n                    values = np.concatenate((values, np.zeros(dataBlock, dtype=np.float32)))\n                rows[numCells] = current_block_start_row + row_in_block\n                cols[numCells] = cols_to_add[index]\n                values[numCells] = values_to_add[index]\n                numCells += 1\n        if time.time() - start_time_printBatch > 300:\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            self._print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(current_block_start_row + block_dim, 100.0 * float(current_block_start_row + block_dim) / Pui.shape[1], float(current_block_start_row + block_dim) / (time.time() - start_time), new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_printBatch = time.time()\n    self.W_sparse = sps.csr_matrix((values[:numCells], (rows[:numCells], cols[:numCells])), shape=(Pui.shape[1], Pui.shape[1]))\n    if self.normalize_similarity:\n        self.W_sparse = normalize(self.W_sparse, norm='l1', axis=1)\n    if self.topK != False:\n        self.W_sparse = similarityMatrixTopK(self.W_sparse, k=self.topK)\n    self.W_sparse = check_matrix(self.W_sparse, format='csr')",
            "def fit(self, topK=100, alpha=1.0, min_rating=0, implicit=False, normalize_similarity=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.topK = topK\n    self.alpha = alpha\n    self.min_rating = min_rating\n    self.implicit = implicit\n    self.normalize_similarity = normalize_similarity\n    if self.min_rating > 0:\n        self.URM_train.data[self.URM_train.data < self.min_rating] = 0\n        self.URM_train.eliminate_zeros()\n        if self.implicit:\n            self.URM_train.data = np.ones(self.URM_train.data.size, dtype=np.float32)\n    Pui = normalize(self.URM_train, norm='l1', axis=1)\n    X_bool = self.URM_train.transpose(copy=True)\n    X_bool.data = np.ones(X_bool.data.size, np.float32)\n    Piu = normalize(X_bool, norm='l1', axis=1)\n    del X_bool\n    if self.alpha != 1.0:\n        Pui = Pui.power(self.alpha)\n        Piu = Piu.power(self.alpha)\n    block_dim = 200\n    d_t = Piu\n    dataBlock = 10000000\n    rows = np.zeros(dataBlock, dtype=np.int32)\n    cols = np.zeros(dataBlock, dtype=np.int32)\n    values = np.zeros(dataBlock, dtype=np.float32)\n    numCells = 0\n    start_time = time.time()\n    start_time_printBatch = start_time\n    for current_block_start_row in range(0, Pui.shape[1], block_dim):\n        if current_block_start_row + block_dim > Pui.shape[1]:\n            block_dim = Pui.shape[1] - current_block_start_row\n        similarity_block = d_t[current_block_start_row:current_block_start_row + block_dim, :] * Pui\n        similarity_block = similarity_block.toarray()\n        for row_in_block in range(block_dim):\n            row_data = similarity_block[row_in_block, :]\n            row_data[current_block_start_row + row_in_block] = 0\n            best = row_data.argsort()[::-1][:self.topK]\n            notZerosMask = row_data[best] != 0.0\n            values_to_add = row_data[best][notZerosMask]\n            cols_to_add = best[notZerosMask]\n            for index in range(len(values_to_add)):\n                if numCells == len(rows):\n                    rows = np.concatenate((rows, np.zeros(dataBlock, dtype=np.int32)))\n                    cols = np.concatenate((cols, np.zeros(dataBlock, dtype=np.int32)))\n                    values = np.concatenate((values, np.zeros(dataBlock, dtype=np.float32)))\n                rows[numCells] = current_block_start_row + row_in_block\n                cols[numCells] = cols_to_add[index]\n                values[numCells] = values_to_add[index]\n                numCells += 1\n        if time.time() - start_time_printBatch > 300:\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            self._print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(current_block_start_row + block_dim, 100.0 * float(current_block_start_row + block_dim) / Pui.shape[1], float(current_block_start_row + block_dim) / (time.time() - start_time), new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_printBatch = time.time()\n    self.W_sparse = sps.csr_matrix((values[:numCells], (rows[:numCells], cols[:numCells])), shape=(Pui.shape[1], Pui.shape[1]))\n    if self.normalize_similarity:\n        self.W_sparse = normalize(self.W_sparse, norm='l1', axis=1)\n    if self.topK != False:\n        self.W_sparse = similarityMatrixTopK(self.W_sparse, k=self.topK)\n    self.W_sparse = check_matrix(self.W_sparse, format='csr')",
            "def fit(self, topK=100, alpha=1.0, min_rating=0, implicit=False, normalize_similarity=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.topK = topK\n    self.alpha = alpha\n    self.min_rating = min_rating\n    self.implicit = implicit\n    self.normalize_similarity = normalize_similarity\n    if self.min_rating > 0:\n        self.URM_train.data[self.URM_train.data < self.min_rating] = 0\n        self.URM_train.eliminate_zeros()\n        if self.implicit:\n            self.URM_train.data = np.ones(self.URM_train.data.size, dtype=np.float32)\n    Pui = normalize(self.URM_train, norm='l1', axis=1)\n    X_bool = self.URM_train.transpose(copy=True)\n    X_bool.data = np.ones(X_bool.data.size, np.float32)\n    Piu = normalize(X_bool, norm='l1', axis=1)\n    del X_bool\n    if self.alpha != 1.0:\n        Pui = Pui.power(self.alpha)\n        Piu = Piu.power(self.alpha)\n    block_dim = 200\n    d_t = Piu\n    dataBlock = 10000000\n    rows = np.zeros(dataBlock, dtype=np.int32)\n    cols = np.zeros(dataBlock, dtype=np.int32)\n    values = np.zeros(dataBlock, dtype=np.float32)\n    numCells = 0\n    start_time = time.time()\n    start_time_printBatch = start_time\n    for current_block_start_row in range(0, Pui.shape[1], block_dim):\n        if current_block_start_row + block_dim > Pui.shape[1]:\n            block_dim = Pui.shape[1] - current_block_start_row\n        similarity_block = d_t[current_block_start_row:current_block_start_row + block_dim, :] * Pui\n        similarity_block = similarity_block.toarray()\n        for row_in_block in range(block_dim):\n            row_data = similarity_block[row_in_block, :]\n            row_data[current_block_start_row + row_in_block] = 0\n            best = row_data.argsort()[::-1][:self.topK]\n            notZerosMask = row_data[best] != 0.0\n            values_to_add = row_data[best][notZerosMask]\n            cols_to_add = best[notZerosMask]\n            for index in range(len(values_to_add)):\n                if numCells == len(rows):\n                    rows = np.concatenate((rows, np.zeros(dataBlock, dtype=np.int32)))\n                    cols = np.concatenate((cols, np.zeros(dataBlock, dtype=np.int32)))\n                    values = np.concatenate((values, np.zeros(dataBlock, dtype=np.float32)))\n                rows[numCells] = current_block_start_row + row_in_block\n                cols[numCells] = cols_to_add[index]\n                values[numCells] = values_to_add[index]\n                numCells += 1\n        if time.time() - start_time_printBatch > 300:\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            self._print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(current_block_start_row + block_dim, 100.0 * float(current_block_start_row + block_dim) / Pui.shape[1], float(current_block_start_row + block_dim) / (time.time() - start_time), new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_printBatch = time.time()\n    self.W_sparse = sps.csr_matrix((values[:numCells], (rows[:numCells], cols[:numCells])), shape=(Pui.shape[1], Pui.shape[1]))\n    if self.normalize_similarity:\n        self.W_sparse = normalize(self.W_sparse, norm='l1', axis=1)\n    if self.topK != False:\n        self.W_sparse = similarityMatrixTopK(self.W_sparse, k=self.topK)\n    self.W_sparse = check_matrix(self.W_sparse, format='csr')",
            "def fit(self, topK=100, alpha=1.0, min_rating=0, implicit=False, normalize_similarity=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.topK = topK\n    self.alpha = alpha\n    self.min_rating = min_rating\n    self.implicit = implicit\n    self.normalize_similarity = normalize_similarity\n    if self.min_rating > 0:\n        self.URM_train.data[self.URM_train.data < self.min_rating] = 0\n        self.URM_train.eliminate_zeros()\n        if self.implicit:\n            self.URM_train.data = np.ones(self.URM_train.data.size, dtype=np.float32)\n    Pui = normalize(self.URM_train, norm='l1', axis=1)\n    X_bool = self.URM_train.transpose(copy=True)\n    X_bool.data = np.ones(X_bool.data.size, np.float32)\n    Piu = normalize(X_bool, norm='l1', axis=1)\n    del X_bool\n    if self.alpha != 1.0:\n        Pui = Pui.power(self.alpha)\n        Piu = Piu.power(self.alpha)\n    block_dim = 200\n    d_t = Piu\n    dataBlock = 10000000\n    rows = np.zeros(dataBlock, dtype=np.int32)\n    cols = np.zeros(dataBlock, dtype=np.int32)\n    values = np.zeros(dataBlock, dtype=np.float32)\n    numCells = 0\n    start_time = time.time()\n    start_time_printBatch = start_time\n    for current_block_start_row in range(0, Pui.shape[1], block_dim):\n        if current_block_start_row + block_dim > Pui.shape[1]:\n            block_dim = Pui.shape[1] - current_block_start_row\n        similarity_block = d_t[current_block_start_row:current_block_start_row + block_dim, :] * Pui\n        similarity_block = similarity_block.toarray()\n        for row_in_block in range(block_dim):\n            row_data = similarity_block[row_in_block, :]\n            row_data[current_block_start_row + row_in_block] = 0\n            best = row_data.argsort()[::-1][:self.topK]\n            notZerosMask = row_data[best] != 0.0\n            values_to_add = row_data[best][notZerosMask]\n            cols_to_add = best[notZerosMask]\n            for index in range(len(values_to_add)):\n                if numCells == len(rows):\n                    rows = np.concatenate((rows, np.zeros(dataBlock, dtype=np.int32)))\n                    cols = np.concatenate((cols, np.zeros(dataBlock, dtype=np.int32)))\n                    values = np.concatenate((values, np.zeros(dataBlock, dtype=np.float32)))\n                rows[numCells] = current_block_start_row + row_in_block\n                cols[numCells] = cols_to_add[index]\n                values[numCells] = values_to_add[index]\n                numCells += 1\n        if time.time() - start_time_printBatch > 300:\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            self._print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(current_block_start_row + block_dim, 100.0 * float(current_block_start_row + block_dim) / Pui.shape[1], float(current_block_start_row + block_dim) / (time.time() - start_time), new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_printBatch = time.time()\n    self.W_sparse = sps.csr_matrix((values[:numCells], (rows[:numCells], cols[:numCells])), shape=(Pui.shape[1], Pui.shape[1]))\n    if self.normalize_similarity:\n        self.W_sparse = normalize(self.W_sparse, norm='l1', axis=1)\n    if self.topK != False:\n        self.W_sparse = similarityMatrixTopK(self.W_sparse, k=self.topK)\n    self.W_sparse = check_matrix(self.W_sparse, format='csr')",
            "def fit(self, topK=100, alpha=1.0, min_rating=0, implicit=False, normalize_similarity=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.topK = topK\n    self.alpha = alpha\n    self.min_rating = min_rating\n    self.implicit = implicit\n    self.normalize_similarity = normalize_similarity\n    if self.min_rating > 0:\n        self.URM_train.data[self.URM_train.data < self.min_rating] = 0\n        self.URM_train.eliminate_zeros()\n        if self.implicit:\n            self.URM_train.data = np.ones(self.URM_train.data.size, dtype=np.float32)\n    Pui = normalize(self.URM_train, norm='l1', axis=1)\n    X_bool = self.URM_train.transpose(copy=True)\n    X_bool.data = np.ones(X_bool.data.size, np.float32)\n    Piu = normalize(X_bool, norm='l1', axis=1)\n    del X_bool\n    if self.alpha != 1.0:\n        Pui = Pui.power(self.alpha)\n        Piu = Piu.power(self.alpha)\n    block_dim = 200\n    d_t = Piu\n    dataBlock = 10000000\n    rows = np.zeros(dataBlock, dtype=np.int32)\n    cols = np.zeros(dataBlock, dtype=np.int32)\n    values = np.zeros(dataBlock, dtype=np.float32)\n    numCells = 0\n    start_time = time.time()\n    start_time_printBatch = start_time\n    for current_block_start_row in range(0, Pui.shape[1], block_dim):\n        if current_block_start_row + block_dim > Pui.shape[1]:\n            block_dim = Pui.shape[1] - current_block_start_row\n        similarity_block = d_t[current_block_start_row:current_block_start_row + block_dim, :] * Pui\n        similarity_block = similarity_block.toarray()\n        for row_in_block in range(block_dim):\n            row_data = similarity_block[row_in_block, :]\n            row_data[current_block_start_row + row_in_block] = 0\n            best = row_data.argsort()[::-1][:self.topK]\n            notZerosMask = row_data[best] != 0.0\n            values_to_add = row_data[best][notZerosMask]\n            cols_to_add = best[notZerosMask]\n            for index in range(len(values_to_add)):\n                if numCells == len(rows):\n                    rows = np.concatenate((rows, np.zeros(dataBlock, dtype=np.int32)))\n                    cols = np.concatenate((cols, np.zeros(dataBlock, dtype=np.int32)))\n                    values = np.concatenate((values, np.zeros(dataBlock, dtype=np.float32)))\n                rows[numCells] = current_block_start_row + row_in_block\n                cols[numCells] = cols_to_add[index]\n                values[numCells] = values_to_add[index]\n                numCells += 1\n        if time.time() - start_time_printBatch > 300:\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            self._print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(current_block_start_row + block_dim, 100.0 * float(current_block_start_row + block_dim) / Pui.shape[1], float(current_block_start_row + block_dim) / (time.time() - start_time), new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_printBatch = time.time()\n    self.W_sparse = sps.csr_matrix((values[:numCells], (rows[:numCells], cols[:numCells])), shape=(Pui.shape[1], Pui.shape[1]))\n    if self.normalize_similarity:\n        self.W_sparse = normalize(self.W_sparse, norm='l1', axis=1)\n    if self.topK != False:\n        self.W_sparse = similarityMatrixTopK(self.W_sparse, k=self.topK)\n    self.W_sparse = check_matrix(self.W_sparse, format='csr')"
        ]
    }
]