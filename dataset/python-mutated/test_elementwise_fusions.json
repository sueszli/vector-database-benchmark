[
    {
        "func_name": "prog",
        "original": "@mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\ndef prog(x):\n    kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n    if prebuilt_bias:\n        kwargs['bias'] = np.random.rand(Cout)\n    x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n    if use_sub_instead:\n        x = mb.sub(x=x, y=const)\n    else:\n        x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n    x = mb.relu(x=x)\n    return x",
        "mutated": [
            "@mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\ndef prog(x):\n    if False:\n        i = 10\n    kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n    if prebuilt_bias:\n        kwargs['bias'] = np.random.rand(Cout)\n    x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n    if use_sub_instead:\n        x = mb.sub(x=x, y=const)\n    else:\n        x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n    x = mb.relu(x=x)\n    return x",
            "@mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\ndef prog(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n    if prebuilt_bias:\n        kwargs['bias'] = np.random.rand(Cout)\n    x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n    if use_sub_instead:\n        x = mb.sub(x=x, y=const)\n    else:\n        x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n    x = mb.relu(x=x)\n    return x",
            "@mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\ndef prog(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n    if prebuilt_bias:\n        kwargs['bias'] = np.random.rand(Cout)\n    x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n    if use_sub_instead:\n        x = mb.sub(x=x, y=const)\n    else:\n        x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n    x = mb.relu(x=x)\n    return x",
            "@mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\ndef prog(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n    if prebuilt_bias:\n        kwargs['bias'] = np.random.rand(Cout)\n    x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n    if use_sub_instead:\n        x = mb.sub(x=x, y=const)\n    else:\n        x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n    x = mb.relu(x=x)\n    return x",
            "@mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\ndef prog(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n    if prebuilt_bias:\n        kwargs['bias'] = np.random.rand(Cout)\n    x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n    if use_sub_instead:\n        x = mb.sub(x=x, y=const)\n    else:\n        x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n    x = mb.relu(x=x)\n    return x"
        ]
    },
    {
        "func_name": "test_fuse_bias_conv",
        "original": "@pytest.mark.parametrize('conv_dim,                              flip_add_input_order,                              add_batch_dim_to_const,                              use_sub_instead,                              prebuilt_bias,                              scalar_elementwise,                              use_conv_transpose', itertools.product([2, 3], [True, False], [True, False], [True, False], [True, False], [True, False], [True, False]))\ndef test_fuse_bias_conv(self, conv_dim, flip_add_input_order, add_batch_dim_to_const, use_sub_instead, prebuilt_bias, scalar_elementwise, use_conv_transpose):\n    if flip_add_input_order and use_sub_instead:\n        return\n    if use_conv_transpose and conv_dim != 2:\n        return\n    input_shape = None\n    W = None\n    Cout = 8\n    Cin = 3\n    D = 10\n    const = np.random.rand(Cout) if add_batch_dim_to_const else np.random.rand(1, Cout)\n    const = np.expand_dims(const, axis=-1)\n    if conv_dim == 1:\n        input_shape = (1, Cin, D)\n        W = np.random.rand(Cout, Cin, 1)\n    elif conv_dim == 2:\n        input_shape = (1, Cin, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n    elif conv_dim == 3:\n        input_shape = (1, Cin, D, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n        const = np.expand_dims(const, axis=-1)\n    output_shape = list(input_shape)\n    output_shape[1] = Cout\n    if scalar_elementwise:\n        const = np.random.uniform(0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\n    def prog(x):\n        kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n        if prebuilt_bias:\n            kwargs['bias'] = np.random.rand(Cout)\n        x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n        if use_sub_instead:\n            x = mb.sub(x=x, y=const)\n        else:\n            x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n        x = mb.relu(x=x)\n        return x\n    element_op = 'sub' if use_sub_instead else 'add'\n    conv_op = 'conv' if not use_conv_transpose else 'conv_transpose'\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_bias_conv')\n    assert get_op_types_in_program(prev_prog) == [conv_op, element_op, 'relu']\n    assert get_op_types_in_program(prog) == [conv_op, 'relu']\n    old_bias = prev_block.find_ops(op_type=conv_op)[0].inputs.get('bias', None)\n    old_bias_val = 0 if old_bias is None else old_bias.val\n    assert old_bias_val is not None\n    assert block.find_ops(op_type=conv_op)[0].inputs['bias'] is not None\n    new_bias_val = block.find_ops(op_type=conv_op)[0].inputs['bias'].val\n    assert new_bias_val is not None\n    if use_sub_instead:\n        np.testing.assert_almost_equal(old_bias_val - np.squeeze(const), new_bias_val)\n    else:\n        np.testing.assert_almost_equal(old_bias_val + np.squeeze(const), new_bias_val)\n    assert_model_is_valid(prog, {'x': input_shape}, expected_output_shapes={block.outputs[0].name: tuple(output_shape)})",
        "mutated": [
            "@pytest.mark.parametrize('conv_dim,                              flip_add_input_order,                              add_batch_dim_to_const,                              use_sub_instead,                              prebuilt_bias,                              scalar_elementwise,                              use_conv_transpose', itertools.product([2, 3], [True, False], [True, False], [True, False], [True, False], [True, False], [True, False]))\ndef test_fuse_bias_conv(self, conv_dim, flip_add_input_order, add_batch_dim_to_const, use_sub_instead, prebuilt_bias, scalar_elementwise, use_conv_transpose):\n    if False:\n        i = 10\n    if flip_add_input_order and use_sub_instead:\n        return\n    if use_conv_transpose and conv_dim != 2:\n        return\n    input_shape = None\n    W = None\n    Cout = 8\n    Cin = 3\n    D = 10\n    const = np.random.rand(Cout) if add_batch_dim_to_const else np.random.rand(1, Cout)\n    const = np.expand_dims(const, axis=-1)\n    if conv_dim == 1:\n        input_shape = (1, Cin, D)\n        W = np.random.rand(Cout, Cin, 1)\n    elif conv_dim == 2:\n        input_shape = (1, Cin, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n    elif conv_dim == 3:\n        input_shape = (1, Cin, D, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n        const = np.expand_dims(const, axis=-1)\n    output_shape = list(input_shape)\n    output_shape[1] = Cout\n    if scalar_elementwise:\n        const = np.random.uniform(0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\n    def prog(x):\n        kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n        if prebuilt_bias:\n            kwargs['bias'] = np.random.rand(Cout)\n        x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n        if use_sub_instead:\n            x = mb.sub(x=x, y=const)\n        else:\n            x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n        x = mb.relu(x=x)\n        return x\n    element_op = 'sub' if use_sub_instead else 'add'\n    conv_op = 'conv' if not use_conv_transpose else 'conv_transpose'\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_bias_conv')\n    assert get_op_types_in_program(prev_prog) == [conv_op, element_op, 'relu']\n    assert get_op_types_in_program(prog) == [conv_op, 'relu']\n    old_bias = prev_block.find_ops(op_type=conv_op)[0].inputs.get('bias', None)\n    old_bias_val = 0 if old_bias is None else old_bias.val\n    assert old_bias_val is not None\n    assert block.find_ops(op_type=conv_op)[0].inputs['bias'] is not None\n    new_bias_val = block.find_ops(op_type=conv_op)[0].inputs['bias'].val\n    assert new_bias_val is not None\n    if use_sub_instead:\n        np.testing.assert_almost_equal(old_bias_val - np.squeeze(const), new_bias_val)\n    else:\n        np.testing.assert_almost_equal(old_bias_val + np.squeeze(const), new_bias_val)\n    assert_model_is_valid(prog, {'x': input_shape}, expected_output_shapes={block.outputs[0].name: tuple(output_shape)})",
            "@pytest.mark.parametrize('conv_dim,                              flip_add_input_order,                              add_batch_dim_to_const,                              use_sub_instead,                              prebuilt_bias,                              scalar_elementwise,                              use_conv_transpose', itertools.product([2, 3], [True, False], [True, False], [True, False], [True, False], [True, False], [True, False]))\ndef test_fuse_bias_conv(self, conv_dim, flip_add_input_order, add_batch_dim_to_const, use_sub_instead, prebuilt_bias, scalar_elementwise, use_conv_transpose):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if flip_add_input_order and use_sub_instead:\n        return\n    if use_conv_transpose and conv_dim != 2:\n        return\n    input_shape = None\n    W = None\n    Cout = 8\n    Cin = 3\n    D = 10\n    const = np.random.rand(Cout) if add_batch_dim_to_const else np.random.rand(1, Cout)\n    const = np.expand_dims(const, axis=-1)\n    if conv_dim == 1:\n        input_shape = (1, Cin, D)\n        W = np.random.rand(Cout, Cin, 1)\n    elif conv_dim == 2:\n        input_shape = (1, Cin, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n    elif conv_dim == 3:\n        input_shape = (1, Cin, D, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n        const = np.expand_dims(const, axis=-1)\n    output_shape = list(input_shape)\n    output_shape[1] = Cout\n    if scalar_elementwise:\n        const = np.random.uniform(0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\n    def prog(x):\n        kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n        if prebuilt_bias:\n            kwargs['bias'] = np.random.rand(Cout)\n        x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n        if use_sub_instead:\n            x = mb.sub(x=x, y=const)\n        else:\n            x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n        x = mb.relu(x=x)\n        return x\n    element_op = 'sub' if use_sub_instead else 'add'\n    conv_op = 'conv' if not use_conv_transpose else 'conv_transpose'\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_bias_conv')\n    assert get_op_types_in_program(prev_prog) == [conv_op, element_op, 'relu']\n    assert get_op_types_in_program(prog) == [conv_op, 'relu']\n    old_bias = prev_block.find_ops(op_type=conv_op)[0].inputs.get('bias', None)\n    old_bias_val = 0 if old_bias is None else old_bias.val\n    assert old_bias_val is not None\n    assert block.find_ops(op_type=conv_op)[0].inputs['bias'] is not None\n    new_bias_val = block.find_ops(op_type=conv_op)[0].inputs['bias'].val\n    assert new_bias_val is not None\n    if use_sub_instead:\n        np.testing.assert_almost_equal(old_bias_val - np.squeeze(const), new_bias_val)\n    else:\n        np.testing.assert_almost_equal(old_bias_val + np.squeeze(const), new_bias_val)\n    assert_model_is_valid(prog, {'x': input_shape}, expected_output_shapes={block.outputs[0].name: tuple(output_shape)})",
            "@pytest.mark.parametrize('conv_dim,                              flip_add_input_order,                              add_batch_dim_to_const,                              use_sub_instead,                              prebuilt_bias,                              scalar_elementwise,                              use_conv_transpose', itertools.product([2, 3], [True, False], [True, False], [True, False], [True, False], [True, False], [True, False]))\ndef test_fuse_bias_conv(self, conv_dim, flip_add_input_order, add_batch_dim_to_const, use_sub_instead, prebuilt_bias, scalar_elementwise, use_conv_transpose):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if flip_add_input_order and use_sub_instead:\n        return\n    if use_conv_transpose and conv_dim != 2:\n        return\n    input_shape = None\n    W = None\n    Cout = 8\n    Cin = 3\n    D = 10\n    const = np.random.rand(Cout) if add_batch_dim_to_const else np.random.rand(1, Cout)\n    const = np.expand_dims(const, axis=-1)\n    if conv_dim == 1:\n        input_shape = (1, Cin, D)\n        W = np.random.rand(Cout, Cin, 1)\n    elif conv_dim == 2:\n        input_shape = (1, Cin, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n    elif conv_dim == 3:\n        input_shape = (1, Cin, D, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n        const = np.expand_dims(const, axis=-1)\n    output_shape = list(input_shape)\n    output_shape[1] = Cout\n    if scalar_elementwise:\n        const = np.random.uniform(0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\n    def prog(x):\n        kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n        if prebuilt_bias:\n            kwargs['bias'] = np.random.rand(Cout)\n        x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n        if use_sub_instead:\n            x = mb.sub(x=x, y=const)\n        else:\n            x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n        x = mb.relu(x=x)\n        return x\n    element_op = 'sub' if use_sub_instead else 'add'\n    conv_op = 'conv' if not use_conv_transpose else 'conv_transpose'\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_bias_conv')\n    assert get_op_types_in_program(prev_prog) == [conv_op, element_op, 'relu']\n    assert get_op_types_in_program(prog) == [conv_op, 'relu']\n    old_bias = prev_block.find_ops(op_type=conv_op)[0].inputs.get('bias', None)\n    old_bias_val = 0 if old_bias is None else old_bias.val\n    assert old_bias_val is not None\n    assert block.find_ops(op_type=conv_op)[0].inputs['bias'] is not None\n    new_bias_val = block.find_ops(op_type=conv_op)[0].inputs['bias'].val\n    assert new_bias_val is not None\n    if use_sub_instead:\n        np.testing.assert_almost_equal(old_bias_val - np.squeeze(const), new_bias_val)\n    else:\n        np.testing.assert_almost_equal(old_bias_val + np.squeeze(const), new_bias_val)\n    assert_model_is_valid(prog, {'x': input_shape}, expected_output_shapes={block.outputs[0].name: tuple(output_shape)})",
            "@pytest.mark.parametrize('conv_dim,                              flip_add_input_order,                              add_batch_dim_to_const,                              use_sub_instead,                              prebuilt_bias,                              scalar_elementwise,                              use_conv_transpose', itertools.product([2, 3], [True, False], [True, False], [True, False], [True, False], [True, False], [True, False]))\ndef test_fuse_bias_conv(self, conv_dim, flip_add_input_order, add_batch_dim_to_const, use_sub_instead, prebuilt_bias, scalar_elementwise, use_conv_transpose):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if flip_add_input_order and use_sub_instead:\n        return\n    if use_conv_transpose and conv_dim != 2:\n        return\n    input_shape = None\n    W = None\n    Cout = 8\n    Cin = 3\n    D = 10\n    const = np.random.rand(Cout) if add_batch_dim_to_const else np.random.rand(1, Cout)\n    const = np.expand_dims(const, axis=-1)\n    if conv_dim == 1:\n        input_shape = (1, Cin, D)\n        W = np.random.rand(Cout, Cin, 1)\n    elif conv_dim == 2:\n        input_shape = (1, Cin, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n    elif conv_dim == 3:\n        input_shape = (1, Cin, D, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n        const = np.expand_dims(const, axis=-1)\n    output_shape = list(input_shape)\n    output_shape[1] = Cout\n    if scalar_elementwise:\n        const = np.random.uniform(0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\n    def prog(x):\n        kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n        if prebuilt_bias:\n            kwargs['bias'] = np.random.rand(Cout)\n        x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n        if use_sub_instead:\n            x = mb.sub(x=x, y=const)\n        else:\n            x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n        x = mb.relu(x=x)\n        return x\n    element_op = 'sub' if use_sub_instead else 'add'\n    conv_op = 'conv' if not use_conv_transpose else 'conv_transpose'\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_bias_conv')\n    assert get_op_types_in_program(prev_prog) == [conv_op, element_op, 'relu']\n    assert get_op_types_in_program(prog) == [conv_op, 'relu']\n    old_bias = prev_block.find_ops(op_type=conv_op)[0].inputs.get('bias', None)\n    old_bias_val = 0 if old_bias is None else old_bias.val\n    assert old_bias_val is not None\n    assert block.find_ops(op_type=conv_op)[0].inputs['bias'] is not None\n    new_bias_val = block.find_ops(op_type=conv_op)[0].inputs['bias'].val\n    assert new_bias_val is not None\n    if use_sub_instead:\n        np.testing.assert_almost_equal(old_bias_val - np.squeeze(const), new_bias_val)\n    else:\n        np.testing.assert_almost_equal(old_bias_val + np.squeeze(const), new_bias_val)\n    assert_model_is_valid(prog, {'x': input_shape}, expected_output_shapes={block.outputs[0].name: tuple(output_shape)})",
            "@pytest.mark.parametrize('conv_dim,                              flip_add_input_order,                              add_batch_dim_to_const,                              use_sub_instead,                              prebuilt_bias,                              scalar_elementwise,                              use_conv_transpose', itertools.product([2, 3], [True, False], [True, False], [True, False], [True, False], [True, False], [True, False]))\ndef test_fuse_bias_conv(self, conv_dim, flip_add_input_order, add_batch_dim_to_const, use_sub_instead, prebuilt_bias, scalar_elementwise, use_conv_transpose):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if flip_add_input_order and use_sub_instead:\n        return\n    if use_conv_transpose and conv_dim != 2:\n        return\n    input_shape = None\n    W = None\n    Cout = 8\n    Cin = 3\n    D = 10\n    const = np.random.rand(Cout) if add_batch_dim_to_const else np.random.rand(1, Cout)\n    const = np.expand_dims(const, axis=-1)\n    if conv_dim == 1:\n        input_shape = (1, Cin, D)\n        W = np.random.rand(Cout, Cin, 1)\n    elif conv_dim == 2:\n        input_shape = (1, Cin, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n    elif conv_dim == 3:\n        input_shape = (1, Cin, D, D, D)\n        W = np.random.rand(Cout, Cin, 1, 1, 1)\n        const = np.expand_dims(const, axis=-1)\n        const = np.expand_dims(const, axis=-1)\n    output_shape = list(input_shape)\n    output_shape[1] = Cout\n    if scalar_elementwise:\n        const = np.random.uniform(0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=input_shape)])\n    def prog(x):\n        kwargs = {'x': x, 'weight': W, 'pad_type': 'valid', 'dilations': [1] * conv_dim, 'strides': [1] * conv_dim}\n        if prebuilt_bias:\n            kwargs['bias'] = np.random.rand(Cout)\n        x = mb.conv_transpose(**kwargs) if use_conv_transpose else mb.conv(**kwargs)\n        if use_sub_instead:\n            x = mb.sub(x=x, y=const)\n        else:\n            x = mb.add(x=const if flip_add_input_order else x, y=x if flip_add_input_order else const)\n        x = mb.relu(x=x)\n        return x\n    element_op = 'sub' if use_sub_instead else 'add'\n    conv_op = 'conv' if not use_conv_transpose else 'conv_transpose'\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_bias_conv')\n    assert get_op_types_in_program(prev_prog) == [conv_op, element_op, 'relu']\n    assert get_op_types_in_program(prog) == [conv_op, 'relu']\n    old_bias = prev_block.find_ops(op_type=conv_op)[0].inputs.get('bias', None)\n    old_bias_val = 0 if old_bias is None else old_bias.val\n    assert old_bias_val is not None\n    assert block.find_ops(op_type=conv_op)[0].inputs['bias'] is not None\n    new_bias_val = block.find_ops(op_type=conv_op)[0].inputs['bias'].val\n    assert new_bias_val is not None\n    if use_sub_instead:\n        np.testing.assert_almost_equal(old_bias_val - np.squeeze(const), new_bias_val)\n    else:\n        np.testing.assert_almost_equal(old_bias_val + np.squeeze(const), new_bias_val)\n    assert_model_is_valid(prog, {'x': input_shape}, expected_output_shapes={block.outputs[0].name: tuple(output_shape)})"
        ]
    },
    {
        "func_name": "prog",
        "original": "@mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\ndef prog(x):\n    x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n    if flip_mul_input_order:\n        x = mb.mul(x=gamma, y=x)\n    else:\n        x = mb.mul(x=x, y=gamma)\n    if flip_add_input_order:\n        x = mb.add(x=beta, y=x)\n    else:\n        x = mb.add(x=x, y=beta)\n    return x",
        "mutated": [
            "@mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\ndef prog(x):\n    if False:\n        i = 10\n    x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n    if flip_mul_input_order:\n        x = mb.mul(x=gamma, y=x)\n    else:\n        x = mb.mul(x=x, y=gamma)\n    if flip_add_input_order:\n        x = mb.add(x=beta, y=x)\n    else:\n        x = mb.add(x=x, y=beta)\n    return x",
            "@mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\ndef prog(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n    if flip_mul_input_order:\n        x = mb.mul(x=gamma, y=x)\n    else:\n        x = mb.mul(x=x, y=gamma)\n    if flip_add_input_order:\n        x = mb.add(x=beta, y=x)\n    else:\n        x = mb.add(x=x, y=beta)\n    return x",
            "@mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\ndef prog(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n    if flip_mul_input_order:\n        x = mb.mul(x=gamma, y=x)\n    else:\n        x = mb.mul(x=x, y=gamma)\n    if flip_add_input_order:\n        x = mb.add(x=beta, y=x)\n    else:\n        x = mb.add(x=x, y=beta)\n    return x",
            "@mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\ndef prog(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n    if flip_mul_input_order:\n        x = mb.mul(x=gamma, y=x)\n    else:\n        x = mb.mul(x=x, y=gamma)\n    if flip_add_input_order:\n        x = mb.add(x=beta, y=x)\n    else:\n        x = mb.add(x=x, y=beta)\n    return x",
            "@mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\ndef prog(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n    if flip_mul_input_order:\n        x = mb.mul(x=gamma, y=x)\n    else:\n        x = mb.mul(x=x, y=gamma)\n    if flip_add_input_order:\n        x = mb.add(x=beta, y=x)\n    else:\n        x = mb.add(x=x, y=beta)\n    return x"
        ]
    },
    {
        "func_name": "test_mul_add_fusion_to_batchnorm",
        "original": "@pytest.mark.parametrize('flip_mul_input_order, flip_add_input_order, rank_3_const_input', itertools.product([False, True], [False, True], [False, True]))\ndef test_mul_add_fusion_to_batchnorm(self, flip_mul_input_order, flip_add_input_order, rank_3_const_input):\n    C = 3\n    gamma = np.random.rand(1, C, 1, 1)\n    beta = np.random.rand(1, C, 1, 1)\n    if rank_3_const_input:\n        gamma = np.squeeze(gamma, axis=0)\n        beta = np.squeeze(beta, axis=0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\n    def prog(x):\n        x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n        if flip_mul_input_order:\n            x = mb.mul(x=gamma, y=x)\n        else:\n            x = mb.mul(x=x, y=gamma)\n        if flip_add_input_order:\n            x = mb.add(x=beta, y=x)\n        else:\n            x = mb.add(x=x, y=beta)\n        return x\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_elementwise_to_batchnorm')\n    assert get_op_types_in_program(prev_prog) == ['transpose', 'mul', 'add']\n    assert get_op_types_in_program(prog) == ['transpose', 'batch_norm']\n    assert_model_is_valid(prog, {'x': (1, 10, 10, C)}, expected_output_shapes={block.outputs[0].name: (1, C, 10, 10)})",
        "mutated": [
            "@pytest.mark.parametrize('flip_mul_input_order, flip_add_input_order, rank_3_const_input', itertools.product([False, True], [False, True], [False, True]))\ndef test_mul_add_fusion_to_batchnorm(self, flip_mul_input_order, flip_add_input_order, rank_3_const_input):\n    if False:\n        i = 10\n    C = 3\n    gamma = np.random.rand(1, C, 1, 1)\n    beta = np.random.rand(1, C, 1, 1)\n    if rank_3_const_input:\n        gamma = np.squeeze(gamma, axis=0)\n        beta = np.squeeze(beta, axis=0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\n    def prog(x):\n        x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n        if flip_mul_input_order:\n            x = mb.mul(x=gamma, y=x)\n        else:\n            x = mb.mul(x=x, y=gamma)\n        if flip_add_input_order:\n            x = mb.add(x=beta, y=x)\n        else:\n            x = mb.add(x=x, y=beta)\n        return x\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_elementwise_to_batchnorm')\n    assert get_op_types_in_program(prev_prog) == ['transpose', 'mul', 'add']\n    assert get_op_types_in_program(prog) == ['transpose', 'batch_norm']\n    assert_model_is_valid(prog, {'x': (1, 10, 10, C)}, expected_output_shapes={block.outputs[0].name: (1, C, 10, 10)})",
            "@pytest.mark.parametrize('flip_mul_input_order, flip_add_input_order, rank_3_const_input', itertools.product([False, True], [False, True], [False, True]))\ndef test_mul_add_fusion_to_batchnorm(self, flip_mul_input_order, flip_add_input_order, rank_3_const_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    C = 3\n    gamma = np.random.rand(1, C, 1, 1)\n    beta = np.random.rand(1, C, 1, 1)\n    if rank_3_const_input:\n        gamma = np.squeeze(gamma, axis=0)\n        beta = np.squeeze(beta, axis=0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\n    def prog(x):\n        x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n        if flip_mul_input_order:\n            x = mb.mul(x=gamma, y=x)\n        else:\n            x = mb.mul(x=x, y=gamma)\n        if flip_add_input_order:\n            x = mb.add(x=beta, y=x)\n        else:\n            x = mb.add(x=x, y=beta)\n        return x\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_elementwise_to_batchnorm')\n    assert get_op_types_in_program(prev_prog) == ['transpose', 'mul', 'add']\n    assert get_op_types_in_program(prog) == ['transpose', 'batch_norm']\n    assert_model_is_valid(prog, {'x': (1, 10, 10, C)}, expected_output_shapes={block.outputs[0].name: (1, C, 10, 10)})",
            "@pytest.mark.parametrize('flip_mul_input_order, flip_add_input_order, rank_3_const_input', itertools.product([False, True], [False, True], [False, True]))\ndef test_mul_add_fusion_to_batchnorm(self, flip_mul_input_order, flip_add_input_order, rank_3_const_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    C = 3\n    gamma = np.random.rand(1, C, 1, 1)\n    beta = np.random.rand(1, C, 1, 1)\n    if rank_3_const_input:\n        gamma = np.squeeze(gamma, axis=0)\n        beta = np.squeeze(beta, axis=0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\n    def prog(x):\n        x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n        if flip_mul_input_order:\n            x = mb.mul(x=gamma, y=x)\n        else:\n            x = mb.mul(x=x, y=gamma)\n        if flip_add_input_order:\n            x = mb.add(x=beta, y=x)\n        else:\n            x = mb.add(x=x, y=beta)\n        return x\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_elementwise_to_batchnorm')\n    assert get_op_types_in_program(prev_prog) == ['transpose', 'mul', 'add']\n    assert get_op_types_in_program(prog) == ['transpose', 'batch_norm']\n    assert_model_is_valid(prog, {'x': (1, 10, 10, C)}, expected_output_shapes={block.outputs[0].name: (1, C, 10, 10)})",
            "@pytest.mark.parametrize('flip_mul_input_order, flip_add_input_order, rank_3_const_input', itertools.product([False, True], [False, True], [False, True]))\ndef test_mul_add_fusion_to_batchnorm(self, flip_mul_input_order, flip_add_input_order, rank_3_const_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    C = 3\n    gamma = np.random.rand(1, C, 1, 1)\n    beta = np.random.rand(1, C, 1, 1)\n    if rank_3_const_input:\n        gamma = np.squeeze(gamma, axis=0)\n        beta = np.squeeze(beta, axis=0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\n    def prog(x):\n        x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n        if flip_mul_input_order:\n            x = mb.mul(x=gamma, y=x)\n        else:\n            x = mb.mul(x=x, y=gamma)\n        if flip_add_input_order:\n            x = mb.add(x=beta, y=x)\n        else:\n            x = mb.add(x=x, y=beta)\n        return x\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_elementwise_to_batchnorm')\n    assert get_op_types_in_program(prev_prog) == ['transpose', 'mul', 'add']\n    assert get_op_types_in_program(prog) == ['transpose', 'batch_norm']\n    assert_model_is_valid(prog, {'x': (1, 10, 10, C)}, expected_output_shapes={block.outputs[0].name: (1, C, 10, 10)})",
            "@pytest.mark.parametrize('flip_mul_input_order, flip_add_input_order, rank_3_const_input', itertools.product([False, True], [False, True], [False, True]))\ndef test_mul_add_fusion_to_batchnorm(self, flip_mul_input_order, flip_add_input_order, rank_3_const_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    C = 3\n    gamma = np.random.rand(1, C, 1, 1)\n    beta = np.random.rand(1, C, 1, 1)\n    if rank_3_const_input:\n        gamma = np.squeeze(gamma, axis=0)\n        beta = np.squeeze(beta, axis=0)\n\n    @mb.program(input_specs=[mb.TensorSpec(shape=(1, 10, 10, C))])\n    def prog(x):\n        x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n        if flip_mul_input_order:\n            x = mb.mul(x=gamma, y=x)\n        else:\n            x = mb.mul(x=x, y=gamma)\n        if flip_add_input_order:\n            x = mb.add(x=beta, y=x)\n        else:\n            x = mb.add(x=x, y=beta)\n        return x\n    (prev_prog, prev_block, block) = apply_pass_and_basic_check(prog, 'common::fuse_elementwise_to_batchnorm')\n    assert get_op_types_in_program(prev_prog) == ['transpose', 'mul', 'add']\n    assert get_op_types_in_program(prog) == ['transpose', 'batch_norm']\n    assert_model_is_valid(prog, {'x': (1, 10, 10, C)}, expected_output_shapes={block.outputs[0].name: (1, C, 10, 10)})"
        ]
    }
]