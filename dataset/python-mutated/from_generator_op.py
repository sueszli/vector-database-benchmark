"""The implementation of `tf.data.Dataset.from_generator`."""
import numpy as np
from tensorflow.python.data.ops import dataset_ops
from tensorflow.python.data.ops import structured_function
from tensorflow.python.data.util import nest
from tensorflow.python.data.util import structure
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.framework import tensor_shape
from tensorflow.python.framework import tensor_spec
from tensorflow.python.framework import type_spec
from tensorflow.python.ops import gen_dataset_ops
from tensorflow.python.ops import script_ops

def _from_generator(generator, output_types, output_shapes, args, output_signature, name):
    if False:
        print('Hello World!')
    'Creates a `Dataset` whose elements are generated by `generator`.\n\n  Note: The current implementation of `Dataset.from_generator()` uses\n  `tf.numpy_function` and inherits the same constraints. In particular, it\n  requires the dataset and iterator related operations to be placed\n  on a device in the same process as the Python program that called\n  `Dataset.from_generator()`. In particular, using `from_generator` will\n  preclude the use of tf.data service for scaling out dataset processing.\n  The body of `generator` will not be serialized in a `GraphDef`, and you\n  should not use this method if you need to serialize your model and restore\n  it in a different environment.\n\n  The `generator` argument must be a callable object that returns\n  an object that supports the `iter()` protocol (e.g. a generator function).\n\n  The elements generated by `generator` must be compatible with either the\n  given `output_signature` argument or with the given `output_types` and\n  (optionally) `output_shapes` arguments, whichever was specified.\n\n  The recommended way to call `from_generator` is to use the\n  `output_signature` argument. In this case the output will be assumed to\n  consist of objects with the classes, shapes and types defined by\n  `tf.TypeSpec` objects from `output_signature` argument:\n\n  >>> def gen():\n  ...   ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  ...   yield 42, ragged_tensor\n  >>>\n  >>> dataset = tf.data.Dataset.from_generator(\n  ...      gen,\n  ...      output_signature=(\n  ...          tf.TensorSpec(shape=(), dtype=tf.int32),\n  ...          tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n  >>>\n  >>> list(dataset.take(1))\n  [(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n  <tf.RaggedTensor [[1, 2], [3]]>)]\n\n  There is also a deprecated way to call `from_generator` by either with\n  `output_types` argument alone or together with `output_shapes` argument.\n  In this case the output of the function will be assumed to consist of\n  `tf.Tensor` objects with the types defined by `output_types` and with the\n  shapes which are either unknown or defined by `output_shapes`.\n\n  Note: If `generator` depends on mutable global variables or other external\n  state, be aware that the runtime may invoke `generator` multiple times\n  (in order to support repeating the `Dataset`) and at any time\n  between the call to `Dataset.from_generator()` and the production of the\n  first element from the generator. Mutating global variables or external\n  state can cause undefined behavior, and we recommend that you explicitly\n  cache any external state in `generator` before calling\n  `Dataset.from_generator()`.\n\n  Note: While the `output_signature` parameter makes it possible to yield\n  `Dataset` elements, the scope of `Dataset.from_generator()` should be\n  limited to logic that cannot be expressed through tf.data operations. Using\n  tf.data operations within the generator function is an anti-pattern and may\n  result in incremental memory growth.\n\n  Args:\n    generator: A callable object that returns an object that supports the\n      `iter()` protocol. If `args` is not specified, `generator` must take no\n      arguments; otherwise it must take as many arguments as there are values in\n      `args`.\n    output_types: (Optional.) A (nested) structure of `tf.DType` objects\n      corresponding to each component of an element yielded by `generator`.\n    output_shapes: (Optional.) A (nested) structure of `tf.TensorShape` objects\n      corresponding to each component of an element yielded by `generator`.\n    args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated and\n      passed to `generator` as NumPy-array arguments.\n    output_signature: (Optional.) A (nested) structure of `tf.TypeSpec` objects\n      corresponding to each component of an element yielded by `generator`.\n    name: (Optional.) A name for the tf.data operations used by\n      `from_generator`.\n\n  Returns:\n    Dataset: A `Dataset`.\n  '
    if not callable(generator):
        raise TypeError('`generator` must be a Python callable.')
    if output_signature is not None:
        if output_types is not None:
            raise TypeError('The `output_types` argument can not be used together with the `output_signature` argument.')
        if output_shapes is not None:
            raise TypeError('The `output_shapes` argument can not be used together with the `output_signature` argument.')
        for spec in nest.flatten(output_signature):
            if not isinstance(spec, type_spec.TypeSpec):
                raise TypeError(f'`output_signature` must contain objects that are subclass of `tf.TypeSpec` but found {type(spec)} which is not.')
    elif output_types is None:
        raise TypeError('To specify the output signature you need to provide either the `output_signature` argument or the `output_types` argument.')
    if output_signature is None:
        if output_shapes is None:
            output_shapes = nest.map_structure(lambda _: tensor_shape.TensorShape(None), output_types)
        else:
            output_shapes = nest.map_structure_up_to(output_types, tensor_shape.as_shape, output_shapes)
        output_signature = nest.map_structure_up_to(output_types, tensor_spec.TensorSpec, output_shapes, output_types)
    if all((isinstance(x, tensor_spec.TensorSpec) for x in nest.flatten(output_signature))):
        output_types = nest.pack_sequence_as(output_signature, [x.dtype for x in nest.flatten(output_signature)])
        output_shapes = nest.pack_sequence_as(output_signature, [x.shape for x in nest.flatten(output_signature)])
    if args is None:
        args = ()
    else:
        args = tuple(ops.convert_n_to_tensor(args, name='args'))
    generator_state = dataset_ops.DatasetV2._GeneratorState(generator)

    def get_iterator_id_fn(unused_dummy):
        if False:
            return 10
        'Creates a unique `iterator_id` for each pass over the dataset.\n\n    The returned `iterator_id` disambiguates between multiple concurrently\n    existing iterators.\n\n    Args:\n      unused_dummy: Ignored value.\n\n    Returns:\n      A `tf.int64` tensor whose value uniquely identifies an iterator in\n      `generator_state`.\n    '
        return script_ops.numpy_function(generator_state.get_next_id, args, dtypes.int64)

    def generator_next_fn(iterator_id_t):
        if False:
            for i in range(10):
                print('nop')
        'Generates the next element from iterator with ID `iterator_id_t`.\n\n    We map this function across an infinite repetition of the\n    `iterator_id_t`, and raise `StopIteration` to terminate the iteration.\n\n    Args:\n      iterator_id_t: A `tf.int64` tensor whose value uniquely identifies the\n        iterator in `generator_state` from which to generate an element.\n\n    Returns:\n      The next element to generate from the iterator.\n    '
        if output_types and output_shapes:
            flattened_types = [dtypes.as_dtype(dt) for dt in nest.flatten(output_types)]
            flattened_shapes = nest.flatten(output_shapes)

            def generator_py_func(iterator_id):
                if False:
                    while True:
                        i = 10
                'A `py_func` that will be called to invoke the iterator.'
                values = next(generator_state.get_iterator(iterator_id))
                try:
                    flattened_values = nest.flatten_up_to(output_types, values)
                except (TypeError, ValueError) as e:
                    raise TypeError(f'`generator` yielded an element that did not match the expected structure. The expected structure was {output_types}, but the yielded element was {values}.') from e
                ret_arrays = []
                for (ret, dtype) in zip(flattened_values, flattened_types):
                    try:
                        ret_arrays.append(script_ops.FuncRegistry._convert(ret, dtype=dtype.as_numpy_dtype))
                    except (TypeError, ValueError) as e:
                        raise TypeError(f'`generator` yielded an element that could not be converted to the expected type. The expected type was {dtype.name}, but the yielded element was {ret}.') from e
                for (ret_array, expected_dtype, expected_shape) in zip(ret_arrays, flattened_types, flattened_shapes):
                    if ret_array.dtype != expected_dtype.as_numpy_dtype:
                        raise TypeError(f'`generator` yielded an element of type {ret_array.dtype} where an element of type {expected_dtype.as_numpy_dtype} was expected.')
                    if not expected_shape.is_compatible_with(ret_array.shape):
                        raise TypeError(f'`generator` yielded an element of shape {ret_array.shape} where an element of shape {expected_shape} was expected.')
                return ret_arrays
            flat_values = script_ops.numpy_function(generator_py_func, [iterator_id_t], flattened_types)
            if not isinstance(flat_values, (list, tuple)):
                flat_values = [flat_values]
            if output_shapes is not None:
                for (ret_t, shape) in zip(flat_values, flattened_shapes):
                    ret_t.set_shape(shape)
            return nest.pack_sequence_as(output_types, flat_values)
        else:
            flat_output_types = structure.get_flat_tensor_types(output_signature)

            def generator_py_func(iterator_id):
                if False:
                    return 10
                'A `py_func` that will be called to invoke the iterator.'
                values = next(generator_state.get_iterator(iterator_id.numpy()))
                try:
                    values = structure.normalize_element(values, output_signature)
                except (TypeError, ValueError) as e:
                    raise TypeError(f'`generator` yielded an element that did not match the expected structure. The expected structure was {output_signature}, but the yielded element was {values}.') from e
                values_spec = structure.type_spec_from_value(values)
                if not structure.are_compatible(values_spec, output_signature):
                    raise TypeError(f'`generator` yielded an element of {values_spec} where an element of {output_signature} was expected.')
                return structure.to_tensor_list(output_signature, values)
            return script_ops.eager_py_func(generator_py_func, inp=[iterator_id_t], Tout=flat_output_types)

    def finalize_fn(iterator_id_t):
        if False:
            i = 10
            return i + 15
        'Releases host-side state for the iterator with ID `iterator_id_t`.'

        def finalize_py_func(iterator_id):
            if False:
                i = 10
                return i + 15
            generator_state.iterator_completed(iterator_id)
            return np.array(0, dtype=np.int64)
        return script_ops.numpy_function(finalize_py_func, [iterator_id_t], dtypes.int64)

    def flat_map_fn(dummy_arg):
        if False:
            i = 10
            return i + 15
        return _GeneratorDataset(dummy_arg, get_iterator_id_fn, generator_next_fn, finalize_fn, output_signature, name=name)
    dummy = 0
    id_dataset = dataset_ops.Dataset.from_tensors(dummy, name=name)
    return id_dataset.flat_map(flat_map_fn, name=name)

class _GeneratorDataset(dataset_ops.DatasetSource):
    """A `Dataset` that generates elements by invoking a function."""

    def __init__(self, init_args, init_func, next_func, finalize_func, output_signature, name=None):
        if False:
            i = 10
            return i + 15
        'Constructs a `_GeneratorDataset`.\n\n    Args:\n      init_args: A (nested) structure representing the arguments to `init_func`.\n      init_func: A TensorFlow function that will be called on `init_args` each\n        time a C++ iterator over this dataset is constructed. Returns a (nested)\n        structure representing the "state" of the dataset.\n      next_func: A TensorFlow function that will be called on the result of\n        `init_func` to produce each element, and that raises `OutOfRangeError`\n        to terminate iteration.\n      finalize_func: A TensorFlow function that will be called on the result of\n        `init_func` immediately before a C++ iterator over this dataset is\n        destroyed. The return value is ignored.\n      output_signature: A (nested) structure of `tf.TypeSpec` objects describing\n        the output of `next_func`.\n      name: Optional. A name for the tf.data transformation.\n    '
        self._init_args = init_args
        self._init_structure = structure.type_spec_from_value(init_args)
        self._init_func = structured_function.StructuredFunctionWrapper(init_func, self._transformation_name(), input_structure=self._init_structure)
        self._next_func = structured_function.StructuredFunctionWrapper(next_func, self._transformation_name(), input_structure=self._init_func.output_structure)
        self._finalize_func = structured_function.StructuredFunctionWrapper(finalize_func, self._transformation_name(), input_structure=self._init_func.output_structure)
        self._output_signature = output_signature
        self._name = name
        variant_tensor = gen_dataset_ops.generator_dataset(structure.to_tensor_list(self._init_structure, self._init_args) + self._init_func.function.captured_inputs, self._next_func.function.captured_inputs, self._finalize_func.function.captured_inputs, init_func=self._init_func.function, next_func=self._next_func.function, finalize_func=self._finalize_func.function, **self._common_args)
        super().__init__(variant_tensor)

    @property
    def element_spec(self):
        if False:
            for i in range(10):
                print('nop')
        return self._output_signature

    def _transformation_name(self):
        if False:
            while True:
                i = 10
        return 'Dataset.from_generator()'