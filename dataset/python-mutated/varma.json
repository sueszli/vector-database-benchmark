[
    {
        "func_name": "VAR",
        "original": "def VAR(x, B, const=0):\n    \"\"\" multivariate linear filter\n\n    Parameters\n    ----------\n    x: (TxK) array\n        columns are variables, rows are observations for time period\n    B: (PxKxK) array\n        b_t-1 is bottom \"row\", b_t-P is top \"row\" when printing\n        B(:,:,0) is lag polynomial matrix for variable 1\n        B(:,:,k) is lag polynomial matrix for variable k\n        B(p,:,k) is pth lag for variable k\n        B[p,:,:].T corresponds to A_p in Wikipedia\n    const : float or array (not tested)\n        constant added to autoregression\n\n    Returns\n    -------\n    xhat: (TxK) array\n        filtered, predicted values of x array\n\n    Notes\n    -----\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) }  for all i = 0,K-1, for all t=p..T\n\n    xhat does not include the forecasting observation, xhat(T+1),\n    xhat is 1 row shorter than signal.correlate\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Vector_Autoregression\n    https://en.wikipedia.org/wiki/General_matrix_notation_of_a_VAR(p)\n    \"\"\"\n    p = B.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    for t in range(p, T):\n        xhat[t, :] = const + (x[t - p:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0)\n    return xhat",
        "mutated": [
            "def VAR(x, B, const=0):\n    if False:\n        i = 10\n    ' multivariate linear filter\\n\\n    Parameters\\n    ----------\\n    x: (TxK) array\\n        columns are variables, rows are observations for time period\\n    B: (PxKxK) array\\n        b_t-1 is bottom \"row\", b_t-P is top \"row\" when printing\\n        B(:,:,0) is lag polynomial matrix for variable 1\\n        B(:,:,k) is lag polynomial matrix for variable k\\n        B(p,:,k) is pth lag for variable k\\n        B[p,:,:].T corresponds to A_p in Wikipedia\\n    const : float or array (not tested)\\n        constant added to autoregression\\n\\n    Returns\\n    -------\\n    xhat: (TxK) array\\n        filtered, predicted values of x array\\n\\n    Notes\\n    -----\\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) }  for all i = 0,K-1, for all t=p..T\\n\\n    xhat does not include the forecasting observation, xhat(T+1),\\n    xhat is 1 row shorter than signal.correlate\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Vector_Autoregression\\n    https://en.wikipedia.org/wiki/General_matrix_notation_of_a_VAR(p)\\n    '\n    p = B.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    for t in range(p, T):\n        xhat[t, :] = const + (x[t - p:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0)\n    return xhat",
            "def VAR(x, B, const=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' multivariate linear filter\\n\\n    Parameters\\n    ----------\\n    x: (TxK) array\\n        columns are variables, rows are observations for time period\\n    B: (PxKxK) array\\n        b_t-1 is bottom \"row\", b_t-P is top \"row\" when printing\\n        B(:,:,0) is lag polynomial matrix for variable 1\\n        B(:,:,k) is lag polynomial matrix for variable k\\n        B(p,:,k) is pth lag for variable k\\n        B[p,:,:].T corresponds to A_p in Wikipedia\\n    const : float or array (not tested)\\n        constant added to autoregression\\n\\n    Returns\\n    -------\\n    xhat: (TxK) array\\n        filtered, predicted values of x array\\n\\n    Notes\\n    -----\\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) }  for all i = 0,K-1, for all t=p..T\\n\\n    xhat does not include the forecasting observation, xhat(T+1),\\n    xhat is 1 row shorter than signal.correlate\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Vector_Autoregression\\n    https://en.wikipedia.org/wiki/General_matrix_notation_of_a_VAR(p)\\n    '\n    p = B.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    for t in range(p, T):\n        xhat[t, :] = const + (x[t - p:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0)\n    return xhat",
            "def VAR(x, B, const=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' multivariate linear filter\\n\\n    Parameters\\n    ----------\\n    x: (TxK) array\\n        columns are variables, rows are observations for time period\\n    B: (PxKxK) array\\n        b_t-1 is bottom \"row\", b_t-P is top \"row\" when printing\\n        B(:,:,0) is lag polynomial matrix for variable 1\\n        B(:,:,k) is lag polynomial matrix for variable k\\n        B(p,:,k) is pth lag for variable k\\n        B[p,:,:].T corresponds to A_p in Wikipedia\\n    const : float or array (not tested)\\n        constant added to autoregression\\n\\n    Returns\\n    -------\\n    xhat: (TxK) array\\n        filtered, predicted values of x array\\n\\n    Notes\\n    -----\\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) }  for all i = 0,K-1, for all t=p..T\\n\\n    xhat does not include the forecasting observation, xhat(T+1),\\n    xhat is 1 row shorter than signal.correlate\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Vector_Autoregression\\n    https://en.wikipedia.org/wiki/General_matrix_notation_of_a_VAR(p)\\n    '\n    p = B.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    for t in range(p, T):\n        xhat[t, :] = const + (x[t - p:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0)\n    return xhat",
            "def VAR(x, B, const=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' multivariate linear filter\\n\\n    Parameters\\n    ----------\\n    x: (TxK) array\\n        columns are variables, rows are observations for time period\\n    B: (PxKxK) array\\n        b_t-1 is bottom \"row\", b_t-P is top \"row\" when printing\\n        B(:,:,0) is lag polynomial matrix for variable 1\\n        B(:,:,k) is lag polynomial matrix for variable k\\n        B(p,:,k) is pth lag for variable k\\n        B[p,:,:].T corresponds to A_p in Wikipedia\\n    const : float or array (not tested)\\n        constant added to autoregression\\n\\n    Returns\\n    -------\\n    xhat: (TxK) array\\n        filtered, predicted values of x array\\n\\n    Notes\\n    -----\\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) }  for all i = 0,K-1, for all t=p..T\\n\\n    xhat does not include the forecasting observation, xhat(T+1),\\n    xhat is 1 row shorter than signal.correlate\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Vector_Autoregression\\n    https://en.wikipedia.org/wiki/General_matrix_notation_of_a_VAR(p)\\n    '\n    p = B.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    for t in range(p, T):\n        xhat[t, :] = const + (x[t - p:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0)\n    return xhat",
            "def VAR(x, B, const=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' multivariate linear filter\\n\\n    Parameters\\n    ----------\\n    x: (TxK) array\\n        columns are variables, rows are observations for time period\\n    B: (PxKxK) array\\n        b_t-1 is bottom \"row\", b_t-P is top \"row\" when printing\\n        B(:,:,0) is lag polynomial matrix for variable 1\\n        B(:,:,k) is lag polynomial matrix for variable k\\n        B(p,:,k) is pth lag for variable k\\n        B[p,:,:].T corresponds to A_p in Wikipedia\\n    const : float or array (not tested)\\n        constant added to autoregression\\n\\n    Returns\\n    -------\\n    xhat: (TxK) array\\n        filtered, predicted values of x array\\n\\n    Notes\\n    -----\\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) }  for all i = 0,K-1, for all t=p..T\\n\\n    xhat does not include the forecasting observation, xhat(T+1),\\n    xhat is 1 row shorter than signal.correlate\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Vector_Autoregression\\n    https://en.wikipedia.org/wiki/General_matrix_notation_of_a_VAR(p)\\n    '\n    p = B.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    for t in range(p, T):\n        xhat[t, :] = const + (x[t - p:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0)\n    return xhat"
        ]
    },
    {
        "func_name": "VARMA",
        "original": "def VARMA(x, B, C, const=0):\n    \"\"\" multivariate linear filter\n\n    x (TxK)\n    B (PxKxK)\n\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) } +\n                sum{_q}sum{_k} { e(t-Q:t,:) .* C(:,:,i) }for all i = 0,K-1\n\n    \"\"\"\n    P = B.shape[0]\n    Q = C.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    e = np.zeros(x.shape)\n    start = max(P, Q)\n    for t in range(start, T):\n        xhat[t, :] = const + (x[t - P:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0) + (e[t - Q:t, :, np.newaxis] * C).sum(axis=1).sum(axis=0)\n        e[t, :] = x[t, :] - xhat[t, :]\n    return (xhat, e)",
        "mutated": [
            "def VARMA(x, B, C, const=0):\n    if False:\n        i = 10\n    ' multivariate linear filter\\n\\n    x (TxK)\\n    B (PxKxK)\\n\\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) } +\\n                sum{_q}sum{_k} { e(t-Q:t,:) .* C(:,:,i) }for all i = 0,K-1\\n\\n    '\n    P = B.shape[0]\n    Q = C.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    e = np.zeros(x.shape)\n    start = max(P, Q)\n    for t in range(start, T):\n        xhat[t, :] = const + (x[t - P:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0) + (e[t - Q:t, :, np.newaxis] * C).sum(axis=1).sum(axis=0)\n        e[t, :] = x[t, :] - xhat[t, :]\n    return (xhat, e)",
            "def VARMA(x, B, C, const=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' multivariate linear filter\\n\\n    x (TxK)\\n    B (PxKxK)\\n\\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) } +\\n                sum{_q}sum{_k} { e(t-Q:t,:) .* C(:,:,i) }for all i = 0,K-1\\n\\n    '\n    P = B.shape[0]\n    Q = C.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    e = np.zeros(x.shape)\n    start = max(P, Q)\n    for t in range(start, T):\n        xhat[t, :] = const + (x[t - P:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0) + (e[t - Q:t, :, np.newaxis] * C).sum(axis=1).sum(axis=0)\n        e[t, :] = x[t, :] - xhat[t, :]\n    return (xhat, e)",
            "def VARMA(x, B, C, const=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' multivariate linear filter\\n\\n    x (TxK)\\n    B (PxKxK)\\n\\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) } +\\n                sum{_q}sum{_k} { e(t-Q:t,:) .* C(:,:,i) }for all i = 0,K-1\\n\\n    '\n    P = B.shape[0]\n    Q = C.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    e = np.zeros(x.shape)\n    start = max(P, Q)\n    for t in range(start, T):\n        xhat[t, :] = const + (x[t - P:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0) + (e[t - Q:t, :, np.newaxis] * C).sum(axis=1).sum(axis=0)\n        e[t, :] = x[t, :] - xhat[t, :]\n    return (xhat, e)",
            "def VARMA(x, B, C, const=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' multivariate linear filter\\n\\n    x (TxK)\\n    B (PxKxK)\\n\\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) } +\\n                sum{_q}sum{_k} { e(t-Q:t,:) .* C(:,:,i) }for all i = 0,K-1\\n\\n    '\n    P = B.shape[0]\n    Q = C.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    e = np.zeros(x.shape)\n    start = max(P, Q)\n    for t in range(start, T):\n        xhat[t, :] = const + (x[t - P:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0) + (e[t - Q:t, :, np.newaxis] * C).sum(axis=1).sum(axis=0)\n        e[t, :] = x[t, :] - xhat[t, :]\n    return (xhat, e)",
            "def VARMA(x, B, C, const=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' multivariate linear filter\\n\\n    x (TxK)\\n    B (PxKxK)\\n\\n    xhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) } +\\n                sum{_q}sum{_k} { e(t-Q:t,:) .* C(:,:,i) }for all i = 0,K-1\\n\\n    '\n    P = B.shape[0]\n    Q = C.shape[0]\n    T = x.shape[0]\n    xhat = np.zeros(x.shape)\n    e = np.zeros(x.shape)\n    start = max(P, Q)\n    for t in range(start, T):\n        xhat[t, :] = const + (x[t - P:t, :, np.newaxis] * B).sum(axis=1).sum(axis=0) + (e[t - Q:t, :, np.newaxis] * C).sum(axis=1).sum(axis=0)\n        e[t, :] = x[t, :] - xhat[t, :]\n    return (xhat, e)"
        ]
    }
]