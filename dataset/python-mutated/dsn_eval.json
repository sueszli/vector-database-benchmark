[
    {
        "func_name": "quaternion_metric",
        "original": "def quaternion_metric(predictions, labels):\n    params = {'batch_size': FLAGS.batch_size, 'use_logging': False}\n    logcost = losses.log_quaternion_loss_batch(predictions, labels, params)\n    return slim.metrics.streaming_mean(logcost)",
        "mutated": [
            "def quaternion_metric(predictions, labels):\n    if False:\n        i = 10\n    params = {'batch_size': FLAGS.batch_size, 'use_logging': False}\n    logcost = losses.log_quaternion_loss_batch(predictions, labels, params)\n    return slim.metrics.streaming_mean(logcost)",
            "def quaternion_metric(predictions, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    params = {'batch_size': FLAGS.batch_size, 'use_logging': False}\n    logcost = losses.log_quaternion_loss_batch(predictions, labels, params)\n    return slim.metrics.streaming_mean(logcost)",
            "def quaternion_metric(predictions, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    params = {'batch_size': FLAGS.batch_size, 'use_logging': False}\n    logcost = losses.log_quaternion_loss_batch(predictions, labels, params)\n    return slim.metrics.streaming_mean(logcost)",
            "def quaternion_metric(predictions, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    params = {'batch_size': FLAGS.batch_size, 'use_logging': False}\n    logcost = losses.log_quaternion_loss_batch(predictions, labels, params)\n    return slim.metrics.streaming_mean(logcost)",
            "def quaternion_metric(predictions, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    params = {'batch_size': FLAGS.batch_size, 'use_logging': False}\n    logcost = losses.log_quaternion_loss_batch(predictions, labels, params)\n    return slim.metrics.streaming_mean(logcost)"
        ]
    },
    {
        "func_name": "angle_diff",
        "original": "def angle_diff(true_q, pred_q):\n    angles = 2 * (180.0 / np.pi) * np.arccos(np.abs(np.sum(np.multiply(pred_q, true_q), axis=1)))\n    return angles",
        "mutated": [
            "def angle_diff(true_q, pred_q):\n    if False:\n        i = 10\n    angles = 2 * (180.0 / np.pi) * np.arccos(np.abs(np.sum(np.multiply(pred_q, true_q), axis=1)))\n    return angles",
            "def angle_diff(true_q, pred_q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    angles = 2 * (180.0 / np.pi) * np.arccos(np.abs(np.sum(np.multiply(pred_q, true_q), axis=1)))\n    return angles",
            "def angle_diff(true_q, pred_q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    angles = 2 * (180.0 / np.pi) * np.arccos(np.abs(np.sum(np.multiply(pred_q, true_q), axis=1)))\n    return angles",
            "def angle_diff(true_q, pred_q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    angles = 2 * (180.0 / np.pi) * np.arccos(np.abs(np.sum(np.multiply(pred_q, true_q), axis=1)))\n    return angles",
            "def angle_diff(true_q, pred_q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    angles = 2 * (180.0 / np.pi) * np.arccos(np.abs(np.sum(np.multiply(pred_q, true_q), axis=1)))\n    return angles"
        ]
    },
    {
        "func_name": "provide_batch_fn",
        "original": "def provide_batch_fn():\n    \"\"\" The provide_batch function to use. \"\"\"\n    return dataset_factory.provide_batch",
        "mutated": [
            "def provide_batch_fn():\n    if False:\n        i = 10\n    ' The provide_batch function to use. '\n    return dataset_factory.provide_batch",
            "def provide_batch_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' The provide_batch function to use. '\n    return dataset_factory.provide_batch",
            "def provide_batch_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' The provide_batch function to use. '\n    return dataset_factory.provide_batch",
            "def provide_batch_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' The provide_batch function to use. '\n    return dataset_factory.provide_batch",
            "def provide_batch_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' The provide_batch function to use. '\n    return dataset_factory.provide_batch"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(_):\n    g = tf.Graph()\n    with g.as_default():\n        (images, labels) = provide_batch_fn()(FLAGS.dataset, FLAGS.split, FLAGS.dataset_dir, 4, FLAGS.batch_size, 4)\n        num_classes = labels['classes'].get_shape().as_list()[1]\n        tf.summary.image('eval_images', images, max_outputs=3)\n        with tf.variable_scope('towers'):\n            basic_tower = getattr(models, FLAGS.basic_tower)\n            (predictions, endpoints) = basic_tower(images, num_classes=num_classes, is_training=False, batch_norm_params=None)\n        metric_names_to_values = {}\n        if 'quaternions' in labels:\n            quaternion_loss = quaternion_metric(labels['quaternions'], endpoints['quaternion_pred'])\n            (angle_errors,) = tf.py_func(angle_diff, [labels['quaternions'], endpoints['quaternion_pred']], [tf.float32])\n            metric_names_to_values['Angular mean error'] = slim.metrics.streaming_mean(angle_errors)\n            metric_names_to_values['Quaternion Loss'] = quaternion_loss\n        accuracy = tf.contrib.metrics.streaming_accuracy(tf.argmax(predictions, 1), tf.argmax(labels['classes'], 1))\n        predictions = tf.argmax(predictions, 1)\n        labels = tf.argmax(labels['classes'], 1)\n        metric_names_to_values['Accuracy'] = accuracy\n        if FLAGS.enable_precision_recall:\n            for i in xrange(num_classes):\n                index_map = tf.one_hot(i, depth=num_classes)\n                name = 'PR/Precision_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_precision(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n                name = 'PR/Recall_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_recall(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n        (names_to_values, names_to_updates) = slim.metrics.aggregate_metric_map(metric_names_to_values)\n        summary_ops = []\n        for (metric_name, metric_value) in names_to_values.iteritems():\n            op = tf.summary.scalar(metric_name, metric_value)\n            op = tf.Print(op, [metric_value], metric_name)\n            summary_ops.append(op)\n        num_batches = math.ceil(FLAGS.num_examples / float(FLAGS.batch_size))\n        slim.get_or_create_global_step()\n        slim.evaluation.evaluation_loop(FLAGS.master, checkpoint_dir=FLAGS.checkpoint_dir, logdir=FLAGS.eval_dir, num_evals=num_batches, eval_op=names_to_updates.values(), summary_op=tf.summary.merge(summary_ops))",
        "mutated": [
            "def main(_):\n    if False:\n        i = 10\n    g = tf.Graph()\n    with g.as_default():\n        (images, labels) = provide_batch_fn()(FLAGS.dataset, FLAGS.split, FLAGS.dataset_dir, 4, FLAGS.batch_size, 4)\n        num_classes = labels['classes'].get_shape().as_list()[1]\n        tf.summary.image('eval_images', images, max_outputs=3)\n        with tf.variable_scope('towers'):\n            basic_tower = getattr(models, FLAGS.basic_tower)\n            (predictions, endpoints) = basic_tower(images, num_classes=num_classes, is_training=False, batch_norm_params=None)\n        metric_names_to_values = {}\n        if 'quaternions' in labels:\n            quaternion_loss = quaternion_metric(labels['quaternions'], endpoints['quaternion_pred'])\n            (angle_errors,) = tf.py_func(angle_diff, [labels['quaternions'], endpoints['quaternion_pred']], [tf.float32])\n            metric_names_to_values['Angular mean error'] = slim.metrics.streaming_mean(angle_errors)\n            metric_names_to_values['Quaternion Loss'] = quaternion_loss\n        accuracy = tf.contrib.metrics.streaming_accuracy(tf.argmax(predictions, 1), tf.argmax(labels['classes'], 1))\n        predictions = tf.argmax(predictions, 1)\n        labels = tf.argmax(labels['classes'], 1)\n        metric_names_to_values['Accuracy'] = accuracy\n        if FLAGS.enable_precision_recall:\n            for i in xrange(num_classes):\n                index_map = tf.one_hot(i, depth=num_classes)\n                name = 'PR/Precision_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_precision(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n                name = 'PR/Recall_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_recall(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n        (names_to_values, names_to_updates) = slim.metrics.aggregate_metric_map(metric_names_to_values)\n        summary_ops = []\n        for (metric_name, metric_value) in names_to_values.iteritems():\n            op = tf.summary.scalar(metric_name, metric_value)\n            op = tf.Print(op, [metric_value], metric_name)\n            summary_ops.append(op)\n        num_batches = math.ceil(FLAGS.num_examples / float(FLAGS.batch_size))\n        slim.get_or_create_global_step()\n        slim.evaluation.evaluation_loop(FLAGS.master, checkpoint_dir=FLAGS.checkpoint_dir, logdir=FLAGS.eval_dir, num_evals=num_batches, eval_op=names_to_updates.values(), summary_op=tf.summary.merge(summary_ops))",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    g = tf.Graph()\n    with g.as_default():\n        (images, labels) = provide_batch_fn()(FLAGS.dataset, FLAGS.split, FLAGS.dataset_dir, 4, FLAGS.batch_size, 4)\n        num_classes = labels['classes'].get_shape().as_list()[1]\n        tf.summary.image('eval_images', images, max_outputs=3)\n        with tf.variable_scope('towers'):\n            basic_tower = getattr(models, FLAGS.basic_tower)\n            (predictions, endpoints) = basic_tower(images, num_classes=num_classes, is_training=False, batch_norm_params=None)\n        metric_names_to_values = {}\n        if 'quaternions' in labels:\n            quaternion_loss = quaternion_metric(labels['quaternions'], endpoints['quaternion_pred'])\n            (angle_errors,) = tf.py_func(angle_diff, [labels['quaternions'], endpoints['quaternion_pred']], [tf.float32])\n            metric_names_to_values['Angular mean error'] = slim.metrics.streaming_mean(angle_errors)\n            metric_names_to_values['Quaternion Loss'] = quaternion_loss\n        accuracy = tf.contrib.metrics.streaming_accuracy(tf.argmax(predictions, 1), tf.argmax(labels['classes'], 1))\n        predictions = tf.argmax(predictions, 1)\n        labels = tf.argmax(labels['classes'], 1)\n        metric_names_to_values['Accuracy'] = accuracy\n        if FLAGS.enable_precision_recall:\n            for i in xrange(num_classes):\n                index_map = tf.one_hot(i, depth=num_classes)\n                name = 'PR/Precision_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_precision(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n                name = 'PR/Recall_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_recall(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n        (names_to_values, names_to_updates) = slim.metrics.aggregate_metric_map(metric_names_to_values)\n        summary_ops = []\n        for (metric_name, metric_value) in names_to_values.iteritems():\n            op = tf.summary.scalar(metric_name, metric_value)\n            op = tf.Print(op, [metric_value], metric_name)\n            summary_ops.append(op)\n        num_batches = math.ceil(FLAGS.num_examples / float(FLAGS.batch_size))\n        slim.get_or_create_global_step()\n        slim.evaluation.evaluation_loop(FLAGS.master, checkpoint_dir=FLAGS.checkpoint_dir, logdir=FLAGS.eval_dir, num_evals=num_batches, eval_op=names_to_updates.values(), summary_op=tf.summary.merge(summary_ops))",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    g = tf.Graph()\n    with g.as_default():\n        (images, labels) = provide_batch_fn()(FLAGS.dataset, FLAGS.split, FLAGS.dataset_dir, 4, FLAGS.batch_size, 4)\n        num_classes = labels['classes'].get_shape().as_list()[1]\n        tf.summary.image('eval_images', images, max_outputs=3)\n        with tf.variable_scope('towers'):\n            basic_tower = getattr(models, FLAGS.basic_tower)\n            (predictions, endpoints) = basic_tower(images, num_classes=num_classes, is_training=False, batch_norm_params=None)\n        metric_names_to_values = {}\n        if 'quaternions' in labels:\n            quaternion_loss = quaternion_metric(labels['quaternions'], endpoints['quaternion_pred'])\n            (angle_errors,) = tf.py_func(angle_diff, [labels['quaternions'], endpoints['quaternion_pred']], [tf.float32])\n            metric_names_to_values['Angular mean error'] = slim.metrics.streaming_mean(angle_errors)\n            metric_names_to_values['Quaternion Loss'] = quaternion_loss\n        accuracy = tf.contrib.metrics.streaming_accuracy(tf.argmax(predictions, 1), tf.argmax(labels['classes'], 1))\n        predictions = tf.argmax(predictions, 1)\n        labels = tf.argmax(labels['classes'], 1)\n        metric_names_to_values['Accuracy'] = accuracy\n        if FLAGS.enable_precision_recall:\n            for i in xrange(num_classes):\n                index_map = tf.one_hot(i, depth=num_classes)\n                name = 'PR/Precision_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_precision(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n                name = 'PR/Recall_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_recall(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n        (names_to_values, names_to_updates) = slim.metrics.aggregate_metric_map(metric_names_to_values)\n        summary_ops = []\n        for (metric_name, metric_value) in names_to_values.iteritems():\n            op = tf.summary.scalar(metric_name, metric_value)\n            op = tf.Print(op, [metric_value], metric_name)\n            summary_ops.append(op)\n        num_batches = math.ceil(FLAGS.num_examples / float(FLAGS.batch_size))\n        slim.get_or_create_global_step()\n        slim.evaluation.evaluation_loop(FLAGS.master, checkpoint_dir=FLAGS.checkpoint_dir, logdir=FLAGS.eval_dir, num_evals=num_batches, eval_op=names_to_updates.values(), summary_op=tf.summary.merge(summary_ops))",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    g = tf.Graph()\n    with g.as_default():\n        (images, labels) = provide_batch_fn()(FLAGS.dataset, FLAGS.split, FLAGS.dataset_dir, 4, FLAGS.batch_size, 4)\n        num_classes = labels['classes'].get_shape().as_list()[1]\n        tf.summary.image('eval_images', images, max_outputs=3)\n        with tf.variable_scope('towers'):\n            basic_tower = getattr(models, FLAGS.basic_tower)\n            (predictions, endpoints) = basic_tower(images, num_classes=num_classes, is_training=False, batch_norm_params=None)\n        metric_names_to_values = {}\n        if 'quaternions' in labels:\n            quaternion_loss = quaternion_metric(labels['quaternions'], endpoints['quaternion_pred'])\n            (angle_errors,) = tf.py_func(angle_diff, [labels['quaternions'], endpoints['quaternion_pred']], [tf.float32])\n            metric_names_to_values['Angular mean error'] = slim.metrics.streaming_mean(angle_errors)\n            metric_names_to_values['Quaternion Loss'] = quaternion_loss\n        accuracy = tf.contrib.metrics.streaming_accuracy(tf.argmax(predictions, 1), tf.argmax(labels['classes'], 1))\n        predictions = tf.argmax(predictions, 1)\n        labels = tf.argmax(labels['classes'], 1)\n        metric_names_to_values['Accuracy'] = accuracy\n        if FLAGS.enable_precision_recall:\n            for i in xrange(num_classes):\n                index_map = tf.one_hot(i, depth=num_classes)\n                name = 'PR/Precision_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_precision(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n                name = 'PR/Recall_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_recall(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n        (names_to_values, names_to_updates) = slim.metrics.aggregate_metric_map(metric_names_to_values)\n        summary_ops = []\n        for (metric_name, metric_value) in names_to_values.iteritems():\n            op = tf.summary.scalar(metric_name, metric_value)\n            op = tf.Print(op, [metric_value], metric_name)\n            summary_ops.append(op)\n        num_batches = math.ceil(FLAGS.num_examples / float(FLAGS.batch_size))\n        slim.get_or_create_global_step()\n        slim.evaluation.evaluation_loop(FLAGS.master, checkpoint_dir=FLAGS.checkpoint_dir, logdir=FLAGS.eval_dir, num_evals=num_batches, eval_op=names_to_updates.values(), summary_op=tf.summary.merge(summary_ops))",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    g = tf.Graph()\n    with g.as_default():\n        (images, labels) = provide_batch_fn()(FLAGS.dataset, FLAGS.split, FLAGS.dataset_dir, 4, FLAGS.batch_size, 4)\n        num_classes = labels['classes'].get_shape().as_list()[1]\n        tf.summary.image('eval_images', images, max_outputs=3)\n        with tf.variable_scope('towers'):\n            basic_tower = getattr(models, FLAGS.basic_tower)\n            (predictions, endpoints) = basic_tower(images, num_classes=num_classes, is_training=False, batch_norm_params=None)\n        metric_names_to_values = {}\n        if 'quaternions' in labels:\n            quaternion_loss = quaternion_metric(labels['quaternions'], endpoints['quaternion_pred'])\n            (angle_errors,) = tf.py_func(angle_diff, [labels['quaternions'], endpoints['quaternion_pred']], [tf.float32])\n            metric_names_to_values['Angular mean error'] = slim.metrics.streaming_mean(angle_errors)\n            metric_names_to_values['Quaternion Loss'] = quaternion_loss\n        accuracy = tf.contrib.metrics.streaming_accuracy(tf.argmax(predictions, 1), tf.argmax(labels['classes'], 1))\n        predictions = tf.argmax(predictions, 1)\n        labels = tf.argmax(labels['classes'], 1)\n        metric_names_to_values['Accuracy'] = accuracy\n        if FLAGS.enable_precision_recall:\n            for i in xrange(num_classes):\n                index_map = tf.one_hot(i, depth=num_classes)\n                name = 'PR/Precision_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_precision(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n                name = 'PR/Recall_{}'.format(i)\n                metric_names_to_values[name] = slim.metrics.streaming_recall(tf.gather(index_map, predictions), tf.gather(index_map, labels))\n        (names_to_values, names_to_updates) = slim.metrics.aggregate_metric_map(metric_names_to_values)\n        summary_ops = []\n        for (metric_name, metric_value) in names_to_values.iteritems():\n            op = tf.summary.scalar(metric_name, metric_value)\n            op = tf.Print(op, [metric_value], metric_name)\n            summary_ops.append(op)\n        num_batches = math.ceil(FLAGS.num_examples / float(FLAGS.batch_size))\n        slim.get_or_create_global_step()\n        slim.evaluation.evaluation_loop(FLAGS.master, checkpoint_dir=FLAGS.checkpoint_dir, logdir=FLAGS.eval_dir, num_evals=num_batches, eval_op=names_to_updates.values(), summary_op=tf.summary.merge(summary_ops))"
        ]
    }
]