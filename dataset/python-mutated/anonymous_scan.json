[
    {
        "func_name": "_scan_pyarrow_dataset",
        "original": "def _scan_pyarrow_dataset(ds: pa.dataset.Dataset, *, allow_pyarrow_filter: bool=True, batch_size: int | None=None) -> LazyFrame:\n    \"\"\"\n    Pickle the partially applied function `_scan_pyarrow_dataset_impl`.\n\n    The bytes are then sent to the polars logical plan. It can be deserialized once\n    executed and ran.\n\n    Parameters\n    ----------\n    ds\n        pyarrow dataset\n    allow_pyarrow_filter\n        Allow predicates to be pushed down to pyarrow. This can lead to different\n        results if comparisons are done with null values as pyarrow handles this\n        different than polars does.\n    batch_size\n        The maximum row count for scanned pyarrow record batches.\n\n    \"\"\"\n    func = partial(_scan_pyarrow_dataset_impl, ds, batch_size=batch_size)\n    return pl.LazyFrame._scan_python_function(ds.schema, func, pyarrow=allow_pyarrow_filter)",
        "mutated": [
            "def _scan_pyarrow_dataset(ds: pa.dataset.Dataset, *, allow_pyarrow_filter: bool=True, batch_size: int | None=None) -> LazyFrame:\n    if False:\n        i = 10\n    '\\n    Pickle the partially applied function `_scan_pyarrow_dataset_impl`.\\n\\n    The bytes are then sent to the polars logical plan. It can be deserialized once\\n    executed and ran.\\n\\n    Parameters\\n    ----------\\n    ds\\n        pyarrow dataset\\n    allow_pyarrow_filter\\n        Allow predicates to be pushed down to pyarrow. This can lead to different\\n        results if comparisons are done with null values as pyarrow handles this\\n        different than polars does.\\n    batch_size\\n        The maximum row count for scanned pyarrow record batches.\\n\\n    '\n    func = partial(_scan_pyarrow_dataset_impl, ds, batch_size=batch_size)\n    return pl.LazyFrame._scan_python_function(ds.schema, func, pyarrow=allow_pyarrow_filter)",
            "def _scan_pyarrow_dataset(ds: pa.dataset.Dataset, *, allow_pyarrow_filter: bool=True, batch_size: int | None=None) -> LazyFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Pickle the partially applied function `_scan_pyarrow_dataset_impl`.\\n\\n    The bytes are then sent to the polars logical plan. It can be deserialized once\\n    executed and ran.\\n\\n    Parameters\\n    ----------\\n    ds\\n        pyarrow dataset\\n    allow_pyarrow_filter\\n        Allow predicates to be pushed down to pyarrow. This can lead to different\\n        results if comparisons are done with null values as pyarrow handles this\\n        different than polars does.\\n    batch_size\\n        The maximum row count for scanned pyarrow record batches.\\n\\n    '\n    func = partial(_scan_pyarrow_dataset_impl, ds, batch_size=batch_size)\n    return pl.LazyFrame._scan_python_function(ds.schema, func, pyarrow=allow_pyarrow_filter)",
            "def _scan_pyarrow_dataset(ds: pa.dataset.Dataset, *, allow_pyarrow_filter: bool=True, batch_size: int | None=None) -> LazyFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Pickle the partially applied function `_scan_pyarrow_dataset_impl`.\\n\\n    The bytes are then sent to the polars logical plan. It can be deserialized once\\n    executed and ran.\\n\\n    Parameters\\n    ----------\\n    ds\\n        pyarrow dataset\\n    allow_pyarrow_filter\\n        Allow predicates to be pushed down to pyarrow. This can lead to different\\n        results if comparisons are done with null values as pyarrow handles this\\n        different than polars does.\\n    batch_size\\n        The maximum row count for scanned pyarrow record batches.\\n\\n    '\n    func = partial(_scan_pyarrow_dataset_impl, ds, batch_size=batch_size)\n    return pl.LazyFrame._scan_python_function(ds.schema, func, pyarrow=allow_pyarrow_filter)",
            "def _scan_pyarrow_dataset(ds: pa.dataset.Dataset, *, allow_pyarrow_filter: bool=True, batch_size: int | None=None) -> LazyFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Pickle the partially applied function `_scan_pyarrow_dataset_impl`.\\n\\n    The bytes are then sent to the polars logical plan. It can be deserialized once\\n    executed and ran.\\n\\n    Parameters\\n    ----------\\n    ds\\n        pyarrow dataset\\n    allow_pyarrow_filter\\n        Allow predicates to be pushed down to pyarrow. This can lead to different\\n        results if comparisons are done with null values as pyarrow handles this\\n        different than polars does.\\n    batch_size\\n        The maximum row count for scanned pyarrow record batches.\\n\\n    '\n    func = partial(_scan_pyarrow_dataset_impl, ds, batch_size=batch_size)\n    return pl.LazyFrame._scan_python_function(ds.schema, func, pyarrow=allow_pyarrow_filter)",
            "def _scan_pyarrow_dataset(ds: pa.dataset.Dataset, *, allow_pyarrow_filter: bool=True, batch_size: int | None=None) -> LazyFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Pickle the partially applied function `_scan_pyarrow_dataset_impl`.\\n\\n    The bytes are then sent to the polars logical plan. It can be deserialized once\\n    executed and ran.\\n\\n    Parameters\\n    ----------\\n    ds\\n        pyarrow dataset\\n    allow_pyarrow_filter\\n        Allow predicates to be pushed down to pyarrow. This can lead to different\\n        results if comparisons are done with null values as pyarrow handles this\\n        different than polars does.\\n    batch_size\\n        The maximum row count for scanned pyarrow record batches.\\n\\n    '\n    func = partial(_scan_pyarrow_dataset_impl, ds, batch_size=batch_size)\n    return pl.LazyFrame._scan_python_function(ds.schema, func, pyarrow=allow_pyarrow_filter)"
        ]
    },
    {
        "func_name": "_scan_pyarrow_dataset_impl",
        "original": "def _scan_pyarrow_dataset_impl(ds: pa.dataset.Dataset, with_columns: list[str] | None, predicate: str | None, n_rows: int | None, batch_size: int | None) -> DataFrame:\n    \"\"\"\n    Take the projected columns and materialize an arrow table.\n\n    Parameters\n    ----------\n    ds\n        pyarrow dataset\n    with_columns\n        Columns that are projected\n    predicate\n        pyarrow expression that can be evaluated with eval\n    n_rows:\n        Materialize only n rows from the arrow dataset\n    batch_size\n        The maximum row count for scanned pyarrow record batches.\n\n    Returns\n    -------\n    DataFrame\n\n    \"\"\"\n    from polars import from_arrow\n    _filter = None\n    if predicate:\n        from polars.datatypes import Date, Datetime, Duration\n        from polars.utils.convert import _to_python_date, _to_python_datetime, _to_python_time, _to_python_timedelta\n        _filter = eval(predicate, {'pa': pa, 'Date': Date, 'Datetime': Datetime, 'Duration': Duration, '_to_python_date': _to_python_date, '_to_python_datetime': _to_python_datetime, '_to_python_time': _to_python_time, '_to_python_timedelta': _to_python_timedelta})\n    common_params = {'columns': with_columns, 'filter': _filter}\n    if batch_size is not None:\n        common_params['batch_size'] = batch_size\n    if n_rows:\n        return from_arrow(ds.head(n_rows, **common_params))\n    return from_arrow(ds.to_table(**common_params))",
        "mutated": [
            "def _scan_pyarrow_dataset_impl(ds: pa.dataset.Dataset, with_columns: list[str] | None, predicate: str | None, n_rows: int | None, batch_size: int | None) -> DataFrame:\n    if False:\n        i = 10\n    '\\n    Take the projected columns and materialize an arrow table.\\n\\n    Parameters\\n    ----------\\n    ds\\n        pyarrow dataset\\n    with_columns\\n        Columns that are projected\\n    predicate\\n        pyarrow expression that can be evaluated with eval\\n    n_rows:\\n        Materialize only n rows from the arrow dataset\\n    batch_size\\n        The maximum row count for scanned pyarrow record batches.\\n\\n    Returns\\n    -------\\n    DataFrame\\n\\n    '\n    from polars import from_arrow\n    _filter = None\n    if predicate:\n        from polars.datatypes import Date, Datetime, Duration\n        from polars.utils.convert import _to_python_date, _to_python_datetime, _to_python_time, _to_python_timedelta\n        _filter = eval(predicate, {'pa': pa, 'Date': Date, 'Datetime': Datetime, 'Duration': Duration, '_to_python_date': _to_python_date, '_to_python_datetime': _to_python_datetime, '_to_python_time': _to_python_time, '_to_python_timedelta': _to_python_timedelta})\n    common_params = {'columns': with_columns, 'filter': _filter}\n    if batch_size is not None:\n        common_params['batch_size'] = batch_size\n    if n_rows:\n        return from_arrow(ds.head(n_rows, **common_params))\n    return from_arrow(ds.to_table(**common_params))",
            "def _scan_pyarrow_dataset_impl(ds: pa.dataset.Dataset, with_columns: list[str] | None, predicate: str | None, n_rows: int | None, batch_size: int | None) -> DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Take the projected columns and materialize an arrow table.\\n\\n    Parameters\\n    ----------\\n    ds\\n        pyarrow dataset\\n    with_columns\\n        Columns that are projected\\n    predicate\\n        pyarrow expression that can be evaluated with eval\\n    n_rows:\\n        Materialize only n rows from the arrow dataset\\n    batch_size\\n        The maximum row count for scanned pyarrow record batches.\\n\\n    Returns\\n    -------\\n    DataFrame\\n\\n    '\n    from polars import from_arrow\n    _filter = None\n    if predicate:\n        from polars.datatypes import Date, Datetime, Duration\n        from polars.utils.convert import _to_python_date, _to_python_datetime, _to_python_time, _to_python_timedelta\n        _filter = eval(predicate, {'pa': pa, 'Date': Date, 'Datetime': Datetime, 'Duration': Duration, '_to_python_date': _to_python_date, '_to_python_datetime': _to_python_datetime, '_to_python_time': _to_python_time, '_to_python_timedelta': _to_python_timedelta})\n    common_params = {'columns': with_columns, 'filter': _filter}\n    if batch_size is not None:\n        common_params['batch_size'] = batch_size\n    if n_rows:\n        return from_arrow(ds.head(n_rows, **common_params))\n    return from_arrow(ds.to_table(**common_params))",
            "def _scan_pyarrow_dataset_impl(ds: pa.dataset.Dataset, with_columns: list[str] | None, predicate: str | None, n_rows: int | None, batch_size: int | None) -> DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Take the projected columns and materialize an arrow table.\\n\\n    Parameters\\n    ----------\\n    ds\\n        pyarrow dataset\\n    with_columns\\n        Columns that are projected\\n    predicate\\n        pyarrow expression that can be evaluated with eval\\n    n_rows:\\n        Materialize only n rows from the arrow dataset\\n    batch_size\\n        The maximum row count for scanned pyarrow record batches.\\n\\n    Returns\\n    -------\\n    DataFrame\\n\\n    '\n    from polars import from_arrow\n    _filter = None\n    if predicate:\n        from polars.datatypes import Date, Datetime, Duration\n        from polars.utils.convert import _to_python_date, _to_python_datetime, _to_python_time, _to_python_timedelta\n        _filter = eval(predicate, {'pa': pa, 'Date': Date, 'Datetime': Datetime, 'Duration': Duration, '_to_python_date': _to_python_date, '_to_python_datetime': _to_python_datetime, '_to_python_time': _to_python_time, '_to_python_timedelta': _to_python_timedelta})\n    common_params = {'columns': with_columns, 'filter': _filter}\n    if batch_size is not None:\n        common_params['batch_size'] = batch_size\n    if n_rows:\n        return from_arrow(ds.head(n_rows, **common_params))\n    return from_arrow(ds.to_table(**common_params))",
            "def _scan_pyarrow_dataset_impl(ds: pa.dataset.Dataset, with_columns: list[str] | None, predicate: str | None, n_rows: int | None, batch_size: int | None) -> DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Take the projected columns and materialize an arrow table.\\n\\n    Parameters\\n    ----------\\n    ds\\n        pyarrow dataset\\n    with_columns\\n        Columns that are projected\\n    predicate\\n        pyarrow expression that can be evaluated with eval\\n    n_rows:\\n        Materialize only n rows from the arrow dataset\\n    batch_size\\n        The maximum row count for scanned pyarrow record batches.\\n\\n    Returns\\n    -------\\n    DataFrame\\n\\n    '\n    from polars import from_arrow\n    _filter = None\n    if predicate:\n        from polars.datatypes import Date, Datetime, Duration\n        from polars.utils.convert import _to_python_date, _to_python_datetime, _to_python_time, _to_python_timedelta\n        _filter = eval(predicate, {'pa': pa, 'Date': Date, 'Datetime': Datetime, 'Duration': Duration, '_to_python_date': _to_python_date, '_to_python_datetime': _to_python_datetime, '_to_python_time': _to_python_time, '_to_python_timedelta': _to_python_timedelta})\n    common_params = {'columns': with_columns, 'filter': _filter}\n    if batch_size is not None:\n        common_params['batch_size'] = batch_size\n    if n_rows:\n        return from_arrow(ds.head(n_rows, **common_params))\n    return from_arrow(ds.to_table(**common_params))",
            "def _scan_pyarrow_dataset_impl(ds: pa.dataset.Dataset, with_columns: list[str] | None, predicate: str | None, n_rows: int | None, batch_size: int | None) -> DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Take the projected columns and materialize an arrow table.\\n\\n    Parameters\\n    ----------\\n    ds\\n        pyarrow dataset\\n    with_columns\\n        Columns that are projected\\n    predicate\\n        pyarrow expression that can be evaluated with eval\\n    n_rows:\\n        Materialize only n rows from the arrow dataset\\n    batch_size\\n        The maximum row count for scanned pyarrow record batches.\\n\\n    Returns\\n    -------\\n    DataFrame\\n\\n    '\n    from polars import from_arrow\n    _filter = None\n    if predicate:\n        from polars.datatypes import Date, Datetime, Duration\n        from polars.utils.convert import _to_python_date, _to_python_datetime, _to_python_time, _to_python_timedelta\n        _filter = eval(predicate, {'pa': pa, 'Date': Date, 'Datetime': Datetime, 'Duration': Duration, '_to_python_date': _to_python_date, '_to_python_datetime': _to_python_datetime, '_to_python_time': _to_python_time, '_to_python_timedelta': _to_python_timedelta})\n    common_params = {'columns': with_columns, 'filter': _filter}\n    if batch_size is not None:\n        common_params['batch_size'] = batch_size\n    if n_rows:\n        return from_arrow(ds.head(n_rows, **common_params))\n    return from_arrow(ds.to_table(**common_params))"
        ]
    }
]