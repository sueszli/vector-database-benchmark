[
    {
        "func_name": "generate_rnn",
        "original": "def generate_rnn(rng, N, g, tau, dt, max_firing_rate):\n    \"\"\"Create a (vanilla) RNN with a bunch of hyper parameters for generating\nchaotic data.\n  Args:\n    rng: numpy random number generator\n    N: number of hidden units\n    g: scaling of recurrent weight matrix in g W, with W ~ N(0,1/N)\n    tau: time scale of individual unit dynamics\n    dt: time step for equation updates\n    max_firing_rate: how to resecale the -1,1 firing rates\n  Returns:\n    the dictionary of these parameters, plus some others.\n\"\"\"\n    rnn = {}\n    rnn['N'] = N\n    rnn['W'] = rng.randn(N, N) / np.sqrt(N)\n    rnn['Bin'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['Bin2'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['b'] = np.zeros(N)\n    rnn['g'] = g\n    rnn['tau'] = tau\n    rnn['dt'] = dt\n    rnn['max_firing_rate'] = max_firing_rate\n    mfr = rnn['max_firing_rate']\n    nbins_per_sec = 1.0 / rnn['dt']\n    rnn['conversion_factor'] = mfr / nbins_per_sec\n    return rnn",
        "mutated": [
            "def generate_rnn(rng, N, g, tau, dt, max_firing_rate):\n    if False:\n        i = 10\n    'Create a (vanilla) RNN with a bunch of hyper parameters for generating\\nchaotic data.\\n  Args:\\n    rng: numpy random number generator\\n    N: number of hidden units\\n    g: scaling of recurrent weight matrix in g W, with W ~ N(0,1/N)\\n    tau: time scale of individual unit dynamics\\n    dt: time step for equation updates\\n    max_firing_rate: how to resecale the -1,1 firing rates\\n  Returns:\\n    the dictionary of these parameters, plus some others.\\n'\n    rnn = {}\n    rnn['N'] = N\n    rnn['W'] = rng.randn(N, N) / np.sqrt(N)\n    rnn['Bin'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['Bin2'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['b'] = np.zeros(N)\n    rnn['g'] = g\n    rnn['tau'] = tau\n    rnn['dt'] = dt\n    rnn['max_firing_rate'] = max_firing_rate\n    mfr = rnn['max_firing_rate']\n    nbins_per_sec = 1.0 / rnn['dt']\n    rnn['conversion_factor'] = mfr / nbins_per_sec\n    return rnn",
            "def generate_rnn(rng, N, g, tau, dt, max_firing_rate):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a (vanilla) RNN with a bunch of hyper parameters for generating\\nchaotic data.\\n  Args:\\n    rng: numpy random number generator\\n    N: number of hidden units\\n    g: scaling of recurrent weight matrix in g W, with W ~ N(0,1/N)\\n    tau: time scale of individual unit dynamics\\n    dt: time step for equation updates\\n    max_firing_rate: how to resecale the -1,1 firing rates\\n  Returns:\\n    the dictionary of these parameters, plus some others.\\n'\n    rnn = {}\n    rnn['N'] = N\n    rnn['W'] = rng.randn(N, N) / np.sqrt(N)\n    rnn['Bin'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['Bin2'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['b'] = np.zeros(N)\n    rnn['g'] = g\n    rnn['tau'] = tau\n    rnn['dt'] = dt\n    rnn['max_firing_rate'] = max_firing_rate\n    mfr = rnn['max_firing_rate']\n    nbins_per_sec = 1.0 / rnn['dt']\n    rnn['conversion_factor'] = mfr / nbins_per_sec\n    return rnn",
            "def generate_rnn(rng, N, g, tau, dt, max_firing_rate):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a (vanilla) RNN with a bunch of hyper parameters for generating\\nchaotic data.\\n  Args:\\n    rng: numpy random number generator\\n    N: number of hidden units\\n    g: scaling of recurrent weight matrix in g W, with W ~ N(0,1/N)\\n    tau: time scale of individual unit dynamics\\n    dt: time step for equation updates\\n    max_firing_rate: how to resecale the -1,1 firing rates\\n  Returns:\\n    the dictionary of these parameters, plus some others.\\n'\n    rnn = {}\n    rnn['N'] = N\n    rnn['W'] = rng.randn(N, N) / np.sqrt(N)\n    rnn['Bin'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['Bin2'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['b'] = np.zeros(N)\n    rnn['g'] = g\n    rnn['tau'] = tau\n    rnn['dt'] = dt\n    rnn['max_firing_rate'] = max_firing_rate\n    mfr = rnn['max_firing_rate']\n    nbins_per_sec = 1.0 / rnn['dt']\n    rnn['conversion_factor'] = mfr / nbins_per_sec\n    return rnn",
            "def generate_rnn(rng, N, g, tau, dt, max_firing_rate):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a (vanilla) RNN with a bunch of hyper parameters for generating\\nchaotic data.\\n  Args:\\n    rng: numpy random number generator\\n    N: number of hidden units\\n    g: scaling of recurrent weight matrix in g W, with W ~ N(0,1/N)\\n    tau: time scale of individual unit dynamics\\n    dt: time step for equation updates\\n    max_firing_rate: how to resecale the -1,1 firing rates\\n  Returns:\\n    the dictionary of these parameters, plus some others.\\n'\n    rnn = {}\n    rnn['N'] = N\n    rnn['W'] = rng.randn(N, N) / np.sqrt(N)\n    rnn['Bin'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['Bin2'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['b'] = np.zeros(N)\n    rnn['g'] = g\n    rnn['tau'] = tau\n    rnn['dt'] = dt\n    rnn['max_firing_rate'] = max_firing_rate\n    mfr = rnn['max_firing_rate']\n    nbins_per_sec = 1.0 / rnn['dt']\n    rnn['conversion_factor'] = mfr / nbins_per_sec\n    return rnn",
            "def generate_rnn(rng, N, g, tau, dt, max_firing_rate):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a (vanilla) RNN with a bunch of hyper parameters for generating\\nchaotic data.\\n  Args:\\n    rng: numpy random number generator\\n    N: number of hidden units\\n    g: scaling of recurrent weight matrix in g W, with W ~ N(0,1/N)\\n    tau: time scale of individual unit dynamics\\n    dt: time step for equation updates\\n    max_firing_rate: how to resecale the -1,1 firing rates\\n  Returns:\\n    the dictionary of these parameters, plus some others.\\n'\n    rnn = {}\n    rnn['N'] = N\n    rnn['W'] = rng.randn(N, N) / np.sqrt(N)\n    rnn['Bin'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['Bin2'] = rng.randn(N) / np.sqrt(1.0)\n    rnn['b'] = np.zeros(N)\n    rnn['g'] = g\n    rnn['tau'] = tau\n    rnn['dt'] = dt\n    rnn['max_firing_rate'] = max_firing_rate\n    mfr = rnn['max_firing_rate']\n    nbins_per_sec = 1.0 / rnn['dt']\n    rnn['conversion_factor'] = mfr / nbins_per_sec\n    return rnn"
        ]
    },
    {
        "func_name": "run_rnn",
        "original": "def run_rnn(rnn, x0, ntime_steps, input_time=None):\n    rs = np.zeros([N, ntime_steps])\n    x_tm1 = x0\n    r_tm1 = np.tanh(x0)\n    tau = rnn['tau']\n    dt = rnn['dt']\n    alpha = 1.0 - dt / tau\n    W = dt / tau * rnn['W'] * rnn['g']\n    Bin = dt / tau * rnn['Bin']\n    Bin2 = dt / tau * rnn['Bin2']\n    b = dt / tau * rnn['b']\n    us = np.zeros([1, ntime_steps])\n    for t in range(ntime_steps):\n        x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n        if input_time is not None and t == input_time:\n            us[0, t] = input_magnitude\n            x_t += Bin * us[0, t]\n        r_t = np.tanh(x_t)\n        x_tm1 = x_t\n        r_tm1 = r_t\n        rs[:, t] = r_t\n    return (rs, us)",
        "mutated": [
            "def run_rnn(rnn, x0, ntime_steps, input_time=None):\n    if False:\n        i = 10\n    rs = np.zeros([N, ntime_steps])\n    x_tm1 = x0\n    r_tm1 = np.tanh(x0)\n    tau = rnn['tau']\n    dt = rnn['dt']\n    alpha = 1.0 - dt / tau\n    W = dt / tau * rnn['W'] * rnn['g']\n    Bin = dt / tau * rnn['Bin']\n    Bin2 = dt / tau * rnn['Bin2']\n    b = dt / tau * rnn['b']\n    us = np.zeros([1, ntime_steps])\n    for t in range(ntime_steps):\n        x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n        if input_time is not None and t == input_time:\n            us[0, t] = input_magnitude\n            x_t += Bin * us[0, t]\n        r_t = np.tanh(x_t)\n        x_tm1 = x_t\n        r_tm1 = r_t\n        rs[:, t] = r_t\n    return (rs, us)",
            "def run_rnn(rnn, x0, ntime_steps, input_time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    rs = np.zeros([N, ntime_steps])\n    x_tm1 = x0\n    r_tm1 = np.tanh(x0)\n    tau = rnn['tau']\n    dt = rnn['dt']\n    alpha = 1.0 - dt / tau\n    W = dt / tau * rnn['W'] * rnn['g']\n    Bin = dt / tau * rnn['Bin']\n    Bin2 = dt / tau * rnn['Bin2']\n    b = dt / tau * rnn['b']\n    us = np.zeros([1, ntime_steps])\n    for t in range(ntime_steps):\n        x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n        if input_time is not None and t == input_time:\n            us[0, t] = input_magnitude\n            x_t += Bin * us[0, t]\n        r_t = np.tanh(x_t)\n        x_tm1 = x_t\n        r_tm1 = r_t\n        rs[:, t] = r_t\n    return (rs, us)",
            "def run_rnn(rnn, x0, ntime_steps, input_time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    rs = np.zeros([N, ntime_steps])\n    x_tm1 = x0\n    r_tm1 = np.tanh(x0)\n    tau = rnn['tau']\n    dt = rnn['dt']\n    alpha = 1.0 - dt / tau\n    W = dt / tau * rnn['W'] * rnn['g']\n    Bin = dt / tau * rnn['Bin']\n    Bin2 = dt / tau * rnn['Bin2']\n    b = dt / tau * rnn['b']\n    us = np.zeros([1, ntime_steps])\n    for t in range(ntime_steps):\n        x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n        if input_time is not None and t == input_time:\n            us[0, t] = input_magnitude\n            x_t += Bin * us[0, t]\n        r_t = np.tanh(x_t)\n        x_tm1 = x_t\n        r_tm1 = r_t\n        rs[:, t] = r_t\n    return (rs, us)",
            "def run_rnn(rnn, x0, ntime_steps, input_time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    rs = np.zeros([N, ntime_steps])\n    x_tm1 = x0\n    r_tm1 = np.tanh(x0)\n    tau = rnn['tau']\n    dt = rnn['dt']\n    alpha = 1.0 - dt / tau\n    W = dt / tau * rnn['W'] * rnn['g']\n    Bin = dt / tau * rnn['Bin']\n    Bin2 = dt / tau * rnn['Bin2']\n    b = dt / tau * rnn['b']\n    us = np.zeros([1, ntime_steps])\n    for t in range(ntime_steps):\n        x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n        if input_time is not None and t == input_time:\n            us[0, t] = input_magnitude\n            x_t += Bin * us[0, t]\n        r_t = np.tanh(x_t)\n        x_tm1 = x_t\n        r_tm1 = r_t\n        rs[:, t] = r_t\n    return (rs, us)",
            "def run_rnn(rnn, x0, ntime_steps, input_time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    rs = np.zeros([N, ntime_steps])\n    x_tm1 = x0\n    r_tm1 = np.tanh(x0)\n    tau = rnn['tau']\n    dt = rnn['dt']\n    alpha = 1.0 - dt / tau\n    W = dt / tau * rnn['W'] * rnn['g']\n    Bin = dt / tau * rnn['Bin']\n    Bin2 = dt / tau * rnn['Bin2']\n    b = dt / tau * rnn['b']\n    us = np.zeros([1, ntime_steps])\n    for t in range(ntime_steps):\n        x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n        if input_time is not None and t == input_time:\n            us[0, t] = input_magnitude\n            x_t += Bin * us[0, t]\n        r_t = np.tanh(x_t)\n        x_tm1 = x_t\n        r_tm1 = r_t\n        rs[:, t] = r_t\n    return (rs, us)"
        ]
    },
    {
        "func_name": "generate_data",
        "original": "def generate_data(rnn, T, E, x0s=None, P_sxn=None, input_magnitude=0.0, input_times=None):\n    \"\"\" Generates data from an randomly initialized RNN.\n  Args:\n    rnn: the rnn\n    T: Time in seconds to run (divided by rnn['dt'] to get steps, rounded down.\n    E: total number of examples\n    S: number of samples (subsampling N)\n  Returns:\n    A list of length E of NxT tensors of the network being run.\n  \"\"\"\n    N = rnn['N']\n\n    def run_rnn(rnn, x0, ntime_steps, input_time=None):\n        rs = np.zeros([N, ntime_steps])\n        x_tm1 = x0\n        r_tm1 = np.tanh(x0)\n        tau = rnn['tau']\n        dt = rnn['dt']\n        alpha = 1.0 - dt / tau\n        W = dt / tau * rnn['W'] * rnn['g']\n        Bin = dt / tau * rnn['Bin']\n        Bin2 = dt / tau * rnn['Bin2']\n        b = dt / tau * rnn['b']\n        us = np.zeros([1, ntime_steps])\n        for t in range(ntime_steps):\n            x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n            if input_time is not None and t == input_time:\n                us[0, t] = input_magnitude\n                x_t += Bin * us[0, t]\n            r_t = np.tanh(x_t)\n            x_tm1 = x_t\n            r_tm1 = r_t\n            rs[:, t] = r_t\n        return (rs, us)\n    if P_sxn is None:\n        P_sxn = np.eye(N)\n    ntime_steps = int(T / rnn['dt'])\n    data_e = []\n    inputs_e = []\n    for e in range(E):\n        input_time = input_times[e] if input_times is not None else None\n        (r_nxt, u_uxt) = run_rnn(rnn, x0s[:, e], ntime_steps, input_time)\n        r_sxt = np.dot(P_sxn, r_nxt)\n        inputs_e.append(u_uxt)\n        data_e.append(r_sxt)\n    S = P_sxn.shape[0]\n    data_e = normalize_rates(data_e, E, S)\n    return (data_e, x0s, inputs_e)",
        "mutated": [
            "def generate_data(rnn, T, E, x0s=None, P_sxn=None, input_magnitude=0.0, input_times=None):\n    if False:\n        i = 10\n    \" Generates data from an randomly initialized RNN.\\n  Args:\\n    rnn: the rnn\\n    T: Time in seconds to run (divided by rnn['dt'] to get steps, rounded down.\\n    E: total number of examples\\n    S: number of samples (subsampling N)\\n  Returns:\\n    A list of length E of NxT tensors of the network being run.\\n  \"\n    N = rnn['N']\n\n    def run_rnn(rnn, x0, ntime_steps, input_time=None):\n        rs = np.zeros([N, ntime_steps])\n        x_tm1 = x0\n        r_tm1 = np.tanh(x0)\n        tau = rnn['tau']\n        dt = rnn['dt']\n        alpha = 1.0 - dt / tau\n        W = dt / tau * rnn['W'] * rnn['g']\n        Bin = dt / tau * rnn['Bin']\n        Bin2 = dt / tau * rnn['Bin2']\n        b = dt / tau * rnn['b']\n        us = np.zeros([1, ntime_steps])\n        for t in range(ntime_steps):\n            x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n            if input_time is not None and t == input_time:\n                us[0, t] = input_magnitude\n                x_t += Bin * us[0, t]\n            r_t = np.tanh(x_t)\n            x_tm1 = x_t\n            r_tm1 = r_t\n            rs[:, t] = r_t\n        return (rs, us)\n    if P_sxn is None:\n        P_sxn = np.eye(N)\n    ntime_steps = int(T / rnn['dt'])\n    data_e = []\n    inputs_e = []\n    for e in range(E):\n        input_time = input_times[e] if input_times is not None else None\n        (r_nxt, u_uxt) = run_rnn(rnn, x0s[:, e], ntime_steps, input_time)\n        r_sxt = np.dot(P_sxn, r_nxt)\n        inputs_e.append(u_uxt)\n        data_e.append(r_sxt)\n    S = P_sxn.shape[0]\n    data_e = normalize_rates(data_e, E, S)\n    return (data_e, x0s, inputs_e)",
            "def generate_data(rnn, T, E, x0s=None, P_sxn=None, input_magnitude=0.0, input_times=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \" Generates data from an randomly initialized RNN.\\n  Args:\\n    rnn: the rnn\\n    T: Time in seconds to run (divided by rnn['dt'] to get steps, rounded down.\\n    E: total number of examples\\n    S: number of samples (subsampling N)\\n  Returns:\\n    A list of length E of NxT tensors of the network being run.\\n  \"\n    N = rnn['N']\n\n    def run_rnn(rnn, x0, ntime_steps, input_time=None):\n        rs = np.zeros([N, ntime_steps])\n        x_tm1 = x0\n        r_tm1 = np.tanh(x0)\n        tau = rnn['tau']\n        dt = rnn['dt']\n        alpha = 1.0 - dt / tau\n        W = dt / tau * rnn['W'] * rnn['g']\n        Bin = dt / tau * rnn['Bin']\n        Bin2 = dt / tau * rnn['Bin2']\n        b = dt / tau * rnn['b']\n        us = np.zeros([1, ntime_steps])\n        for t in range(ntime_steps):\n            x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n            if input_time is not None and t == input_time:\n                us[0, t] = input_magnitude\n                x_t += Bin * us[0, t]\n            r_t = np.tanh(x_t)\n            x_tm1 = x_t\n            r_tm1 = r_t\n            rs[:, t] = r_t\n        return (rs, us)\n    if P_sxn is None:\n        P_sxn = np.eye(N)\n    ntime_steps = int(T / rnn['dt'])\n    data_e = []\n    inputs_e = []\n    for e in range(E):\n        input_time = input_times[e] if input_times is not None else None\n        (r_nxt, u_uxt) = run_rnn(rnn, x0s[:, e], ntime_steps, input_time)\n        r_sxt = np.dot(P_sxn, r_nxt)\n        inputs_e.append(u_uxt)\n        data_e.append(r_sxt)\n    S = P_sxn.shape[0]\n    data_e = normalize_rates(data_e, E, S)\n    return (data_e, x0s, inputs_e)",
            "def generate_data(rnn, T, E, x0s=None, P_sxn=None, input_magnitude=0.0, input_times=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \" Generates data from an randomly initialized RNN.\\n  Args:\\n    rnn: the rnn\\n    T: Time in seconds to run (divided by rnn['dt'] to get steps, rounded down.\\n    E: total number of examples\\n    S: number of samples (subsampling N)\\n  Returns:\\n    A list of length E of NxT tensors of the network being run.\\n  \"\n    N = rnn['N']\n\n    def run_rnn(rnn, x0, ntime_steps, input_time=None):\n        rs = np.zeros([N, ntime_steps])\n        x_tm1 = x0\n        r_tm1 = np.tanh(x0)\n        tau = rnn['tau']\n        dt = rnn['dt']\n        alpha = 1.0 - dt / tau\n        W = dt / tau * rnn['W'] * rnn['g']\n        Bin = dt / tau * rnn['Bin']\n        Bin2 = dt / tau * rnn['Bin2']\n        b = dt / tau * rnn['b']\n        us = np.zeros([1, ntime_steps])\n        for t in range(ntime_steps):\n            x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n            if input_time is not None and t == input_time:\n                us[0, t] = input_magnitude\n                x_t += Bin * us[0, t]\n            r_t = np.tanh(x_t)\n            x_tm1 = x_t\n            r_tm1 = r_t\n            rs[:, t] = r_t\n        return (rs, us)\n    if P_sxn is None:\n        P_sxn = np.eye(N)\n    ntime_steps = int(T / rnn['dt'])\n    data_e = []\n    inputs_e = []\n    for e in range(E):\n        input_time = input_times[e] if input_times is not None else None\n        (r_nxt, u_uxt) = run_rnn(rnn, x0s[:, e], ntime_steps, input_time)\n        r_sxt = np.dot(P_sxn, r_nxt)\n        inputs_e.append(u_uxt)\n        data_e.append(r_sxt)\n    S = P_sxn.shape[0]\n    data_e = normalize_rates(data_e, E, S)\n    return (data_e, x0s, inputs_e)",
            "def generate_data(rnn, T, E, x0s=None, P_sxn=None, input_magnitude=0.0, input_times=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \" Generates data from an randomly initialized RNN.\\n  Args:\\n    rnn: the rnn\\n    T: Time in seconds to run (divided by rnn['dt'] to get steps, rounded down.\\n    E: total number of examples\\n    S: number of samples (subsampling N)\\n  Returns:\\n    A list of length E of NxT tensors of the network being run.\\n  \"\n    N = rnn['N']\n\n    def run_rnn(rnn, x0, ntime_steps, input_time=None):\n        rs = np.zeros([N, ntime_steps])\n        x_tm1 = x0\n        r_tm1 = np.tanh(x0)\n        tau = rnn['tau']\n        dt = rnn['dt']\n        alpha = 1.0 - dt / tau\n        W = dt / tau * rnn['W'] * rnn['g']\n        Bin = dt / tau * rnn['Bin']\n        Bin2 = dt / tau * rnn['Bin2']\n        b = dt / tau * rnn['b']\n        us = np.zeros([1, ntime_steps])\n        for t in range(ntime_steps):\n            x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n            if input_time is not None and t == input_time:\n                us[0, t] = input_magnitude\n                x_t += Bin * us[0, t]\n            r_t = np.tanh(x_t)\n            x_tm1 = x_t\n            r_tm1 = r_t\n            rs[:, t] = r_t\n        return (rs, us)\n    if P_sxn is None:\n        P_sxn = np.eye(N)\n    ntime_steps = int(T / rnn['dt'])\n    data_e = []\n    inputs_e = []\n    for e in range(E):\n        input_time = input_times[e] if input_times is not None else None\n        (r_nxt, u_uxt) = run_rnn(rnn, x0s[:, e], ntime_steps, input_time)\n        r_sxt = np.dot(P_sxn, r_nxt)\n        inputs_e.append(u_uxt)\n        data_e.append(r_sxt)\n    S = P_sxn.shape[0]\n    data_e = normalize_rates(data_e, E, S)\n    return (data_e, x0s, inputs_e)",
            "def generate_data(rnn, T, E, x0s=None, P_sxn=None, input_magnitude=0.0, input_times=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \" Generates data from an randomly initialized RNN.\\n  Args:\\n    rnn: the rnn\\n    T: Time in seconds to run (divided by rnn['dt'] to get steps, rounded down.\\n    E: total number of examples\\n    S: number of samples (subsampling N)\\n  Returns:\\n    A list of length E of NxT tensors of the network being run.\\n  \"\n    N = rnn['N']\n\n    def run_rnn(rnn, x0, ntime_steps, input_time=None):\n        rs = np.zeros([N, ntime_steps])\n        x_tm1 = x0\n        r_tm1 = np.tanh(x0)\n        tau = rnn['tau']\n        dt = rnn['dt']\n        alpha = 1.0 - dt / tau\n        W = dt / tau * rnn['W'] * rnn['g']\n        Bin = dt / tau * rnn['Bin']\n        Bin2 = dt / tau * rnn['Bin2']\n        b = dt / tau * rnn['b']\n        us = np.zeros([1, ntime_steps])\n        for t in range(ntime_steps):\n            x_t = alpha * x_tm1 + np.dot(W, r_tm1) + b\n            if input_time is not None and t == input_time:\n                us[0, t] = input_magnitude\n                x_t += Bin * us[0, t]\n            r_t = np.tanh(x_t)\n            x_tm1 = x_t\n            r_tm1 = r_t\n            rs[:, t] = r_t\n        return (rs, us)\n    if P_sxn is None:\n        P_sxn = np.eye(N)\n    ntime_steps = int(T / rnn['dt'])\n    data_e = []\n    inputs_e = []\n    for e in range(E):\n        input_time = input_times[e] if input_times is not None else None\n        (r_nxt, u_uxt) = run_rnn(rnn, x0s[:, e], ntime_steps, input_time)\n        r_sxt = np.dot(P_sxn, r_nxt)\n        inputs_e.append(u_uxt)\n        data_e.append(r_sxt)\n    S = P_sxn.shape[0]\n    data_e = normalize_rates(data_e, E, S)\n    return (data_e, x0s, inputs_e)"
        ]
    },
    {
        "func_name": "normalize_rates",
        "original": "def normalize_rates(data_e, E, S):\n    for e in range(E):\n        r_sxt = data_e[e]\n        for i in range(S):\n            rmin = np.min(r_sxt[i, :])\n            rmax = np.max(r_sxt[i, :])\n            assert rmax - rmin != 0, 'Something wrong'\n            r_sxt[i, :] = (r_sxt[i, :] - rmin) / (rmax - rmin)\n        data_e[e] = r_sxt\n    return data_e",
        "mutated": [
            "def normalize_rates(data_e, E, S):\n    if False:\n        i = 10\n    for e in range(E):\n        r_sxt = data_e[e]\n        for i in range(S):\n            rmin = np.min(r_sxt[i, :])\n            rmax = np.max(r_sxt[i, :])\n            assert rmax - rmin != 0, 'Something wrong'\n            r_sxt[i, :] = (r_sxt[i, :] - rmin) / (rmax - rmin)\n        data_e[e] = r_sxt\n    return data_e",
            "def normalize_rates(data_e, E, S):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for e in range(E):\n        r_sxt = data_e[e]\n        for i in range(S):\n            rmin = np.min(r_sxt[i, :])\n            rmax = np.max(r_sxt[i, :])\n            assert rmax - rmin != 0, 'Something wrong'\n            r_sxt[i, :] = (r_sxt[i, :] - rmin) / (rmax - rmin)\n        data_e[e] = r_sxt\n    return data_e",
            "def normalize_rates(data_e, E, S):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for e in range(E):\n        r_sxt = data_e[e]\n        for i in range(S):\n            rmin = np.min(r_sxt[i, :])\n            rmax = np.max(r_sxt[i, :])\n            assert rmax - rmin != 0, 'Something wrong'\n            r_sxt[i, :] = (r_sxt[i, :] - rmin) / (rmax - rmin)\n        data_e[e] = r_sxt\n    return data_e",
            "def normalize_rates(data_e, E, S):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for e in range(E):\n        r_sxt = data_e[e]\n        for i in range(S):\n            rmin = np.min(r_sxt[i, :])\n            rmax = np.max(r_sxt[i, :])\n            assert rmax - rmin != 0, 'Something wrong'\n            r_sxt[i, :] = (r_sxt[i, :] - rmin) / (rmax - rmin)\n        data_e[e] = r_sxt\n    return data_e",
            "def normalize_rates(data_e, E, S):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for e in range(E):\n        r_sxt = data_e[e]\n        for i in range(S):\n            rmin = np.min(r_sxt[i, :])\n            rmax = np.max(r_sxt[i, :])\n            assert rmax - rmin != 0, 'Something wrong'\n            r_sxt[i, :] = (r_sxt[i, :] - rmin) / (rmax - rmin)\n        data_e[e] = r_sxt\n    return data_e"
        ]
    },
    {
        "func_name": "spikify_data",
        "original": "def spikify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    \"\"\" Apply spikes to a continuous dataset whose values are between 0.0 and 1.0\n  Args:\n    data_e: nexamples length list of NxT trials\n    dt: how often the data are sampled\n    max_firing_rate: the firing rate that is associated with a value of 1.0\n  Returns:\n    spikified_e: a list of length b of the data represented as spikes,\n    sampled from the underlying poisson process.\n    \"\"\"\n    E = len(data_e)\n    spikes_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        data_s = np.zeros([N, T]).astype(np.int)\n        for n in range(N):\n            f = data[n, :]\n            s = rng.poisson(f * max_firing_rate * dt, size=T)\n            data_s[n, :] = s\n        spikes_e.append(data_s)\n    return spikes_e",
        "mutated": [
            "def spikify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    if False:\n        i = 10\n    ' Apply spikes to a continuous dataset whose values are between 0.0 and 1.0\\n  Args:\\n    data_e: nexamples length list of NxT trials\\n    dt: how often the data are sampled\\n    max_firing_rate: the firing rate that is associated with a value of 1.0\\n  Returns:\\n    spikified_e: a list of length b of the data represented as spikes,\\n    sampled from the underlying poisson process.\\n    '\n    E = len(data_e)\n    spikes_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        data_s = np.zeros([N, T]).astype(np.int)\n        for n in range(N):\n            f = data[n, :]\n            s = rng.poisson(f * max_firing_rate * dt, size=T)\n            data_s[n, :] = s\n        spikes_e.append(data_s)\n    return spikes_e",
            "def spikify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Apply spikes to a continuous dataset whose values are between 0.0 and 1.0\\n  Args:\\n    data_e: nexamples length list of NxT trials\\n    dt: how often the data are sampled\\n    max_firing_rate: the firing rate that is associated with a value of 1.0\\n  Returns:\\n    spikified_e: a list of length b of the data represented as spikes,\\n    sampled from the underlying poisson process.\\n    '\n    E = len(data_e)\n    spikes_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        data_s = np.zeros([N, T]).astype(np.int)\n        for n in range(N):\n            f = data[n, :]\n            s = rng.poisson(f * max_firing_rate * dt, size=T)\n            data_s[n, :] = s\n        spikes_e.append(data_s)\n    return spikes_e",
            "def spikify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Apply spikes to a continuous dataset whose values are between 0.0 and 1.0\\n  Args:\\n    data_e: nexamples length list of NxT trials\\n    dt: how often the data are sampled\\n    max_firing_rate: the firing rate that is associated with a value of 1.0\\n  Returns:\\n    spikified_e: a list of length b of the data represented as spikes,\\n    sampled from the underlying poisson process.\\n    '\n    E = len(data_e)\n    spikes_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        data_s = np.zeros([N, T]).astype(np.int)\n        for n in range(N):\n            f = data[n, :]\n            s = rng.poisson(f * max_firing_rate * dt, size=T)\n            data_s[n, :] = s\n        spikes_e.append(data_s)\n    return spikes_e",
            "def spikify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Apply spikes to a continuous dataset whose values are between 0.0 and 1.0\\n  Args:\\n    data_e: nexamples length list of NxT trials\\n    dt: how often the data are sampled\\n    max_firing_rate: the firing rate that is associated with a value of 1.0\\n  Returns:\\n    spikified_e: a list of length b of the data represented as spikes,\\n    sampled from the underlying poisson process.\\n    '\n    E = len(data_e)\n    spikes_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        data_s = np.zeros([N, T]).astype(np.int)\n        for n in range(N):\n            f = data[n, :]\n            s = rng.poisson(f * max_firing_rate * dt, size=T)\n            data_s[n, :] = s\n        spikes_e.append(data_s)\n    return spikes_e",
            "def spikify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Apply spikes to a continuous dataset whose values are between 0.0 and 1.0\\n  Args:\\n    data_e: nexamples length list of NxT trials\\n    dt: how often the data are sampled\\n    max_firing_rate: the firing rate that is associated with a value of 1.0\\n  Returns:\\n    spikified_e: a list of length b of the data represented as spikes,\\n    sampled from the underlying poisson process.\\n    '\n    E = len(data_e)\n    spikes_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        data_s = np.zeros([N, T]).astype(np.int)\n        for n in range(N):\n            f = data[n, :]\n            s = rng.poisson(f * max_firing_rate * dt, size=T)\n            data_s[n, :] = s\n        spikes_e.append(data_s)\n    return spikes_e"
        ]
    },
    {
        "func_name": "gaussify_data",
        "original": "def gaussify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    \"\"\" Apply gaussian noise to a continuous dataset whose values are between\n  0.0 and 1.0\n\n  Args:\n    data_e: nexamples length list of NxT trials\n    dt: how often the data are sampled\n    max_firing_rate: the firing rate that is associated with a value of 1.0\n  Returns:\n    gauss_e: a list of length b of the data with noise.\n    \"\"\"\n    E = len(data_e)\n    mfr = max_firing_rate\n    gauss_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        noisy_data = data * mfr + np.random.randn(N, T) * (5.0 * mfr) * np.sqrt(dt)\n        gauss_e.append(noisy_data)\n    return gauss_e",
        "mutated": [
            "def gaussify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    if False:\n        i = 10\n    ' Apply gaussian noise to a continuous dataset whose values are between\\n  0.0 and 1.0\\n\\n  Args:\\n    data_e: nexamples length list of NxT trials\\n    dt: how often the data are sampled\\n    max_firing_rate: the firing rate that is associated with a value of 1.0\\n  Returns:\\n    gauss_e: a list of length b of the data with noise.\\n    '\n    E = len(data_e)\n    mfr = max_firing_rate\n    gauss_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        noisy_data = data * mfr + np.random.randn(N, T) * (5.0 * mfr) * np.sqrt(dt)\n        gauss_e.append(noisy_data)\n    return gauss_e",
            "def gaussify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Apply gaussian noise to a continuous dataset whose values are between\\n  0.0 and 1.0\\n\\n  Args:\\n    data_e: nexamples length list of NxT trials\\n    dt: how often the data are sampled\\n    max_firing_rate: the firing rate that is associated with a value of 1.0\\n  Returns:\\n    gauss_e: a list of length b of the data with noise.\\n    '\n    E = len(data_e)\n    mfr = max_firing_rate\n    gauss_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        noisy_data = data * mfr + np.random.randn(N, T) * (5.0 * mfr) * np.sqrt(dt)\n        gauss_e.append(noisy_data)\n    return gauss_e",
            "def gaussify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Apply gaussian noise to a continuous dataset whose values are between\\n  0.0 and 1.0\\n\\n  Args:\\n    data_e: nexamples length list of NxT trials\\n    dt: how often the data are sampled\\n    max_firing_rate: the firing rate that is associated with a value of 1.0\\n  Returns:\\n    gauss_e: a list of length b of the data with noise.\\n    '\n    E = len(data_e)\n    mfr = max_firing_rate\n    gauss_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        noisy_data = data * mfr + np.random.randn(N, T) * (5.0 * mfr) * np.sqrt(dt)\n        gauss_e.append(noisy_data)\n    return gauss_e",
            "def gaussify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Apply gaussian noise to a continuous dataset whose values are between\\n  0.0 and 1.0\\n\\n  Args:\\n    data_e: nexamples length list of NxT trials\\n    dt: how often the data are sampled\\n    max_firing_rate: the firing rate that is associated with a value of 1.0\\n  Returns:\\n    gauss_e: a list of length b of the data with noise.\\n    '\n    E = len(data_e)\n    mfr = max_firing_rate\n    gauss_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        noisy_data = data * mfr + np.random.randn(N, T) * (5.0 * mfr) * np.sqrt(dt)\n        gauss_e.append(noisy_data)\n    return gauss_e",
            "def gaussify_data(data_e, rng, dt=1.0, max_firing_rate=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Apply gaussian noise to a continuous dataset whose values are between\\n  0.0 and 1.0\\n\\n  Args:\\n    data_e: nexamples length list of NxT trials\\n    dt: how often the data are sampled\\n    max_firing_rate: the firing rate that is associated with a value of 1.0\\n  Returns:\\n    gauss_e: a list of length b of the data with noise.\\n    '\n    E = len(data_e)\n    mfr = max_firing_rate\n    gauss_e = []\n    for e in range(E):\n        data = data_e[e]\n        (N, T) = data.shape\n        noisy_data = data * mfr + np.random.randn(N, T) * (5.0 * mfr) * np.sqrt(dt)\n        gauss_e.append(noisy_data)\n    return gauss_e"
        ]
    },
    {
        "func_name": "get_train_n_valid_inds",
        "original": "def get_train_n_valid_inds(num_trials, train_fraction, nreplications):\n    \"\"\"Split the numbers between 0 and num_trials-1 into two portions for\n  training and validation, based on the train fraction.\n  Args:\n    num_trials: the number of trials\n    train_fraction: (e.g. .80)\n    nreplications: the number of spiking trials per initial condition\n  Returns:\n    a 2-tuple of two lists: the training indices and validation indices\n    \"\"\"\n    train_inds = []\n    valid_inds = []\n    for i in range(num_trials):\n        if i % nreplications + 1 > train_fraction * nreplications:\n            valid_inds.append(i)\n        else:\n            train_inds.append(i)\n    return (train_inds, valid_inds)",
        "mutated": [
            "def get_train_n_valid_inds(num_trials, train_fraction, nreplications):\n    if False:\n        i = 10\n    'Split the numbers between 0 and num_trials-1 into two portions for\\n  training and validation, based on the train fraction.\\n  Args:\\n    num_trials: the number of trials\\n    train_fraction: (e.g. .80)\\n    nreplications: the number of spiking trials per initial condition\\n  Returns:\\n    a 2-tuple of two lists: the training indices and validation indices\\n    '\n    train_inds = []\n    valid_inds = []\n    for i in range(num_trials):\n        if i % nreplications + 1 > train_fraction * nreplications:\n            valid_inds.append(i)\n        else:\n            train_inds.append(i)\n    return (train_inds, valid_inds)",
            "def get_train_n_valid_inds(num_trials, train_fraction, nreplications):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Split the numbers between 0 and num_trials-1 into two portions for\\n  training and validation, based on the train fraction.\\n  Args:\\n    num_trials: the number of trials\\n    train_fraction: (e.g. .80)\\n    nreplications: the number of spiking trials per initial condition\\n  Returns:\\n    a 2-tuple of two lists: the training indices and validation indices\\n    '\n    train_inds = []\n    valid_inds = []\n    for i in range(num_trials):\n        if i % nreplications + 1 > train_fraction * nreplications:\n            valid_inds.append(i)\n        else:\n            train_inds.append(i)\n    return (train_inds, valid_inds)",
            "def get_train_n_valid_inds(num_trials, train_fraction, nreplications):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Split the numbers between 0 and num_trials-1 into two portions for\\n  training and validation, based on the train fraction.\\n  Args:\\n    num_trials: the number of trials\\n    train_fraction: (e.g. .80)\\n    nreplications: the number of spiking trials per initial condition\\n  Returns:\\n    a 2-tuple of two lists: the training indices and validation indices\\n    '\n    train_inds = []\n    valid_inds = []\n    for i in range(num_trials):\n        if i % nreplications + 1 > train_fraction * nreplications:\n            valid_inds.append(i)\n        else:\n            train_inds.append(i)\n    return (train_inds, valid_inds)",
            "def get_train_n_valid_inds(num_trials, train_fraction, nreplications):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Split the numbers between 0 and num_trials-1 into two portions for\\n  training and validation, based on the train fraction.\\n  Args:\\n    num_trials: the number of trials\\n    train_fraction: (e.g. .80)\\n    nreplications: the number of spiking trials per initial condition\\n  Returns:\\n    a 2-tuple of two lists: the training indices and validation indices\\n    '\n    train_inds = []\n    valid_inds = []\n    for i in range(num_trials):\n        if i % nreplications + 1 > train_fraction * nreplications:\n            valid_inds.append(i)\n        else:\n            train_inds.append(i)\n    return (train_inds, valid_inds)",
            "def get_train_n_valid_inds(num_trials, train_fraction, nreplications):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Split the numbers between 0 and num_trials-1 into two portions for\\n  training and validation, based on the train fraction.\\n  Args:\\n    num_trials: the number of trials\\n    train_fraction: (e.g. .80)\\n    nreplications: the number of spiking trials per initial condition\\n  Returns:\\n    a 2-tuple of two lists: the training indices and validation indices\\n    '\n    train_inds = []\n    valid_inds = []\n    for i in range(num_trials):\n        if i % nreplications + 1 > train_fraction * nreplications:\n            valid_inds.append(i)\n        else:\n            train_inds.append(i)\n    return (train_inds, valid_inds)"
        ]
    },
    {
        "func_name": "split_list_by_inds",
        "original": "def split_list_by_inds(data, inds1, inds2):\n    \"\"\"Take the data, a list, and split it up based on the indices in inds1 and\n  inds2.\n  Args:\n    data: the list of data to split\n    inds1, the first list of indices\n    inds2, the second list of indices\n  Returns: a 2-tuple of two lists.\n  \"\"\"\n    if data is None or len(data) == 0:\n        return ([], [])\n    else:\n        dout1 = [data[i] for i in inds1]\n        dout2 = [data[i] for i in inds2]\n        return (dout1, dout2)",
        "mutated": [
            "def split_list_by_inds(data, inds1, inds2):\n    if False:\n        i = 10\n    'Take the data, a list, and split it up based on the indices in inds1 and\\n  inds2.\\n  Args:\\n    data: the list of data to split\\n    inds1, the first list of indices\\n    inds2, the second list of indices\\n  Returns: a 2-tuple of two lists.\\n  '\n    if data is None or len(data) == 0:\n        return ([], [])\n    else:\n        dout1 = [data[i] for i in inds1]\n        dout2 = [data[i] for i in inds2]\n        return (dout1, dout2)",
            "def split_list_by_inds(data, inds1, inds2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Take the data, a list, and split it up based on the indices in inds1 and\\n  inds2.\\n  Args:\\n    data: the list of data to split\\n    inds1, the first list of indices\\n    inds2, the second list of indices\\n  Returns: a 2-tuple of two lists.\\n  '\n    if data is None or len(data) == 0:\n        return ([], [])\n    else:\n        dout1 = [data[i] for i in inds1]\n        dout2 = [data[i] for i in inds2]\n        return (dout1, dout2)",
            "def split_list_by_inds(data, inds1, inds2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Take the data, a list, and split it up based on the indices in inds1 and\\n  inds2.\\n  Args:\\n    data: the list of data to split\\n    inds1, the first list of indices\\n    inds2, the second list of indices\\n  Returns: a 2-tuple of two lists.\\n  '\n    if data is None or len(data) == 0:\n        return ([], [])\n    else:\n        dout1 = [data[i] for i in inds1]\n        dout2 = [data[i] for i in inds2]\n        return (dout1, dout2)",
            "def split_list_by_inds(data, inds1, inds2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Take the data, a list, and split it up based on the indices in inds1 and\\n  inds2.\\n  Args:\\n    data: the list of data to split\\n    inds1, the first list of indices\\n    inds2, the second list of indices\\n  Returns: a 2-tuple of two lists.\\n  '\n    if data is None or len(data) == 0:\n        return ([], [])\n    else:\n        dout1 = [data[i] for i in inds1]\n        dout2 = [data[i] for i in inds2]\n        return (dout1, dout2)",
            "def split_list_by_inds(data, inds1, inds2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Take the data, a list, and split it up based on the indices in inds1 and\\n  inds2.\\n  Args:\\n    data: the list of data to split\\n    inds1, the first list of indices\\n    inds2, the second list of indices\\n  Returns: a 2-tuple of two lists.\\n  '\n    if data is None or len(data) == 0:\n        return ([], [])\n    else:\n        dout1 = [data[i] for i in inds1]\n        dout2 = [data[i] for i in inds2]\n        return (dout1, dout2)"
        ]
    },
    {
        "func_name": "nparray_and_transpose",
        "original": "def nparray_and_transpose(data_a_b_c):\n    \"\"\"Convert the list of items in data to a numpy array, and transpose it\n  Args:\n    data: data_asbsc: a nested, nested list of length a, with sublist length\n      b, with sublist length c.\n  Returns:\n    a numpy 3-tensor with dimensions a x c x b\n\"\"\"\n    data_axbxc = np.array([datum_b_c for datum_b_c in data_a_b_c])\n    data_axcxb = np.transpose(data_axbxc, axes=[0, 2, 1])\n    return data_axcxb",
        "mutated": [
            "def nparray_and_transpose(data_a_b_c):\n    if False:\n        i = 10\n    'Convert the list of items in data to a numpy array, and transpose it\\n  Args:\\n    data: data_asbsc: a nested, nested list of length a, with sublist length\\n      b, with sublist length c.\\n  Returns:\\n    a numpy 3-tensor with dimensions a x c x b\\n'\n    data_axbxc = np.array([datum_b_c for datum_b_c in data_a_b_c])\n    data_axcxb = np.transpose(data_axbxc, axes=[0, 2, 1])\n    return data_axcxb",
            "def nparray_and_transpose(data_a_b_c):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convert the list of items in data to a numpy array, and transpose it\\n  Args:\\n    data: data_asbsc: a nested, nested list of length a, with sublist length\\n      b, with sublist length c.\\n  Returns:\\n    a numpy 3-tensor with dimensions a x c x b\\n'\n    data_axbxc = np.array([datum_b_c for datum_b_c in data_a_b_c])\n    data_axcxb = np.transpose(data_axbxc, axes=[0, 2, 1])\n    return data_axcxb",
            "def nparray_and_transpose(data_a_b_c):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convert the list of items in data to a numpy array, and transpose it\\n  Args:\\n    data: data_asbsc: a nested, nested list of length a, with sublist length\\n      b, with sublist length c.\\n  Returns:\\n    a numpy 3-tensor with dimensions a x c x b\\n'\n    data_axbxc = np.array([datum_b_c for datum_b_c in data_a_b_c])\n    data_axcxb = np.transpose(data_axbxc, axes=[0, 2, 1])\n    return data_axcxb",
            "def nparray_and_transpose(data_a_b_c):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convert the list of items in data to a numpy array, and transpose it\\n  Args:\\n    data: data_asbsc: a nested, nested list of length a, with sublist length\\n      b, with sublist length c.\\n  Returns:\\n    a numpy 3-tensor with dimensions a x c x b\\n'\n    data_axbxc = np.array([datum_b_c for datum_b_c in data_a_b_c])\n    data_axcxb = np.transpose(data_axbxc, axes=[0, 2, 1])\n    return data_axcxb",
            "def nparray_and_transpose(data_a_b_c):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convert the list of items in data to a numpy array, and transpose it\\n  Args:\\n    data: data_asbsc: a nested, nested list of length a, with sublist length\\n      b, with sublist length c.\\n  Returns:\\n    a numpy 3-tensor with dimensions a x c x b\\n'\n    data_axbxc = np.array([datum_b_c for datum_b_c in data_a_b_c])\n    data_axcxb = np.transpose(data_axbxc, axes=[0, 2, 1])\n    return data_axcxb"
        ]
    },
    {
        "func_name": "add_alignment_projections",
        "original": "def add_alignment_projections(datasets, npcs, ntime=None, nsamples=None):\n    \"\"\"Create a matrix that aligns the datasets a bit, under\n  the assumption that each dataset is observing the same underlying dynamical\n  system.\n\n  Args:\n    datasets: The dictionary of dataset structures.\n    npcs:  The number of pcs for each, basically like lfads factors.\n    nsamples (optional): Number of samples to take for each dataset.\n    ntime (optional): Number of time steps to take in each sample.\n\n  Returns:\n    The dataset structures, with the field alignment_matrix_cxf added.\n    This is # channels x npcs dimension\n\"\"\"\n    nchannels_all = 0\n    channel_idxs = {}\n    conditions_all = {}\n    nconditions_all = 0\n    for (name, dataset) in datasets.items():\n        cidxs = np.where(dataset['P_sxn'])[1]\n        channel_idxs[name] = [cidxs[0], cidxs[-1] + 1]\n        nchannels_all += cidxs[-1] + 1 - cidxs[0]\n        conditions_all[name] = np.unique(dataset['condition_labels_train'])\n    all_conditions_list = np.unique(np.ndarray.flatten(np.array(conditions_all.values())))\n    nconditions_all = all_conditions_list.shape[0]\n    if ntime is None:\n        ntime = dataset['train_data'].shape[1]\n    if nsamples is None:\n        nsamples = dataset['train_data'].shape[0]\n    avg_data_all = {}\n    for (name, conditions) in conditions_all.items():\n        dataset = datasets[name]\n        avg_data_all[name] = {}\n        for cname in conditions:\n            td_idxs = np.argwhere(np.array(dataset['condition_labels_train']) == cname)\n            data = np.squeeze(dataset['train_data'][td_idxs, :, :], axis=1)\n            avg_data = np.mean(data, axis=0)\n            avg_data_all[name][cname] = avg_data\n    all_data_nxtc = np.zeros([nchannels_all, ntime * nconditions_all])\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        for cname in conditions_all[name]:\n            cidxs = np.argwhere(all_conditions_list == cname)\n            if cidxs.shape[0] > 0:\n                cidx = cidxs[0][0]\n                all_tidxs = np.arange(0, ntime + 1) + cidx * ntime\n                all_data_nxtc[cidx_s:cidx_f, all_tidxs[0]:all_tidxs[-1]] = avg_data_all[name][cname].T\n    filt_len = 6\n    bc_filt = np.ones([filt_len]) / float(filt_len)\n    for c in range(nchannels_all):\n        all_data_nxtc[c, :] = scipy.signal.filtfilt(bc_filt, [1.0], all_data_nxtc[c, :])\n    all_data_mean_nx1 = np.mean(all_data_nxtc, axis=1, keepdims=True)\n    all_data_zm_nxtc = all_data_nxtc - all_data_mean_nx1\n    corr_mat_nxn = np.dot(all_data_zm_nxtc, all_data_zm_nxtc.T)\n    (evals_n, evecs_nxn) = np.linalg.eigh(corr_mat_nxn)\n    sidxs = np.flipud(np.argsort(evals_n))\n    evals_n = evals_n[sidxs]\n    evecs_nxn = evecs_nxn[:, sidxs]\n    all_data_pca_pxtc = np.dot(evecs_nxn[:, 0:npcs].T, all_data_zm_nxtc)\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        all_data_zm_chxtc = all_data_zm_nxtc[cidx_s:cidx_f, :]\n        (W_chxp, _, _, _) = np.linalg.lstsq(all_data_zm_chxtc.T, all_data_pca_pxtc.T)\n        dataset['alignment_matrix_cxf'] = W_chxp\n        alignment_bias_cx1 = all_data_mean_nx1[cidx_s:cidx_f]\n        dataset['alignment_bias_c'] = np.squeeze(alignment_bias_cx1, axis=1)\n    do_debug_plot = False\n    if do_debug_plot:\n        pc_vecs = evecs_nxn[:, 0:npcs]\n        ntoplot = 400\n        plt.figure()\n        plt.plot(np.log10(evals_n), '-x')\n        plt.figure()\n        plt.subplot(311)\n        plt.imshow(all_data_pca_pxtc)\n        plt.colorbar()\n        plt.subplot(312)\n        plt.imshow(np.dot(W_chxp.T, all_data_zm_chxtc))\n        plt.colorbar()\n        plt.subplot(313)\n        plt.imshow(np.dot(all_data_zm_chxtc.T, W_chxp).T - all_data_pca_pxtc)\n        plt.colorbar()\n        import pdb\n        pdb.set_trace()\n    return datasets",
        "mutated": [
            "def add_alignment_projections(datasets, npcs, ntime=None, nsamples=None):\n    if False:\n        i = 10\n    'Create a matrix that aligns the datasets a bit, under\\n  the assumption that each dataset is observing the same underlying dynamical\\n  system.\\n\\n  Args:\\n    datasets: The dictionary of dataset structures.\\n    npcs:  The number of pcs for each, basically like lfads factors.\\n    nsamples (optional): Number of samples to take for each dataset.\\n    ntime (optional): Number of time steps to take in each sample.\\n\\n  Returns:\\n    The dataset structures, with the field alignment_matrix_cxf added.\\n    This is # channels x npcs dimension\\n'\n    nchannels_all = 0\n    channel_idxs = {}\n    conditions_all = {}\n    nconditions_all = 0\n    for (name, dataset) in datasets.items():\n        cidxs = np.where(dataset['P_sxn'])[1]\n        channel_idxs[name] = [cidxs[0], cidxs[-1] + 1]\n        nchannels_all += cidxs[-1] + 1 - cidxs[0]\n        conditions_all[name] = np.unique(dataset['condition_labels_train'])\n    all_conditions_list = np.unique(np.ndarray.flatten(np.array(conditions_all.values())))\n    nconditions_all = all_conditions_list.shape[0]\n    if ntime is None:\n        ntime = dataset['train_data'].shape[1]\n    if nsamples is None:\n        nsamples = dataset['train_data'].shape[0]\n    avg_data_all = {}\n    for (name, conditions) in conditions_all.items():\n        dataset = datasets[name]\n        avg_data_all[name] = {}\n        for cname in conditions:\n            td_idxs = np.argwhere(np.array(dataset['condition_labels_train']) == cname)\n            data = np.squeeze(dataset['train_data'][td_idxs, :, :], axis=1)\n            avg_data = np.mean(data, axis=0)\n            avg_data_all[name][cname] = avg_data\n    all_data_nxtc = np.zeros([nchannels_all, ntime * nconditions_all])\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        for cname in conditions_all[name]:\n            cidxs = np.argwhere(all_conditions_list == cname)\n            if cidxs.shape[0] > 0:\n                cidx = cidxs[0][0]\n                all_tidxs = np.arange(0, ntime + 1) + cidx * ntime\n                all_data_nxtc[cidx_s:cidx_f, all_tidxs[0]:all_tidxs[-1]] = avg_data_all[name][cname].T\n    filt_len = 6\n    bc_filt = np.ones([filt_len]) / float(filt_len)\n    for c in range(nchannels_all):\n        all_data_nxtc[c, :] = scipy.signal.filtfilt(bc_filt, [1.0], all_data_nxtc[c, :])\n    all_data_mean_nx1 = np.mean(all_data_nxtc, axis=1, keepdims=True)\n    all_data_zm_nxtc = all_data_nxtc - all_data_mean_nx1\n    corr_mat_nxn = np.dot(all_data_zm_nxtc, all_data_zm_nxtc.T)\n    (evals_n, evecs_nxn) = np.linalg.eigh(corr_mat_nxn)\n    sidxs = np.flipud(np.argsort(evals_n))\n    evals_n = evals_n[sidxs]\n    evecs_nxn = evecs_nxn[:, sidxs]\n    all_data_pca_pxtc = np.dot(evecs_nxn[:, 0:npcs].T, all_data_zm_nxtc)\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        all_data_zm_chxtc = all_data_zm_nxtc[cidx_s:cidx_f, :]\n        (W_chxp, _, _, _) = np.linalg.lstsq(all_data_zm_chxtc.T, all_data_pca_pxtc.T)\n        dataset['alignment_matrix_cxf'] = W_chxp\n        alignment_bias_cx1 = all_data_mean_nx1[cidx_s:cidx_f]\n        dataset['alignment_bias_c'] = np.squeeze(alignment_bias_cx1, axis=1)\n    do_debug_plot = False\n    if do_debug_plot:\n        pc_vecs = evecs_nxn[:, 0:npcs]\n        ntoplot = 400\n        plt.figure()\n        plt.plot(np.log10(evals_n), '-x')\n        plt.figure()\n        plt.subplot(311)\n        plt.imshow(all_data_pca_pxtc)\n        plt.colorbar()\n        plt.subplot(312)\n        plt.imshow(np.dot(W_chxp.T, all_data_zm_chxtc))\n        plt.colorbar()\n        plt.subplot(313)\n        plt.imshow(np.dot(all_data_zm_chxtc.T, W_chxp).T - all_data_pca_pxtc)\n        plt.colorbar()\n        import pdb\n        pdb.set_trace()\n    return datasets",
            "def add_alignment_projections(datasets, npcs, ntime=None, nsamples=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a matrix that aligns the datasets a bit, under\\n  the assumption that each dataset is observing the same underlying dynamical\\n  system.\\n\\n  Args:\\n    datasets: The dictionary of dataset structures.\\n    npcs:  The number of pcs for each, basically like lfads factors.\\n    nsamples (optional): Number of samples to take for each dataset.\\n    ntime (optional): Number of time steps to take in each sample.\\n\\n  Returns:\\n    The dataset structures, with the field alignment_matrix_cxf added.\\n    This is # channels x npcs dimension\\n'\n    nchannels_all = 0\n    channel_idxs = {}\n    conditions_all = {}\n    nconditions_all = 0\n    for (name, dataset) in datasets.items():\n        cidxs = np.where(dataset['P_sxn'])[1]\n        channel_idxs[name] = [cidxs[0], cidxs[-1] + 1]\n        nchannels_all += cidxs[-1] + 1 - cidxs[0]\n        conditions_all[name] = np.unique(dataset['condition_labels_train'])\n    all_conditions_list = np.unique(np.ndarray.flatten(np.array(conditions_all.values())))\n    nconditions_all = all_conditions_list.shape[0]\n    if ntime is None:\n        ntime = dataset['train_data'].shape[1]\n    if nsamples is None:\n        nsamples = dataset['train_data'].shape[0]\n    avg_data_all = {}\n    for (name, conditions) in conditions_all.items():\n        dataset = datasets[name]\n        avg_data_all[name] = {}\n        for cname in conditions:\n            td_idxs = np.argwhere(np.array(dataset['condition_labels_train']) == cname)\n            data = np.squeeze(dataset['train_data'][td_idxs, :, :], axis=1)\n            avg_data = np.mean(data, axis=0)\n            avg_data_all[name][cname] = avg_data\n    all_data_nxtc = np.zeros([nchannels_all, ntime * nconditions_all])\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        for cname in conditions_all[name]:\n            cidxs = np.argwhere(all_conditions_list == cname)\n            if cidxs.shape[0] > 0:\n                cidx = cidxs[0][0]\n                all_tidxs = np.arange(0, ntime + 1) + cidx * ntime\n                all_data_nxtc[cidx_s:cidx_f, all_tidxs[0]:all_tidxs[-1]] = avg_data_all[name][cname].T\n    filt_len = 6\n    bc_filt = np.ones([filt_len]) / float(filt_len)\n    for c in range(nchannels_all):\n        all_data_nxtc[c, :] = scipy.signal.filtfilt(bc_filt, [1.0], all_data_nxtc[c, :])\n    all_data_mean_nx1 = np.mean(all_data_nxtc, axis=1, keepdims=True)\n    all_data_zm_nxtc = all_data_nxtc - all_data_mean_nx1\n    corr_mat_nxn = np.dot(all_data_zm_nxtc, all_data_zm_nxtc.T)\n    (evals_n, evecs_nxn) = np.linalg.eigh(corr_mat_nxn)\n    sidxs = np.flipud(np.argsort(evals_n))\n    evals_n = evals_n[sidxs]\n    evecs_nxn = evecs_nxn[:, sidxs]\n    all_data_pca_pxtc = np.dot(evecs_nxn[:, 0:npcs].T, all_data_zm_nxtc)\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        all_data_zm_chxtc = all_data_zm_nxtc[cidx_s:cidx_f, :]\n        (W_chxp, _, _, _) = np.linalg.lstsq(all_data_zm_chxtc.T, all_data_pca_pxtc.T)\n        dataset['alignment_matrix_cxf'] = W_chxp\n        alignment_bias_cx1 = all_data_mean_nx1[cidx_s:cidx_f]\n        dataset['alignment_bias_c'] = np.squeeze(alignment_bias_cx1, axis=1)\n    do_debug_plot = False\n    if do_debug_plot:\n        pc_vecs = evecs_nxn[:, 0:npcs]\n        ntoplot = 400\n        plt.figure()\n        plt.plot(np.log10(evals_n), '-x')\n        plt.figure()\n        plt.subplot(311)\n        plt.imshow(all_data_pca_pxtc)\n        plt.colorbar()\n        plt.subplot(312)\n        plt.imshow(np.dot(W_chxp.T, all_data_zm_chxtc))\n        plt.colorbar()\n        plt.subplot(313)\n        plt.imshow(np.dot(all_data_zm_chxtc.T, W_chxp).T - all_data_pca_pxtc)\n        plt.colorbar()\n        import pdb\n        pdb.set_trace()\n    return datasets",
            "def add_alignment_projections(datasets, npcs, ntime=None, nsamples=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a matrix that aligns the datasets a bit, under\\n  the assumption that each dataset is observing the same underlying dynamical\\n  system.\\n\\n  Args:\\n    datasets: The dictionary of dataset structures.\\n    npcs:  The number of pcs for each, basically like lfads factors.\\n    nsamples (optional): Number of samples to take for each dataset.\\n    ntime (optional): Number of time steps to take in each sample.\\n\\n  Returns:\\n    The dataset structures, with the field alignment_matrix_cxf added.\\n    This is # channels x npcs dimension\\n'\n    nchannels_all = 0\n    channel_idxs = {}\n    conditions_all = {}\n    nconditions_all = 0\n    for (name, dataset) in datasets.items():\n        cidxs = np.where(dataset['P_sxn'])[1]\n        channel_idxs[name] = [cidxs[0], cidxs[-1] + 1]\n        nchannels_all += cidxs[-1] + 1 - cidxs[0]\n        conditions_all[name] = np.unique(dataset['condition_labels_train'])\n    all_conditions_list = np.unique(np.ndarray.flatten(np.array(conditions_all.values())))\n    nconditions_all = all_conditions_list.shape[0]\n    if ntime is None:\n        ntime = dataset['train_data'].shape[1]\n    if nsamples is None:\n        nsamples = dataset['train_data'].shape[0]\n    avg_data_all = {}\n    for (name, conditions) in conditions_all.items():\n        dataset = datasets[name]\n        avg_data_all[name] = {}\n        for cname in conditions:\n            td_idxs = np.argwhere(np.array(dataset['condition_labels_train']) == cname)\n            data = np.squeeze(dataset['train_data'][td_idxs, :, :], axis=1)\n            avg_data = np.mean(data, axis=0)\n            avg_data_all[name][cname] = avg_data\n    all_data_nxtc = np.zeros([nchannels_all, ntime * nconditions_all])\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        for cname in conditions_all[name]:\n            cidxs = np.argwhere(all_conditions_list == cname)\n            if cidxs.shape[0] > 0:\n                cidx = cidxs[0][0]\n                all_tidxs = np.arange(0, ntime + 1) + cidx * ntime\n                all_data_nxtc[cidx_s:cidx_f, all_tidxs[0]:all_tidxs[-1]] = avg_data_all[name][cname].T\n    filt_len = 6\n    bc_filt = np.ones([filt_len]) / float(filt_len)\n    for c in range(nchannels_all):\n        all_data_nxtc[c, :] = scipy.signal.filtfilt(bc_filt, [1.0], all_data_nxtc[c, :])\n    all_data_mean_nx1 = np.mean(all_data_nxtc, axis=1, keepdims=True)\n    all_data_zm_nxtc = all_data_nxtc - all_data_mean_nx1\n    corr_mat_nxn = np.dot(all_data_zm_nxtc, all_data_zm_nxtc.T)\n    (evals_n, evecs_nxn) = np.linalg.eigh(corr_mat_nxn)\n    sidxs = np.flipud(np.argsort(evals_n))\n    evals_n = evals_n[sidxs]\n    evecs_nxn = evecs_nxn[:, sidxs]\n    all_data_pca_pxtc = np.dot(evecs_nxn[:, 0:npcs].T, all_data_zm_nxtc)\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        all_data_zm_chxtc = all_data_zm_nxtc[cidx_s:cidx_f, :]\n        (W_chxp, _, _, _) = np.linalg.lstsq(all_data_zm_chxtc.T, all_data_pca_pxtc.T)\n        dataset['alignment_matrix_cxf'] = W_chxp\n        alignment_bias_cx1 = all_data_mean_nx1[cidx_s:cidx_f]\n        dataset['alignment_bias_c'] = np.squeeze(alignment_bias_cx1, axis=1)\n    do_debug_plot = False\n    if do_debug_plot:\n        pc_vecs = evecs_nxn[:, 0:npcs]\n        ntoplot = 400\n        plt.figure()\n        plt.plot(np.log10(evals_n), '-x')\n        plt.figure()\n        plt.subplot(311)\n        plt.imshow(all_data_pca_pxtc)\n        plt.colorbar()\n        plt.subplot(312)\n        plt.imshow(np.dot(W_chxp.T, all_data_zm_chxtc))\n        plt.colorbar()\n        plt.subplot(313)\n        plt.imshow(np.dot(all_data_zm_chxtc.T, W_chxp).T - all_data_pca_pxtc)\n        plt.colorbar()\n        import pdb\n        pdb.set_trace()\n    return datasets",
            "def add_alignment_projections(datasets, npcs, ntime=None, nsamples=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a matrix that aligns the datasets a bit, under\\n  the assumption that each dataset is observing the same underlying dynamical\\n  system.\\n\\n  Args:\\n    datasets: The dictionary of dataset structures.\\n    npcs:  The number of pcs for each, basically like lfads factors.\\n    nsamples (optional): Number of samples to take for each dataset.\\n    ntime (optional): Number of time steps to take in each sample.\\n\\n  Returns:\\n    The dataset structures, with the field alignment_matrix_cxf added.\\n    This is # channels x npcs dimension\\n'\n    nchannels_all = 0\n    channel_idxs = {}\n    conditions_all = {}\n    nconditions_all = 0\n    for (name, dataset) in datasets.items():\n        cidxs = np.where(dataset['P_sxn'])[1]\n        channel_idxs[name] = [cidxs[0], cidxs[-1] + 1]\n        nchannels_all += cidxs[-1] + 1 - cidxs[0]\n        conditions_all[name] = np.unique(dataset['condition_labels_train'])\n    all_conditions_list = np.unique(np.ndarray.flatten(np.array(conditions_all.values())))\n    nconditions_all = all_conditions_list.shape[0]\n    if ntime is None:\n        ntime = dataset['train_data'].shape[1]\n    if nsamples is None:\n        nsamples = dataset['train_data'].shape[0]\n    avg_data_all = {}\n    for (name, conditions) in conditions_all.items():\n        dataset = datasets[name]\n        avg_data_all[name] = {}\n        for cname in conditions:\n            td_idxs = np.argwhere(np.array(dataset['condition_labels_train']) == cname)\n            data = np.squeeze(dataset['train_data'][td_idxs, :, :], axis=1)\n            avg_data = np.mean(data, axis=0)\n            avg_data_all[name][cname] = avg_data\n    all_data_nxtc = np.zeros([nchannels_all, ntime * nconditions_all])\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        for cname in conditions_all[name]:\n            cidxs = np.argwhere(all_conditions_list == cname)\n            if cidxs.shape[0] > 0:\n                cidx = cidxs[0][0]\n                all_tidxs = np.arange(0, ntime + 1) + cidx * ntime\n                all_data_nxtc[cidx_s:cidx_f, all_tidxs[0]:all_tidxs[-1]] = avg_data_all[name][cname].T\n    filt_len = 6\n    bc_filt = np.ones([filt_len]) / float(filt_len)\n    for c in range(nchannels_all):\n        all_data_nxtc[c, :] = scipy.signal.filtfilt(bc_filt, [1.0], all_data_nxtc[c, :])\n    all_data_mean_nx1 = np.mean(all_data_nxtc, axis=1, keepdims=True)\n    all_data_zm_nxtc = all_data_nxtc - all_data_mean_nx1\n    corr_mat_nxn = np.dot(all_data_zm_nxtc, all_data_zm_nxtc.T)\n    (evals_n, evecs_nxn) = np.linalg.eigh(corr_mat_nxn)\n    sidxs = np.flipud(np.argsort(evals_n))\n    evals_n = evals_n[sidxs]\n    evecs_nxn = evecs_nxn[:, sidxs]\n    all_data_pca_pxtc = np.dot(evecs_nxn[:, 0:npcs].T, all_data_zm_nxtc)\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        all_data_zm_chxtc = all_data_zm_nxtc[cidx_s:cidx_f, :]\n        (W_chxp, _, _, _) = np.linalg.lstsq(all_data_zm_chxtc.T, all_data_pca_pxtc.T)\n        dataset['alignment_matrix_cxf'] = W_chxp\n        alignment_bias_cx1 = all_data_mean_nx1[cidx_s:cidx_f]\n        dataset['alignment_bias_c'] = np.squeeze(alignment_bias_cx1, axis=1)\n    do_debug_plot = False\n    if do_debug_plot:\n        pc_vecs = evecs_nxn[:, 0:npcs]\n        ntoplot = 400\n        plt.figure()\n        plt.plot(np.log10(evals_n), '-x')\n        plt.figure()\n        plt.subplot(311)\n        plt.imshow(all_data_pca_pxtc)\n        plt.colorbar()\n        plt.subplot(312)\n        plt.imshow(np.dot(W_chxp.T, all_data_zm_chxtc))\n        plt.colorbar()\n        plt.subplot(313)\n        plt.imshow(np.dot(all_data_zm_chxtc.T, W_chxp).T - all_data_pca_pxtc)\n        plt.colorbar()\n        import pdb\n        pdb.set_trace()\n    return datasets",
            "def add_alignment_projections(datasets, npcs, ntime=None, nsamples=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a matrix that aligns the datasets a bit, under\\n  the assumption that each dataset is observing the same underlying dynamical\\n  system.\\n\\n  Args:\\n    datasets: The dictionary of dataset structures.\\n    npcs:  The number of pcs for each, basically like lfads factors.\\n    nsamples (optional): Number of samples to take for each dataset.\\n    ntime (optional): Number of time steps to take in each sample.\\n\\n  Returns:\\n    The dataset structures, with the field alignment_matrix_cxf added.\\n    This is # channels x npcs dimension\\n'\n    nchannels_all = 0\n    channel_idxs = {}\n    conditions_all = {}\n    nconditions_all = 0\n    for (name, dataset) in datasets.items():\n        cidxs = np.where(dataset['P_sxn'])[1]\n        channel_idxs[name] = [cidxs[0], cidxs[-1] + 1]\n        nchannels_all += cidxs[-1] + 1 - cidxs[0]\n        conditions_all[name] = np.unique(dataset['condition_labels_train'])\n    all_conditions_list = np.unique(np.ndarray.flatten(np.array(conditions_all.values())))\n    nconditions_all = all_conditions_list.shape[0]\n    if ntime is None:\n        ntime = dataset['train_data'].shape[1]\n    if nsamples is None:\n        nsamples = dataset['train_data'].shape[0]\n    avg_data_all = {}\n    for (name, conditions) in conditions_all.items():\n        dataset = datasets[name]\n        avg_data_all[name] = {}\n        for cname in conditions:\n            td_idxs = np.argwhere(np.array(dataset['condition_labels_train']) == cname)\n            data = np.squeeze(dataset['train_data'][td_idxs, :, :], axis=1)\n            avg_data = np.mean(data, axis=0)\n            avg_data_all[name][cname] = avg_data\n    all_data_nxtc = np.zeros([nchannels_all, ntime * nconditions_all])\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        for cname in conditions_all[name]:\n            cidxs = np.argwhere(all_conditions_list == cname)\n            if cidxs.shape[0] > 0:\n                cidx = cidxs[0][0]\n                all_tidxs = np.arange(0, ntime + 1) + cidx * ntime\n                all_data_nxtc[cidx_s:cidx_f, all_tidxs[0]:all_tidxs[-1]] = avg_data_all[name][cname].T\n    filt_len = 6\n    bc_filt = np.ones([filt_len]) / float(filt_len)\n    for c in range(nchannels_all):\n        all_data_nxtc[c, :] = scipy.signal.filtfilt(bc_filt, [1.0], all_data_nxtc[c, :])\n    all_data_mean_nx1 = np.mean(all_data_nxtc, axis=1, keepdims=True)\n    all_data_zm_nxtc = all_data_nxtc - all_data_mean_nx1\n    corr_mat_nxn = np.dot(all_data_zm_nxtc, all_data_zm_nxtc.T)\n    (evals_n, evecs_nxn) = np.linalg.eigh(corr_mat_nxn)\n    sidxs = np.flipud(np.argsort(evals_n))\n    evals_n = evals_n[sidxs]\n    evecs_nxn = evecs_nxn[:, sidxs]\n    all_data_pca_pxtc = np.dot(evecs_nxn[:, 0:npcs].T, all_data_zm_nxtc)\n    for (name, dataset) in datasets.items():\n        cidx_s = channel_idxs[name][0]\n        cidx_f = channel_idxs[name][1]\n        all_data_zm_chxtc = all_data_zm_nxtc[cidx_s:cidx_f, :]\n        (W_chxp, _, _, _) = np.linalg.lstsq(all_data_zm_chxtc.T, all_data_pca_pxtc.T)\n        dataset['alignment_matrix_cxf'] = W_chxp\n        alignment_bias_cx1 = all_data_mean_nx1[cidx_s:cidx_f]\n        dataset['alignment_bias_c'] = np.squeeze(alignment_bias_cx1, axis=1)\n    do_debug_plot = False\n    if do_debug_plot:\n        pc_vecs = evecs_nxn[:, 0:npcs]\n        ntoplot = 400\n        plt.figure()\n        plt.plot(np.log10(evals_n), '-x')\n        plt.figure()\n        plt.subplot(311)\n        plt.imshow(all_data_pca_pxtc)\n        plt.colorbar()\n        plt.subplot(312)\n        plt.imshow(np.dot(W_chxp.T, all_data_zm_chxtc))\n        plt.colorbar()\n        plt.subplot(313)\n        plt.imshow(np.dot(all_data_zm_chxtc.T, W_chxp).T - all_data_pca_pxtc)\n        plt.colorbar()\n        import pdb\n        pdb.set_trace()\n    return datasets"
        ]
    }
]