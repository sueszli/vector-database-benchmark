[
    {
        "func_name": "run_confusion_matrix_check",
        "original": "def run_confusion_matrix_check(y_pred: np.ndarray, y_true: np.ndarray, with_display=True, normalize_display=True) -> CheckResult:\n    \"\"\"Calculate confusion matrix based on predictions and true label values.\"\"\"\n    total_classes = sorted([str(x) for x in set(y_pred).union(set(y_true))])\n    result = confusion_matrix(y_true, y_pred)\n    if with_display:\n        displays = create_confusion_matrix_figure(result, total_classes, normalize_display)\n    else:\n        displays = None\n    result = pd.DataFrame(result, index=total_classes, columns=total_classes)\n    return CheckResult(result, display=displays)",
        "mutated": [
            "def run_confusion_matrix_check(y_pred: np.ndarray, y_true: np.ndarray, with_display=True, normalize_display=True) -> CheckResult:\n    if False:\n        i = 10\n    'Calculate confusion matrix based on predictions and true label values.'\n    total_classes = sorted([str(x) for x in set(y_pred).union(set(y_true))])\n    result = confusion_matrix(y_true, y_pred)\n    if with_display:\n        displays = create_confusion_matrix_figure(result, total_classes, normalize_display)\n    else:\n        displays = None\n    result = pd.DataFrame(result, index=total_classes, columns=total_classes)\n    return CheckResult(result, display=displays)",
            "def run_confusion_matrix_check(y_pred: np.ndarray, y_true: np.ndarray, with_display=True, normalize_display=True) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate confusion matrix based on predictions and true label values.'\n    total_classes = sorted([str(x) for x in set(y_pred).union(set(y_true))])\n    result = confusion_matrix(y_true, y_pred)\n    if with_display:\n        displays = create_confusion_matrix_figure(result, total_classes, normalize_display)\n    else:\n        displays = None\n    result = pd.DataFrame(result, index=total_classes, columns=total_classes)\n    return CheckResult(result, display=displays)",
            "def run_confusion_matrix_check(y_pred: np.ndarray, y_true: np.ndarray, with_display=True, normalize_display=True) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate confusion matrix based on predictions and true label values.'\n    total_classes = sorted([str(x) for x in set(y_pred).union(set(y_true))])\n    result = confusion_matrix(y_true, y_pred)\n    if with_display:\n        displays = create_confusion_matrix_figure(result, total_classes, normalize_display)\n    else:\n        displays = None\n    result = pd.DataFrame(result, index=total_classes, columns=total_classes)\n    return CheckResult(result, display=displays)",
            "def run_confusion_matrix_check(y_pred: np.ndarray, y_true: np.ndarray, with_display=True, normalize_display=True) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate confusion matrix based on predictions and true label values.'\n    total_classes = sorted([str(x) for x in set(y_pred).union(set(y_true))])\n    result = confusion_matrix(y_true, y_pred)\n    if with_display:\n        displays = create_confusion_matrix_figure(result, total_classes, normalize_display)\n    else:\n        displays = None\n    result = pd.DataFrame(result, index=total_classes, columns=total_classes)\n    return CheckResult(result, display=displays)",
            "def run_confusion_matrix_check(y_pred: np.ndarray, y_true: np.ndarray, with_display=True, normalize_display=True) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate confusion matrix based on predictions and true label values.'\n    total_classes = sorted([str(x) for x in set(y_pred).union(set(y_true))])\n    result = confusion_matrix(y_true, y_pred)\n    if with_display:\n        displays = create_confusion_matrix_figure(result, total_classes, normalize_display)\n    else:\n        displays = None\n    result = pd.DataFrame(result, index=total_classes, columns=total_classes)\n    return CheckResult(result, display=displays)"
        ]
    },
    {
        "func_name": "create_confusion_matrix_figure",
        "original": "def create_confusion_matrix_figure(confusion_matrix_data: np.ndarray, classes_names: List[str], normalize_display: bool):\n    \"\"\"Create a confusion matrix figure.\n\n    Parameters\n    ----------\n    confusion_matrix_data: np.ndarray\n        2D array containing the confusion matrix.\n    classes_names: List[str]\n        the names of the classes to display as the axis.\n    normalize_display: bool\n        if True will also show normalized values by the true values.\n\n    Returns\n    -------\n    plotly Figure object\n        confusion matrix figure\n\n    \"\"\"\n    confusion_matrix_norm = confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=1)[:, np.newaxis] + np.finfo(float).eps) * 100\n    if normalize_display:\n        z = np.vectorize(format_number_if_not_nan)(confusion_matrix_norm)\n    else:\n        z = confusion_matrix_data\n    accuracy_array = np.diag(confusion_matrix_norm).round(decimals=2)\n    display = []\n    display_msg = f'The overall accuracy of your model is: {round(np.sum(accuracy_array) / len(accuracy_array), 2)}%.'\n    if min(accuracy_array) < 100:\n        display_msg += f'<br>Best accuracy achieved on samples with <b>{classes_names[np.argmax(accuracy_array)]}</b> label ({np.max(accuracy_array)}%).'\n        display_msg += f'<br>Worst accuracy achieved on samples with <b>{classes_names[np.argmin(accuracy_array)]}</b> label ({np.min(accuracy_array)}%).'\n    display.append(display_msg)\n    total_samples = np.nansum(confusion_matrix_data)\n    percent_data_each_row = np.round(confusion_matrix_norm, decimals=2)\n    percent_data_each_cell = np.round(np.divide(np.nan_to_num(confusion_matrix_data, nan=0.0), total_samples) * 100, decimals=2)\n    percent_data_each_col = (confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=0)[:, np.newaxis] + np.finfo(float).eps) * 100).round(decimals=2)\n    custom_hoverdata = np.dstack((percent_data_each_cell, percent_data_each_row, percent_data_each_col))\n    fig = go.Figure(data=go.Heatmap(x=classes_names, y=classes_names, z=z, customdata=custom_hoverdata, xgap=1, ygap=1, text=confusion_matrix_data, texttemplate='%{text}', hovertemplate='% out of all data: <b>%{customdata[0]}%</b><br>% out of row: <b>%{customdata[1]}%</b><br>% out of column: <b>%{customdata[2]}%</b><extra></extra>', showscale=False))\n    fig.update_layout(title='Confusion Matrix (# Samples)', title_x=0.5)\n    fig.update_layout(height=600)\n    fig.update_xaxes(title='Predicted Value', type='category', scaleanchor='y', constrain='domain')\n    fig.update_yaxes(title='True Value', type='category', constrain='domain', autorange='reversed')\n    display.append(fig)\n    return display",
        "mutated": [
            "def create_confusion_matrix_figure(confusion_matrix_data: np.ndarray, classes_names: List[str], normalize_display: bool):\n    if False:\n        i = 10\n    'Create a confusion matrix figure.\\n\\n    Parameters\\n    ----------\\n    confusion_matrix_data: np.ndarray\\n        2D array containing the confusion matrix.\\n    classes_names: List[str]\\n        the names of the classes to display as the axis.\\n    normalize_display: bool\\n        if True will also show normalized values by the true values.\\n\\n    Returns\\n    -------\\n    plotly Figure object\\n        confusion matrix figure\\n\\n    '\n    confusion_matrix_norm = confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=1)[:, np.newaxis] + np.finfo(float).eps) * 100\n    if normalize_display:\n        z = np.vectorize(format_number_if_not_nan)(confusion_matrix_norm)\n    else:\n        z = confusion_matrix_data\n    accuracy_array = np.diag(confusion_matrix_norm).round(decimals=2)\n    display = []\n    display_msg = f'The overall accuracy of your model is: {round(np.sum(accuracy_array) / len(accuracy_array), 2)}%.'\n    if min(accuracy_array) < 100:\n        display_msg += f'<br>Best accuracy achieved on samples with <b>{classes_names[np.argmax(accuracy_array)]}</b> label ({np.max(accuracy_array)}%).'\n        display_msg += f'<br>Worst accuracy achieved on samples with <b>{classes_names[np.argmin(accuracy_array)]}</b> label ({np.min(accuracy_array)}%).'\n    display.append(display_msg)\n    total_samples = np.nansum(confusion_matrix_data)\n    percent_data_each_row = np.round(confusion_matrix_norm, decimals=2)\n    percent_data_each_cell = np.round(np.divide(np.nan_to_num(confusion_matrix_data, nan=0.0), total_samples) * 100, decimals=2)\n    percent_data_each_col = (confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=0)[:, np.newaxis] + np.finfo(float).eps) * 100).round(decimals=2)\n    custom_hoverdata = np.dstack((percent_data_each_cell, percent_data_each_row, percent_data_each_col))\n    fig = go.Figure(data=go.Heatmap(x=classes_names, y=classes_names, z=z, customdata=custom_hoverdata, xgap=1, ygap=1, text=confusion_matrix_data, texttemplate='%{text}', hovertemplate='% out of all data: <b>%{customdata[0]}%</b><br>% out of row: <b>%{customdata[1]}%</b><br>% out of column: <b>%{customdata[2]}%</b><extra></extra>', showscale=False))\n    fig.update_layout(title='Confusion Matrix (# Samples)', title_x=0.5)\n    fig.update_layout(height=600)\n    fig.update_xaxes(title='Predicted Value', type='category', scaleanchor='y', constrain='domain')\n    fig.update_yaxes(title='True Value', type='category', constrain='domain', autorange='reversed')\n    display.append(fig)\n    return display",
            "def create_confusion_matrix_figure(confusion_matrix_data: np.ndarray, classes_names: List[str], normalize_display: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a confusion matrix figure.\\n\\n    Parameters\\n    ----------\\n    confusion_matrix_data: np.ndarray\\n        2D array containing the confusion matrix.\\n    classes_names: List[str]\\n        the names of the classes to display as the axis.\\n    normalize_display: bool\\n        if True will also show normalized values by the true values.\\n\\n    Returns\\n    -------\\n    plotly Figure object\\n        confusion matrix figure\\n\\n    '\n    confusion_matrix_norm = confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=1)[:, np.newaxis] + np.finfo(float).eps) * 100\n    if normalize_display:\n        z = np.vectorize(format_number_if_not_nan)(confusion_matrix_norm)\n    else:\n        z = confusion_matrix_data\n    accuracy_array = np.diag(confusion_matrix_norm).round(decimals=2)\n    display = []\n    display_msg = f'The overall accuracy of your model is: {round(np.sum(accuracy_array) / len(accuracy_array), 2)}%.'\n    if min(accuracy_array) < 100:\n        display_msg += f'<br>Best accuracy achieved on samples with <b>{classes_names[np.argmax(accuracy_array)]}</b> label ({np.max(accuracy_array)}%).'\n        display_msg += f'<br>Worst accuracy achieved on samples with <b>{classes_names[np.argmin(accuracy_array)]}</b> label ({np.min(accuracy_array)}%).'\n    display.append(display_msg)\n    total_samples = np.nansum(confusion_matrix_data)\n    percent_data_each_row = np.round(confusion_matrix_norm, decimals=2)\n    percent_data_each_cell = np.round(np.divide(np.nan_to_num(confusion_matrix_data, nan=0.0), total_samples) * 100, decimals=2)\n    percent_data_each_col = (confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=0)[:, np.newaxis] + np.finfo(float).eps) * 100).round(decimals=2)\n    custom_hoverdata = np.dstack((percent_data_each_cell, percent_data_each_row, percent_data_each_col))\n    fig = go.Figure(data=go.Heatmap(x=classes_names, y=classes_names, z=z, customdata=custom_hoverdata, xgap=1, ygap=1, text=confusion_matrix_data, texttemplate='%{text}', hovertemplate='% out of all data: <b>%{customdata[0]}%</b><br>% out of row: <b>%{customdata[1]}%</b><br>% out of column: <b>%{customdata[2]}%</b><extra></extra>', showscale=False))\n    fig.update_layout(title='Confusion Matrix (# Samples)', title_x=0.5)\n    fig.update_layout(height=600)\n    fig.update_xaxes(title='Predicted Value', type='category', scaleanchor='y', constrain='domain')\n    fig.update_yaxes(title='True Value', type='category', constrain='domain', autorange='reversed')\n    display.append(fig)\n    return display",
            "def create_confusion_matrix_figure(confusion_matrix_data: np.ndarray, classes_names: List[str], normalize_display: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a confusion matrix figure.\\n\\n    Parameters\\n    ----------\\n    confusion_matrix_data: np.ndarray\\n        2D array containing the confusion matrix.\\n    classes_names: List[str]\\n        the names of the classes to display as the axis.\\n    normalize_display: bool\\n        if True will also show normalized values by the true values.\\n\\n    Returns\\n    -------\\n    plotly Figure object\\n        confusion matrix figure\\n\\n    '\n    confusion_matrix_norm = confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=1)[:, np.newaxis] + np.finfo(float).eps) * 100\n    if normalize_display:\n        z = np.vectorize(format_number_if_not_nan)(confusion_matrix_norm)\n    else:\n        z = confusion_matrix_data\n    accuracy_array = np.diag(confusion_matrix_norm).round(decimals=2)\n    display = []\n    display_msg = f'The overall accuracy of your model is: {round(np.sum(accuracy_array) / len(accuracy_array), 2)}%.'\n    if min(accuracy_array) < 100:\n        display_msg += f'<br>Best accuracy achieved on samples with <b>{classes_names[np.argmax(accuracy_array)]}</b> label ({np.max(accuracy_array)}%).'\n        display_msg += f'<br>Worst accuracy achieved on samples with <b>{classes_names[np.argmin(accuracy_array)]}</b> label ({np.min(accuracy_array)}%).'\n    display.append(display_msg)\n    total_samples = np.nansum(confusion_matrix_data)\n    percent_data_each_row = np.round(confusion_matrix_norm, decimals=2)\n    percent_data_each_cell = np.round(np.divide(np.nan_to_num(confusion_matrix_data, nan=0.0), total_samples) * 100, decimals=2)\n    percent_data_each_col = (confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=0)[:, np.newaxis] + np.finfo(float).eps) * 100).round(decimals=2)\n    custom_hoverdata = np.dstack((percent_data_each_cell, percent_data_each_row, percent_data_each_col))\n    fig = go.Figure(data=go.Heatmap(x=classes_names, y=classes_names, z=z, customdata=custom_hoverdata, xgap=1, ygap=1, text=confusion_matrix_data, texttemplate='%{text}', hovertemplate='% out of all data: <b>%{customdata[0]}%</b><br>% out of row: <b>%{customdata[1]}%</b><br>% out of column: <b>%{customdata[2]}%</b><extra></extra>', showscale=False))\n    fig.update_layout(title='Confusion Matrix (# Samples)', title_x=0.5)\n    fig.update_layout(height=600)\n    fig.update_xaxes(title='Predicted Value', type='category', scaleanchor='y', constrain='domain')\n    fig.update_yaxes(title='True Value', type='category', constrain='domain', autorange='reversed')\n    display.append(fig)\n    return display",
            "def create_confusion_matrix_figure(confusion_matrix_data: np.ndarray, classes_names: List[str], normalize_display: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a confusion matrix figure.\\n\\n    Parameters\\n    ----------\\n    confusion_matrix_data: np.ndarray\\n        2D array containing the confusion matrix.\\n    classes_names: List[str]\\n        the names of the classes to display as the axis.\\n    normalize_display: bool\\n        if True will also show normalized values by the true values.\\n\\n    Returns\\n    -------\\n    plotly Figure object\\n        confusion matrix figure\\n\\n    '\n    confusion_matrix_norm = confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=1)[:, np.newaxis] + np.finfo(float).eps) * 100\n    if normalize_display:\n        z = np.vectorize(format_number_if_not_nan)(confusion_matrix_norm)\n    else:\n        z = confusion_matrix_data\n    accuracy_array = np.diag(confusion_matrix_norm).round(decimals=2)\n    display = []\n    display_msg = f'The overall accuracy of your model is: {round(np.sum(accuracy_array) / len(accuracy_array), 2)}%.'\n    if min(accuracy_array) < 100:\n        display_msg += f'<br>Best accuracy achieved on samples with <b>{classes_names[np.argmax(accuracy_array)]}</b> label ({np.max(accuracy_array)}%).'\n        display_msg += f'<br>Worst accuracy achieved on samples with <b>{classes_names[np.argmin(accuracy_array)]}</b> label ({np.min(accuracy_array)}%).'\n    display.append(display_msg)\n    total_samples = np.nansum(confusion_matrix_data)\n    percent_data_each_row = np.round(confusion_matrix_norm, decimals=2)\n    percent_data_each_cell = np.round(np.divide(np.nan_to_num(confusion_matrix_data, nan=0.0), total_samples) * 100, decimals=2)\n    percent_data_each_col = (confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=0)[:, np.newaxis] + np.finfo(float).eps) * 100).round(decimals=2)\n    custom_hoverdata = np.dstack((percent_data_each_cell, percent_data_each_row, percent_data_each_col))\n    fig = go.Figure(data=go.Heatmap(x=classes_names, y=classes_names, z=z, customdata=custom_hoverdata, xgap=1, ygap=1, text=confusion_matrix_data, texttemplate='%{text}', hovertemplate='% out of all data: <b>%{customdata[0]}%</b><br>% out of row: <b>%{customdata[1]}%</b><br>% out of column: <b>%{customdata[2]}%</b><extra></extra>', showscale=False))\n    fig.update_layout(title='Confusion Matrix (# Samples)', title_x=0.5)\n    fig.update_layout(height=600)\n    fig.update_xaxes(title='Predicted Value', type='category', scaleanchor='y', constrain='domain')\n    fig.update_yaxes(title='True Value', type='category', constrain='domain', autorange='reversed')\n    display.append(fig)\n    return display",
            "def create_confusion_matrix_figure(confusion_matrix_data: np.ndarray, classes_names: List[str], normalize_display: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a confusion matrix figure.\\n\\n    Parameters\\n    ----------\\n    confusion_matrix_data: np.ndarray\\n        2D array containing the confusion matrix.\\n    classes_names: List[str]\\n        the names of the classes to display as the axis.\\n    normalize_display: bool\\n        if True will also show normalized values by the true values.\\n\\n    Returns\\n    -------\\n    plotly Figure object\\n        confusion matrix figure\\n\\n    '\n    confusion_matrix_norm = confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=1)[:, np.newaxis] + np.finfo(float).eps) * 100\n    if normalize_display:\n        z = np.vectorize(format_number_if_not_nan)(confusion_matrix_norm)\n    else:\n        z = confusion_matrix_data\n    accuracy_array = np.diag(confusion_matrix_norm).round(decimals=2)\n    display = []\n    display_msg = f'The overall accuracy of your model is: {round(np.sum(accuracy_array) / len(accuracy_array), 2)}%.'\n    if min(accuracy_array) < 100:\n        display_msg += f'<br>Best accuracy achieved on samples with <b>{classes_names[np.argmax(accuracy_array)]}</b> label ({np.max(accuracy_array)}%).'\n        display_msg += f'<br>Worst accuracy achieved on samples with <b>{classes_names[np.argmin(accuracy_array)]}</b> label ({np.min(accuracy_array)}%).'\n    display.append(display_msg)\n    total_samples = np.nansum(confusion_matrix_data)\n    percent_data_each_row = np.round(confusion_matrix_norm, decimals=2)\n    percent_data_each_cell = np.round(np.divide(np.nan_to_num(confusion_matrix_data, nan=0.0), total_samples) * 100, decimals=2)\n    percent_data_each_col = (confusion_matrix_data.astype('float') / (confusion_matrix_data.sum(axis=0)[:, np.newaxis] + np.finfo(float).eps) * 100).round(decimals=2)\n    custom_hoverdata = np.dstack((percent_data_each_cell, percent_data_each_row, percent_data_each_col))\n    fig = go.Figure(data=go.Heatmap(x=classes_names, y=classes_names, z=z, customdata=custom_hoverdata, xgap=1, ygap=1, text=confusion_matrix_data, texttemplate='%{text}', hovertemplate='% out of all data: <b>%{customdata[0]}%</b><br>% out of row: <b>%{customdata[1]}%</b><br>% out of column: <b>%{customdata[2]}%</b><extra></extra>', showscale=False))\n    fig.update_layout(title='Confusion Matrix (# Samples)', title_x=0.5)\n    fig.update_layout(height=600)\n    fig.update_xaxes(title='Predicted Value', type='category', scaleanchor='y', constrain='domain')\n    fig.update_yaxes(title='True Value', type='category', constrain='domain', autorange='reversed')\n    display.append(fig)\n    return display"
        ]
    },
    {
        "func_name": "misclassified_samples_lower_than_condition",
        "original": "def misclassified_samples_lower_than_condition(value: pd.DataFrame, misclassified_samples_threshold: float) -> ConditionResult:\n    \"\"\"Condition function that checks if the misclassified samples in the confusion matrix is below threshold.\n\n    Parameters\n    ----------\n    value: pd.DataFrame\n        Dataframe containing the confusion matrix\n    misclassified_samples_threshold: float\n        Ratio of samples to be used for comparison in the condition (Value should be between 0 - 1 inclusive)\n\n    Raises\n    ------\n    DeepchecksValueError\n        if the value of `misclassified_samples_threshold` parameter is not between 0 - 1 inclusive.\n\n    Returns\n    -------\n    ConditionResult\n        - ConditionCategory.PASS, if all the misclassified samples in the confusion\n        matrix are less than `misclassified_samples_threshold` ratio\n        - ConditionCategory.FAIL, if the misclassified samples in the confusion matrix\n        are more than the `misclassified_samples_threshold` ratio\n    \"\"\"\n    if misclassified_samples_threshold < 0 or misclassified_samples_threshold > 1:\n        raise DeepchecksValueError(f'Condition requires the parameter \"misclassified_samples_threshold\" to be between 0 and 1 inclusive but got {misclassified_samples_threshold}')\n    class_names = value.columns\n    value = value.to_numpy()\n    total_samples = np.sum(value)\n    thresh_samples = round(np.ceil(misclassified_samples_threshold * total_samples))\n    (m, n) = (value.shape[0], value.shape[1])\n    n_cells_above_thresh = 0\n    max_misclassified_cell_idx = (0, 1)\n    for i in range(m):\n        for j in range(n):\n            if i != j:\n                n_samples = value[i][j]\n                if n_samples > thresh_samples:\n                    n_cells_above_thresh += 1\n                    (x, y) = max_misclassified_cell_idx\n                    max_misclassified_samples = value[x][y]\n                    if n_samples > max_misclassified_samples:\n                        max_misclassified_cell_idx = (i, j)\n    if n_cells_above_thresh > 0:\n        (x, y) = max_misclassified_cell_idx\n        max_misclassified_samples = value[x][y]\n        max_misclassified_samples_ratio = max_misclassified_samples / total_samples\n        details = f'Detected {n_cells_above_thresh} misclassified confusion matrix cell(s) each one containing more than {format_percent(misclassified_samples_threshold)} of the data. Largest misclassified cell ({format_percent(max_misclassified_samples_ratio)} of the data) is samples with a true value of \"{class_names[x]}\" and a predicted value of \"{class_names[y]}\".'\n        return ConditionResult(ConditionCategory.FAIL, details)\n    details = f'All misclassified confusion matrix cells contain less than {format_percent(misclassified_samples_threshold)} of the data.'\n    return ConditionResult(ConditionCategory.PASS, details)",
        "mutated": [
            "def misclassified_samples_lower_than_condition(value: pd.DataFrame, misclassified_samples_threshold: float) -> ConditionResult:\n    if False:\n        i = 10\n    'Condition function that checks if the misclassified samples in the confusion matrix is below threshold.\\n\\n    Parameters\\n    ----------\\n    value: pd.DataFrame\\n        Dataframe containing the confusion matrix\\n    misclassified_samples_threshold: float\\n        Ratio of samples to be used for comparison in the condition (Value should be between 0 - 1 inclusive)\\n\\n    Raises\\n    ------\\n    DeepchecksValueError\\n        if the value of `misclassified_samples_threshold` parameter is not between 0 - 1 inclusive.\\n\\n    Returns\\n    -------\\n    ConditionResult\\n        - ConditionCategory.PASS, if all the misclassified samples in the confusion\\n        matrix are less than `misclassified_samples_threshold` ratio\\n        - ConditionCategory.FAIL, if the misclassified samples in the confusion matrix\\n        are more than the `misclassified_samples_threshold` ratio\\n    '\n    if misclassified_samples_threshold < 0 or misclassified_samples_threshold > 1:\n        raise DeepchecksValueError(f'Condition requires the parameter \"misclassified_samples_threshold\" to be between 0 and 1 inclusive but got {misclassified_samples_threshold}')\n    class_names = value.columns\n    value = value.to_numpy()\n    total_samples = np.sum(value)\n    thresh_samples = round(np.ceil(misclassified_samples_threshold * total_samples))\n    (m, n) = (value.shape[0], value.shape[1])\n    n_cells_above_thresh = 0\n    max_misclassified_cell_idx = (0, 1)\n    for i in range(m):\n        for j in range(n):\n            if i != j:\n                n_samples = value[i][j]\n                if n_samples > thresh_samples:\n                    n_cells_above_thresh += 1\n                    (x, y) = max_misclassified_cell_idx\n                    max_misclassified_samples = value[x][y]\n                    if n_samples > max_misclassified_samples:\n                        max_misclassified_cell_idx = (i, j)\n    if n_cells_above_thresh > 0:\n        (x, y) = max_misclassified_cell_idx\n        max_misclassified_samples = value[x][y]\n        max_misclassified_samples_ratio = max_misclassified_samples / total_samples\n        details = f'Detected {n_cells_above_thresh} misclassified confusion matrix cell(s) each one containing more than {format_percent(misclassified_samples_threshold)} of the data. Largest misclassified cell ({format_percent(max_misclassified_samples_ratio)} of the data) is samples with a true value of \"{class_names[x]}\" and a predicted value of \"{class_names[y]}\".'\n        return ConditionResult(ConditionCategory.FAIL, details)\n    details = f'All misclassified confusion matrix cells contain less than {format_percent(misclassified_samples_threshold)} of the data.'\n    return ConditionResult(ConditionCategory.PASS, details)",
            "def misclassified_samples_lower_than_condition(value: pd.DataFrame, misclassified_samples_threshold: float) -> ConditionResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Condition function that checks if the misclassified samples in the confusion matrix is below threshold.\\n\\n    Parameters\\n    ----------\\n    value: pd.DataFrame\\n        Dataframe containing the confusion matrix\\n    misclassified_samples_threshold: float\\n        Ratio of samples to be used for comparison in the condition (Value should be between 0 - 1 inclusive)\\n\\n    Raises\\n    ------\\n    DeepchecksValueError\\n        if the value of `misclassified_samples_threshold` parameter is not between 0 - 1 inclusive.\\n\\n    Returns\\n    -------\\n    ConditionResult\\n        - ConditionCategory.PASS, if all the misclassified samples in the confusion\\n        matrix are less than `misclassified_samples_threshold` ratio\\n        - ConditionCategory.FAIL, if the misclassified samples in the confusion matrix\\n        are more than the `misclassified_samples_threshold` ratio\\n    '\n    if misclassified_samples_threshold < 0 or misclassified_samples_threshold > 1:\n        raise DeepchecksValueError(f'Condition requires the parameter \"misclassified_samples_threshold\" to be between 0 and 1 inclusive but got {misclassified_samples_threshold}')\n    class_names = value.columns\n    value = value.to_numpy()\n    total_samples = np.sum(value)\n    thresh_samples = round(np.ceil(misclassified_samples_threshold * total_samples))\n    (m, n) = (value.shape[0], value.shape[1])\n    n_cells_above_thresh = 0\n    max_misclassified_cell_idx = (0, 1)\n    for i in range(m):\n        for j in range(n):\n            if i != j:\n                n_samples = value[i][j]\n                if n_samples > thresh_samples:\n                    n_cells_above_thresh += 1\n                    (x, y) = max_misclassified_cell_idx\n                    max_misclassified_samples = value[x][y]\n                    if n_samples > max_misclassified_samples:\n                        max_misclassified_cell_idx = (i, j)\n    if n_cells_above_thresh > 0:\n        (x, y) = max_misclassified_cell_idx\n        max_misclassified_samples = value[x][y]\n        max_misclassified_samples_ratio = max_misclassified_samples / total_samples\n        details = f'Detected {n_cells_above_thresh} misclassified confusion matrix cell(s) each one containing more than {format_percent(misclassified_samples_threshold)} of the data. Largest misclassified cell ({format_percent(max_misclassified_samples_ratio)} of the data) is samples with a true value of \"{class_names[x]}\" and a predicted value of \"{class_names[y]}\".'\n        return ConditionResult(ConditionCategory.FAIL, details)\n    details = f'All misclassified confusion matrix cells contain less than {format_percent(misclassified_samples_threshold)} of the data.'\n    return ConditionResult(ConditionCategory.PASS, details)",
            "def misclassified_samples_lower_than_condition(value: pd.DataFrame, misclassified_samples_threshold: float) -> ConditionResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Condition function that checks if the misclassified samples in the confusion matrix is below threshold.\\n\\n    Parameters\\n    ----------\\n    value: pd.DataFrame\\n        Dataframe containing the confusion matrix\\n    misclassified_samples_threshold: float\\n        Ratio of samples to be used for comparison in the condition (Value should be between 0 - 1 inclusive)\\n\\n    Raises\\n    ------\\n    DeepchecksValueError\\n        if the value of `misclassified_samples_threshold` parameter is not between 0 - 1 inclusive.\\n\\n    Returns\\n    -------\\n    ConditionResult\\n        - ConditionCategory.PASS, if all the misclassified samples in the confusion\\n        matrix are less than `misclassified_samples_threshold` ratio\\n        - ConditionCategory.FAIL, if the misclassified samples in the confusion matrix\\n        are more than the `misclassified_samples_threshold` ratio\\n    '\n    if misclassified_samples_threshold < 0 or misclassified_samples_threshold > 1:\n        raise DeepchecksValueError(f'Condition requires the parameter \"misclassified_samples_threshold\" to be between 0 and 1 inclusive but got {misclassified_samples_threshold}')\n    class_names = value.columns\n    value = value.to_numpy()\n    total_samples = np.sum(value)\n    thresh_samples = round(np.ceil(misclassified_samples_threshold * total_samples))\n    (m, n) = (value.shape[0], value.shape[1])\n    n_cells_above_thresh = 0\n    max_misclassified_cell_idx = (0, 1)\n    for i in range(m):\n        for j in range(n):\n            if i != j:\n                n_samples = value[i][j]\n                if n_samples > thresh_samples:\n                    n_cells_above_thresh += 1\n                    (x, y) = max_misclassified_cell_idx\n                    max_misclassified_samples = value[x][y]\n                    if n_samples > max_misclassified_samples:\n                        max_misclassified_cell_idx = (i, j)\n    if n_cells_above_thresh > 0:\n        (x, y) = max_misclassified_cell_idx\n        max_misclassified_samples = value[x][y]\n        max_misclassified_samples_ratio = max_misclassified_samples / total_samples\n        details = f'Detected {n_cells_above_thresh} misclassified confusion matrix cell(s) each one containing more than {format_percent(misclassified_samples_threshold)} of the data. Largest misclassified cell ({format_percent(max_misclassified_samples_ratio)} of the data) is samples with a true value of \"{class_names[x]}\" and a predicted value of \"{class_names[y]}\".'\n        return ConditionResult(ConditionCategory.FAIL, details)\n    details = f'All misclassified confusion matrix cells contain less than {format_percent(misclassified_samples_threshold)} of the data.'\n    return ConditionResult(ConditionCategory.PASS, details)",
            "def misclassified_samples_lower_than_condition(value: pd.DataFrame, misclassified_samples_threshold: float) -> ConditionResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Condition function that checks if the misclassified samples in the confusion matrix is below threshold.\\n\\n    Parameters\\n    ----------\\n    value: pd.DataFrame\\n        Dataframe containing the confusion matrix\\n    misclassified_samples_threshold: float\\n        Ratio of samples to be used for comparison in the condition (Value should be between 0 - 1 inclusive)\\n\\n    Raises\\n    ------\\n    DeepchecksValueError\\n        if the value of `misclassified_samples_threshold` parameter is not between 0 - 1 inclusive.\\n\\n    Returns\\n    -------\\n    ConditionResult\\n        - ConditionCategory.PASS, if all the misclassified samples in the confusion\\n        matrix are less than `misclassified_samples_threshold` ratio\\n        - ConditionCategory.FAIL, if the misclassified samples in the confusion matrix\\n        are more than the `misclassified_samples_threshold` ratio\\n    '\n    if misclassified_samples_threshold < 0 or misclassified_samples_threshold > 1:\n        raise DeepchecksValueError(f'Condition requires the parameter \"misclassified_samples_threshold\" to be between 0 and 1 inclusive but got {misclassified_samples_threshold}')\n    class_names = value.columns\n    value = value.to_numpy()\n    total_samples = np.sum(value)\n    thresh_samples = round(np.ceil(misclassified_samples_threshold * total_samples))\n    (m, n) = (value.shape[0], value.shape[1])\n    n_cells_above_thresh = 0\n    max_misclassified_cell_idx = (0, 1)\n    for i in range(m):\n        for j in range(n):\n            if i != j:\n                n_samples = value[i][j]\n                if n_samples > thresh_samples:\n                    n_cells_above_thresh += 1\n                    (x, y) = max_misclassified_cell_idx\n                    max_misclassified_samples = value[x][y]\n                    if n_samples > max_misclassified_samples:\n                        max_misclassified_cell_idx = (i, j)\n    if n_cells_above_thresh > 0:\n        (x, y) = max_misclassified_cell_idx\n        max_misclassified_samples = value[x][y]\n        max_misclassified_samples_ratio = max_misclassified_samples / total_samples\n        details = f'Detected {n_cells_above_thresh} misclassified confusion matrix cell(s) each one containing more than {format_percent(misclassified_samples_threshold)} of the data. Largest misclassified cell ({format_percent(max_misclassified_samples_ratio)} of the data) is samples with a true value of \"{class_names[x]}\" and a predicted value of \"{class_names[y]}\".'\n        return ConditionResult(ConditionCategory.FAIL, details)\n    details = f'All misclassified confusion matrix cells contain less than {format_percent(misclassified_samples_threshold)} of the data.'\n    return ConditionResult(ConditionCategory.PASS, details)",
            "def misclassified_samples_lower_than_condition(value: pd.DataFrame, misclassified_samples_threshold: float) -> ConditionResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Condition function that checks if the misclassified samples in the confusion matrix is below threshold.\\n\\n    Parameters\\n    ----------\\n    value: pd.DataFrame\\n        Dataframe containing the confusion matrix\\n    misclassified_samples_threshold: float\\n        Ratio of samples to be used for comparison in the condition (Value should be between 0 - 1 inclusive)\\n\\n    Raises\\n    ------\\n    DeepchecksValueError\\n        if the value of `misclassified_samples_threshold` parameter is not between 0 - 1 inclusive.\\n\\n    Returns\\n    -------\\n    ConditionResult\\n        - ConditionCategory.PASS, if all the misclassified samples in the confusion\\n        matrix are less than `misclassified_samples_threshold` ratio\\n        - ConditionCategory.FAIL, if the misclassified samples in the confusion matrix\\n        are more than the `misclassified_samples_threshold` ratio\\n    '\n    if misclassified_samples_threshold < 0 or misclassified_samples_threshold > 1:\n        raise DeepchecksValueError(f'Condition requires the parameter \"misclassified_samples_threshold\" to be between 0 and 1 inclusive but got {misclassified_samples_threshold}')\n    class_names = value.columns\n    value = value.to_numpy()\n    total_samples = np.sum(value)\n    thresh_samples = round(np.ceil(misclassified_samples_threshold * total_samples))\n    (m, n) = (value.shape[0], value.shape[1])\n    n_cells_above_thresh = 0\n    max_misclassified_cell_idx = (0, 1)\n    for i in range(m):\n        for j in range(n):\n            if i != j:\n                n_samples = value[i][j]\n                if n_samples > thresh_samples:\n                    n_cells_above_thresh += 1\n                    (x, y) = max_misclassified_cell_idx\n                    max_misclassified_samples = value[x][y]\n                    if n_samples > max_misclassified_samples:\n                        max_misclassified_cell_idx = (i, j)\n    if n_cells_above_thresh > 0:\n        (x, y) = max_misclassified_cell_idx\n        max_misclassified_samples = value[x][y]\n        max_misclassified_samples_ratio = max_misclassified_samples / total_samples\n        details = f'Detected {n_cells_above_thresh} misclassified confusion matrix cell(s) each one containing more than {format_percent(misclassified_samples_threshold)} of the data. Largest misclassified cell ({format_percent(max_misclassified_samples_ratio)} of the data) is samples with a true value of \"{class_names[x]}\" and a predicted value of \"{class_names[y]}\".'\n        return ConditionResult(ConditionCategory.FAIL, details)\n    details = f'All misclassified confusion matrix cells contain less than {format_percent(misclassified_samples_threshold)} of the data.'\n    return ConditionResult(ConditionCategory.PASS, details)"
        ]
    }
]