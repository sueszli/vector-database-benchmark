[
    {
        "func_name": "DistributedFairseqModel",
        "original": "def DistributedFairseqModel(args, model, process_group, device):\n    \"\"\"\n    Wrap a *model* to support distributed data parallel training.\n\n    This is similar to the built-in DistributedDataParallel, but allows\n    additional configuration of the DistributedDataParallel class to\n    use, and also provides easier access to the wrapped model by\n    forwarding requests for missing attributes to the wrapped model.\n\n    Args:\n        args (argparse.Namespace): fairseq args\n        model (BaseFairseqModel): model to wrap\n        process_group: the c10d process group to be used for distributed data\n            parallel all-reduction.\n        device: device to move model to\n    \"\"\"\n    assert isinstance(model, nn.Module)\n    if args.tpu:\n        wrapped_model = TPUDistributedDataParallel(module=model.to(device), process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'c10d', 'pytorch_ddp'}:\n        wrapped_model = DistributedDataParallel(module=model.to(device), device_ids=[args.device_id], output_device=args.device_id, broadcast_buffers=args.broadcast_buffers, bucket_cap_mb=args.bucket_cap_mb, process_group=process_group, find_unused_parameters=args.find_unused_parameters, gradient_as_bucket_view=args.gradient_as_bucket_view)\n        if args.ddp_comm_hook == 'fp16':\n            logger.info('enable fp16 communication hook in DDP')\n            try:\n                from torch.distributed.algorithms.ddp_comm_hooks import DDPCommHookType, register_ddp_comm_hook\n            except:\n                logger.error('Could not import from torch.distributed.algorithms.ddp_comm_hooks; you may need to update your pytorch version')\n                raise\n            register_ddp_comm_hook(DDPCommHookType.FP16_COMPRESS, wrapped_model)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'no_c10d', 'legacy_ddp'}:\n        wrapped_model = LegacyDistributedDataParallel(module=model.to(device), buffer_size=2 ** 28, process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'slowmo':\n        if _SLOWMO_DDP_DISABLED:\n            raise ImportError('Cannot find SlowMoDistributedDataParallel. Please install fairscale with: pip install fairscale')\n        if args.slowmo_momentum is None:\n            if args.distributed_world_size <= 16:\n                args.slowmo_momentum = 0.0\n            elif args.distributed_world_size <= 32:\n                args.slowmo_momentum = 0.2\n            elif args.distributed_world_size <= 64:\n                args.slowmo_momentum = 0.5\n            else:\n                args.slowmo_momentum = 0.6\n        slowmo_base_algorithm = SlowMoBaseAlgorithm[args.slowmo_base_algorithm.upper()]\n        wrapped_model = SlowMoDistributedDataParallel(module=model.to(device), broadcast_buffers=args.broadcast_buffers, nprocs_per_node=args.nprocs_per_node, slowmo_momentum=args.slowmo_momentum, slowmo_base_algorithm=slowmo_base_algorithm, localsgd_frequency=args.localsgd_frequency)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'fully_sharded':\n        try:\n            from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n        except ImportError:\n            raise ImportError('Cannot find FullyShardedDataParallel. Please install fairscale with: pip install fairscale')\n        assert isinstance(model, FSDP), 'expected model to already be wrapped in FSDP'\n        wrapped_model = model\n        if args.memory_efficient_fp16:\n            wrapped_model = wrapped_model.half()\n        if not args.cpu_offload:\n            wrapped_model = wrapped_model.to(device=device)\n    else:\n        raise ValueError('Unknown --ddp-backend: ' + args.ddp_backend)\n    if getattr(args, 'heartbeat_timeout', -1) > 0:\n        wrapped_model = DistributedTimeoutWrapper(wrapped_model, timeout=getattr(args, 'heartbeat_timeout', -1))\n    return wrapped_model",
        "mutated": [
            "def DistributedFairseqModel(args, model, process_group, device):\n    if False:\n        i = 10\n    '\\n    Wrap a *model* to support distributed data parallel training.\\n\\n    This is similar to the built-in DistributedDataParallel, but allows\\n    additional configuration of the DistributedDataParallel class to\\n    use, and also provides easier access to the wrapped model by\\n    forwarding requests for missing attributes to the wrapped model.\\n\\n    Args:\\n        args (argparse.Namespace): fairseq args\\n        model (BaseFairseqModel): model to wrap\\n        process_group: the c10d process group to be used for distributed data\\n            parallel all-reduction.\\n        device: device to move model to\\n    '\n    assert isinstance(model, nn.Module)\n    if args.tpu:\n        wrapped_model = TPUDistributedDataParallel(module=model.to(device), process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'c10d', 'pytorch_ddp'}:\n        wrapped_model = DistributedDataParallel(module=model.to(device), device_ids=[args.device_id], output_device=args.device_id, broadcast_buffers=args.broadcast_buffers, bucket_cap_mb=args.bucket_cap_mb, process_group=process_group, find_unused_parameters=args.find_unused_parameters, gradient_as_bucket_view=args.gradient_as_bucket_view)\n        if args.ddp_comm_hook == 'fp16':\n            logger.info('enable fp16 communication hook in DDP')\n            try:\n                from torch.distributed.algorithms.ddp_comm_hooks import DDPCommHookType, register_ddp_comm_hook\n            except:\n                logger.error('Could not import from torch.distributed.algorithms.ddp_comm_hooks; you may need to update your pytorch version')\n                raise\n            register_ddp_comm_hook(DDPCommHookType.FP16_COMPRESS, wrapped_model)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'no_c10d', 'legacy_ddp'}:\n        wrapped_model = LegacyDistributedDataParallel(module=model.to(device), buffer_size=2 ** 28, process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'slowmo':\n        if _SLOWMO_DDP_DISABLED:\n            raise ImportError('Cannot find SlowMoDistributedDataParallel. Please install fairscale with: pip install fairscale')\n        if args.slowmo_momentum is None:\n            if args.distributed_world_size <= 16:\n                args.slowmo_momentum = 0.0\n            elif args.distributed_world_size <= 32:\n                args.slowmo_momentum = 0.2\n            elif args.distributed_world_size <= 64:\n                args.slowmo_momentum = 0.5\n            else:\n                args.slowmo_momentum = 0.6\n        slowmo_base_algorithm = SlowMoBaseAlgorithm[args.slowmo_base_algorithm.upper()]\n        wrapped_model = SlowMoDistributedDataParallel(module=model.to(device), broadcast_buffers=args.broadcast_buffers, nprocs_per_node=args.nprocs_per_node, slowmo_momentum=args.slowmo_momentum, slowmo_base_algorithm=slowmo_base_algorithm, localsgd_frequency=args.localsgd_frequency)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'fully_sharded':\n        try:\n            from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n        except ImportError:\n            raise ImportError('Cannot find FullyShardedDataParallel. Please install fairscale with: pip install fairscale')\n        assert isinstance(model, FSDP), 'expected model to already be wrapped in FSDP'\n        wrapped_model = model\n        if args.memory_efficient_fp16:\n            wrapped_model = wrapped_model.half()\n        if not args.cpu_offload:\n            wrapped_model = wrapped_model.to(device=device)\n    else:\n        raise ValueError('Unknown --ddp-backend: ' + args.ddp_backend)\n    if getattr(args, 'heartbeat_timeout', -1) > 0:\n        wrapped_model = DistributedTimeoutWrapper(wrapped_model, timeout=getattr(args, 'heartbeat_timeout', -1))\n    return wrapped_model",
            "def DistributedFairseqModel(args, model, process_group, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Wrap a *model* to support distributed data parallel training.\\n\\n    This is similar to the built-in DistributedDataParallel, but allows\\n    additional configuration of the DistributedDataParallel class to\\n    use, and also provides easier access to the wrapped model by\\n    forwarding requests for missing attributes to the wrapped model.\\n\\n    Args:\\n        args (argparse.Namespace): fairseq args\\n        model (BaseFairseqModel): model to wrap\\n        process_group: the c10d process group to be used for distributed data\\n            parallel all-reduction.\\n        device: device to move model to\\n    '\n    assert isinstance(model, nn.Module)\n    if args.tpu:\n        wrapped_model = TPUDistributedDataParallel(module=model.to(device), process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'c10d', 'pytorch_ddp'}:\n        wrapped_model = DistributedDataParallel(module=model.to(device), device_ids=[args.device_id], output_device=args.device_id, broadcast_buffers=args.broadcast_buffers, bucket_cap_mb=args.bucket_cap_mb, process_group=process_group, find_unused_parameters=args.find_unused_parameters, gradient_as_bucket_view=args.gradient_as_bucket_view)\n        if args.ddp_comm_hook == 'fp16':\n            logger.info('enable fp16 communication hook in DDP')\n            try:\n                from torch.distributed.algorithms.ddp_comm_hooks import DDPCommHookType, register_ddp_comm_hook\n            except:\n                logger.error('Could not import from torch.distributed.algorithms.ddp_comm_hooks; you may need to update your pytorch version')\n                raise\n            register_ddp_comm_hook(DDPCommHookType.FP16_COMPRESS, wrapped_model)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'no_c10d', 'legacy_ddp'}:\n        wrapped_model = LegacyDistributedDataParallel(module=model.to(device), buffer_size=2 ** 28, process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'slowmo':\n        if _SLOWMO_DDP_DISABLED:\n            raise ImportError('Cannot find SlowMoDistributedDataParallel. Please install fairscale with: pip install fairscale')\n        if args.slowmo_momentum is None:\n            if args.distributed_world_size <= 16:\n                args.slowmo_momentum = 0.0\n            elif args.distributed_world_size <= 32:\n                args.slowmo_momentum = 0.2\n            elif args.distributed_world_size <= 64:\n                args.slowmo_momentum = 0.5\n            else:\n                args.slowmo_momentum = 0.6\n        slowmo_base_algorithm = SlowMoBaseAlgorithm[args.slowmo_base_algorithm.upper()]\n        wrapped_model = SlowMoDistributedDataParallel(module=model.to(device), broadcast_buffers=args.broadcast_buffers, nprocs_per_node=args.nprocs_per_node, slowmo_momentum=args.slowmo_momentum, slowmo_base_algorithm=slowmo_base_algorithm, localsgd_frequency=args.localsgd_frequency)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'fully_sharded':\n        try:\n            from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n        except ImportError:\n            raise ImportError('Cannot find FullyShardedDataParallel. Please install fairscale with: pip install fairscale')\n        assert isinstance(model, FSDP), 'expected model to already be wrapped in FSDP'\n        wrapped_model = model\n        if args.memory_efficient_fp16:\n            wrapped_model = wrapped_model.half()\n        if not args.cpu_offload:\n            wrapped_model = wrapped_model.to(device=device)\n    else:\n        raise ValueError('Unknown --ddp-backend: ' + args.ddp_backend)\n    if getattr(args, 'heartbeat_timeout', -1) > 0:\n        wrapped_model = DistributedTimeoutWrapper(wrapped_model, timeout=getattr(args, 'heartbeat_timeout', -1))\n    return wrapped_model",
            "def DistributedFairseqModel(args, model, process_group, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Wrap a *model* to support distributed data parallel training.\\n\\n    This is similar to the built-in DistributedDataParallel, but allows\\n    additional configuration of the DistributedDataParallel class to\\n    use, and also provides easier access to the wrapped model by\\n    forwarding requests for missing attributes to the wrapped model.\\n\\n    Args:\\n        args (argparse.Namespace): fairseq args\\n        model (BaseFairseqModel): model to wrap\\n        process_group: the c10d process group to be used for distributed data\\n            parallel all-reduction.\\n        device: device to move model to\\n    '\n    assert isinstance(model, nn.Module)\n    if args.tpu:\n        wrapped_model = TPUDistributedDataParallel(module=model.to(device), process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'c10d', 'pytorch_ddp'}:\n        wrapped_model = DistributedDataParallel(module=model.to(device), device_ids=[args.device_id], output_device=args.device_id, broadcast_buffers=args.broadcast_buffers, bucket_cap_mb=args.bucket_cap_mb, process_group=process_group, find_unused_parameters=args.find_unused_parameters, gradient_as_bucket_view=args.gradient_as_bucket_view)\n        if args.ddp_comm_hook == 'fp16':\n            logger.info('enable fp16 communication hook in DDP')\n            try:\n                from torch.distributed.algorithms.ddp_comm_hooks import DDPCommHookType, register_ddp_comm_hook\n            except:\n                logger.error('Could not import from torch.distributed.algorithms.ddp_comm_hooks; you may need to update your pytorch version')\n                raise\n            register_ddp_comm_hook(DDPCommHookType.FP16_COMPRESS, wrapped_model)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'no_c10d', 'legacy_ddp'}:\n        wrapped_model = LegacyDistributedDataParallel(module=model.to(device), buffer_size=2 ** 28, process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'slowmo':\n        if _SLOWMO_DDP_DISABLED:\n            raise ImportError('Cannot find SlowMoDistributedDataParallel. Please install fairscale with: pip install fairscale')\n        if args.slowmo_momentum is None:\n            if args.distributed_world_size <= 16:\n                args.slowmo_momentum = 0.0\n            elif args.distributed_world_size <= 32:\n                args.slowmo_momentum = 0.2\n            elif args.distributed_world_size <= 64:\n                args.slowmo_momentum = 0.5\n            else:\n                args.slowmo_momentum = 0.6\n        slowmo_base_algorithm = SlowMoBaseAlgorithm[args.slowmo_base_algorithm.upper()]\n        wrapped_model = SlowMoDistributedDataParallel(module=model.to(device), broadcast_buffers=args.broadcast_buffers, nprocs_per_node=args.nprocs_per_node, slowmo_momentum=args.slowmo_momentum, slowmo_base_algorithm=slowmo_base_algorithm, localsgd_frequency=args.localsgd_frequency)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'fully_sharded':\n        try:\n            from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n        except ImportError:\n            raise ImportError('Cannot find FullyShardedDataParallel. Please install fairscale with: pip install fairscale')\n        assert isinstance(model, FSDP), 'expected model to already be wrapped in FSDP'\n        wrapped_model = model\n        if args.memory_efficient_fp16:\n            wrapped_model = wrapped_model.half()\n        if not args.cpu_offload:\n            wrapped_model = wrapped_model.to(device=device)\n    else:\n        raise ValueError('Unknown --ddp-backend: ' + args.ddp_backend)\n    if getattr(args, 'heartbeat_timeout', -1) > 0:\n        wrapped_model = DistributedTimeoutWrapper(wrapped_model, timeout=getattr(args, 'heartbeat_timeout', -1))\n    return wrapped_model",
            "def DistributedFairseqModel(args, model, process_group, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Wrap a *model* to support distributed data parallel training.\\n\\n    This is similar to the built-in DistributedDataParallel, but allows\\n    additional configuration of the DistributedDataParallel class to\\n    use, and also provides easier access to the wrapped model by\\n    forwarding requests for missing attributes to the wrapped model.\\n\\n    Args:\\n        args (argparse.Namespace): fairseq args\\n        model (BaseFairseqModel): model to wrap\\n        process_group: the c10d process group to be used for distributed data\\n            parallel all-reduction.\\n        device: device to move model to\\n    '\n    assert isinstance(model, nn.Module)\n    if args.tpu:\n        wrapped_model = TPUDistributedDataParallel(module=model.to(device), process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'c10d', 'pytorch_ddp'}:\n        wrapped_model = DistributedDataParallel(module=model.to(device), device_ids=[args.device_id], output_device=args.device_id, broadcast_buffers=args.broadcast_buffers, bucket_cap_mb=args.bucket_cap_mb, process_group=process_group, find_unused_parameters=args.find_unused_parameters, gradient_as_bucket_view=args.gradient_as_bucket_view)\n        if args.ddp_comm_hook == 'fp16':\n            logger.info('enable fp16 communication hook in DDP')\n            try:\n                from torch.distributed.algorithms.ddp_comm_hooks import DDPCommHookType, register_ddp_comm_hook\n            except:\n                logger.error('Could not import from torch.distributed.algorithms.ddp_comm_hooks; you may need to update your pytorch version')\n                raise\n            register_ddp_comm_hook(DDPCommHookType.FP16_COMPRESS, wrapped_model)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'no_c10d', 'legacy_ddp'}:\n        wrapped_model = LegacyDistributedDataParallel(module=model.to(device), buffer_size=2 ** 28, process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'slowmo':\n        if _SLOWMO_DDP_DISABLED:\n            raise ImportError('Cannot find SlowMoDistributedDataParallel. Please install fairscale with: pip install fairscale')\n        if args.slowmo_momentum is None:\n            if args.distributed_world_size <= 16:\n                args.slowmo_momentum = 0.0\n            elif args.distributed_world_size <= 32:\n                args.slowmo_momentum = 0.2\n            elif args.distributed_world_size <= 64:\n                args.slowmo_momentum = 0.5\n            else:\n                args.slowmo_momentum = 0.6\n        slowmo_base_algorithm = SlowMoBaseAlgorithm[args.slowmo_base_algorithm.upper()]\n        wrapped_model = SlowMoDistributedDataParallel(module=model.to(device), broadcast_buffers=args.broadcast_buffers, nprocs_per_node=args.nprocs_per_node, slowmo_momentum=args.slowmo_momentum, slowmo_base_algorithm=slowmo_base_algorithm, localsgd_frequency=args.localsgd_frequency)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'fully_sharded':\n        try:\n            from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n        except ImportError:\n            raise ImportError('Cannot find FullyShardedDataParallel. Please install fairscale with: pip install fairscale')\n        assert isinstance(model, FSDP), 'expected model to already be wrapped in FSDP'\n        wrapped_model = model\n        if args.memory_efficient_fp16:\n            wrapped_model = wrapped_model.half()\n        if not args.cpu_offload:\n            wrapped_model = wrapped_model.to(device=device)\n    else:\n        raise ValueError('Unknown --ddp-backend: ' + args.ddp_backend)\n    if getattr(args, 'heartbeat_timeout', -1) > 0:\n        wrapped_model = DistributedTimeoutWrapper(wrapped_model, timeout=getattr(args, 'heartbeat_timeout', -1))\n    return wrapped_model",
            "def DistributedFairseqModel(args, model, process_group, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Wrap a *model* to support distributed data parallel training.\\n\\n    This is similar to the built-in DistributedDataParallel, but allows\\n    additional configuration of the DistributedDataParallel class to\\n    use, and also provides easier access to the wrapped model by\\n    forwarding requests for missing attributes to the wrapped model.\\n\\n    Args:\\n        args (argparse.Namespace): fairseq args\\n        model (BaseFairseqModel): model to wrap\\n        process_group: the c10d process group to be used for distributed data\\n            parallel all-reduction.\\n        device: device to move model to\\n    '\n    assert isinstance(model, nn.Module)\n    if args.tpu:\n        wrapped_model = TPUDistributedDataParallel(module=model.to(device), process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'c10d', 'pytorch_ddp'}:\n        wrapped_model = DistributedDataParallel(module=model.to(device), device_ids=[args.device_id], output_device=args.device_id, broadcast_buffers=args.broadcast_buffers, bucket_cap_mb=args.bucket_cap_mb, process_group=process_group, find_unused_parameters=args.find_unused_parameters, gradient_as_bucket_view=args.gradient_as_bucket_view)\n        if args.ddp_comm_hook == 'fp16':\n            logger.info('enable fp16 communication hook in DDP')\n            try:\n                from torch.distributed.algorithms.ddp_comm_hooks import DDPCommHookType, register_ddp_comm_hook\n            except:\n                logger.error('Could not import from torch.distributed.algorithms.ddp_comm_hooks; you may need to update your pytorch version')\n                raise\n            register_ddp_comm_hook(DDPCommHookType.FP16_COMPRESS, wrapped_model)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend in {'no_c10d', 'legacy_ddp'}:\n        wrapped_model = LegacyDistributedDataParallel(module=model.to(device), buffer_size=2 ** 28, process_group=process_group)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'slowmo':\n        if _SLOWMO_DDP_DISABLED:\n            raise ImportError('Cannot find SlowMoDistributedDataParallel. Please install fairscale with: pip install fairscale')\n        if args.slowmo_momentum is None:\n            if args.distributed_world_size <= 16:\n                args.slowmo_momentum = 0.0\n            elif args.distributed_world_size <= 32:\n                args.slowmo_momentum = 0.2\n            elif args.distributed_world_size <= 64:\n                args.slowmo_momentum = 0.5\n            else:\n                args.slowmo_momentum = 0.6\n        slowmo_base_algorithm = SlowMoBaseAlgorithm[args.slowmo_base_algorithm.upper()]\n        wrapped_model = SlowMoDistributedDataParallel(module=model.to(device), broadcast_buffers=args.broadcast_buffers, nprocs_per_node=args.nprocs_per_node, slowmo_momentum=args.slowmo_momentum, slowmo_base_algorithm=slowmo_base_algorithm, localsgd_frequency=args.localsgd_frequency)\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == 'fully_sharded':\n        try:\n            from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n        except ImportError:\n            raise ImportError('Cannot find FullyShardedDataParallel. Please install fairscale with: pip install fairscale')\n        assert isinstance(model, FSDP), 'expected model to already be wrapped in FSDP'\n        wrapped_model = model\n        if args.memory_efficient_fp16:\n            wrapped_model = wrapped_model.half()\n        if not args.cpu_offload:\n            wrapped_model = wrapped_model.to(device=device)\n    else:\n        raise ValueError('Unknown --ddp-backend: ' + args.ddp_backend)\n    if getattr(args, 'heartbeat_timeout', -1) > 0:\n        wrapped_model = DistributedTimeoutWrapper(wrapped_model, timeout=getattr(args, 'heartbeat_timeout', -1))\n    return wrapped_model"
        ]
    }
]