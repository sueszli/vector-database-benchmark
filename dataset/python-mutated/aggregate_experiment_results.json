[
    {
        "func_name": "make_csv_string",
        "original": "def make_csv_string(table):\n    \"\"\"Convert 2D list to CSV string.\"\"\"\n    s = StringIO.StringIO()\n    writer = csv.writer(s)\n    writer.writerows(table)\n    value = s.getvalue()\n    s.close()\n    return value",
        "mutated": [
            "def make_csv_string(table):\n    if False:\n        i = 10\n    'Convert 2D list to CSV string.'\n    s = StringIO.StringIO()\n    writer = csv.writer(s)\n    writer.writerows(table)\n    value = s.getvalue()\n    s.close()\n    return value",
            "def make_csv_string(table):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convert 2D list to CSV string.'\n    s = StringIO.StringIO()\n    writer = csv.writer(s)\n    writer.writerows(table)\n    value = s.getvalue()\n    s.close()\n    return value",
            "def make_csv_string(table):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convert 2D list to CSV string.'\n    s = StringIO.StringIO()\n    writer = csv.writer(s)\n    writer.writerows(table)\n    value = s.getvalue()\n    s.close()\n    return value",
            "def make_csv_string(table):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convert 2D list to CSV string.'\n    s = StringIO.StringIO()\n    writer = csv.writer(s)\n    writer.writerows(table)\n    value = s.getvalue()\n    s.close()\n    return value",
            "def make_csv_string(table):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convert 2D list to CSV string.'\n    s = StringIO.StringIO()\n    writer = csv.writer(s)\n    writer.writerows(table)\n    value = s.getvalue()\n    s.close()\n    return value"
        ]
    },
    {
        "func_name": "process_results",
        "original": "def process_results(metrics):\n    \"\"\"Extract useful information from given metrics.\n\n  Args:\n    metrics: List of results dicts. These should have been written to disk by\n        training jobs.\n\n  Returns:\n    Dict mapping stats names to values.\n\n  Raises:\n    ValueError: If max_npe or max_global_repetitions values are inconsistant\n        across dicts in the `metrics` list.\n  \"\"\"\n    count = len(metrics)\n    success_count = 0\n    total_npe = 0\n    success_npe = 0\n    max_npe = 0\n    max_repetitions = 0\n    for metric_dict in metrics:\n        if not max_npe:\n            max_npe = metric_dict['max_npe']\n        elif max_npe != metric_dict['max_npe']:\n            raise ValueError('Invalid experiment. Different reps have different max-NPE settings.')\n        if not max_repetitions:\n            max_repetitions = metric_dict['max_global_repetitions']\n        elif max_repetitions != metric_dict['max_global_repetitions']:\n            raise ValueError('Invalid experiment. Different reps have different num-repetition settings.')\n        if metric_dict['found_solution']:\n            success_count += 1\n            success_npe += metric_dict['npe']\n        total_npe += metric_dict['npe']\n    stats = {}\n    stats['max_npe'] = max_npe\n    stats['max_repetitions'] = max_repetitions\n    stats['repetitions'] = count\n    stats['successes'] = success_count\n    stats['failures'] = count - success_count\n    stats['success_npe'] = success_npe\n    stats['total_npe'] = total_npe\n    if success_count:\n        stats['avg_success_npe'] = stats['success_npe'] / float(success_count)\n    else:\n        stats['avg_success_npe'] = 0.0\n    if count:\n        stats['success_rate'] = success_count / float(count)\n        stats['avg_total_npe'] = stats['total_npe'] / float(count)\n    else:\n        stats['success_rate'] = 0.0\n        stats['avg_total_npe'] = 0.0\n    return stats",
        "mutated": [
            "def process_results(metrics):\n    if False:\n        i = 10\n    'Extract useful information from given metrics.\\n\\n  Args:\\n    metrics: List of results dicts. These should have been written to disk by\\n        training jobs.\\n\\n  Returns:\\n    Dict mapping stats names to values.\\n\\n  Raises:\\n    ValueError: If max_npe or max_global_repetitions values are inconsistant\\n        across dicts in the `metrics` list.\\n  '\n    count = len(metrics)\n    success_count = 0\n    total_npe = 0\n    success_npe = 0\n    max_npe = 0\n    max_repetitions = 0\n    for metric_dict in metrics:\n        if not max_npe:\n            max_npe = metric_dict['max_npe']\n        elif max_npe != metric_dict['max_npe']:\n            raise ValueError('Invalid experiment. Different reps have different max-NPE settings.')\n        if not max_repetitions:\n            max_repetitions = metric_dict['max_global_repetitions']\n        elif max_repetitions != metric_dict['max_global_repetitions']:\n            raise ValueError('Invalid experiment. Different reps have different num-repetition settings.')\n        if metric_dict['found_solution']:\n            success_count += 1\n            success_npe += metric_dict['npe']\n        total_npe += metric_dict['npe']\n    stats = {}\n    stats['max_npe'] = max_npe\n    stats['max_repetitions'] = max_repetitions\n    stats['repetitions'] = count\n    stats['successes'] = success_count\n    stats['failures'] = count - success_count\n    stats['success_npe'] = success_npe\n    stats['total_npe'] = total_npe\n    if success_count:\n        stats['avg_success_npe'] = stats['success_npe'] / float(success_count)\n    else:\n        stats['avg_success_npe'] = 0.0\n    if count:\n        stats['success_rate'] = success_count / float(count)\n        stats['avg_total_npe'] = stats['total_npe'] / float(count)\n    else:\n        stats['success_rate'] = 0.0\n        stats['avg_total_npe'] = 0.0\n    return stats",
            "def process_results(metrics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Extract useful information from given metrics.\\n\\n  Args:\\n    metrics: List of results dicts. These should have been written to disk by\\n        training jobs.\\n\\n  Returns:\\n    Dict mapping stats names to values.\\n\\n  Raises:\\n    ValueError: If max_npe or max_global_repetitions values are inconsistant\\n        across dicts in the `metrics` list.\\n  '\n    count = len(metrics)\n    success_count = 0\n    total_npe = 0\n    success_npe = 0\n    max_npe = 0\n    max_repetitions = 0\n    for metric_dict in metrics:\n        if not max_npe:\n            max_npe = metric_dict['max_npe']\n        elif max_npe != metric_dict['max_npe']:\n            raise ValueError('Invalid experiment. Different reps have different max-NPE settings.')\n        if not max_repetitions:\n            max_repetitions = metric_dict['max_global_repetitions']\n        elif max_repetitions != metric_dict['max_global_repetitions']:\n            raise ValueError('Invalid experiment. Different reps have different num-repetition settings.')\n        if metric_dict['found_solution']:\n            success_count += 1\n            success_npe += metric_dict['npe']\n        total_npe += metric_dict['npe']\n    stats = {}\n    stats['max_npe'] = max_npe\n    stats['max_repetitions'] = max_repetitions\n    stats['repetitions'] = count\n    stats['successes'] = success_count\n    stats['failures'] = count - success_count\n    stats['success_npe'] = success_npe\n    stats['total_npe'] = total_npe\n    if success_count:\n        stats['avg_success_npe'] = stats['success_npe'] / float(success_count)\n    else:\n        stats['avg_success_npe'] = 0.0\n    if count:\n        stats['success_rate'] = success_count / float(count)\n        stats['avg_total_npe'] = stats['total_npe'] / float(count)\n    else:\n        stats['success_rate'] = 0.0\n        stats['avg_total_npe'] = 0.0\n    return stats",
            "def process_results(metrics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Extract useful information from given metrics.\\n\\n  Args:\\n    metrics: List of results dicts. These should have been written to disk by\\n        training jobs.\\n\\n  Returns:\\n    Dict mapping stats names to values.\\n\\n  Raises:\\n    ValueError: If max_npe or max_global_repetitions values are inconsistant\\n        across dicts in the `metrics` list.\\n  '\n    count = len(metrics)\n    success_count = 0\n    total_npe = 0\n    success_npe = 0\n    max_npe = 0\n    max_repetitions = 0\n    for metric_dict in metrics:\n        if not max_npe:\n            max_npe = metric_dict['max_npe']\n        elif max_npe != metric_dict['max_npe']:\n            raise ValueError('Invalid experiment. Different reps have different max-NPE settings.')\n        if not max_repetitions:\n            max_repetitions = metric_dict['max_global_repetitions']\n        elif max_repetitions != metric_dict['max_global_repetitions']:\n            raise ValueError('Invalid experiment. Different reps have different num-repetition settings.')\n        if metric_dict['found_solution']:\n            success_count += 1\n            success_npe += metric_dict['npe']\n        total_npe += metric_dict['npe']\n    stats = {}\n    stats['max_npe'] = max_npe\n    stats['max_repetitions'] = max_repetitions\n    stats['repetitions'] = count\n    stats['successes'] = success_count\n    stats['failures'] = count - success_count\n    stats['success_npe'] = success_npe\n    stats['total_npe'] = total_npe\n    if success_count:\n        stats['avg_success_npe'] = stats['success_npe'] / float(success_count)\n    else:\n        stats['avg_success_npe'] = 0.0\n    if count:\n        stats['success_rate'] = success_count / float(count)\n        stats['avg_total_npe'] = stats['total_npe'] / float(count)\n    else:\n        stats['success_rate'] = 0.0\n        stats['avg_total_npe'] = 0.0\n    return stats",
            "def process_results(metrics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Extract useful information from given metrics.\\n\\n  Args:\\n    metrics: List of results dicts. These should have been written to disk by\\n        training jobs.\\n\\n  Returns:\\n    Dict mapping stats names to values.\\n\\n  Raises:\\n    ValueError: If max_npe or max_global_repetitions values are inconsistant\\n        across dicts in the `metrics` list.\\n  '\n    count = len(metrics)\n    success_count = 0\n    total_npe = 0\n    success_npe = 0\n    max_npe = 0\n    max_repetitions = 0\n    for metric_dict in metrics:\n        if not max_npe:\n            max_npe = metric_dict['max_npe']\n        elif max_npe != metric_dict['max_npe']:\n            raise ValueError('Invalid experiment. Different reps have different max-NPE settings.')\n        if not max_repetitions:\n            max_repetitions = metric_dict['max_global_repetitions']\n        elif max_repetitions != metric_dict['max_global_repetitions']:\n            raise ValueError('Invalid experiment. Different reps have different num-repetition settings.')\n        if metric_dict['found_solution']:\n            success_count += 1\n            success_npe += metric_dict['npe']\n        total_npe += metric_dict['npe']\n    stats = {}\n    stats['max_npe'] = max_npe\n    stats['max_repetitions'] = max_repetitions\n    stats['repetitions'] = count\n    stats['successes'] = success_count\n    stats['failures'] = count - success_count\n    stats['success_npe'] = success_npe\n    stats['total_npe'] = total_npe\n    if success_count:\n        stats['avg_success_npe'] = stats['success_npe'] / float(success_count)\n    else:\n        stats['avg_success_npe'] = 0.0\n    if count:\n        stats['success_rate'] = success_count / float(count)\n        stats['avg_total_npe'] = stats['total_npe'] / float(count)\n    else:\n        stats['success_rate'] = 0.0\n        stats['avg_total_npe'] = 0.0\n    return stats",
            "def process_results(metrics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Extract useful information from given metrics.\\n\\n  Args:\\n    metrics: List of results dicts. These should have been written to disk by\\n        training jobs.\\n\\n  Returns:\\n    Dict mapping stats names to values.\\n\\n  Raises:\\n    ValueError: If max_npe or max_global_repetitions values are inconsistant\\n        across dicts in the `metrics` list.\\n  '\n    count = len(metrics)\n    success_count = 0\n    total_npe = 0\n    success_npe = 0\n    max_npe = 0\n    max_repetitions = 0\n    for metric_dict in metrics:\n        if not max_npe:\n            max_npe = metric_dict['max_npe']\n        elif max_npe != metric_dict['max_npe']:\n            raise ValueError('Invalid experiment. Different reps have different max-NPE settings.')\n        if not max_repetitions:\n            max_repetitions = metric_dict['max_global_repetitions']\n        elif max_repetitions != metric_dict['max_global_repetitions']:\n            raise ValueError('Invalid experiment. Different reps have different num-repetition settings.')\n        if metric_dict['found_solution']:\n            success_count += 1\n            success_npe += metric_dict['npe']\n        total_npe += metric_dict['npe']\n    stats = {}\n    stats['max_npe'] = max_npe\n    stats['max_repetitions'] = max_repetitions\n    stats['repetitions'] = count\n    stats['successes'] = success_count\n    stats['failures'] = count - success_count\n    stats['success_npe'] = success_npe\n    stats['total_npe'] = total_npe\n    if success_count:\n        stats['avg_success_npe'] = stats['success_npe'] / float(success_count)\n    else:\n        stats['avg_success_npe'] = 0.0\n    if count:\n        stats['success_rate'] = success_count / float(count)\n        stats['avg_total_npe'] = stats['total_npe'] / float(count)\n    else:\n        stats['success_rate'] = 0.0\n        stats['avg_total_npe'] = 0.0\n    return stats"
        ]
    },
    {
        "func_name": "get_results_for_experiment",
        "original": "def get_results_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc='v0', name_prefix='bf_rl_paper', extra_desc=''):\n    \"\"\"Get and process results for a given experiment.\n\n  An experiment is a set of runs with the same hyperparameters and environment.\n  It is uniquely specified by a (task_name, model_type, max_npe) triple, as\n  well as an optional description.\n\n  We assume that each experiment has a folder with the same name as the job that\n  ran the experiment. The name is computed by\n  \"%name_prefix%.%desc%-%max_npe%_%task_name%\".\n\n  Args:\n    models_dir: Parent directory containing experiment folders.\n    task_name: String name of task (the coding env). See code_tasks.py or\n        run_eval_tasks.py\n    model_type: Name of the algorithm, such as 'pg', 'topk', 'ga', 'rand'.\n    max_npe: String SI unit representation of the maximum NPE threshold for the\n        experiment. For example, \"5M\" means 5 million.\n    desc: Description.\n    name_prefix: Prefix of job names. Normally leave this as default.\n    extra_desc: Optional extra description at the end of the job name.\n\n  Returns:\n    ProcessedResults namedtuple instance, containing\n    metrics: Raw dicts read from disk.\n    processed: Stats computed by `process_results`.\n\n  Raises:\n    ValueError: If max_npe in the metrics does not match NPE in the experiment\n        folder name.\n  \"\"\"\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    results = results_lib.Results(os.path.join(models_dir, folder))\n    (metrics, _) = results.read_all()\n    processed = process_results(metrics)\n    if not np.isclose(processed['max_npe'], misc.si_to_int(max_npe)) and processed['repetitions']:\n        raise ValueError('Invalid experiment. Max-NPE setting does not match expected max-NPE in experiment name.')\n    return ProcessedResults(metrics=metrics, processed=processed)",
        "mutated": [
            "def get_results_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc='v0', name_prefix='bf_rl_paper', extra_desc=''):\n    if False:\n        i = 10\n    'Get and process results for a given experiment.\\n\\n  An experiment is a set of runs with the same hyperparameters and environment.\\n  It is uniquely specified by a (task_name, model_type, max_npe) triple, as\\n  well as an optional description.\\n\\n  We assume that each experiment has a folder with the same name as the job that\\n  ran the experiment. The name is computed by\\n  \"%name_prefix%.%desc%-%max_npe%_%task_name%\".\\n\\n  Args:\\n    models_dir: Parent directory containing experiment folders.\\n    task_name: String name of task (the coding env). See code_tasks.py or\\n        run_eval_tasks.py\\n    model_type: Name of the algorithm, such as \\'pg\\', \\'topk\\', \\'ga\\', \\'rand\\'.\\n    max_npe: String SI unit representation of the maximum NPE threshold for the\\n        experiment. For example, \"5M\" means 5 million.\\n    desc: Description.\\n    name_prefix: Prefix of job names. Normally leave this as default.\\n    extra_desc: Optional extra description at the end of the job name.\\n\\n  Returns:\\n    ProcessedResults namedtuple instance, containing\\n    metrics: Raw dicts read from disk.\\n    processed: Stats computed by `process_results`.\\n\\n  Raises:\\n    ValueError: If max_npe in the metrics does not match NPE in the experiment\\n        folder name.\\n  '\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    results = results_lib.Results(os.path.join(models_dir, folder))\n    (metrics, _) = results.read_all()\n    processed = process_results(metrics)\n    if not np.isclose(processed['max_npe'], misc.si_to_int(max_npe)) and processed['repetitions']:\n        raise ValueError('Invalid experiment. Max-NPE setting does not match expected max-NPE in experiment name.')\n    return ProcessedResults(metrics=metrics, processed=processed)",
            "def get_results_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc='v0', name_prefix='bf_rl_paper', extra_desc=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get and process results for a given experiment.\\n\\n  An experiment is a set of runs with the same hyperparameters and environment.\\n  It is uniquely specified by a (task_name, model_type, max_npe) triple, as\\n  well as an optional description.\\n\\n  We assume that each experiment has a folder with the same name as the job that\\n  ran the experiment. The name is computed by\\n  \"%name_prefix%.%desc%-%max_npe%_%task_name%\".\\n\\n  Args:\\n    models_dir: Parent directory containing experiment folders.\\n    task_name: String name of task (the coding env). See code_tasks.py or\\n        run_eval_tasks.py\\n    model_type: Name of the algorithm, such as \\'pg\\', \\'topk\\', \\'ga\\', \\'rand\\'.\\n    max_npe: String SI unit representation of the maximum NPE threshold for the\\n        experiment. For example, \"5M\" means 5 million.\\n    desc: Description.\\n    name_prefix: Prefix of job names. Normally leave this as default.\\n    extra_desc: Optional extra description at the end of the job name.\\n\\n  Returns:\\n    ProcessedResults namedtuple instance, containing\\n    metrics: Raw dicts read from disk.\\n    processed: Stats computed by `process_results`.\\n\\n  Raises:\\n    ValueError: If max_npe in the metrics does not match NPE in the experiment\\n        folder name.\\n  '\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    results = results_lib.Results(os.path.join(models_dir, folder))\n    (metrics, _) = results.read_all()\n    processed = process_results(metrics)\n    if not np.isclose(processed['max_npe'], misc.si_to_int(max_npe)) and processed['repetitions']:\n        raise ValueError('Invalid experiment. Max-NPE setting does not match expected max-NPE in experiment name.')\n    return ProcessedResults(metrics=metrics, processed=processed)",
            "def get_results_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc='v0', name_prefix='bf_rl_paper', extra_desc=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get and process results for a given experiment.\\n\\n  An experiment is a set of runs with the same hyperparameters and environment.\\n  It is uniquely specified by a (task_name, model_type, max_npe) triple, as\\n  well as an optional description.\\n\\n  We assume that each experiment has a folder with the same name as the job that\\n  ran the experiment. The name is computed by\\n  \"%name_prefix%.%desc%-%max_npe%_%task_name%\".\\n\\n  Args:\\n    models_dir: Parent directory containing experiment folders.\\n    task_name: String name of task (the coding env). See code_tasks.py or\\n        run_eval_tasks.py\\n    model_type: Name of the algorithm, such as \\'pg\\', \\'topk\\', \\'ga\\', \\'rand\\'.\\n    max_npe: String SI unit representation of the maximum NPE threshold for the\\n        experiment. For example, \"5M\" means 5 million.\\n    desc: Description.\\n    name_prefix: Prefix of job names. Normally leave this as default.\\n    extra_desc: Optional extra description at the end of the job name.\\n\\n  Returns:\\n    ProcessedResults namedtuple instance, containing\\n    metrics: Raw dicts read from disk.\\n    processed: Stats computed by `process_results`.\\n\\n  Raises:\\n    ValueError: If max_npe in the metrics does not match NPE in the experiment\\n        folder name.\\n  '\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    results = results_lib.Results(os.path.join(models_dir, folder))\n    (metrics, _) = results.read_all()\n    processed = process_results(metrics)\n    if not np.isclose(processed['max_npe'], misc.si_to_int(max_npe)) and processed['repetitions']:\n        raise ValueError('Invalid experiment. Max-NPE setting does not match expected max-NPE in experiment name.')\n    return ProcessedResults(metrics=metrics, processed=processed)",
            "def get_results_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc='v0', name_prefix='bf_rl_paper', extra_desc=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get and process results for a given experiment.\\n\\n  An experiment is a set of runs with the same hyperparameters and environment.\\n  It is uniquely specified by a (task_name, model_type, max_npe) triple, as\\n  well as an optional description.\\n\\n  We assume that each experiment has a folder with the same name as the job that\\n  ran the experiment. The name is computed by\\n  \"%name_prefix%.%desc%-%max_npe%_%task_name%\".\\n\\n  Args:\\n    models_dir: Parent directory containing experiment folders.\\n    task_name: String name of task (the coding env). See code_tasks.py or\\n        run_eval_tasks.py\\n    model_type: Name of the algorithm, such as \\'pg\\', \\'topk\\', \\'ga\\', \\'rand\\'.\\n    max_npe: String SI unit representation of the maximum NPE threshold for the\\n        experiment. For example, \"5M\" means 5 million.\\n    desc: Description.\\n    name_prefix: Prefix of job names. Normally leave this as default.\\n    extra_desc: Optional extra description at the end of the job name.\\n\\n  Returns:\\n    ProcessedResults namedtuple instance, containing\\n    metrics: Raw dicts read from disk.\\n    processed: Stats computed by `process_results`.\\n\\n  Raises:\\n    ValueError: If max_npe in the metrics does not match NPE in the experiment\\n        folder name.\\n  '\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    results = results_lib.Results(os.path.join(models_dir, folder))\n    (metrics, _) = results.read_all()\n    processed = process_results(metrics)\n    if not np.isclose(processed['max_npe'], misc.si_to_int(max_npe)) and processed['repetitions']:\n        raise ValueError('Invalid experiment. Max-NPE setting does not match expected max-NPE in experiment name.')\n    return ProcessedResults(metrics=metrics, processed=processed)",
            "def get_results_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc='v0', name_prefix='bf_rl_paper', extra_desc=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get and process results for a given experiment.\\n\\n  An experiment is a set of runs with the same hyperparameters and environment.\\n  It is uniquely specified by a (task_name, model_type, max_npe) triple, as\\n  well as an optional description.\\n\\n  We assume that each experiment has a folder with the same name as the job that\\n  ran the experiment. The name is computed by\\n  \"%name_prefix%.%desc%-%max_npe%_%task_name%\".\\n\\n  Args:\\n    models_dir: Parent directory containing experiment folders.\\n    task_name: String name of task (the coding env). See code_tasks.py or\\n        run_eval_tasks.py\\n    model_type: Name of the algorithm, such as \\'pg\\', \\'topk\\', \\'ga\\', \\'rand\\'.\\n    max_npe: String SI unit representation of the maximum NPE threshold for the\\n        experiment. For example, \"5M\" means 5 million.\\n    desc: Description.\\n    name_prefix: Prefix of job names. Normally leave this as default.\\n    extra_desc: Optional extra description at the end of the job name.\\n\\n  Returns:\\n    ProcessedResults namedtuple instance, containing\\n    metrics: Raw dicts read from disk.\\n    processed: Stats computed by `process_results`.\\n\\n  Raises:\\n    ValueError: If max_npe in the metrics does not match NPE in the experiment\\n        folder name.\\n  '\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    results = results_lib.Results(os.path.join(models_dir, folder))\n    (metrics, _) = results.read_all()\n    processed = process_results(metrics)\n    if not np.isclose(processed['max_npe'], misc.si_to_int(max_npe)) and processed['repetitions']:\n        raise ValueError('Invalid experiment. Max-NPE setting does not match expected max-NPE in experiment name.')\n    return ProcessedResults(metrics=metrics, processed=processed)"
        ]
    },
    {
        "func_name": "get_best_code_for_experiment",
        "original": "def get_best_code_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc=0, name_prefix='bf_rl_paper', extra_desc=''):\n    \"\"\"Like `get_results_for_experiment`, but fetches the code solutions.\"\"\"\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    log_dir = os.path.join(models_dir, folder, 'logs')\n    search_regex = '^solutions_([0-9])+\\\\.txt$'\n    try:\n        all_children = tf.gfile.ListDirectory(log_dir)\n    except tf.errors.NotFoundError:\n        return BestCodeResults(code=None, reward=0.0, npe=0, folder=folder, finished=False, error=BestCodeResultError.experiment_does_not_exist)\n    solution_files = [fname for fname in all_children if re.search(search_regex, fname)]\n    max_reward = 0.0\n    npe = 0\n    best_code = None\n    for fname in solution_files:\n        with tf.gfile.FastGFile(os.path.join(log_dir, fname), 'r') as reader:\n            results = [ast.literal_eval(entry) for entry in reader]\n        for res in results:\n            if res['reward'] > max_reward:\n                best_code = res['code']\n                max_reward = res['reward']\n                npe = res['npe']\n    error = BestCodeResultError.success if best_code else BestCodeResultError.no_solution_found\n    try:\n        with tf.gfile.FastGFile(os.path.join(log_dir, 'status.txt'), 'r') as f:\n            finished = f.read().lower().strip() == 'done'\n    except tf.errors.NotFoundError:\n        finished = False\n    return BestCodeResults(code=best_code, reward=max_reward, npe=npe, folder=folder, finished=finished, error=error)",
        "mutated": [
            "def get_best_code_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc=0, name_prefix='bf_rl_paper', extra_desc=''):\n    if False:\n        i = 10\n    'Like `get_results_for_experiment`, but fetches the code solutions.'\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    log_dir = os.path.join(models_dir, folder, 'logs')\n    search_regex = '^solutions_([0-9])+\\\\.txt$'\n    try:\n        all_children = tf.gfile.ListDirectory(log_dir)\n    except tf.errors.NotFoundError:\n        return BestCodeResults(code=None, reward=0.0, npe=0, folder=folder, finished=False, error=BestCodeResultError.experiment_does_not_exist)\n    solution_files = [fname for fname in all_children if re.search(search_regex, fname)]\n    max_reward = 0.0\n    npe = 0\n    best_code = None\n    for fname in solution_files:\n        with tf.gfile.FastGFile(os.path.join(log_dir, fname), 'r') as reader:\n            results = [ast.literal_eval(entry) for entry in reader]\n        for res in results:\n            if res['reward'] > max_reward:\n                best_code = res['code']\n                max_reward = res['reward']\n                npe = res['npe']\n    error = BestCodeResultError.success if best_code else BestCodeResultError.no_solution_found\n    try:\n        with tf.gfile.FastGFile(os.path.join(log_dir, 'status.txt'), 'r') as f:\n            finished = f.read().lower().strip() == 'done'\n    except tf.errors.NotFoundError:\n        finished = False\n    return BestCodeResults(code=best_code, reward=max_reward, npe=npe, folder=folder, finished=finished, error=error)",
            "def get_best_code_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc=0, name_prefix='bf_rl_paper', extra_desc=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Like `get_results_for_experiment`, but fetches the code solutions.'\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    log_dir = os.path.join(models_dir, folder, 'logs')\n    search_regex = '^solutions_([0-9])+\\\\.txt$'\n    try:\n        all_children = tf.gfile.ListDirectory(log_dir)\n    except tf.errors.NotFoundError:\n        return BestCodeResults(code=None, reward=0.0, npe=0, folder=folder, finished=False, error=BestCodeResultError.experiment_does_not_exist)\n    solution_files = [fname for fname in all_children if re.search(search_regex, fname)]\n    max_reward = 0.0\n    npe = 0\n    best_code = None\n    for fname in solution_files:\n        with tf.gfile.FastGFile(os.path.join(log_dir, fname), 'r') as reader:\n            results = [ast.literal_eval(entry) for entry in reader]\n        for res in results:\n            if res['reward'] > max_reward:\n                best_code = res['code']\n                max_reward = res['reward']\n                npe = res['npe']\n    error = BestCodeResultError.success if best_code else BestCodeResultError.no_solution_found\n    try:\n        with tf.gfile.FastGFile(os.path.join(log_dir, 'status.txt'), 'r') as f:\n            finished = f.read().lower().strip() == 'done'\n    except tf.errors.NotFoundError:\n        finished = False\n    return BestCodeResults(code=best_code, reward=max_reward, npe=npe, folder=folder, finished=finished, error=error)",
            "def get_best_code_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc=0, name_prefix='bf_rl_paper', extra_desc=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Like `get_results_for_experiment`, but fetches the code solutions.'\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    log_dir = os.path.join(models_dir, folder, 'logs')\n    search_regex = '^solutions_([0-9])+\\\\.txt$'\n    try:\n        all_children = tf.gfile.ListDirectory(log_dir)\n    except tf.errors.NotFoundError:\n        return BestCodeResults(code=None, reward=0.0, npe=0, folder=folder, finished=False, error=BestCodeResultError.experiment_does_not_exist)\n    solution_files = [fname for fname in all_children if re.search(search_regex, fname)]\n    max_reward = 0.0\n    npe = 0\n    best_code = None\n    for fname in solution_files:\n        with tf.gfile.FastGFile(os.path.join(log_dir, fname), 'r') as reader:\n            results = [ast.literal_eval(entry) for entry in reader]\n        for res in results:\n            if res['reward'] > max_reward:\n                best_code = res['code']\n                max_reward = res['reward']\n                npe = res['npe']\n    error = BestCodeResultError.success if best_code else BestCodeResultError.no_solution_found\n    try:\n        with tf.gfile.FastGFile(os.path.join(log_dir, 'status.txt'), 'r') as f:\n            finished = f.read().lower().strip() == 'done'\n    except tf.errors.NotFoundError:\n        finished = False\n    return BestCodeResults(code=best_code, reward=max_reward, npe=npe, folder=folder, finished=finished, error=error)",
            "def get_best_code_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc=0, name_prefix='bf_rl_paper', extra_desc=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Like `get_results_for_experiment`, but fetches the code solutions.'\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    log_dir = os.path.join(models_dir, folder, 'logs')\n    search_regex = '^solutions_([0-9])+\\\\.txt$'\n    try:\n        all_children = tf.gfile.ListDirectory(log_dir)\n    except tf.errors.NotFoundError:\n        return BestCodeResults(code=None, reward=0.0, npe=0, folder=folder, finished=False, error=BestCodeResultError.experiment_does_not_exist)\n    solution_files = [fname for fname in all_children if re.search(search_regex, fname)]\n    max_reward = 0.0\n    npe = 0\n    best_code = None\n    for fname in solution_files:\n        with tf.gfile.FastGFile(os.path.join(log_dir, fname), 'r') as reader:\n            results = [ast.literal_eval(entry) for entry in reader]\n        for res in results:\n            if res['reward'] > max_reward:\n                best_code = res['code']\n                max_reward = res['reward']\n                npe = res['npe']\n    error = BestCodeResultError.success if best_code else BestCodeResultError.no_solution_found\n    try:\n        with tf.gfile.FastGFile(os.path.join(log_dir, 'status.txt'), 'r') as f:\n            finished = f.read().lower().strip() == 'done'\n    except tf.errors.NotFoundError:\n        finished = False\n    return BestCodeResults(code=best_code, reward=max_reward, npe=npe, folder=folder, finished=finished, error=error)",
            "def get_best_code_for_experiment(models_dir, task_name, model_type='pg', max_npe='5M', desc=0, name_prefix='bf_rl_paper', extra_desc=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Like `get_results_for_experiment`, but fetches the code solutions.'\n    folder = name_prefix + '.{0}.{1}-{2}_{3}'.format(desc, model_type, max_npe, task_name)\n    if extra_desc:\n        folder += '.' + extra_desc\n    log_dir = os.path.join(models_dir, folder, 'logs')\n    search_regex = '^solutions_([0-9])+\\\\.txt$'\n    try:\n        all_children = tf.gfile.ListDirectory(log_dir)\n    except tf.errors.NotFoundError:\n        return BestCodeResults(code=None, reward=0.0, npe=0, folder=folder, finished=False, error=BestCodeResultError.experiment_does_not_exist)\n    solution_files = [fname for fname in all_children if re.search(search_regex, fname)]\n    max_reward = 0.0\n    npe = 0\n    best_code = None\n    for fname in solution_files:\n        with tf.gfile.FastGFile(os.path.join(log_dir, fname), 'r') as reader:\n            results = [ast.literal_eval(entry) for entry in reader]\n        for res in results:\n            if res['reward'] > max_reward:\n                best_code = res['code']\n                max_reward = res['reward']\n                npe = res['npe']\n    error = BestCodeResultError.success if best_code else BestCodeResultError.no_solution_found\n    try:\n        with tf.gfile.FastGFile(os.path.join(log_dir, 'status.txt'), 'r') as f:\n            finished = f.read().lower().strip() == 'done'\n    except tf.errors.NotFoundError:\n        finished = False\n    return BestCodeResults(code=best_code, reward=max_reward, npe=npe, folder=folder, finished=finished, error=error)"
        ]
    },
    {
        "func_name": "info",
        "original": "def info(stats):\n    return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]",
        "mutated": [
            "def info(stats):\n    if False:\n        i = 10\n    return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]",
            "def info(stats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]",
            "def info(stats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]",
            "def info(stats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]",
            "def info(stats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]"
        ]
    },
    {
        "func_name": "make_results_table",
        "original": "def make_results_table(models=None, tasks=None, max_npe='5M', name_prefix='bf_rl_paper', extra_desc='', models_dir='/tmp'):\n    \"\"\"Creates a table of results: algorithm + version by tasks.\n\n  Args:\n    models: The table columns. A list of (algorithm, desc) tuples.\n    tasks: The table rows. List of task names.\n    max_npe: String SI unit representation of the maximum NPE threshold for the\n        experiment. For example, \"5M\" means 5 million. All entries in the table\n        share the same max-NPE.\n    name_prefix: Name prefix used in logging directory for the experiment.\n    extra_desc: Extra description added to name of logging directory for the\n        experiment.\n    models_dir: Parent directory containing all experiment folders.\n\n  Returns:\n    A 2D list holding the table cells.\n  \"\"\"\n    if models is None:\n        models = DEFAULT_MODELS\n    if tasks is None:\n        tasks = DEFAULT_TASKS\n    model_results = {}\n    for (model_type, desc) in models:\n        model_results[model_type] = {tname: get_results_for_experiment(models_dir, tname, model_type, max_npe, desc, name_prefix=name_prefix, extra_desc=extra_desc).processed for tname in tasks}\n\n    def info(stats):\n        return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]\n    rows = [['max NPE: ' + max_npe] + misc.flatten([['{0} ({1})'.format(m, d), '', ''] for (m, d) in models])]\n    rows.append([''] + misc.flatten([['reps', 'success rate', 'avg NPE'] for _ in models]))\n    for tname in tasks:\n        rows.append([tname] + misc.flatten([info(model_results[model][tname]) for (model, _) in models]))\n    return rows",
        "mutated": [
            "def make_results_table(models=None, tasks=None, max_npe='5M', name_prefix='bf_rl_paper', extra_desc='', models_dir='/tmp'):\n    if False:\n        i = 10\n    'Creates a table of results: algorithm + version by tasks.\\n\\n  Args:\\n    models: The table columns. A list of (algorithm, desc) tuples.\\n    tasks: The table rows. List of task names.\\n    max_npe: String SI unit representation of the maximum NPE threshold for the\\n        experiment. For example, \"5M\" means 5 million. All entries in the table\\n        share the same max-NPE.\\n    name_prefix: Name prefix used in logging directory for the experiment.\\n    extra_desc: Extra description added to name of logging directory for the\\n        experiment.\\n    models_dir: Parent directory containing all experiment folders.\\n\\n  Returns:\\n    A 2D list holding the table cells.\\n  '\n    if models is None:\n        models = DEFAULT_MODELS\n    if tasks is None:\n        tasks = DEFAULT_TASKS\n    model_results = {}\n    for (model_type, desc) in models:\n        model_results[model_type] = {tname: get_results_for_experiment(models_dir, tname, model_type, max_npe, desc, name_prefix=name_prefix, extra_desc=extra_desc).processed for tname in tasks}\n\n    def info(stats):\n        return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]\n    rows = [['max NPE: ' + max_npe] + misc.flatten([['{0} ({1})'.format(m, d), '', ''] for (m, d) in models])]\n    rows.append([''] + misc.flatten([['reps', 'success rate', 'avg NPE'] for _ in models]))\n    for tname in tasks:\n        rows.append([tname] + misc.flatten([info(model_results[model][tname]) for (model, _) in models]))\n    return rows",
            "def make_results_table(models=None, tasks=None, max_npe='5M', name_prefix='bf_rl_paper', extra_desc='', models_dir='/tmp'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a table of results: algorithm + version by tasks.\\n\\n  Args:\\n    models: The table columns. A list of (algorithm, desc) tuples.\\n    tasks: The table rows. List of task names.\\n    max_npe: String SI unit representation of the maximum NPE threshold for the\\n        experiment. For example, \"5M\" means 5 million. All entries in the table\\n        share the same max-NPE.\\n    name_prefix: Name prefix used in logging directory for the experiment.\\n    extra_desc: Extra description added to name of logging directory for the\\n        experiment.\\n    models_dir: Parent directory containing all experiment folders.\\n\\n  Returns:\\n    A 2D list holding the table cells.\\n  '\n    if models is None:\n        models = DEFAULT_MODELS\n    if tasks is None:\n        tasks = DEFAULT_TASKS\n    model_results = {}\n    for (model_type, desc) in models:\n        model_results[model_type] = {tname: get_results_for_experiment(models_dir, tname, model_type, max_npe, desc, name_prefix=name_prefix, extra_desc=extra_desc).processed for tname in tasks}\n\n    def info(stats):\n        return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]\n    rows = [['max NPE: ' + max_npe] + misc.flatten([['{0} ({1})'.format(m, d), '', ''] for (m, d) in models])]\n    rows.append([''] + misc.flatten([['reps', 'success rate', 'avg NPE'] for _ in models]))\n    for tname in tasks:\n        rows.append([tname] + misc.flatten([info(model_results[model][tname]) for (model, _) in models]))\n    return rows",
            "def make_results_table(models=None, tasks=None, max_npe='5M', name_prefix='bf_rl_paper', extra_desc='', models_dir='/tmp'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a table of results: algorithm + version by tasks.\\n\\n  Args:\\n    models: The table columns. A list of (algorithm, desc) tuples.\\n    tasks: The table rows. List of task names.\\n    max_npe: String SI unit representation of the maximum NPE threshold for the\\n        experiment. For example, \"5M\" means 5 million. All entries in the table\\n        share the same max-NPE.\\n    name_prefix: Name prefix used in logging directory for the experiment.\\n    extra_desc: Extra description added to name of logging directory for the\\n        experiment.\\n    models_dir: Parent directory containing all experiment folders.\\n\\n  Returns:\\n    A 2D list holding the table cells.\\n  '\n    if models is None:\n        models = DEFAULT_MODELS\n    if tasks is None:\n        tasks = DEFAULT_TASKS\n    model_results = {}\n    for (model_type, desc) in models:\n        model_results[model_type] = {tname: get_results_for_experiment(models_dir, tname, model_type, max_npe, desc, name_prefix=name_prefix, extra_desc=extra_desc).processed for tname in tasks}\n\n    def info(stats):\n        return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]\n    rows = [['max NPE: ' + max_npe] + misc.flatten([['{0} ({1})'.format(m, d), '', ''] for (m, d) in models])]\n    rows.append([''] + misc.flatten([['reps', 'success rate', 'avg NPE'] for _ in models]))\n    for tname in tasks:\n        rows.append([tname] + misc.flatten([info(model_results[model][tname]) for (model, _) in models]))\n    return rows",
            "def make_results_table(models=None, tasks=None, max_npe='5M', name_prefix='bf_rl_paper', extra_desc='', models_dir='/tmp'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a table of results: algorithm + version by tasks.\\n\\n  Args:\\n    models: The table columns. A list of (algorithm, desc) tuples.\\n    tasks: The table rows. List of task names.\\n    max_npe: String SI unit representation of the maximum NPE threshold for the\\n        experiment. For example, \"5M\" means 5 million. All entries in the table\\n        share the same max-NPE.\\n    name_prefix: Name prefix used in logging directory for the experiment.\\n    extra_desc: Extra description added to name of logging directory for the\\n        experiment.\\n    models_dir: Parent directory containing all experiment folders.\\n\\n  Returns:\\n    A 2D list holding the table cells.\\n  '\n    if models is None:\n        models = DEFAULT_MODELS\n    if tasks is None:\n        tasks = DEFAULT_TASKS\n    model_results = {}\n    for (model_type, desc) in models:\n        model_results[model_type] = {tname: get_results_for_experiment(models_dir, tname, model_type, max_npe, desc, name_prefix=name_prefix, extra_desc=extra_desc).processed for tname in tasks}\n\n    def info(stats):\n        return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]\n    rows = [['max NPE: ' + max_npe] + misc.flatten([['{0} ({1})'.format(m, d), '', ''] for (m, d) in models])]\n    rows.append([''] + misc.flatten([['reps', 'success rate', 'avg NPE'] for _ in models]))\n    for tname in tasks:\n        rows.append([tname] + misc.flatten([info(model_results[model][tname]) for (model, _) in models]))\n    return rows",
            "def make_results_table(models=None, tasks=None, max_npe='5M', name_prefix='bf_rl_paper', extra_desc='', models_dir='/tmp'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a table of results: algorithm + version by tasks.\\n\\n  Args:\\n    models: The table columns. A list of (algorithm, desc) tuples.\\n    tasks: The table rows. List of task names.\\n    max_npe: String SI unit representation of the maximum NPE threshold for the\\n        experiment. For example, \"5M\" means 5 million. All entries in the table\\n        share the same max-NPE.\\n    name_prefix: Name prefix used in logging directory for the experiment.\\n    extra_desc: Extra description added to name of logging directory for the\\n        experiment.\\n    models_dir: Parent directory containing all experiment folders.\\n\\n  Returns:\\n    A 2D list holding the table cells.\\n  '\n    if models is None:\n        models = DEFAULT_MODELS\n    if tasks is None:\n        tasks = DEFAULT_TASKS\n    model_results = {}\n    for (model_type, desc) in models:\n        model_results[model_type] = {tname: get_results_for_experiment(models_dir, tname, model_type, max_npe, desc, name_prefix=name_prefix, extra_desc=extra_desc).processed for tname in tasks}\n\n    def info(stats):\n        return [str(stats['repetitions']), '%.2f' % stats['success_rate'], str(int(stats['avg_total_npe']))]\n    rows = [['max NPE: ' + max_npe] + misc.flatten([['{0} ({1})'.format(m, d), '', ''] for (m, d) in models])]\n    rows.append([''] + misc.flatten([['reps', 'success rate', 'avg NPE'] for _ in models]))\n    for tname in tasks:\n        rows.append([tname] + misc.flatten([info(model_results[model][tname]) for (model, _) in models]))\n    return rows"
        ]
    },
    {
        "func_name": "info_str",
        "original": "def info_str(info_row):\n    if not info_row[0]:\n        return '0'\n    return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])",
        "mutated": [
            "def info_str(info_row):\n    if False:\n        i = 10\n    if not info_row[0]:\n        return '0'\n    return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])",
            "def info_str(info_row):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not info_row[0]:\n        return '0'\n    return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])",
            "def info_str(info_row):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not info_row[0]:\n        return '0'\n    return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])",
            "def info_str(info_row):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not info_row[0]:\n        return '0'\n    return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])",
            "def info_str(info_row):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not info_row[0]:\n        return '0'\n    return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])"
        ]
    },
    {
        "func_name": "print_results_table",
        "original": "def print_results_table(results_table):\n    \"\"\"Print human readable results table to stdout.\"\"\"\n    print('')\n    print('=== Results Table ===')\n    print('Format: # reps [success rate, avg total NPE]')\n\n    def info_str(info_row):\n        if not info_row[0]:\n            return '0'\n        return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])\n    nc = len(results_table[0])\n    out_table = [[results_table[0][0]] + [results_table[0][i] for i in range(1, nc, 3)]]\n    for row in results_table[2:]:\n        out_table.append([row[0]] + [info_str(row[i:i + 3]) for i in range(1, nc, 3)])\n    nc = len(out_table[0])\n    col_widths = [max((len(row[col]) for row in out_table)) for col in range(nc)]\n    table_string = ''\n    for row in out_table:\n        table_string += ''.join([row[c].ljust(col_widths[c] + 2) for c in range(nc)]) + '\\n'\n    print(table_string)",
        "mutated": [
            "def print_results_table(results_table):\n    if False:\n        i = 10\n    'Print human readable results table to stdout.'\n    print('')\n    print('=== Results Table ===')\n    print('Format: # reps [success rate, avg total NPE]')\n\n    def info_str(info_row):\n        if not info_row[0]:\n            return '0'\n        return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])\n    nc = len(results_table[0])\n    out_table = [[results_table[0][0]] + [results_table[0][i] for i in range(1, nc, 3)]]\n    for row in results_table[2:]:\n        out_table.append([row[0]] + [info_str(row[i:i + 3]) for i in range(1, nc, 3)])\n    nc = len(out_table[0])\n    col_widths = [max((len(row[col]) for row in out_table)) for col in range(nc)]\n    table_string = ''\n    for row in out_table:\n        table_string += ''.join([row[c].ljust(col_widths[c] + 2) for c in range(nc)]) + '\\n'\n    print(table_string)",
            "def print_results_table(results_table):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Print human readable results table to stdout.'\n    print('')\n    print('=== Results Table ===')\n    print('Format: # reps [success rate, avg total NPE]')\n\n    def info_str(info_row):\n        if not info_row[0]:\n            return '0'\n        return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])\n    nc = len(results_table[0])\n    out_table = [[results_table[0][0]] + [results_table[0][i] for i in range(1, nc, 3)]]\n    for row in results_table[2:]:\n        out_table.append([row[0]] + [info_str(row[i:i + 3]) for i in range(1, nc, 3)])\n    nc = len(out_table[0])\n    col_widths = [max((len(row[col]) for row in out_table)) for col in range(nc)]\n    table_string = ''\n    for row in out_table:\n        table_string += ''.join([row[c].ljust(col_widths[c] + 2) for c in range(nc)]) + '\\n'\n    print(table_string)",
            "def print_results_table(results_table):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Print human readable results table to stdout.'\n    print('')\n    print('=== Results Table ===')\n    print('Format: # reps [success rate, avg total NPE]')\n\n    def info_str(info_row):\n        if not info_row[0]:\n            return '0'\n        return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])\n    nc = len(results_table[0])\n    out_table = [[results_table[0][0]] + [results_table[0][i] for i in range(1, nc, 3)]]\n    for row in results_table[2:]:\n        out_table.append([row[0]] + [info_str(row[i:i + 3]) for i in range(1, nc, 3)])\n    nc = len(out_table[0])\n    col_widths = [max((len(row[col]) for row in out_table)) for col in range(nc)]\n    table_string = ''\n    for row in out_table:\n        table_string += ''.join([row[c].ljust(col_widths[c] + 2) for c in range(nc)]) + '\\n'\n    print(table_string)",
            "def print_results_table(results_table):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Print human readable results table to stdout.'\n    print('')\n    print('=== Results Table ===')\n    print('Format: # reps [success rate, avg total NPE]')\n\n    def info_str(info_row):\n        if not info_row[0]:\n            return '0'\n        return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])\n    nc = len(results_table[0])\n    out_table = [[results_table[0][0]] + [results_table[0][i] for i in range(1, nc, 3)]]\n    for row in results_table[2:]:\n        out_table.append([row[0]] + [info_str(row[i:i + 3]) for i in range(1, nc, 3)])\n    nc = len(out_table[0])\n    col_widths = [max((len(row[col]) for row in out_table)) for col in range(nc)]\n    table_string = ''\n    for row in out_table:\n        table_string += ''.join([row[c].ljust(col_widths[c] + 2) for c in range(nc)]) + '\\n'\n    print(table_string)",
            "def print_results_table(results_table):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Print human readable results table to stdout.'\n    print('')\n    print('=== Results Table ===')\n    print('Format: # reps [success rate, avg total NPE]')\n\n    def info_str(info_row):\n        if not info_row[0]:\n            return '0'\n        return '%s [%s, %s]' % (str(info_row[0]).ljust(2), info_row[1], info_row[2])\n    nc = len(results_table[0])\n    out_table = [[results_table[0][0]] + [results_table[0][i] for i in range(1, nc, 3)]]\n    for row in results_table[2:]:\n        out_table.append([row[0]] + [info_str(row[i:i + 3]) for i in range(1, nc, 3)])\n    nc = len(out_table[0])\n    col_widths = [max((len(row[col]) for row in out_table)) for col in range(nc)]\n    table_string = ''\n    for row in out_table:\n        table_string += ''.join([row[c].ljust(col_widths[c] + 2) for c in range(nc)]) + '\\n'\n    print(table_string)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(argv):\n    del argv\n    name_prefix = FLAGS.exp_prefix\n    print('Experiments prefix: %s' % name_prefix)\n    model_types = ast.literal_eval(FLAGS.model_types)\n    if FLAGS.data == 'success_rates':\n        results_table = make_results_table(models=model_types, tasks=FLAGS.task_list, max_npe=FLAGS.max_npe, models_dir=FLAGS.models_dir, name_prefix=name_prefix, extra_desc='')\n        with tf.gfile.FastGFile(FLAGS.csv_file, 'w') as f:\n            f.write(make_csv_string(results_table))\n        print_results_table(results_table)\n    else:\n        print('* = experiment is still running')\n        print('')\n        print('=== Best Synthesized Code ===')\n        for (model_type, desc) in model_types:\n            print('%s (%s)' % (model_type, desc))\n            sys.stdout.flush()\n            for tname in FLAGS.task_list:\n                res = get_best_code_for_experiment(FLAGS.models_dir, tname, model_type, FLAGS.max_npe, desc, name_prefix=name_prefix, extra_desc='')\n                unfinished_mark = '' if res.finished else ' *'\n                tname += unfinished_mark\n                if res.error == BestCodeResultError.success:\n                    print('  %s' % tname)\n                    print('    %s' % res.code)\n                    print('    R=%.6f, NPE=%s' % (res.reward, misc.int_to_si(res.npe)))\n                elif res.error == BestCodeResultError.experiment_does_not_exist:\n                    print('  Experiment does not exist. Check arguments.')\n                    print('  Experiment folder: %s' % res.folder)\n                    break\n                else:\n                    print('  %s' % tname)\n                    print('    (none)')\n                sys.stdout.flush()",
        "mutated": [
            "def main(argv):\n    if False:\n        i = 10\n    del argv\n    name_prefix = FLAGS.exp_prefix\n    print('Experiments prefix: %s' % name_prefix)\n    model_types = ast.literal_eval(FLAGS.model_types)\n    if FLAGS.data == 'success_rates':\n        results_table = make_results_table(models=model_types, tasks=FLAGS.task_list, max_npe=FLAGS.max_npe, models_dir=FLAGS.models_dir, name_prefix=name_prefix, extra_desc='')\n        with tf.gfile.FastGFile(FLAGS.csv_file, 'w') as f:\n            f.write(make_csv_string(results_table))\n        print_results_table(results_table)\n    else:\n        print('* = experiment is still running')\n        print('')\n        print('=== Best Synthesized Code ===')\n        for (model_type, desc) in model_types:\n            print('%s (%s)' % (model_type, desc))\n            sys.stdout.flush()\n            for tname in FLAGS.task_list:\n                res = get_best_code_for_experiment(FLAGS.models_dir, tname, model_type, FLAGS.max_npe, desc, name_prefix=name_prefix, extra_desc='')\n                unfinished_mark = '' if res.finished else ' *'\n                tname += unfinished_mark\n                if res.error == BestCodeResultError.success:\n                    print('  %s' % tname)\n                    print('    %s' % res.code)\n                    print('    R=%.6f, NPE=%s' % (res.reward, misc.int_to_si(res.npe)))\n                elif res.error == BestCodeResultError.experiment_does_not_exist:\n                    print('  Experiment does not exist. Check arguments.')\n                    print('  Experiment folder: %s' % res.folder)\n                    break\n                else:\n                    print('  %s' % tname)\n                    print('    (none)')\n                sys.stdout.flush()",
            "def main(argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    del argv\n    name_prefix = FLAGS.exp_prefix\n    print('Experiments prefix: %s' % name_prefix)\n    model_types = ast.literal_eval(FLAGS.model_types)\n    if FLAGS.data == 'success_rates':\n        results_table = make_results_table(models=model_types, tasks=FLAGS.task_list, max_npe=FLAGS.max_npe, models_dir=FLAGS.models_dir, name_prefix=name_prefix, extra_desc='')\n        with tf.gfile.FastGFile(FLAGS.csv_file, 'w') as f:\n            f.write(make_csv_string(results_table))\n        print_results_table(results_table)\n    else:\n        print('* = experiment is still running')\n        print('')\n        print('=== Best Synthesized Code ===')\n        for (model_type, desc) in model_types:\n            print('%s (%s)' % (model_type, desc))\n            sys.stdout.flush()\n            for tname in FLAGS.task_list:\n                res = get_best_code_for_experiment(FLAGS.models_dir, tname, model_type, FLAGS.max_npe, desc, name_prefix=name_prefix, extra_desc='')\n                unfinished_mark = '' if res.finished else ' *'\n                tname += unfinished_mark\n                if res.error == BestCodeResultError.success:\n                    print('  %s' % tname)\n                    print('    %s' % res.code)\n                    print('    R=%.6f, NPE=%s' % (res.reward, misc.int_to_si(res.npe)))\n                elif res.error == BestCodeResultError.experiment_does_not_exist:\n                    print('  Experiment does not exist. Check arguments.')\n                    print('  Experiment folder: %s' % res.folder)\n                    break\n                else:\n                    print('  %s' % tname)\n                    print('    (none)')\n                sys.stdout.flush()",
            "def main(argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    del argv\n    name_prefix = FLAGS.exp_prefix\n    print('Experiments prefix: %s' % name_prefix)\n    model_types = ast.literal_eval(FLAGS.model_types)\n    if FLAGS.data == 'success_rates':\n        results_table = make_results_table(models=model_types, tasks=FLAGS.task_list, max_npe=FLAGS.max_npe, models_dir=FLAGS.models_dir, name_prefix=name_prefix, extra_desc='')\n        with tf.gfile.FastGFile(FLAGS.csv_file, 'w') as f:\n            f.write(make_csv_string(results_table))\n        print_results_table(results_table)\n    else:\n        print('* = experiment is still running')\n        print('')\n        print('=== Best Synthesized Code ===')\n        for (model_type, desc) in model_types:\n            print('%s (%s)' % (model_type, desc))\n            sys.stdout.flush()\n            for tname in FLAGS.task_list:\n                res = get_best_code_for_experiment(FLAGS.models_dir, tname, model_type, FLAGS.max_npe, desc, name_prefix=name_prefix, extra_desc='')\n                unfinished_mark = '' if res.finished else ' *'\n                tname += unfinished_mark\n                if res.error == BestCodeResultError.success:\n                    print('  %s' % tname)\n                    print('    %s' % res.code)\n                    print('    R=%.6f, NPE=%s' % (res.reward, misc.int_to_si(res.npe)))\n                elif res.error == BestCodeResultError.experiment_does_not_exist:\n                    print('  Experiment does not exist. Check arguments.')\n                    print('  Experiment folder: %s' % res.folder)\n                    break\n                else:\n                    print('  %s' % tname)\n                    print('    (none)')\n                sys.stdout.flush()",
            "def main(argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    del argv\n    name_prefix = FLAGS.exp_prefix\n    print('Experiments prefix: %s' % name_prefix)\n    model_types = ast.literal_eval(FLAGS.model_types)\n    if FLAGS.data == 'success_rates':\n        results_table = make_results_table(models=model_types, tasks=FLAGS.task_list, max_npe=FLAGS.max_npe, models_dir=FLAGS.models_dir, name_prefix=name_prefix, extra_desc='')\n        with tf.gfile.FastGFile(FLAGS.csv_file, 'w') as f:\n            f.write(make_csv_string(results_table))\n        print_results_table(results_table)\n    else:\n        print('* = experiment is still running')\n        print('')\n        print('=== Best Synthesized Code ===')\n        for (model_type, desc) in model_types:\n            print('%s (%s)' % (model_type, desc))\n            sys.stdout.flush()\n            for tname in FLAGS.task_list:\n                res = get_best_code_for_experiment(FLAGS.models_dir, tname, model_type, FLAGS.max_npe, desc, name_prefix=name_prefix, extra_desc='')\n                unfinished_mark = '' if res.finished else ' *'\n                tname += unfinished_mark\n                if res.error == BestCodeResultError.success:\n                    print('  %s' % tname)\n                    print('    %s' % res.code)\n                    print('    R=%.6f, NPE=%s' % (res.reward, misc.int_to_si(res.npe)))\n                elif res.error == BestCodeResultError.experiment_does_not_exist:\n                    print('  Experiment does not exist. Check arguments.')\n                    print('  Experiment folder: %s' % res.folder)\n                    break\n                else:\n                    print('  %s' % tname)\n                    print('    (none)')\n                sys.stdout.flush()",
            "def main(argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    del argv\n    name_prefix = FLAGS.exp_prefix\n    print('Experiments prefix: %s' % name_prefix)\n    model_types = ast.literal_eval(FLAGS.model_types)\n    if FLAGS.data == 'success_rates':\n        results_table = make_results_table(models=model_types, tasks=FLAGS.task_list, max_npe=FLAGS.max_npe, models_dir=FLAGS.models_dir, name_prefix=name_prefix, extra_desc='')\n        with tf.gfile.FastGFile(FLAGS.csv_file, 'w') as f:\n            f.write(make_csv_string(results_table))\n        print_results_table(results_table)\n    else:\n        print('* = experiment is still running')\n        print('')\n        print('=== Best Synthesized Code ===')\n        for (model_type, desc) in model_types:\n            print('%s (%s)' % (model_type, desc))\n            sys.stdout.flush()\n            for tname in FLAGS.task_list:\n                res = get_best_code_for_experiment(FLAGS.models_dir, tname, model_type, FLAGS.max_npe, desc, name_prefix=name_prefix, extra_desc='')\n                unfinished_mark = '' if res.finished else ' *'\n                tname += unfinished_mark\n                if res.error == BestCodeResultError.success:\n                    print('  %s' % tname)\n                    print('    %s' % res.code)\n                    print('    R=%.6f, NPE=%s' % (res.reward, misc.int_to_si(res.npe)))\n                elif res.error == BestCodeResultError.experiment_does_not_exist:\n                    print('  Experiment does not exist. Check arguments.')\n                    print('  Experiment folder: %s' % res.folder)\n                    break\n                else:\n                    print('  %s' % tname)\n                    print('    (none)')\n                sys.stdout.flush()"
        ]
    }
]