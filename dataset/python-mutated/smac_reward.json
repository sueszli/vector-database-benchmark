[
    {
        "func_name": "__init__",
        "original": "def __init__(self, n_agents, n_enemies, two_player, reward_type, max_reward, reward_scale=True, reduce_agent=True, reward_only_positive=True):\n    self.reward_only_positive = reward_only_positive\n    self.reward_scale = reward_scale\n    self.max_reward = max_reward\n    self.reward_death_value = 10\n    self.reward_win = 200\n    self.reward_defeat = 0\n    self.reward_negative_scale = 0.5\n    self.reward_scale_rate = 20\n    self.reduce_agent = reduce_agent\n    self.reward_type = reward_type\n    assert self.reward_type in ['sparse', 'original', 'new']\n    self.n_agents = n_agents\n    self.n_enemies = n_enemies\n    self.death_tracker_ally = np.zeros(n_agents)\n    self.death_tracker_enemy = np.zeros(n_enemies)\n    self.two_player = two_player",
        "mutated": [
            "def __init__(self, n_agents, n_enemies, two_player, reward_type, max_reward, reward_scale=True, reduce_agent=True, reward_only_positive=True):\n    if False:\n        i = 10\n    self.reward_only_positive = reward_only_positive\n    self.reward_scale = reward_scale\n    self.max_reward = max_reward\n    self.reward_death_value = 10\n    self.reward_win = 200\n    self.reward_defeat = 0\n    self.reward_negative_scale = 0.5\n    self.reward_scale_rate = 20\n    self.reduce_agent = reduce_agent\n    self.reward_type = reward_type\n    assert self.reward_type in ['sparse', 'original', 'new']\n    self.n_agents = n_agents\n    self.n_enemies = n_enemies\n    self.death_tracker_ally = np.zeros(n_agents)\n    self.death_tracker_enemy = np.zeros(n_enemies)\n    self.two_player = two_player",
            "def __init__(self, n_agents, n_enemies, two_player, reward_type, max_reward, reward_scale=True, reduce_agent=True, reward_only_positive=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.reward_only_positive = reward_only_positive\n    self.reward_scale = reward_scale\n    self.max_reward = max_reward\n    self.reward_death_value = 10\n    self.reward_win = 200\n    self.reward_defeat = 0\n    self.reward_negative_scale = 0.5\n    self.reward_scale_rate = 20\n    self.reduce_agent = reduce_agent\n    self.reward_type = reward_type\n    assert self.reward_type in ['sparse', 'original', 'new']\n    self.n_agents = n_agents\n    self.n_enemies = n_enemies\n    self.death_tracker_ally = np.zeros(n_agents)\n    self.death_tracker_enemy = np.zeros(n_enemies)\n    self.two_player = two_player",
            "def __init__(self, n_agents, n_enemies, two_player, reward_type, max_reward, reward_scale=True, reduce_agent=True, reward_only_positive=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.reward_only_positive = reward_only_positive\n    self.reward_scale = reward_scale\n    self.max_reward = max_reward\n    self.reward_death_value = 10\n    self.reward_win = 200\n    self.reward_defeat = 0\n    self.reward_negative_scale = 0.5\n    self.reward_scale_rate = 20\n    self.reduce_agent = reduce_agent\n    self.reward_type = reward_type\n    assert self.reward_type in ['sparse', 'original', 'new']\n    self.n_agents = n_agents\n    self.n_enemies = n_enemies\n    self.death_tracker_ally = np.zeros(n_agents)\n    self.death_tracker_enemy = np.zeros(n_enemies)\n    self.two_player = two_player",
            "def __init__(self, n_agents, n_enemies, two_player, reward_type, max_reward, reward_scale=True, reduce_agent=True, reward_only_positive=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.reward_only_positive = reward_only_positive\n    self.reward_scale = reward_scale\n    self.max_reward = max_reward\n    self.reward_death_value = 10\n    self.reward_win = 200\n    self.reward_defeat = 0\n    self.reward_negative_scale = 0.5\n    self.reward_scale_rate = 20\n    self.reduce_agent = reduce_agent\n    self.reward_type = reward_type\n    assert self.reward_type in ['sparse', 'original', 'new']\n    self.n_agents = n_agents\n    self.n_enemies = n_enemies\n    self.death_tracker_ally = np.zeros(n_agents)\n    self.death_tracker_enemy = np.zeros(n_enemies)\n    self.two_player = two_player",
            "def __init__(self, n_agents, n_enemies, two_player, reward_type, max_reward, reward_scale=True, reduce_agent=True, reward_only_positive=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.reward_only_positive = reward_only_positive\n    self.reward_scale = reward_scale\n    self.max_reward = max_reward\n    self.reward_death_value = 10\n    self.reward_win = 200\n    self.reward_defeat = 0\n    self.reward_negative_scale = 0.5\n    self.reward_scale_rate = 20\n    self.reduce_agent = reduce_agent\n    self.reward_type = reward_type\n    assert self.reward_type in ['sparse', 'original', 'new']\n    self.n_agents = n_agents\n    self.n_enemies = n_enemies\n    self.death_tracker_ally = np.zeros(n_agents)\n    self.death_tracker_enemy = np.zeros(n_enemies)\n    self.two_player = two_player"
        ]
    },
    {
        "func_name": "reset",
        "original": "def reset(self, max_reward):\n    self.max_reward = max_reward\n    if self.reward_type == 'original':\n        self.info().value['max'] = self.max_reward / self.reward_scale_rate\n    self.death_tracker_ally.fill(0)\n    self.death_tracker_enemy.fill(0)",
        "mutated": [
            "def reset(self, max_reward):\n    if False:\n        i = 10\n    self.max_reward = max_reward\n    if self.reward_type == 'original':\n        self.info().value['max'] = self.max_reward / self.reward_scale_rate\n    self.death_tracker_ally.fill(0)\n    self.death_tracker_enemy.fill(0)",
            "def reset(self, max_reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.max_reward = max_reward\n    if self.reward_type == 'original':\n        self.info().value['max'] = self.max_reward / self.reward_scale_rate\n    self.death_tracker_ally.fill(0)\n    self.death_tracker_enemy.fill(0)",
            "def reset(self, max_reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.max_reward = max_reward\n    if self.reward_type == 'original':\n        self.info().value['max'] = self.max_reward / self.reward_scale_rate\n    self.death_tracker_ally.fill(0)\n    self.death_tracker_enemy.fill(0)",
            "def reset(self, max_reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.max_reward = max_reward\n    if self.reward_type == 'original':\n        self.info().value['max'] = self.max_reward / self.reward_scale_rate\n    self.death_tracker_ally.fill(0)\n    self.death_tracker_enemy.fill(0)",
            "def reset(self, max_reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.max_reward = max_reward\n    if self.reward_type == 'original':\n        self.info().value['max'] = self.max_reward / self.reward_scale_rate\n    self.death_tracker_ally.fill(0)\n    self.death_tracker_enemy.fill(0)"
        ]
    },
    {
        "func_name": "get_reward",
        "original": "def get_reward(self, engine, action, game_end_code, win_counted, defeat_counted):\n    reward = {ORIGINAL_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=False)), OPPONENT_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=True))}\n    for k in reward:\n        if reward[k].shape == ():\n            reward[k] = np.expand_dims(reward[k], 0)\n    if game_end_code is not None:\n        if game_end_code == 1 and (not win_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_win\n                reward[OPPONENT_AGENT] += self.reward_defeat\n            else:\n                reward[ORIGINAL_AGENT] += 1\n                reward[OPPONENT_AGENT] += -1\n        elif game_end_code == -1 and (not defeat_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_defeat\n                reward[OPPONENT_AGENT] += self.reward_win\n            else:\n                reward[ORIGINAL_AGENT] += -1\n                reward[OPPONENT_AGENT] += 1\n    if self.reward_scale:\n        (min_val, max_val) = (self.info().value['min'], self.info().value['max'])\n        reward[ORIGINAL_AGENT] = (reward[ORIGINAL_AGENT] - min_val) / (max_val - min_val)\n        reward[OPPONENT_AGENT] = (reward[OPPONENT_AGENT] - min_val) / (max_val - min_val)\n    return reward",
        "mutated": [
            "def get_reward(self, engine, action, game_end_code, win_counted, defeat_counted):\n    if False:\n        i = 10\n    reward = {ORIGINAL_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=False)), OPPONENT_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=True))}\n    for k in reward:\n        if reward[k].shape == ():\n            reward[k] = np.expand_dims(reward[k], 0)\n    if game_end_code is not None:\n        if game_end_code == 1 and (not win_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_win\n                reward[OPPONENT_AGENT] += self.reward_defeat\n            else:\n                reward[ORIGINAL_AGENT] += 1\n                reward[OPPONENT_AGENT] += -1\n        elif game_end_code == -1 and (not defeat_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_defeat\n                reward[OPPONENT_AGENT] += self.reward_win\n            else:\n                reward[ORIGINAL_AGENT] += -1\n                reward[OPPONENT_AGENT] += 1\n    if self.reward_scale:\n        (min_val, max_val) = (self.info().value['min'], self.info().value['max'])\n        reward[ORIGINAL_AGENT] = (reward[ORIGINAL_AGENT] - min_val) / (max_val - min_val)\n        reward[OPPONENT_AGENT] = (reward[OPPONENT_AGENT] - min_val) / (max_val - min_val)\n    return reward",
            "def get_reward(self, engine, action, game_end_code, win_counted, defeat_counted):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    reward = {ORIGINAL_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=False)), OPPONENT_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=True))}\n    for k in reward:\n        if reward[k].shape == ():\n            reward[k] = np.expand_dims(reward[k], 0)\n    if game_end_code is not None:\n        if game_end_code == 1 and (not win_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_win\n                reward[OPPONENT_AGENT] += self.reward_defeat\n            else:\n                reward[ORIGINAL_AGENT] += 1\n                reward[OPPONENT_AGENT] += -1\n        elif game_end_code == -1 and (not defeat_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_defeat\n                reward[OPPONENT_AGENT] += self.reward_win\n            else:\n                reward[ORIGINAL_AGENT] += -1\n                reward[OPPONENT_AGENT] += 1\n    if self.reward_scale:\n        (min_val, max_val) = (self.info().value['min'], self.info().value['max'])\n        reward[ORIGINAL_AGENT] = (reward[ORIGINAL_AGENT] - min_val) / (max_val - min_val)\n        reward[OPPONENT_AGENT] = (reward[OPPONENT_AGENT] - min_val) / (max_val - min_val)\n    return reward",
            "def get_reward(self, engine, action, game_end_code, win_counted, defeat_counted):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    reward = {ORIGINAL_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=False)), OPPONENT_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=True))}\n    for k in reward:\n        if reward[k].shape == ():\n            reward[k] = np.expand_dims(reward[k], 0)\n    if game_end_code is not None:\n        if game_end_code == 1 and (not win_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_win\n                reward[OPPONENT_AGENT] += self.reward_defeat\n            else:\n                reward[ORIGINAL_AGENT] += 1\n                reward[OPPONENT_AGENT] += -1\n        elif game_end_code == -1 and (not defeat_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_defeat\n                reward[OPPONENT_AGENT] += self.reward_win\n            else:\n                reward[ORIGINAL_AGENT] += -1\n                reward[OPPONENT_AGENT] += 1\n    if self.reward_scale:\n        (min_val, max_val) = (self.info().value['min'], self.info().value['max'])\n        reward[ORIGINAL_AGENT] = (reward[ORIGINAL_AGENT] - min_val) / (max_val - min_val)\n        reward[OPPONENT_AGENT] = (reward[OPPONENT_AGENT] - min_val) / (max_val - min_val)\n    return reward",
            "def get_reward(self, engine, action, game_end_code, win_counted, defeat_counted):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    reward = {ORIGINAL_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=False)), OPPONENT_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=True))}\n    for k in reward:\n        if reward[k].shape == ():\n            reward[k] = np.expand_dims(reward[k], 0)\n    if game_end_code is not None:\n        if game_end_code == 1 and (not win_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_win\n                reward[OPPONENT_AGENT] += self.reward_defeat\n            else:\n                reward[ORIGINAL_AGENT] += 1\n                reward[OPPONENT_AGENT] += -1\n        elif game_end_code == -1 and (not defeat_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_defeat\n                reward[OPPONENT_AGENT] += self.reward_win\n            else:\n                reward[ORIGINAL_AGENT] += -1\n                reward[OPPONENT_AGENT] += 1\n    if self.reward_scale:\n        (min_val, max_val) = (self.info().value['min'], self.info().value['max'])\n        reward[ORIGINAL_AGENT] = (reward[ORIGINAL_AGENT] - min_val) / (max_val - min_val)\n        reward[OPPONENT_AGENT] = (reward[OPPONENT_AGENT] - min_val) / (max_val - min_val)\n    return reward",
            "def get_reward(self, engine, action, game_end_code, win_counted, defeat_counted):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    reward = {ORIGINAL_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=False)), OPPONENT_AGENT: np.asarray(self.reward_battle_split(engine, action, is_opponent=True))}\n    for k in reward:\n        if reward[k].shape == ():\n            reward[k] = np.expand_dims(reward[k], 0)\n    if game_end_code is not None:\n        if game_end_code == 1 and (not win_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_win\n                reward[OPPONENT_AGENT] += self.reward_defeat\n            else:\n                reward[ORIGINAL_AGENT] += 1\n                reward[OPPONENT_AGENT] += -1\n        elif game_end_code == -1 and (not defeat_counted):\n            if self.reward_type != 'sparse':\n                reward[ORIGINAL_AGENT] += self.reward_defeat\n                reward[OPPONENT_AGENT] += self.reward_win\n            else:\n                reward[ORIGINAL_AGENT] += -1\n                reward[OPPONENT_AGENT] += 1\n    if self.reward_scale:\n        (min_val, max_val) = (self.info().value['min'], self.info().value['max'])\n        reward[ORIGINAL_AGENT] = (reward[ORIGINAL_AGENT] - min_val) / (max_val - min_val)\n        reward[OPPONENT_AGENT] = (reward[OPPONENT_AGENT] - min_val) / (max_val - min_val)\n    return reward"
        ]
    },
    {
        "func_name": "reward_battle_split",
        "original": "def reward_battle_split(self, engine, action, is_opponent=False):\n    \"\"\"Reward function when self.reward_type != 'sparse'.\n        Returns accumulative hit/shield point damage dealt to the enemy\n        + reward_death_value per enemy unit killed, and, in case\n        self.reward_only_positive == False, - (damage dealt to ally units\n        + reward_death_value per ally unit killed) * self.reward_negative_scale\n        \"\"\"\n    num_agents = engine.n_agents if not is_opponent else engine.n_enemies\n    num_enmies = engine.n_agents if is_opponent else engine.n_enemies\n    if self.reward_type == 'sparse':\n        if self.reduce_agent:\n            return 0.0\n        else:\n            return np.zeros(num_agents)\n    assert self.reward_type == 'original', 'reward_type={} is not supported!'.format(self.reward_type)\n    delta_deaths = np.zeros([num_agents])\n    reward = np.zeros([num_agents])\n    delta_ally = np.zeros([num_agents])\n    delta_enemy = np.zeros([num_enmies])\n    delta_death_enemy = np.zeros([num_enmies])\n    neg_scale = self.reward_negative_scale\n    if is_opponent:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    else:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    num_players = 2 if self.two_player else 1\n    for (al_id, al_unit) in iterator:\n        if death_tracker[al_id] < num_players:\n            prev_health = previous_units[al_id].health + previous_units[al_id].shield\n            if al_unit.health == 0:\n                death_tracker[al_id] += 1\n                delta_deaths[al_id] -= self.reward_death_value * neg_scale\n                delta_ally[al_id] += prev_health * neg_scale\n            else:\n                delta_ally[al_id] += neg_scale * (prev_health - al_unit.health - al_unit.shield)\n    if is_opponent:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    else:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    for (e_id, e_unit) in iterator:\n        if death_tracker[e_id] < num_players:\n            prev_health = previous_units[e_id].health + previous_units[e_id].shield\n            if e_unit.health == 0:\n                death_tracker[e_id] += 1\n                delta_death_enemy[e_id] += self.reward_death_value\n                delta_enemy[e_id] += prev_health\n            else:\n                delta_enemy[e_id] += prev_health - e_unit.health - e_unit.shield\n    if self.reward_only_positive:\n        reward = abs(delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum())\n    else:\n        reward = delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum() - delta_ally.sum()\n    return reward",
        "mutated": [
            "def reward_battle_split(self, engine, action, is_opponent=False):\n    if False:\n        i = 10\n    \"Reward function when self.reward_type != 'sparse'.\\n        Returns accumulative hit/shield point damage dealt to the enemy\\n        + reward_death_value per enemy unit killed, and, in case\\n        self.reward_only_positive == False, - (damage dealt to ally units\\n        + reward_death_value per ally unit killed) * self.reward_negative_scale\\n        \"\n    num_agents = engine.n_agents if not is_opponent else engine.n_enemies\n    num_enmies = engine.n_agents if is_opponent else engine.n_enemies\n    if self.reward_type == 'sparse':\n        if self.reduce_agent:\n            return 0.0\n        else:\n            return np.zeros(num_agents)\n    assert self.reward_type == 'original', 'reward_type={} is not supported!'.format(self.reward_type)\n    delta_deaths = np.zeros([num_agents])\n    reward = np.zeros([num_agents])\n    delta_ally = np.zeros([num_agents])\n    delta_enemy = np.zeros([num_enmies])\n    delta_death_enemy = np.zeros([num_enmies])\n    neg_scale = self.reward_negative_scale\n    if is_opponent:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    else:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    num_players = 2 if self.two_player else 1\n    for (al_id, al_unit) in iterator:\n        if death_tracker[al_id] < num_players:\n            prev_health = previous_units[al_id].health + previous_units[al_id].shield\n            if al_unit.health == 0:\n                death_tracker[al_id] += 1\n                delta_deaths[al_id] -= self.reward_death_value * neg_scale\n                delta_ally[al_id] += prev_health * neg_scale\n            else:\n                delta_ally[al_id] += neg_scale * (prev_health - al_unit.health - al_unit.shield)\n    if is_opponent:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    else:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    for (e_id, e_unit) in iterator:\n        if death_tracker[e_id] < num_players:\n            prev_health = previous_units[e_id].health + previous_units[e_id].shield\n            if e_unit.health == 0:\n                death_tracker[e_id] += 1\n                delta_death_enemy[e_id] += self.reward_death_value\n                delta_enemy[e_id] += prev_health\n            else:\n                delta_enemy[e_id] += prev_health - e_unit.health - e_unit.shield\n    if self.reward_only_positive:\n        reward = abs(delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum())\n    else:\n        reward = delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum() - delta_ally.sum()\n    return reward",
            "def reward_battle_split(self, engine, action, is_opponent=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Reward function when self.reward_type != 'sparse'.\\n        Returns accumulative hit/shield point damage dealt to the enemy\\n        + reward_death_value per enemy unit killed, and, in case\\n        self.reward_only_positive == False, - (damage dealt to ally units\\n        + reward_death_value per ally unit killed) * self.reward_negative_scale\\n        \"\n    num_agents = engine.n_agents if not is_opponent else engine.n_enemies\n    num_enmies = engine.n_agents if is_opponent else engine.n_enemies\n    if self.reward_type == 'sparse':\n        if self.reduce_agent:\n            return 0.0\n        else:\n            return np.zeros(num_agents)\n    assert self.reward_type == 'original', 'reward_type={} is not supported!'.format(self.reward_type)\n    delta_deaths = np.zeros([num_agents])\n    reward = np.zeros([num_agents])\n    delta_ally = np.zeros([num_agents])\n    delta_enemy = np.zeros([num_enmies])\n    delta_death_enemy = np.zeros([num_enmies])\n    neg_scale = self.reward_negative_scale\n    if is_opponent:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    else:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    num_players = 2 if self.two_player else 1\n    for (al_id, al_unit) in iterator:\n        if death_tracker[al_id] < num_players:\n            prev_health = previous_units[al_id].health + previous_units[al_id].shield\n            if al_unit.health == 0:\n                death_tracker[al_id] += 1\n                delta_deaths[al_id] -= self.reward_death_value * neg_scale\n                delta_ally[al_id] += prev_health * neg_scale\n            else:\n                delta_ally[al_id] += neg_scale * (prev_health - al_unit.health - al_unit.shield)\n    if is_opponent:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    else:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    for (e_id, e_unit) in iterator:\n        if death_tracker[e_id] < num_players:\n            prev_health = previous_units[e_id].health + previous_units[e_id].shield\n            if e_unit.health == 0:\n                death_tracker[e_id] += 1\n                delta_death_enemy[e_id] += self.reward_death_value\n                delta_enemy[e_id] += prev_health\n            else:\n                delta_enemy[e_id] += prev_health - e_unit.health - e_unit.shield\n    if self.reward_only_positive:\n        reward = abs(delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum())\n    else:\n        reward = delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum() - delta_ally.sum()\n    return reward",
            "def reward_battle_split(self, engine, action, is_opponent=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Reward function when self.reward_type != 'sparse'.\\n        Returns accumulative hit/shield point damage dealt to the enemy\\n        + reward_death_value per enemy unit killed, and, in case\\n        self.reward_only_positive == False, - (damage dealt to ally units\\n        + reward_death_value per ally unit killed) * self.reward_negative_scale\\n        \"\n    num_agents = engine.n_agents if not is_opponent else engine.n_enemies\n    num_enmies = engine.n_agents if is_opponent else engine.n_enemies\n    if self.reward_type == 'sparse':\n        if self.reduce_agent:\n            return 0.0\n        else:\n            return np.zeros(num_agents)\n    assert self.reward_type == 'original', 'reward_type={} is not supported!'.format(self.reward_type)\n    delta_deaths = np.zeros([num_agents])\n    reward = np.zeros([num_agents])\n    delta_ally = np.zeros([num_agents])\n    delta_enemy = np.zeros([num_enmies])\n    delta_death_enemy = np.zeros([num_enmies])\n    neg_scale = self.reward_negative_scale\n    if is_opponent:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    else:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    num_players = 2 if self.two_player else 1\n    for (al_id, al_unit) in iterator:\n        if death_tracker[al_id] < num_players:\n            prev_health = previous_units[al_id].health + previous_units[al_id].shield\n            if al_unit.health == 0:\n                death_tracker[al_id] += 1\n                delta_deaths[al_id] -= self.reward_death_value * neg_scale\n                delta_ally[al_id] += prev_health * neg_scale\n            else:\n                delta_ally[al_id] += neg_scale * (prev_health - al_unit.health - al_unit.shield)\n    if is_opponent:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    else:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    for (e_id, e_unit) in iterator:\n        if death_tracker[e_id] < num_players:\n            prev_health = previous_units[e_id].health + previous_units[e_id].shield\n            if e_unit.health == 0:\n                death_tracker[e_id] += 1\n                delta_death_enemy[e_id] += self.reward_death_value\n                delta_enemy[e_id] += prev_health\n            else:\n                delta_enemy[e_id] += prev_health - e_unit.health - e_unit.shield\n    if self.reward_only_positive:\n        reward = abs(delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum())\n    else:\n        reward = delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum() - delta_ally.sum()\n    return reward",
            "def reward_battle_split(self, engine, action, is_opponent=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Reward function when self.reward_type != 'sparse'.\\n        Returns accumulative hit/shield point damage dealt to the enemy\\n        + reward_death_value per enemy unit killed, and, in case\\n        self.reward_only_positive == False, - (damage dealt to ally units\\n        + reward_death_value per ally unit killed) * self.reward_negative_scale\\n        \"\n    num_agents = engine.n_agents if not is_opponent else engine.n_enemies\n    num_enmies = engine.n_agents if is_opponent else engine.n_enemies\n    if self.reward_type == 'sparse':\n        if self.reduce_agent:\n            return 0.0\n        else:\n            return np.zeros(num_agents)\n    assert self.reward_type == 'original', 'reward_type={} is not supported!'.format(self.reward_type)\n    delta_deaths = np.zeros([num_agents])\n    reward = np.zeros([num_agents])\n    delta_ally = np.zeros([num_agents])\n    delta_enemy = np.zeros([num_enmies])\n    delta_death_enemy = np.zeros([num_enmies])\n    neg_scale = self.reward_negative_scale\n    if is_opponent:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    else:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    num_players = 2 if self.two_player else 1\n    for (al_id, al_unit) in iterator:\n        if death_tracker[al_id] < num_players:\n            prev_health = previous_units[al_id].health + previous_units[al_id].shield\n            if al_unit.health == 0:\n                death_tracker[al_id] += 1\n                delta_deaths[al_id] -= self.reward_death_value * neg_scale\n                delta_ally[al_id] += prev_health * neg_scale\n            else:\n                delta_ally[al_id] += neg_scale * (prev_health - al_unit.health - al_unit.shield)\n    if is_opponent:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    else:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    for (e_id, e_unit) in iterator:\n        if death_tracker[e_id] < num_players:\n            prev_health = previous_units[e_id].health + previous_units[e_id].shield\n            if e_unit.health == 0:\n                death_tracker[e_id] += 1\n                delta_death_enemy[e_id] += self.reward_death_value\n                delta_enemy[e_id] += prev_health\n            else:\n                delta_enemy[e_id] += prev_health - e_unit.health - e_unit.shield\n    if self.reward_only_positive:\n        reward = abs(delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum())\n    else:\n        reward = delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum() - delta_ally.sum()\n    return reward",
            "def reward_battle_split(self, engine, action, is_opponent=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Reward function when self.reward_type != 'sparse'.\\n        Returns accumulative hit/shield point damage dealt to the enemy\\n        + reward_death_value per enemy unit killed, and, in case\\n        self.reward_only_positive == False, - (damage dealt to ally units\\n        + reward_death_value per ally unit killed) * self.reward_negative_scale\\n        \"\n    num_agents = engine.n_agents if not is_opponent else engine.n_enemies\n    num_enmies = engine.n_agents if is_opponent else engine.n_enemies\n    if self.reward_type == 'sparse':\n        if self.reduce_agent:\n            return 0.0\n        else:\n            return np.zeros(num_agents)\n    assert self.reward_type == 'original', 'reward_type={} is not supported!'.format(self.reward_type)\n    delta_deaths = np.zeros([num_agents])\n    reward = np.zeros([num_agents])\n    delta_ally = np.zeros([num_agents])\n    delta_enemy = np.zeros([num_enmies])\n    delta_death_enemy = np.zeros([num_enmies])\n    neg_scale = self.reward_negative_scale\n    if is_opponent:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    else:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    num_players = 2 if self.two_player else 1\n    for (al_id, al_unit) in iterator:\n        if death_tracker[al_id] < num_players:\n            prev_health = previous_units[al_id].health + previous_units[al_id].shield\n            if al_unit.health == 0:\n                death_tracker[al_id] += 1\n                delta_deaths[al_id] -= self.reward_death_value * neg_scale\n                delta_ally[al_id] += prev_health * neg_scale\n            else:\n                delta_ally[al_id] += neg_scale * (prev_health - al_unit.health - al_unit.shield)\n    if is_opponent:\n        iterator = engine.agents.items()\n        previous_units = engine.previous_ally_units\n        death_tracker = self.death_tracker_ally\n    else:\n        iterator = engine.enemies.items()\n        previous_units = engine.previous_enemy_units\n        death_tracker = self.death_tracker_enemy\n    for (e_id, e_unit) in iterator:\n        if death_tracker[e_id] < num_players:\n            prev_health = previous_units[e_id].health + previous_units[e_id].shield\n            if e_unit.health == 0:\n                death_tracker[e_id] += 1\n                delta_death_enemy[e_id] += self.reward_death_value\n                delta_enemy[e_id] += prev_health\n            else:\n                delta_enemy[e_id] += prev_health - e_unit.health - e_unit.shield\n    if self.reward_only_positive:\n        reward = abs(delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum())\n    else:\n        reward = delta_deaths.sum() + delta_death_enemy.sum() + delta_enemy.sum() - delta_ally.sum()\n    return reward"
        ]
    },
    {
        "func_name": "info",
        "original": "def info(self):\n    if self.reward_type == 'sparse':\n        value = {'min': -1, 'max': 1}\n    elif self.reward_type == 'original':\n        value = {'min': 0, 'max': self.max_reward / self.reward_scale_rate}\n    shape = (1,) if self.reduce_agent else (self.n_agents,)\n    return SMACReward.info_template(shape, value, None, None)",
        "mutated": [
            "def info(self):\n    if False:\n        i = 10\n    if self.reward_type == 'sparse':\n        value = {'min': -1, 'max': 1}\n    elif self.reward_type == 'original':\n        value = {'min': 0, 'max': self.max_reward / self.reward_scale_rate}\n    shape = (1,) if self.reduce_agent else (self.n_agents,)\n    return SMACReward.info_template(shape, value, None, None)",
            "def info(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.reward_type == 'sparse':\n        value = {'min': -1, 'max': 1}\n    elif self.reward_type == 'original':\n        value = {'min': 0, 'max': self.max_reward / self.reward_scale_rate}\n    shape = (1,) if self.reduce_agent else (self.n_agents,)\n    return SMACReward.info_template(shape, value, None, None)",
            "def info(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.reward_type == 'sparse':\n        value = {'min': -1, 'max': 1}\n    elif self.reward_type == 'original':\n        value = {'min': 0, 'max': self.max_reward / self.reward_scale_rate}\n    shape = (1,) if self.reduce_agent else (self.n_agents,)\n    return SMACReward.info_template(shape, value, None, None)",
            "def info(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.reward_type == 'sparse':\n        value = {'min': -1, 'max': 1}\n    elif self.reward_type == 'original':\n        value = {'min': 0, 'max': self.max_reward / self.reward_scale_rate}\n    shape = (1,) if self.reduce_agent else (self.n_agents,)\n    return SMACReward.info_template(shape, value, None, None)",
            "def info(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.reward_type == 'sparse':\n        value = {'min': -1, 'max': 1}\n    elif self.reward_type == 'original':\n        value = {'min': 0, 'max': self.max_reward / self.reward_scale_rate}\n    shape = (1,) if self.reduce_agent else (self.n_agents,)\n    return SMACReward.info_template(shape, value, None, None)"
        ]
    }
]