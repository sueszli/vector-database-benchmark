[
    {
        "func_name": "_partial_multi_dot",
        "original": "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    \"\"\"Computes a generalized dot product avoiding one dimension.\n\n  This is used to directly get the expected return of a given action, given\n  other players' strategies, for the player indexed by index_avoided.\n  Note that the numpy.dot function is used to compute this product, as it ended\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\n  reduce function proved slower for both np.dot and np.tensordot.\n\n  Args:\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\n    strategies: Meta strategy probabilities for each player.\n    index_avoided: Player for which we do not compute the dot product.\n\n  Returns:\n    Vector of expected returns for each action of player [the player indexed by\n      index_avoided].\n  \"\"\"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator",
        "mutated": [
            "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    if False:\n        i = 10\n    \"Computes a generalized dot product avoiding one dimension.\\n\\n  This is used to directly get the expected return of a given action, given\\n  other players' strategies, for the player indexed by index_avoided.\\n  Note that the numpy.dot function is used to compute this product, as it ended\\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\\n  reduce function proved slower for both np.dot and np.tensordot.\\n\\n  Args:\\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\\n    strategies: Meta strategy probabilities for each player.\\n    index_avoided: Player for which we do not compute the dot product.\\n\\n  Returns:\\n    Vector of expected returns for each action of player [the player indexed by\\n      index_avoided].\\n  \"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator",
            "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes a generalized dot product avoiding one dimension.\\n\\n  This is used to directly get the expected return of a given action, given\\n  other players' strategies, for the player indexed by index_avoided.\\n  Note that the numpy.dot function is used to compute this product, as it ended\\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\\n  reduce function proved slower for both np.dot and np.tensordot.\\n\\n  Args:\\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\\n    strategies: Meta strategy probabilities for each player.\\n    index_avoided: Player for which we do not compute the dot product.\\n\\n  Returns:\\n    Vector of expected returns for each action of player [the player indexed by\\n      index_avoided].\\n  \"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator",
            "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes a generalized dot product avoiding one dimension.\\n\\n  This is used to directly get the expected return of a given action, given\\n  other players' strategies, for the player indexed by index_avoided.\\n  Note that the numpy.dot function is used to compute this product, as it ended\\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\\n  reduce function proved slower for both np.dot and np.tensordot.\\n\\n  Args:\\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\\n    strategies: Meta strategy probabilities for each player.\\n    index_avoided: Player for which we do not compute the dot product.\\n\\n  Returns:\\n    Vector of expected returns for each action of player [the player indexed by\\n      index_avoided].\\n  \"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator",
            "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes a generalized dot product avoiding one dimension.\\n\\n  This is used to directly get the expected return of a given action, given\\n  other players' strategies, for the player indexed by index_avoided.\\n  Note that the numpy.dot function is used to compute this product, as it ended\\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\\n  reduce function proved slower for both np.dot and np.tensordot.\\n\\n  Args:\\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\\n    strategies: Meta strategy probabilities for each player.\\n    index_avoided: Player for which we do not compute the dot product.\\n\\n  Returns:\\n    Vector of expected returns for each action of player [the player indexed by\\n      index_avoided].\\n  \"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator",
            "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes a generalized dot product avoiding one dimension.\\n\\n  This is used to directly get the expected return of a given action, given\\n  other players' strategies, for the player indexed by index_avoided.\\n  Note that the numpy.dot function is used to compute this product, as it ended\\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\\n  reduce function proved slower for both np.dot and np.tensordot.\\n\\n  Args:\\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\\n    strategies: Meta strategy probabilities for each player.\\n    index_avoided: Player for which we do not compute the dot product.\\n\\n  Returns:\\n    Vector of expected returns for each action of player [the player indexed by\\n      index_avoided].\\n  \"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator"
        ]
    },
    {
        "func_name": "_regret_matching_step",
        "original": "def _regret_matching_step(payoff_tensors, strategies, regrets, gamma):\n    \"\"\"Does one step of the projected replicator dynamics algorithm.\n\n  Args:\n    payoff_tensors: List of payoff tensors for each player.\n    strategies: List of the strategies used by each player.\n    regrets: List of cumulative regrets used by each player.\n    gamma: Minimum exploratory probability term.\n\n  Returns:\n    A list of updated strategies for each player.\n  \"\"\"\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        regrets[player] += values_per_strategy - average_return\n        updated_strategy = regrets[player].copy()\n        updated_strategy[updated_strategy < 0] = 0.0\n        sum_regret = updated_strategy.sum()\n        uniform_strategy = np.ones(len(updated_strategy)) / len(updated_strategy)\n        if sum_regret > 0:\n            updated_strategy /= sum_regret\n            updated_strategy = gamma * uniform_strategy + (1 - gamma) * updated_strategy\n        else:\n            updated_strategy = uniform_strategy\n        new_strategies.append(updated_strategy)\n    return new_strategies",
        "mutated": [
            "def _regret_matching_step(payoff_tensors, strategies, regrets, gamma):\n    if False:\n        i = 10\n    'Does one step of the projected replicator dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    strategies: List of the strategies used by each player.\\n    regrets: List of cumulative regrets used by each player.\\n    gamma: Minimum exploratory probability term.\\n\\n  Returns:\\n    A list of updated strategies for each player.\\n  '\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        regrets[player] += values_per_strategy - average_return\n        updated_strategy = regrets[player].copy()\n        updated_strategy[updated_strategy < 0] = 0.0\n        sum_regret = updated_strategy.sum()\n        uniform_strategy = np.ones(len(updated_strategy)) / len(updated_strategy)\n        if sum_regret > 0:\n            updated_strategy /= sum_regret\n            updated_strategy = gamma * uniform_strategy + (1 - gamma) * updated_strategy\n        else:\n            updated_strategy = uniform_strategy\n        new_strategies.append(updated_strategy)\n    return new_strategies",
            "def _regret_matching_step(payoff_tensors, strategies, regrets, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Does one step of the projected replicator dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    strategies: List of the strategies used by each player.\\n    regrets: List of cumulative regrets used by each player.\\n    gamma: Minimum exploratory probability term.\\n\\n  Returns:\\n    A list of updated strategies for each player.\\n  '\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        regrets[player] += values_per_strategy - average_return\n        updated_strategy = regrets[player].copy()\n        updated_strategy[updated_strategy < 0] = 0.0\n        sum_regret = updated_strategy.sum()\n        uniform_strategy = np.ones(len(updated_strategy)) / len(updated_strategy)\n        if sum_regret > 0:\n            updated_strategy /= sum_regret\n            updated_strategy = gamma * uniform_strategy + (1 - gamma) * updated_strategy\n        else:\n            updated_strategy = uniform_strategy\n        new_strategies.append(updated_strategy)\n    return new_strategies",
            "def _regret_matching_step(payoff_tensors, strategies, regrets, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Does one step of the projected replicator dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    strategies: List of the strategies used by each player.\\n    regrets: List of cumulative regrets used by each player.\\n    gamma: Minimum exploratory probability term.\\n\\n  Returns:\\n    A list of updated strategies for each player.\\n  '\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        regrets[player] += values_per_strategy - average_return\n        updated_strategy = regrets[player].copy()\n        updated_strategy[updated_strategy < 0] = 0.0\n        sum_regret = updated_strategy.sum()\n        uniform_strategy = np.ones(len(updated_strategy)) / len(updated_strategy)\n        if sum_regret > 0:\n            updated_strategy /= sum_regret\n            updated_strategy = gamma * uniform_strategy + (1 - gamma) * updated_strategy\n        else:\n            updated_strategy = uniform_strategy\n        new_strategies.append(updated_strategy)\n    return new_strategies",
            "def _regret_matching_step(payoff_tensors, strategies, regrets, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Does one step of the projected replicator dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    strategies: List of the strategies used by each player.\\n    regrets: List of cumulative regrets used by each player.\\n    gamma: Minimum exploratory probability term.\\n\\n  Returns:\\n    A list of updated strategies for each player.\\n  '\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        regrets[player] += values_per_strategy - average_return\n        updated_strategy = regrets[player].copy()\n        updated_strategy[updated_strategy < 0] = 0.0\n        sum_regret = updated_strategy.sum()\n        uniform_strategy = np.ones(len(updated_strategy)) / len(updated_strategy)\n        if sum_regret > 0:\n            updated_strategy /= sum_regret\n            updated_strategy = gamma * uniform_strategy + (1 - gamma) * updated_strategy\n        else:\n            updated_strategy = uniform_strategy\n        new_strategies.append(updated_strategy)\n    return new_strategies",
            "def _regret_matching_step(payoff_tensors, strategies, regrets, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Does one step of the projected replicator dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    strategies: List of the strategies used by each player.\\n    regrets: List of cumulative regrets used by each player.\\n    gamma: Minimum exploratory probability term.\\n\\n  Returns:\\n    A list of updated strategies for each player.\\n  '\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        regrets[player] += values_per_strategy - average_return\n        updated_strategy = regrets[player].copy()\n        updated_strategy[updated_strategy < 0] = 0.0\n        sum_regret = updated_strategy.sum()\n        uniform_strategy = np.ones(len(updated_strategy)) / len(updated_strategy)\n        if sum_regret > 0:\n            updated_strategy /= sum_regret\n            updated_strategy = gamma * uniform_strategy + (1 - gamma) * updated_strategy\n        else:\n            updated_strategy = uniform_strategy\n        new_strategies.append(updated_strategy)\n    return new_strategies"
        ]
    },
    {
        "func_name": "regret_matching",
        "original": "def regret_matching(payoff_tensors, initial_strategies=None, iterations=int(100000.0), gamma=1e-06, average_over_last_n_strategies=None, **unused_kwargs):\n    \"\"\"Runs regret-matching for the stated number of iterations.\n\n  Args:\n    payoff_tensors: List of payoff tensors for each player.\n    initial_strategies: Initial list of the strategies used by each player, if\n      any. Could be used to speed up the search by providing a good initial\n      solution.\n    iterations: Number of algorithmic steps to take before returning an answer.\n    gamma: Minimum exploratory probability term.\n    average_over_last_n_strategies: Running average window size for average\n      policy computation. If None, use the whole trajectory.\n    **unused_kwargs: Convenient way of exposing an API compatible with other\n      methods with possibly different arguments.\n\n  Returns:\n    RM-computed strategies.\n  \"\"\"\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    regrets = [np.ones(action_space_shapes[k]) / INITIAL_REGRET_DENOM for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(iterations):\n        new_strategies = _regret_matching_step(payoff_tensors, new_strategies, regrets, gamma)\n        averager.append(new_strategies)\n    return averager.average_strategies()",
        "mutated": [
            "def regret_matching(payoff_tensors, initial_strategies=None, iterations=int(100000.0), gamma=1e-06, average_over_last_n_strategies=None, **unused_kwargs):\n    if False:\n        i = 10\n    'Runs regret-matching for the stated number of iterations.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    initial_strategies: Initial list of the strategies used by each player, if\\n      any. Could be used to speed up the search by providing a good initial\\n      solution.\\n    iterations: Number of algorithmic steps to take before returning an answer.\\n    gamma: Minimum exploratory probability term.\\n    average_over_last_n_strategies: Running average window size for average\\n      policy computation. If None, use the whole trajectory.\\n    **unused_kwargs: Convenient way of exposing an API compatible with other\\n      methods with possibly different arguments.\\n\\n  Returns:\\n    RM-computed strategies.\\n  '\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    regrets = [np.ones(action_space_shapes[k]) / INITIAL_REGRET_DENOM for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(iterations):\n        new_strategies = _regret_matching_step(payoff_tensors, new_strategies, regrets, gamma)\n        averager.append(new_strategies)\n    return averager.average_strategies()",
            "def regret_matching(payoff_tensors, initial_strategies=None, iterations=int(100000.0), gamma=1e-06, average_over_last_n_strategies=None, **unused_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Runs regret-matching for the stated number of iterations.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    initial_strategies: Initial list of the strategies used by each player, if\\n      any. Could be used to speed up the search by providing a good initial\\n      solution.\\n    iterations: Number of algorithmic steps to take before returning an answer.\\n    gamma: Minimum exploratory probability term.\\n    average_over_last_n_strategies: Running average window size for average\\n      policy computation. If None, use the whole trajectory.\\n    **unused_kwargs: Convenient way of exposing an API compatible with other\\n      methods with possibly different arguments.\\n\\n  Returns:\\n    RM-computed strategies.\\n  '\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    regrets = [np.ones(action_space_shapes[k]) / INITIAL_REGRET_DENOM for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(iterations):\n        new_strategies = _regret_matching_step(payoff_tensors, new_strategies, regrets, gamma)\n        averager.append(new_strategies)\n    return averager.average_strategies()",
            "def regret_matching(payoff_tensors, initial_strategies=None, iterations=int(100000.0), gamma=1e-06, average_over_last_n_strategies=None, **unused_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Runs regret-matching for the stated number of iterations.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    initial_strategies: Initial list of the strategies used by each player, if\\n      any. Could be used to speed up the search by providing a good initial\\n      solution.\\n    iterations: Number of algorithmic steps to take before returning an answer.\\n    gamma: Minimum exploratory probability term.\\n    average_over_last_n_strategies: Running average window size for average\\n      policy computation. If None, use the whole trajectory.\\n    **unused_kwargs: Convenient way of exposing an API compatible with other\\n      methods with possibly different arguments.\\n\\n  Returns:\\n    RM-computed strategies.\\n  '\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    regrets = [np.ones(action_space_shapes[k]) / INITIAL_REGRET_DENOM for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(iterations):\n        new_strategies = _regret_matching_step(payoff_tensors, new_strategies, regrets, gamma)\n        averager.append(new_strategies)\n    return averager.average_strategies()",
            "def regret_matching(payoff_tensors, initial_strategies=None, iterations=int(100000.0), gamma=1e-06, average_over_last_n_strategies=None, **unused_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Runs regret-matching for the stated number of iterations.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    initial_strategies: Initial list of the strategies used by each player, if\\n      any. Could be used to speed up the search by providing a good initial\\n      solution.\\n    iterations: Number of algorithmic steps to take before returning an answer.\\n    gamma: Minimum exploratory probability term.\\n    average_over_last_n_strategies: Running average window size for average\\n      policy computation. If None, use the whole trajectory.\\n    **unused_kwargs: Convenient way of exposing an API compatible with other\\n      methods with possibly different arguments.\\n\\n  Returns:\\n    RM-computed strategies.\\n  '\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    regrets = [np.ones(action_space_shapes[k]) / INITIAL_REGRET_DENOM for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(iterations):\n        new_strategies = _regret_matching_step(payoff_tensors, new_strategies, regrets, gamma)\n        averager.append(new_strategies)\n    return averager.average_strategies()",
            "def regret_matching(payoff_tensors, initial_strategies=None, iterations=int(100000.0), gamma=1e-06, average_over_last_n_strategies=None, **unused_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Runs regret-matching for the stated number of iterations.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    initial_strategies: Initial list of the strategies used by each player, if\\n      any. Could be used to speed up the search by providing a good initial\\n      solution.\\n    iterations: Number of algorithmic steps to take before returning an answer.\\n    gamma: Minimum exploratory probability term.\\n    average_over_last_n_strategies: Running average window size for average\\n      policy computation. If None, use the whole trajectory.\\n    **unused_kwargs: Convenient way of exposing an API compatible with other\\n      methods with possibly different arguments.\\n\\n  Returns:\\n    RM-computed strategies.\\n  '\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    regrets = [np.ones(action_space_shapes[k]) / INITIAL_REGRET_DENOM for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(iterations):\n        new_strategies = _regret_matching_step(payoff_tensors, new_strategies, regrets, gamma)\n        averager.append(new_strategies)\n    return averager.average_strategies()"
        ]
    }
]