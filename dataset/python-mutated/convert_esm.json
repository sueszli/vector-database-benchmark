[
    {
        "func_name": "get_esmfold_tokenizer",
        "original": "def get_esmfold_tokenizer():\n    with TemporaryDirectory() as tempdir:\n        vocab = '\\n'.join(restypes_with_extras)\n        vocab_file = Path(tempdir) / 'vocab.txt'\n        vocab_file.write_text(vocab)\n        hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n    hf_tokenizer.pad_token_id = 0\n    return hf_tokenizer",
        "mutated": [
            "def get_esmfold_tokenizer():\n    if False:\n        i = 10\n    with TemporaryDirectory() as tempdir:\n        vocab = '\\n'.join(restypes_with_extras)\n        vocab_file = Path(tempdir) / 'vocab.txt'\n        vocab_file.write_text(vocab)\n        hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n    hf_tokenizer.pad_token_id = 0\n    return hf_tokenizer",
            "def get_esmfold_tokenizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with TemporaryDirectory() as tempdir:\n        vocab = '\\n'.join(restypes_with_extras)\n        vocab_file = Path(tempdir) / 'vocab.txt'\n        vocab_file.write_text(vocab)\n        hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n    hf_tokenizer.pad_token_id = 0\n    return hf_tokenizer",
            "def get_esmfold_tokenizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with TemporaryDirectory() as tempdir:\n        vocab = '\\n'.join(restypes_with_extras)\n        vocab_file = Path(tempdir) / 'vocab.txt'\n        vocab_file.write_text(vocab)\n        hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n    hf_tokenizer.pad_token_id = 0\n    return hf_tokenizer",
            "def get_esmfold_tokenizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with TemporaryDirectory() as tempdir:\n        vocab = '\\n'.join(restypes_with_extras)\n        vocab_file = Path(tempdir) / 'vocab.txt'\n        vocab_file.write_text(vocab)\n        hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n    hf_tokenizer.pad_token_id = 0\n    return hf_tokenizer",
            "def get_esmfold_tokenizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with TemporaryDirectory() as tempdir:\n        vocab = '\\n'.join(restypes_with_extras)\n        vocab_file = Path(tempdir) / 'vocab.txt'\n        vocab_file.write_text(vocab)\n        hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n    hf_tokenizer.pad_token_id = 0\n    return hf_tokenizer"
        ]
    },
    {
        "func_name": "transfer_and_check_weights",
        "original": "def transfer_and_check_weights(original_module, our_module):\n    status = our_module.load_state_dict(original_module.state_dict())\n    if status.missing_keys:\n        raise ValueError(f'Missing keys: {status.missing_keys}')\n    if status.unexpected_keys:\n        raise ValueError(f'Unexpected keys: {status.unexpected_keys}')",
        "mutated": [
            "def transfer_and_check_weights(original_module, our_module):\n    if False:\n        i = 10\n    status = our_module.load_state_dict(original_module.state_dict())\n    if status.missing_keys:\n        raise ValueError(f'Missing keys: {status.missing_keys}')\n    if status.unexpected_keys:\n        raise ValueError(f'Unexpected keys: {status.unexpected_keys}')",
            "def transfer_and_check_weights(original_module, our_module):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    status = our_module.load_state_dict(original_module.state_dict())\n    if status.missing_keys:\n        raise ValueError(f'Missing keys: {status.missing_keys}')\n    if status.unexpected_keys:\n        raise ValueError(f'Unexpected keys: {status.unexpected_keys}')",
            "def transfer_and_check_weights(original_module, our_module):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    status = our_module.load_state_dict(original_module.state_dict())\n    if status.missing_keys:\n        raise ValueError(f'Missing keys: {status.missing_keys}')\n    if status.unexpected_keys:\n        raise ValueError(f'Unexpected keys: {status.unexpected_keys}')",
            "def transfer_and_check_weights(original_module, our_module):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    status = our_module.load_state_dict(original_module.state_dict())\n    if status.missing_keys:\n        raise ValueError(f'Missing keys: {status.missing_keys}')\n    if status.unexpected_keys:\n        raise ValueError(f'Unexpected keys: {status.unexpected_keys}')",
            "def transfer_and_check_weights(original_module, our_module):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    status = our_module.load_state_dict(original_module.state_dict())\n    if status.missing_keys:\n        raise ValueError(f'Missing keys: {status.missing_keys}')\n    if status.unexpected_keys:\n        raise ValueError(f'Unexpected keys: {status.unexpected_keys}')"
        ]
    },
    {
        "func_name": "convert_esm_checkpoint_to_pytorch",
        "original": "def convert_esm_checkpoint_to_pytorch(model: str, pytorch_dump_folder_path: str, classification_head: bool, push_to_repo: str, auth_token: str):\n    \"\"\"\n    Copy/paste/tweak esm's weights to our BERT structure.\n    \"\"\"\n    if model.startswith('esmfold'):\n        esm = MODEL_MAPPING[model]()\n    else:\n        (esm, alphabet) = MODEL_MAPPING[model]()\n    esm.eval()\n    if model.startswith('esmfold'):\n        embed_dim = esm.esm.embed_dim\n        num_layers = esm.esm.num_layers\n        num_attention_heads = esm.esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = True\n        esmfold_config = EsmFoldConfig()\n        for (key, val) in esm.cfg.items():\n            if hasattr(esmfold_config, key) and key != 'trunk':\n                setattr(esmfold_config, key, val)\n        for (key, val) in esm.cfg.trunk.items():\n            if hasattr(esmfold_config.trunk, key) and key != 'structure_module':\n                setattr(esmfold_config.trunk, key, val)\n        for (key, val) in esm.cfg.trunk.structure_module.items():\n            if hasattr(esmfold_config.trunk.structure_module, key):\n                setattr(esmfold_config.trunk.structure_module, key, val)\n    elif hasattr(esm, 'args'):\n        embed_dim = esm.args.embed_dim\n        num_layers = esm.args.layers\n        num_attention_heads = esm.args.attention_heads\n        intermediate_size = esm.args.ffn_embed_dim\n        token_dropout = esm.args.token_dropout\n        emb_layer_norm_before = True if esm.emb_layer_norm_before else False\n        position_embedding_type = 'absolute'\n        is_folding_model = False\n        esmfold_config = None\n    else:\n        embed_dim = esm.embed_dim\n        num_layers = esm.num_layers\n        num_attention_heads = esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = False\n        esmfold_config = None\n    if is_folding_model:\n        alphabet = esm.esm.alphabet\n    vocab_list = tuple(alphabet.all_toks)\n    mask_token_id = alphabet.mask_idx\n    pad_token_id = alphabet.padding_idx\n    if is_folding_model:\n        original_esm_model = esm.esm\n    else:\n        original_esm_model = esm\n    config = EsmConfig(vocab_size=original_esm_model.embed_tokens.num_embeddings, mask_token_id=mask_token_id, hidden_size=embed_dim, num_hidden_layers=num_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=1026, layer_norm_eps=1e-05, attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, pad_token_id=pad_token_id, emb_layer_norm_before=emb_layer_norm_before, token_dropout=token_dropout, position_embedding_type=position_embedding_type, is_folding_model=is_folding_model, esmfold_config=esmfold_config, vocab_list=vocab_list)\n    if classification_head:\n        config.num_labels = esm.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our ESM config:', config)\n    if model.startswith('esmfold'):\n        model_class = EsmForProteinFolding\n    elif classification_head:\n        model_class = EsmForSequenceClassification\n    else:\n        model_class = EsmForMaskedLM\n    model = model_class(config)\n    model.eval()\n    model.esm.embeddings.word_embeddings.weight = original_esm_model.embed_tokens.weight\n    if position_embedding_type == 'absolute':\n        model.esm.embeddings.position_embeddings.weight = original_esm_model.embed_positions.weight\n    if config.emb_layer_norm_before:\n        model.esm.embeddings.layer_norm.weight = original_esm_model.emb_layer_norm_before.weight\n        model.esm.embeddings.layer_norm.bias = original_esm_model.emb_layer_norm_before.bias\n    model.esm.encoder.emb_layer_norm_after.weight = original_esm_model.emb_layer_norm_after.weight\n    model.esm.encoder.emb_layer_norm_after.bias = original_esm_model.emb_layer_norm_after.bias\n    for i in range(config.num_hidden_layers):\n        layer: EsmLayer = model.esm.encoder.layer[i]\n        esm_layer = original_esm_model.layers[i]\n        self_attn: EsmSelfAttention = layer.attention.self\n        assert esm_layer.self_attn.k_proj.weight.data.shape == esm_layer.self_attn.q_proj.weight.data.shape == esm_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = esm_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = esm_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = esm_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = esm_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = esm_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = esm_layer.self_attn.v_proj.bias\n        if getattr(esm_layer.self_attn, 'rot_emb', None) is not None:\n            self_attn.rotary_embeddings.inv_freq.data = esm_layer.self_attn.rot_emb.inv_freq\n        layer.attention.LayerNorm.weight = esm_layer.self_attn_layer_norm.weight\n        layer.attention.LayerNorm.bias = esm_layer.self_attn_layer_norm.bias\n        layer.LayerNorm.weight = esm_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = esm_layer.final_layer_norm.bias\n        self_output: EsmSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == esm_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = esm_layer.self_attn.out_proj.weight\n        self_output.dense.bias = esm_layer.self_attn.out_proj.bias\n        intermediate: EsmIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == esm_layer.fc1.weight.shape\n        intermediate.dense.weight = esm_layer.fc1.weight\n        intermediate.dense.bias = esm_layer.fc1.bias\n        bert_output: EsmOutput = layer.output\n        assert bert_output.dense.weight.shape == esm_layer.fc2.weight.shape\n        bert_output.dense.weight = esm_layer.fc2.weight\n        bert_output.dense.bias = esm_layer.fc2.bias\n    if is_folding_model:\n        model.esm_s_combine.data = esm.esm_s_combine.data\n        model.af2_to_esm.data = esm.af2_to_esm.data\n        transfer_and_check_weights(esm.embedding, model.embedding)\n        transfer_and_check_weights(esm.esm_s_mlp, model.esm_s_mlp)\n        transfer_and_check_weights(esm.trunk, model.trunk)\n        transfer_and_check_weights(esm.distogram_head, model.distogram_head)\n        transfer_and_check_weights(esm.ptm_head, model.ptm_head)\n        transfer_and_check_weights(esm.lm_head, model.lm_head)\n        transfer_and_check_weights(esm.lddt_head, model.lddt_head)\n    elif classification_head:\n        model.classifier.dense.weight = esm.esm.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = esm.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = esm.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = esm.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = esm.lm_head.dense.weight\n        model.lm_head.dense.bias = esm.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = esm.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = esm.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = esm.lm_head.weight\n        model.lm_head.bias = esm.lm_head.bias\n    transfer_and_check_weights(esm.contact_head, model.esm.contact_head)\n    if is_folding_model:\n        sample_data = SAMPLE_DATA[:2]\n    else:\n        sample_data = SAMPLE_DATA\n    if is_folding_model:\n        hf_tokenizer = get_esmfold_tokenizer()\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True, add_special_tokens=False)\n        (esmfold_aas, esmfold_mask, _, _, _) = esmfold_encode_sequences([row[1] for row in sample_data])\n        success = torch.all(hf_tokens['input_ids'] == esmfold_aas) and torch.all(hf_tokens['attention_mask'] == esmfold_mask)\n    else:\n        batch_converter = alphabet.get_batch_converter()\n        (batch_labels, batch_strs, batch_tokens) = batch_converter(sample_data)\n        with TemporaryDirectory() as tempdir:\n            vocab = '\\n'.join(alphabet.all_toks)\n            vocab_file = Path(tempdir) / 'vocab.txt'\n            vocab_file.write_text(vocab)\n            hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True)\n        success = torch.all(hf_tokens['input_ids'] == batch_tokens)\n    print('Do both models tokenizers output the same tokens?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Tokenization does not match!')\n    with torch.no_grad():\n        if is_folding_model:\n            their_output = esm.cuda().infer([row[1] for row in sample_data])\n            our_output = model.cuda()(input_ids=hf_tokens['input_ids'].cuda(), attention_mask=hf_tokens['attention_mask'].cuda())\n        else:\n            our_output = model(**hf_tokens, output_hidden_states=True)\n            our_output = our_output['logits']\n            if classification_head:\n                their_output = esm.model.classification_heads['mnli'](esm.extract_features(batch_tokens))\n            else:\n                their_output = esm(hf_tokens['input_ids'], repr_layers=list(range(999)))\n                their_output = their_output['logits']\n        if is_folding_model:\n            max_absolute_diff = torch.max(torch.abs(our_output['positions'] - their_output['positions'])).item()\n            success = torch.allclose(our_output['positions'], their_output['positions'], atol=1e-05)\n        else:\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n        print(f'max_absolute_diff = {max_absolute_diff}')\n        print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n        if not success:\n            raise Exception('Something went wRoNg')\n        if not is_folding_model:\n            our_output = model.predict_contacts(hf_tokens['input_ids'], hf_tokens['attention_mask'])\n            their_output = esm.predict_contacts(hf_tokens['input_ids'])\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n            print('Contact prediction testing:')\n            print(f'max_absolute_diff = {max_absolute_diff}')\n            print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n            if not success:\n                raise Exception('Something went wRoNg')\n        pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n        print(f'Saving model to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n        del esm\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    hf_tokenizer.save_pretrained(pytorch_dump_folder_path)\n    if push_to_repo:\n        model.push_to_hub(repo_id=push_to_repo, token_token=auth_token)\n        hf_tokenizer.push_to_hub(repo_id=push_to_repo, token_token=auth_token)",
        "mutated": [
            "def convert_esm_checkpoint_to_pytorch(model: str, pytorch_dump_folder_path: str, classification_head: bool, push_to_repo: str, auth_token: str):\n    if False:\n        i = 10\n    \"\\n    Copy/paste/tweak esm's weights to our BERT structure.\\n    \"\n    if model.startswith('esmfold'):\n        esm = MODEL_MAPPING[model]()\n    else:\n        (esm, alphabet) = MODEL_MAPPING[model]()\n    esm.eval()\n    if model.startswith('esmfold'):\n        embed_dim = esm.esm.embed_dim\n        num_layers = esm.esm.num_layers\n        num_attention_heads = esm.esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = True\n        esmfold_config = EsmFoldConfig()\n        for (key, val) in esm.cfg.items():\n            if hasattr(esmfold_config, key) and key != 'trunk':\n                setattr(esmfold_config, key, val)\n        for (key, val) in esm.cfg.trunk.items():\n            if hasattr(esmfold_config.trunk, key) and key != 'structure_module':\n                setattr(esmfold_config.trunk, key, val)\n        for (key, val) in esm.cfg.trunk.structure_module.items():\n            if hasattr(esmfold_config.trunk.structure_module, key):\n                setattr(esmfold_config.trunk.structure_module, key, val)\n    elif hasattr(esm, 'args'):\n        embed_dim = esm.args.embed_dim\n        num_layers = esm.args.layers\n        num_attention_heads = esm.args.attention_heads\n        intermediate_size = esm.args.ffn_embed_dim\n        token_dropout = esm.args.token_dropout\n        emb_layer_norm_before = True if esm.emb_layer_norm_before else False\n        position_embedding_type = 'absolute'\n        is_folding_model = False\n        esmfold_config = None\n    else:\n        embed_dim = esm.embed_dim\n        num_layers = esm.num_layers\n        num_attention_heads = esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = False\n        esmfold_config = None\n    if is_folding_model:\n        alphabet = esm.esm.alphabet\n    vocab_list = tuple(alphabet.all_toks)\n    mask_token_id = alphabet.mask_idx\n    pad_token_id = alphabet.padding_idx\n    if is_folding_model:\n        original_esm_model = esm.esm\n    else:\n        original_esm_model = esm\n    config = EsmConfig(vocab_size=original_esm_model.embed_tokens.num_embeddings, mask_token_id=mask_token_id, hidden_size=embed_dim, num_hidden_layers=num_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=1026, layer_norm_eps=1e-05, attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, pad_token_id=pad_token_id, emb_layer_norm_before=emb_layer_norm_before, token_dropout=token_dropout, position_embedding_type=position_embedding_type, is_folding_model=is_folding_model, esmfold_config=esmfold_config, vocab_list=vocab_list)\n    if classification_head:\n        config.num_labels = esm.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our ESM config:', config)\n    if model.startswith('esmfold'):\n        model_class = EsmForProteinFolding\n    elif classification_head:\n        model_class = EsmForSequenceClassification\n    else:\n        model_class = EsmForMaskedLM\n    model = model_class(config)\n    model.eval()\n    model.esm.embeddings.word_embeddings.weight = original_esm_model.embed_tokens.weight\n    if position_embedding_type == 'absolute':\n        model.esm.embeddings.position_embeddings.weight = original_esm_model.embed_positions.weight\n    if config.emb_layer_norm_before:\n        model.esm.embeddings.layer_norm.weight = original_esm_model.emb_layer_norm_before.weight\n        model.esm.embeddings.layer_norm.bias = original_esm_model.emb_layer_norm_before.bias\n    model.esm.encoder.emb_layer_norm_after.weight = original_esm_model.emb_layer_norm_after.weight\n    model.esm.encoder.emb_layer_norm_after.bias = original_esm_model.emb_layer_norm_after.bias\n    for i in range(config.num_hidden_layers):\n        layer: EsmLayer = model.esm.encoder.layer[i]\n        esm_layer = original_esm_model.layers[i]\n        self_attn: EsmSelfAttention = layer.attention.self\n        assert esm_layer.self_attn.k_proj.weight.data.shape == esm_layer.self_attn.q_proj.weight.data.shape == esm_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = esm_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = esm_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = esm_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = esm_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = esm_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = esm_layer.self_attn.v_proj.bias\n        if getattr(esm_layer.self_attn, 'rot_emb', None) is not None:\n            self_attn.rotary_embeddings.inv_freq.data = esm_layer.self_attn.rot_emb.inv_freq\n        layer.attention.LayerNorm.weight = esm_layer.self_attn_layer_norm.weight\n        layer.attention.LayerNorm.bias = esm_layer.self_attn_layer_norm.bias\n        layer.LayerNorm.weight = esm_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = esm_layer.final_layer_norm.bias\n        self_output: EsmSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == esm_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = esm_layer.self_attn.out_proj.weight\n        self_output.dense.bias = esm_layer.self_attn.out_proj.bias\n        intermediate: EsmIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == esm_layer.fc1.weight.shape\n        intermediate.dense.weight = esm_layer.fc1.weight\n        intermediate.dense.bias = esm_layer.fc1.bias\n        bert_output: EsmOutput = layer.output\n        assert bert_output.dense.weight.shape == esm_layer.fc2.weight.shape\n        bert_output.dense.weight = esm_layer.fc2.weight\n        bert_output.dense.bias = esm_layer.fc2.bias\n    if is_folding_model:\n        model.esm_s_combine.data = esm.esm_s_combine.data\n        model.af2_to_esm.data = esm.af2_to_esm.data\n        transfer_and_check_weights(esm.embedding, model.embedding)\n        transfer_and_check_weights(esm.esm_s_mlp, model.esm_s_mlp)\n        transfer_and_check_weights(esm.trunk, model.trunk)\n        transfer_and_check_weights(esm.distogram_head, model.distogram_head)\n        transfer_and_check_weights(esm.ptm_head, model.ptm_head)\n        transfer_and_check_weights(esm.lm_head, model.lm_head)\n        transfer_and_check_weights(esm.lddt_head, model.lddt_head)\n    elif classification_head:\n        model.classifier.dense.weight = esm.esm.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = esm.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = esm.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = esm.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = esm.lm_head.dense.weight\n        model.lm_head.dense.bias = esm.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = esm.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = esm.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = esm.lm_head.weight\n        model.lm_head.bias = esm.lm_head.bias\n    transfer_and_check_weights(esm.contact_head, model.esm.contact_head)\n    if is_folding_model:\n        sample_data = SAMPLE_DATA[:2]\n    else:\n        sample_data = SAMPLE_DATA\n    if is_folding_model:\n        hf_tokenizer = get_esmfold_tokenizer()\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True, add_special_tokens=False)\n        (esmfold_aas, esmfold_mask, _, _, _) = esmfold_encode_sequences([row[1] for row in sample_data])\n        success = torch.all(hf_tokens['input_ids'] == esmfold_aas) and torch.all(hf_tokens['attention_mask'] == esmfold_mask)\n    else:\n        batch_converter = alphabet.get_batch_converter()\n        (batch_labels, batch_strs, batch_tokens) = batch_converter(sample_data)\n        with TemporaryDirectory() as tempdir:\n            vocab = '\\n'.join(alphabet.all_toks)\n            vocab_file = Path(tempdir) / 'vocab.txt'\n            vocab_file.write_text(vocab)\n            hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True)\n        success = torch.all(hf_tokens['input_ids'] == batch_tokens)\n    print('Do both models tokenizers output the same tokens?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Tokenization does not match!')\n    with torch.no_grad():\n        if is_folding_model:\n            their_output = esm.cuda().infer([row[1] for row in sample_data])\n            our_output = model.cuda()(input_ids=hf_tokens['input_ids'].cuda(), attention_mask=hf_tokens['attention_mask'].cuda())\n        else:\n            our_output = model(**hf_tokens, output_hidden_states=True)\n            our_output = our_output['logits']\n            if classification_head:\n                their_output = esm.model.classification_heads['mnli'](esm.extract_features(batch_tokens))\n            else:\n                their_output = esm(hf_tokens['input_ids'], repr_layers=list(range(999)))\n                their_output = their_output['logits']\n        if is_folding_model:\n            max_absolute_diff = torch.max(torch.abs(our_output['positions'] - their_output['positions'])).item()\n            success = torch.allclose(our_output['positions'], their_output['positions'], atol=1e-05)\n        else:\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n        print(f'max_absolute_diff = {max_absolute_diff}')\n        print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n        if not success:\n            raise Exception('Something went wRoNg')\n        if not is_folding_model:\n            our_output = model.predict_contacts(hf_tokens['input_ids'], hf_tokens['attention_mask'])\n            their_output = esm.predict_contacts(hf_tokens['input_ids'])\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n            print('Contact prediction testing:')\n            print(f'max_absolute_diff = {max_absolute_diff}')\n            print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n            if not success:\n                raise Exception('Something went wRoNg')\n        pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n        print(f'Saving model to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n        del esm\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    hf_tokenizer.save_pretrained(pytorch_dump_folder_path)\n    if push_to_repo:\n        model.push_to_hub(repo_id=push_to_repo, token_token=auth_token)\n        hf_tokenizer.push_to_hub(repo_id=push_to_repo, token_token=auth_token)",
            "def convert_esm_checkpoint_to_pytorch(model: str, pytorch_dump_folder_path: str, classification_head: bool, push_to_repo: str, auth_token: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Copy/paste/tweak esm's weights to our BERT structure.\\n    \"\n    if model.startswith('esmfold'):\n        esm = MODEL_MAPPING[model]()\n    else:\n        (esm, alphabet) = MODEL_MAPPING[model]()\n    esm.eval()\n    if model.startswith('esmfold'):\n        embed_dim = esm.esm.embed_dim\n        num_layers = esm.esm.num_layers\n        num_attention_heads = esm.esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = True\n        esmfold_config = EsmFoldConfig()\n        for (key, val) in esm.cfg.items():\n            if hasattr(esmfold_config, key) and key != 'trunk':\n                setattr(esmfold_config, key, val)\n        for (key, val) in esm.cfg.trunk.items():\n            if hasattr(esmfold_config.trunk, key) and key != 'structure_module':\n                setattr(esmfold_config.trunk, key, val)\n        for (key, val) in esm.cfg.trunk.structure_module.items():\n            if hasattr(esmfold_config.trunk.structure_module, key):\n                setattr(esmfold_config.trunk.structure_module, key, val)\n    elif hasattr(esm, 'args'):\n        embed_dim = esm.args.embed_dim\n        num_layers = esm.args.layers\n        num_attention_heads = esm.args.attention_heads\n        intermediate_size = esm.args.ffn_embed_dim\n        token_dropout = esm.args.token_dropout\n        emb_layer_norm_before = True if esm.emb_layer_norm_before else False\n        position_embedding_type = 'absolute'\n        is_folding_model = False\n        esmfold_config = None\n    else:\n        embed_dim = esm.embed_dim\n        num_layers = esm.num_layers\n        num_attention_heads = esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = False\n        esmfold_config = None\n    if is_folding_model:\n        alphabet = esm.esm.alphabet\n    vocab_list = tuple(alphabet.all_toks)\n    mask_token_id = alphabet.mask_idx\n    pad_token_id = alphabet.padding_idx\n    if is_folding_model:\n        original_esm_model = esm.esm\n    else:\n        original_esm_model = esm\n    config = EsmConfig(vocab_size=original_esm_model.embed_tokens.num_embeddings, mask_token_id=mask_token_id, hidden_size=embed_dim, num_hidden_layers=num_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=1026, layer_norm_eps=1e-05, attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, pad_token_id=pad_token_id, emb_layer_norm_before=emb_layer_norm_before, token_dropout=token_dropout, position_embedding_type=position_embedding_type, is_folding_model=is_folding_model, esmfold_config=esmfold_config, vocab_list=vocab_list)\n    if classification_head:\n        config.num_labels = esm.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our ESM config:', config)\n    if model.startswith('esmfold'):\n        model_class = EsmForProteinFolding\n    elif classification_head:\n        model_class = EsmForSequenceClassification\n    else:\n        model_class = EsmForMaskedLM\n    model = model_class(config)\n    model.eval()\n    model.esm.embeddings.word_embeddings.weight = original_esm_model.embed_tokens.weight\n    if position_embedding_type == 'absolute':\n        model.esm.embeddings.position_embeddings.weight = original_esm_model.embed_positions.weight\n    if config.emb_layer_norm_before:\n        model.esm.embeddings.layer_norm.weight = original_esm_model.emb_layer_norm_before.weight\n        model.esm.embeddings.layer_norm.bias = original_esm_model.emb_layer_norm_before.bias\n    model.esm.encoder.emb_layer_norm_after.weight = original_esm_model.emb_layer_norm_after.weight\n    model.esm.encoder.emb_layer_norm_after.bias = original_esm_model.emb_layer_norm_after.bias\n    for i in range(config.num_hidden_layers):\n        layer: EsmLayer = model.esm.encoder.layer[i]\n        esm_layer = original_esm_model.layers[i]\n        self_attn: EsmSelfAttention = layer.attention.self\n        assert esm_layer.self_attn.k_proj.weight.data.shape == esm_layer.self_attn.q_proj.weight.data.shape == esm_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = esm_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = esm_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = esm_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = esm_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = esm_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = esm_layer.self_attn.v_proj.bias\n        if getattr(esm_layer.self_attn, 'rot_emb', None) is not None:\n            self_attn.rotary_embeddings.inv_freq.data = esm_layer.self_attn.rot_emb.inv_freq\n        layer.attention.LayerNorm.weight = esm_layer.self_attn_layer_norm.weight\n        layer.attention.LayerNorm.bias = esm_layer.self_attn_layer_norm.bias\n        layer.LayerNorm.weight = esm_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = esm_layer.final_layer_norm.bias\n        self_output: EsmSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == esm_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = esm_layer.self_attn.out_proj.weight\n        self_output.dense.bias = esm_layer.self_attn.out_proj.bias\n        intermediate: EsmIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == esm_layer.fc1.weight.shape\n        intermediate.dense.weight = esm_layer.fc1.weight\n        intermediate.dense.bias = esm_layer.fc1.bias\n        bert_output: EsmOutput = layer.output\n        assert bert_output.dense.weight.shape == esm_layer.fc2.weight.shape\n        bert_output.dense.weight = esm_layer.fc2.weight\n        bert_output.dense.bias = esm_layer.fc2.bias\n    if is_folding_model:\n        model.esm_s_combine.data = esm.esm_s_combine.data\n        model.af2_to_esm.data = esm.af2_to_esm.data\n        transfer_and_check_weights(esm.embedding, model.embedding)\n        transfer_and_check_weights(esm.esm_s_mlp, model.esm_s_mlp)\n        transfer_and_check_weights(esm.trunk, model.trunk)\n        transfer_and_check_weights(esm.distogram_head, model.distogram_head)\n        transfer_and_check_weights(esm.ptm_head, model.ptm_head)\n        transfer_and_check_weights(esm.lm_head, model.lm_head)\n        transfer_and_check_weights(esm.lddt_head, model.lddt_head)\n    elif classification_head:\n        model.classifier.dense.weight = esm.esm.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = esm.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = esm.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = esm.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = esm.lm_head.dense.weight\n        model.lm_head.dense.bias = esm.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = esm.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = esm.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = esm.lm_head.weight\n        model.lm_head.bias = esm.lm_head.bias\n    transfer_and_check_weights(esm.contact_head, model.esm.contact_head)\n    if is_folding_model:\n        sample_data = SAMPLE_DATA[:2]\n    else:\n        sample_data = SAMPLE_DATA\n    if is_folding_model:\n        hf_tokenizer = get_esmfold_tokenizer()\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True, add_special_tokens=False)\n        (esmfold_aas, esmfold_mask, _, _, _) = esmfold_encode_sequences([row[1] for row in sample_data])\n        success = torch.all(hf_tokens['input_ids'] == esmfold_aas) and torch.all(hf_tokens['attention_mask'] == esmfold_mask)\n    else:\n        batch_converter = alphabet.get_batch_converter()\n        (batch_labels, batch_strs, batch_tokens) = batch_converter(sample_data)\n        with TemporaryDirectory() as tempdir:\n            vocab = '\\n'.join(alphabet.all_toks)\n            vocab_file = Path(tempdir) / 'vocab.txt'\n            vocab_file.write_text(vocab)\n            hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True)\n        success = torch.all(hf_tokens['input_ids'] == batch_tokens)\n    print('Do both models tokenizers output the same tokens?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Tokenization does not match!')\n    with torch.no_grad():\n        if is_folding_model:\n            their_output = esm.cuda().infer([row[1] for row in sample_data])\n            our_output = model.cuda()(input_ids=hf_tokens['input_ids'].cuda(), attention_mask=hf_tokens['attention_mask'].cuda())\n        else:\n            our_output = model(**hf_tokens, output_hidden_states=True)\n            our_output = our_output['logits']\n            if classification_head:\n                their_output = esm.model.classification_heads['mnli'](esm.extract_features(batch_tokens))\n            else:\n                their_output = esm(hf_tokens['input_ids'], repr_layers=list(range(999)))\n                their_output = their_output['logits']\n        if is_folding_model:\n            max_absolute_diff = torch.max(torch.abs(our_output['positions'] - their_output['positions'])).item()\n            success = torch.allclose(our_output['positions'], their_output['positions'], atol=1e-05)\n        else:\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n        print(f'max_absolute_diff = {max_absolute_diff}')\n        print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n        if not success:\n            raise Exception('Something went wRoNg')\n        if not is_folding_model:\n            our_output = model.predict_contacts(hf_tokens['input_ids'], hf_tokens['attention_mask'])\n            their_output = esm.predict_contacts(hf_tokens['input_ids'])\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n            print('Contact prediction testing:')\n            print(f'max_absolute_diff = {max_absolute_diff}')\n            print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n            if not success:\n                raise Exception('Something went wRoNg')\n        pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n        print(f'Saving model to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n        del esm\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    hf_tokenizer.save_pretrained(pytorch_dump_folder_path)\n    if push_to_repo:\n        model.push_to_hub(repo_id=push_to_repo, token_token=auth_token)\n        hf_tokenizer.push_to_hub(repo_id=push_to_repo, token_token=auth_token)",
            "def convert_esm_checkpoint_to_pytorch(model: str, pytorch_dump_folder_path: str, classification_head: bool, push_to_repo: str, auth_token: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Copy/paste/tweak esm's weights to our BERT structure.\\n    \"\n    if model.startswith('esmfold'):\n        esm = MODEL_MAPPING[model]()\n    else:\n        (esm, alphabet) = MODEL_MAPPING[model]()\n    esm.eval()\n    if model.startswith('esmfold'):\n        embed_dim = esm.esm.embed_dim\n        num_layers = esm.esm.num_layers\n        num_attention_heads = esm.esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = True\n        esmfold_config = EsmFoldConfig()\n        for (key, val) in esm.cfg.items():\n            if hasattr(esmfold_config, key) and key != 'trunk':\n                setattr(esmfold_config, key, val)\n        for (key, val) in esm.cfg.trunk.items():\n            if hasattr(esmfold_config.trunk, key) and key != 'structure_module':\n                setattr(esmfold_config.trunk, key, val)\n        for (key, val) in esm.cfg.trunk.structure_module.items():\n            if hasattr(esmfold_config.trunk.structure_module, key):\n                setattr(esmfold_config.trunk.structure_module, key, val)\n    elif hasattr(esm, 'args'):\n        embed_dim = esm.args.embed_dim\n        num_layers = esm.args.layers\n        num_attention_heads = esm.args.attention_heads\n        intermediate_size = esm.args.ffn_embed_dim\n        token_dropout = esm.args.token_dropout\n        emb_layer_norm_before = True if esm.emb_layer_norm_before else False\n        position_embedding_type = 'absolute'\n        is_folding_model = False\n        esmfold_config = None\n    else:\n        embed_dim = esm.embed_dim\n        num_layers = esm.num_layers\n        num_attention_heads = esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = False\n        esmfold_config = None\n    if is_folding_model:\n        alphabet = esm.esm.alphabet\n    vocab_list = tuple(alphabet.all_toks)\n    mask_token_id = alphabet.mask_idx\n    pad_token_id = alphabet.padding_idx\n    if is_folding_model:\n        original_esm_model = esm.esm\n    else:\n        original_esm_model = esm\n    config = EsmConfig(vocab_size=original_esm_model.embed_tokens.num_embeddings, mask_token_id=mask_token_id, hidden_size=embed_dim, num_hidden_layers=num_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=1026, layer_norm_eps=1e-05, attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, pad_token_id=pad_token_id, emb_layer_norm_before=emb_layer_norm_before, token_dropout=token_dropout, position_embedding_type=position_embedding_type, is_folding_model=is_folding_model, esmfold_config=esmfold_config, vocab_list=vocab_list)\n    if classification_head:\n        config.num_labels = esm.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our ESM config:', config)\n    if model.startswith('esmfold'):\n        model_class = EsmForProteinFolding\n    elif classification_head:\n        model_class = EsmForSequenceClassification\n    else:\n        model_class = EsmForMaskedLM\n    model = model_class(config)\n    model.eval()\n    model.esm.embeddings.word_embeddings.weight = original_esm_model.embed_tokens.weight\n    if position_embedding_type == 'absolute':\n        model.esm.embeddings.position_embeddings.weight = original_esm_model.embed_positions.weight\n    if config.emb_layer_norm_before:\n        model.esm.embeddings.layer_norm.weight = original_esm_model.emb_layer_norm_before.weight\n        model.esm.embeddings.layer_norm.bias = original_esm_model.emb_layer_norm_before.bias\n    model.esm.encoder.emb_layer_norm_after.weight = original_esm_model.emb_layer_norm_after.weight\n    model.esm.encoder.emb_layer_norm_after.bias = original_esm_model.emb_layer_norm_after.bias\n    for i in range(config.num_hidden_layers):\n        layer: EsmLayer = model.esm.encoder.layer[i]\n        esm_layer = original_esm_model.layers[i]\n        self_attn: EsmSelfAttention = layer.attention.self\n        assert esm_layer.self_attn.k_proj.weight.data.shape == esm_layer.self_attn.q_proj.weight.data.shape == esm_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = esm_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = esm_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = esm_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = esm_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = esm_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = esm_layer.self_attn.v_proj.bias\n        if getattr(esm_layer.self_attn, 'rot_emb', None) is not None:\n            self_attn.rotary_embeddings.inv_freq.data = esm_layer.self_attn.rot_emb.inv_freq\n        layer.attention.LayerNorm.weight = esm_layer.self_attn_layer_norm.weight\n        layer.attention.LayerNorm.bias = esm_layer.self_attn_layer_norm.bias\n        layer.LayerNorm.weight = esm_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = esm_layer.final_layer_norm.bias\n        self_output: EsmSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == esm_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = esm_layer.self_attn.out_proj.weight\n        self_output.dense.bias = esm_layer.self_attn.out_proj.bias\n        intermediate: EsmIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == esm_layer.fc1.weight.shape\n        intermediate.dense.weight = esm_layer.fc1.weight\n        intermediate.dense.bias = esm_layer.fc1.bias\n        bert_output: EsmOutput = layer.output\n        assert bert_output.dense.weight.shape == esm_layer.fc2.weight.shape\n        bert_output.dense.weight = esm_layer.fc2.weight\n        bert_output.dense.bias = esm_layer.fc2.bias\n    if is_folding_model:\n        model.esm_s_combine.data = esm.esm_s_combine.data\n        model.af2_to_esm.data = esm.af2_to_esm.data\n        transfer_and_check_weights(esm.embedding, model.embedding)\n        transfer_and_check_weights(esm.esm_s_mlp, model.esm_s_mlp)\n        transfer_and_check_weights(esm.trunk, model.trunk)\n        transfer_and_check_weights(esm.distogram_head, model.distogram_head)\n        transfer_and_check_weights(esm.ptm_head, model.ptm_head)\n        transfer_and_check_weights(esm.lm_head, model.lm_head)\n        transfer_and_check_weights(esm.lddt_head, model.lddt_head)\n    elif classification_head:\n        model.classifier.dense.weight = esm.esm.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = esm.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = esm.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = esm.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = esm.lm_head.dense.weight\n        model.lm_head.dense.bias = esm.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = esm.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = esm.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = esm.lm_head.weight\n        model.lm_head.bias = esm.lm_head.bias\n    transfer_and_check_weights(esm.contact_head, model.esm.contact_head)\n    if is_folding_model:\n        sample_data = SAMPLE_DATA[:2]\n    else:\n        sample_data = SAMPLE_DATA\n    if is_folding_model:\n        hf_tokenizer = get_esmfold_tokenizer()\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True, add_special_tokens=False)\n        (esmfold_aas, esmfold_mask, _, _, _) = esmfold_encode_sequences([row[1] for row in sample_data])\n        success = torch.all(hf_tokens['input_ids'] == esmfold_aas) and torch.all(hf_tokens['attention_mask'] == esmfold_mask)\n    else:\n        batch_converter = alphabet.get_batch_converter()\n        (batch_labels, batch_strs, batch_tokens) = batch_converter(sample_data)\n        with TemporaryDirectory() as tempdir:\n            vocab = '\\n'.join(alphabet.all_toks)\n            vocab_file = Path(tempdir) / 'vocab.txt'\n            vocab_file.write_text(vocab)\n            hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True)\n        success = torch.all(hf_tokens['input_ids'] == batch_tokens)\n    print('Do both models tokenizers output the same tokens?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Tokenization does not match!')\n    with torch.no_grad():\n        if is_folding_model:\n            their_output = esm.cuda().infer([row[1] for row in sample_data])\n            our_output = model.cuda()(input_ids=hf_tokens['input_ids'].cuda(), attention_mask=hf_tokens['attention_mask'].cuda())\n        else:\n            our_output = model(**hf_tokens, output_hidden_states=True)\n            our_output = our_output['logits']\n            if classification_head:\n                their_output = esm.model.classification_heads['mnli'](esm.extract_features(batch_tokens))\n            else:\n                their_output = esm(hf_tokens['input_ids'], repr_layers=list(range(999)))\n                their_output = their_output['logits']\n        if is_folding_model:\n            max_absolute_diff = torch.max(torch.abs(our_output['positions'] - their_output['positions'])).item()\n            success = torch.allclose(our_output['positions'], their_output['positions'], atol=1e-05)\n        else:\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n        print(f'max_absolute_diff = {max_absolute_diff}')\n        print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n        if not success:\n            raise Exception('Something went wRoNg')\n        if not is_folding_model:\n            our_output = model.predict_contacts(hf_tokens['input_ids'], hf_tokens['attention_mask'])\n            their_output = esm.predict_contacts(hf_tokens['input_ids'])\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n            print('Contact prediction testing:')\n            print(f'max_absolute_diff = {max_absolute_diff}')\n            print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n            if not success:\n                raise Exception('Something went wRoNg')\n        pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n        print(f'Saving model to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n        del esm\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    hf_tokenizer.save_pretrained(pytorch_dump_folder_path)\n    if push_to_repo:\n        model.push_to_hub(repo_id=push_to_repo, token_token=auth_token)\n        hf_tokenizer.push_to_hub(repo_id=push_to_repo, token_token=auth_token)",
            "def convert_esm_checkpoint_to_pytorch(model: str, pytorch_dump_folder_path: str, classification_head: bool, push_to_repo: str, auth_token: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Copy/paste/tweak esm's weights to our BERT structure.\\n    \"\n    if model.startswith('esmfold'):\n        esm = MODEL_MAPPING[model]()\n    else:\n        (esm, alphabet) = MODEL_MAPPING[model]()\n    esm.eval()\n    if model.startswith('esmfold'):\n        embed_dim = esm.esm.embed_dim\n        num_layers = esm.esm.num_layers\n        num_attention_heads = esm.esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = True\n        esmfold_config = EsmFoldConfig()\n        for (key, val) in esm.cfg.items():\n            if hasattr(esmfold_config, key) and key != 'trunk':\n                setattr(esmfold_config, key, val)\n        for (key, val) in esm.cfg.trunk.items():\n            if hasattr(esmfold_config.trunk, key) and key != 'structure_module':\n                setattr(esmfold_config.trunk, key, val)\n        for (key, val) in esm.cfg.trunk.structure_module.items():\n            if hasattr(esmfold_config.trunk.structure_module, key):\n                setattr(esmfold_config.trunk.structure_module, key, val)\n    elif hasattr(esm, 'args'):\n        embed_dim = esm.args.embed_dim\n        num_layers = esm.args.layers\n        num_attention_heads = esm.args.attention_heads\n        intermediate_size = esm.args.ffn_embed_dim\n        token_dropout = esm.args.token_dropout\n        emb_layer_norm_before = True if esm.emb_layer_norm_before else False\n        position_embedding_type = 'absolute'\n        is_folding_model = False\n        esmfold_config = None\n    else:\n        embed_dim = esm.embed_dim\n        num_layers = esm.num_layers\n        num_attention_heads = esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = False\n        esmfold_config = None\n    if is_folding_model:\n        alphabet = esm.esm.alphabet\n    vocab_list = tuple(alphabet.all_toks)\n    mask_token_id = alphabet.mask_idx\n    pad_token_id = alphabet.padding_idx\n    if is_folding_model:\n        original_esm_model = esm.esm\n    else:\n        original_esm_model = esm\n    config = EsmConfig(vocab_size=original_esm_model.embed_tokens.num_embeddings, mask_token_id=mask_token_id, hidden_size=embed_dim, num_hidden_layers=num_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=1026, layer_norm_eps=1e-05, attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, pad_token_id=pad_token_id, emb_layer_norm_before=emb_layer_norm_before, token_dropout=token_dropout, position_embedding_type=position_embedding_type, is_folding_model=is_folding_model, esmfold_config=esmfold_config, vocab_list=vocab_list)\n    if classification_head:\n        config.num_labels = esm.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our ESM config:', config)\n    if model.startswith('esmfold'):\n        model_class = EsmForProteinFolding\n    elif classification_head:\n        model_class = EsmForSequenceClassification\n    else:\n        model_class = EsmForMaskedLM\n    model = model_class(config)\n    model.eval()\n    model.esm.embeddings.word_embeddings.weight = original_esm_model.embed_tokens.weight\n    if position_embedding_type == 'absolute':\n        model.esm.embeddings.position_embeddings.weight = original_esm_model.embed_positions.weight\n    if config.emb_layer_norm_before:\n        model.esm.embeddings.layer_norm.weight = original_esm_model.emb_layer_norm_before.weight\n        model.esm.embeddings.layer_norm.bias = original_esm_model.emb_layer_norm_before.bias\n    model.esm.encoder.emb_layer_norm_after.weight = original_esm_model.emb_layer_norm_after.weight\n    model.esm.encoder.emb_layer_norm_after.bias = original_esm_model.emb_layer_norm_after.bias\n    for i in range(config.num_hidden_layers):\n        layer: EsmLayer = model.esm.encoder.layer[i]\n        esm_layer = original_esm_model.layers[i]\n        self_attn: EsmSelfAttention = layer.attention.self\n        assert esm_layer.self_attn.k_proj.weight.data.shape == esm_layer.self_attn.q_proj.weight.data.shape == esm_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = esm_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = esm_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = esm_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = esm_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = esm_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = esm_layer.self_attn.v_proj.bias\n        if getattr(esm_layer.self_attn, 'rot_emb', None) is not None:\n            self_attn.rotary_embeddings.inv_freq.data = esm_layer.self_attn.rot_emb.inv_freq\n        layer.attention.LayerNorm.weight = esm_layer.self_attn_layer_norm.weight\n        layer.attention.LayerNorm.bias = esm_layer.self_attn_layer_norm.bias\n        layer.LayerNorm.weight = esm_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = esm_layer.final_layer_norm.bias\n        self_output: EsmSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == esm_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = esm_layer.self_attn.out_proj.weight\n        self_output.dense.bias = esm_layer.self_attn.out_proj.bias\n        intermediate: EsmIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == esm_layer.fc1.weight.shape\n        intermediate.dense.weight = esm_layer.fc1.weight\n        intermediate.dense.bias = esm_layer.fc1.bias\n        bert_output: EsmOutput = layer.output\n        assert bert_output.dense.weight.shape == esm_layer.fc2.weight.shape\n        bert_output.dense.weight = esm_layer.fc2.weight\n        bert_output.dense.bias = esm_layer.fc2.bias\n    if is_folding_model:\n        model.esm_s_combine.data = esm.esm_s_combine.data\n        model.af2_to_esm.data = esm.af2_to_esm.data\n        transfer_and_check_weights(esm.embedding, model.embedding)\n        transfer_and_check_weights(esm.esm_s_mlp, model.esm_s_mlp)\n        transfer_and_check_weights(esm.trunk, model.trunk)\n        transfer_and_check_weights(esm.distogram_head, model.distogram_head)\n        transfer_and_check_weights(esm.ptm_head, model.ptm_head)\n        transfer_and_check_weights(esm.lm_head, model.lm_head)\n        transfer_and_check_weights(esm.lddt_head, model.lddt_head)\n    elif classification_head:\n        model.classifier.dense.weight = esm.esm.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = esm.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = esm.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = esm.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = esm.lm_head.dense.weight\n        model.lm_head.dense.bias = esm.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = esm.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = esm.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = esm.lm_head.weight\n        model.lm_head.bias = esm.lm_head.bias\n    transfer_and_check_weights(esm.contact_head, model.esm.contact_head)\n    if is_folding_model:\n        sample_data = SAMPLE_DATA[:2]\n    else:\n        sample_data = SAMPLE_DATA\n    if is_folding_model:\n        hf_tokenizer = get_esmfold_tokenizer()\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True, add_special_tokens=False)\n        (esmfold_aas, esmfold_mask, _, _, _) = esmfold_encode_sequences([row[1] for row in sample_data])\n        success = torch.all(hf_tokens['input_ids'] == esmfold_aas) and torch.all(hf_tokens['attention_mask'] == esmfold_mask)\n    else:\n        batch_converter = alphabet.get_batch_converter()\n        (batch_labels, batch_strs, batch_tokens) = batch_converter(sample_data)\n        with TemporaryDirectory() as tempdir:\n            vocab = '\\n'.join(alphabet.all_toks)\n            vocab_file = Path(tempdir) / 'vocab.txt'\n            vocab_file.write_text(vocab)\n            hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True)\n        success = torch.all(hf_tokens['input_ids'] == batch_tokens)\n    print('Do both models tokenizers output the same tokens?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Tokenization does not match!')\n    with torch.no_grad():\n        if is_folding_model:\n            their_output = esm.cuda().infer([row[1] for row in sample_data])\n            our_output = model.cuda()(input_ids=hf_tokens['input_ids'].cuda(), attention_mask=hf_tokens['attention_mask'].cuda())\n        else:\n            our_output = model(**hf_tokens, output_hidden_states=True)\n            our_output = our_output['logits']\n            if classification_head:\n                their_output = esm.model.classification_heads['mnli'](esm.extract_features(batch_tokens))\n            else:\n                their_output = esm(hf_tokens['input_ids'], repr_layers=list(range(999)))\n                their_output = their_output['logits']\n        if is_folding_model:\n            max_absolute_diff = torch.max(torch.abs(our_output['positions'] - their_output['positions'])).item()\n            success = torch.allclose(our_output['positions'], their_output['positions'], atol=1e-05)\n        else:\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n        print(f'max_absolute_diff = {max_absolute_diff}')\n        print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n        if not success:\n            raise Exception('Something went wRoNg')\n        if not is_folding_model:\n            our_output = model.predict_contacts(hf_tokens['input_ids'], hf_tokens['attention_mask'])\n            their_output = esm.predict_contacts(hf_tokens['input_ids'])\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n            print('Contact prediction testing:')\n            print(f'max_absolute_diff = {max_absolute_diff}')\n            print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n            if not success:\n                raise Exception('Something went wRoNg')\n        pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n        print(f'Saving model to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n        del esm\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    hf_tokenizer.save_pretrained(pytorch_dump_folder_path)\n    if push_to_repo:\n        model.push_to_hub(repo_id=push_to_repo, token_token=auth_token)\n        hf_tokenizer.push_to_hub(repo_id=push_to_repo, token_token=auth_token)",
            "def convert_esm_checkpoint_to_pytorch(model: str, pytorch_dump_folder_path: str, classification_head: bool, push_to_repo: str, auth_token: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Copy/paste/tweak esm's weights to our BERT structure.\\n    \"\n    if model.startswith('esmfold'):\n        esm = MODEL_MAPPING[model]()\n    else:\n        (esm, alphabet) = MODEL_MAPPING[model]()\n    esm.eval()\n    if model.startswith('esmfold'):\n        embed_dim = esm.esm.embed_dim\n        num_layers = esm.esm.num_layers\n        num_attention_heads = esm.esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = True\n        esmfold_config = EsmFoldConfig()\n        for (key, val) in esm.cfg.items():\n            if hasattr(esmfold_config, key) and key != 'trunk':\n                setattr(esmfold_config, key, val)\n        for (key, val) in esm.cfg.trunk.items():\n            if hasattr(esmfold_config.trunk, key) and key != 'structure_module':\n                setattr(esmfold_config.trunk, key, val)\n        for (key, val) in esm.cfg.trunk.structure_module.items():\n            if hasattr(esmfold_config.trunk.structure_module, key):\n                setattr(esmfold_config.trunk.structure_module, key, val)\n    elif hasattr(esm, 'args'):\n        embed_dim = esm.args.embed_dim\n        num_layers = esm.args.layers\n        num_attention_heads = esm.args.attention_heads\n        intermediate_size = esm.args.ffn_embed_dim\n        token_dropout = esm.args.token_dropout\n        emb_layer_norm_before = True if esm.emb_layer_norm_before else False\n        position_embedding_type = 'absolute'\n        is_folding_model = False\n        esmfold_config = None\n    else:\n        embed_dim = esm.embed_dim\n        num_layers = esm.num_layers\n        num_attention_heads = esm.attention_heads\n        intermediate_size = 4 * embed_dim\n        token_dropout = esm.token_dropout\n        emb_layer_norm_before = False\n        position_embedding_type = 'rotary'\n        is_folding_model = False\n        esmfold_config = None\n    if is_folding_model:\n        alphabet = esm.esm.alphabet\n    vocab_list = tuple(alphabet.all_toks)\n    mask_token_id = alphabet.mask_idx\n    pad_token_id = alphabet.padding_idx\n    if is_folding_model:\n        original_esm_model = esm.esm\n    else:\n        original_esm_model = esm\n    config = EsmConfig(vocab_size=original_esm_model.embed_tokens.num_embeddings, mask_token_id=mask_token_id, hidden_size=embed_dim, num_hidden_layers=num_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=1026, layer_norm_eps=1e-05, attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0, pad_token_id=pad_token_id, emb_layer_norm_before=emb_layer_norm_before, token_dropout=token_dropout, position_embedding_type=position_embedding_type, is_folding_model=is_folding_model, esmfold_config=esmfold_config, vocab_list=vocab_list)\n    if classification_head:\n        config.num_labels = esm.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our ESM config:', config)\n    if model.startswith('esmfold'):\n        model_class = EsmForProteinFolding\n    elif classification_head:\n        model_class = EsmForSequenceClassification\n    else:\n        model_class = EsmForMaskedLM\n    model = model_class(config)\n    model.eval()\n    model.esm.embeddings.word_embeddings.weight = original_esm_model.embed_tokens.weight\n    if position_embedding_type == 'absolute':\n        model.esm.embeddings.position_embeddings.weight = original_esm_model.embed_positions.weight\n    if config.emb_layer_norm_before:\n        model.esm.embeddings.layer_norm.weight = original_esm_model.emb_layer_norm_before.weight\n        model.esm.embeddings.layer_norm.bias = original_esm_model.emb_layer_norm_before.bias\n    model.esm.encoder.emb_layer_norm_after.weight = original_esm_model.emb_layer_norm_after.weight\n    model.esm.encoder.emb_layer_norm_after.bias = original_esm_model.emb_layer_norm_after.bias\n    for i in range(config.num_hidden_layers):\n        layer: EsmLayer = model.esm.encoder.layer[i]\n        esm_layer = original_esm_model.layers[i]\n        self_attn: EsmSelfAttention = layer.attention.self\n        assert esm_layer.self_attn.k_proj.weight.data.shape == esm_layer.self_attn.q_proj.weight.data.shape == esm_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = esm_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = esm_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = esm_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = esm_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = esm_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = esm_layer.self_attn.v_proj.bias\n        if getattr(esm_layer.self_attn, 'rot_emb', None) is not None:\n            self_attn.rotary_embeddings.inv_freq.data = esm_layer.self_attn.rot_emb.inv_freq\n        layer.attention.LayerNorm.weight = esm_layer.self_attn_layer_norm.weight\n        layer.attention.LayerNorm.bias = esm_layer.self_attn_layer_norm.bias\n        layer.LayerNorm.weight = esm_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = esm_layer.final_layer_norm.bias\n        self_output: EsmSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == esm_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = esm_layer.self_attn.out_proj.weight\n        self_output.dense.bias = esm_layer.self_attn.out_proj.bias\n        intermediate: EsmIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == esm_layer.fc1.weight.shape\n        intermediate.dense.weight = esm_layer.fc1.weight\n        intermediate.dense.bias = esm_layer.fc1.bias\n        bert_output: EsmOutput = layer.output\n        assert bert_output.dense.weight.shape == esm_layer.fc2.weight.shape\n        bert_output.dense.weight = esm_layer.fc2.weight\n        bert_output.dense.bias = esm_layer.fc2.bias\n    if is_folding_model:\n        model.esm_s_combine.data = esm.esm_s_combine.data\n        model.af2_to_esm.data = esm.af2_to_esm.data\n        transfer_and_check_weights(esm.embedding, model.embedding)\n        transfer_and_check_weights(esm.esm_s_mlp, model.esm_s_mlp)\n        transfer_and_check_weights(esm.trunk, model.trunk)\n        transfer_and_check_weights(esm.distogram_head, model.distogram_head)\n        transfer_and_check_weights(esm.ptm_head, model.ptm_head)\n        transfer_and_check_weights(esm.lm_head, model.lm_head)\n        transfer_and_check_weights(esm.lddt_head, model.lddt_head)\n    elif classification_head:\n        model.classifier.dense.weight = esm.esm.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = esm.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = esm.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = esm.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = esm.lm_head.dense.weight\n        model.lm_head.dense.bias = esm.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = esm.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = esm.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = esm.lm_head.weight\n        model.lm_head.bias = esm.lm_head.bias\n    transfer_and_check_weights(esm.contact_head, model.esm.contact_head)\n    if is_folding_model:\n        sample_data = SAMPLE_DATA[:2]\n    else:\n        sample_data = SAMPLE_DATA\n    if is_folding_model:\n        hf_tokenizer = get_esmfold_tokenizer()\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True, add_special_tokens=False)\n        (esmfold_aas, esmfold_mask, _, _, _) = esmfold_encode_sequences([row[1] for row in sample_data])\n        success = torch.all(hf_tokens['input_ids'] == esmfold_aas) and torch.all(hf_tokens['attention_mask'] == esmfold_mask)\n    else:\n        batch_converter = alphabet.get_batch_converter()\n        (batch_labels, batch_strs, batch_tokens) = batch_converter(sample_data)\n        with TemporaryDirectory() as tempdir:\n            vocab = '\\n'.join(alphabet.all_toks)\n            vocab_file = Path(tempdir) / 'vocab.txt'\n            vocab_file.write_text(vocab)\n            hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))\n        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors='pt', padding=True)\n        success = torch.all(hf_tokens['input_ids'] == batch_tokens)\n    print('Do both models tokenizers output the same tokens?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Tokenization does not match!')\n    with torch.no_grad():\n        if is_folding_model:\n            their_output = esm.cuda().infer([row[1] for row in sample_data])\n            our_output = model.cuda()(input_ids=hf_tokens['input_ids'].cuda(), attention_mask=hf_tokens['attention_mask'].cuda())\n        else:\n            our_output = model(**hf_tokens, output_hidden_states=True)\n            our_output = our_output['logits']\n            if classification_head:\n                their_output = esm.model.classification_heads['mnli'](esm.extract_features(batch_tokens))\n            else:\n                their_output = esm(hf_tokens['input_ids'], repr_layers=list(range(999)))\n                their_output = their_output['logits']\n        if is_folding_model:\n            max_absolute_diff = torch.max(torch.abs(our_output['positions'] - their_output['positions'])).item()\n            success = torch.allclose(our_output['positions'], their_output['positions'], atol=1e-05)\n        else:\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n        print(f'max_absolute_diff = {max_absolute_diff}')\n        print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n        if not success:\n            raise Exception('Something went wRoNg')\n        if not is_folding_model:\n            our_output = model.predict_contacts(hf_tokens['input_ids'], hf_tokens['attention_mask'])\n            their_output = esm.predict_contacts(hf_tokens['input_ids'])\n            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n            success = torch.allclose(our_output, their_output, atol=1e-05)\n            print('Contact prediction testing:')\n            print(f'max_absolute_diff = {max_absolute_diff}')\n            print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n            if not success:\n                raise Exception('Something went wRoNg')\n        pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n        print(f'Saving model to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n        del esm\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    hf_tokenizer.save_pretrained(pytorch_dump_folder_path)\n    if push_to_repo:\n        model.push_to_hub(repo_id=push_to_repo, token_token=auth_token)\n        hf_tokenizer.push_to_hub(repo_id=push_to_repo, token_token=auth_token)"
        ]
    }
]