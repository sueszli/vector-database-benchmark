[
    {
        "func_name": "_get_examples_batch",
        "original": "def _get_examples_batch():\n    \"\"\"Returns a shuffled batch of examples of all audio classes.\n\n  Note that this is just a toy function because this is a simple demo intended\n  to illustrate how the training code might work.\n\n  Returns:\n    a tuple (features, labels) where features is a NumPy array of shape\n    [batch_size, num_frames, num_bands] where the batch_size is variable and\n    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\n    suitable for feeding VGGish, while labels is a NumPy array of shape\n    [batch_size, num_classes] where each row is a multi-hot label vector that\n    provides the labels for corresponding rows in features.\n  \"\"\"\n    num_seconds = 5\n    sr = 44100\n    t = np.linspace(0, num_seconds, int(num_seconds * sr))\n    freq = np.random.uniform(100, 1000)\n    sine = np.sin(2 * np.pi * freq * t)\n    magnitude = np.random.uniform(-1, 1)\n    const = magnitude * t\n    noise = np.random.normal(-1, 1, size=t.shape)\n    sine_examples = vggish_input.waveform_to_examples(sine, sr)\n    sine_labels = np.array([[1, 0, 0]] * sine_examples.shape[0])\n    const_examples = vggish_input.waveform_to_examples(const, sr)\n    const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n    noise_examples = vggish_input.waveform_to_examples(noise, sr)\n    noise_labels = np.array([[0, 0, 1]] * noise_examples.shape[0])\n    all_examples = np.concatenate((sine_examples, const_examples, noise_examples))\n    all_labels = np.concatenate((sine_labels, const_labels, noise_labels))\n    labeled_examples = list(zip(all_examples, all_labels))\n    shuffle(labeled_examples)\n    features = [example for (example, _) in labeled_examples]\n    labels = [label for (_, label) in labeled_examples]\n    return (features, labels)",
        "mutated": [
            "def _get_examples_batch():\n    if False:\n        i = 10\n    'Returns a shuffled batch of examples of all audio classes.\\n\\n  Note that this is just a toy function because this is a simple demo intended\\n  to illustrate how the training code might work.\\n\\n  Returns:\\n    a tuple (features, labels) where features is a NumPy array of shape\\n    [batch_size, num_frames, num_bands] where the batch_size is variable and\\n    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\\n    suitable for feeding VGGish, while labels is a NumPy array of shape\\n    [batch_size, num_classes] where each row is a multi-hot label vector that\\n    provides the labels for corresponding rows in features.\\n  '\n    num_seconds = 5\n    sr = 44100\n    t = np.linspace(0, num_seconds, int(num_seconds * sr))\n    freq = np.random.uniform(100, 1000)\n    sine = np.sin(2 * np.pi * freq * t)\n    magnitude = np.random.uniform(-1, 1)\n    const = magnitude * t\n    noise = np.random.normal(-1, 1, size=t.shape)\n    sine_examples = vggish_input.waveform_to_examples(sine, sr)\n    sine_labels = np.array([[1, 0, 0]] * sine_examples.shape[0])\n    const_examples = vggish_input.waveform_to_examples(const, sr)\n    const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n    noise_examples = vggish_input.waveform_to_examples(noise, sr)\n    noise_labels = np.array([[0, 0, 1]] * noise_examples.shape[0])\n    all_examples = np.concatenate((sine_examples, const_examples, noise_examples))\n    all_labels = np.concatenate((sine_labels, const_labels, noise_labels))\n    labeled_examples = list(zip(all_examples, all_labels))\n    shuffle(labeled_examples)\n    features = [example for (example, _) in labeled_examples]\n    labels = [label for (_, label) in labeled_examples]\n    return (features, labels)",
            "def _get_examples_batch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a shuffled batch of examples of all audio classes.\\n\\n  Note that this is just a toy function because this is a simple demo intended\\n  to illustrate how the training code might work.\\n\\n  Returns:\\n    a tuple (features, labels) where features is a NumPy array of shape\\n    [batch_size, num_frames, num_bands] where the batch_size is variable and\\n    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\\n    suitable for feeding VGGish, while labels is a NumPy array of shape\\n    [batch_size, num_classes] where each row is a multi-hot label vector that\\n    provides the labels for corresponding rows in features.\\n  '\n    num_seconds = 5\n    sr = 44100\n    t = np.linspace(0, num_seconds, int(num_seconds * sr))\n    freq = np.random.uniform(100, 1000)\n    sine = np.sin(2 * np.pi * freq * t)\n    magnitude = np.random.uniform(-1, 1)\n    const = magnitude * t\n    noise = np.random.normal(-1, 1, size=t.shape)\n    sine_examples = vggish_input.waveform_to_examples(sine, sr)\n    sine_labels = np.array([[1, 0, 0]] * sine_examples.shape[0])\n    const_examples = vggish_input.waveform_to_examples(const, sr)\n    const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n    noise_examples = vggish_input.waveform_to_examples(noise, sr)\n    noise_labels = np.array([[0, 0, 1]] * noise_examples.shape[0])\n    all_examples = np.concatenate((sine_examples, const_examples, noise_examples))\n    all_labels = np.concatenate((sine_labels, const_labels, noise_labels))\n    labeled_examples = list(zip(all_examples, all_labels))\n    shuffle(labeled_examples)\n    features = [example for (example, _) in labeled_examples]\n    labels = [label for (_, label) in labeled_examples]\n    return (features, labels)",
            "def _get_examples_batch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a shuffled batch of examples of all audio classes.\\n\\n  Note that this is just a toy function because this is a simple demo intended\\n  to illustrate how the training code might work.\\n\\n  Returns:\\n    a tuple (features, labels) where features is a NumPy array of shape\\n    [batch_size, num_frames, num_bands] where the batch_size is variable and\\n    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\\n    suitable for feeding VGGish, while labels is a NumPy array of shape\\n    [batch_size, num_classes] where each row is a multi-hot label vector that\\n    provides the labels for corresponding rows in features.\\n  '\n    num_seconds = 5\n    sr = 44100\n    t = np.linspace(0, num_seconds, int(num_seconds * sr))\n    freq = np.random.uniform(100, 1000)\n    sine = np.sin(2 * np.pi * freq * t)\n    magnitude = np.random.uniform(-1, 1)\n    const = magnitude * t\n    noise = np.random.normal(-1, 1, size=t.shape)\n    sine_examples = vggish_input.waveform_to_examples(sine, sr)\n    sine_labels = np.array([[1, 0, 0]] * sine_examples.shape[0])\n    const_examples = vggish_input.waveform_to_examples(const, sr)\n    const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n    noise_examples = vggish_input.waveform_to_examples(noise, sr)\n    noise_labels = np.array([[0, 0, 1]] * noise_examples.shape[0])\n    all_examples = np.concatenate((sine_examples, const_examples, noise_examples))\n    all_labels = np.concatenate((sine_labels, const_labels, noise_labels))\n    labeled_examples = list(zip(all_examples, all_labels))\n    shuffle(labeled_examples)\n    features = [example for (example, _) in labeled_examples]\n    labels = [label for (_, label) in labeled_examples]\n    return (features, labels)",
            "def _get_examples_batch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a shuffled batch of examples of all audio classes.\\n\\n  Note that this is just a toy function because this is a simple demo intended\\n  to illustrate how the training code might work.\\n\\n  Returns:\\n    a tuple (features, labels) where features is a NumPy array of shape\\n    [batch_size, num_frames, num_bands] where the batch_size is variable and\\n    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\\n    suitable for feeding VGGish, while labels is a NumPy array of shape\\n    [batch_size, num_classes] where each row is a multi-hot label vector that\\n    provides the labels for corresponding rows in features.\\n  '\n    num_seconds = 5\n    sr = 44100\n    t = np.linspace(0, num_seconds, int(num_seconds * sr))\n    freq = np.random.uniform(100, 1000)\n    sine = np.sin(2 * np.pi * freq * t)\n    magnitude = np.random.uniform(-1, 1)\n    const = magnitude * t\n    noise = np.random.normal(-1, 1, size=t.shape)\n    sine_examples = vggish_input.waveform_to_examples(sine, sr)\n    sine_labels = np.array([[1, 0, 0]] * sine_examples.shape[0])\n    const_examples = vggish_input.waveform_to_examples(const, sr)\n    const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n    noise_examples = vggish_input.waveform_to_examples(noise, sr)\n    noise_labels = np.array([[0, 0, 1]] * noise_examples.shape[0])\n    all_examples = np.concatenate((sine_examples, const_examples, noise_examples))\n    all_labels = np.concatenate((sine_labels, const_labels, noise_labels))\n    labeled_examples = list(zip(all_examples, all_labels))\n    shuffle(labeled_examples)\n    features = [example for (example, _) in labeled_examples]\n    labels = [label for (_, label) in labeled_examples]\n    return (features, labels)",
            "def _get_examples_batch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a shuffled batch of examples of all audio classes.\\n\\n  Note that this is just a toy function because this is a simple demo intended\\n  to illustrate how the training code might work.\\n\\n  Returns:\\n    a tuple (features, labels) where features is a NumPy array of shape\\n    [batch_size, num_frames, num_bands] where the batch_size is variable and\\n    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\\n    suitable for feeding VGGish, while labels is a NumPy array of shape\\n    [batch_size, num_classes] where each row is a multi-hot label vector that\\n    provides the labels for corresponding rows in features.\\n  '\n    num_seconds = 5\n    sr = 44100\n    t = np.linspace(0, num_seconds, int(num_seconds * sr))\n    freq = np.random.uniform(100, 1000)\n    sine = np.sin(2 * np.pi * freq * t)\n    magnitude = np.random.uniform(-1, 1)\n    const = magnitude * t\n    noise = np.random.normal(-1, 1, size=t.shape)\n    sine_examples = vggish_input.waveform_to_examples(sine, sr)\n    sine_labels = np.array([[1, 0, 0]] * sine_examples.shape[0])\n    const_examples = vggish_input.waveform_to_examples(const, sr)\n    const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n    noise_examples = vggish_input.waveform_to_examples(noise, sr)\n    noise_labels = np.array([[0, 0, 1]] * noise_examples.shape[0])\n    all_examples = np.concatenate((sine_examples, const_examples, noise_examples))\n    all_labels = np.concatenate((sine_labels, const_labels, noise_labels))\n    labeled_examples = list(zip(all_examples, all_labels))\n    shuffle(labeled_examples)\n    features = [example for (example, _) in labeled_examples]\n    labels = [label for (_, label) in labeled_examples]\n    return (features, labels)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(_):\n    with tf.Graph().as_default(), tf.Session() as sess:\n        embeddings = vggish_slim.define_vggish_slim(FLAGS.train_vggish)\n        with tf.variable_scope('mymodel'):\n            num_units = 100\n            fc = slim.fully_connected(embeddings, num_units)\n            logits = slim.fully_connected(fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n            tf.sigmoid(logits, name='prediction')\n            with tf.variable_scope('train'):\n                global_step = tf.Variable(0, name='global_step', trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n                labels = tf.placeholder(tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n                xent = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels, name='xent')\n                loss = tf.reduce_mean(xent, name='loss_op')\n                tf.summary.scalar('loss', loss)\n                optimizer = tf.train.AdamOptimizer(learning_rate=vggish_params.LEARNING_RATE, epsilon=vggish_params.ADAM_EPSILON)\n                optimizer.minimize(loss, global_step=global_step, name='train_op')\n        sess.run(tf.global_variables_initializer())\n        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n        features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n        labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n        global_step_tensor = sess.graph.get_tensor_by_name('mymodel/train/global_step:0')\n        loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n        train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n        for _ in range(FLAGS.num_batches):\n            (features, labels) = _get_examples_batch()\n            [num_steps, loss, _] = sess.run([global_step_tensor, loss_tensor, train_op], feed_dict={features_tensor: features, labels_tensor: labels})\n            print('Step %d: loss %g' % (num_steps, loss))",
        "mutated": [
            "def main(_):\n    if False:\n        i = 10\n    with tf.Graph().as_default(), tf.Session() as sess:\n        embeddings = vggish_slim.define_vggish_slim(FLAGS.train_vggish)\n        with tf.variable_scope('mymodel'):\n            num_units = 100\n            fc = slim.fully_connected(embeddings, num_units)\n            logits = slim.fully_connected(fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n            tf.sigmoid(logits, name='prediction')\n            with tf.variable_scope('train'):\n                global_step = tf.Variable(0, name='global_step', trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n                labels = tf.placeholder(tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n                xent = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels, name='xent')\n                loss = tf.reduce_mean(xent, name='loss_op')\n                tf.summary.scalar('loss', loss)\n                optimizer = tf.train.AdamOptimizer(learning_rate=vggish_params.LEARNING_RATE, epsilon=vggish_params.ADAM_EPSILON)\n                optimizer.minimize(loss, global_step=global_step, name='train_op')\n        sess.run(tf.global_variables_initializer())\n        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n        features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n        labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n        global_step_tensor = sess.graph.get_tensor_by_name('mymodel/train/global_step:0')\n        loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n        train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n        for _ in range(FLAGS.num_batches):\n            (features, labels) = _get_examples_batch()\n            [num_steps, loss, _] = sess.run([global_step_tensor, loss_tensor, train_op], feed_dict={features_tensor: features, labels_tensor: labels})\n            print('Step %d: loss %g' % (num_steps, loss))",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with tf.Graph().as_default(), tf.Session() as sess:\n        embeddings = vggish_slim.define_vggish_slim(FLAGS.train_vggish)\n        with tf.variable_scope('mymodel'):\n            num_units = 100\n            fc = slim.fully_connected(embeddings, num_units)\n            logits = slim.fully_connected(fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n            tf.sigmoid(logits, name='prediction')\n            with tf.variable_scope('train'):\n                global_step = tf.Variable(0, name='global_step', trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n                labels = tf.placeholder(tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n                xent = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels, name='xent')\n                loss = tf.reduce_mean(xent, name='loss_op')\n                tf.summary.scalar('loss', loss)\n                optimizer = tf.train.AdamOptimizer(learning_rate=vggish_params.LEARNING_RATE, epsilon=vggish_params.ADAM_EPSILON)\n                optimizer.minimize(loss, global_step=global_step, name='train_op')\n        sess.run(tf.global_variables_initializer())\n        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n        features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n        labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n        global_step_tensor = sess.graph.get_tensor_by_name('mymodel/train/global_step:0')\n        loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n        train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n        for _ in range(FLAGS.num_batches):\n            (features, labels) = _get_examples_batch()\n            [num_steps, loss, _] = sess.run([global_step_tensor, loss_tensor, train_op], feed_dict={features_tensor: features, labels_tensor: labels})\n            print('Step %d: loss %g' % (num_steps, loss))",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with tf.Graph().as_default(), tf.Session() as sess:\n        embeddings = vggish_slim.define_vggish_slim(FLAGS.train_vggish)\n        with tf.variable_scope('mymodel'):\n            num_units = 100\n            fc = slim.fully_connected(embeddings, num_units)\n            logits = slim.fully_connected(fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n            tf.sigmoid(logits, name='prediction')\n            with tf.variable_scope('train'):\n                global_step = tf.Variable(0, name='global_step', trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n                labels = tf.placeholder(tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n                xent = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels, name='xent')\n                loss = tf.reduce_mean(xent, name='loss_op')\n                tf.summary.scalar('loss', loss)\n                optimizer = tf.train.AdamOptimizer(learning_rate=vggish_params.LEARNING_RATE, epsilon=vggish_params.ADAM_EPSILON)\n                optimizer.minimize(loss, global_step=global_step, name='train_op')\n        sess.run(tf.global_variables_initializer())\n        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n        features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n        labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n        global_step_tensor = sess.graph.get_tensor_by_name('mymodel/train/global_step:0')\n        loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n        train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n        for _ in range(FLAGS.num_batches):\n            (features, labels) = _get_examples_batch()\n            [num_steps, loss, _] = sess.run([global_step_tensor, loss_tensor, train_op], feed_dict={features_tensor: features, labels_tensor: labels})\n            print('Step %d: loss %g' % (num_steps, loss))",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with tf.Graph().as_default(), tf.Session() as sess:\n        embeddings = vggish_slim.define_vggish_slim(FLAGS.train_vggish)\n        with tf.variable_scope('mymodel'):\n            num_units = 100\n            fc = slim.fully_connected(embeddings, num_units)\n            logits = slim.fully_connected(fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n            tf.sigmoid(logits, name='prediction')\n            with tf.variable_scope('train'):\n                global_step = tf.Variable(0, name='global_step', trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n                labels = tf.placeholder(tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n                xent = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels, name='xent')\n                loss = tf.reduce_mean(xent, name='loss_op')\n                tf.summary.scalar('loss', loss)\n                optimizer = tf.train.AdamOptimizer(learning_rate=vggish_params.LEARNING_RATE, epsilon=vggish_params.ADAM_EPSILON)\n                optimizer.minimize(loss, global_step=global_step, name='train_op')\n        sess.run(tf.global_variables_initializer())\n        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n        features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n        labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n        global_step_tensor = sess.graph.get_tensor_by_name('mymodel/train/global_step:0')\n        loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n        train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n        for _ in range(FLAGS.num_batches):\n            (features, labels) = _get_examples_batch()\n            [num_steps, loss, _] = sess.run([global_step_tensor, loss_tensor, train_op], feed_dict={features_tensor: features, labels_tensor: labels})\n            print('Step %d: loss %g' % (num_steps, loss))",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with tf.Graph().as_default(), tf.Session() as sess:\n        embeddings = vggish_slim.define_vggish_slim(FLAGS.train_vggish)\n        with tf.variable_scope('mymodel'):\n            num_units = 100\n            fc = slim.fully_connected(embeddings, num_units)\n            logits = slim.fully_connected(fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n            tf.sigmoid(logits, name='prediction')\n            with tf.variable_scope('train'):\n                global_step = tf.Variable(0, name='global_step', trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n                labels = tf.placeholder(tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n                xent = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels, name='xent')\n                loss = tf.reduce_mean(xent, name='loss_op')\n                tf.summary.scalar('loss', loss)\n                optimizer = tf.train.AdamOptimizer(learning_rate=vggish_params.LEARNING_RATE, epsilon=vggish_params.ADAM_EPSILON)\n                optimizer.minimize(loss, global_step=global_step, name='train_op')\n        sess.run(tf.global_variables_initializer())\n        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n        features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n        labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n        global_step_tensor = sess.graph.get_tensor_by_name('mymodel/train/global_step:0')\n        loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n        train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n        for _ in range(FLAGS.num_batches):\n            (features, labels) = _get_examples_batch()\n            [num_steps, loss, _] = sess.run([global_step_tensor, loss_tensor, train_op], feed_dict={features_tensor: features, labels_tensor: labels})\n            print('Step %d: loss %g' % (num_steps, loss))"
        ]
    }
]