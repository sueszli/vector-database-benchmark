[
    {
        "func_name": "main",
        "original": "def main(_):\n    config = prepare_dirs_and_logger(config_raw)\n    save_config(config)\n    rng = np.random.RandomState(config.random_seed)\n    tf.set_random_seed(config.random_seed)\n    config.rng = rng\n    config.module_names = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    config.gt_layout_tokens = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    assembler = Assembler(config)\n    sample_builder = SampleBuilder(config)\n    config = sample_builder.config\n    data_train = sample_builder.data_all['train']\n    data_reader_train = DataReader(config, data_train, assembler, shuffle=True, one_pass=False)\n    num_vocab_txt = len(sample_builder.dict_all)\n    num_vocab_nmn = len(assembler.module_names)\n    num_choices = len(sample_builder.dict_all)\n    text_seq_batch = tf.placeholder(tf.int32, [None, None])\n    seq_len_batch = tf.placeholder(tf.int32, [None])\n    ans_label_batch = tf.placeholder(tf.int32, [None])\n    use_gt_layout = tf.constant(True, dtype=tf.bool)\n    gt_layout_batch = tf.placeholder(tf.int32, [None, None])\n    model = Model(config, sample_builder.kb, text_seq_batch, seq_len_batch, num_vocab_txt=num_vocab_txt, num_vocab_nmn=num_vocab_nmn, EOS_idx=assembler.EOS_idx, num_choices=num_choices, decoder_sampling=True, use_gt_layout=use_gt_layout, gt_layout_batch=gt_layout_batch)\n    compiler = model.compiler\n    scores = model.scores\n    log_seq_prob = model.log_seq_prob\n    softmax_loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=ans_label_batch)\n    final_loss_per_sample = softmax_loss_per_sample\n    avg_sample_loss = tf.reduce_mean(final_loss_per_sample)\n    seq_likelihood_loss = tf.reduce_mean(-log_seq_prob)\n    total_training_loss = seq_likelihood_loss + avg_sample_loss\n    total_loss = total_training_loss + config.weight_decay * model.l2_reg\n    solver = tf.train.AdamOptimizer()\n    gradients = solver.compute_gradients(total_loss)\n    gradients = [(tf.clip_by_norm(g, config.max_grad_norm), v) for (g, v) in gradients]\n    solver_op = solver.apply_gradients(gradients)\n    with tf.control_dependencies([solver_op]):\n        train_step = tf.constant(0)\n    log_writer = tf.summary.FileWriter(config.log_dir, tf.get_default_graph())\n    loss_ph = tf.placeholder(tf.float32, [])\n    entropy_ph = tf.placeholder(tf.float32, [])\n    accuracy_ph = tf.placeholder(tf.float32, [])\n    summary_train = [tf.summary.scalar('avg_sample_loss', loss_ph), tf.summary.scalar('entropy', entropy_ph), tf.summary.scalar('avg_accuracy', accuracy_ph)]\n    log_step_train = tf.summary.merge(summary_train)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    snapshot_saver = tf.train.Saver(max_to_keep=None)\n    show_all_variables()\n    avg_accuracy = 0\n    accuracy_decay = 0.99\n    for (n_iter, batch) in enumerate(data_reader_train.batches()):\n        if n_iter >= config.max_iter:\n            break\n        h = sess.partial_run_setup(fetches=[model.predicted_tokens, model.entropy_reg, scores, avg_sample_loss, train_step], feeds=[text_seq_batch, seq_len_batch, gt_layout_batch, compiler.loom_input_tensor, ans_label_batch])\n        (tokens, entropy_reg_val) = sess.partial_run(h, fetches=(model.predicted_tokens, model.entropy_reg), feed_dict={text_seq_batch: batch['input_seq_batch'], seq_len_batch: batch['seq_len_batch'], gt_layout_batch: batch['gt_layout_batch']})\n        (expr_list, expr_validity_array) = assembler.assemble(tokens)\n        assert np.all(expr_validity_array)\n        labels = batch['ans_label_batch']\n        expr_feed = compiler.build_feed_dict(expr_list)\n        expr_feed[ans_label_batch] = labels\n        (scores_val, avg_sample_loss_val, _) = sess.partial_run(h, fetches=(scores, avg_sample_loss, train_step), feed_dict=expr_feed)\n        predictions = np.argmax(scores_val, axis=1)\n        accuracy = np.mean(np.logical_and(expr_validity_array, predictions == labels))\n        avg_accuracy += (1 - accuracy_decay) * (accuracy - avg_accuracy)\n        if (n_iter + 1) % config.log_interval == 0:\n            tf.logging.info('iter = %d\\n\\tloss = %f, accuracy (cur) = %f, accuracy (avg) = %f, entropy = %f' % (n_iter + 1, avg_sample_loss_val, accuracy, avg_accuracy, -entropy_reg_val))\n            summary = sess.run(fetches=log_step_train, feed_dict={loss_ph: avg_sample_loss_val, entropy_ph: -entropy_reg_val, accuracy_ph: avg_accuracy})\n            log_writer.add_summary(summary, n_iter + 1)\n        if (n_iter + 1) % config.snapshot_interval == 0:\n            snapshot_file = os.path.join(config.model_dir, '%08d' % (n_iter + 1))\n            snapshot_saver.save(sess, snapshot_file, write_meta_graph=False)\n            tf.logging.info('Snapshot saved to %s' % snapshot_file)\n    tf.logging.info('Run finished.')",
        "mutated": [
            "def main(_):\n    if False:\n        i = 10\n    config = prepare_dirs_and_logger(config_raw)\n    save_config(config)\n    rng = np.random.RandomState(config.random_seed)\n    tf.set_random_seed(config.random_seed)\n    config.rng = rng\n    config.module_names = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    config.gt_layout_tokens = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    assembler = Assembler(config)\n    sample_builder = SampleBuilder(config)\n    config = sample_builder.config\n    data_train = sample_builder.data_all['train']\n    data_reader_train = DataReader(config, data_train, assembler, shuffle=True, one_pass=False)\n    num_vocab_txt = len(sample_builder.dict_all)\n    num_vocab_nmn = len(assembler.module_names)\n    num_choices = len(sample_builder.dict_all)\n    text_seq_batch = tf.placeholder(tf.int32, [None, None])\n    seq_len_batch = tf.placeholder(tf.int32, [None])\n    ans_label_batch = tf.placeholder(tf.int32, [None])\n    use_gt_layout = tf.constant(True, dtype=tf.bool)\n    gt_layout_batch = tf.placeholder(tf.int32, [None, None])\n    model = Model(config, sample_builder.kb, text_seq_batch, seq_len_batch, num_vocab_txt=num_vocab_txt, num_vocab_nmn=num_vocab_nmn, EOS_idx=assembler.EOS_idx, num_choices=num_choices, decoder_sampling=True, use_gt_layout=use_gt_layout, gt_layout_batch=gt_layout_batch)\n    compiler = model.compiler\n    scores = model.scores\n    log_seq_prob = model.log_seq_prob\n    softmax_loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=ans_label_batch)\n    final_loss_per_sample = softmax_loss_per_sample\n    avg_sample_loss = tf.reduce_mean(final_loss_per_sample)\n    seq_likelihood_loss = tf.reduce_mean(-log_seq_prob)\n    total_training_loss = seq_likelihood_loss + avg_sample_loss\n    total_loss = total_training_loss + config.weight_decay * model.l2_reg\n    solver = tf.train.AdamOptimizer()\n    gradients = solver.compute_gradients(total_loss)\n    gradients = [(tf.clip_by_norm(g, config.max_grad_norm), v) for (g, v) in gradients]\n    solver_op = solver.apply_gradients(gradients)\n    with tf.control_dependencies([solver_op]):\n        train_step = tf.constant(0)\n    log_writer = tf.summary.FileWriter(config.log_dir, tf.get_default_graph())\n    loss_ph = tf.placeholder(tf.float32, [])\n    entropy_ph = tf.placeholder(tf.float32, [])\n    accuracy_ph = tf.placeholder(tf.float32, [])\n    summary_train = [tf.summary.scalar('avg_sample_loss', loss_ph), tf.summary.scalar('entropy', entropy_ph), tf.summary.scalar('avg_accuracy', accuracy_ph)]\n    log_step_train = tf.summary.merge(summary_train)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    snapshot_saver = tf.train.Saver(max_to_keep=None)\n    show_all_variables()\n    avg_accuracy = 0\n    accuracy_decay = 0.99\n    for (n_iter, batch) in enumerate(data_reader_train.batches()):\n        if n_iter >= config.max_iter:\n            break\n        h = sess.partial_run_setup(fetches=[model.predicted_tokens, model.entropy_reg, scores, avg_sample_loss, train_step], feeds=[text_seq_batch, seq_len_batch, gt_layout_batch, compiler.loom_input_tensor, ans_label_batch])\n        (tokens, entropy_reg_val) = sess.partial_run(h, fetches=(model.predicted_tokens, model.entropy_reg), feed_dict={text_seq_batch: batch['input_seq_batch'], seq_len_batch: batch['seq_len_batch'], gt_layout_batch: batch['gt_layout_batch']})\n        (expr_list, expr_validity_array) = assembler.assemble(tokens)\n        assert np.all(expr_validity_array)\n        labels = batch['ans_label_batch']\n        expr_feed = compiler.build_feed_dict(expr_list)\n        expr_feed[ans_label_batch] = labels\n        (scores_val, avg_sample_loss_val, _) = sess.partial_run(h, fetches=(scores, avg_sample_loss, train_step), feed_dict=expr_feed)\n        predictions = np.argmax(scores_val, axis=1)\n        accuracy = np.mean(np.logical_and(expr_validity_array, predictions == labels))\n        avg_accuracy += (1 - accuracy_decay) * (accuracy - avg_accuracy)\n        if (n_iter + 1) % config.log_interval == 0:\n            tf.logging.info('iter = %d\\n\\tloss = %f, accuracy (cur) = %f, accuracy (avg) = %f, entropy = %f' % (n_iter + 1, avg_sample_loss_val, accuracy, avg_accuracy, -entropy_reg_val))\n            summary = sess.run(fetches=log_step_train, feed_dict={loss_ph: avg_sample_loss_val, entropy_ph: -entropy_reg_val, accuracy_ph: avg_accuracy})\n            log_writer.add_summary(summary, n_iter + 1)\n        if (n_iter + 1) % config.snapshot_interval == 0:\n            snapshot_file = os.path.join(config.model_dir, '%08d' % (n_iter + 1))\n            snapshot_saver.save(sess, snapshot_file, write_meta_graph=False)\n            tf.logging.info('Snapshot saved to %s' % snapshot_file)\n    tf.logging.info('Run finished.')",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    config = prepare_dirs_and_logger(config_raw)\n    save_config(config)\n    rng = np.random.RandomState(config.random_seed)\n    tf.set_random_seed(config.random_seed)\n    config.rng = rng\n    config.module_names = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    config.gt_layout_tokens = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    assembler = Assembler(config)\n    sample_builder = SampleBuilder(config)\n    config = sample_builder.config\n    data_train = sample_builder.data_all['train']\n    data_reader_train = DataReader(config, data_train, assembler, shuffle=True, one_pass=False)\n    num_vocab_txt = len(sample_builder.dict_all)\n    num_vocab_nmn = len(assembler.module_names)\n    num_choices = len(sample_builder.dict_all)\n    text_seq_batch = tf.placeholder(tf.int32, [None, None])\n    seq_len_batch = tf.placeholder(tf.int32, [None])\n    ans_label_batch = tf.placeholder(tf.int32, [None])\n    use_gt_layout = tf.constant(True, dtype=tf.bool)\n    gt_layout_batch = tf.placeholder(tf.int32, [None, None])\n    model = Model(config, sample_builder.kb, text_seq_batch, seq_len_batch, num_vocab_txt=num_vocab_txt, num_vocab_nmn=num_vocab_nmn, EOS_idx=assembler.EOS_idx, num_choices=num_choices, decoder_sampling=True, use_gt_layout=use_gt_layout, gt_layout_batch=gt_layout_batch)\n    compiler = model.compiler\n    scores = model.scores\n    log_seq_prob = model.log_seq_prob\n    softmax_loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=ans_label_batch)\n    final_loss_per_sample = softmax_loss_per_sample\n    avg_sample_loss = tf.reduce_mean(final_loss_per_sample)\n    seq_likelihood_loss = tf.reduce_mean(-log_seq_prob)\n    total_training_loss = seq_likelihood_loss + avg_sample_loss\n    total_loss = total_training_loss + config.weight_decay * model.l2_reg\n    solver = tf.train.AdamOptimizer()\n    gradients = solver.compute_gradients(total_loss)\n    gradients = [(tf.clip_by_norm(g, config.max_grad_norm), v) for (g, v) in gradients]\n    solver_op = solver.apply_gradients(gradients)\n    with tf.control_dependencies([solver_op]):\n        train_step = tf.constant(0)\n    log_writer = tf.summary.FileWriter(config.log_dir, tf.get_default_graph())\n    loss_ph = tf.placeholder(tf.float32, [])\n    entropy_ph = tf.placeholder(tf.float32, [])\n    accuracy_ph = tf.placeholder(tf.float32, [])\n    summary_train = [tf.summary.scalar('avg_sample_loss', loss_ph), tf.summary.scalar('entropy', entropy_ph), tf.summary.scalar('avg_accuracy', accuracy_ph)]\n    log_step_train = tf.summary.merge(summary_train)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    snapshot_saver = tf.train.Saver(max_to_keep=None)\n    show_all_variables()\n    avg_accuracy = 0\n    accuracy_decay = 0.99\n    for (n_iter, batch) in enumerate(data_reader_train.batches()):\n        if n_iter >= config.max_iter:\n            break\n        h = sess.partial_run_setup(fetches=[model.predicted_tokens, model.entropy_reg, scores, avg_sample_loss, train_step], feeds=[text_seq_batch, seq_len_batch, gt_layout_batch, compiler.loom_input_tensor, ans_label_batch])\n        (tokens, entropy_reg_val) = sess.partial_run(h, fetches=(model.predicted_tokens, model.entropy_reg), feed_dict={text_seq_batch: batch['input_seq_batch'], seq_len_batch: batch['seq_len_batch'], gt_layout_batch: batch['gt_layout_batch']})\n        (expr_list, expr_validity_array) = assembler.assemble(tokens)\n        assert np.all(expr_validity_array)\n        labels = batch['ans_label_batch']\n        expr_feed = compiler.build_feed_dict(expr_list)\n        expr_feed[ans_label_batch] = labels\n        (scores_val, avg_sample_loss_val, _) = sess.partial_run(h, fetches=(scores, avg_sample_loss, train_step), feed_dict=expr_feed)\n        predictions = np.argmax(scores_val, axis=1)\n        accuracy = np.mean(np.logical_and(expr_validity_array, predictions == labels))\n        avg_accuracy += (1 - accuracy_decay) * (accuracy - avg_accuracy)\n        if (n_iter + 1) % config.log_interval == 0:\n            tf.logging.info('iter = %d\\n\\tloss = %f, accuracy (cur) = %f, accuracy (avg) = %f, entropy = %f' % (n_iter + 1, avg_sample_loss_val, accuracy, avg_accuracy, -entropy_reg_val))\n            summary = sess.run(fetches=log_step_train, feed_dict={loss_ph: avg_sample_loss_val, entropy_ph: -entropy_reg_val, accuracy_ph: avg_accuracy})\n            log_writer.add_summary(summary, n_iter + 1)\n        if (n_iter + 1) % config.snapshot_interval == 0:\n            snapshot_file = os.path.join(config.model_dir, '%08d' % (n_iter + 1))\n            snapshot_saver.save(sess, snapshot_file, write_meta_graph=False)\n            tf.logging.info('Snapshot saved to %s' % snapshot_file)\n    tf.logging.info('Run finished.')",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    config = prepare_dirs_and_logger(config_raw)\n    save_config(config)\n    rng = np.random.RandomState(config.random_seed)\n    tf.set_random_seed(config.random_seed)\n    config.rng = rng\n    config.module_names = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    config.gt_layout_tokens = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    assembler = Assembler(config)\n    sample_builder = SampleBuilder(config)\n    config = sample_builder.config\n    data_train = sample_builder.data_all['train']\n    data_reader_train = DataReader(config, data_train, assembler, shuffle=True, one_pass=False)\n    num_vocab_txt = len(sample_builder.dict_all)\n    num_vocab_nmn = len(assembler.module_names)\n    num_choices = len(sample_builder.dict_all)\n    text_seq_batch = tf.placeholder(tf.int32, [None, None])\n    seq_len_batch = tf.placeholder(tf.int32, [None])\n    ans_label_batch = tf.placeholder(tf.int32, [None])\n    use_gt_layout = tf.constant(True, dtype=tf.bool)\n    gt_layout_batch = tf.placeholder(tf.int32, [None, None])\n    model = Model(config, sample_builder.kb, text_seq_batch, seq_len_batch, num_vocab_txt=num_vocab_txt, num_vocab_nmn=num_vocab_nmn, EOS_idx=assembler.EOS_idx, num_choices=num_choices, decoder_sampling=True, use_gt_layout=use_gt_layout, gt_layout_batch=gt_layout_batch)\n    compiler = model.compiler\n    scores = model.scores\n    log_seq_prob = model.log_seq_prob\n    softmax_loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=ans_label_batch)\n    final_loss_per_sample = softmax_loss_per_sample\n    avg_sample_loss = tf.reduce_mean(final_loss_per_sample)\n    seq_likelihood_loss = tf.reduce_mean(-log_seq_prob)\n    total_training_loss = seq_likelihood_loss + avg_sample_loss\n    total_loss = total_training_loss + config.weight_decay * model.l2_reg\n    solver = tf.train.AdamOptimizer()\n    gradients = solver.compute_gradients(total_loss)\n    gradients = [(tf.clip_by_norm(g, config.max_grad_norm), v) for (g, v) in gradients]\n    solver_op = solver.apply_gradients(gradients)\n    with tf.control_dependencies([solver_op]):\n        train_step = tf.constant(0)\n    log_writer = tf.summary.FileWriter(config.log_dir, tf.get_default_graph())\n    loss_ph = tf.placeholder(tf.float32, [])\n    entropy_ph = tf.placeholder(tf.float32, [])\n    accuracy_ph = tf.placeholder(tf.float32, [])\n    summary_train = [tf.summary.scalar('avg_sample_loss', loss_ph), tf.summary.scalar('entropy', entropy_ph), tf.summary.scalar('avg_accuracy', accuracy_ph)]\n    log_step_train = tf.summary.merge(summary_train)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    snapshot_saver = tf.train.Saver(max_to_keep=None)\n    show_all_variables()\n    avg_accuracy = 0\n    accuracy_decay = 0.99\n    for (n_iter, batch) in enumerate(data_reader_train.batches()):\n        if n_iter >= config.max_iter:\n            break\n        h = sess.partial_run_setup(fetches=[model.predicted_tokens, model.entropy_reg, scores, avg_sample_loss, train_step], feeds=[text_seq_batch, seq_len_batch, gt_layout_batch, compiler.loom_input_tensor, ans_label_batch])\n        (tokens, entropy_reg_val) = sess.partial_run(h, fetches=(model.predicted_tokens, model.entropy_reg), feed_dict={text_seq_batch: batch['input_seq_batch'], seq_len_batch: batch['seq_len_batch'], gt_layout_batch: batch['gt_layout_batch']})\n        (expr_list, expr_validity_array) = assembler.assemble(tokens)\n        assert np.all(expr_validity_array)\n        labels = batch['ans_label_batch']\n        expr_feed = compiler.build_feed_dict(expr_list)\n        expr_feed[ans_label_batch] = labels\n        (scores_val, avg_sample_loss_val, _) = sess.partial_run(h, fetches=(scores, avg_sample_loss, train_step), feed_dict=expr_feed)\n        predictions = np.argmax(scores_val, axis=1)\n        accuracy = np.mean(np.logical_and(expr_validity_array, predictions == labels))\n        avg_accuracy += (1 - accuracy_decay) * (accuracy - avg_accuracy)\n        if (n_iter + 1) % config.log_interval == 0:\n            tf.logging.info('iter = %d\\n\\tloss = %f, accuracy (cur) = %f, accuracy (avg) = %f, entropy = %f' % (n_iter + 1, avg_sample_loss_val, accuracy, avg_accuracy, -entropy_reg_val))\n            summary = sess.run(fetches=log_step_train, feed_dict={loss_ph: avg_sample_loss_val, entropy_ph: -entropy_reg_val, accuracy_ph: avg_accuracy})\n            log_writer.add_summary(summary, n_iter + 1)\n        if (n_iter + 1) % config.snapshot_interval == 0:\n            snapshot_file = os.path.join(config.model_dir, '%08d' % (n_iter + 1))\n            snapshot_saver.save(sess, snapshot_file, write_meta_graph=False)\n            tf.logging.info('Snapshot saved to %s' % snapshot_file)\n    tf.logging.info('Run finished.')",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    config = prepare_dirs_and_logger(config_raw)\n    save_config(config)\n    rng = np.random.RandomState(config.random_seed)\n    tf.set_random_seed(config.random_seed)\n    config.rng = rng\n    config.module_names = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    config.gt_layout_tokens = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    assembler = Assembler(config)\n    sample_builder = SampleBuilder(config)\n    config = sample_builder.config\n    data_train = sample_builder.data_all['train']\n    data_reader_train = DataReader(config, data_train, assembler, shuffle=True, one_pass=False)\n    num_vocab_txt = len(sample_builder.dict_all)\n    num_vocab_nmn = len(assembler.module_names)\n    num_choices = len(sample_builder.dict_all)\n    text_seq_batch = tf.placeholder(tf.int32, [None, None])\n    seq_len_batch = tf.placeholder(tf.int32, [None])\n    ans_label_batch = tf.placeholder(tf.int32, [None])\n    use_gt_layout = tf.constant(True, dtype=tf.bool)\n    gt_layout_batch = tf.placeholder(tf.int32, [None, None])\n    model = Model(config, sample_builder.kb, text_seq_batch, seq_len_batch, num_vocab_txt=num_vocab_txt, num_vocab_nmn=num_vocab_nmn, EOS_idx=assembler.EOS_idx, num_choices=num_choices, decoder_sampling=True, use_gt_layout=use_gt_layout, gt_layout_batch=gt_layout_batch)\n    compiler = model.compiler\n    scores = model.scores\n    log_seq_prob = model.log_seq_prob\n    softmax_loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=ans_label_batch)\n    final_loss_per_sample = softmax_loss_per_sample\n    avg_sample_loss = tf.reduce_mean(final_loss_per_sample)\n    seq_likelihood_loss = tf.reduce_mean(-log_seq_prob)\n    total_training_loss = seq_likelihood_loss + avg_sample_loss\n    total_loss = total_training_loss + config.weight_decay * model.l2_reg\n    solver = tf.train.AdamOptimizer()\n    gradients = solver.compute_gradients(total_loss)\n    gradients = [(tf.clip_by_norm(g, config.max_grad_norm), v) for (g, v) in gradients]\n    solver_op = solver.apply_gradients(gradients)\n    with tf.control_dependencies([solver_op]):\n        train_step = tf.constant(0)\n    log_writer = tf.summary.FileWriter(config.log_dir, tf.get_default_graph())\n    loss_ph = tf.placeholder(tf.float32, [])\n    entropy_ph = tf.placeholder(tf.float32, [])\n    accuracy_ph = tf.placeholder(tf.float32, [])\n    summary_train = [tf.summary.scalar('avg_sample_loss', loss_ph), tf.summary.scalar('entropy', entropy_ph), tf.summary.scalar('avg_accuracy', accuracy_ph)]\n    log_step_train = tf.summary.merge(summary_train)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    snapshot_saver = tf.train.Saver(max_to_keep=None)\n    show_all_variables()\n    avg_accuracy = 0\n    accuracy_decay = 0.99\n    for (n_iter, batch) in enumerate(data_reader_train.batches()):\n        if n_iter >= config.max_iter:\n            break\n        h = sess.partial_run_setup(fetches=[model.predicted_tokens, model.entropy_reg, scores, avg_sample_loss, train_step], feeds=[text_seq_batch, seq_len_batch, gt_layout_batch, compiler.loom_input_tensor, ans_label_batch])\n        (tokens, entropy_reg_val) = sess.partial_run(h, fetches=(model.predicted_tokens, model.entropy_reg), feed_dict={text_seq_batch: batch['input_seq_batch'], seq_len_batch: batch['seq_len_batch'], gt_layout_batch: batch['gt_layout_batch']})\n        (expr_list, expr_validity_array) = assembler.assemble(tokens)\n        assert np.all(expr_validity_array)\n        labels = batch['ans_label_batch']\n        expr_feed = compiler.build_feed_dict(expr_list)\n        expr_feed[ans_label_batch] = labels\n        (scores_val, avg_sample_loss_val, _) = sess.partial_run(h, fetches=(scores, avg_sample_loss, train_step), feed_dict=expr_feed)\n        predictions = np.argmax(scores_val, axis=1)\n        accuracy = np.mean(np.logical_and(expr_validity_array, predictions == labels))\n        avg_accuracy += (1 - accuracy_decay) * (accuracy - avg_accuracy)\n        if (n_iter + 1) % config.log_interval == 0:\n            tf.logging.info('iter = %d\\n\\tloss = %f, accuracy (cur) = %f, accuracy (avg) = %f, entropy = %f' % (n_iter + 1, avg_sample_loss_val, accuracy, avg_accuracy, -entropy_reg_val))\n            summary = sess.run(fetches=log_step_train, feed_dict={loss_ph: avg_sample_loss_val, entropy_ph: -entropy_reg_val, accuracy_ph: avg_accuracy})\n            log_writer.add_summary(summary, n_iter + 1)\n        if (n_iter + 1) % config.snapshot_interval == 0:\n            snapshot_file = os.path.join(config.model_dir, '%08d' % (n_iter + 1))\n            snapshot_saver.save(sess, snapshot_file, write_meta_graph=False)\n            tf.logging.info('Snapshot saved to %s' % snapshot_file)\n    tf.logging.info('Run finished.')",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    config = prepare_dirs_and_logger(config_raw)\n    save_config(config)\n    rng = np.random.RandomState(config.random_seed)\n    tf.set_random_seed(config.random_seed)\n    config.rng = rng\n    config.module_names = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    config.gt_layout_tokens = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n    assembler = Assembler(config)\n    sample_builder = SampleBuilder(config)\n    config = sample_builder.config\n    data_train = sample_builder.data_all['train']\n    data_reader_train = DataReader(config, data_train, assembler, shuffle=True, one_pass=False)\n    num_vocab_txt = len(sample_builder.dict_all)\n    num_vocab_nmn = len(assembler.module_names)\n    num_choices = len(sample_builder.dict_all)\n    text_seq_batch = tf.placeholder(tf.int32, [None, None])\n    seq_len_batch = tf.placeholder(tf.int32, [None])\n    ans_label_batch = tf.placeholder(tf.int32, [None])\n    use_gt_layout = tf.constant(True, dtype=tf.bool)\n    gt_layout_batch = tf.placeholder(tf.int32, [None, None])\n    model = Model(config, sample_builder.kb, text_seq_batch, seq_len_batch, num_vocab_txt=num_vocab_txt, num_vocab_nmn=num_vocab_nmn, EOS_idx=assembler.EOS_idx, num_choices=num_choices, decoder_sampling=True, use_gt_layout=use_gt_layout, gt_layout_batch=gt_layout_batch)\n    compiler = model.compiler\n    scores = model.scores\n    log_seq_prob = model.log_seq_prob\n    softmax_loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=ans_label_batch)\n    final_loss_per_sample = softmax_loss_per_sample\n    avg_sample_loss = tf.reduce_mean(final_loss_per_sample)\n    seq_likelihood_loss = tf.reduce_mean(-log_seq_prob)\n    total_training_loss = seq_likelihood_loss + avg_sample_loss\n    total_loss = total_training_loss + config.weight_decay * model.l2_reg\n    solver = tf.train.AdamOptimizer()\n    gradients = solver.compute_gradients(total_loss)\n    gradients = [(tf.clip_by_norm(g, config.max_grad_norm), v) for (g, v) in gradients]\n    solver_op = solver.apply_gradients(gradients)\n    with tf.control_dependencies([solver_op]):\n        train_step = tf.constant(0)\n    log_writer = tf.summary.FileWriter(config.log_dir, tf.get_default_graph())\n    loss_ph = tf.placeholder(tf.float32, [])\n    entropy_ph = tf.placeholder(tf.float32, [])\n    accuracy_ph = tf.placeholder(tf.float32, [])\n    summary_train = [tf.summary.scalar('avg_sample_loss', loss_ph), tf.summary.scalar('entropy', entropy_ph), tf.summary.scalar('avg_accuracy', accuracy_ph)]\n    log_step_train = tf.summary.merge(summary_train)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    snapshot_saver = tf.train.Saver(max_to_keep=None)\n    show_all_variables()\n    avg_accuracy = 0\n    accuracy_decay = 0.99\n    for (n_iter, batch) in enumerate(data_reader_train.batches()):\n        if n_iter >= config.max_iter:\n            break\n        h = sess.partial_run_setup(fetches=[model.predicted_tokens, model.entropy_reg, scores, avg_sample_loss, train_step], feeds=[text_seq_batch, seq_len_batch, gt_layout_batch, compiler.loom_input_tensor, ans_label_batch])\n        (tokens, entropy_reg_val) = sess.partial_run(h, fetches=(model.predicted_tokens, model.entropy_reg), feed_dict={text_seq_batch: batch['input_seq_batch'], seq_len_batch: batch['seq_len_batch'], gt_layout_batch: batch['gt_layout_batch']})\n        (expr_list, expr_validity_array) = assembler.assemble(tokens)\n        assert np.all(expr_validity_array)\n        labels = batch['ans_label_batch']\n        expr_feed = compiler.build_feed_dict(expr_list)\n        expr_feed[ans_label_batch] = labels\n        (scores_val, avg_sample_loss_val, _) = sess.partial_run(h, fetches=(scores, avg_sample_loss, train_step), feed_dict=expr_feed)\n        predictions = np.argmax(scores_val, axis=1)\n        accuracy = np.mean(np.logical_and(expr_validity_array, predictions == labels))\n        avg_accuracy += (1 - accuracy_decay) * (accuracy - avg_accuracy)\n        if (n_iter + 1) % config.log_interval == 0:\n            tf.logging.info('iter = %d\\n\\tloss = %f, accuracy (cur) = %f, accuracy (avg) = %f, entropy = %f' % (n_iter + 1, avg_sample_loss_val, accuracy, avg_accuracy, -entropy_reg_val))\n            summary = sess.run(fetches=log_step_train, feed_dict={loss_ph: avg_sample_loss_val, entropy_ph: -entropy_reg_val, accuracy_ph: avg_accuracy})\n            log_writer.add_summary(summary, n_iter + 1)\n        if (n_iter + 1) % config.snapshot_interval == 0:\n            snapshot_file = os.path.join(config.model_dir, '%08d' % (n_iter + 1))\n            snapshot_saver.save(sess, snapshot_file, write_meta_graph=False)\n            tf.logging.info('Snapshot saved to %s' % snapshot_file)\n    tf.logging.info('Run finished.')"
        ]
    }
]