[
    {
        "func_name": "mask_pts_at_padded_regions",
        "original": "@torch.no_grad()\ndef mask_pts_at_padded_regions(grid_pt: Tensor, mask: Tensor) -> Tensor:\n    \"\"\"For megadepth dataset, zero-padding exists in images.\"\"\"\n    (n, h, w) = mask.shape\n    mask = mask.reshape(n, h * w).unsqueeze(-1).repeat(1, 1, 2)\n    grid_pt[~mask.bool()] = 0\n    return grid_pt",
        "mutated": [
            "@torch.no_grad()\ndef mask_pts_at_padded_regions(grid_pt: Tensor, mask: Tensor) -> Tensor:\n    if False:\n        i = 10\n    'For megadepth dataset, zero-padding exists in images.'\n    (n, h, w) = mask.shape\n    mask = mask.reshape(n, h * w).unsqueeze(-1).repeat(1, 1, 2)\n    grid_pt[~mask.bool()] = 0\n    return grid_pt",
            "@torch.no_grad()\ndef mask_pts_at_padded_regions(grid_pt: Tensor, mask: Tensor) -> Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'For megadepth dataset, zero-padding exists in images.'\n    (n, h, w) = mask.shape\n    mask = mask.reshape(n, h * w).unsqueeze(-1).repeat(1, 1, 2)\n    grid_pt[~mask.bool()] = 0\n    return grid_pt",
            "@torch.no_grad()\ndef mask_pts_at_padded_regions(grid_pt: Tensor, mask: Tensor) -> Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'For megadepth dataset, zero-padding exists in images.'\n    (n, h, w) = mask.shape\n    mask = mask.reshape(n, h * w).unsqueeze(-1).repeat(1, 1, 2)\n    grid_pt[~mask.bool()] = 0\n    return grid_pt",
            "@torch.no_grad()\ndef mask_pts_at_padded_regions(grid_pt: Tensor, mask: Tensor) -> Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'For megadepth dataset, zero-padding exists in images.'\n    (n, h, w) = mask.shape\n    mask = mask.reshape(n, h * w).unsqueeze(-1).repeat(1, 1, 2)\n    grid_pt[~mask.bool()] = 0\n    return grid_pt",
            "@torch.no_grad()\ndef mask_pts_at_padded_regions(grid_pt: Tensor, mask: Tensor) -> Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'For megadepth dataset, zero-padding exists in images.'\n    (n, h, w) = mask.shape\n    mask = mask.reshape(n, h * w).unsqueeze(-1).repeat(1, 1, 2)\n    grid_pt[~mask.bool()] = 0\n    return grid_pt"
        ]
    },
    {
        "func_name": "out_bound_mask",
        "original": "def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n    return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)",
        "mutated": [
            "def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n    if False:\n        i = 10\n    return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)",
            "def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)",
            "def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)",
            "def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)",
            "def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)"
        ]
    },
    {
        "func_name": "spvs_coarse",
        "original": "@torch.no_grad()\ndef spvs_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    \"\"\"\n    Update:\n        data (dict): {\n            \"conf_matrix_gt\": [N, hw0, hw1],\n            'spv_b_ids': [M]\n            'spv_i_ids': [M]\n            'spv_j_ids': [M]\n            'spv_w_pt0_i': [N, hw0, 2], in original image resolution\n            'spv_pt1_i': [N, hw1, 2], in original image resolution\n        }\n\n    NOTE:\n        - for scannet dataset, there're 3 kinds of resolution {i, c, f}\n        - for megadepth dataset, there're 4 kinds of resolution {i, i_resize, c, f}\n    \"\"\"\n    device = data['image0'].device\n    (N, _, H0, W0) = data['image0'].shape\n    (_, _, H1, W1) = data['image1'].shape\n    scale = config['LOFTR']['RESOLUTION'][0]\n    scale0 = scale * data['scale0'][:, None] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][:, None] if 'scale0' in data else scale\n    (h0, w0, h1, w1) = (x // scale for x in [H0, W0, H1, W1])\n    grid_pt0_c = create_meshgrid(h0, w0, False, device).reshape(1, h0 * w0, 2).expand(N, h0 * w0, 2)\n    grid_pt0_i = scale0 * grid_pt0_c\n    grid_pt1_c = create_meshgrid(h1, w1, False, device).reshape(1, h1 * w1, 2).expand(N, h1 * w1, 2)\n    grid_pt1_i = scale1 * grid_pt1_c\n    if 'mask0' in data:\n        grid_pt0_i = mask_pts_at_padded_regions(grid_pt0_i, data['mask0'])\n        grid_pt1_i = mask_pts_at_padded_regions(grid_pt1_i, data['mask1'])\n    (_, w_pt0_i) = warp_kpts(grid_pt0_i, data['depth0'], data['depth1'], data['T_0to1'], data['K0'], data['K1'])\n    (_, w_pt1_i) = warp_kpts(grid_pt1_i, data['depth1'], data['depth0'], data['T_1to0'], data['K1'], data['K0'])\n    w_pt0_c = w_pt0_i / scale1\n    w_pt1_c = w_pt1_i / scale0\n    w_pt0_c_round = w_pt0_c[:, :, :].round().long()\n    nearest_index1 = w_pt0_c_round[..., 0] + w_pt0_c_round[..., 1] * w1\n    w_pt1_c_round = w_pt1_c[:, :, :].round().long()\n    nearest_index0 = w_pt1_c_round[..., 0] + w_pt1_c_round[..., 1] * w0\n\n    def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n        return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)\n    nearest_index1[out_bound_mask(w_pt0_c_round, w1, h1)] = 0\n    nearest_index0[out_bound_mask(w_pt1_c_round, w0, h0)] = 0\n    loop_back = torch.stack([nearest_index0[_b][_i] for (_b, _i) in enumerate(nearest_index1)], dim=0)\n    correct_0to1 = loop_back == torch.arange(h0 * w0, device=device)[None].repeat(N, 1)\n    correct_0to1[:, 0] = False\n    conf_matrix_gt = torch.zeros(N, h0 * w0, h1 * w1, device=device)\n    (b_ids, i_ids) = torch.where(correct_0to1 != 0)\n    j_ids = nearest_index1[b_ids, i_ids]\n    conf_matrix_gt[b_ids, i_ids, j_ids] = 1\n    data.update({'conf_matrix_gt': conf_matrix_gt})\n    if len(b_ids) == 0:\n        b_ids = torch.tensor([0], device=device)\n        i_ids = torch.tensor([0], device=device)\n        j_ids = torch.tensor([0], device=device)\n    data.update({'spv_b_ids': b_ids, 'spv_i_ids': i_ids, 'spv_j_ids': j_ids})\n    data.update({'spv_w_pt0_i': w_pt0_i, 'spv_pt1_i': grid_pt1_i})",
        "mutated": [
            "@torch.no_grad()\ndef spvs_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n    '\\n    Update:\\n        data (dict): {\\n            \"conf_matrix_gt\": [N, hw0, hw1],\\n            \\'spv_b_ids\\': [M]\\n            \\'spv_i_ids\\': [M]\\n            \\'spv_j_ids\\': [M]\\n            \\'spv_w_pt0_i\\': [N, hw0, 2], in original image resolution\\n            \\'spv_pt1_i\\': [N, hw1, 2], in original image resolution\\n        }\\n\\n    NOTE:\\n        - for scannet dataset, there\\'re 3 kinds of resolution {i, c, f}\\n        - for megadepth dataset, there\\'re 4 kinds of resolution {i, i_resize, c, f}\\n    '\n    device = data['image0'].device\n    (N, _, H0, W0) = data['image0'].shape\n    (_, _, H1, W1) = data['image1'].shape\n    scale = config['LOFTR']['RESOLUTION'][0]\n    scale0 = scale * data['scale0'][:, None] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][:, None] if 'scale0' in data else scale\n    (h0, w0, h1, w1) = (x // scale for x in [H0, W0, H1, W1])\n    grid_pt0_c = create_meshgrid(h0, w0, False, device).reshape(1, h0 * w0, 2).expand(N, h0 * w0, 2)\n    grid_pt0_i = scale0 * grid_pt0_c\n    grid_pt1_c = create_meshgrid(h1, w1, False, device).reshape(1, h1 * w1, 2).expand(N, h1 * w1, 2)\n    grid_pt1_i = scale1 * grid_pt1_c\n    if 'mask0' in data:\n        grid_pt0_i = mask_pts_at_padded_regions(grid_pt0_i, data['mask0'])\n        grid_pt1_i = mask_pts_at_padded_regions(grid_pt1_i, data['mask1'])\n    (_, w_pt0_i) = warp_kpts(grid_pt0_i, data['depth0'], data['depth1'], data['T_0to1'], data['K0'], data['K1'])\n    (_, w_pt1_i) = warp_kpts(grid_pt1_i, data['depth1'], data['depth0'], data['T_1to0'], data['K1'], data['K0'])\n    w_pt0_c = w_pt0_i / scale1\n    w_pt1_c = w_pt1_i / scale0\n    w_pt0_c_round = w_pt0_c[:, :, :].round().long()\n    nearest_index1 = w_pt0_c_round[..., 0] + w_pt0_c_round[..., 1] * w1\n    w_pt1_c_round = w_pt1_c[:, :, :].round().long()\n    nearest_index0 = w_pt1_c_round[..., 0] + w_pt1_c_round[..., 1] * w0\n\n    def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n        return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)\n    nearest_index1[out_bound_mask(w_pt0_c_round, w1, h1)] = 0\n    nearest_index0[out_bound_mask(w_pt1_c_round, w0, h0)] = 0\n    loop_back = torch.stack([nearest_index0[_b][_i] for (_b, _i) in enumerate(nearest_index1)], dim=0)\n    correct_0to1 = loop_back == torch.arange(h0 * w0, device=device)[None].repeat(N, 1)\n    correct_0to1[:, 0] = False\n    conf_matrix_gt = torch.zeros(N, h0 * w0, h1 * w1, device=device)\n    (b_ids, i_ids) = torch.where(correct_0to1 != 0)\n    j_ids = nearest_index1[b_ids, i_ids]\n    conf_matrix_gt[b_ids, i_ids, j_ids] = 1\n    data.update({'conf_matrix_gt': conf_matrix_gt})\n    if len(b_ids) == 0:\n        b_ids = torch.tensor([0], device=device)\n        i_ids = torch.tensor([0], device=device)\n        j_ids = torch.tensor([0], device=device)\n    data.update({'spv_b_ids': b_ids, 'spv_i_ids': i_ids, 'spv_j_ids': j_ids})\n    data.update({'spv_w_pt0_i': w_pt0_i, 'spv_pt1_i': grid_pt1_i})",
            "@torch.no_grad()\ndef spvs_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Update:\\n        data (dict): {\\n            \"conf_matrix_gt\": [N, hw0, hw1],\\n            \\'spv_b_ids\\': [M]\\n            \\'spv_i_ids\\': [M]\\n            \\'spv_j_ids\\': [M]\\n            \\'spv_w_pt0_i\\': [N, hw0, 2], in original image resolution\\n            \\'spv_pt1_i\\': [N, hw1, 2], in original image resolution\\n        }\\n\\n    NOTE:\\n        - for scannet dataset, there\\'re 3 kinds of resolution {i, c, f}\\n        - for megadepth dataset, there\\'re 4 kinds of resolution {i, i_resize, c, f}\\n    '\n    device = data['image0'].device\n    (N, _, H0, W0) = data['image0'].shape\n    (_, _, H1, W1) = data['image1'].shape\n    scale = config['LOFTR']['RESOLUTION'][0]\n    scale0 = scale * data['scale0'][:, None] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][:, None] if 'scale0' in data else scale\n    (h0, w0, h1, w1) = (x // scale for x in [H0, W0, H1, W1])\n    grid_pt0_c = create_meshgrid(h0, w0, False, device).reshape(1, h0 * w0, 2).expand(N, h0 * w0, 2)\n    grid_pt0_i = scale0 * grid_pt0_c\n    grid_pt1_c = create_meshgrid(h1, w1, False, device).reshape(1, h1 * w1, 2).expand(N, h1 * w1, 2)\n    grid_pt1_i = scale1 * grid_pt1_c\n    if 'mask0' in data:\n        grid_pt0_i = mask_pts_at_padded_regions(grid_pt0_i, data['mask0'])\n        grid_pt1_i = mask_pts_at_padded_regions(grid_pt1_i, data['mask1'])\n    (_, w_pt0_i) = warp_kpts(grid_pt0_i, data['depth0'], data['depth1'], data['T_0to1'], data['K0'], data['K1'])\n    (_, w_pt1_i) = warp_kpts(grid_pt1_i, data['depth1'], data['depth0'], data['T_1to0'], data['K1'], data['K0'])\n    w_pt0_c = w_pt0_i / scale1\n    w_pt1_c = w_pt1_i / scale0\n    w_pt0_c_round = w_pt0_c[:, :, :].round().long()\n    nearest_index1 = w_pt0_c_round[..., 0] + w_pt0_c_round[..., 1] * w1\n    w_pt1_c_round = w_pt1_c[:, :, :].round().long()\n    nearest_index0 = w_pt1_c_round[..., 0] + w_pt1_c_round[..., 1] * w0\n\n    def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n        return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)\n    nearest_index1[out_bound_mask(w_pt0_c_round, w1, h1)] = 0\n    nearest_index0[out_bound_mask(w_pt1_c_round, w0, h0)] = 0\n    loop_back = torch.stack([nearest_index0[_b][_i] for (_b, _i) in enumerate(nearest_index1)], dim=0)\n    correct_0to1 = loop_back == torch.arange(h0 * w0, device=device)[None].repeat(N, 1)\n    correct_0to1[:, 0] = False\n    conf_matrix_gt = torch.zeros(N, h0 * w0, h1 * w1, device=device)\n    (b_ids, i_ids) = torch.where(correct_0to1 != 0)\n    j_ids = nearest_index1[b_ids, i_ids]\n    conf_matrix_gt[b_ids, i_ids, j_ids] = 1\n    data.update({'conf_matrix_gt': conf_matrix_gt})\n    if len(b_ids) == 0:\n        b_ids = torch.tensor([0], device=device)\n        i_ids = torch.tensor([0], device=device)\n        j_ids = torch.tensor([0], device=device)\n    data.update({'spv_b_ids': b_ids, 'spv_i_ids': i_ids, 'spv_j_ids': j_ids})\n    data.update({'spv_w_pt0_i': w_pt0_i, 'spv_pt1_i': grid_pt1_i})",
            "@torch.no_grad()\ndef spvs_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Update:\\n        data (dict): {\\n            \"conf_matrix_gt\": [N, hw0, hw1],\\n            \\'spv_b_ids\\': [M]\\n            \\'spv_i_ids\\': [M]\\n            \\'spv_j_ids\\': [M]\\n            \\'spv_w_pt0_i\\': [N, hw0, 2], in original image resolution\\n            \\'spv_pt1_i\\': [N, hw1, 2], in original image resolution\\n        }\\n\\n    NOTE:\\n        - for scannet dataset, there\\'re 3 kinds of resolution {i, c, f}\\n        - for megadepth dataset, there\\'re 4 kinds of resolution {i, i_resize, c, f}\\n    '\n    device = data['image0'].device\n    (N, _, H0, W0) = data['image0'].shape\n    (_, _, H1, W1) = data['image1'].shape\n    scale = config['LOFTR']['RESOLUTION'][0]\n    scale0 = scale * data['scale0'][:, None] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][:, None] if 'scale0' in data else scale\n    (h0, w0, h1, w1) = (x // scale for x in [H0, W0, H1, W1])\n    grid_pt0_c = create_meshgrid(h0, w0, False, device).reshape(1, h0 * w0, 2).expand(N, h0 * w0, 2)\n    grid_pt0_i = scale0 * grid_pt0_c\n    grid_pt1_c = create_meshgrid(h1, w1, False, device).reshape(1, h1 * w1, 2).expand(N, h1 * w1, 2)\n    grid_pt1_i = scale1 * grid_pt1_c\n    if 'mask0' in data:\n        grid_pt0_i = mask_pts_at_padded_regions(grid_pt0_i, data['mask0'])\n        grid_pt1_i = mask_pts_at_padded_regions(grid_pt1_i, data['mask1'])\n    (_, w_pt0_i) = warp_kpts(grid_pt0_i, data['depth0'], data['depth1'], data['T_0to1'], data['K0'], data['K1'])\n    (_, w_pt1_i) = warp_kpts(grid_pt1_i, data['depth1'], data['depth0'], data['T_1to0'], data['K1'], data['K0'])\n    w_pt0_c = w_pt0_i / scale1\n    w_pt1_c = w_pt1_i / scale0\n    w_pt0_c_round = w_pt0_c[:, :, :].round().long()\n    nearest_index1 = w_pt0_c_round[..., 0] + w_pt0_c_round[..., 1] * w1\n    w_pt1_c_round = w_pt1_c[:, :, :].round().long()\n    nearest_index0 = w_pt1_c_round[..., 0] + w_pt1_c_round[..., 1] * w0\n\n    def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n        return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)\n    nearest_index1[out_bound_mask(w_pt0_c_round, w1, h1)] = 0\n    nearest_index0[out_bound_mask(w_pt1_c_round, w0, h0)] = 0\n    loop_back = torch.stack([nearest_index0[_b][_i] for (_b, _i) in enumerate(nearest_index1)], dim=0)\n    correct_0to1 = loop_back == torch.arange(h0 * w0, device=device)[None].repeat(N, 1)\n    correct_0to1[:, 0] = False\n    conf_matrix_gt = torch.zeros(N, h0 * w0, h1 * w1, device=device)\n    (b_ids, i_ids) = torch.where(correct_0to1 != 0)\n    j_ids = nearest_index1[b_ids, i_ids]\n    conf_matrix_gt[b_ids, i_ids, j_ids] = 1\n    data.update({'conf_matrix_gt': conf_matrix_gt})\n    if len(b_ids) == 0:\n        b_ids = torch.tensor([0], device=device)\n        i_ids = torch.tensor([0], device=device)\n        j_ids = torch.tensor([0], device=device)\n    data.update({'spv_b_ids': b_ids, 'spv_i_ids': i_ids, 'spv_j_ids': j_ids})\n    data.update({'spv_w_pt0_i': w_pt0_i, 'spv_pt1_i': grid_pt1_i})",
            "@torch.no_grad()\ndef spvs_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Update:\\n        data (dict): {\\n            \"conf_matrix_gt\": [N, hw0, hw1],\\n            \\'spv_b_ids\\': [M]\\n            \\'spv_i_ids\\': [M]\\n            \\'spv_j_ids\\': [M]\\n            \\'spv_w_pt0_i\\': [N, hw0, 2], in original image resolution\\n            \\'spv_pt1_i\\': [N, hw1, 2], in original image resolution\\n        }\\n\\n    NOTE:\\n        - for scannet dataset, there\\'re 3 kinds of resolution {i, c, f}\\n        - for megadepth dataset, there\\'re 4 kinds of resolution {i, i_resize, c, f}\\n    '\n    device = data['image0'].device\n    (N, _, H0, W0) = data['image0'].shape\n    (_, _, H1, W1) = data['image1'].shape\n    scale = config['LOFTR']['RESOLUTION'][0]\n    scale0 = scale * data['scale0'][:, None] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][:, None] if 'scale0' in data else scale\n    (h0, w0, h1, w1) = (x // scale for x in [H0, W0, H1, W1])\n    grid_pt0_c = create_meshgrid(h0, w0, False, device).reshape(1, h0 * w0, 2).expand(N, h0 * w0, 2)\n    grid_pt0_i = scale0 * grid_pt0_c\n    grid_pt1_c = create_meshgrid(h1, w1, False, device).reshape(1, h1 * w1, 2).expand(N, h1 * w1, 2)\n    grid_pt1_i = scale1 * grid_pt1_c\n    if 'mask0' in data:\n        grid_pt0_i = mask_pts_at_padded_regions(grid_pt0_i, data['mask0'])\n        grid_pt1_i = mask_pts_at_padded_regions(grid_pt1_i, data['mask1'])\n    (_, w_pt0_i) = warp_kpts(grid_pt0_i, data['depth0'], data['depth1'], data['T_0to1'], data['K0'], data['K1'])\n    (_, w_pt1_i) = warp_kpts(grid_pt1_i, data['depth1'], data['depth0'], data['T_1to0'], data['K1'], data['K0'])\n    w_pt0_c = w_pt0_i / scale1\n    w_pt1_c = w_pt1_i / scale0\n    w_pt0_c_round = w_pt0_c[:, :, :].round().long()\n    nearest_index1 = w_pt0_c_round[..., 0] + w_pt0_c_round[..., 1] * w1\n    w_pt1_c_round = w_pt1_c[:, :, :].round().long()\n    nearest_index0 = w_pt1_c_round[..., 0] + w_pt1_c_round[..., 1] * w0\n\n    def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n        return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)\n    nearest_index1[out_bound_mask(w_pt0_c_round, w1, h1)] = 0\n    nearest_index0[out_bound_mask(w_pt1_c_round, w0, h0)] = 0\n    loop_back = torch.stack([nearest_index0[_b][_i] for (_b, _i) in enumerate(nearest_index1)], dim=0)\n    correct_0to1 = loop_back == torch.arange(h0 * w0, device=device)[None].repeat(N, 1)\n    correct_0to1[:, 0] = False\n    conf_matrix_gt = torch.zeros(N, h0 * w0, h1 * w1, device=device)\n    (b_ids, i_ids) = torch.where(correct_0to1 != 0)\n    j_ids = nearest_index1[b_ids, i_ids]\n    conf_matrix_gt[b_ids, i_ids, j_ids] = 1\n    data.update({'conf_matrix_gt': conf_matrix_gt})\n    if len(b_ids) == 0:\n        b_ids = torch.tensor([0], device=device)\n        i_ids = torch.tensor([0], device=device)\n        j_ids = torch.tensor([0], device=device)\n    data.update({'spv_b_ids': b_ids, 'spv_i_ids': i_ids, 'spv_j_ids': j_ids})\n    data.update({'spv_w_pt0_i': w_pt0_i, 'spv_pt1_i': grid_pt1_i})",
            "@torch.no_grad()\ndef spvs_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Update:\\n        data (dict): {\\n            \"conf_matrix_gt\": [N, hw0, hw1],\\n            \\'spv_b_ids\\': [M]\\n            \\'spv_i_ids\\': [M]\\n            \\'spv_j_ids\\': [M]\\n            \\'spv_w_pt0_i\\': [N, hw0, 2], in original image resolution\\n            \\'spv_pt1_i\\': [N, hw1, 2], in original image resolution\\n        }\\n\\n    NOTE:\\n        - for scannet dataset, there\\'re 3 kinds of resolution {i, c, f}\\n        - for megadepth dataset, there\\'re 4 kinds of resolution {i, i_resize, c, f}\\n    '\n    device = data['image0'].device\n    (N, _, H0, W0) = data['image0'].shape\n    (_, _, H1, W1) = data['image1'].shape\n    scale = config['LOFTR']['RESOLUTION'][0]\n    scale0 = scale * data['scale0'][:, None] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][:, None] if 'scale0' in data else scale\n    (h0, w0, h1, w1) = (x // scale for x in [H0, W0, H1, W1])\n    grid_pt0_c = create_meshgrid(h0, w0, False, device).reshape(1, h0 * w0, 2).expand(N, h0 * w0, 2)\n    grid_pt0_i = scale0 * grid_pt0_c\n    grid_pt1_c = create_meshgrid(h1, w1, False, device).reshape(1, h1 * w1, 2).expand(N, h1 * w1, 2)\n    grid_pt1_i = scale1 * grid_pt1_c\n    if 'mask0' in data:\n        grid_pt0_i = mask_pts_at_padded_regions(grid_pt0_i, data['mask0'])\n        grid_pt1_i = mask_pts_at_padded_regions(grid_pt1_i, data['mask1'])\n    (_, w_pt0_i) = warp_kpts(grid_pt0_i, data['depth0'], data['depth1'], data['T_0to1'], data['K0'], data['K1'])\n    (_, w_pt1_i) = warp_kpts(grid_pt1_i, data['depth1'], data['depth0'], data['T_1to0'], data['K1'], data['K0'])\n    w_pt0_c = w_pt0_i / scale1\n    w_pt1_c = w_pt1_i / scale0\n    w_pt0_c_round = w_pt0_c[:, :, :].round().long()\n    nearest_index1 = w_pt0_c_round[..., 0] + w_pt0_c_round[..., 1] * w1\n    w_pt1_c_round = w_pt1_c[:, :, :].round().long()\n    nearest_index0 = w_pt1_c_round[..., 0] + w_pt1_c_round[..., 1] * w0\n\n    def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:\n        return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)\n    nearest_index1[out_bound_mask(w_pt0_c_round, w1, h1)] = 0\n    nearest_index0[out_bound_mask(w_pt1_c_round, w0, h0)] = 0\n    loop_back = torch.stack([nearest_index0[_b][_i] for (_b, _i) in enumerate(nearest_index1)], dim=0)\n    correct_0to1 = loop_back == torch.arange(h0 * w0, device=device)[None].repeat(N, 1)\n    correct_0to1[:, 0] = False\n    conf_matrix_gt = torch.zeros(N, h0 * w0, h1 * w1, device=device)\n    (b_ids, i_ids) = torch.where(correct_0to1 != 0)\n    j_ids = nearest_index1[b_ids, i_ids]\n    conf_matrix_gt[b_ids, i_ids, j_ids] = 1\n    data.update({'conf_matrix_gt': conf_matrix_gt})\n    if len(b_ids) == 0:\n        b_ids = torch.tensor([0], device=device)\n        i_ids = torch.tensor([0], device=device)\n        j_ids = torch.tensor([0], device=device)\n    data.update({'spv_b_ids': b_ids, 'spv_i_ids': i_ids, 'spv_j_ids': j_ids})\n    data.update({'spv_w_pt0_i': w_pt0_i, 'spv_pt1_i': grid_pt1_i})"
        ]
    },
    {
        "func_name": "compute_supervision_coarse",
        "original": "def compute_supervision_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if len(set(data['dataset_name'])) != 1:\n        raise ValueError('Do not support mixed datasets training!')\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_coarse(data, config)\n    else:\n        raise ValueError(f'Unknown data source: {data_source}')",
        "mutated": [
            "def compute_supervision_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n    if len(set(data['dataset_name'])) != 1:\n        raise ValueError('Do not support mixed datasets training!')\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_coarse(data, config)\n    else:\n        raise ValueError(f'Unknown data source: {data_source}')",
            "def compute_supervision_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if len(set(data['dataset_name'])) != 1:\n        raise ValueError('Do not support mixed datasets training!')\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_coarse(data, config)\n    else:\n        raise ValueError(f'Unknown data source: {data_source}')",
            "def compute_supervision_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if len(set(data['dataset_name'])) != 1:\n        raise ValueError('Do not support mixed datasets training!')\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_coarse(data, config)\n    else:\n        raise ValueError(f'Unknown data source: {data_source}')",
            "def compute_supervision_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if len(set(data['dataset_name'])) != 1:\n        raise ValueError('Do not support mixed datasets training!')\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_coarse(data, config)\n    else:\n        raise ValueError(f'Unknown data source: {data_source}')",
            "def compute_supervision_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if len(set(data['dataset_name'])) != 1:\n        raise ValueError('Do not support mixed datasets training!')\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_coarse(data, config)\n    else:\n        raise ValueError(f'Unknown data source: {data_source}')"
        ]
    },
    {
        "func_name": "spvs_fine",
        "original": "@torch.no_grad()\ndef spvs_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    \"\"\"\n    Update:\n        data (dict):{\n            \"expec_f_gt\": [M, 2]}\n    \"\"\"\n    (w_pt0_i, pt1_i) = (data['spv_w_pt0_i'], data['spv_pt1_i'])\n    scale = config['LOFTR']['RESOLUTION'][1]\n    radius = config['LOFTR']['FINE_WINDOW_SIZE'] // 2\n    (b_ids, i_ids, j_ids) = (data['b_ids'], data['i_ids'], data['j_ids'])\n    scale = scale * data['scale1'][b_ids] if 'scale0' in data else scale\n    expec_f_gt = (w_pt0_i[b_ids, i_ids] - pt1_i[b_ids, j_ids]) / scale / radius\n    data.update({'expec_f_gt': expec_f_gt})",
        "mutated": [
            "@torch.no_grad()\ndef spvs_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n    '\\n    Update:\\n        data (dict):{\\n            \"expec_f_gt\": [M, 2]}\\n    '\n    (w_pt0_i, pt1_i) = (data['spv_w_pt0_i'], data['spv_pt1_i'])\n    scale = config['LOFTR']['RESOLUTION'][1]\n    radius = config['LOFTR']['FINE_WINDOW_SIZE'] // 2\n    (b_ids, i_ids, j_ids) = (data['b_ids'], data['i_ids'], data['j_ids'])\n    scale = scale * data['scale1'][b_ids] if 'scale0' in data else scale\n    expec_f_gt = (w_pt0_i[b_ids, i_ids] - pt1_i[b_ids, j_ids]) / scale / radius\n    data.update({'expec_f_gt': expec_f_gt})",
            "@torch.no_grad()\ndef spvs_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Update:\\n        data (dict):{\\n            \"expec_f_gt\": [M, 2]}\\n    '\n    (w_pt0_i, pt1_i) = (data['spv_w_pt0_i'], data['spv_pt1_i'])\n    scale = config['LOFTR']['RESOLUTION'][1]\n    radius = config['LOFTR']['FINE_WINDOW_SIZE'] // 2\n    (b_ids, i_ids, j_ids) = (data['b_ids'], data['i_ids'], data['j_ids'])\n    scale = scale * data['scale1'][b_ids] if 'scale0' in data else scale\n    expec_f_gt = (w_pt0_i[b_ids, i_ids] - pt1_i[b_ids, j_ids]) / scale / radius\n    data.update({'expec_f_gt': expec_f_gt})",
            "@torch.no_grad()\ndef spvs_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Update:\\n        data (dict):{\\n            \"expec_f_gt\": [M, 2]}\\n    '\n    (w_pt0_i, pt1_i) = (data['spv_w_pt0_i'], data['spv_pt1_i'])\n    scale = config['LOFTR']['RESOLUTION'][1]\n    radius = config['LOFTR']['FINE_WINDOW_SIZE'] // 2\n    (b_ids, i_ids, j_ids) = (data['b_ids'], data['i_ids'], data['j_ids'])\n    scale = scale * data['scale1'][b_ids] if 'scale0' in data else scale\n    expec_f_gt = (w_pt0_i[b_ids, i_ids] - pt1_i[b_ids, j_ids]) / scale / radius\n    data.update({'expec_f_gt': expec_f_gt})",
            "@torch.no_grad()\ndef spvs_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Update:\\n        data (dict):{\\n            \"expec_f_gt\": [M, 2]}\\n    '\n    (w_pt0_i, pt1_i) = (data['spv_w_pt0_i'], data['spv_pt1_i'])\n    scale = config['LOFTR']['RESOLUTION'][1]\n    radius = config['LOFTR']['FINE_WINDOW_SIZE'] // 2\n    (b_ids, i_ids, j_ids) = (data['b_ids'], data['i_ids'], data['j_ids'])\n    scale = scale * data['scale1'][b_ids] if 'scale0' in data else scale\n    expec_f_gt = (w_pt0_i[b_ids, i_ids] - pt1_i[b_ids, j_ids]) / scale / radius\n    data.update({'expec_f_gt': expec_f_gt})",
            "@torch.no_grad()\ndef spvs_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Update:\\n        data (dict):{\\n            \"expec_f_gt\": [M, 2]}\\n    '\n    (w_pt0_i, pt1_i) = (data['spv_w_pt0_i'], data['spv_pt1_i'])\n    scale = config['LOFTR']['RESOLUTION'][1]\n    radius = config['LOFTR']['FINE_WINDOW_SIZE'] // 2\n    (b_ids, i_ids, j_ids) = (data['b_ids'], data['i_ids'], data['j_ids'])\n    scale = scale * data['scale1'][b_ids] if 'scale0' in data else scale\n    expec_f_gt = (w_pt0_i[b_ids, i_ids] - pt1_i[b_ids, j_ids]) / scale / radius\n    data.update({'expec_f_gt': expec_f_gt})"
        ]
    },
    {
        "func_name": "compute_supervision_fine",
        "original": "def compute_supervision_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_fine(data, config)\n    else:\n        raise NotImplementedError",
        "mutated": [
            "def compute_supervision_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_fine(data, config)\n    else:\n        raise NotImplementedError",
            "def compute_supervision_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_fine(data, config)\n    else:\n        raise NotImplementedError",
            "def compute_supervision_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_fine(data, config)\n    else:\n        raise NotImplementedError",
            "def compute_supervision_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_fine(data, config)\n    else:\n        raise NotImplementedError",
            "def compute_supervision_fine(data: dict[str, Any], config: dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    data_source = data['dataset_name'][0]\n    if data_source.lower() in ['scannet', 'megadepth']:\n        spvs_fine(data, config)\n    else:\n        raise NotImplementedError"
        ]
    }
]