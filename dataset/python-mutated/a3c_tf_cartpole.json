[
    {
        "func_name": "__init__",
        "original": "def __init__(self, state_size, action_size):\n    super(ActorCritic, self).__init__()\n    self.state_size = state_size\n    self.action_size = action_size\n    self.dense1 = layers.Dense(128, activation='relu')\n    self.policy_logits = layers.Dense(action_size)\n    self.dense2 = layers.Dense(128, activation='relu')\n    self.values = layers.Dense(1)",
        "mutated": [
            "def __init__(self, state_size, action_size):\n    if False:\n        i = 10\n    super(ActorCritic, self).__init__()\n    self.state_size = state_size\n    self.action_size = action_size\n    self.dense1 = layers.Dense(128, activation='relu')\n    self.policy_logits = layers.Dense(action_size)\n    self.dense2 = layers.Dense(128, activation='relu')\n    self.values = layers.Dense(1)",
            "def __init__(self, state_size, action_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(ActorCritic, self).__init__()\n    self.state_size = state_size\n    self.action_size = action_size\n    self.dense1 = layers.Dense(128, activation='relu')\n    self.policy_logits = layers.Dense(action_size)\n    self.dense2 = layers.Dense(128, activation='relu')\n    self.values = layers.Dense(1)",
            "def __init__(self, state_size, action_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(ActorCritic, self).__init__()\n    self.state_size = state_size\n    self.action_size = action_size\n    self.dense1 = layers.Dense(128, activation='relu')\n    self.policy_logits = layers.Dense(action_size)\n    self.dense2 = layers.Dense(128, activation='relu')\n    self.values = layers.Dense(1)",
            "def __init__(self, state_size, action_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(ActorCritic, self).__init__()\n    self.state_size = state_size\n    self.action_size = action_size\n    self.dense1 = layers.Dense(128, activation='relu')\n    self.policy_logits = layers.Dense(action_size)\n    self.dense2 = layers.Dense(128, activation='relu')\n    self.values = layers.Dense(1)",
            "def __init__(self, state_size, action_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(ActorCritic, self).__init__()\n    self.state_size = state_size\n    self.action_size = action_size\n    self.dense1 = layers.Dense(128, activation='relu')\n    self.policy_logits = layers.Dense(action_size)\n    self.dense2 = layers.Dense(128, activation='relu')\n    self.values = layers.Dense(1)"
        ]
    },
    {
        "func_name": "call",
        "original": "def call(self, inputs):\n    x = self.dense1(inputs)\n    logits = self.policy_logits(x)\n    v = self.dense2(inputs)\n    values = self.values(v)\n    return (logits, values)",
        "mutated": [
            "def call(self, inputs):\n    if False:\n        i = 10\n    x = self.dense1(inputs)\n    logits = self.policy_logits(x)\n    v = self.dense2(inputs)\n    values = self.values(v)\n    return (logits, values)",
            "def call(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x = self.dense1(inputs)\n    logits = self.policy_logits(x)\n    v = self.dense2(inputs)\n    values = self.values(v)\n    return (logits, values)",
            "def call(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x = self.dense1(inputs)\n    logits = self.policy_logits(x)\n    v = self.dense2(inputs)\n    values = self.values(v)\n    return (logits, values)",
            "def call(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x = self.dense1(inputs)\n    logits = self.policy_logits(x)\n    v = self.dense2(inputs)\n    values = self.values(v)\n    return (logits, values)",
            "def call(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x = self.dense1(inputs)\n    logits = self.policy_logits(x)\n    v = self.dense2(inputs)\n    values = self.values(v)\n    return (logits, values)"
        ]
    },
    {
        "func_name": "record",
        "original": "def record(episode, episode_reward, worker_idx, global_ep_reward, result_queue, total_loss, num_steps):\n    if global_ep_reward == 0:\n        global_ep_reward = episode_reward\n    else:\n        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n    print(f'{episode} | Average Reward: {int(global_ep_reward)} | Episode Reward: {int(episode_reward)} | Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | Steps: {num_steps} | Worker: {worker_idx}')\n    result_queue.put(global_ep_reward)\n    return global_ep_reward",
        "mutated": [
            "def record(episode, episode_reward, worker_idx, global_ep_reward, result_queue, total_loss, num_steps):\n    if False:\n        i = 10\n    if global_ep_reward == 0:\n        global_ep_reward = episode_reward\n    else:\n        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n    print(f'{episode} | Average Reward: {int(global_ep_reward)} | Episode Reward: {int(episode_reward)} | Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | Steps: {num_steps} | Worker: {worker_idx}')\n    result_queue.put(global_ep_reward)\n    return global_ep_reward",
            "def record(episode, episode_reward, worker_idx, global_ep_reward, result_queue, total_loss, num_steps):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if global_ep_reward == 0:\n        global_ep_reward = episode_reward\n    else:\n        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n    print(f'{episode} | Average Reward: {int(global_ep_reward)} | Episode Reward: {int(episode_reward)} | Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | Steps: {num_steps} | Worker: {worker_idx}')\n    result_queue.put(global_ep_reward)\n    return global_ep_reward",
            "def record(episode, episode_reward, worker_idx, global_ep_reward, result_queue, total_loss, num_steps):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if global_ep_reward == 0:\n        global_ep_reward = episode_reward\n    else:\n        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n    print(f'{episode} | Average Reward: {int(global_ep_reward)} | Episode Reward: {int(episode_reward)} | Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | Steps: {num_steps} | Worker: {worker_idx}')\n    result_queue.put(global_ep_reward)\n    return global_ep_reward",
            "def record(episode, episode_reward, worker_idx, global_ep_reward, result_queue, total_loss, num_steps):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if global_ep_reward == 0:\n        global_ep_reward = episode_reward\n    else:\n        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n    print(f'{episode} | Average Reward: {int(global_ep_reward)} | Episode Reward: {int(episode_reward)} | Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | Steps: {num_steps} | Worker: {worker_idx}')\n    result_queue.put(global_ep_reward)\n    return global_ep_reward",
            "def record(episode, episode_reward, worker_idx, global_ep_reward, result_queue, total_loss, num_steps):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if global_ep_reward == 0:\n        global_ep_reward = episode_reward\n    else:\n        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n    print(f'{episode} | Average Reward: {int(global_ep_reward)} | Episode Reward: {int(episode_reward)} | Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | Steps: {num_steps} | Worker: {worker_idx}')\n    result_queue.put(global_ep_reward)\n    return global_ep_reward"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    self.states = []\n    self.actions = []\n    self.rewards = []",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    self.states = []\n    self.actions = []\n    self.rewards = []",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.states = []\n    self.actions = []\n    self.rewards = []",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.states = []\n    self.actions = []\n    self.rewards = []",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.states = []\n    self.actions = []\n    self.rewards = []",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.states = []\n    self.actions = []\n    self.rewards = []"
        ]
    },
    {
        "func_name": "store",
        "original": "def store(self, state, action, reward):\n    self.states.append(state)\n    self.actions.append(action)\n    self.rewards.append(reward)",
        "mutated": [
            "def store(self, state, action, reward):\n    if False:\n        i = 10\n    self.states.append(state)\n    self.actions.append(action)\n    self.rewards.append(reward)",
            "def store(self, state, action, reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.states.append(state)\n    self.actions.append(action)\n    self.rewards.append(reward)",
            "def store(self, state, action, reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.states.append(state)\n    self.actions.append(action)\n    self.rewards.append(reward)",
            "def store(self, state, action, reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.states.append(state)\n    self.actions.append(action)\n    self.rewards.append(reward)",
            "def store(self, state, action, reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.states.append(state)\n    self.actions.append(action)\n    self.rewards.append(reward)"
        ]
    },
    {
        "func_name": "clear",
        "original": "def clear(self):\n    self.states = []\n    self.actions = []\n    self.rewards = []",
        "mutated": [
            "def clear(self):\n    if False:\n        i = 10\n    self.states = []\n    self.actions = []\n    self.rewards = []",
            "def clear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.states = []\n    self.actions = []\n    self.rewards = []",
            "def clear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.states = []\n    self.actions = []\n    self.rewards = []",
            "def clear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.states = []\n    self.actions = []\n    self.rewards = []",
            "def clear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.states = []\n    self.actions = []\n    self.rewards = []"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    self.opt = optimizers.Adam(0.001)\n    self.server = ActorCritic(4, 2)\n    self.server(tf.random.normal((2, 4)))",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    self.opt = optimizers.Adam(0.001)\n    self.server = ActorCritic(4, 2)\n    self.server(tf.random.normal((2, 4)))",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.opt = optimizers.Adam(0.001)\n    self.server = ActorCritic(4, 2)\n    self.server(tf.random.normal((2, 4)))",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.opt = optimizers.Adam(0.001)\n    self.server = ActorCritic(4, 2)\n    self.server(tf.random.normal((2, 4)))",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.opt = optimizers.Adam(0.001)\n    self.server = ActorCritic(4, 2)\n    self.server(tf.random.normal((2, 4)))",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.opt = optimizers.Adam(0.001)\n    self.server = ActorCritic(4, 2)\n    self.server(tf.random.normal((2, 4)))"
        ]
    },
    {
        "func_name": "train",
        "original": "def train(self):\n    res_queue = Queue()\n    workers = [Worker(self.server, self.opt, res_queue, i) for i in range(multiprocessing.cpu_count())]\n    for (i, worker) in enumerate(workers):\n        print('Starting worker {}'.format(i))\n        worker.start()\n    returns = []\n    while True:\n        reward = res_queue.get()\n        if reward is not None:\n            returns.append(reward)\n        else:\n            break\n    [w.join() for w in workers]\n    print(returns)\n    plt.figure()\n    plt.plot(np.arange(len(returns)), returns)\n    plt.xlabel('\u56de\u5408\u6570')\n    plt.ylabel('\u603b\u56de\u62a5')\n    plt.savefig('a3c-tf-cartpole.svg')",
        "mutated": [
            "def train(self):\n    if False:\n        i = 10\n    res_queue = Queue()\n    workers = [Worker(self.server, self.opt, res_queue, i) for i in range(multiprocessing.cpu_count())]\n    for (i, worker) in enumerate(workers):\n        print('Starting worker {}'.format(i))\n        worker.start()\n    returns = []\n    while True:\n        reward = res_queue.get()\n        if reward is not None:\n            returns.append(reward)\n        else:\n            break\n    [w.join() for w in workers]\n    print(returns)\n    plt.figure()\n    plt.plot(np.arange(len(returns)), returns)\n    plt.xlabel('\u56de\u5408\u6570')\n    plt.ylabel('\u603b\u56de\u62a5')\n    plt.savefig('a3c-tf-cartpole.svg')",
            "def train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    res_queue = Queue()\n    workers = [Worker(self.server, self.opt, res_queue, i) for i in range(multiprocessing.cpu_count())]\n    for (i, worker) in enumerate(workers):\n        print('Starting worker {}'.format(i))\n        worker.start()\n    returns = []\n    while True:\n        reward = res_queue.get()\n        if reward is not None:\n            returns.append(reward)\n        else:\n            break\n    [w.join() for w in workers]\n    print(returns)\n    plt.figure()\n    plt.plot(np.arange(len(returns)), returns)\n    plt.xlabel('\u56de\u5408\u6570')\n    plt.ylabel('\u603b\u56de\u62a5')\n    plt.savefig('a3c-tf-cartpole.svg')",
            "def train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    res_queue = Queue()\n    workers = [Worker(self.server, self.opt, res_queue, i) for i in range(multiprocessing.cpu_count())]\n    for (i, worker) in enumerate(workers):\n        print('Starting worker {}'.format(i))\n        worker.start()\n    returns = []\n    while True:\n        reward = res_queue.get()\n        if reward is not None:\n            returns.append(reward)\n        else:\n            break\n    [w.join() for w in workers]\n    print(returns)\n    plt.figure()\n    plt.plot(np.arange(len(returns)), returns)\n    plt.xlabel('\u56de\u5408\u6570')\n    plt.ylabel('\u603b\u56de\u62a5')\n    plt.savefig('a3c-tf-cartpole.svg')",
            "def train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    res_queue = Queue()\n    workers = [Worker(self.server, self.opt, res_queue, i) for i in range(multiprocessing.cpu_count())]\n    for (i, worker) in enumerate(workers):\n        print('Starting worker {}'.format(i))\n        worker.start()\n    returns = []\n    while True:\n        reward = res_queue.get()\n        if reward is not None:\n            returns.append(reward)\n        else:\n            break\n    [w.join() for w in workers]\n    print(returns)\n    plt.figure()\n    plt.plot(np.arange(len(returns)), returns)\n    plt.xlabel('\u56de\u5408\u6570')\n    plt.ylabel('\u603b\u56de\u62a5')\n    plt.savefig('a3c-tf-cartpole.svg')",
            "def train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    res_queue = Queue()\n    workers = [Worker(self.server, self.opt, res_queue, i) for i in range(multiprocessing.cpu_count())]\n    for (i, worker) in enumerate(workers):\n        print('Starting worker {}'.format(i))\n        worker.start()\n    returns = []\n    while True:\n        reward = res_queue.get()\n        if reward is not None:\n            returns.append(reward)\n        else:\n            break\n    [w.join() for w in workers]\n    print(returns)\n    plt.figure()\n    plt.plot(np.arange(len(returns)), returns)\n    plt.xlabel('\u56de\u5408\u6570')\n    plt.ylabel('\u603b\u56de\u62a5')\n    plt.savefig('a3c-tf-cartpole.svg')"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, server, opt, result_queue, idx):\n    super(Worker, self).__init__()\n    self.result_queue = result_queue\n    self.server = server\n    self.opt = opt\n    self.client = ActorCritic(4, 2)\n    self.worker_idx = idx\n    self.env = gym.make('CartPole-v1').unwrapped\n    self.ep_loss = 0.0",
        "mutated": [
            "def __init__(self, server, opt, result_queue, idx):\n    if False:\n        i = 10\n    super(Worker, self).__init__()\n    self.result_queue = result_queue\n    self.server = server\n    self.opt = opt\n    self.client = ActorCritic(4, 2)\n    self.worker_idx = idx\n    self.env = gym.make('CartPole-v1').unwrapped\n    self.ep_loss = 0.0",
            "def __init__(self, server, opt, result_queue, idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(Worker, self).__init__()\n    self.result_queue = result_queue\n    self.server = server\n    self.opt = opt\n    self.client = ActorCritic(4, 2)\n    self.worker_idx = idx\n    self.env = gym.make('CartPole-v1').unwrapped\n    self.ep_loss = 0.0",
            "def __init__(self, server, opt, result_queue, idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(Worker, self).__init__()\n    self.result_queue = result_queue\n    self.server = server\n    self.opt = opt\n    self.client = ActorCritic(4, 2)\n    self.worker_idx = idx\n    self.env = gym.make('CartPole-v1').unwrapped\n    self.ep_loss = 0.0",
            "def __init__(self, server, opt, result_queue, idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(Worker, self).__init__()\n    self.result_queue = result_queue\n    self.server = server\n    self.opt = opt\n    self.client = ActorCritic(4, 2)\n    self.worker_idx = idx\n    self.env = gym.make('CartPole-v1').unwrapped\n    self.ep_loss = 0.0",
            "def __init__(self, server, opt, result_queue, idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(Worker, self).__init__()\n    self.result_queue = result_queue\n    self.server = server\n    self.opt = opt\n    self.client = ActorCritic(4, 2)\n    self.worker_idx = idx\n    self.env = gym.make('CartPole-v1').unwrapped\n    self.ep_loss = 0.0"
        ]
    },
    {
        "func_name": "run",
        "original": "def run(self):\n    mem = Memory()\n    for epi_counter in range(500):\n        current_state = self.env.reset()\n        mem.clear()\n        ep_reward = 0.0\n        ep_steps = 0\n        done = False\n        while not done:\n            (logits, _) = self.client(tf.constant(current_state[None, :], dtype=tf.float32))\n            probs = tf.nn.softmax(logits)\n            action = np.random.choice(2, p=probs.numpy()[0])\n            (new_state, reward, done, _) = self.env.step(action)\n            ep_reward += reward\n            mem.store(current_state, action, reward)\n            ep_steps += 1\n            current_state = new_state\n            if ep_steps >= 500 or done:\n                with tf.GradientTape() as tape:\n                    total_loss = self.compute_loss(done, new_state, mem)\n                grads = tape.gradient(total_loss, self.client.trainable_weights)\n                self.opt.apply_gradients(zip(grads, self.server.trainable_weights))\n                self.client.set_weights(self.server.get_weights())\n                mem.clear()\n                self.result_queue.put(ep_reward)\n                print(self.worker_idx, ep_reward)\n                break\n    self.result_queue.put(None)",
        "mutated": [
            "def run(self):\n    if False:\n        i = 10\n    mem = Memory()\n    for epi_counter in range(500):\n        current_state = self.env.reset()\n        mem.clear()\n        ep_reward = 0.0\n        ep_steps = 0\n        done = False\n        while not done:\n            (logits, _) = self.client(tf.constant(current_state[None, :], dtype=tf.float32))\n            probs = tf.nn.softmax(logits)\n            action = np.random.choice(2, p=probs.numpy()[0])\n            (new_state, reward, done, _) = self.env.step(action)\n            ep_reward += reward\n            mem.store(current_state, action, reward)\n            ep_steps += 1\n            current_state = new_state\n            if ep_steps >= 500 or done:\n                with tf.GradientTape() as tape:\n                    total_loss = self.compute_loss(done, new_state, mem)\n                grads = tape.gradient(total_loss, self.client.trainable_weights)\n                self.opt.apply_gradients(zip(grads, self.server.trainable_weights))\n                self.client.set_weights(self.server.get_weights())\n                mem.clear()\n                self.result_queue.put(ep_reward)\n                print(self.worker_idx, ep_reward)\n                break\n    self.result_queue.put(None)",
            "def run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mem = Memory()\n    for epi_counter in range(500):\n        current_state = self.env.reset()\n        mem.clear()\n        ep_reward = 0.0\n        ep_steps = 0\n        done = False\n        while not done:\n            (logits, _) = self.client(tf.constant(current_state[None, :], dtype=tf.float32))\n            probs = tf.nn.softmax(logits)\n            action = np.random.choice(2, p=probs.numpy()[0])\n            (new_state, reward, done, _) = self.env.step(action)\n            ep_reward += reward\n            mem.store(current_state, action, reward)\n            ep_steps += 1\n            current_state = new_state\n            if ep_steps >= 500 or done:\n                with tf.GradientTape() as tape:\n                    total_loss = self.compute_loss(done, new_state, mem)\n                grads = tape.gradient(total_loss, self.client.trainable_weights)\n                self.opt.apply_gradients(zip(grads, self.server.trainable_weights))\n                self.client.set_weights(self.server.get_weights())\n                mem.clear()\n                self.result_queue.put(ep_reward)\n                print(self.worker_idx, ep_reward)\n                break\n    self.result_queue.put(None)",
            "def run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mem = Memory()\n    for epi_counter in range(500):\n        current_state = self.env.reset()\n        mem.clear()\n        ep_reward = 0.0\n        ep_steps = 0\n        done = False\n        while not done:\n            (logits, _) = self.client(tf.constant(current_state[None, :], dtype=tf.float32))\n            probs = tf.nn.softmax(logits)\n            action = np.random.choice(2, p=probs.numpy()[0])\n            (new_state, reward, done, _) = self.env.step(action)\n            ep_reward += reward\n            mem.store(current_state, action, reward)\n            ep_steps += 1\n            current_state = new_state\n            if ep_steps >= 500 or done:\n                with tf.GradientTape() as tape:\n                    total_loss = self.compute_loss(done, new_state, mem)\n                grads = tape.gradient(total_loss, self.client.trainable_weights)\n                self.opt.apply_gradients(zip(grads, self.server.trainable_weights))\n                self.client.set_weights(self.server.get_weights())\n                mem.clear()\n                self.result_queue.put(ep_reward)\n                print(self.worker_idx, ep_reward)\n                break\n    self.result_queue.put(None)",
            "def run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mem = Memory()\n    for epi_counter in range(500):\n        current_state = self.env.reset()\n        mem.clear()\n        ep_reward = 0.0\n        ep_steps = 0\n        done = False\n        while not done:\n            (logits, _) = self.client(tf.constant(current_state[None, :], dtype=tf.float32))\n            probs = tf.nn.softmax(logits)\n            action = np.random.choice(2, p=probs.numpy()[0])\n            (new_state, reward, done, _) = self.env.step(action)\n            ep_reward += reward\n            mem.store(current_state, action, reward)\n            ep_steps += 1\n            current_state = new_state\n            if ep_steps >= 500 or done:\n                with tf.GradientTape() as tape:\n                    total_loss = self.compute_loss(done, new_state, mem)\n                grads = tape.gradient(total_loss, self.client.trainable_weights)\n                self.opt.apply_gradients(zip(grads, self.server.trainable_weights))\n                self.client.set_weights(self.server.get_weights())\n                mem.clear()\n                self.result_queue.put(ep_reward)\n                print(self.worker_idx, ep_reward)\n                break\n    self.result_queue.put(None)",
            "def run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mem = Memory()\n    for epi_counter in range(500):\n        current_state = self.env.reset()\n        mem.clear()\n        ep_reward = 0.0\n        ep_steps = 0\n        done = False\n        while not done:\n            (logits, _) = self.client(tf.constant(current_state[None, :], dtype=tf.float32))\n            probs = tf.nn.softmax(logits)\n            action = np.random.choice(2, p=probs.numpy()[0])\n            (new_state, reward, done, _) = self.env.step(action)\n            ep_reward += reward\n            mem.store(current_state, action, reward)\n            ep_steps += 1\n            current_state = new_state\n            if ep_steps >= 500 or done:\n                with tf.GradientTape() as tape:\n                    total_loss = self.compute_loss(done, new_state, mem)\n                grads = tape.gradient(total_loss, self.client.trainable_weights)\n                self.opt.apply_gradients(zip(grads, self.server.trainable_weights))\n                self.client.set_weights(self.server.get_weights())\n                mem.clear()\n                self.result_queue.put(ep_reward)\n                print(self.worker_idx, ep_reward)\n                break\n    self.result_queue.put(None)"
        ]
    },
    {
        "func_name": "compute_loss",
        "original": "def compute_loss(self, done, new_state, memory, gamma=0.99):\n    if done:\n        reward_sum = 0.0\n    else:\n        reward_sum = self.client(tf.constant(new_state[None, :], dtype=tf.float32))[-1].numpy()[0]\n    discounted_rewards = []\n    for reward in memory.rewards[::-1]:\n        reward_sum = reward + gamma * reward_sum\n        discounted_rewards.append(reward_sum)\n    discounted_rewards.reverse()\n    (logits, values) = self.client(tf.constant(np.vstack(memory.states), dtype=tf.float32))\n    advantage = tf.constant(np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n    value_loss = advantage ** 2\n    policy = tf.nn.softmax(logits)\n    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=memory.actions, logits=logits)\n    policy_loss = policy_loss * tf.stop_gradient(advantage)\n    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy, logits=logits)\n    policy_loss = policy_loss - 0.01 * entropy\n    total_loss = tf.reduce_mean(0.5 * value_loss + policy_loss)\n    return total_loss",
        "mutated": [
            "def compute_loss(self, done, new_state, memory, gamma=0.99):\n    if False:\n        i = 10\n    if done:\n        reward_sum = 0.0\n    else:\n        reward_sum = self.client(tf.constant(new_state[None, :], dtype=tf.float32))[-1].numpy()[0]\n    discounted_rewards = []\n    for reward in memory.rewards[::-1]:\n        reward_sum = reward + gamma * reward_sum\n        discounted_rewards.append(reward_sum)\n    discounted_rewards.reverse()\n    (logits, values) = self.client(tf.constant(np.vstack(memory.states), dtype=tf.float32))\n    advantage = tf.constant(np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n    value_loss = advantage ** 2\n    policy = tf.nn.softmax(logits)\n    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=memory.actions, logits=logits)\n    policy_loss = policy_loss * tf.stop_gradient(advantage)\n    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy, logits=logits)\n    policy_loss = policy_loss - 0.01 * entropy\n    total_loss = tf.reduce_mean(0.5 * value_loss + policy_loss)\n    return total_loss",
            "def compute_loss(self, done, new_state, memory, gamma=0.99):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if done:\n        reward_sum = 0.0\n    else:\n        reward_sum = self.client(tf.constant(new_state[None, :], dtype=tf.float32))[-1].numpy()[0]\n    discounted_rewards = []\n    for reward in memory.rewards[::-1]:\n        reward_sum = reward + gamma * reward_sum\n        discounted_rewards.append(reward_sum)\n    discounted_rewards.reverse()\n    (logits, values) = self.client(tf.constant(np.vstack(memory.states), dtype=tf.float32))\n    advantage = tf.constant(np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n    value_loss = advantage ** 2\n    policy = tf.nn.softmax(logits)\n    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=memory.actions, logits=logits)\n    policy_loss = policy_loss * tf.stop_gradient(advantage)\n    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy, logits=logits)\n    policy_loss = policy_loss - 0.01 * entropy\n    total_loss = tf.reduce_mean(0.5 * value_loss + policy_loss)\n    return total_loss",
            "def compute_loss(self, done, new_state, memory, gamma=0.99):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if done:\n        reward_sum = 0.0\n    else:\n        reward_sum = self.client(tf.constant(new_state[None, :], dtype=tf.float32))[-1].numpy()[0]\n    discounted_rewards = []\n    for reward in memory.rewards[::-1]:\n        reward_sum = reward + gamma * reward_sum\n        discounted_rewards.append(reward_sum)\n    discounted_rewards.reverse()\n    (logits, values) = self.client(tf.constant(np.vstack(memory.states), dtype=tf.float32))\n    advantage = tf.constant(np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n    value_loss = advantage ** 2\n    policy = tf.nn.softmax(logits)\n    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=memory.actions, logits=logits)\n    policy_loss = policy_loss * tf.stop_gradient(advantage)\n    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy, logits=logits)\n    policy_loss = policy_loss - 0.01 * entropy\n    total_loss = tf.reduce_mean(0.5 * value_loss + policy_loss)\n    return total_loss",
            "def compute_loss(self, done, new_state, memory, gamma=0.99):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if done:\n        reward_sum = 0.0\n    else:\n        reward_sum = self.client(tf.constant(new_state[None, :], dtype=tf.float32))[-1].numpy()[0]\n    discounted_rewards = []\n    for reward in memory.rewards[::-1]:\n        reward_sum = reward + gamma * reward_sum\n        discounted_rewards.append(reward_sum)\n    discounted_rewards.reverse()\n    (logits, values) = self.client(tf.constant(np.vstack(memory.states), dtype=tf.float32))\n    advantage = tf.constant(np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n    value_loss = advantage ** 2\n    policy = tf.nn.softmax(logits)\n    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=memory.actions, logits=logits)\n    policy_loss = policy_loss * tf.stop_gradient(advantage)\n    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy, logits=logits)\n    policy_loss = policy_loss - 0.01 * entropy\n    total_loss = tf.reduce_mean(0.5 * value_loss + policy_loss)\n    return total_loss",
            "def compute_loss(self, done, new_state, memory, gamma=0.99):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if done:\n        reward_sum = 0.0\n    else:\n        reward_sum = self.client(tf.constant(new_state[None, :], dtype=tf.float32))[-1].numpy()[0]\n    discounted_rewards = []\n    for reward in memory.rewards[::-1]:\n        reward_sum = reward + gamma * reward_sum\n        discounted_rewards.append(reward_sum)\n    discounted_rewards.reverse()\n    (logits, values) = self.client(tf.constant(np.vstack(memory.states), dtype=tf.float32))\n    advantage = tf.constant(np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n    value_loss = advantage ** 2\n    policy = tf.nn.softmax(logits)\n    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=memory.actions, logits=logits)\n    policy_loss = policy_loss * tf.stop_gradient(advantage)\n    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy, logits=logits)\n    policy_loss = policy_loss - 0.01 * entropy\n    total_loss = tf.reduce_mean(0.5 * value_loss + policy_loss)\n    return total_loss"
        ]
    }
]