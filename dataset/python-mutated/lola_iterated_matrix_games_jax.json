[
    {
        "func_name": "get_action_probs",
        "original": "def get_action_probs(agent: OpponentShapingAgent, game: str) -> List[typing.Dict[str, typing.Any]]:\n    \"\"\"Returns the probability of cooperation and a string repr for each state.\n  \n  Args:\n      agent: The agent.\n      game: The name of the game.\n\n  Returns:\n      A list of dictionaries, each containing the probability of cooperation\n      and a string representation\n  \"\"\"\n    actions = ['C', 'D'] if game == 'ipd' else ['H', 'T']\n    states = ['s0'] + [''.join(s) for s in itertools.product(actions, repeat=2)]\n    params = agent.train_state.policy_params[agent.player_id]\n    action_probs = []\n    for (i, state_str) in enumerate(states):\n        state = np.eye(len(states))[i]\n        prob = agent.policy_network.apply(params, state).prob(0)\n        action = actions[0]\n        action_probs.append({'prob': prob.item(), 'name': f'P({action}|{state_str})'})\n    return action_probs",
        "mutated": [
            "def get_action_probs(agent: OpponentShapingAgent, game: str) -> List[typing.Dict[str, typing.Any]]:\n    if False:\n        i = 10\n    'Returns the probability of cooperation and a string repr for each state.\\n  \\n  Args:\\n      agent: The agent.\\n      game: The name of the game.\\n\\n  Returns:\\n      A list of dictionaries, each containing the probability of cooperation\\n      and a string representation\\n  '\n    actions = ['C', 'D'] if game == 'ipd' else ['H', 'T']\n    states = ['s0'] + [''.join(s) for s in itertools.product(actions, repeat=2)]\n    params = agent.train_state.policy_params[agent.player_id]\n    action_probs = []\n    for (i, state_str) in enumerate(states):\n        state = np.eye(len(states))[i]\n        prob = agent.policy_network.apply(params, state).prob(0)\n        action = actions[0]\n        action_probs.append({'prob': prob.item(), 'name': f'P({action}|{state_str})'})\n    return action_probs",
            "def get_action_probs(agent: OpponentShapingAgent, game: str) -> List[typing.Dict[str, typing.Any]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the probability of cooperation and a string repr for each state.\\n  \\n  Args:\\n      agent: The agent.\\n      game: The name of the game.\\n\\n  Returns:\\n      A list of dictionaries, each containing the probability of cooperation\\n      and a string representation\\n  '\n    actions = ['C', 'D'] if game == 'ipd' else ['H', 'T']\n    states = ['s0'] + [''.join(s) for s in itertools.product(actions, repeat=2)]\n    params = agent.train_state.policy_params[agent.player_id]\n    action_probs = []\n    for (i, state_str) in enumerate(states):\n        state = np.eye(len(states))[i]\n        prob = agent.policy_network.apply(params, state).prob(0)\n        action = actions[0]\n        action_probs.append({'prob': prob.item(), 'name': f'P({action}|{state_str})'})\n    return action_probs",
            "def get_action_probs(agent: OpponentShapingAgent, game: str) -> List[typing.Dict[str, typing.Any]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the probability of cooperation and a string repr for each state.\\n  \\n  Args:\\n      agent: The agent.\\n      game: The name of the game.\\n\\n  Returns:\\n      A list of dictionaries, each containing the probability of cooperation\\n      and a string representation\\n  '\n    actions = ['C', 'D'] if game == 'ipd' else ['H', 'T']\n    states = ['s0'] + [''.join(s) for s in itertools.product(actions, repeat=2)]\n    params = agent.train_state.policy_params[agent.player_id]\n    action_probs = []\n    for (i, state_str) in enumerate(states):\n        state = np.eye(len(states))[i]\n        prob = agent.policy_network.apply(params, state).prob(0)\n        action = actions[0]\n        action_probs.append({'prob': prob.item(), 'name': f'P({action}|{state_str})'})\n    return action_probs",
            "def get_action_probs(agent: OpponentShapingAgent, game: str) -> List[typing.Dict[str, typing.Any]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the probability of cooperation and a string repr for each state.\\n  \\n  Args:\\n      agent: The agent.\\n      game: The name of the game.\\n\\n  Returns:\\n      A list of dictionaries, each containing the probability of cooperation\\n      and a string representation\\n  '\n    actions = ['C', 'D'] if game == 'ipd' else ['H', 'T']\n    states = ['s0'] + [''.join(s) for s in itertools.product(actions, repeat=2)]\n    params = agent.train_state.policy_params[agent.player_id]\n    action_probs = []\n    for (i, state_str) in enumerate(states):\n        state = np.eye(len(states))[i]\n        prob = agent.policy_network.apply(params, state).prob(0)\n        action = actions[0]\n        action_probs.append({'prob': prob.item(), 'name': f'P({action}|{state_str})'})\n    return action_probs",
            "def get_action_probs(agent: OpponentShapingAgent, game: str) -> List[typing.Dict[str, typing.Any]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the probability of cooperation and a string repr for each state.\\n  \\n  Args:\\n      agent: The agent.\\n      game: The name of the game.\\n\\n  Returns:\\n      A list of dictionaries, each containing the probability of cooperation\\n      and a string representation\\n  '\n    actions = ['C', 'D'] if game == 'ipd' else ['H', 'T']\n    states = ['s0'] + [''.join(s) for s in itertools.product(actions, repeat=2)]\n    params = agent.train_state.policy_params[agent.player_id]\n    action_probs = []\n    for (i, state_str) in enumerate(states):\n        state = np.eye(len(states))[i]\n        prob = agent.policy_network.apply(params, state).prob(0)\n        action = actions[0]\n        action_probs.append({'prob': prob.item(), 'name': f'P({action}|{state_str})'})\n    return action_probs"
        ]
    },
    {
        "func_name": "log_epoch_data",
        "original": "def log_epoch_data(epoch: int, agents: List[OpponentShapingAgent], eval_batch):\n    \"\"\"Logs data to wandb and prints it to the console.\n\n  Args:\n      epoch: The current epoch.\n      agents: A list of agents.\n      eval_batch: A batch of episodes.\n  \"\"\"\n    logs = {}\n    for agent in agents:\n        avg_step_reward = np.mean([ts.rewards[agent.player_id] for ts in eval_batch])\n        probs = get_action_probs(agent, game=FLAGS.game)\n        for info in probs:\n            logs[f\"agent_{agent.player_id}/{info['name']}\"] = info['prob']\n        probs = ', '.join([f\"{info['name']}: {info['prob']:.2f}\" for info in probs])\n        metrics = agent.metrics()\n        logs.update({f'agent_{agent.player_id}/avg_step_reward': avg_step_reward, **{f'agent_{agent.player_id}/{k}': v.item() for (k, v) in metrics.items()}})\n        print(f'[epoch {epoch}] Agent {agent.player_id}: {avg_step_reward:.2f} | {probs}')\n    wandb.log(logs)",
        "mutated": [
            "def log_epoch_data(epoch: int, agents: List[OpponentShapingAgent], eval_batch):\n    if False:\n        i = 10\n    'Logs data to wandb and prints it to the console.\\n\\n  Args:\\n      epoch: The current epoch.\\n      agents: A list of agents.\\n      eval_batch: A batch of episodes.\\n  '\n    logs = {}\n    for agent in agents:\n        avg_step_reward = np.mean([ts.rewards[agent.player_id] for ts in eval_batch])\n        probs = get_action_probs(agent, game=FLAGS.game)\n        for info in probs:\n            logs[f\"agent_{agent.player_id}/{info['name']}\"] = info['prob']\n        probs = ', '.join([f\"{info['name']}: {info['prob']:.2f}\" for info in probs])\n        metrics = agent.metrics()\n        logs.update({f'agent_{agent.player_id}/avg_step_reward': avg_step_reward, **{f'agent_{agent.player_id}/{k}': v.item() for (k, v) in metrics.items()}})\n        print(f'[epoch {epoch}] Agent {agent.player_id}: {avg_step_reward:.2f} | {probs}')\n    wandb.log(logs)",
            "def log_epoch_data(epoch: int, agents: List[OpponentShapingAgent], eval_batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Logs data to wandb and prints it to the console.\\n\\n  Args:\\n      epoch: The current epoch.\\n      agents: A list of agents.\\n      eval_batch: A batch of episodes.\\n  '\n    logs = {}\n    for agent in agents:\n        avg_step_reward = np.mean([ts.rewards[agent.player_id] for ts in eval_batch])\n        probs = get_action_probs(agent, game=FLAGS.game)\n        for info in probs:\n            logs[f\"agent_{agent.player_id}/{info['name']}\"] = info['prob']\n        probs = ', '.join([f\"{info['name']}: {info['prob']:.2f}\" for info in probs])\n        metrics = agent.metrics()\n        logs.update({f'agent_{agent.player_id}/avg_step_reward': avg_step_reward, **{f'agent_{agent.player_id}/{k}': v.item() for (k, v) in metrics.items()}})\n        print(f'[epoch {epoch}] Agent {agent.player_id}: {avg_step_reward:.2f} | {probs}')\n    wandb.log(logs)",
            "def log_epoch_data(epoch: int, agents: List[OpponentShapingAgent], eval_batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Logs data to wandb and prints it to the console.\\n\\n  Args:\\n      epoch: The current epoch.\\n      agents: A list of agents.\\n      eval_batch: A batch of episodes.\\n  '\n    logs = {}\n    for agent in agents:\n        avg_step_reward = np.mean([ts.rewards[agent.player_id] for ts in eval_batch])\n        probs = get_action_probs(agent, game=FLAGS.game)\n        for info in probs:\n            logs[f\"agent_{agent.player_id}/{info['name']}\"] = info['prob']\n        probs = ', '.join([f\"{info['name']}: {info['prob']:.2f}\" for info in probs])\n        metrics = agent.metrics()\n        logs.update({f'agent_{agent.player_id}/avg_step_reward': avg_step_reward, **{f'agent_{agent.player_id}/{k}': v.item() for (k, v) in metrics.items()}})\n        print(f'[epoch {epoch}] Agent {agent.player_id}: {avg_step_reward:.2f} | {probs}')\n    wandb.log(logs)",
            "def log_epoch_data(epoch: int, agents: List[OpponentShapingAgent], eval_batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Logs data to wandb and prints it to the console.\\n\\n  Args:\\n      epoch: The current epoch.\\n      agents: A list of agents.\\n      eval_batch: A batch of episodes.\\n  '\n    logs = {}\n    for agent in agents:\n        avg_step_reward = np.mean([ts.rewards[agent.player_id] for ts in eval_batch])\n        probs = get_action_probs(agent, game=FLAGS.game)\n        for info in probs:\n            logs[f\"agent_{agent.player_id}/{info['name']}\"] = info['prob']\n        probs = ', '.join([f\"{info['name']}: {info['prob']:.2f}\" for info in probs])\n        metrics = agent.metrics()\n        logs.update({f'agent_{agent.player_id}/avg_step_reward': avg_step_reward, **{f'agent_{agent.player_id}/{k}': v.item() for (k, v) in metrics.items()}})\n        print(f'[epoch {epoch}] Agent {agent.player_id}: {avg_step_reward:.2f} | {probs}')\n    wandb.log(logs)",
            "def log_epoch_data(epoch: int, agents: List[OpponentShapingAgent], eval_batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Logs data to wandb and prints it to the console.\\n\\n  Args:\\n      epoch: The current epoch.\\n      agents: A list of agents.\\n      eval_batch: A batch of episodes.\\n  '\n    logs = {}\n    for agent in agents:\n        avg_step_reward = np.mean([ts.rewards[agent.player_id] for ts in eval_batch])\n        probs = get_action_probs(agent, game=FLAGS.game)\n        for info in probs:\n            logs[f\"agent_{agent.player_id}/{info['name']}\"] = info['prob']\n        probs = ', '.join([f\"{info['name']}: {info['prob']:.2f}\" for info in probs])\n        metrics = agent.metrics()\n        logs.update({f'agent_{agent.player_id}/avg_step_reward': avg_step_reward, **{f'agent_{agent.player_id}/{k}': v.item() for (k, v) in metrics.items()}})\n        print(f'[epoch {epoch}] Agent {agent.player_id}: {avg_step_reward:.2f} | {probs}')\n    wandb.log(logs)"
        ]
    },
    {
        "func_name": "collect_batch",
        "original": "def collect_batch(env: Environment, agents: List[OpponentShapingAgent], eval_mode: bool) -> List[TimeStep]:\n    \"\"\"Collects one episode.\n\n  Args:\n      env: The environment.\n      agents: A list of opponent shaping agents.\n      eval_mode: If true, the agents will be run in evaluation mode.\n\n  Returns:\n      A list of time steps.\n  \"\"\"\n    episode = []\n    time_step = env.reset()\n    episode.append(time_step)\n    while not time_step.last():\n        actions = []\n        for agent in agents:\n            (action, _) = agent.step(time_step, is_evaluation=eval_mode)\n            if action is not None:\n                action = action.squeeze()\n            actions.append(action)\n        time_step = env.step(np.stack(actions, axis=1))\n        time_step.observations['actions'] = actions\n        episode.append(time_step)\n    for agent in agents:\n        agent.step(time_step, is_evaluation=eval_mode)\n    return episode",
        "mutated": [
            "def collect_batch(env: Environment, agents: List[OpponentShapingAgent], eval_mode: bool) -> List[TimeStep]:\n    if False:\n        i = 10\n    'Collects one episode.\\n\\n  Args:\\n      env: The environment.\\n      agents: A list of opponent shaping agents.\\n      eval_mode: If true, the agents will be run in evaluation mode.\\n\\n  Returns:\\n      A list of time steps.\\n  '\n    episode = []\n    time_step = env.reset()\n    episode.append(time_step)\n    while not time_step.last():\n        actions = []\n        for agent in agents:\n            (action, _) = agent.step(time_step, is_evaluation=eval_mode)\n            if action is not None:\n                action = action.squeeze()\n            actions.append(action)\n        time_step = env.step(np.stack(actions, axis=1))\n        time_step.observations['actions'] = actions\n        episode.append(time_step)\n    for agent in agents:\n        agent.step(time_step, is_evaluation=eval_mode)\n    return episode",
            "def collect_batch(env: Environment, agents: List[OpponentShapingAgent], eval_mode: bool) -> List[TimeStep]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Collects one episode.\\n\\n  Args:\\n      env: The environment.\\n      agents: A list of opponent shaping agents.\\n      eval_mode: If true, the agents will be run in evaluation mode.\\n\\n  Returns:\\n      A list of time steps.\\n  '\n    episode = []\n    time_step = env.reset()\n    episode.append(time_step)\n    while not time_step.last():\n        actions = []\n        for agent in agents:\n            (action, _) = agent.step(time_step, is_evaluation=eval_mode)\n            if action is not None:\n                action = action.squeeze()\n            actions.append(action)\n        time_step = env.step(np.stack(actions, axis=1))\n        time_step.observations['actions'] = actions\n        episode.append(time_step)\n    for agent in agents:\n        agent.step(time_step, is_evaluation=eval_mode)\n    return episode",
            "def collect_batch(env: Environment, agents: List[OpponentShapingAgent], eval_mode: bool) -> List[TimeStep]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Collects one episode.\\n\\n  Args:\\n      env: The environment.\\n      agents: A list of opponent shaping agents.\\n      eval_mode: If true, the agents will be run in evaluation mode.\\n\\n  Returns:\\n      A list of time steps.\\n  '\n    episode = []\n    time_step = env.reset()\n    episode.append(time_step)\n    while not time_step.last():\n        actions = []\n        for agent in agents:\n            (action, _) = agent.step(time_step, is_evaluation=eval_mode)\n            if action is not None:\n                action = action.squeeze()\n            actions.append(action)\n        time_step = env.step(np.stack(actions, axis=1))\n        time_step.observations['actions'] = actions\n        episode.append(time_step)\n    for agent in agents:\n        agent.step(time_step, is_evaluation=eval_mode)\n    return episode",
            "def collect_batch(env: Environment, agents: List[OpponentShapingAgent], eval_mode: bool) -> List[TimeStep]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Collects one episode.\\n\\n  Args:\\n      env: The environment.\\n      agents: A list of opponent shaping agents.\\n      eval_mode: If true, the agents will be run in evaluation mode.\\n\\n  Returns:\\n      A list of time steps.\\n  '\n    episode = []\n    time_step = env.reset()\n    episode.append(time_step)\n    while not time_step.last():\n        actions = []\n        for agent in agents:\n            (action, _) = agent.step(time_step, is_evaluation=eval_mode)\n            if action is not None:\n                action = action.squeeze()\n            actions.append(action)\n        time_step = env.step(np.stack(actions, axis=1))\n        time_step.observations['actions'] = actions\n        episode.append(time_step)\n    for agent in agents:\n        agent.step(time_step, is_evaluation=eval_mode)\n    return episode",
            "def collect_batch(env: Environment, agents: List[OpponentShapingAgent], eval_mode: bool) -> List[TimeStep]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Collects one episode.\\n\\n  Args:\\n      env: The environment.\\n      agents: A list of opponent shaping agents.\\n      eval_mode: If true, the agents will be run in evaluation mode.\\n\\n  Returns:\\n      A list of time steps.\\n  '\n    episode = []\n    time_step = env.reset()\n    episode.append(time_step)\n    while not time_step.last():\n        actions = []\n        for agent in agents:\n            (action, _) = agent.step(time_step, is_evaluation=eval_mode)\n            if action is not None:\n                action = action.squeeze()\n            actions.append(action)\n        time_step = env.step(np.stack(actions, axis=1))\n        time_step.observations['actions'] = actions\n        episode.append(time_step)\n    for agent in agents:\n        agent.step(time_step, is_evaluation=eval_mode)\n    return episode"
        ]
    },
    {
        "func_name": "make_agent",
        "original": "def make_agent(key: jax.random.PRNGKey, player_id: int, env: Environment, networks: Tuple[hk.Transformed, hk.Transformed]) -> OpponentShapingAgent:\n    \"\"\"Creates an opponent shaping agent.\n\n  Args:\n      key: A random seed key.\n      player_id: The id of the player.\n      env: The environment.\n      networks: A tuple of policy and critic networks transformed by\n        hk.transform.\n\n  Returns:\n      An opponent shaping agent instance.\n  \"\"\"\n    (policy_network, critic_network) = networks\n    return OpponentShapingAgent(player_id=player_id, opponent_ids=[1 - player_id], seed=key, info_state_size=env.observation_spec()['info_state'][player_id], num_actions=env.action_spec()['num_actions'][player_id], policy=policy_network, critic=critic_network, batch_size=FLAGS.batch_size, num_critic_mini_batches=FLAGS.critic_mini_batches, pi_learning_rate=FLAGS.policy_lr, opp_policy_learning_rate=FLAGS.opp_policy_lr, num_opponent_updates=FLAGS.opp_policy_mini_batches, critic_learning_rate=FLAGS.critic_lr, opponent_model_learning_rate=FLAGS.opponent_model_learning_rate, policy_update_interval=FLAGS.policy_update_interval, discount=FLAGS.discount, critic_discount=0, correction_type=FLAGS.correction_type, clip_grad_norm=FLAGS.correction_max_grad_norm, use_jit=FLAGS.use_jit, n_lookaheads=FLAGS.n_lookaheads, env=env)",
        "mutated": [
            "def make_agent(key: jax.random.PRNGKey, player_id: int, env: Environment, networks: Tuple[hk.Transformed, hk.Transformed]) -> OpponentShapingAgent:\n    if False:\n        i = 10\n    'Creates an opponent shaping agent.\\n\\n  Args:\\n      key: A random seed key.\\n      player_id: The id of the player.\\n      env: The environment.\\n      networks: A tuple of policy and critic networks transformed by\\n        hk.transform.\\n\\n  Returns:\\n      An opponent shaping agent instance.\\n  '\n    (policy_network, critic_network) = networks\n    return OpponentShapingAgent(player_id=player_id, opponent_ids=[1 - player_id], seed=key, info_state_size=env.observation_spec()['info_state'][player_id], num_actions=env.action_spec()['num_actions'][player_id], policy=policy_network, critic=critic_network, batch_size=FLAGS.batch_size, num_critic_mini_batches=FLAGS.critic_mini_batches, pi_learning_rate=FLAGS.policy_lr, opp_policy_learning_rate=FLAGS.opp_policy_lr, num_opponent_updates=FLAGS.opp_policy_mini_batches, critic_learning_rate=FLAGS.critic_lr, opponent_model_learning_rate=FLAGS.opponent_model_learning_rate, policy_update_interval=FLAGS.policy_update_interval, discount=FLAGS.discount, critic_discount=0, correction_type=FLAGS.correction_type, clip_grad_norm=FLAGS.correction_max_grad_norm, use_jit=FLAGS.use_jit, n_lookaheads=FLAGS.n_lookaheads, env=env)",
            "def make_agent(key: jax.random.PRNGKey, player_id: int, env: Environment, networks: Tuple[hk.Transformed, hk.Transformed]) -> OpponentShapingAgent:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates an opponent shaping agent.\\n\\n  Args:\\n      key: A random seed key.\\n      player_id: The id of the player.\\n      env: The environment.\\n      networks: A tuple of policy and critic networks transformed by\\n        hk.transform.\\n\\n  Returns:\\n      An opponent shaping agent instance.\\n  '\n    (policy_network, critic_network) = networks\n    return OpponentShapingAgent(player_id=player_id, opponent_ids=[1 - player_id], seed=key, info_state_size=env.observation_spec()['info_state'][player_id], num_actions=env.action_spec()['num_actions'][player_id], policy=policy_network, critic=critic_network, batch_size=FLAGS.batch_size, num_critic_mini_batches=FLAGS.critic_mini_batches, pi_learning_rate=FLAGS.policy_lr, opp_policy_learning_rate=FLAGS.opp_policy_lr, num_opponent_updates=FLAGS.opp_policy_mini_batches, critic_learning_rate=FLAGS.critic_lr, opponent_model_learning_rate=FLAGS.opponent_model_learning_rate, policy_update_interval=FLAGS.policy_update_interval, discount=FLAGS.discount, critic_discount=0, correction_type=FLAGS.correction_type, clip_grad_norm=FLAGS.correction_max_grad_norm, use_jit=FLAGS.use_jit, n_lookaheads=FLAGS.n_lookaheads, env=env)",
            "def make_agent(key: jax.random.PRNGKey, player_id: int, env: Environment, networks: Tuple[hk.Transformed, hk.Transformed]) -> OpponentShapingAgent:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates an opponent shaping agent.\\n\\n  Args:\\n      key: A random seed key.\\n      player_id: The id of the player.\\n      env: The environment.\\n      networks: A tuple of policy and critic networks transformed by\\n        hk.transform.\\n\\n  Returns:\\n      An opponent shaping agent instance.\\n  '\n    (policy_network, critic_network) = networks\n    return OpponentShapingAgent(player_id=player_id, opponent_ids=[1 - player_id], seed=key, info_state_size=env.observation_spec()['info_state'][player_id], num_actions=env.action_spec()['num_actions'][player_id], policy=policy_network, critic=critic_network, batch_size=FLAGS.batch_size, num_critic_mini_batches=FLAGS.critic_mini_batches, pi_learning_rate=FLAGS.policy_lr, opp_policy_learning_rate=FLAGS.opp_policy_lr, num_opponent_updates=FLAGS.opp_policy_mini_batches, critic_learning_rate=FLAGS.critic_lr, opponent_model_learning_rate=FLAGS.opponent_model_learning_rate, policy_update_interval=FLAGS.policy_update_interval, discount=FLAGS.discount, critic_discount=0, correction_type=FLAGS.correction_type, clip_grad_norm=FLAGS.correction_max_grad_norm, use_jit=FLAGS.use_jit, n_lookaheads=FLAGS.n_lookaheads, env=env)",
            "def make_agent(key: jax.random.PRNGKey, player_id: int, env: Environment, networks: Tuple[hk.Transformed, hk.Transformed]) -> OpponentShapingAgent:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates an opponent shaping agent.\\n\\n  Args:\\n      key: A random seed key.\\n      player_id: The id of the player.\\n      env: The environment.\\n      networks: A tuple of policy and critic networks transformed by\\n        hk.transform.\\n\\n  Returns:\\n      An opponent shaping agent instance.\\n  '\n    (policy_network, critic_network) = networks\n    return OpponentShapingAgent(player_id=player_id, opponent_ids=[1 - player_id], seed=key, info_state_size=env.observation_spec()['info_state'][player_id], num_actions=env.action_spec()['num_actions'][player_id], policy=policy_network, critic=critic_network, batch_size=FLAGS.batch_size, num_critic_mini_batches=FLAGS.critic_mini_batches, pi_learning_rate=FLAGS.policy_lr, opp_policy_learning_rate=FLAGS.opp_policy_lr, num_opponent_updates=FLAGS.opp_policy_mini_batches, critic_learning_rate=FLAGS.critic_lr, opponent_model_learning_rate=FLAGS.opponent_model_learning_rate, policy_update_interval=FLAGS.policy_update_interval, discount=FLAGS.discount, critic_discount=0, correction_type=FLAGS.correction_type, clip_grad_norm=FLAGS.correction_max_grad_norm, use_jit=FLAGS.use_jit, n_lookaheads=FLAGS.n_lookaheads, env=env)",
            "def make_agent(key: jax.random.PRNGKey, player_id: int, env: Environment, networks: Tuple[hk.Transformed, hk.Transformed]) -> OpponentShapingAgent:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates an opponent shaping agent.\\n\\n  Args:\\n      key: A random seed key.\\n      player_id: The id of the player.\\n      env: The environment.\\n      networks: A tuple of policy and critic networks transformed by\\n        hk.transform.\\n\\n  Returns:\\n      An opponent shaping agent instance.\\n  '\n    (policy_network, critic_network) = networks\n    return OpponentShapingAgent(player_id=player_id, opponent_ids=[1 - player_id], seed=key, info_state_size=env.observation_spec()['info_state'][player_id], num_actions=env.action_spec()['num_actions'][player_id], policy=policy_network, critic=critic_network, batch_size=FLAGS.batch_size, num_critic_mini_batches=FLAGS.critic_mini_batches, pi_learning_rate=FLAGS.policy_lr, opp_policy_learning_rate=FLAGS.opp_policy_lr, num_opponent_updates=FLAGS.opp_policy_mini_batches, critic_learning_rate=FLAGS.critic_lr, opponent_model_learning_rate=FLAGS.opponent_model_learning_rate, policy_update_interval=FLAGS.policy_update_interval, discount=FLAGS.discount, critic_discount=0, correction_type=FLAGS.correction_type, clip_grad_norm=FLAGS.correction_max_grad_norm, use_jit=FLAGS.use_jit, n_lookaheads=FLAGS.n_lookaheads, env=env)"
        ]
    },
    {
        "func_name": "policy",
        "original": "def policy(obs):\n    theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n    logits = jnp.select(obs, theta)\n    logits = jnp.nan_to_num(logits)\n    return distrax.Categorical(logits=logits)",
        "mutated": [
            "def policy(obs):\n    if False:\n        i = 10\n    theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n    logits = jnp.select(obs, theta)\n    logits = jnp.nan_to_num(logits)\n    return distrax.Categorical(logits=logits)",
            "def policy(obs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n    logits = jnp.select(obs, theta)\n    logits = jnp.nan_to_num(logits)\n    return distrax.Categorical(logits=logits)",
            "def policy(obs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n    logits = jnp.select(obs, theta)\n    logits = jnp.nan_to_num(logits)\n    return distrax.Categorical(logits=logits)",
            "def policy(obs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n    logits = jnp.select(obs, theta)\n    logits = jnp.nan_to_num(logits)\n    return distrax.Categorical(logits=logits)",
            "def policy(obs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n    logits = jnp.select(obs, theta)\n    logits = jnp.nan_to_num(logits)\n    return distrax.Categorical(logits=logits)"
        ]
    },
    {
        "func_name": "value_fn",
        "original": "def value_fn(obs):\n    w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n    return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)",
        "mutated": [
            "def value_fn(obs):\n    if False:\n        i = 10\n    w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n    return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)",
            "def value_fn(obs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n    return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)",
            "def value_fn(obs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n    return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)",
            "def value_fn(obs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n    return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)",
            "def value_fn(obs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n    return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)"
        ]
    },
    {
        "func_name": "make_agent_networks",
        "original": "def make_agent_networks(num_states: int, num_actions: int) -> Tuple[hk.Transformed, hk.Transformed]:\n    \"\"\"Creates action weights for each state-action pair and values for each state.\n\n  Args:\n      num_states: The number of distinct states.\n      num_actions: The number of distinct actions.\n\n  Returns:\n      A tuple of policy and critic networks transformed by hk.transform.\n  \"\"\"\n\n    def policy(obs):\n        theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n        logits = jnp.select(obs, theta)\n        logits = jnp.nan_to_num(logits)\n        return distrax.Categorical(logits=logits)\n\n    def value_fn(obs):\n        w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n        return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)\n    return (hk.without_apply_rng(hk.transform(policy)), hk.without_apply_rng(hk.transform(value_fn)))",
        "mutated": [
            "def make_agent_networks(num_states: int, num_actions: int) -> Tuple[hk.Transformed, hk.Transformed]:\n    if False:\n        i = 10\n    'Creates action weights for each state-action pair and values for each state.\\n\\n  Args:\\n      num_states: The number of distinct states.\\n      num_actions: The number of distinct actions.\\n\\n  Returns:\\n      A tuple of policy and critic networks transformed by hk.transform.\\n  '\n\n    def policy(obs):\n        theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n        logits = jnp.select(obs, theta)\n        logits = jnp.nan_to_num(logits)\n        return distrax.Categorical(logits=logits)\n\n    def value_fn(obs):\n        w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n        return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)\n    return (hk.without_apply_rng(hk.transform(policy)), hk.without_apply_rng(hk.transform(value_fn)))",
            "def make_agent_networks(num_states: int, num_actions: int) -> Tuple[hk.Transformed, hk.Transformed]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates action weights for each state-action pair and values for each state.\\n\\n  Args:\\n      num_states: The number of distinct states.\\n      num_actions: The number of distinct actions.\\n\\n  Returns:\\n      A tuple of policy and critic networks transformed by hk.transform.\\n  '\n\n    def policy(obs):\n        theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n        logits = jnp.select(obs, theta)\n        logits = jnp.nan_to_num(logits)\n        return distrax.Categorical(logits=logits)\n\n    def value_fn(obs):\n        w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n        return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)\n    return (hk.without_apply_rng(hk.transform(policy)), hk.without_apply_rng(hk.transform(value_fn)))",
            "def make_agent_networks(num_states: int, num_actions: int) -> Tuple[hk.Transformed, hk.Transformed]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates action weights for each state-action pair and values for each state.\\n\\n  Args:\\n      num_states: The number of distinct states.\\n      num_actions: The number of distinct actions.\\n\\n  Returns:\\n      A tuple of policy and critic networks transformed by hk.transform.\\n  '\n\n    def policy(obs):\n        theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n        logits = jnp.select(obs, theta)\n        logits = jnp.nan_to_num(logits)\n        return distrax.Categorical(logits=logits)\n\n    def value_fn(obs):\n        w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n        return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)\n    return (hk.without_apply_rng(hk.transform(policy)), hk.without_apply_rng(hk.transform(value_fn)))",
            "def make_agent_networks(num_states: int, num_actions: int) -> Tuple[hk.Transformed, hk.Transformed]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates action weights for each state-action pair and values for each state.\\n\\n  Args:\\n      num_states: The number of distinct states.\\n      num_actions: The number of distinct actions.\\n\\n  Returns:\\n      A tuple of policy and critic networks transformed by hk.transform.\\n  '\n\n    def policy(obs):\n        theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n        logits = jnp.select(obs, theta)\n        logits = jnp.nan_to_num(logits)\n        return distrax.Categorical(logits=logits)\n\n    def value_fn(obs):\n        w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n        return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)\n    return (hk.without_apply_rng(hk.transform(policy)), hk.without_apply_rng(hk.transform(value_fn)))",
            "def make_agent_networks(num_states: int, num_actions: int) -> Tuple[hk.Transformed, hk.Transformed]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates action weights for each state-action pair and values for each state.\\n\\n  Args:\\n      num_states: The number of distinct states.\\n      num_actions: The number of distinct actions.\\n\\n  Returns:\\n      A tuple of policy and critic networks transformed by hk.transform.\\n  '\n\n    def policy(obs):\n        theta = hk.get_parameter('theta', init=hk.initializers.Constant(0), shape=(num_states, num_actions))\n        logits = jnp.select(obs, theta)\n        logits = jnp.nan_to_num(logits)\n        return distrax.Categorical(logits=logits)\n\n    def value_fn(obs):\n        w = hk.get_parameter('w', [num_states], init=jnp.zeros)\n        return w[jnp.argmax(obs, axis=-1)].reshape(*obs.shape[:-1], 1)\n    return (hk.without_apply_rng(hk.transform(policy)), hk.without_apply_rng(hk.transform(value_fn)))"
        ]
    },
    {
        "func_name": "make_env",
        "original": "def make_env(game: str, iterations: int, batch_size: int) -> Environment:\n    \"\"\"Creates an environment.\n\n  The environment is either iterated prisoners dilemma or iterated matching\n  pennies.\n  \n  Args:\n      game: The game to play. Either 'ipd' or 'imp'.\n      iterations: The number of iterations to play.\n      batch_size: The batch size.\n\n  Returns:\n      An environment instance.\n  \"\"\"\n    if game == 'ipd':\n        env = IteratedPrisonersDilemma(iterations=iterations, batch_size=batch_size)\n    elif game == 'imp':\n        env = IteratedMatchingPennies(iterations=iterations, batch_size=batch_size)\n    else:\n        raise ValueError(f'Unknown game: {game}')\n    return env",
        "mutated": [
            "def make_env(game: str, iterations: int, batch_size: int) -> Environment:\n    if False:\n        i = 10\n    \"Creates an environment.\\n\\n  The environment is either iterated prisoners dilemma or iterated matching\\n  pennies.\\n  \\n  Args:\\n      game: The game to play. Either 'ipd' or 'imp'.\\n      iterations: The number of iterations to play.\\n      batch_size: The batch size.\\n\\n  Returns:\\n      An environment instance.\\n  \"\n    if game == 'ipd':\n        env = IteratedPrisonersDilemma(iterations=iterations, batch_size=batch_size)\n    elif game == 'imp':\n        env = IteratedMatchingPennies(iterations=iterations, batch_size=batch_size)\n    else:\n        raise ValueError(f'Unknown game: {game}')\n    return env",
            "def make_env(game: str, iterations: int, batch_size: int) -> Environment:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Creates an environment.\\n\\n  The environment is either iterated prisoners dilemma or iterated matching\\n  pennies.\\n  \\n  Args:\\n      game: The game to play. Either 'ipd' or 'imp'.\\n      iterations: The number of iterations to play.\\n      batch_size: The batch size.\\n\\n  Returns:\\n      An environment instance.\\n  \"\n    if game == 'ipd':\n        env = IteratedPrisonersDilemma(iterations=iterations, batch_size=batch_size)\n    elif game == 'imp':\n        env = IteratedMatchingPennies(iterations=iterations, batch_size=batch_size)\n    else:\n        raise ValueError(f'Unknown game: {game}')\n    return env",
            "def make_env(game: str, iterations: int, batch_size: int) -> Environment:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Creates an environment.\\n\\n  The environment is either iterated prisoners dilemma or iterated matching\\n  pennies.\\n  \\n  Args:\\n      game: The game to play. Either 'ipd' or 'imp'.\\n      iterations: The number of iterations to play.\\n      batch_size: The batch size.\\n\\n  Returns:\\n      An environment instance.\\n  \"\n    if game == 'ipd':\n        env = IteratedPrisonersDilemma(iterations=iterations, batch_size=batch_size)\n    elif game == 'imp':\n        env = IteratedMatchingPennies(iterations=iterations, batch_size=batch_size)\n    else:\n        raise ValueError(f'Unknown game: {game}')\n    return env",
            "def make_env(game: str, iterations: int, batch_size: int) -> Environment:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Creates an environment.\\n\\n  The environment is either iterated prisoners dilemma or iterated matching\\n  pennies.\\n  \\n  Args:\\n      game: The game to play. Either 'ipd' or 'imp'.\\n      iterations: The number of iterations to play.\\n      batch_size: The batch size.\\n\\n  Returns:\\n      An environment instance.\\n  \"\n    if game == 'ipd':\n        env = IteratedPrisonersDilemma(iterations=iterations, batch_size=batch_size)\n    elif game == 'imp':\n        env = IteratedMatchingPennies(iterations=iterations, batch_size=batch_size)\n    else:\n        raise ValueError(f'Unknown game: {game}')\n    return env",
            "def make_env(game: str, iterations: int, batch_size: int) -> Environment:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Creates an environment.\\n\\n  The environment is either iterated prisoners dilemma or iterated matching\\n  pennies.\\n  \\n  Args:\\n      game: The game to play. Either 'ipd' or 'imp'.\\n      iterations: The number of iterations to play.\\n      batch_size: The batch size.\\n\\n  Returns:\\n      An environment instance.\\n  \"\n    if game == 'ipd':\n        env = IteratedPrisonersDilemma(iterations=iterations, batch_size=batch_size)\n    elif game == 'imp':\n        env = IteratedMatchingPennies(iterations=iterations, batch_size=batch_size)\n    else:\n        raise ValueError(f'Unknown game: {game}')\n    return env"
        ]
    },
    {
        "func_name": "setup_agents",
        "original": "def setup_agents(env: Environment, rng: hk.PRNGSequence) -> List[OpponentShapingAgent]:\n    \"\"\"Creates an opponent shaping agent for each player in the environment.\n\n  Args:\n      env: The environment.\n      rng: A random seed key.\n\n  Returns:\n      A list of opponent shaping agents.\n  \"\"\"\n    agents = []\n    num_actions = env.action_spec()['num_actions']\n    info_state_shape = env.observation_spec()['info_state']\n    for player_id in range(env.num_players):\n        networks = make_agent_networks(num_states=info_state_shape[player_id][0], num_actions=num_actions[player_id])\n        agent = make_agent(key=next(rng), player_id=player_id, env=env, networks=networks)\n        agents.append(agent)\n    return agents",
        "mutated": [
            "def setup_agents(env: Environment, rng: hk.PRNGSequence) -> List[OpponentShapingAgent]:\n    if False:\n        i = 10\n    'Creates an opponent shaping agent for each player in the environment.\\n\\n  Args:\\n      env: The environment.\\n      rng: A random seed key.\\n\\n  Returns:\\n      A list of opponent shaping agents.\\n  '\n    agents = []\n    num_actions = env.action_spec()['num_actions']\n    info_state_shape = env.observation_spec()['info_state']\n    for player_id in range(env.num_players):\n        networks = make_agent_networks(num_states=info_state_shape[player_id][0], num_actions=num_actions[player_id])\n        agent = make_agent(key=next(rng), player_id=player_id, env=env, networks=networks)\n        agents.append(agent)\n    return agents",
            "def setup_agents(env: Environment, rng: hk.PRNGSequence) -> List[OpponentShapingAgent]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates an opponent shaping agent for each player in the environment.\\n\\n  Args:\\n      env: The environment.\\n      rng: A random seed key.\\n\\n  Returns:\\n      A list of opponent shaping agents.\\n  '\n    agents = []\n    num_actions = env.action_spec()['num_actions']\n    info_state_shape = env.observation_spec()['info_state']\n    for player_id in range(env.num_players):\n        networks = make_agent_networks(num_states=info_state_shape[player_id][0], num_actions=num_actions[player_id])\n        agent = make_agent(key=next(rng), player_id=player_id, env=env, networks=networks)\n        agents.append(agent)\n    return agents",
            "def setup_agents(env: Environment, rng: hk.PRNGSequence) -> List[OpponentShapingAgent]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates an opponent shaping agent for each player in the environment.\\n\\n  Args:\\n      env: The environment.\\n      rng: A random seed key.\\n\\n  Returns:\\n      A list of opponent shaping agents.\\n  '\n    agents = []\n    num_actions = env.action_spec()['num_actions']\n    info_state_shape = env.observation_spec()['info_state']\n    for player_id in range(env.num_players):\n        networks = make_agent_networks(num_states=info_state_shape[player_id][0], num_actions=num_actions[player_id])\n        agent = make_agent(key=next(rng), player_id=player_id, env=env, networks=networks)\n        agents.append(agent)\n    return agents",
            "def setup_agents(env: Environment, rng: hk.PRNGSequence) -> List[OpponentShapingAgent]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates an opponent shaping agent for each player in the environment.\\n\\n  Args:\\n      env: The environment.\\n      rng: A random seed key.\\n\\n  Returns:\\n      A list of opponent shaping agents.\\n  '\n    agents = []\n    num_actions = env.action_spec()['num_actions']\n    info_state_shape = env.observation_spec()['info_state']\n    for player_id in range(env.num_players):\n        networks = make_agent_networks(num_states=info_state_shape[player_id][0], num_actions=num_actions[player_id])\n        agent = make_agent(key=next(rng), player_id=player_id, env=env, networks=networks)\n        agents.append(agent)\n    return agents",
            "def setup_agents(env: Environment, rng: hk.PRNGSequence) -> List[OpponentShapingAgent]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates an opponent shaping agent for each player in the environment.\\n\\n  Args:\\n      env: The environment.\\n      rng: A random seed key.\\n\\n  Returns:\\n      A list of opponent shaping agents.\\n  '\n    agents = []\n    num_actions = env.action_spec()['num_actions']\n    info_state_shape = env.observation_spec()['info_state']\n    for player_id in range(env.num_players):\n        networks = make_agent_networks(num_states=info_state_shape[player_id][0], num_actions=num_actions[player_id])\n        agent = make_agent(key=next(rng), player_id=player_id, env=env, networks=networks)\n        agents.append(agent)\n    return agents"
        ]
    },
    {
        "func_name": "update_weights",
        "original": "def update_weights(agents: List[OpponentShapingAgent]):\n    \"\"\"Updates the weights of the opponent models.\n\n  Args:\n      agents: A list of opponent shaping agents.\n\n  Returns:\n      None\n  \"\"\"\n    agent: OpponentShapingAgent\n    for agent in agents:\n        for opp in [a for a in agents if a.player_id != agent.player_id]:\n            agent.update_params(state=opp.train_state, player_id=opp.player_id)",
        "mutated": [
            "def update_weights(agents: List[OpponentShapingAgent]):\n    if False:\n        i = 10\n    'Updates the weights of the opponent models.\\n\\n  Args:\\n      agents: A list of opponent shaping agents.\\n\\n  Returns:\\n      None\\n  '\n    agent: OpponentShapingAgent\n    for agent in agents:\n        for opp in [a for a in agents if a.player_id != agent.player_id]:\n            agent.update_params(state=opp.train_state, player_id=opp.player_id)",
            "def update_weights(agents: List[OpponentShapingAgent]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Updates the weights of the opponent models.\\n\\n  Args:\\n      agents: A list of opponent shaping agents.\\n\\n  Returns:\\n      None\\n  '\n    agent: OpponentShapingAgent\n    for agent in agents:\n        for opp in [a for a in agents if a.player_id != agent.player_id]:\n            agent.update_params(state=opp.train_state, player_id=opp.player_id)",
            "def update_weights(agents: List[OpponentShapingAgent]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Updates the weights of the opponent models.\\n\\n  Args:\\n      agents: A list of opponent shaping agents.\\n\\n  Returns:\\n      None\\n  '\n    agent: OpponentShapingAgent\n    for agent in agents:\n        for opp in [a for a in agents if a.player_id != agent.player_id]:\n            agent.update_params(state=opp.train_state, player_id=opp.player_id)",
            "def update_weights(agents: List[OpponentShapingAgent]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Updates the weights of the opponent models.\\n\\n  Args:\\n      agents: A list of opponent shaping agents.\\n\\n  Returns:\\n      None\\n  '\n    agent: OpponentShapingAgent\n    for agent in agents:\n        for opp in [a for a in agents if a.player_id != agent.player_id]:\n            agent.update_params(state=opp.train_state, player_id=opp.player_id)",
            "def update_weights(agents: List[OpponentShapingAgent]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Updates the weights of the opponent models.\\n\\n  Args:\\n      agents: A list of opponent shaping agents.\\n\\n  Returns:\\n      None\\n  '\n    agent: OpponentShapingAgent\n    for agent in agents:\n        for opp in [a for a in agents if a.player_id != agent.player_id]:\n            agent.update_params(state=opp.train_state, player_id=opp.player_id)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(_):\n    \"\"\"Main function. Runs the experiment.\"\"\"\n    if FLAGS.exp_name is None:\n        FLAGS.exp_name = f'{FLAGS.game}_{FLAGS.seed}'\n    if not FLAGS.debug:\n        wandb.login(key=os.environ.get('WANDB_API_KEY', None))\n    wandb.init(project='open-spiel-opponent-modelling', group=FLAGS.exp_name, config={'game': FLAGS.game, 'seed': FLAGS.seed, 'epochs': FLAGS.epochs, 'batch_size': FLAGS.batch_size, 'critic_mini_batches': FLAGS.critic_mini_batches, 'game_iterations': FLAGS.game_iterations, 'policy_lr': FLAGS.policy_lr, 'opp_policy_lr': FLAGS.opp_policy_lr, 'critic_lr': FLAGS.critic_lr, 'correction_type': FLAGS.correction_type, 'n_lookaheads': FLAGS.n_lookaheads, 'correction_max_grad_norm': FLAGS.correction_max_grad_norm, 'discount': FLAGS.discount, 'policy_update_interval': FLAGS.policy_update_interval, 'use_opponent_modelling': FLAGS.use_opponent_modelling, 'opp_policy_mini_batches': FLAGS.opp_policy_mini_batches, 'opponent_model_learning_rate': FLAGS.opponent_model_learning_rate}, mode='disabled' if FLAGS.debug else 'online')\n    rng = hk.PRNGSequence(key_or_seed=FLAGS.seed)\n    env = make_env(iterations=FLAGS.game_iterations, batch_size=FLAGS.batch_size, game=FLAGS.game)\n    agents = setup_agents(env=env, rng=rng)\n    if not FLAGS.use_opponent_modelling:\n        update_weights(agents)\n    batch = collect_batch(env=env, agents=agents, eval_mode=True)\n    log_epoch_data(epoch=0, agents=agents, eval_batch=batch)\n    for epoch in range(1, FLAGS.epochs + 1):\n        batch = collect_batch(env=env, agents=agents, eval_mode=False)\n        if not FLAGS.use_opponent_modelling:\n            update_weights(agents)\n        log_epoch_data(epoch=epoch, agents=agents, eval_batch=batch)\n        print('#' * 100)\n    wandb.finish()",
        "mutated": [
            "def main(_):\n    if False:\n        i = 10\n    'Main function. Runs the experiment.'\n    if FLAGS.exp_name is None:\n        FLAGS.exp_name = f'{FLAGS.game}_{FLAGS.seed}'\n    if not FLAGS.debug:\n        wandb.login(key=os.environ.get('WANDB_API_KEY', None))\n    wandb.init(project='open-spiel-opponent-modelling', group=FLAGS.exp_name, config={'game': FLAGS.game, 'seed': FLAGS.seed, 'epochs': FLAGS.epochs, 'batch_size': FLAGS.batch_size, 'critic_mini_batches': FLAGS.critic_mini_batches, 'game_iterations': FLAGS.game_iterations, 'policy_lr': FLAGS.policy_lr, 'opp_policy_lr': FLAGS.opp_policy_lr, 'critic_lr': FLAGS.critic_lr, 'correction_type': FLAGS.correction_type, 'n_lookaheads': FLAGS.n_lookaheads, 'correction_max_grad_norm': FLAGS.correction_max_grad_norm, 'discount': FLAGS.discount, 'policy_update_interval': FLAGS.policy_update_interval, 'use_opponent_modelling': FLAGS.use_opponent_modelling, 'opp_policy_mini_batches': FLAGS.opp_policy_mini_batches, 'opponent_model_learning_rate': FLAGS.opponent_model_learning_rate}, mode='disabled' if FLAGS.debug else 'online')\n    rng = hk.PRNGSequence(key_or_seed=FLAGS.seed)\n    env = make_env(iterations=FLAGS.game_iterations, batch_size=FLAGS.batch_size, game=FLAGS.game)\n    agents = setup_agents(env=env, rng=rng)\n    if not FLAGS.use_opponent_modelling:\n        update_weights(agents)\n    batch = collect_batch(env=env, agents=agents, eval_mode=True)\n    log_epoch_data(epoch=0, agents=agents, eval_batch=batch)\n    for epoch in range(1, FLAGS.epochs + 1):\n        batch = collect_batch(env=env, agents=agents, eval_mode=False)\n        if not FLAGS.use_opponent_modelling:\n            update_weights(agents)\n        log_epoch_data(epoch=epoch, agents=agents, eval_batch=batch)\n        print('#' * 100)\n    wandb.finish()",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Main function. Runs the experiment.'\n    if FLAGS.exp_name is None:\n        FLAGS.exp_name = f'{FLAGS.game}_{FLAGS.seed}'\n    if not FLAGS.debug:\n        wandb.login(key=os.environ.get('WANDB_API_KEY', None))\n    wandb.init(project='open-spiel-opponent-modelling', group=FLAGS.exp_name, config={'game': FLAGS.game, 'seed': FLAGS.seed, 'epochs': FLAGS.epochs, 'batch_size': FLAGS.batch_size, 'critic_mini_batches': FLAGS.critic_mini_batches, 'game_iterations': FLAGS.game_iterations, 'policy_lr': FLAGS.policy_lr, 'opp_policy_lr': FLAGS.opp_policy_lr, 'critic_lr': FLAGS.critic_lr, 'correction_type': FLAGS.correction_type, 'n_lookaheads': FLAGS.n_lookaheads, 'correction_max_grad_norm': FLAGS.correction_max_grad_norm, 'discount': FLAGS.discount, 'policy_update_interval': FLAGS.policy_update_interval, 'use_opponent_modelling': FLAGS.use_opponent_modelling, 'opp_policy_mini_batches': FLAGS.opp_policy_mini_batches, 'opponent_model_learning_rate': FLAGS.opponent_model_learning_rate}, mode='disabled' if FLAGS.debug else 'online')\n    rng = hk.PRNGSequence(key_or_seed=FLAGS.seed)\n    env = make_env(iterations=FLAGS.game_iterations, batch_size=FLAGS.batch_size, game=FLAGS.game)\n    agents = setup_agents(env=env, rng=rng)\n    if not FLAGS.use_opponent_modelling:\n        update_weights(agents)\n    batch = collect_batch(env=env, agents=agents, eval_mode=True)\n    log_epoch_data(epoch=0, agents=agents, eval_batch=batch)\n    for epoch in range(1, FLAGS.epochs + 1):\n        batch = collect_batch(env=env, agents=agents, eval_mode=False)\n        if not FLAGS.use_opponent_modelling:\n            update_weights(agents)\n        log_epoch_data(epoch=epoch, agents=agents, eval_batch=batch)\n        print('#' * 100)\n    wandb.finish()",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Main function. Runs the experiment.'\n    if FLAGS.exp_name is None:\n        FLAGS.exp_name = f'{FLAGS.game}_{FLAGS.seed}'\n    if not FLAGS.debug:\n        wandb.login(key=os.environ.get('WANDB_API_KEY', None))\n    wandb.init(project='open-spiel-opponent-modelling', group=FLAGS.exp_name, config={'game': FLAGS.game, 'seed': FLAGS.seed, 'epochs': FLAGS.epochs, 'batch_size': FLAGS.batch_size, 'critic_mini_batches': FLAGS.critic_mini_batches, 'game_iterations': FLAGS.game_iterations, 'policy_lr': FLAGS.policy_lr, 'opp_policy_lr': FLAGS.opp_policy_lr, 'critic_lr': FLAGS.critic_lr, 'correction_type': FLAGS.correction_type, 'n_lookaheads': FLAGS.n_lookaheads, 'correction_max_grad_norm': FLAGS.correction_max_grad_norm, 'discount': FLAGS.discount, 'policy_update_interval': FLAGS.policy_update_interval, 'use_opponent_modelling': FLAGS.use_opponent_modelling, 'opp_policy_mini_batches': FLAGS.opp_policy_mini_batches, 'opponent_model_learning_rate': FLAGS.opponent_model_learning_rate}, mode='disabled' if FLAGS.debug else 'online')\n    rng = hk.PRNGSequence(key_or_seed=FLAGS.seed)\n    env = make_env(iterations=FLAGS.game_iterations, batch_size=FLAGS.batch_size, game=FLAGS.game)\n    agents = setup_agents(env=env, rng=rng)\n    if not FLAGS.use_opponent_modelling:\n        update_weights(agents)\n    batch = collect_batch(env=env, agents=agents, eval_mode=True)\n    log_epoch_data(epoch=0, agents=agents, eval_batch=batch)\n    for epoch in range(1, FLAGS.epochs + 1):\n        batch = collect_batch(env=env, agents=agents, eval_mode=False)\n        if not FLAGS.use_opponent_modelling:\n            update_weights(agents)\n        log_epoch_data(epoch=epoch, agents=agents, eval_batch=batch)\n        print('#' * 100)\n    wandb.finish()",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Main function. Runs the experiment.'\n    if FLAGS.exp_name is None:\n        FLAGS.exp_name = f'{FLAGS.game}_{FLAGS.seed}'\n    if not FLAGS.debug:\n        wandb.login(key=os.environ.get('WANDB_API_KEY', None))\n    wandb.init(project='open-spiel-opponent-modelling', group=FLAGS.exp_name, config={'game': FLAGS.game, 'seed': FLAGS.seed, 'epochs': FLAGS.epochs, 'batch_size': FLAGS.batch_size, 'critic_mini_batches': FLAGS.critic_mini_batches, 'game_iterations': FLAGS.game_iterations, 'policy_lr': FLAGS.policy_lr, 'opp_policy_lr': FLAGS.opp_policy_lr, 'critic_lr': FLAGS.critic_lr, 'correction_type': FLAGS.correction_type, 'n_lookaheads': FLAGS.n_lookaheads, 'correction_max_grad_norm': FLAGS.correction_max_grad_norm, 'discount': FLAGS.discount, 'policy_update_interval': FLAGS.policy_update_interval, 'use_opponent_modelling': FLAGS.use_opponent_modelling, 'opp_policy_mini_batches': FLAGS.opp_policy_mini_batches, 'opponent_model_learning_rate': FLAGS.opponent_model_learning_rate}, mode='disabled' if FLAGS.debug else 'online')\n    rng = hk.PRNGSequence(key_or_seed=FLAGS.seed)\n    env = make_env(iterations=FLAGS.game_iterations, batch_size=FLAGS.batch_size, game=FLAGS.game)\n    agents = setup_agents(env=env, rng=rng)\n    if not FLAGS.use_opponent_modelling:\n        update_weights(agents)\n    batch = collect_batch(env=env, agents=agents, eval_mode=True)\n    log_epoch_data(epoch=0, agents=agents, eval_batch=batch)\n    for epoch in range(1, FLAGS.epochs + 1):\n        batch = collect_batch(env=env, agents=agents, eval_mode=False)\n        if not FLAGS.use_opponent_modelling:\n            update_weights(agents)\n        log_epoch_data(epoch=epoch, agents=agents, eval_batch=batch)\n        print('#' * 100)\n    wandb.finish()",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Main function. Runs the experiment.'\n    if FLAGS.exp_name is None:\n        FLAGS.exp_name = f'{FLAGS.game}_{FLAGS.seed}'\n    if not FLAGS.debug:\n        wandb.login(key=os.environ.get('WANDB_API_KEY', None))\n    wandb.init(project='open-spiel-opponent-modelling', group=FLAGS.exp_name, config={'game': FLAGS.game, 'seed': FLAGS.seed, 'epochs': FLAGS.epochs, 'batch_size': FLAGS.batch_size, 'critic_mini_batches': FLAGS.critic_mini_batches, 'game_iterations': FLAGS.game_iterations, 'policy_lr': FLAGS.policy_lr, 'opp_policy_lr': FLAGS.opp_policy_lr, 'critic_lr': FLAGS.critic_lr, 'correction_type': FLAGS.correction_type, 'n_lookaheads': FLAGS.n_lookaheads, 'correction_max_grad_norm': FLAGS.correction_max_grad_norm, 'discount': FLAGS.discount, 'policy_update_interval': FLAGS.policy_update_interval, 'use_opponent_modelling': FLAGS.use_opponent_modelling, 'opp_policy_mini_batches': FLAGS.opp_policy_mini_batches, 'opponent_model_learning_rate': FLAGS.opponent_model_learning_rate}, mode='disabled' if FLAGS.debug else 'online')\n    rng = hk.PRNGSequence(key_or_seed=FLAGS.seed)\n    env = make_env(iterations=FLAGS.game_iterations, batch_size=FLAGS.batch_size, game=FLAGS.game)\n    agents = setup_agents(env=env, rng=rng)\n    if not FLAGS.use_opponent_modelling:\n        update_weights(agents)\n    batch = collect_batch(env=env, agents=agents, eval_mode=True)\n    log_epoch_data(epoch=0, agents=agents, eval_batch=batch)\n    for epoch in range(1, FLAGS.epochs + 1):\n        batch = collect_batch(env=env, agents=agents, eval_mode=False)\n        if not FLAGS.use_opponent_modelling:\n            update_weights(agents)\n        log_epoch_data(epoch=epoch, agents=agents, eval_batch=batch)\n        print('#' * 100)\n    wandb.finish()"
        ]
    }
]