[
    {
        "func_name": "linregress",
        "original": "def linregress(x, y=None, alternative='two-sided'):\n    \"\"\"\n    Calculate a linear least-squares regression for two sets of measurements.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Two sets of measurements.  Both arrays should have the same length.  If\n        only `x` is given (and ``y=None``), then it must be a two-dimensional\n        array where one dimension has length 2.  The two sets of measurements\n        are then found by splitting the array along the length-2 dimension. In\n        the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is\n        equivalent to ``linregress(x[0], x[1])``.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the slope of the regression line is nonzero\n        * 'less': the slope of the regression line is less than zero\n        * 'greater':  the slope of the regression line is greater than zero\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    result : ``LinregressResult`` instance\n        The return value is an object with the following attributes:\n\n        slope : float\n            Slope of the regression line.\n        intercept : float\n            Intercept of the regression line.\n        rvalue : float\n            The Pearson correlation coefficient. The square of ``rvalue``\n            is equal to the coefficient of determination.\n        pvalue : float\n            The p-value for a hypothesis test whose null hypothesis is\n            that the slope is zero, using Wald Test with t-distribution of\n            the test statistic. See `alternative` above for alternative\n            hypotheses.\n        stderr : float\n            Standard error of the estimated slope (gradient), under the\n            assumption of residual normality.\n        intercept_stderr : float\n            Standard error of the estimated intercept, under the assumption\n            of residual normality.\n\n    See Also\n    --------\n    scipy.optimize.curve_fit :\n        Use non-linear least squares to fit a function to data.\n    scipy.optimize.leastsq :\n        Minimize the sum of squares of a set of equations.\n\n    Notes\n    -----\n    Missing values are considered pair-wise: if a value is missing in `x`,\n    the corresponding value in `y` is masked.\n\n    For compatibility with older versions of SciPy, the return value acts\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\n\n        slope, intercept, r, p, se = linregress(x, y)\n\n    With that style, however, the standard error of the intercept is not\n    available.  To have access to all the computed values, including the\n    standard error of the intercept, use the return value as an object\n    with attributes, e.g.::\n\n        result = linregress(x, y)\n        print(result.intercept, result.intercept_stderr)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n\n    Generate some data:\n\n    >>> x = rng.random(10)\n    >>> y = 1.6*x + rng.random(10)\n\n    Perform the linear regression:\n\n    >>> res = stats.linregress(x, y)\n\n    Coefficient of determination (R-squared):\n\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\n    R-squared: 0.717533\n\n    Plot the data along with the fitted line:\n\n    >>> plt.plot(x, y, 'o', label='original data')\n    >>> plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')\n    >>> plt.legend()\n    >>> plt.show()\n\n    Calculate 95% confidence interval on slope and intercept:\n\n    >>> # Two-sided inverse Students t-distribution\n    >>> # p - probability, df - degrees of freedom\n    >>> from scipy.stats import t\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\n\n    >>> ts = tinv(0.05, len(x)-2)\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\n    slope (95%): 1.453392 +/- 0.743465\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\n    intercept (95%): 0.616950 +/- 0.544475\n\n    \"\"\"\n    TINY = 1e-20\n    if y is None:\n        x = np.asarray(x)\n        if x.shape[0] == 2:\n            (x, y) = x\n        elif x.shape[1] == 2:\n            (x, y) = x.T\n        else:\n            raise ValueError(f'If only `x` is given as input, it has to be of shape (2, N) or (N, 2); provided shape was {x.shape}.')\n    else:\n        x = np.asarray(x)\n        y = np.asarray(y)\n    if x.size == 0 or y.size == 0:\n        raise ValueError('Inputs must not be empty.')\n    if np.amax(x) == np.amin(x) and len(x) > 1:\n        raise ValueError('Cannot calculate a linear regression if all x values are identical')\n    n = len(x)\n    xmean = np.mean(x, None)\n    ymean = np.mean(y, None)\n    (ssxm, ssxym, _, ssym) = np.cov(x, y, bias=1).flat\n    if ssxm == 0.0 or ssym == 0.0:\n        r = 0.0\n    else:\n        r = ssxym / np.sqrt(ssxm * ssym)\n        if r > 1.0:\n            r = 1.0\n        elif r < -1.0:\n            r = -1.0\n    slope = ssxym / ssxm\n    intercept = ymean - slope * xmean\n    if n == 2:\n        if y[0] == y[1]:\n            prob = 1.0\n        else:\n            prob = 0.0\n        slope_stderr = 0.0\n        intercept_stderr = 0.0\n    else:\n        df = n - 2\n        t = r * np.sqrt(df / ((1.0 - r + TINY) * (1.0 + r + TINY)))\n        (t, prob) = scipy.stats._stats_py._ttest_finish(df, t, alternative)\n        slope_stderr = np.sqrt((1 - r ** 2) * ssym / ssxm / df)\n        intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean ** 2)\n    return LinregressResult(slope=slope, intercept=intercept, rvalue=r, pvalue=prob, stderr=slope_stderr, intercept_stderr=intercept_stderr)",
        "mutated": [
            "def linregress(x, y=None, alternative='two-sided'):\n    if False:\n        i = 10\n    '\\n    Calculate a linear least-squares regression for two sets of measurements.\\n\\n    Parameters\\n    ----------\\n    x, y : array_like\\n        Two sets of measurements.  Both arrays should have the same length.  If\\n        only `x` is given (and ``y=None``), then it must be a two-dimensional\\n        array where one dimension has length 2.  The two sets of measurements\\n        are then found by splitting the array along the length-2 dimension. In\\n        the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is\\n        equivalent to ``linregress(x[0], x[1])``.\\n    alternative : {\\'two-sided\\', \\'less\\', \\'greater\\'}, optional\\n        Defines the alternative hypothesis. Default is \\'two-sided\\'.\\n        The following options are available:\\n\\n        * \\'two-sided\\': the slope of the regression line is nonzero\\n        * \\'less\\': the slope of the regression line is less than zero\\n        * \\'greater\\':  the slope of the regression line is greater than zero\\n\\n        .. versionadded:: 1.7.0\\n\\n    Returns\\n    -------\\n    result : ``LinregressResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Slope of the regression line.\\n        intercept : float\\n            Intercept of the regression line.\\n        rvalue : float\\n            The Pearson correlation coefficient. The square of ``rvalue``\\n            is equal to the coefficient of determination.\\n        pvalue : float\\n            The p-value for a hypothesis test whose null hypothesis is\\n            that the slope is zero, using Wald Test with t-distribution of\\n            the test statistic. See `alternative` above for alternative\\n            hypotheses.\\n        stderr : float\\n            Standard error of the estimated slope (gradient), under the\\n            assumption of residual normality.\\n        intercept_stderr : float\\n            Standard error of the estimated intercept, under the assumption\\n            of residual normality.\\n\\n    See Also\\n    --------\\n    scipy.optimize.curve_fit :\\n        Use non-linear least squares to fit a function to data.\\n    scipy.optimize.leastsq :\\n        Minimize the sum of squares of a set of equations.\\n\\n    Notes\\n    -----\\n    Missing values are considered pair-wise: if a value is missing in `x`,\\n    the corresponding value in `y` is masked.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\\n\\n        slope, intercept, r, p, se = linregress(x, y)\\n\\n    With that style, however, the standard error of the intercept is not\\n    available.  To have access to all the computed values, including the\\n    standard error of the intercept, use the return value as an object\\n    with attributes, e.g.::\\n\\n        result = linregress(x, y)\\n        print(result.intercept, result.intercept_stderr)\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> import matplotlib.pyplot as plt\\n    >>> from scipy import stats\\n    >>> rng = np.random.default_rng()\\n\\n    Generate some data:\\n\\n    >>> x = rng.random(10)\\n    >>> y = 1.6*x + rng.random(10)\\n\\n    Perform the linear regression:\\n\\n    >>> res = stats.linregress(x, y)\\n\\n    Coefficient of determination (R-squared):\\n\\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\\n    R-squared: 0.717533\\n\\n    Plot the data along with the fitted line:\\n\\n    >>> plt.plot(x, y, \\'o\\', label=\\'original data\\')\\n    >>> plt.plot(x, res.intercept + res.slope*x, \\'r\\', label=\\'fitted line\\')\\n    >>> plt.legend()\\n    >>> plt.show()\\n\\n    Calculate 95% confidence interval on slope and intercept:\\n\\n    >>> # Two-sided inverse Students t-distribution\\n    >>> # p - probability, df - degrees of freedom\\n    >>> from scipy.stats import t\\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\\n\\n    >>> ts = tinv(0.05, len(x)-2)\\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\\n    slope (95%): 1.453392 +/- 0.743465\\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\\n    intercept (95%): 0.616950 +/- 0.544475\\n\\n    '\n    TINY = 1e-20\n    if y is None:\n        x = np.asarray(x)\n        if x.shape[0] == 2:\n            (x, y) = x\n        elif x.shape[1] == 2:\n            (x, y) = x.T\n        else:\n            raise ValueError(f'If only `x` is given as input, it has to be of shape (2, N) or (N, 2); provided shape was {x.shape}.')\n    else:\n        x = np.asarray(x)\n        y = np.asarray(y)\n    if x.size == 0 or y.size == 0:\n        raise ValueError('Inputs must not be empty.')\n    if np.amax(x) == np.amin(x) and len(x) > 1:\n        raise ValueError('Cannot calculate a linear regression if all x values are identical')\n    n = len(x)\n    xmean = np.mean(x, None)\n    ymean = np.mean(y, None)\n    (ssxm, ssxym, _, ssym) = np.cov(x, y, bias=1).flat\n    if ssxm == 0.0 or ssym == 0.0:\n        r = 0.0\n    else:\n        r = ssxym / np.sqrt(ssxm * ssym)\n        if r > 1.0:\n            r = 1.0\n        elif r < -1.0:\n            r = -1.0\n    slope = ssxym / ssxm\n    intercept = ymean - slope * xmean\n    if n == 2:\n        if y[0] == y[1]:\n            prob = 1.0\n        else:\n            prob = 0.0\n        slope_stderr = 0.0\n        intercept_stderr = 0.0\n    else:\n        df = n - 2\n        t = r * np.sqrt(df / ((1.0 - r + TINY) * (1.0 + r + TINY)))\n        (t, prob) = scipy.stats._stats_py._ttest_finish(df, t, alternative)\n        slope_stderr = np.sqrt((1 - r ** 2) * ssym / ssxm / df)\n        intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean ** 2)\n    return LinregressResult(slope=slope, intercept=intercept, rvalue=r, pvalue=prob, stderr=slope_stderr, intercept_stderr=intercept_stderr)",
            "def linregress(x, y=None, alternative='two-sided'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculate a linear least-squares regression for two sets of measurements.\\n\\n    Parameters\\n    ----------\\n    x, y : array_like\\n        Two sets of measurements.  Both arrays should have the same length.  If\\n        only `x` is given (and ``y=None``), then it must be a two-dimensional\\n        array where one dimension has length 2.  The two sets of measurements\\n        are then found by splitting the array along the length-2 dimension. In\\n        the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is\\n        equivalent to ``linregress(x[0], x[1])``.\\n    alternative : {\\'two-sided\\', \\'less\\', \\'greater\\'}, optional\\n        Defines the alternative hypothesis. Default is \\'two-sided\\'.\\n        The following options are available:\\n\\n        * \\'two-sided\\': the slope of the regression line is nonzero\\n        * \\'less\\': the slope of the regression line is less than zero\\n        * \\'greater\\':  the slope of the regression line is greater than zero\\n\\n        .. versionadded:: 1.7.0\\n\\n    Returns\\n    -------\\n    result : ``LinregressResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Slope of the regression line.\\n        intercept : float\\n            Intercept of the regression line.\\n        rvalue : float\\n            The Pearson correlation coefficient. The square of ``rvalue``\\n            is equal to the coefficient of determination.\\n        pvalue : float\\n            The p-value for a hypothesis test whose null hypothesis is\\n            that the slope is zero, using Wald Test with t-distribution of\\n            the test statistic. See `alternative` above for alternative\\n            hypotheses.\\n        stderr : float\\n            Standard error of the estimated slope (gradient), under the\\n            assumption of residual normality.\\n        intercept_stderr : float\\n            Standard error of the estimated intercept, under the assumption\\n            of residual normality.\\n\\n    See Also\\n    --------\\n    scipy.optimize.curve_fit :\\n        Use non-linear least squares to fit a function to data.\\n    scipy.optimize.leastsq :\\n        Minimize the sum of squares of a set of equations.\\n\\n    Notes\\n    -----\\n    Missing values are considered pair-wise: if a value is missing in `x`,\\n    the corresponding value in `y` is masked.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\\n\\n        slope, intercept, r, p, se = linregress(x, y)\\n\\n    With that style, however, the standard error of the intercept is not\\n    available.  To have access to all the computed values, including the\\n    standard error of the intercept, use the return value as an object\\n    with attributes, e.g.::\\n\\n        result = linregress(x, y)\\n        print(result.intercept, result.intercept_stderr)\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> import matplotlib.pyplot as plt\\n    >>> from scipy import stats\\n    >>> rng = np.random.default_rng()\\n\\n    Generate some data:\\n\\n    >>> x = rng.random(10)\\n    >>> y = 1.6*x + rng.random(10)\\n\\n    Perform the linear regression:\\n\\n    >>> res = stats.linregress(x, y)\\n\\n    Coefficient of determination (R-squared):\\n\\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\\n    R-squared: 0.717533\\n\\n    Plot the data along with the fitted line:\\n\\n    >>> plt.plot(x, y, \\'o\\', label=\\'original data\\')\\n    >>> plt.plot(x, res.intercept + res.slope*x, \\'r\\', label=\\'fitted line\\')\\n    >>> plt.legend()\\n    >>> plt.show()\\n\\n    Calculate 95% confidence interval on slope and intercept:\\n\\n    >>> # Two-sided inverse Students t-distribution\\n    >>> # p - probability, df - degrees of freedom\\n    >>> from scipy.stats import t\\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\\n\\n    >>> ts = tinv(0.05, len(x)-2)\\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\\n    slope (95%): 1.453392 +/- 0.743465\\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\\n    intercept (95%): 0.616950 +/- 0.544475\\n\\n    '\n    TINY = 1e-20\n    if y is None:\n        x = np.asarray(x)\n        if x.shape[0] == 2:\n            (x, y) = x\n        elif x.shape[1] == 2:\n            (x, y) = x.T\n        else:\n            raise ValueError(f'If only `x` is given as input, it has to be of shape (2, N) or (N, 2); provided shape was {x.shape}.')\n    else:\n        x = np.asarray(x)\n        y = np.asarray(y)\n    if x.size == 0 or y.size == 0:\n        raise ValueError('Inputs must not be empty.')\n    if np.amax(x) == np.amin(x) and len(x) > 1:\n        raise ValueError('Cannot calculate a linear regression if all x values are identical')\n    n = len(x)\n    xmean = np.mean(x, None)\n    ymean = np.mean(y, None)\n    (ssxm, ssxym, _, ssym) = np.cov(x, y, bias=1).flat\n    if ssxm == 0.0 or ssym == 0.0:\n        r = 0.0\n    else:\n        r = ssxym / np.sqrt(ssxm * ssym)\n        if r > 1.0:\n            r = 1.0\n        elif r < -1.0:\n            r = -1.0\n    slope = ssxym / ssxm\n    intercept = ymean - slope * xmean\n    if n == 2:\n        if y[0] == y[1]:\n            prob = 1.0\n        else:\n            prob = 0.0\n        slope_stderr = 0.0\n        intercept_stderr = 0.0\n    else:\n        df = n - 2\n        t = r * np.sqrt(df / ((1.0 - r + TINY) * (1.0 + r + TINY)))\n        (t, prob) = scipy.stats._stats_py._ttest_finish(df, t, alternative)\n        slope_stderr = np.sqrt((1 - r ** 2) * ssym / ssxm / df)\n        intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean ** 2)\n    return LinregressResult(slope=slope, intercept=intercept, rvalue=r, pvalue=prob, stderr=slope_stderr, intercept_stderr=intercept_stderr)",
            "def linregress(x, y=None, alternative='two-sided'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculate a linear least-squares regression for two sets of measurements.\\n\\n    Parameters\\n    ----------\\n    x, y : array_like\\n        Two sets of measurements.  Both arrays should have the same length.  If\\n        only `x` is given (and ``y=None``), then it must be a two-dimensional\\n        array where one dimension has length 2.  The two sets of measurements\\n        are then found by splitting the array along the length-2 dimension. In\\n        the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is\\n        equivalent to ``linregress(x[0], x[1])``.\\n    alternative : {\\'two-sided\\', \\'less\\', \\'greater\\'}, optional\\n        Defines the alternative hypothesis. Default is \\'two-sided\\'.\\n        The following options are available:\\n\\n        * \\'two-sided\\': the slope of the regression line is nonzero\\n        * \\'less\\': the slope of the regression line is less than zero\\n        * \\'greater\\':  the slope of the regression line is greater than zero\\n\\n        .. versionadded:: 1.7.0\\n\\n    Returns\\n    -------\\n    result : ``LinregressResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Slope of the regression line.\\n        intercept : float\\n            Intercept of the regression line.\\n        rvalue : float\\n            The Pearson correlation coefficient. The square of ``rvalue``\\n            is equal to the coefficient of determination.\\n        pvalue : float\\n            The p-value for a hypothesis test whose null hypothesis is\\n            that the slope is zero, using Wald Test with t-distribution of\\n            the test statistic. See `alternative` above for alternative\\n            hypotheses.\\n        stderr : float\\n            Standard error of the estimated slope (gradient), under the\\n            assumption of residual normality.\\n        intercept_stderr : float\\n            Standard error of the estimated intercept, under the assumption\\n            of residual normality.\\n\\n    See Also\\n    --------\\n    scipy.optimize.curve_fit :\\n        Use non-linear least squares to fit a function to data.\\n    scipy.optimize.leastsq :\\n        Minimize the sum of squares of a set of equations.\\n\\n    Notes\\n    -----\\n    Missing values are considered pair-wise: if a value is missing in `x`,\\n    the corresponding value in `y` is masked.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\\n\\n        slope, intercept, r, p, se = linregress(x, y)\\n\\n    With that style, however, the standard error of the intercept is not\\n    available.  To have access to all the computed values, including the\\n    standard error of the intercept, use the return value as an object\\n    with attributes, e.g.::\\n\\n        result = linregress(x, y)\\n        print(result.intercept, result.intercept_stderr)\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> import matplotlib.pyplot as plt\\n    >>> from scipy import stats\\n    >>> rng = np.random.default_rng()\\n\\n    Generate some data:\\n\\n    >>> x = rng.random(10)\\n    >>> y = 1.6*x + rng.random(10)\\n\\n    Perform the linear regression:\\n\\n    >>> res = stats.linregress(x, y)\\n\\n    Coefficient of determination (R-squared):\\n\\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\\n    R-squared: 0.717533\\n\\n    Plot the data along with the fitted line:\\n\\n    >>> plt.plot(x, y, \\'o\\', label=\\'original data\\')\\n    >>> plt.plot(x, res.intercept + res.slope*x, \\'r\\', label=\\'fitted line\\')\\n    >>> plt.legend()\\n    >>> plt.show()\\n\\n    Calculate 95% confidence interval on slope and intercept:\\n\\n    >>> # Two-sided inverse Students t-distribution\\n    >>> # p - probability, df - degrees of freedom\\n    >>> from scipy.stats import t\\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\\n\\n    >>> ts = tinv(0.05, len(x)-2)\\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\\n    slope (95%): 1.453392 +/- 0.743465\\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\\n    intercept (95%): 0.616950 +/- 0.544475\\n\\n    '\n    TINY = 1e-20\n    if y is None:\n        x = np.asarray(x)\n        if x.shape[0] == 2:\n            (x, y) = x\n        elif x.shape[1] == 2:\n            (x, y) = x.T\n        else:\n            raise ValueError(f'If only `x` is given as input, it has to be of shape (2, N) or (N, 2); provided shape was {x.shape}.')\n    else:\n        x = np.asarray(x)\n        y = np.asarray(y)\n    if x.size == 0 or y.size == 0:\n        raise ValueError('Inputs must not be empty.')\n    if np.amax(x) == np.amin(x) and len(x) > 1:\n        raise ValueError('Cannot calculate a linear regression if all x values are identical')\n    n = len(x)\n    xmean = np.mean(x, None)\n    ymean = np.mean(y, None)\n    (ssxm, ssxym, _, ssym) = np.cov(x, y, bias=1).flat\n    if ssxm == 0.0 or ssym == 0.0:\n        r = 0.0\n    else:\n        r = ssxym / np.sqrt(ssxm * ssym)\n        if r > 1.0:\n            r = 1.0\n        elif r < -1.0:\n            r = -1.0\n    slope = ssxym / ssxm\n    intercept = ymean - slope * xmean\n    if n == 2:\n        if y[0] == y[1]:\n            prob = 1.0\n        else:\n            prob = 0.0\n        slope_stderr = 0.0\n        intercept_stderr = 0.0\n    else:\n        df = n - 2\n        t = r * np.sqrt(df / ((1.0 - r + TINY) * (1.0 + r + TINY)))\n        (t, prob) = scipy.stats._stats_py._ttest_finish(df, t, alternative)\n        slope_stderr = np.sqrt((1 - r ** 2) * ssym / ssxm / df)\n        intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean ** 2)\n    return LinregressResult(slope=slope, intercept=intercept, rvalue=r, pvalue=prob, stderr=slope_stderr, intercept_stderr=intercept_stderr)",
            "def linregress(x, y=None, alternative='two-sided'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculate a linear least-squares regression for two sets of measurements.\\n\\n    Parameters\\n    ----------\\n    x, y : array_like\\n        Two sets of measurements.  Both arrays should have the same length.  If\\n        only `x` is given (and ``y=None``), then it must be a two-dimensional\\n        array where one dimension has length 2.  The two sets of measurements\\n        are then found by splitting the array along the length-2 dimension. In\\n        the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is\\n        equivalent to ``linregress(x[0], x[1])``.\\n    alternative : {\\'two-sided\\', \\'less\\', \\'greater\\'}, optional\\n        Defines the alternative hypothesis. Default is \\'two-sided\\'.\\n        The following options are available:\\n\\n        * \\'two-sided\\': the slope of the regression line is nonzero\\n        * \\'less\\': the slope of the regression line is less than zero\\n        * \\'greater\\':  the slope of the regression line is greater than zero\\n\\n        .. versionadded:: 1.7.0\\n\\n    Returns\\n    -------\\n    result : ``LinregressResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Slope of the regression line.\\n        intercept : float\\n            Intercept of the regression line.\\n        rvalue : float\\n            The Pearson correlation coefficient. The square of ``rvalue``\\n            is equal to the coefficient of determination.\\n        pvalue : float\\n            The p-value for a hypothesis test whose null hypothesis is\\n            that the slope is zero, using Wald Test with t-distribution of\\n            the test statistic. See `alternative` above for alternative\\n            hypotheses.\\n        stderr : float\\n            Standard error of the estimated slope (gradient), under the\\n            assumption of residual normality.\\n        intercept_stderr : float\\n            Standard error of the estimated intercept, under the assumption\\n            of residual normality.\\n\\n    See Also\\n    --------\\n    scipy.optimize.curve_fit :\\n        Use non-linear least squares to fit a function to data.\\n    scipy.optimize.leastsq :\\n        Minimize the sum of squares of a set of equations.\\n\\n    Notes\\n    -----\\n    Missing values are considered pair-wise: if a value is missing in `x`,\\n    the corresponding value in `y` is masked.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\\n\\n        slope, intercept, r, p, se = linregress(x, y)\\n\\n    With that style, however, the standard error of the intercept is not\\n    available.  To have access to all the computed values, including the\\n    standard error of the intercept, use the return value as an object\\n    with attributes, e.g.::\\n\\n        result = linregress(x, y)\\n        print(result.intercept, result.intercept_stderr)\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> import matplotlib.pyplot as plt\\n    >>> from scipy import stats\\n    >>> rng = np.random.default_rng()\\n\\n    Generate some data:\\n\\n    >>> x = rng.random(10)\\n    >>> y = 1.6*x + rng.random(10)\\n\\n    Perform the linear regression:\\n\\n    >>> res = stats.linregress(x, y)\\n\\n    Coefficient of determination (R-squared):\\n\\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\\n    R-squared: 0.717533\\n\\n    Plot the data along with the fitted line:\\n\\n    >>> plt.plot(x, y, \\'o\\', label=\\'original data\\')\\n    >>> plt.plot(x, res.intercept + res.slope*x, \\'r\\', label=\\'fitted line\\')\\n    >>> plt.legend()\\n    >>> plt.show()\\n\\n    Calculate 95% confidence interval on slope and intercept:\\n\\n    >>> # Two-sided inverse Students t-distribution\\n    >>> # p - probability, df - degrees of freedom\\n    >>> from scipy.stats import t\\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\\n\\n    >>> ts = tinv(0.05, len(x)-2)\\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\\n    slope (95%): 1.453392 +/- 0.743465\\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\\n    intercept (95%): 0.616950 +/- 0.544475\\n\\n    '\n    TINY = 1e-20\n    if y is None:\n        x = np.asarray(x)\n        if x.shape[0] == 2:\n            (x, y) = x\n        elif x.shape[1] == 2:\n            (x, y) = x.T\n        else:\n            raise ValueError(f'If only `x` is given as input, it has to be of shape (2, N) or (N, 2); provided shape was {x.shape}.')\n    else:\n        x = np.asarray(x)\n        y = np.asarray(y)\n    if x.size == 0 or y.size == 0:\n        raise ValueError('Inputs must not be empty.')\n    if np.amax(x) == np.amin(x) and len(x) > 1:\n        raise ValueError('Cannot calculate a linear regression if all x values are identical')\n    n = len(x)\n    xmean = np.mean(x, None)\n    ymean = np.mean(y, None)\n    (ssxm, ssxym, _, ssym) = np.cov(x, y, bias=1).flat\n    if ssxm == 0.0 or ssym == 0.0:\n        r = 0.0\n    else:\n        r = ssxym / np.sqrt(ssxm * ssym)\n        if r > 1.0:\n            r = 1.0\n        elif r < -1.0:\n            r = -1.0\n    slope = ssxym / ssxm\n    intercept = ymean - slope * xmean\n    if n == 2:\n        if y[0] == y[1]:\n            prob = 1.0\n        else:\n            prob = 0.0\n        slope_stderr = 0.0\n        intercept_stderr = 0.0\n    else:\n        df = n - 2\n        t = r * np.sqrt(df / ((1.0 - r + TINY) * (1.0 + r + TINY)))\n        (t, prob) = scipy.stats._stats_py._ttest_finish(df, t, alternative)\n        slope_stderr = np.sqrt((1 - r ** 2) * ssym / ssxm / df)\n        intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean ** 2)\n    return LinregressResult(slope=slope, intercept=intercept, rvalue=r, pvalue=prob, stderr=slope_stderr, intercept_stderr=intercept_stderr)",
            "def linregress(x, y=None, alternative='two-sided'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculate a linear least-squares regression for two sets of measurements.\\n\\n    Parameters\\n    ----------\\n    x, y : array_like\\n        Two sets of measurements.  Both arrays should have the same length.  If\\n        only `x` is given (and ``y=None``), then it must be a two-dimensional\\n        array where one dimension has length 2.  The two sets of measurements\\n        are then found by splitting the array along the length-2 dimension. In\\n        the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is\\n        equivalent to ``linregress(x[0], x[1])``.\\n    alternative : {\\'two-sided\\', \\'less\\', \\'greater\\'}, optional\\n        Defines the alternative hypothesis. Default is \\'two-sided\\'.\\n        The following options are available:\\n\\n        * \\'two-sided\\': the slope of the regression line is nonzero\\n        * \\'less\\': the slope of the regression line is less than zero\\n        * \\'greater\\':  the slope of the regression line is greater than zero\\n\\n        .. versionadded:: 1.7.0\\n\\n    Returns\\n    -------\\n    result : ``LinregressResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Slope of the regression line.\\n        intercept : float\\n            Intercept of the regression line.\\n        rvalue : float\\n            The Pearson correlation coefficient. The square of ``rvalue``\\n            is equal to the coefficient of determination.\\n        pvalue : float\\n            The p-value for a hypothesis test whose null hypothesis is\\n            that the slope is zero, using Wald Test with t-distribution of\\n            the test statistic. See `alternative` above for alternative\\n            hypotheses.\\n        stderr : float\\n            Standard error of the estimated slope (gradient), under the\\n            assumption of residual normality.\\n        intercept_stderr : float\\n            Standard error of the estimated intercept, under the assumption\\n            of residual normality.\\n\\n    See Also\\n    --------\\n    scipy.optimize.curve_fit :\\n        Use non-linear least squares to fit a function to data.\\n    scipy.optimize.leastsq :\\n        Minimize the sum of squares of a set of equations.\\n\\n    Notes\\n    -----\\n    Missing values are considered pair-wise: if a value is missing in `x`,\\n    the corresponding value in `y` is masked.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\\n\\n        slope, intercept, r, p, se = linregress(x, y)\\n\\n    With that style, however, the standard error of the intercept is not\\n    available.  To have access to all the computed values, including the\\n    standard error of the intercept, use the return value as an object\\n    with attributes, e.g.::\\n\\n        result = linregress(x, y)\\n        print(result.intercept, result.intercept_stderr)\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> import matplotlib.pyplot as plt\\n    >>> from scipy import stats\\n    >>> rng = np.random.default_rng()\\n\\n    Generate some data:\\n\\n    >>> x = rng.random(10)\\n    >>> y = 1.6*x + rng.random(10)\\n\\n    Perform the linear regression:\\n\\n    >>> res = stats.linregress(x, y)\\n\\n    Coefficient of determination (R-squared):\\n\\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\\n    R-squared: 0.717533\\n\\n    Plot the data along with the fitted line:\\n\\n    >>> plt.plot(x, y, \\'o\\', label=\\'original data\\')\\n    >>> plt.plot(x, res.intercept + res.slope*x, \\'r\\', label=\\'fitted line\\')\\n    >>> plt.legend()\\n    >>> plt.show()\\n\\n    Calculate 95% confidence interval on slope and intercept:\\n\\n    >>> # Two-sided inverse Students t-distribution\\n    >>> # p - probability, df - degrees of freedom\\n    >>> from scipy.stats import t\\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\\n\\n    >>> ts = tinv(0.05, len(x)-2)\\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\\n    slope (95%): 1.453392 +/- 0.743465\\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\\n    intercept (95%): 0.616950 +/- 0.544475\\n\\n    '\n    TINY = 1e-20\n    if y is None:\n        x = np.asarray(x)\n        if x.shape[0] == 2:\n            (x, y) = x\n        elif x.shape[1] == 2:\n            (x, y) = x.T\n        else:\n            raise ValueError(f'If only `x` is given as input, it has to be of shape (2, N) or (N, 2); provided shape was {x.shape}.')\n    else:\n        x = np.asarray(x)\n        y = np.asarray(y)\n    if x.size == 0 or y.size == 0:\n        raise ValueError('Inputs must not be empty.')\n    if np.amax(x) == np.amin(x) and len(x) > 1:\n        raise ValueError('Cannot calculate a linear regression if all x values are identical')\n    n = len(x)\n    xmean = np.mean(x, None)\n    ymean = np.mean(y, None)\n    (ssxm, ssxym, _, ssym) = np.cov(x, y, bias=1).flat\n    if ssxm == 0.0 or ssym == 0.0:\n        r = 0.0\n    else:\n        r = ssxym / np.sqrt(ssxm * ssym)\n        if r > 1.0:\n            r = 1.0\n        elif r < -1.0:\n            r = -1.0\n    slope = ssxym / ssxm\n    intercept = ymean - slope * xmean\n    if n == 2:\n        if y[0] == y[1]:\n            prob = 1.0\n        else:\n            prob = 0.0\n        slope_stderr = 0.0\n        intercept_stderr = 0.0\n    else:\n        df = n - 2\n        t = r * np.sqrt(df / ((1.0 - r + TINY) * (1.0 + r + TINY)))\n        (t, prob) = scipy.stats._stats_py._ttest_finish(df, t, alternative)\n        slope_stderr = np.sqrt((1 - r ** 2) * ssym / ssxm / df)\n        intercept_stderr = slope_stderr * np.sqrt(ssxm + xmean ** 2)\n    return LinregressResult(slope=slope, intercept=intercept, rvalue=r, pvalue=prob, stderr=slope_stderr, intercept_stderr=intercept_stderr)"
        ]
    },
    {
        "func_name": "theilslopes",
        "original": "def theilslopes(y, x=None, alpha=0.95, method='separate'):\n    \"\"\"\n    Computes the Theil-Sen estimator for a set of points (x, y).\n\n    `theilslopes` implements a method for robust linear regression.  It\n    computes the slope as the median of all slopes between paired values.\n\n    Parameters\n    ----------\n    y : array_like\n        Dependent variable.\n    x : array_like or None, optional\n        Independent variable. If None, use ``arange(len(y))`` instead.\n    alpha : float, optional\n        Confidence degree between 0 and 1. Default is 95% confidence.\n        Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\n        interpreted as \"find the 90% confidence interval\".\n    method : {'joint', 'separate'}, optional\n        Method to be used for computing estimate for intercept.\n        Following methods are supported,\n\n            * 'joint': Uses np.median(y - slope * x) as intercept.\n            * 'separate': Uses np.median(y) - slope * np.median(x)\n                          as intercept.\n\n        The default is 'separate'.\n\n        .. versionadded:: 1.8.0\n\n    Returns\n    -------\n    result : ``TheilslopesResult`` instance\n        The return value is an object with the following attributes:\n\n        slope : float\n            Theil slope.\n        intercept : float\n            Intercept of the Theil line.\n        low_slope : float\n            Lower bound of the confidence interval on `slope`.\n        high_slope : float\n            Upper bound of the confidence interval on `slope`.\n\n    See Also\n    --------\n    siegelslopes : a similar technique using repeated medians\n\n    Notes\n    -----\n    The implementation of `theilslopes` follows [1]_. The intercept is\n    not defined in [1]_, and here it is defined as ``median(y) -\n    slope*median(x)``, which is given in [3]_. Other definitions of\n    the intercept exist in the literature such as  ``median(y - slope*x)``\n    in [4]_. The approach to compute the intercept can be determined by the\n    parameter ``method``. A confidence interval for the intercept is not\n    given as this question is not addressed in [1]_.\n\n    For compatibility with older versions of SciPy, the return value acts\n    like a ``namedtuple`` of length 4, with fields ``slope``, ``intercept``,\n    ``low_slope``, and ``high_slope``, so one can continue to write::\n\n        slope, intercept, low_slope, high_slope = theilslopes(y, x)\n\n    References\n    ----------\n    .. [1] P.K. Sen, \"Estimates of the regression coefficient based on\n           Kendall's tau\", J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\n    .. [2] H. Theil, \"A rank-invariant method of linear and polynomial\n           regression analysis I, II and III\",  Nederl. Akad. Wetensch., Proc.\n           53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\n    .. [3] W.L. Conover, \"Practical nonparametric statistics\", 2nd ed.,\n           John Wiley and Sons, New York, pp. 493.\n    .. [4] https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    >>> x = np.linspace(-5, 5, num=150)\n    >>> y = x + np.random.normal(size=x.size)\n    >>> y[11:15] += 10  # add outliers\n    >>> y[-5:] -= 7\n\n    Compute the slope, intercept and 90% confidence interval.  For comparison,\n    also compute the least-squares fit with `linregress`:\n\n    >>> res = stats.theilslopes(y, x, 0.90, method='separate')\n    >>> lsq_res = stats.linregress(x, y)\n\n    Plot the results. The Theil-Sen regression line is shown in red, with the\n    dashed red lines illustrating the confidence interval of the slope (note\n    that the dashed red lines are not the confidence interval of the regression\n    as the confidence interval of the intercept is not included). The green\n    line shows the least-squares fit for comparison.\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(x, y, 'b.')\n    >>> ax.plot(x, res[1] + res[0] * x, 'r-')\n    >>> ax.plot(x, res[1] + res[2] * x, 'r--')\n    >>> ax.plot(x, res[1] + res[3] * x, 'r--')\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')\n    >>> plt.show()\n\n    \"\"\"\n    if method not in ['joint', 'separate']:\n        raise ValueError(\"method must be either 'joint' or 'separate'.'{}' is invalid.\".format(method))\n    y = np.array(y).flatten()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.array(x, dtype=float).flatten()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    deltax = x[:, np.newaxis] - x\n    deltay = y[:, np.newaxis] - y\n    slopes = deltay[deltax > 0] / deltax[deltax > 0]\n    if not slopes.size:\n        msg = 'All `x` coordinates are identical.'\n        warnings.warn(msg, RuntimeWarning, stacklevel=2)\n    slopes.sort()\n    medslope = np.median(slopes)\n    if method == 'joint':\n        medinter = np.median(y - medslope * x)\n    else:\n        medinter = np.median(y) - medslope * np.median(x)\n    if alpha > 0.5:\n        alpha = 1.0 - alpha\n    z = distributions.norm.ppf(alpha / 2.0)\n    (_, nxreps) = _find_repeats(x)\n    (_, nyreps) = _find_repeats(y)\n    nt = len(slopes)\n    ny = len(y)\n    sigsq = 1 / 18.0 * (ny * (ny - 1) * (2 * ny + 5) - sum((k * (k - 1) * (2 * k + 5) for k in nxreps)) - sum((k * (k - 1) * (2 * k + 5) for k in nyreps)))\n    try:\n        sigma = np.sqrt(sigsq)\n        Ru = min(int(np.round((nt - z * sigma) / 2.0)), len(slopes) - 1)\n        Rl = max(int(np.round((nt + z * sigma) / 2.0)) - 1, 0)\n        delta = slopes[[Rl, Ru]]\n    except (ValueError, IndexError):\n        delta = (np.nan, np.nan)\n    return TheilslopesResult(slope=medslope, intercept=medinter, low_slope=delta[0], high_slope=delta[1])",
        "mutated": [
            "def theilslopes(y, x=None, alpha=0.95, method='separate'):\n    if False:\n        i = 10\n    '\\n    Computes the Theil-Sen estimator for a set of points (x, y).\\n\\n    `theilslopes` implements a method for robust linear regression.  It\\n    computes the slope as the median of all slopes between paired values.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Dependent variable.\\n    x : array_like or None, optional\\n        Independent variable. If None, use ``arange(len(y))`` instead.\\n    alpha : float, optional\\n        Confidence degree between 0 and 1. Default is 95% confidence.\\n        Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\\n        interpreted as \"find the 90% confidence interval\".\\n    method : {\\'joint\\', \\'separate\\'}, optional\\n        Method to be used for computing estimate for intercept.\\n        Following methods are supported,\\n\\n            * \\'joint\\': Uses np.median(y - slope * x) as intercept.\\n            * \\'separate\\': Uses np.median(y) - slope * np.median(x)\\n                          as intercept.\\n\\n        The default is \\'separate\\'.\\n\\n        .. versionadded:: 1.8.0\\n\\n    Returns\\n    -------\\n    result : ``TheilslopesResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Theil slope.\\n        intercept : float\\n            Intercept of the Theil line.\\n        low_slope : float\\n            Lower bound of the confidence interval on `slope`.\\n        high_slope : float\\n            Upper bound of the confidence interval on `slope`.\\n\\n    See Also\\n    --------\\n    siegelslopes : a similar technique using repeated medians\\n\\n    Notes\\n    -----\\n    The implementation of `theilslopes` follows [1]_. The intercept is\\n    not defined in [1]_, and here it is defined as ``median(y) -\\n    slope*median(x)``, which is given in [3]_. Other definitions of\\n    the intercept exist in the literature such as  ``median(y - slope*x)``\\n    in [4]_. The approach to compute the intercept can be determined by the\\n    parameter ``method``. A confidence interval for the intercept is not\\n    given as this question is not addressed in [1]_.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 4, with fields ``slope``, ``intercept``,\\n    ``low_slope``, and ``high_slope``, so one can continue to write::\\n\\n        slope, intercept, low_slope, high_slope = theilslopes(y, x)\\n\\n    References\\n    ----------\\n    .. [1] P.K. Sen, \"Estimates of the regression coefficient based on\\n           Kendall\\'s tau\", J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\\n    .. [2] H. Theil, \"A rank-invariant method of linear and polynomial\\n           regression analysis I, II and III\",  Nederl. Akad. Wetensch., Proc.\\n           53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\\n    .. [3] W.L. Conover, \"Practical nonparametric statistics\", 2nd ed.,\\n           John Wiley and Sons, New York, pp. 493.\\n    .. [4] https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> x = np.linspace(-5, 5, num=150)\\n    >>> y = x + np.random.normal(size=x.size)\\n    >>> y[11:15] += 10  # add outliers\\n    >>> y[-5:] -= 7\\n\\n    Compute the slope, intercept and 90% confidence interval.  For comparison,\\n    also compute the least-squares fit with `linregress`:\\n\\n    >>> res = stats.theilslopes(y, x, 0.90, method=\\'separate\\')\\n    >>> lsq_res = stats.linregress(x, y)\\n\\n    Plot the results. The Theil-Sen regression line is shown in red, with the\\n    dashed red lines illustrating the confidence interval of the slope (note\\n    that the dashed red lines are not the confidence interval of the regression\\n    as the confidence interval of the intercept is not included). The green\\n    line shows the least-squares fit for comparison.\\n\\n    >>> fig = plt.figure()\\n    >>> ax = fig.add_subplot(111)\\n    >>> ax.plot(x, y, \\'b.\\')\\n    >>> ax.plot(x, res[1] + res[0] * x, \\'r-\\')\\n    >>> ax.plot(x, res[1] + res[2] * x, \\'r--\\')\\n    >>> ax.plot(x, res[1] + res[3] * x, \\'r--\\')\\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \\'g-\\')\\n    >>> plt.show()\\n\\n    '\n    if method not in ['joint', 'separate']:\n        raise ValueError(\"method must be either 'joint' or 'separate'.'{}' is invalid.\".format(method))\n    y = np.array(y).flatten()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.array(x, dtype=float).flatten()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    deltax = x[:, np.newaxis] - x\n    deltay = y[:, np.newaxis] - y\n    slopes = deltay[deltax > 0] / deltax[deltax > 0]\n    if not slopes.size:\n        msg = 'All `x` coordinates are identical.'\n        warnings.warn(msg, RuntimeWarning, stacklevel=2)\n    slopes.sort()\n    medslope = np.median(slopes)\n    if method == 'joint':\n        medinter = np.median(y - medslope * x)\n    else:\n        medinter = np.median(y) - medslope * np.median(x)\n    if alpha > 0.5:\n        alpha = 1.0 - alpha\n    z = distributions.norm.ppf(alpha / 2.0)\n    (_, nxreps) = _find_repeats(x)\n    (_, nyreps) = _find_repeats(y)\n    nt = len(slopes)\n    ny = len(y)\n    sigsq = 1 / 18.0 * (ny * (ny - 1) * (2 * ny + 5) - sum((k * (k - 1) * (2 * k + 5) for k in nxreps)) - sum((k * (k - 1) * (2 * k + 5) for k in nyreps)))\n    try:\n        sigma = np.sqrt(sigsq)\n        Ru = min(int(np.round((nt - z * sigma) / 2.0)), len(slopes) - 1)\n        Rl = max(int(np.round((nt + z * sigma) / 2.0)) - 1, 0)\n        delta = slopes[[Rl, Ru]]\n    except (ValueError, IndexError):\n        delta = (np.nan, np.nan)\n    return TheilslopesResult(slope=medslope, intercept=medinter, low_slope=delta[0], high_slope=delta[1])",
            "def theilslopes(y, x=None, alpha=0.95, method='separate'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Computes the Theil-Sen estimator for a set of points (x, y).\\n\\n    `theilslopes` implements a method for robust linear regression.  It\\n    computes the slope as the median of all slopes between paired values.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Dependent variable.\\n    x : array_like or None, optional\\n        Independent variable. If None, use ``arange(len(y))`` instead.\\n    alpha : float, optional\\n        Confidence degree between 0 and 1. Default is 95% confidence.\\n        Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\\n        interpreted as \"find the 90% confidence interval\".\\n    method : {\\'joint\\', \\'separate\\'}, optional\\n        Method to be used for computing estimate for intercept.\\n        Following methods are supported,\\n\\n            * \\'joint\\': Uses np.median(y - slope * x) as intercept.\\n            * \\'separate\\': Uses np.median(y) - slope * np.median(x)\\n                          as intercept.\\n\\n        The default is \\'separate\\'.\\n\\n        .. versionadded:: 1.8.0\\n\\n    Returns\\n    -------\\n    result : ``TheilslopesResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Theil slope.\\n        intercept : float\\n            Intercept of the Theil line.\\n        low_slope : float\\n            Lower bound of the confidence interval on `slope`.\\n        high_slope : float\\n            Upper bound of the confidence interval on `slope`.\\n\\n    See Also\\n    --------\\n    siegelslopes : a similar technique using repeated medians\\n\\n    Notes\\n    -----\\n    The implementation of `theilslopes` follows [1]_. The intercept is\\n    not defined in [1]_, and here it is defined as ``median(y) -\\n    slope*median(x)``, which is given in [3]_. Other definitions of\\n    the intercept exist in the literature such as  ``median(y - slope*x)``\\n    in [4]_. The approach to compute the intercept can be determined by the\\n    parameter ``method``. A confidence interval for the intercept is not\\n    given as this question is not addressed in [1]_.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 4, with fields ``slope``, ``intercept``,\\n    ``low_slope``, and ``high_slope``, so one can continue to write::\\n\\n        slope, intercept, low_slope, high_slope = theilslopes(y, x)\\n\\n    References\\n    ----------\\n    .. [1] P.K. Sen, \"Estimates of the regression coefficient based on\\n           Kendall\\'s tau\", J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\\n    .. [2] H. Theil, \"A rank-invariant method of linear and polynomial\\n           regression analysis I, II and III\",  Nederl. Akad. Wetensch., Proc.\\n           53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\\n    .. [3] W.L. Conover, \"Practical nonparametric statistics\", 2nd ed.,\\n           John Wiley and Sons, New York, pp. 493.\\n    .. [4] https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> x = np.linspace(-5, 5, num=150)\\n    >>> y = x + np.random.normal(size=x.size)\\n    >>> y[11:15] += 10  # add outliers\\n    >>> y[-5:] -= 7\\n\\n    Compute the slope, intercept and 90% confidence interval.  For comparison,\\n    also compute the least-squares fit with `linregress`:\\n\\n    >>> res = stats.theilslopes(y, x, 0.90, method=\\'separate\\')\\n    >>> lsq_res = stats.linregress(x, y)\\n\\n    Plot the results. The Theil-Sen regression line is shown in red, with the\\n    dashed red lines illustrating the confidence interval of the slope (note\\n    that the dashed red lines are not the confidence interval of the regression\\n    as the confidence interval of the intercept is not included). The green\\n    line shows the least-squares fit for comparison.\\n\\n    >>> fig = plt.figure()\\n    >>> ax = fig.add_subplot(111)\\n    >>> ax.plot(x, y, \\'b.\\')\\n    >>> ax.plot(x, res[1] + res[0] * x, \\'r-\\')\\n    >>> ax.plot(x, res[1] + res[2] * x, \\'r--\\')\\n    >>> ax.plot(x, res[1] + res[3] * x, \\'r--\\')\\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \\'g-\\')\\n    >>> plt.show()\\n\\n    '\n    if method not in ['joint', 'separate']:\n        raise ValueError(\"method must be either 'joint' or 'separate'.'{}' is invalid.\".format(method))\n    y = np.array(y).flatten()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.array(x, dtype=float).flatten()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    deltax = x[:, np.newaxis] - x\n    deltay = y[:, np.newaxis] - y\n    slopes = deltay[deltax > 0] / deltax[deltax > 0]\n    if not slopes.size:\n        msg = 'All `x` coordinates are identical.'\n        warnings.warn(msg, RuntimeWarning, stacklevel=2)\n    slopes.sort()\n    medslope = np.median(slopes)\n    if method == 'joint':\n        medinter = np.median(y - medslope * x)\n    else:\n        medinter = np.median(y) - medslope * np.median(x)\n    if alpha > 0.5:\n        alpha = 1.0 - alpha\n    z = distributions.norm.ppf(alpha / 2.0)\n    (_, nxreps) = _find_repeats(x)\n    (_, nyreps) = _find_repeats(y)\n    nt = len(slopes)\n    ny = len(y)\n    sigsq = 1 / 18.0 * (ny * (ny - 1) * (2 * ny + 5) - sum((k * (k - 1) * (2 * k + 5) for k in nxreps)) - sum((k * (k - 1) * (2 * k + 5) for k in nyreps)))\n    try:\n        sigma = np.sqrt(sigsq)\n        Ru = min(int(np.round((nt - z * sigma) / 2.0)), len(slopes) - 1)\n        Rl = max(int(np.round((nt + z * sigma) / 2.0)) - 1, 0)\n        delta = slopes[[Rl, Ru]]\n    except (ValueError, IndexError):\n        delta = (np.nan, np.nan)\n    return TheilslopesResult(slope=medslope, intercept=medinter, low_slope=delta[0], high_slope=delta[1])",
            "def theilslopes(y, x=None, alpha=0.95, method='separate'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Computes the Theil-Sen estimator for a set of points (x, y).\\n\\n    `theilslopes` implements a method for robust linear regression.  It\\n    computes the slope as the median of all slopes between paired values.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Dependent variable.\\n    x : array_like or None, optional\\n        Independent variable. If None, use ``arange(len(y))`` instead.\\n    alpha : float, optional\\n        Confidence degree between 0 and 1. Default is 95% confidence.\\n        Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\\n        interpreted as \"find the 90% confidence interval\".\\n    method : {\\'joint\\', \\'separate\\'}, optional\\n        Method to be used for computing estimate for intercept.\\n        Following methods are supported,\\n\\n            * \\'joint\\': Uses np.median(y - slope * x) as intercept.\\n            * \\'separate\\': Uses np.median(y) - slope * np.median(x)\\n                          as intercept.\\n\\n        The default is \\'separate\\'.\\n\\n        .. versionadded:: 1.8.0\\n\\n    Returns\\n    -------\\n    result : ``TheilslopesResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Theil slope.\\n        intercept : float\\n            Intercept of the Theil line.\\n        low_slope : float\\n            Lower bound of the confidence interval on `slope`.\\n        high_slope : float\\n            Upper bound of the confidence interval on `slope`.\\n\\n    See Also\\n    --------\\n    siegelslopes : a similar technique using repeated medians\\n\\n    Notes\\n    -----\\n    The implementation of `theilslopes` follows [1]_. The intercept is\\n    not defined in [1]_, and here it is defined as ``median(y) -\\n    slope*median(x)``, which is given in [3]_. Other definitions of\\n    the intercept exist in the literature such as  ``median(y - slope*x)``\\n    in [4]_. The approach to compute the intercept can be determined by the\\n    parameter ``method``. A confidence interval for the intercept is not\\n    given as this question is not addressed in [1]_.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 4, with fields ``slope``, ``intercept``,\\n    ``low_slope``, and ``high_slope``, so one can continue to write::\\n\\n        slope, intercept, low_slope, high_slope = theilslopes(y, x)\\n\\n    References\\n    ----------\\n    .. [1] P.K. Sen, \"Estimates of the regression coefficient based on\\n           Kendall\\'s tau\", J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\\n    .. [2] H. Theil, \"A rank-invariant method of linear and polynomial\\n           regression analysis I, II and III\",  Nederl. Akad. Wetensch., Proc.\\n           53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\\n    .. [3] W.L. Conover, \"Practical nonparametric statistics\", 2nd ed.,\\n           John Wiley and Sons, New York, pp. 493.\\n    .. [4] https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> x = np.linspace(-5, 5, num=150)\\n    >>> y = x + np.random.normal(size=x.size)\\n    >>> y[11:15] += 10  # add outliers\\n    >>> y[-5:] -= 7\\n\\n    Compute the slope, intercept and 90% confidence interval.  For comparison,\\n    also compute the least-squares fit with `linregress`:\\n\\n    >>> res = stats.theilslopes(y, x, 0.90, method=\\'separate\\')\\n    >>> lsq_res = stats.linregress(x, y)\\n\\n    Plot the results. The Theil-Sen regression line is shown in red, with the\\n    dashed red lines illustrating the confidence interval of the slope (note\\n    that the dashed red lines are not the confidence interval of the regression\\n    as the confidence interval of the intercept is not included). The green\\n    line shows the least-squares fit for comparison.\\n\\n    >>> fig = plt.figure()\\n    >>> ax = fig.add_subplot(111)\\n    >>> ax.plot(x, y, \\'b.\\')\\n    >>> ax.plot(x, res[1] + res[0] * x, \\'r-\\')\\n    >>> ax.plot(x, res[1] + res[2] * x, \\'r--\\')\\n    >>> ax.plot(x, res[1] + res[3] * x, \\'r--\\')\\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \\'g-\\')\\n    >>> plt.show()\\n\\n    '\n    if method not in ['joint', 'separate']:\n        raise ValueError(\"method must be either 'joint' or 'separate'.'{}' is invalid.\".format(method))\n    y = np.array(y).flatten()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.array(x, dtype=float).flatten()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    deltax = x[:, np.newaxis] - x\n    deltay = y[:, np.newaxis] - y\n    slopes = deltay[deltax > 0] / deltax[deltax > 0]\n    if not slopes.size:\n        msg = 'All `x` coordinates are identical.'\n        warnings.warn(msg, RuntimeWarning, stacklevel=2)\n    slopes.sort()\n    medslope = np.median(slopes)\n    if method == 'joint':\n        medinter = np.median(y - medslope * x)\n    else:\n        medinter = np.median(y) - medslope * np.median(x)\n    if alpha > 0.5:\n        alpha = 1.0 - alpha\n    z = distributions.norm.ppf(alpha / 2.0)\n    (_, nxreps) = _find_repeats(x)\n    (_, nyreps) = _find_repeats(y)\n    nt = len(slopes)\n    ny = len(y)\n    sigsq = 1 / 18.0 * (ny * (ny - 1) * (2 * ny + 5) - sum((k * (k - 1) * (2 * k + 5) for k in nxreps)) - sum((k * (k - 1) * (2 * k + 5) for k in nyreps)))\n    try:\n        sigma = np.sqrt(sigsq)\n        Ru = min(int(np.round((nt - z * sigma) / 2.0)), len(slopes) - 1)\n        Rl = max(int(np.round((nt + z * sigma) / 2.0)) - 1, 0)\n        delta = slopes[[Rl, Ru]]\n    except (ValueError, IndexError):\n        delta = (np.nan, np.nan)\n    return TheilslopesResult(slope=medslope, intercept=medinter, low_slope=delta[0], high_slope=delta[1])",
            "def theilslopes(y, x=None, alpha=0.95, method='separate'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Computes the Theil-Sen estimator for a set of points (x, y).\\n\\n    `theilslopes` implements a method for robust linear regression.  It\\n    computes the slope as the median of all slopes between paired values.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Dependent variable.\\n    x : array_like or None, optional\\n        Independent variable. If None, use ``arange(len(y))`` instead.\\n    alpha : float, optional\\n        Confidence degree between 0 and 1. Default is 95% confidence.\\n        Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\\n        interpreted as \"find the 90% confidence interval\".\\n    method : {\\'joint\\', \\'separate\\'}, optional\\n        Method to be used for computing estimate for intercept.\\n        Following methods are supported,\\n\\n            * \\'joint\\': Uses np.median(y - slope * x) as intercept.\\n            * \\'separate\\': Uses np.median(y) - slope * np.median(x)\\n                          as intercept.\\n\\n        The default is \\'separate\\'.\\n\\n        .. versionadded:: 1.8.0\\n\\n    Returns\\n    -------\\n    result : ``TheilslopesResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Theil slope.\\n        intercept : float\\n            Intercept of the Theil line.\\n        low_slope : float\\n            Lower bound of the confidence interval on `slope`.\\n        high_slope : float\\n            Upper bound of the confidence interval on `slope`.\\n\\n    See Also\\n    --------\\n    siegelslopes : a similar technique using repeated medians\\n\\n    Notes\\n    -----\\n    The implementation of `theilslopes` follows [1]_. The intercept is\\n    not defined in [1]_, and here it is defined as ``median(y) -\\n    slope*median(x)``, which is given in [3]_. Other definitions of\\n    the intercept exist in the literature such as  ``median(y - slope*x)``\\n    in [4]_. The approach to compute the intercept can be determined by the\\n    parameter ``method``. A confidence interval for the intercept is not\\n    given as this question is not addressed in [1]_.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 4, with fields ``slope``, ``intercept``,\\n    ``low_slope``, and ``high_slope``, so one can continue to write::\\n\\n        slope, intercept, low_slope, high_slope = theilslopes(y, x)\\n\\n    References\\n    ----------\\n    .. [1] P.K. Sen, \"Estimates of the regression coefficient based on\\n           Kendall\\'s tau\", J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\\n    .. [2] H. Theil, \"A rank-invariant method of linear and polynomial\\n           regression analysis I, II and III\",  Nederl. Akad. Wetensch., Proc.\\n           53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\\n    .. [3] W.L. Conover, \"Practical nonparametric statistics\", 2nd ed.,\\n           John Wiley and Sons, New York, pp. 493.\\n    .. [4] https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> x = np.linspace(-5, 5, num=150)\\n    >>> y = x + np.random.normal(size=x.size)\\n    >>> y[11:15] += 10  # add outliers\\n    >>> y[-5:] -= 7\\n\\n    Compute the slope, intercept and 90% confidence interval.  For comparison,\\n    also compute the least-squares fit with `linregress`:\\n\\n    >>> res = stats.theilslopes(y, x, 0.90, method=\\'separate\\')\\n    >>> lsq_res = stats.linregress(x, y)\\n\\n    Plot the results. The Theil-Sen regression line is shown in red, with the\\n    dashed red lines illustrating the confidence interval of the slope (note\\n    that the dashed red lines are not the confidence interval of the regression\\n    as the confidence interval of the intercept is not included). The green\\n    line shows the least-squares fit for comparison.\\n\\n    >>> fig = plt.figure()\\n    >>> ax = fig.add_subplot(111)\\n    >>> ax.plot(x, y, \\'b.\\')\\n    >>> ax.plot(x, res[1] + res[0] * x, \\'r-\\')\\n    >>> ax.plot(x, res[1] + res[2] * x, \\'r--\\')\\n    >>> ax.plot(x, res[1] + res[3] * x, \\'r--\\')\\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \\'g-\\')\\n    >>> plt.show()\\n\\n    '\n    if method not in ['joint', 'separate']:\n        raise ValueError(\"method must be either 'joint' or 'separate'.'{}' is invalid.\".format(method))\n    y = np.array(y).flatten()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.array(x, dtype=float).flatten()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    deltax = x[:, np.newaxis] - x\n    deltay = y[:, np.newaxis] - y\n    slopes = deltay[deltax > 0] / deltax[deltax > 0]\n    if not slopes.size:\n        msg = 'All `x` coordinates are identical.'\n        warnings.warn(msg, RuntimeWarning, stacklevel=2)\n    slopes.sort()\n    medslope = np.median(slopes)\n    if method == 'joint':\n        medinter = np.median(y - medslope * x)\n    else:\n        medinter = np.median(y) - medslope * np.median(x)\n    if alpha > 0.5:\n        alpha = 1.0 - alpha\n    z = distributions.norm.ppf(alpha / 2.0)\n    (_, nxreps) = _find_repeats(x)\n    (_, nyreps) = _find_repeats(y)\n    nt = len(slopes)\n    ny = len(y)\n    sigsq = 1 / 18.0 * (ny * (ny - 1) * (2 * ny + 5) - sum((k * (k - 1) * (2 * k + 5) for k in nxreps)) - sum((k * (k - 1) * (2 * k + 5) for k in nyreps)))\n    try:\n        sigma = np.sqrt(sigsq)\n        Ru = min(int(np.round((nt - z * sigma) / 2.0)), len(slopes) - 1)\n        Rl = max(int(np.round((nt + z * sigma) / 2.0)) - 1, 0)\n        delta = slopes[[Rl, Ru]]\n    except (ValueError, IndexError):\n        delta = (np.nan, np.nan)\n    return TheilslopesResult(slope=medslope, intercept=medinter, low_slope=delta[0], high_slope=delta[1])",
            "def theilslopes(y, x=None, alpha=0.95, method='separate'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Computes the Theil-Sen estimator for a set of points (x, y).\\n\\n    `theilslopes` implements a method for robust linear regression.  It\\n    computes the slope as the median of all slopes between paired values.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Dependent variable.\\n    x : array_like or None, optional\\n        Independent variable. If None, use ``arange(len(y))`` instead.\\n    alpha : float, optional\\n        Confidence degree between 0 and 1. Default is 95% confidence.\\n        Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\\n        interpreted as \"find the 90% confidence interval\".\\n    method : {\\'joint\\', \\'separate\\'}, optional\\n        Method to be used for computing estimate for intercept.\\n        Following methods are supported,\\n\\n            * \\'joint\\': Uses np.median(y - slope * x) as intercept.\\n            * \\'separate\\': Uses np.median(y) - slope * np.median(x)\\n                          as intercept.\\n\\n        The default is \\'separate\\'.\\n\\n        .. versionadded:: 1.8.0\\n\\n    Returns\\n    -------\\n    result : ``TheilslopesResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Theil slope.\\n        intercept : float\\n            Intercept of the Theil line.\\n        low_slope : float\\n            Lower bound of the confidence interval on `slope`.\\n        high_slope : float\\n            Upper bound of the confidence interval on `slope`.\\n\\n    See Also\\n    --------\\n    siegelslopes : a similar technique using repeated medians\\n\\n    Notes\\n    -----\\n    The implementation of `theilslopes` follows [1]_. The intercept is\\n    not defined in [1]_, and here it is defined as ``median(y) -\\n    slope*median(x)``, which is given in [3]_. Other definitions of\\n    the intercept exist in the literature such as  ``median(y - slope*x)``\\n    in [4]_. The approach to compute the intercept can be determined by the\\n    parameter ``method``. A confidence interval for the intercept is not\\n    given as this question is not addressed in [1]_.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 4, with fields ``slope``, ``intercept``,\\n    ``low_slope``, and ``high_slope``, so one can continue to write::\\n\\n        slope, intercept, low_slope, high_slope = theilslopes(y, x)\\n\\n    References\\n    ----------\\n    .. [1] P.K. Sen, \"Estimates of the regression coefficient based on\\n           Kendall\\'s tau\", J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\\n    .. [2] H. Theil, \"A rank-invariant method of linear and polynomial\\n           regression analysis I, II and III\",  Nederl. Akad. Wetensch., Proc.\\n           53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\\n    .. [3] W.L. Conover, \"Practical nonparametric statistics\", 2nd ed.,\\n           John Wiley and Sons, New York, pp. 493.\\n    .. [4] https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> x = np.linspace(-5, 5, num=150)\\n    >>> y = x + np.random.normal(size=x.size)\\n    >>> y[11:15] += 10  # add outliers\\n    >>> y[-5:] -= 7\\n\\n    Compute the slope, intercept and 90% confidence interval.  For comparison,\\n    also compute the least-squares fit with `linregress`:\\n\\n    >>> res = stats.theilslopes(y, x, 0.90, method=\\'separate\\')\\n    >>> lsq_res = stats.linregress(x, y)\\n\\n    Plot the results. The Theil-Sen regression line is shown in red, with the\\n    dashed red lines illustrating the confidence interval of the slope (note\\n    that the dashed red lines are not the confidence interval of the regression\\n    as the confidence interval of the intercept is not included). The green\\n    line shows the least-squares fit for comparison.\\n\\n    >>> fig = plt.figure()\\n    >>> ax = fig.add_subplot(111)\\n    >>> ax.plot(x, y, \\'b.\\')\\n    >>> ax.plot(x, res[1] + res[0] * x, \\'r-\\')\\n    >>> ax.plot(x, res[1] + res[2] * x, \\'r--\\')\\n    >>> ax.plot(x, res[1] + res[3] * x, \\'r--\\')\\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \\'g-\\')\\n    >>> plt.show()\\n\\n    '\n    if method not in ['joint', 'separate']:\n        raise ValueError(\"method must be either 'joint' or 'separate'.'{}' is invalid.\".format(method))\n    y = np.array(y).flatten()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.array(x, dtype=float).flatten()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    deltax = x[:, np.newaxis] - x\n    deltay = y[:, np.newaxis] - y\n    slopes = deltay[deltax > 0] / deltax[deltax > 0]\n    if not slopes.size:\n        msg = 'All `x` coordinates are identical.'\n        warnings.warn(msg, RuntimeWarning, stacklevel=2)\n    slopes.sort()\n    medslope = np.median(slopes)\n    if method == 'joint':\n        medinter = np.median(y - medslope * x)\n    else:\n        medinter = np.median(y) - medslope * np.median(x)\n    if alpha > 0.5:\n        alpha = 1.0 - alpha\n    z = distributions.norm.ppf(alpha / 2.0)\n    (_, nxreps) = _find_repeats(x)\n    (_, nyreps) = _find_repeats(y)\n    nt = len(slopes)\n    ny = len(y)\n    sigsq = 1 / 18.0 * (ny * (ny - 1) * (2 * ny + 5) - sum((k * (k - 1) * (2 * k + 5) for k in nxreps)) - sum((k * (k - 1) * (2 * k + 5) for k in nyreps)))\n    try:\n        sigma = np.sqrt(sigsq)\n        Ru = min(int(np.round((nt - z * sigma) / 2.0)), len(slopes) - 1)\n        Rl = max(int(np.round((nt + z * sigma) / 2.0)) - 1, 0)\n        delta = slopes[[Rl, Ru]]\n    except (ValueError, IndexError):\n        delta = (np.nan, np.nan)\n    return TheilslopesResult(slope=medslope, intercept=medinter, low_slope=delta[0], high_slope=delta[1])"
        ]
    },
    {
        "func_name": "_find_repeats",
        "original": "def _find_repeats(arr):\n    if len(arr) == 0:\n        return (np.array(0, np.float64), np.array(0, np.intp))\n    arr = np.asarray(arr, np.float64).ravel()\n    arr.sort()\n    change = np.concatenate(([True], arr[1:] != arr[:-1]))\n    unique = arr[change]\n    change_idx = np.concatenate(np.nonzero(change) + ([arr.size],))\n    freq = np.diff(change_idx)\n    atleast2 = freq > 1\n    return (unique[atleast2], freq[atleast2])",
        "mutated": [
            "def _find_repeats(arr):\n    if False:\n        i = 10\n    if len(arr) == 0:\n        return (np.array(0, np.float64), np.array(0, np.intp))\n    arr = np.asarray(arr, np.float64).ravel()\n    arr.sort()\n    change = np.concatenate(([True], arr[1:] != arr[:-1]))\n    unique = arr[change]\n    change_idx = np.concatenate(np.nonzero(change) + ([arr.size],))\n    freq = np.diff(change_idx)\n    atleast2 = freq > 1\n    return (unique[atleast2], freq[atleast2])",
            "def _find_repeats(arr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if len(arr) == 0:\n        return (np.array(0, np.float64), np.array(0, np.intp))\n    arr = np.asarray(arr, np.float64).ravel()\n    arr.sort()\n    change = np.concatenate(([True], arr[1:] != arr[:-1]))\n    unique = arr[change]\n    change_idx = np.concatenate(np.nonzero(change) + ([arr.size],))\n    freq = np.diff(change_idx)\n    atleast2 = freq > 1\n    return (unique[atleast2], freq[atleast2])",
            "def _find_repeats(arr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if len(arr) == 0:\n        return (np.array(0, np.float64), np.array(0, np.intp))\n    arr = np.asarray(arr, np.float64).ravel()\n    arr.sort()\n    change = np.concatenate(([True], arr[1:] != arr[:-1]))\n    unique = arr[change]\n    change_idx = np.concatenate(np.nonzero(change) + ([arr.size],))\n    freq = np.diff(change_idx)\n    atleast2 = freq > 1\n    return (unique[atleast2], freq[atleast2])",
            "def _find_repeats(arr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if len(arr) == 0:\n        return (np.array(0, np.float64), np.array(0, np.intp))\n    arr = np.asarray(arr, np.float64).ravel()\n    arr.sort()\n    change = np.concatenate(([True], arr[1:] != arr[:-1]))\n    unique = arr[change]\n    change_idx = np.concatenate(np.nonzero(change) + ([arr.size],))\n    freq = np.diff(change_idx)\n    atleast2 = freq > 1\n    return (unique[atleast2], freq[atleast2])",
            "def _find_repeats(arr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if len(arr) == 0:\n        return (np.array(0, np.float64), np.array(0, np.intp))\n    arr = np.asarray(arr, np.float64).ravel()\n    arr.sort()\n    change = np.concatenate(([True], arr[1:] != arr[:-1]))\n    unique = arr[change]\n    change_idx = np.concatenate(np.nonzero(change) + ([arr.size],))\n    freq = np.diff(change_idx)\n    atleast2 = freq > 1\n    return (unique[atleast2], freq[atleast2])"
        ]
    },
    {
        "func_name": "siegelslopes",
        "original": "def siegelslopes(y, x=None, method='hierarchical'):\n    \"\"\"\n    Computes the Siegel estimator for a set of points (x, y).\n\n    `siegelslopes` implements a method for robust linear regression\n    using repeated medians (see [1]_) to fit a line to the points (x, y).\n    The method is robust to outliers with an asymptotic breakdown point\n    of 50%.\n\n    Parameters\n    ----------\n    y : array_like\n        Dependent variable.\n    x : array_like or None, optional\n        Independent variable. If None, use ``arange(len(y))`` instead.\n    method : {'hierarchical', 'separate'}\n        If 'hierarchical', estimate the intercept using the estimated\n        slope ``slope`` (default option).\n        If 'separate', estimate the intercept independent of the estimated\n        slope. See Notes for details.\n\n    Returns\n    -------\n    result : ``SiegelslopesResult`` instance\n        The return value is an object with the following attributes:\n\n        slope : float\n            Estimate of the slope of the regression line.\n        intercept : float\n            Estimate of the intercept of the regression line.\n\n    See Also\n    --------\n    theilslopes : a similar technique without repeated medians\n\n    Notes\n    -----\n    With ``n = len(y)``, compute ``m_j`` as the median of\n    the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.\n    ``slope`` is then the median of all slopes ``m_j``.\n    Two ways are given to estimate the intercept in [1]_ which can be chosen\n    via the parameter ``method``.\n    The hierarchical approach uses the estimated slope ``slope``\n    and computes ``intercept`` as the median of ``y - slope*x``.\n    The other approach estimates the intercept separately as follows: for\n    each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`\n    lines through the remaining points and take the median ``i_j``.\n    ``intercept`` is the median of the ``i_j``.\n\n    The implementation computes `n` times the median of a vector of size `n`\n    which can be slow for large vectors. There are more efficient algorithms\n    (see [2]_) which are not implemented here.\n\n    For compatibility with older versions of SciPy, the return value acts\n    like a ``namedtuple`` of length 2, with fields ``slope`` and\n    ``intercept``, so one can continue to write::\n\n        slope, intercept = siegelslopes(y, x)\n\n    References\n    ----------\n    .. [1] A. Siegel, \"Robust Regression Using Repeated Medians\",\n           Biometrika, Vol. 69, pp. 242-244, 1982.\n\n    .. [2] A. Stein and M. Werman, \"Finding the repeated median regression\n           line\", Proceedings of the Third Annual ACM-SIAM Symposium on\n           Discrete Algorithms, pp. 409-413, 1992.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    >>> x = np.linspace(-5, 5, num=150)\n    >>> y = x + np.random.normal(size=x.size)\n    >>> y[11:15] += 10  # add outliers\n    >>> y[-5:] -= 7\n\n    Compute the slope and intercept.  For comparison, also compute the\n    least-squares fit with `linregress`:\n\n    >>> res = stats.siegelslopes(y, x)\n    >>> lsq_res = stats.linregress(x, y)\n\n    Plot the results. The Siegel regression line is shown in red. The green\n    line shows the least-squares fit for comparison.\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(x, y, 'b.')\n    >>> ax.plot(x, res[1] + res[0] * x, 'r-')\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')\n    >>> plt.show()\n\n    \"\"\"\n    if method not in ['hierarchical', 'separate']:\n        raise ValueError(\"method can only be 'hierarchical' or 'separate'\")\n    y = np.asarray(y).ravel()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.asarray(x, dtype=float).ravel()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    dtype = np.result_type(x, y, np.float32)\n    (y, x) = (y.astype(dtype), x.astype(dtype))\n    (medslope, medinter) = siegelslopes_pythran(y, x, method)\n    return SiegelslopesResult(slope=medslope, intercept=medinter)",
        "mutated": [
            "def siegelslopes(y, x=None, method='hierarchical'):\n    if False:\n        i = 10\n    '\\n    Computes the Siegel estimator for a set of points (x, y).\\n\\n    `siegelslopes` implements a method for robust linear regression\\n    using repeated medians (see [1]_) to fit a line to the points (x, y).\\n    The method is robust to outliers with an asymptotic breakdown point\\n    of 50%.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Dependent variable.\\n    x : array_like or None, optional\\n        Independent variable. If None, use ``arange(len(y))`` instead.\\n    method : {\\'hierarchical\\', \\'separate\\'}\\n        If \\'hierarchical\\', estimate the intercept using the estimated\\n        slope ``slope`` (default option).\\n        If \\'separate\\', estimate the intercept independent of the estimated\\n        slope. See Notes for details.\\n\\n    Returns\\n    -------\\n    result : ``SiegelslopesResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Estimate of the slope of the regression line.\\n        intercept : float\\n            Estimate of the intercept of the regression line.\\n\\n    See Also\\n    --------\\n    theilslopes : a similar technique without repeated medians\\n\\n    Notes\\n    -----\\n    With ``n = len(y)``, compute ``m_j`` as the median of\\n    the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.\\n    ``slope`` is then the median of all slopes ``m_j``.\\n    Two ways are given to estimate the intercept in [1]_ which can be chosen\\n    via the parameter ``method``.\\n    The hierarchical approach uses the estimated slope ``slope``\\n    and computes ``intercept`` as the median of ``y - slope*x``.\\n    The other approach estimates the intercept separately as follows: for\\n    each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`\\n    lines through the remaining points and take the median ``i_j``.\\n    ``intercept`` is the median of the ``i_j``.\\n\\n    The implementation computes `n` times the median of a vector of size `n`\\n    which can be slow for large vectors. There are more efficient algorithms\\n    (see [2]_) which are not implemented here.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 2, with fields ``slope`` and\\n    ``intercept``, so one can continue to write::\\n\\n        slope, intercept = siegelslopes(y, x)\\n\\n    References\\n    ----------\\n    .. [1] A. Siegel, \"Robust Regression Using Repeated Medians\",\\n           Biometrika, Vol. 69, pp. 242-244, 1982.\\n\\n    .. [2] A. Stein and M. Werman, \"Finding the repeated median regression\\n           line\", Proceedings of the Third Annual ACM-SIAM Symposium on\\n           Discrete Algorithms, pp. 409-413, 1992.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> x = np.linspace(-5, 5, num=150)\\n    >>> y = x + np.random.normal(size=x.size)\\n    >>> y[11:15] += 10  # add outliers\\n    >>> y[-5:] -= 7\\n\\n    Compute the slope and intercept.  For comparison, also compute the\\n    least-squares fit with `linregress`:\\n\\n    >>> res = stats.siegelslopes(y, x)\\n    >>> lsq_res = stats.linregress(x, y)\\n\\n    Plot the results. The Siegel regression line is shown in red. The green\\n    line shows the least-squares fit for comparison.\\n\\n    >>> fig = plt.figure()\\n    >>> ax = fig.add_subplot(111)\\n    >>> ax.plot(x, y, \\'b.\\')\\n    >>> ax.plot(x, res[1] + res[0] * x, \\'r-\\')\\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \\'g-\\')\\n    >>> plt.show()\\n\\n    '\n    if method not in ['hierarchical', 'separate']:\n        raise ValueError(\"method can only be 'hierarchical' or 'separate'\")\n    y = np.asarray(y).ravel()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.asarray(x, dtype=float).ravel()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    dtype = np.result_type(x, y, np.float32)\n    (y, x) = (y.astype(dtype), x.astype(dtype))\n    (medslope, medinter) = siegelslopes_pythran(y, x, method)\n    return SiegelslopesResult(slope=medslope, intercept=medinter)",
            "def siegelslopes(y, x=None, method='hierarchical'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Computes the Siegel estimator for a set of points (x, y).\\n\\n    `siegelslopes` implements a method for robust linear regression\\n    using repeated medians (see [1]_) to fit a line to the points (x, y).\\n    The method is robust to outliers with an asymptotic breakdown point\\n    of 50%.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Dependent variable.\\n    x : array_like or None, optional\\n        Independent variable. If None, use ``arange(len(y))`` instead.\\n    method : {\\'hierarchical\\', \\'separate\\'}\\n        If \\'hierarchical\\', estimate the intercept using the estimated\\n        slope ``slope`` (default option).\\n        If \\'separate\\', estimate the intercept independent of the estimated\\n        slope. See Notes for details.\\n\\n    Returns\\n    -------\\n    result : ``SiegelslopesResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Estimate of the slope of the regression line.\\n        intercept : float\\n            Estimate of the intercept of the regression line.\\n\\n    See Also\\n    --------\\n    theilslopes : a similar technique without repeated medians\\n\\n    Notes\\n    -----\\n    With ``n = len(y)``, compute ``m_j`` as the median of\\n    the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.\\n    ``slope`` is then the median of all slopes ``m_j``.\\n    Two ways are given to estimate the intercept in [1]_ which can be chosen\\n    via the parameter ``method``.\\n    The hierarchical approach uses the estimated slope ``slope``\\n    and computes ``intercept`` as the median of ``y - slope*x``.\\n    The other approach estimates the intercept separately as follows: for\\n    each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`\\n    lines through the remaining points and take the median ``i_j``.\\n    ``intercept`` is the median of the ``i_j``.\\n\\n    The implementation computes `n` times the median of a vector of size `n`\\n    which can be slow for large vectors. There are more efficient algorithms\\n    (see [2]_) which are not implemented here.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 2, with fields ``slope`` and\\n    ``intercept``, so one can continue to write::\\n\\n        slope, intercept = siegelslopes(y, x)\\n\\n    References\\n    ----------\\n    .. [1] A. Siegel, \"Robust Regression Using Repeated Medians\",\\n           Biometrika, Vol. 69, pp. 242-244, 1982.\\n\\n    .. [2] A. Stein and M. Werman, \"Finding the repeated median regression\\n           line\", Proceedings of the Third Annual ACM-SIAM Symposium on\\n           Discrete Algorithms, pp. 409-413, 1992.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> x = np.linspace(-5, 5, num=150)\\n    >>> y = x + np.random.normal(size=x.size)\\n    >>> y[11:15] += 10  # add outliers\\n    >>> y[-5:] -= 7\\n\\n    Compute the slope and intercept.  For comparison, also compute the\\n    least-squares fit with `linregress`:\\n\\n    >>> res = stats.siegelslopes(y, x)\\n    >>> lsq_res = stats.linregress(x, y)\\n\\n    Plot the results. The Siegel regression line is shown in red. The green\\n    line shows the least-squares fit for comparison.\\n\\n    >>> fig = plt.figure()\\n    >>> ax = fig.add_subplot(111)\\n    >>> ax.plot(x, y, \\'b.\\')\\n    >>> ax.plot(x, res[1] + res[0] * x, \\'r-\\')\\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \\'g-\\')\\n    >>> plt.show()\\n\\n    '\n    if method not in ['hierarchical', 'separate']:\n        raise ValueError(\"method can only be 'hierarchical' or 'separate'\")\n    y = np.asarray(y).ravel()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.asarray(x, dtype=float).ravel()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    dtype = np.result_type(x, y, np.float32)\n    (y, x) = (y.astype(dtype), x.astype(dtype))\n    (medslope, medinter) = siegelslopes_pythran(y, x, method)\n    return SiegelslopesResult(slope=medslope, intercept=medinter)",
            "def siegelslopes(y, x=None, method='hierarchical'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Computes the Siegel estimator for a set of points (x, y).\\n\\n    `siegelslopes` implements a method for robust linear regression\\n    using repeated medians (see [1]_) to fit a line to the points (x, y).\\n    The method is robust to outliers with an asymptotic breakdown point\\n    of 50%.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Dependent variable.\\n    x : array_like or None, optional\\n        Independent variable. If None, use ``arange(len(y))`` instead.\\n    method : {\\'hierarchical\\', \\'separate\\'}\\n        If \\'hierarchical\\', estimate the intercept using the estimated\\n        slope ``slope`` (default option).\\n        If \\'separate\\', estimate the intercept independent of the estimated\\n        slope. See Notes for details.\\n\\n    Returns\\n    -------\\n    result : ``SiegelslopesResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Estimate of the slope of the regression line.\\n        intercept : float\\n            Estimate of the intercept of the regression line.\\n\\n    See Also\\n    --------\\n    theilslopes : a similar technique without repeated medians\\n\\n    Notes\\n    -----\\n    With ``n = len(y)``, compute ``m_j`` as the median of\\n    the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.\\n    ``slope`` is then the median of all slopes ``m_j``.\\n    Two ways are given to estimate the intercept in [1]_ which can be chosen\\n    via the parameter ``method``.\\n    The hierarchical approach uses the estimated slope ``slope``\\n    and computes ``intercept`` as the median of ``y - slope*x``.\\n    The other approach estimates the intercept separately as follows: for\\n    each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`\\n    lines through the remaining points and take the median ``i_j``.\\n    ``intercept`` is the median of the ``i_j``.\\n\\n    The implementation computes `n` times the median of a vector of size `n`\\n    which can be slow for large vectors. There are more efficient algorithms\\n    (see [2]_) which are not implemented here.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 2, with fields ``slope`` and\\n    ``intercept``, so one can continue to write::\\n\\n        slope, intercept = siegelslopes(y, x)\\n\\n    References\\n    ----------\\n    .. [1] A. Siegel, \"Robust Regression Using Repeated Medians\",\\n           Biometrika, Vol. 69, pp. 242-244, 1982.\\n\\n    .. [2] A. Stein and M. Werman, \"Finding the repeated median regression\\n           line\", Proceedings of the Third Annual ACM-SIAM Symposium on\\n           Discrete Algorithms, pp. 409-413, 1992.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> x = np.linspace(-5, 5, num=150)\\n    >>> y = x + np.random.normal(size=x.size)\\n    >>> y[11:15] += 10  # add outliers\\n    >>> y[-5:] -= 7\\n\\n    Compute the slope and intercept.  For comparison, also compute the\\n    least-squares fit with `linregress`:\\n\\n    >>> res = stats.siegelslopes(y, x)\\n    >>> lsq_res = stats.linregress(x, y)\\n\\n    Plot the results. The Siegel regression line is shown in red. The green\\n    line shows the least-squares fit for comparison.\\n\\n    >>> fig = plt.figure()\\n    >>> ax = fig.add_subplot(111)\\n    >>> ax.plot(x, y, \\'b.\\')\\n    >>> ax.plot(x, res[1] + res[0] * x, \\'r-\\')\\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \\'g-\\')\\n    >>> plt.show()\\n\\n    '\n    if method not in ['hierarchical', 'separate']:\n        raise ValueError(\"method can only be 'hierarchical' or 'separate'\")\n    y = np.asarray(y).ravel()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.asarray(x, dtype=float).ravel()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    dtype = np.result_type(x, y, np.float32)\n    (y, x) = (y.astype(dtype), x.astype(dtype))\n    (medslope, medinter) = siegelslopes_pythran(y, x, method)\n    return SiegelslopesResult(slope=medslope, intercept=medinter)",
            "def siegelslopes(y, x=None, method='hierarchical'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Computes the Siegel estimator for a set of points (x, y).\\n\\n    `siegelslopes` implements a method for robust linear regression\\n    using repeated medians (see [1]_) to fit a line to the points (x, y).\\n    The method is robust to outliers with an asymptotic breakdown point\\n    of 50%.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Dependent variable.\\n    x : array_like or None, optional\\n        Independent variable. If None, use ``arange(len(y))`` instead.\\n    method : {\\'hierarchical\\', \\'separate\\'}\\n        If \\'hierarchical\\', estimate the intercept using the estimated\\n        slope ``slope`` (default option).\\n        If \\'separate\\', estimate the intercept independent of the estimated\\n        slope. See Notes for details.\\n\\n    Returns\\n    -------\\n    result : ``SiegelslopesResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Estimate of the slope of the regression line.\\n        intercept : float\\n            Estimate of the intercept of the regression line.\\n\\n    See Also\\n    --------\\n    theilslopes : a similar technique without repeated medians\\n\\n    Notes\\n    -----\\n    With ``n = len(y)``, compute ``m_j`` as the median of\\n    the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.\\n    ``slope`` is then the median of all slopes ``m_j``.\\n    Two ways are given to estimate the intercept in [1]_ which can be chosen\\n    via the parameter ``method``.\\n    The hierarchical approach uses the estimated slope ``slope``\\n    and computes ``intercept`` as the median of ``y - slope*x``.\\n    The other approach estimates the intercept separately as follows: for\\n    each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`\\n    lines through the remaining points and take the median ``i_j``.\\n    ``intercept`` is the median of the ``i_j``.\\n\\n    The implementation computes `n` times the median of a vector of size `n`\\n    which can be slow for large vectors. There are more efficient algorithms\\n    (see [2]_) which are not implemented here.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 2, with fields ``slope`` and\\n    ``intercept``, so one can continue to write::\\n\\n        slope, intercept = siegelslopes(y, x)\\n\\n    References\\n    ----------\\n    .. [1] A. Siegel, \"Robust Regression Using Repeated Medians\",\\n           Biometrika, Vol. 69, pp. 242-244, 1982.\\n\\n    .. [2] A. Stein and M. Werman, \"Finding the repeated median regression\\n           line\", Proceedings of the Third Annual ACM-SIAM Symposium on\\n           Discrete Algorithms, pp. 409-413, 1992.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> x = np.linspace(-5, 5, num=150)\\n    >>> y = x + np.random.normal(size=x.size)\\n    >>> y[11:15] += 10  # add outliers\\n    >>> y[-5:] -= 7\\n\\n    Compute the slope and intercept.  For comparison, also compute the\\n    least-squares fit with `linregress`:\\n\\n    >>> res = stats.siegelslopes(y, x)\\n    >>> lsq_res = stats.linregress(x, y)\\n\\n    Plot the results. The Siegel regression line is shown in red. The green\\n    line shows the least-squares fit for comparison.\\n\\n    >>> fig = plt.figure()\\n    >>> ax = fig.add_subplot(111)\\n    >>> ax.plot(x, y, \\'b.\\')\\n    >>> ax.plot(x, res[1] + res[0] * x, \\'r-\\')\\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \\'g-\\')\\n    >>> plt.show()\\n\\n    '\n    if method not in ['hierarchical', 'separate']:\n        raise ValueError(\"method can only be 'hierarchical' or 'separate'\")\n    y = np.asarray(y).ravel()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.asarray(x, dtype=float).ravel()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    dtype = np.result_type(x, y, np.float32)\n    (y, x) = (y.astype(dtype), x.astype(dtype))\n    (medslope, medinter) = siegelslopes_pythran(y, x, method)\n    return SiegelslopesResult(slope=medslope, intercept=medinter)",
            "def siegelslopes(y, x=None, method='hierarchical'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Computes the Siegel estimator for a set of points (x, y).\\n\\n    `siegelslopes` implements a method for robust linear regression\\n    using repeated medians (see [1]_) to fit a line to the points (x, y).\\n    The method is robust to outliers with an asymptotic breakdown point\\n    of 50%.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Dependent variable.\\n    x : array_like or None, optional\\n        Independent variable. If None, use ``arange(len(y))`` instead.\\n    method : {\\'hierarchical\\', \\'separate\\'}\\n        If \\'hierarchical\\', estimate the intercept using the estimated\\n        slope ``slope`` (default option).\\n        If \\'separate\\', estimate the intercept independent of the estimated\\n        slope. See Notes for details.\\n\\n    Returns\\n    -------\\n    result : ``SiegelslopesResult`` instance\\n        The return value is an object with the following attributes:\\n\\n        slope : float\\n            Estimate of the slope of the regression line.\\n        intercept : float\\n            Estimate of the intercept of the regression line.\\n\\n    See Also\\n    --------\\n    theilslopes : a similar technique without repeated medians\\n\\n    Notes\\n    -----\\n    With ``n = len(y)``, compute ``m_j`` as the median of\\n    the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.\\n    ``slope`` is then the median of all slopes ``m_j``.\\n    Two ways are given to estimate the intercept in [1]_ which can be chosen\\n    via the parameter ``method``.\\n    The hierarchical approach uses the estimated slope ``slope``\\n    and computes ``intercept`` as the median of ``y - slope*x``.\\n    The other approach estimates the intercept separately as follows: for\\n    each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`\\n    lines through the remaining points and take the median ``i_j``.\\n    ``intercept`` is the median of the ``i_j``.\\n\\n    The implementation computes `n` times the median of a vector of size `n`\\n    which can be slow for large vectors. There are more efficient algorithms\\n    (see [2]_) which are not implemented here.\\n\\n    For compatibility with older versions of SciPy, the return value acts\\n    like a ``namedtuple`` of length 2, with fields ``slope`` and\\n    ``intercept``, so one can continue to write::\\n\\n        slope, intercept = siegelslopes(y, x)\\n\\n    References\\n    ----------\\n    .. [1] A. Siegel, \"Robust Regression Using Repeated Medians\",\\n           Biometrika, Vol. 69, pp. 242-244, 1982.\\n\\n    .. [2] A. Stein and M. Werman, \"Finding the repeated median regression\\n           line\", Proceedings of the Third Annual ACM-SIAM Symposium on\\n           Discrete Algorithms, pp. 409-413, 1992.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> x = np.linspace(-5, 5, num=150)\\n    >>> y = x + np.random.normal(size=x.size)\\n    >>> y[11:15] += 10  # add outliers\\n    >>> y[-5:] -= 7\\n\\n    Compute the slope and intercept.  For comparison, also compute the\\n    least-squares fit with `linregress`:\\n\\n    >>> res = stats.siegelslopes(y, x)\\n    >>> lsq_res = stats.linregress(x, y)\\n\\n    Plot the results. The Siegel regression line is shown in red. The green\\n    line shows the least-squares fit for comparison.\\n\\n    >>> fig = plt.figure()\\n    >>> ax = fig.add_subplot(111)\\n    >>> ax.plot(x, y, \\'b.\\')\\n    >>> ax.plot(x, res[1] + res[0] * x, \\'r-\\')\\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \\'g-\\')\\n    >>> plt.show()\\n\\n    '\n    if method not in ['hierarchical', 'separate']:\n        raise ValueError(\"method can only be 'hierarchical' or 'separate'\")\n    y = np.asarray(y).ravel()\n    if x is None:\n        x = np.arange(len(y), dtype=float)\n    else:\n        x = np.asarray(x, dtype=float).ravel()\n        if len(x) != len(y):\n            raise ValueError('Incompatible lengths ! (%s<>%s)' % (len(y), len(x)))\n    dtype = np.result_type(x, y, np.float32)\n    (y, x) = (y.astype(dtype), x.astype(dtype))\n    (medslope, medinter) = siegelslopes_pythran(y, x, method)\n    return SiegelslopesResult(slope=medslope, intercept=medinter)"
        ]
    }
]