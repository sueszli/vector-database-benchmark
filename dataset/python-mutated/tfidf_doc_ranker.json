[
    {
        "func_name": "__init__",
        "original": "def __init__(self, tfidf_path=None, strict=True):\n    \"\"\"\n        Args:\n            tfidf_path: path to saved model file\n            strict: fail on empty queries or continue (and return empty result)\n        \"\"\"\n    tfidf_path = tfidf_path or DEFAULTS['tfidf_path']\n    logger.info('Loading %s' % tfidf_path)\n    (matrix, metadata) = utils.load_sparse_csr(tfidf_path)\n    self.doc_mat = matrix\n    self.ngrams = metadata['ngram']\n    self.hash_size = metadata['hash_size']\n    self.tokenizer = tokenizers.get_class(metadata['tokenizer'])()\n    self.doc_freqs = metadata['doc_freqs'].squeeze()\n    self.doc_dict = metadata['doc_dict']\n    self.num_docs = len(self.doc_dict[0])\n    self.strict = strict",
        "mutated": [
            "def __init__(self, tfidf_path=None, strict=True):\n    if False:\n        i = 10\n    '\\n        Args:\\n            tfidf_path: path to saved model file\\n            strict: fail on empty queries or continue (and return empty result)\\n        '\n    tfidf_path = tfidf_path or DEFAULTS['tfidf_path']\n    logger.info('Loading %s' % tfidf_path)\n    (matrix, metadata) = utils.load_sparse_csr(tfidf_path)\n    self.doc_mat = matrix\n    self.ngrams = metadata['ngram']\n    self.hash_size = metadata['hash_size']\n    self.tokenizer = tokenizers.get_class(metadata['tokenizer'])()\n    self.doc_freqs = metadata['doc_freqs'].squeeze()\n    self.doc_dict = metadata['doc_dict']\n    self.num_docs = len(self.doc_dict[0])\n    self.strict = strict",
            "def __init__(self, tfidf_path=None, strict=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Args:\\n            tfidf_path: path to saved model file\\n            strict: fail on empty queries or continue (and return empty result)\\n        '\n    tfidf_path = tfidf_path or DEFAULTS['tfidf_path']\n    logger.info('Loading %s' % tfidf_path)\n    (matrix, metadata) = utils.load_sparse_csr(tfidf_path)\n    self.doc_mat = matrix\n    self.ngrams = metadata['ngram']\n    self.hash_size = metadata['hash_size']\n    self.tokenizer = tokenizers.get_class(metadata['tokenizer'])()\n    self.doc_freqs = metadata['doc_freqs'].squeeze()\n    self.doc_dict = metadata['doc_dict']\n    self.num_docs = len(self.doc_dict[0])\n    self.strict = strict",
            "def __init__(self, tfidf_path=None, strict=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Args:\\n            tfidf_path: path to saved model file\\n            strict: fail on empty queries or continue (and return empty result)\\n        '\n    tfidf_path = tfidf_path or DEFAULTS['tfidf_path']\n    logger.info('Loading %s' % tfidf_path)\n    (matrix, metadata) = utils.load_sparse_csr(tfidf_path)\n    self.doc_mat = matrix\n    self.ngrams = metadata['ngram']\n    self.hash_size = metadata['hash_size']\n    self.tokenizer = tokenizers.get_class(metadata['tokenizer'])()\n    self.doc_freqs = metadata['doc_freqs'].squeeze()\n    self.doc_dict = metadata['doc_dict']\n    self.num_docs = len(self.doc_dict[0])\n    self.strict = strict",
            "def __init__(self, tfidf_path=None, strict=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Args:\\n            tfidf_path: path to saved model file\\n            strict: fail on empty queries or continue (and return empty result)\\n        '\n    tfidf_path = tfidf_path or DEFAULTS['tfidf_path']\n    logger.info('Loading %s' % tfidf_path)\n    (matrix, metadata) = utils.load_sparse_csr(tfidf_path)\n    self.doc_mat = matrix\n    self.ngrams = metadata['ngram']\n    self.hash_size = metadata['hash_size']\n    self.tokenizer = tokenizers.get_class(metadata['tokenizer'])()\n    self.doc_freqs = metadata['doc_freqs'].squeeze()\n    self.doc_dict = metadata['doc_dict']\n    self.num_docs = len(self.doc_dict[0])\n    self.strict = strict",
            "def __init__(self, tfidf_path=None, strict=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Args:\\n            tfidf_path: path to saved model file\\n            strict: fail on empty queries or continue (and return empty result)\\n        '\n    tfidf_path = tfidf_path or DEFAULTS['tfidf_path']\n    logger.info('Loading %s' % tfidf_path)\n    (matrix, metadata) = utils.load_sparse_csr(tfidf_path)\n    self.doc_mat = matrix\n    self.ngrams = metadata['ngram']\n    self.hash_size = metadata['hash_size']\n    self.tokenizer = tokenizers.get_class(metadata['tokenizer'])()\n    self.doc_freqs = metadata['doc_freqs'].squeeze()\n    self.doc_dict = metadata['doc_dict']\n    self.num_docs = len(self.doc_dict[0])\n    self.strict = strict"
        ]
    },
    {
        "func_name": "get_doc_index",
        "original": "def get_doc_index(self, doc_id):\n    \"\"\"Convert doc_id --> doc_index\"\"\"\n    return self.doc_dict[0][doc_id]",
        "mutated": [
            "def get_doc_index(self, doc_id):\n    if False:\n        i = 10\n    'Convert doc_id --> doc_index'\n    return self.doc_dict[0][doc_id]",
            "def get_doc_index(self, doc_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convert doc_id --> doc_index'\n    return self.doc_dict[0][doc_id]",
            "def get_doc_index(self, doc_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convert doc_id --> doc_index'\n    return self.doc_dict[0][doc_id]",
            "def get_doc_index(self, doc_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convert doc_id --> doc_index'\n    return self.doc_dict[0][doc_id]",
            "def get_doc_index(self, doc_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convert doc_id --> doc_index'\n    return self.doc_dict[0][doc_id]"
        ]
    },
    {
        "func_name": "get_doc_id",
        "original": "def get_doc_id(self, doc_index):\n    \"\"\"Convert doc_index --> doc_id\"\"\"\n    return self.doc_dict[1][doc_index]",
        "mutated": [
            "def get_doc_id(self, doc_index):\n    if False:\n        i = 10\n    'Convert doc_index --> doc_id'\n    return self.doc_dict[1][doc_index]",
            "def get_doc_id(self, doc_index):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convert doc_index --> doc_id'\n    return self.doc_dict[1][doc_index]",
            "def get_doc_id(self, doc_index):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convert doc_index --> doc_id'\n    return self.doc_dict[1][doc_index]",
            "def get_doc_id(self, doc_index):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convert doc_index --> doc_id'\n    return self.doc_dict[1][doc_index]",
            "def get_doc_id(self, doc_index):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convert doc_index --> doc_id'\n    return self.doc_dict[1][doc_index]"
        ]
    },
    {
        "func_name": "closest_docs",
        "original": "def closest_docs(self, query, k=1):\n    \"\"\"Closest docs by dot product between query and documents\n        in tfidf weighted word vector space.\n        \"\"\"\n    spvec = self.text2spvec(query)\n    res = spvec * self.doc_mat\n    if len(res.data) <= k:\n        o_sort = np.argsort(-res.data)\n    else:\n        o = np.argpartition(-res.data, k)[0:k]\n        o_sort = o[np.argsort(-res.data[o])]\n    doc_scores = res.data[o_sort]\n    doc_ids = [self.get_doc_id(i) for i in res.indices[o_sort]]\n    return (doc_ids, doc_scores)",
        "mutated": [
            "def closest_docs(self, query, k=1):\n    if False:\n        i = 10\n    'Closest docs by dot product between query and documents\\n        in tfidf weighted word vector space.\\n        '\n    spvec = self.text2spvec(query)\n    res = spvec * self.doc_mat\n    if len(res.data) <= k:\n        o_sort = np.argsort(-res.data)\n    else:\n        o = np.argpartition(-res.data, k)[0:k]\n        o_sort = o[np.argsort(-res.data[o])]\n    doc_scores = res.data[o_sort]\n    doc_ids = [self.get_doc_id(i) for i in res.indices[o_sort]]\n    return (doc_ids, doc_scores)",
            "def closest_docs(self, query, k=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Closest docs by dot product between query and documents\\n        in tfidf weighted word vector space.\\n        '\n    spvec = self.text2spvec(query)\n    res = spvec * self.doc_mat\n    if len(res.data) <= k:\n        o_sort = np.argsort(-res.data)\n    else:\n        o = np.argpartition(-res.data, k)[0:k]\n        o_sort = o[np.argsort(-res.data[o])]\n    doc_scores = res.data[o_sort]\n    doc_ids = [self.get_doc_id(i) for i in res.indices[o_sort]]\n    return (doc_ids, doc_scores)",
            "def closest_docs(self, query, k=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Closest docs by dot product between query and documents\\n        in tfidf weighted word vector space.\\n        '\n    spvec = self.text2spvec(query)\n    res = spvec * self.doc_mat\n    if len(res.data) <= k:\n        o_sort = np.argsort(-res.data)\n    else:\n        o = np.argpartition(-res.data, k)[0:k]\n        o_sort = o[np.argsort(-res.data[o])]\n    doc_scores = res.data[o_sort]\n    doc_ids = [self.get_doc_id(i) for i in res.indices[o_sort]]\n    return (doc_ids, doc_scores)",
            "def closest_docs(self, query, k=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Closest docs by dot product between query and documents\\n        in tfidf weighted word vector space.\\n        '\n    spvec = self.text2spvec(query)\n    res = spvec * self.doc_mat\n    if len(res.data) <= k:\n        o_sort = np.argsort(-res.data)\n    else:\n        o = np.argpartition(-res.data, k)[0:k]\n        o_sort = o[np.argsort(-res.data[o])]\n    doc_scores = res.data[o_sort]\n    doc_ids = [self.get_doc_id(i) for i in res.indices[o_sort]]\n    return (doc_ids, doc_scores)",
            "def closest_docs(self, query, k=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Closest docs by dot product between query and documents\\n        in tfidf weighted word vector space.\\n        '\n    spvec = self.text2spvec(query)\n    res = spvec * self.doc_mat\n    if len(res.data) <= k:\n        o_sort = np.argsort(-res.data)\n    else:\n        o = np.argpartition(-res.data, k)[0:k]\n        o_sort = o[np.argsort(-res.data[o])]\n    doc_scores = res.data[o_sort]\n    doc_ids = [self.get_doc_id(i) for i in res.indices[o_sort]]\n    return (doc_ids, doc_scores)"
        ]
    },
    {
        "func_name": "batch_closest_docs",
        "original": "def batch_closest_docs(self, queries, k=1, num_workers=None):\n    \"\"\"Process a batch of closest_docs requests multithreaded.\n        Note: we can use plain threads here as scipy is outside of the GIL.\n        \"\"\"\n    with ThreadPool(num_workers) as threads:\n        closest_docs = partial(self.closest_docs, k=k)\n        results = threads.map(closest_docs, queries)\n    return results",
        "mutated": [
            "def batch_closest_docs(self, queries, k=1, num_workers=None):\n    if False:\n        i = 10\n    'Process a batch of closest_docs requests multithreaded.\\n        Note: we can use plain threads here as scipy is outside of the GIL.\\n        '\n    with ThreadPool(num_workers) as threads:\n        closest_docs = partial(self.closest_docs, k=k)\n        results = threads.map(closest_docs, queries)\n    return results",
            "def batch_closest_docs(self, queries, k=1, num_workers=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Process a batch of closest_docs requests multithreaded.\\n        Note: we can use plain threads here as scipy is outside of the GIL.\\n        '\n    with ThreadPool(num_workers) as threads:\n        closest_docs = partial(self.closest_docs, k=k)\n        results = threads.map(closest_docs, queries)\n    return results",
            "def batch_closest_docs(self, queries, k=1, num_workers=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Process a batch of closest_docs requests multithreaded.\\n        Note: we can use plain threads here as scipy is outside of the GIL.\\n        '\n    with ThreadPool(num_workers) as threads:\n        closest_docs = partial(self.closest_docs, k=k)\n        results = threads.map(closest_docs, queries)\n    return results",
            "def batch_closest_docs(self, queries, k=1, num_workers=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Process a batch of closest_docs requests multithreaded.\\n        Note: we can use plain threads here as scipy is outside of the GIL.\\n        '\n    with ThreadPool(num_workers) as threads:\n        closest_docs = partial(self.closest_docs, k=k)\n        results = threads.map(closest_docs, queries)\n    return results",
            "def batch_closest_docs(self, queries, k=1, num_workers=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Process a batch of closest_docs requests multithreaded.\\n        Note: we can use plain threads here as scipy is outside of the GIL.\\n        '\n    with ThreadPool(num_workers) as threads:\n        closest_docs = partial(self.closest_docs, k=k)\n        results = threads.map(closest_docs, queries)\n    return results"
        ]
    },
    {
        "func_name": "parse",
        "original": "def parse(self, query):\n    \"\"\"Parse the query into tokens (either ngrams or tokens).\"\"\"\n    tokens = self.tokenizer.tokenize(query)\n    return tokens.ngrams(n=self.ngrams, uncased=True, filter_fn=utils.filter_ngram)",
        "mutated": [
            "def parse(self, query):\n    if False:\n        i = 10\n    'Parse the query into tokens (either ngrams or tokens).'\n    tokens = self.tokenizer.tokenize(query)\n    return tokens.ngrams(n=self.ngrams, uncased=True, filter_fn=utils.filter_ngram)",
            "def parse(self, query):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Parse the query into tokens (either ngrams or tokens).'\n    tokens = self.tokenizer.tokenize(query)\n    return tokens.ngrams(n=self.ngrams, uncased=True, filter_fn=utils.filter_ngram)",
            "def parse(self, query):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Parse the query into tokens (either ngrams or tokens).'\n    tokens = self.tokenizer.tokenize(query)\n    return tokens.ngrams(n=self.ngrams, uncased=True, filter_fn=utils.filter_ngram)",
            "def parse(self, query):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Parse the query into tokens (either ngrams or tokens).'\n    tokens = self.tokenizer.tokenize(query)\n    return tokens.ngrams(n=self.ngrams, uncased=True, filter_fn=utils.filter_ngram)",
            "def parse(self, query):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Parse the query into tokens (either ngrams or tokens).'\n    tokens = self.tokenizer.tokenize(query)\n    return tokens.ngrams(n=self.ngrams, uncased=True, filter_fn=utils.filter_ngram)"
        ]
    },
    {
        "func_name": "text2spvec",
        "original": "def text2spvec(self, query):\n    \"\"\"Create a sparse tfidf-weighted word vector from query.\n\n        tfidf = log(tf + 1) * log((N - Nt + 0.5) / (Nt + 0.5))\n        \"\"\"\n    words = self.parse(utils.normalize(query))\n    wids = [utils.hash(w, self.hash_size) for w in words]\n    if len(wids) == 0:\n        if self.strict:\n            raise RuntimeError('No valid word in: %s' % query)\n        else:\n            logger.warning('No valid word in: %s' % query)\n            return sp.csr_matrix((1, self.hash_size))\n    (wids_unique, wids_counts) = np.unique(wids, return_counts=True)\n    tfs = np.log1p(wids_counts)\n    Ns = self.doc_freqs[wids_unique]\n    idfs = np.log((self.num_docs - Ns + 0.5) / (Ns + 0.5))\n    idfs[idfs < 0] = 0\n    data = np.multiply(tfs, idfs)\n    indptr = np.array([0, len(wids_unique)])\n    spvec = sp.csr_matrix((data, wids_unique, indptr), shape=(1, self.hash_size))\n    return spvec",
        "mutated": [
            "def text2spvec(self, query):\n    if False:\n        i = 10\n    'Create a sparse tfidf-weighted word vector from query.\\n\\n        tfidf = log(tf + 1) * log((N - Nt + 0.5) / (Nt + 0.5))\\n        '\n    words = self.parse(utils.normalize(query))\n    wids = [utils.hash(w, self.hash_size) for w in words]\n    if len(wids) == 0:\n        if self.strict:\n            raise RuntimeError('No valid word in: %s' % query)\n        else:\n            logger.warning('No valid word in: %s' % query)\n            return sp.csr_matrix((1, self.hash_size))\n    (wids_unique, wids_counts) = np.unique(wids, return_counts=True)\n    tfs = np.log1p(wids_counts)\n    Ns = self.doc_freqs[wids_unique]\n    idfs = np.log((self.num_docs - Ns + 0.5) / (Ns + 0.5))\n    idfs[idfs < 0] = 0\n    data = np.multiply(tfs, idfs)\n    indptr = np.array([0, len(wids_unique)])\n    spvec = sp.csr_matrix((data, wids_unique, indptr), shape=(1, self.hash_size))\n    return spvec",
            "def text2spvec(self, query):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a sparse tfidf-weighted word vector from query.\\n\\n        tfidf = log(tf + 1) * log((N - Nt + 0.5) / (Nt + 0.5))\\n        '\n    words = self.parse(utils.normalize(query))\n    wids = [utils.hash(w, self.hash_size) for w in words]\n    if len(wids) == 0:\n        if self.strict:\n            raise RuntimeError('No valid word in: %s' % query)\n        else:\n            logger.warning('No valid word in: %s' % query)\n            return sp.csr_matrix((1, self.hash_size))\n    (wids_unique, wids_counts) = np.unique(wids, return_counts=True)\n    tfs = np.log1p(wids_counts)\n    Ns = self.doc_freqs[wids_unique]\n    idfs = np.log((self.num_docs - Ns + 0.5) / (Ns + 0.5))\n    idfs[idfs < 0] = 0\n    data = np.multiply(tfs, idfs)\n    indptr = np.array([0, len(wids_unique)])\n    spvec = sp.csr_matrix((data, wids_unique, indptr), shape=(1, self.hash_size))\n    return spvec",
            "def text2spvec(self, query):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a sparse tfidf-weighted word vector from query.\\n\\n        tfidf = log(tf + 1) * log((N - Nt + 0.5) / (Nt + 0.5))\\n        '\n    words = self.parse(utils.normalize(query))\n    wids = [utils.hash(w, self.hash_size) for w in words]\n    if len(wids) == 0:\n        if self.strict:\n            raise RuntimeError('No valid word in: %s' % query)\n        else:\n            logger.warning('No valid word in: %s' % query)\n            return sp.csr_matrix((1, self.hash_size))\n    (wids_unique, wids_counts) = np.unique(wids, return_counts=True)\n    tfs = np.log1p(wids_counts)\n    Ns = self.doc_freqs[wids_unique]\n    idfs = np.log((self.num_docs - Ns + 0.5) / (Ns + 0.5))\n    idfs[idfs < 0] = 0\n    data = np.multiply(tfs, idfs)\n    indptr = np.array([0, len(wids_unique)])\n    spvec = sp.csr_matrix((data, wids_unique, indptr), shape=(1, self.hash_size))\n    return spvec",
            "def text2spvec(self, query):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a sparse tfidf-weighted word vector from query.\\n\\n        tfidf = log(tf + 1) * log((N - Nt + 0.5) / (Nt + 0.5))\\n        '\n    words = self.parse(utils.normalize(query))\n    wids = [utils.hash(w, self.hash_size) for w in words]\n    if len(wids) == 0:\n        if self.strict:\n            raise RuntimeError('No valid word in: %s' % query)\n        else:\n            logger.warning('No valid word in: %s' % query)\n            return sp.csr_matrix((1, self.hash_size))\n    (wids_unique, wids_counts) = np.unique(wids, return_counts=True)\n    tfs = np.log1p(wids_counts)\n    Ns = self.doc_freqs[wids_unique]\n    idfs = np.log((self.num_docs - Ns + 0.5) / (Ns + 0.5))\n    idfs[idfs < 0] = 0\n    data = np.multiply(tfs, idfs)\n    indptr = np.array([0, len(wids_unique)])\n    spvec = sp.csr_matrix((data, wids_unique, indptr), shape=(1, self.hash_size))\n    return spvec",
            "def text2spvec(self, query):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a sparse tfidf-weighted word vector from query.\\n\\n        tfidf = log(tf + 1) * log((N - Nt + 0.5) / (Nt + 0.5))\\n        '\n    words = self.parse(utils.normalize(query))\n    wids = [utils.hash(w, self.hash_size) for w in words]\n    if len(wids) == 0:\n        if self.strict:\n            raise RuntimeError('No valid word in: %s' % query)\n        else:\n            logger.warning('No valid word in: %s' % query)\n            return sp.csr_matrix((1, self.hash_size))\n    (wids_unique, wids_counts) = np.unique(wids, return_counts=True)\n    tfs = np.log1p(wids_counts)\n    Ns = self.doc_freqs[wids_unique]\n    idfs = np.log((self.num_docs - Ns + 0.5) / (Ns + 0.5))\n    idfs[idfs < 0] = 0\n    data = np.multiply(tfs, idfs)\n    indptr = np.array([0, len(wids_unique)])\n    spvec = sp.csr_matrix((data, wids_unique, indptr), shape=(1, self.hash_size))\n    return spvec"
        ]
    }
]