[
    {
        "func_name": "make_tree",
        "original": "def make_tree(trunk, branch1, branch2, alphas):\n    _trunk = [l['layer'](**l['config']) for l in trunk]\n    bnode = [BranchNode(name='bnode')]\n    _branch1 = [l['layer'](**l['config']) for l in branch1]\n    _branch2 = [l['layer'](**l['config']) for l in branch2]\n    v1 = Tree([_trunk + bnode + _branch1, bnode + _branch2], alphas)\n    _trunkb = [l['layer'](**l['config']) for l in trunk]\n    _branch1b = [l['layer'](**l['config']) for l in branch1]\n    _branch2b = [l['layer'](**l['config']) for l in branch2]\n    return (v1, _trunkb, _branch1b, _branch2b)",
        "mutated": [
            "def make_tree(trunk, branch1, branch2, alphas):\n    if False:\n        i = 10\n    _trunk = [l['layer'](**l['config']) for l in trunk]\n    bnode = [BranchNode(name='bnode')]\n    _branch1 = [l['layer'](**l['config']) for l in branch1]\n    _branch2 = [l['layer'](**l['config']) for l in branch2]\n    v1 = Tree([_trunk + bnode + _branch1, bnode + _branch2], alphas)\n    _trunkb = [l['layer'](**l['config']) for l in trunk]\n    _branch1b = [l['layer'](**l['config']) for l in branch1]\n    _branch2b = [l['layer'](**l['config']) for l in branch2]\n    return (v1, _trunkb, _branch1b, _branch2b)",
            "def make_tree(trunk, branch1, branch2, alphas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    _trunk = [l['layer'](**l['config']) for l in trunk]\n    bnode = [BranchNode(name='bnode')]\n    _branch1 = [l['layer'](**l['config']) for l in branch1]\n    _branch2 = [l['layer'](**l['config']) for l in branch2]\n    v1 = Tree([_trunk + bnode + _branch1, bnode + _branch2], alphas)\n    _trunkb = [l['layer'](**l['config']) for l in trunk]\n    _branch1b = [l['layer'](**l['config']) for l in branch1]\n    _branch2b = [l['layer'](**l['config']) for l in branch2]\n    return (v1, _trunkb, _branch1b, _branch2b)",
            "def make_tree(trunk, branch1, branch2, alphas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    _trunk = [l['layer'](**l['config']) for l in trunk]\n    bnode = [BranchNode(name='bnode')]\n    _branch1 = [l['layer'](**l['config']) for l in branch1]\n    _branch2 = [l['layer'](**l['config']) for l in branch2]\n    v1 = Tree([_trunk + bnode + _branch1, bnode + _branch2], alphas)\n    _trunkb = [l['layer'](**l['config']) for l in trunk]\n    _branch1b = [l['layer'](**l['config']) for l in branch1]\n    _branch2b = [l['layer'](**l['config']) for l in branch2]\n    return (v1, _trunkb, _branch1b, _branch2b)",
            "def make_tree(trunk, branch1, branch2, alphas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    _trunk = [l['layer'](**l['config']) for l in trunk]\n    bnode = [BranchNode(name='bnode')]\n    _branch1 = [l['layer'](**l['config']) for l in branch1]\n    _branch2 = [l['layer'](**l['config']) for l in branch2]\n    v1 = Tree([_trunk + bnode + _branch1, bnode + _branch2], alphas)\n    _trunkb = [l['layer'](**l['config']) for l in trunk]\n    _branch1b = [l['layer'](**l['config']) for l in branch1]\n    _branch2b = [l['layer'](**l['config']) for l in branch2]\n    return (v1, _trunkb, _branch1b, _branch2b)",
            "def make_tree(trunk, branch1, branch2, alphas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    _trunk = [l['layer'](**l['config']) for l in trunk]\n    bnode = [BranchNode(name='bnode')]\n    _branch1 = [l['layer'](**l['config']) for l in branch1]\n    _branch2 = [l['layer'](**l['config']) for l in branch2]\n    v1 = Tree([_trunk + bnode + _branch1, bnode + _branch2], alphas)\n    _trunkb = [l['layer'](**l['config']) for l in trunk]\n    _branch1b = [l['layer'](**l['config']) for l in branch1]\n    _branch2b = [l['layer'](**l['config']) for l in branch2]\n    return (v1, _trunkb, _branch1b, _branch2b)"
        ]
    },
    {
        "func_name": "test_branch_model_gpu",
        "original": "@pytest.mark.hasgpu\ndef test_branch_model_gpu(backend_gpu):\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 1e-09\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 1e-09",
        "mutated": [
            "@pytest.mark.hasgpu\ndef test_branch_model_gpu(backend_gpu):\n    if False:\n        i = 10\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 1e-09\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 1e-09",
            "@pytest.mark.hasgpu\ndef test_branch_model_gpu(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 1e-09\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 1e-09",
            "@pytest.mark.hasgpu\ndef test_branch_model_gpu(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 1e-09\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 1e-09",
            "@pytest.mark.hasgpu\ndef test_branch_model_gpu(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 1e-09\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 1e-09",
            "@pytest.mark.hasgpu\ndef test_branch_model_gpu(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 1e-09\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 1e-09"
        ]
    },
    {
        "func_name": "test_branch_model_mkl",
        "original": "@pytest.mark.mkl_only\ndef test_branch_model_mkl(backend_default_mkl):\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 0.01\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 0.001",
        "mutated": [
            "@pytest.mark.mkl_only\ndef test_branch_model_mkl(backend_default_mkl):\n    if False:\n        i = 10\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 0.01\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 0.001",
            "@pytest.mark.mkl_only\ndef test_branch_model_mkl(backend_default_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 0.01\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 0.001",
            "@pytest.mark.mkl_only\ndef test_branch_model_mkl(backend_default_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 0.01\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 0.001",
            "@pytest.mark.mkl_only\ndef test_branch_model_mkl(backend_default_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 0.01\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 0.001",
            "@pytest.mark.mkl_only\ndef test_branch_model_mkl(backend_default_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    be = NervanaObject.be\n    trunk = [{'layer': Conv, 'config': dict(fshape=(5, 5, 16), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}]\n    branch1 = [{'layer': Conv, 'config': dict(fshape=(5, 5, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=200, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    branch2 = [{'layer': Conv, 'config': dict(fshape=(3, 3, 32), **common)}, {'layer': Pooling, 'config': dict(op='max', **pool2s1p1)}, {'layer': Affine, 'config': dict(nout=256, **common)}, {'layer': Affine, 'config': dict(nout=10, init=init1, activation=relu)}]\n    alphas = [1, 1]\n    (neon_layer, t, b1, b2) = make_tree(trunk, branch1, branch2, alphas)\n    inshape = (16, 32, 32)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, be.bsz))\n    inp = be.array(inpa)\n    neon_layer.configure(inshape)\n    neon_layer.allocate()\n    neon_layer.allocate_deltas()\n    neon_out = [i.get() for i in neon_layer.fprop(inp)]\n    ref_layers = [Sequential(t), Sequential(b1), Sequential(b2)]\n    ref_layers[0].configure(inshape)\n    ref_layers[1].configure(ref_layers[0].out_shape)\n    ref_layers[2].configure(ref_layers[0].out_shape)\n    [r.allocate() for r in ref_layers]\n    [r.allocate_deltas() for r in ref_layers]\n    ref_all_layers = ref_layers[0].layers + ref_layers[1].layers + ref_layers[2].layers\n    ref_weight_layers = [l for l in ref_all_layers if l.has_params]\n    neon_weight_layers = neon_layer.layers_to_optimize\n    for (rl, nl) in zip(ref_weight_layers, neon_weight_layers):\n        rl.set_params({'params': {'W': nl.W.get()}})\n    inp_middle = ref_layers[0].fprop(inp)\n    ref_out = [r.fprop(inp_middle).get() for r in ref_layers[1:]]\n    for (h, r) in zip(neon_out, ref_out):\n        difference = np.max(np.abs(h - r))\n        assert difference < 0.01\n    erra = [np.random.random(ll.shape) for ll in neon_out]\n    err = [be.array(e) for e in erra]\n    input_layer = neon_layer.layers[0].layers[0]\n    input_layer.prev_layer = True\n    input_layer.deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    errp = input_layer.deltas.get()\n    for (i, r) in enumerate(ref_layers):\n        r.layers[0].prev_layer = True\n        _inshape = inshape if i == 0 else ref_layers[0].out_shape\n        r.layers[0].deltas = be.iobuf(_inshape)\n    joined_err = be.iobuf(ref_layers[0].out_shape)\n    branch_errs = [r.bprop(e, a) for (r, e, a) in reversed(list(zip(ref_layers[1:], err, alphas)))]\n    joined_err[:] = branch_errs[0] + branch_errs[1]\n    err_ref = ref_layers[0].bprop(joined_err).get()\n    difference = np.max(np.abs(err_ref - errp))\n    neon_logger.display('Max difference: {}'.format(difference))\n    assert difference < 0.001"
        ]
    }
]