[
    {
        "func_name": "load_demo_image",
        "original": "def load_demo_image(image_size, device):\n    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    transform = transforms.Compose([transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC), transforms.ToTensor(), transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n    image = transform(raw_image).unsqueeze(0).to(device)\n    return image",
        "mutated": [
            "def load_demo_image(image_size, device):\n    if False:\n        i = 10\n    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    transform = transforms.Compose([transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC), transforms.ToTensor(), transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n    image = transform(raw_image).unsqueeze(0).to(device)\n    return image",
            "def load_demo_image(image_size, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    transform = transforms.Compose([transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC), transforms.ToTensor(), transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n    image = transform(raw_image).unsqueeze(0).to(device)\n    return image",
            "def load_demo_image(image_size, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    transform = transforms.Compose([transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC), transforms.ToTensor(), transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n    image = transform(raw_image).unsqueeze(0).to(device)\n    return image",
            "def load_demo_image(image_size, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    transform = transforms.Compose([transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC), transforms.ToTensor(), transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n    image = transform(raw_image).unsqueeze(0).to(device)\n    return image",
            "def load_demo_image(image_size, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    transform = transforms.Compose([transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC), transforms.ToTensor(), transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n    image = transform(raw_image).unsqueeze(0).to(device)\n    return image"
        ]
    },
    {
        "func_name": "rename_key",
        "original": "def rename_key(key):\n    if 'visual_encoder' in key:\n        key = re.sub('visual_encoder*', 'vision_model.encoder', key)\n    if 'blocks' in key:\n        key = re.sub('blocks', 'layers', key)\n    if 'attn' in key:\n        key = re.sub('attn', 'self_attn', key)\n    if 'norm1' in key:\n        key = re.sub('norm1', 'layer_norm1', key)\n    if 'norm2' in key:\n        key = re.sub('norm2', 'layer_norm2', key)\n    if 'encoder.norm' in key:\n        key = re.sub('encoder.norm', 'post_layernorm', key)\n    if 'encoder.patch_embed.proj' in key:\n        key = re.sub('encoder.patch_embed.proj', 'embeddings.patch_embedding', key)\n    if 'encoder.pos_embed' in key:\n        key = re.sub('encoder.pos_embed', 'embeddings.position_embedding', key)\n    if 'encoder.cls_token' in key:\n        key = re.sub('encoder.cls_token', 'embeddings.class_embedding', key)\n    if 'self_attn' in key:\n        key = re.sub('self_attn.proj', 'self_attn.projection', key)\n    return key",
        "mutated": [
            "def rename_key(key):\n    if False:\n        i = 10\n    if 'visual_encoder' in key:\n        key = re.sub('visual_encoder*', 'vision_model.encoder', key)\n    if 'blocks' in key:\n        key = re.sub('blocks', 'layers', key)\n    if 'attn' in key:\n        key = re.sub('attn', 'self_attn', key)\n    if 'norm1' in key:\n        key = re.sub('norm1', 'layer_norm1', key)\n    if 'norm2' in key:\n        key = re.sub('norm2', 'layer_norm2', key)\n    if 'encoder.norm' in key:\n        key = re.sub('encoder.norm', 'post_layernorm', key)\n    if 'encoder.patch_embed.proj' in key:\n        key = re.sub('encoder.patch_embed.proj', 'embeddings.patch_embedding', key)\n    if 'encoder.pos_embed' in key:\n        key = re.sub('encoder.pos_embed', 'embeddings.position_embedding', key)\n    if 'encoder.cls_token' in key:\n        key = re.sub('encoder.cls_token', 'embeddings.class_embedding', key)\n    if 'self_attn' in key:\n        key = re.sub('self_attn.proj', 'self_attn.projection', key)\n    return key",
            "def rename_key(key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if 'visual_encoder' in key:\n        key = re.sub('visual_encoder*', 'vision_model.encoder', key)\n    if 'blocks' in key:\n        key = re.sub('blocks', 'layers', key)\n    if 'attn' in key:\n        key = re.sub('attn', 'self_attn', key)\n    if 'norm1' in key:\n        key = re.sub('norm1', 'layer_norm1', key)\n    if 'norm2' in key:\n        key = re.sub('norm2', 'layer_norm2', key)\n    if 'encoder.norm' in key:\n        key = re.sub('encoder.norm', 'post_layernorm', key)\n    if 'encoder.patch_embed.proj' in key:\n        key = re.sub('encoder.patch_embed.proj', 'embeddings.patch_embedding', key)\n    if 'encoder.pos_embed' in key:\n        key = re.sub('encoder.pos_embed', 'embeddings.position_embedding', key)\n    if 'encoder.cls_token' in key:\n        key = re.sub('encoder.cls_token', 'embeddings.class_embedding', key)\n    if 'self_attn' in key:\n        key = re.sub('self_attn.proj', 'self_attn.projection', key)\n    return key",
            "def rename_key(key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if 'visual_encoder' in key:\n        key = re.sub('visual_encoder*', 'vision_model.encoder', key)\n    if 'blocks' in key:\n        key = re.sub('blocks', 'layers', key)\n    if 'attn' in key:\n        key = re.sub('attn', 'self_attn', key)\n    if 'norm1' in key:\n        key = re.sub('norm1', 'layer_norm1', key)\n    if 'norm2' in key:\n        key = re.sub('norm2', 'layer_norm2', key)\n    if 'encoder.norm' in key:\n        key = re.sub('encoder.norm', 'post_layernorm', key)\n    if 'encoder.patch_embed.proj' in key:\n        key = re.sub('encoder.patch_embed.proj', 'embeddings.patch_embedding', key)\n    if 'encoder.pos_embed' in key:\n        key = re.sub('encoder.pos_embed', 'embeddings.position_embedding', key)\n    if 'encoder.cls_token' in key:\n        key = re.sub('encoder.cls_token', 'embeddings.class_embedding', key)\n    if 'self_attn' in key:\n        key = re.sub('self_attn.proj', 'self_attn.projection', key)\n    return key",
            "def rename_key(key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if 'visual_encoder' in key:\n        key = re.sub('visual_encoder*', 'vision_model.encoder', key)\n    if 'blocks' in key:\n        key = re.sub('blocks', 'layers', key)\n    if 'attn' in key:\n        key = re.sub('attn', 'self_attn', key)\n    if 'norm1' in key:\n        key = re.sub('norm1', 'layer_norm1', key)\n    if 'norm2' in key:\n        key = re.sub('norm2', 'layer_norm2', key)\n    if 'encoder.norm' in key:\n        key = re.sub('encoder.norm', 'post_layernorm', key)\n    if 'encoder.patch_embed.proj' in key:\n        key = re.sub('encoder.patch_embed.proj', 'embeddings.patch_embedding', key)\n    if 'encoder.pos_embed' in key:\n        key = re.sub('encoder.pos_embed', 'embeddings.position_embedding', key)\n    if 'encoder.cls_token' in key:\n        key = re.sub('encoder.cls_token', 'embeddings.class_embedding', key)\n    if 'self_attn' in key:\n        key = re.sub('self_attn.proj', 'self_attn.projection', key)\n    return key",
            "def rename_key(key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if 'visual_encoder' in key:\n        key = re.sub('visual_encoder*', 'vision_model.encoder', key)\n    if 'blocks' in key:\n        key = re.sub('blocks', 'layers', key)\n    if 'attn' in key:\n        key = re.sub('attn', 'self_attn', key)\n    if 'norm1' in key:\n        key = re.sub('norm1', 'layer_norm1', key)\n    if 'norm2' in key:\n        key = re.sub('norm2', 'layer_norm2', key)\n    if 'encoder.norm' in key:\n        key = re.sub('encoder.norm', 'post_layernorm', key)\n    if 'encoder.patch_embed.proj' in key:\n        key = re.sub('encoder.patch_embed.proj', 'embeddings.patch_embedding', key)\n    if 'encoder.pos_embed' in key:\n        key = re.sub('encoder.pos_embed', 'embeddings.position_embedding', key)\n    if 'encoder.cls_token' in key:\n        key = re.sub('encoder.cls_token', 'embeddings.class_embedding', key)\n    if 'self_attn' in key:\n        key = re.sub('self_attn.proj', 'self_attn.projection', key)\n    return key"
        ]
    },
    {
        "func_name": "convert_blip_checkpoint",
        "original": "@torch.no_grad()\ndef convert_blip_checkpoint(pytorch_dump_folder_path, config_path=None):\n    \"\"\"\n    Copy/paste/tweak model's weights to transformers design.\n    \"\"\"\n    if config_path is not None:\n        config = BlipConfig.from_pretrained(config_path)\n    else:\n        config = BlipConfig(projection_dim=512, text_config={}, vision_config={})\n    hf_model = BlipForConditionalGeneration(config).eval()\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n    pt_model = blip_decoder(pretrained=model_url, image_size=384, vit='base')\n    pt_model = pt_model.eval()\n    modified_state_dict = pt_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_model.load_state_dict(modified_state_dict)\n    image_size = 384\n    image = load_demo_image(image_size=image_size, device='cpu')\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    input_ids = tokenizer(['a picture of']).input_ids\n    out = hf_model.generate(image, input_ids)\n    assert out[0].tolist() == [30522, 1037, 3861, 1997, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    out = hf_model.generate(image)\n    assert out[0].tolist() == [30522, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    if pytorch_dump_folder_path is not None:\n        hf_model.save_pretrained(pytorch_dump_folder_path)\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n    vqa_model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n    vqa_model.eval()\n    modified_state_dict = vqa_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_vqa_model = BlipForQuestionAnswering(config)\n    hf_vqa_model.load_state_dict(modified_state_dict)\n    question = ['How many dogs are in this image?']\n    question_input_ids = tokenizer(question, return_tensors='pt').input_ids\n    answer = hf_vqa_model.generate(question_input_ids, image)\n    print(tokenizer.decode(answer[0]))\n    assert tokenizer.decode(answer[0]) == '[UNK] 1 [SEP]'\n    if pytorch_dump_folder_path is not None:\n        hf_vqa_model.save_pretrained(pytorch_dump_folder_path + '_vqa')\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n    itm_model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n    itm_model.eval()\n    modified_state_dict = itm_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_itm_model = BlipForImageTextRetrieval(config)\n    question = ['A picture of a woman with a dog sitting in a beach']\n    question_input_ids = tokenizer(question, return_tensors='pt', padding='max_length', truncation=True, max_length=35).input_ids\n    hf_itm_model.load_state_dict(modified_state_dict)\n    hf_itm_model.eval()\n    out_itm = hf_itm_model(question_input_ids, image, use_itm_head=True)\n    out = hf_itm_model(question_input_ids, image, use_itm_head=False)\n    assert out[0].item() == 0.2110687494277954\n    assert torch.nn.functional.softmax(out_itm[0], dim=1)[:, 1].item() == 0.45698845386505127\n    if pytorch_dump_folder_path is not None:\n        hf_itm_model.save_pretrained(pytorch_dump_folder_path + '_itm')",
        "mutated": [
            "@torch.no_grad()\ndef convert_blip_checkpoint(pytorch_dump_folder_path, config_path=None):\n    if False:\n        i = 10\n    \"\\n    Copy/paste/tweak model's weights to transformers design.\\n    \"\n    if config_path is not None:\n        config = BlipConfig.from_pretrained(config_path)\n    else:\n        config = BlipConfig(projection_dim=512, text_config={}, vision_config={})\n    hf_model = BlipForConditionalGeneration(config).eval()\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n    pt_model = blip_decoder(pretrained=model_url, image_size=384, vit='base')\n    pt_model = pt_model.eval()\n    modified_state_dict = pt_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_model.load_state_dict(modified_state_dict)\n    image_size = 384\n    image = load_demo_image(image_size=image_size, device='cpu')\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    input_ids = tokenizer(['a picture of']).input_ids\n    out = hf_model.generate(image, input_ids)\n    assert out[0].tolist() == [30522, 1037, 3861, 1997, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    out = hf_model.generate(image)\n    assert out[0].tolist() == [30522, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    if pytorch_dump_folder_path is not None:\n        hf_model.save_pretrained(pytorch_dump_folder_path)\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n    vqa_model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n    vqa_model.eval()\n    modified_state_dict = vqa_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_vqa_model = BlipForQuestionAnswering(config)\n    hf_vqa_model.load_state_dict(modified_state_dict)\n    question = ['How many dogs are in this image?']\n    question_input_ids = tokenizer(question, return_tensors='pt').input_ids\n    answer = hf_vqa_model.generate(question_input_ids, image)\n    print(tokenizer.decode(answer[0]))\n    assert tokenizer.decode(answer[0]) == '[UNK] 1 [SEP]'\n    if pytorch_dump_folder_path is not None:\n        hf_vqa_model.save_pretrained(pytorch_dump_folder_path + '_vqa')\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n    itm_model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n    itm_model.eval()\n    modified_state_dict = itm_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_itm_model = BlipForImageTextRetrieval(config)\n    question = ['A picture of a woman with a dog sitting in a beach']\n    question_input_ids = tokenizer(question, return_tensors='pt', padding='max_length', truncation=True, max_length=35).input_ids\n    hf_itm_model.load_state_dict(modified_state_dict)\n    hf_itm_model.eval()\n    out_itm = hf_itm_model(question_input_ids, image, use_itm_head=True)\n    out = hf_itm_model(question_input_ids, image, use_itm_head=False)\n    assert out[0].item() == 0.2110687494277954\n    assert torch.nn.functional.softmax(out_itm[0], dim=1)[:, 1].item() == 0.45698845386505127\n    if pytorch_dump_folder_path is not None:\n        hf_itm_model.save_pretrained(pytorch_dump_folder_path + '_itm')",
            "@torch.no_grad()\ndef convert_blip_checkpoint(pytorch_dump_folder_path, config_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Copy/paste/tweak model's weights to transformers design.\\n    \"\n    if config_path is not None:\n        config = BlipConfig.from_pretrained(config_path)\n    else:\n        config = BlipConfig(projection_dim=512, text_config={}, vision_config={})\n    hf_model = BlipForConditionalGeneration(config).eval()\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n    pt_model = blip_decoder(pretrained=model_url, image_size=384, vit='base')\n    pt_model = pt_model.eval()\n    modified_state_dict = pt_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_model.load_state_dict(modified_state_dict)\n    image_size = 384\n    image = load_demo_image(image_size=image_size, device='cpu')\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    input_ids = tokenizer(['a picture of']).input_ids\n    out = hf_model.generate(image, input_ids)\n    assert out[0].tolist() == [30522, 1037, 3861, 1997, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    out = hf_model.generate(image)\n    assert out[0].tolist() == [30522, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    if pytorch_dump_folder_path is not None:\n        hf_model.save_pretrained(pytorch_dump_folder_path)\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n    vqa_model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n    vqa_model.eval()\n    modified_state_dict = vqa_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_vqa_model = BlipForQuestionAnswering(config)\n    hf_vqa_model.load_state_dict(modified_state_dict)\n    question = ['How many dogs are in this image?']\n    question_input_ids = tokenizer(question, return_tensors='pt').input_ids\n    answer = hf_vqa_model.generate(question_input_ids, image)\n    print(tokenizer.decode(answer[0]))\n    assert tokenizer.decode(answer[0]) == '[UNK] 1 [SEP]'\n    if pytorch_dump_folder_path is not None:\n        hf_vqa_model.save_pretrained(pytorch_dump_folder_path + '_vqa')\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n    itm_model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n    itm_model.eval()\n    modified_state_dict = itm_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_itm_model = BlipForImageTextRetrieval(config)\n    question = ['A picture of a woman with a dog sitting in a beach']\n    question_input_ids = tokenizer(question, return_tensors='pt', padding='max_length', truncation=True, max_length=35).input_ids\n    hf_itm_model.load_state_dict(modified_state_dict)\n    hf_itm_model.eval()\n    out_itm = hf_itm_model(question_input_ids, image, use_itm_head=True)\n    out = hf_itm_model(question_input_ids, image, use_itm_head=False)\n    assert out[0].item() == 0.2110687494277954\n    assert torch.nn.functional.softmax(out_itm[0], dim=1)[:, 1].item() == 0.45698845386505127\n    if pytorch_dump_folder_path is not None:\n        hf_itm_model.save_pretrained(pytorch_dump_folder_path + '_itm')",
            "@torch.no_grad()\ndef convert_blip_checkpoint(pytorch_dump_folder_path, config_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Copy/paste/tweak model's weights to transformers design.\\n    \"\n    if config_path is not None:\n        config = BlipConfig.from_pretrained(config_path)\n    else:\n        config = BlipConfig(projection_dim=512, text_config={}, vision_config={})\n    hf_model = BlipForConditionalGeneration(config).eval()\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n    pt_model = blip_decoder(pretrained=model_url, image_size=384, vit='base')\n    pt_model = pt_model.eval()\n    modified_state_dict = pt_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_model.load_state_dict(modified_state_dict)\n    image_size = 384\n    image = load_demo_image(image_size=image_size, device='cpu')\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    input_ids = tokenizer(['a picture of']).input_ids\n    out = hf_model.generate(image, input_ids)\n    assert out[0].tolist() == [30522, 1037, 3861, 1997, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    out = hf_model.generate(image)\n    assert out[0].tolist() == [30522, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    if pytorch_dump_folder_path is not None:\n        hf_model.save_pretrained(pytorch_dump_folder_path)\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n    vqa_model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n    vqa_model.eval()\n    modified_state_dict = vqa_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_vqa_model = BlipForQuestionAnswering(config)\n    hf_vqa_model.load_state_dict(modified_state_dict)\n    question = ['How many dogs are in this image?']\n    question_input_ids = tokenizer(question, return_tensors='pt').input_ids\n    answer = hf_vqa_model.generate(question_input_ids, image)\n    print(tokenizer.decode(answer[0]))\n    assert tokenizer.decode(answer[0]) == '[UNK] 1 [SEP]'\n    if pytorch_dump_folder_path is not None:\n        hf_vqa_model.save_pretrained(pytorch_dump_folder_path + '_vqa')\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n    itm_model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n    itm_model.eval()\n    modified_state_dict = itm_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_itm_model = BlipForImageTextRetrieval(config)\n    question = ['A picture of a woman with a dog sitting in a beach']\n    question_input_ids = tokenizer(question, return_tensors='pt', padding='max_length', truncation=True, max_length=35).input_ids\n    hf_itm_model.load_state_dict(modified_state_dict)\n    hf_itm_model.eval()\n    out_itm = hf_itm_model(question_input_ids, image, use_itm_head=True)\n    out = hf_itm_model(question_input_ids, image, use_itm_head=False)\n    assert out[0].item() == 0.2110687494277954\n    assert torch.nn.functional.softmax(out_itm[0], dim=1)[:, 1].item() == 0.45698845386505127\n    if pytorch_dump_folder_path is not None:\n        hf_itm_model.save_pretrained(pytorch_dump_folder_path + '_itm')",
            "@torch.no_grad()\ndef convert_blip_checkpoint(pytorch_dump_folder_path, config_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Copy/paste/tweak model's weights to transformers design.\\n    \"\n    if config_path is not None:\n        config = BlipConfig.from_pretrained(config_path)\n    else:\n        config = BlipConfig(projection_dim=512, text_config={}, vision_config={})\n    hf_model = BlipForConditionalGeneration(config).eval()\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n    pt_model = blip_decoder(pretrained=model_url, image_size=384, vit='base')\n    pt_model = pt_model.eval()\n    modified_state_dict = pt_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_model.load_state_dict(modified_state_dict)\n    image_size = 384\n    image = load_demo_image(image_size=image_size, device='cpu')\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    input_ids = tokenizer(['a picture of']).input_ids\n    out = hf_model.generate(image, input_ids)\n    assert out[0].tolist() == [30522, 1037, 3861, 1997, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    out = hf_model.generate(image)\n    assert out[0].tolist() == [30522, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    if pytorch_dump_folder_path is not None:\n        hf_model.save_pretrained(pytorch_dump_folder_path)\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n    vqa_model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n    vqa_model.eval()\n    modified_state_dict = vqa_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_vqa_model = BlipForQuestionAnswering(config)\n    hf_vqa_model.load_state_dict(modified_state_dict)\n    question = ['How many dogs are in this image?']\n    question_input_ids = tokenizer(question, return_tensors='pt').input_ids\n    answer = hf_vqa_model.generate(question_input_ids, image)\n    print(tokenizer.decode(answer[0]))\n    assert tokenizer.decode(answer[0]) == '[UNK] 1 [SEP]'\n    if pytorch_dump_folder_path is not None:\n        hf_vqa_model.save_pretrained(pytorch_dump_folder_path + '_vqa')\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n    itm_model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n    itm_model.eval()\n    modified_state_dict = itm_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_itm_model = BlipForImageTextRetrieval(config)\n    question = ['A picture of a woman with a dog sitting in a beach']\n    question_input_ids = tokenizer(question, return_tensors='pt', padding='max_length', truncation=True, max_length=35).input_ids\n    hf_itm_model.load_state_dict(modified_state_dict)\n    hf_itm_model.eval()\n    out_itm = hf_itm_model(question_input_ids, image, use_itm_head=True)\n    out = hf_itm_model(question_input_ids, image, use_itm_head=False)\n    assert out[0].item() == 0.2110687494277954\n    assert torch.nn.functional.softmax(out_itm[0], dim=1)[:, 1].item() == 0.45698845386505127\n    if pytorch_dump_folder_path is not None:\n        hf_itm_model.save_pretrained(pytorch_dump_folder_path + '_itm')",
            "@torch.no_grad()\ndef convert_blip_checkpoint(pytorch_dump_folder_path, config_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Copy/paste/tweak model's weights to transformers design.\\n    \"\n    if config_path is not None:\n        config = BlipConfig.from_pretrained(config_path)\n    else:\n        config = BlipConfig(projection_dim=512, text_config={}, vision_config={})\n    hf_model = BlipForConditionalGeneration(config).eval()\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n    pt_model = blip_decoder(pretrained=model_url, image_size=384, vit='base')\n    pt_model = pt_model.eval()\n    modified_state_dict = pt_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_model.load_state_dict(modified_state_dict)\n    image_size = 384\n    image = load_demo_image(image_size=image_size, device='cpu')\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    input_ids = tokenizer(['a picture of']).input_ids\n    out = hf_model.generate(image, input_ids)\n    assert out[0].tolist() == [30522, 1037, 3861, 1997, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    out = hf_model.generate(image)\n    assert out[0].tolist() == [30522, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n    if pytorch_dump_folder_path is not None:\n        hf_model.save_pretrained(pytorch_dump_folder_path)\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n    vqa_model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n    vqa_model.eval()\n    modified_state_dict = vqa_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_vqa_model = BlipForQuestionAnswering(config)\n    hf_vqa_model.load_state_dict(modified_state_dict)\n    question = ['How many dogs are in this image?']\n    question_input_ids = tokenizer(question, return_tensors='pt').input_ids\n    answer = hf_vqa_model.generate(question_input_ids, image)\n    print(tokenizer.decode(answer[0]))\n    assert tokenizer.decode(answer[0]) == '[UNK] 1 [SEP]'\n    if pytorch_dump_folder_path is not None:\n        hf_vqa_model.save_pretrained(pytorch_dump_folder_path + '_vqa')\n    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n    itm_model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n    itm_model.eval()\n    modified_state_dict = itm_model.state_dict()\n    for key in modified_state_dict.copy():\n        value = modified_state_dict.pop(key)\n        renamed_key = rename_key(key)\n        modified_state_dict[renamed_key] = value\n    hf_itm_model = BlipForImageTextRetrieval(config)\n    question = ['A picture of a woman with a dog sitting in a beach']\n    question_input_ids = tokenizer(question, return_tensors='pt', padding='max_length', truncation=True, max_length=35).input_ids\n    hf_itm_model.load_state_dict(modified_state_dict)\n    hf_itm_model.eval()\n    out_itm = hf_itm_model(question_input_ids, image, use_itm_head=True)\n    out = hf_itm_model(question_input_ids, image, use_itm_head=False)\n    assert out[0].item() == 0.2110687494277954\n    assert torch.nn.functional.softmax(out_itm[0], dim=1)[:, 1].item() == 0.45698845386505127\n    if pytorch_dump_folder_path is not None:\n        hf_itm_model.save_pretrained(pytorch_dump_folder_path + '_itm')"
        ]
    }
]