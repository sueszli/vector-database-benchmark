[
    {
        "func_name": "exp_lr_scheduler",
        "original": "def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n    current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n    if step % lr_decay_step == 0:\n        print('learning rate is set to %f' % current_lr)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = current_lr\n    return optimizer",
        "mutated": [
            "def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n    if False:\n        i = 10\n    current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n    if step % lr_decay_step == 0:\n        print('learning rate is set to %f' % current_lr)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = current_lr\n    return optimizer",
            "def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n    if step % lr_decay_step == 0:\n        print('learning rate is set to %f' % current_lr)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = current_lr\n    return optimizer",
            "def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n    if step % lr_decay_step == 0:\n        print('learning rate is set to %f' % current_lr)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = current_lr\n    return optimizer",
            "def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n    if step % lr_decay_step == 0:\n        print('learning rate is set to %f' % current_lr)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = current_lr\n    return optimizer",
            "def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n    if step % lr_decay_step == 0:\n        print('learning rate is set to %f' % current_lr)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = current_lr\n    return optimizer"
        ]
    },
    {
        "func_name": "run",
        "original": "def run(net_str):\n    net_str = os.path.join('D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2', net_str)\n    source_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset1')\n    target_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset2')\n    target = 'dataset2'\n    p = str(8)\n    model_root = 'dataset1' + p + 'dataset2'\n    if not os.path.exists(model_root):\n        os.mkdir(model_root)\n    if not os.path.exists(model_root):\n        os.makedirs(model_root)\n    log_path = os.path.join(model_root, 'train.txt')\n    sys.stdout = Logger(log_path)\n    cuda = False\n    cudnn.benchmark = True\n    lr = 0.01\n    batch_size = 16\n    image_size = 28\n    n_epoch = 1\n    step_decay_weight = 0.95\n    lr_decay_step = 20000\n    active_domain_loss_step = 10000\n    weight_decay = 1e-06\n    alpha_weight = 0.01\n    beta_weight = 0.075\n    gamma_weight = 0.25\n    momentum = 0.9\n    manual_seed = random.randint(1, 10000)\n    random.seed(manual_seed)\n    torch.manual_seed(manual_seed)\n    img_transform_source = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.1307,), std=(0.3081,))])\n    img_transform_target = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n    source_list = os.path.join(source_image_root, 'dataset1_train_labels.txt')\n    dataset_source = GetLoader(data_root=os.path.join(source_image_root, 'dataset1_train'), data_list=source_list, transform=img_transform_target)\n    dataloader_source = torch.utils.data.DataLoader(dataset=dataset_source, batch_size=batch_size, shuffle=True, num_workers=0)\n    target_list = os.path.join(target_image_root, 'dataset2_train_labels.txt')\n    dataset_target = GetLoader(data_root=os.path.join(target_image_root, 'dataset2_train'), data_list=target_list, transform=img_transform_target)\n    dataloader_target = torch.utils.data.DataLoader(dataset=dataset_target, batch_size=batch_size, shuffle=True, num_workers=0)\n    my_net = DSN()\n    my_net.load_state_dict(torch.load(net_str))\n\n    def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n        current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n        if step % lr_decay_step == 0:\n            print('learning rate is set to %f' % current_lr)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = current_lr\n        return optimizer\n    optimizer = optim.SGD(my_net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    loss_classfication = torch.nn.CrossEntropyLoss()\n    loss_recon1 = MSE()\n    loss_recon2 = SIMSE()\n    loss_diff = DiffLoss_tfTrans()\n    loss_similarity = torch.nn.CrossEntropyLoss()\n    if cuda:\n        my_net = my_net.cuda()\n        loss_classification = loss_classification.cuda()\n        loss_recon1 = loss_recon1.cuda()\n        loss_recon2 = loss_recon2.cuda()\n        loss_diff = loss_diff.cuda()\n        loss_similarity = loss_similarity.cuda()\n    for p in my_net.parameters():\n        p.requires_grad = True\n    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n    dann_epoch = np.floor(active_domain_loss_step / len_dataloader * 1.0)\n    current_step = 0\n    accu_total1 = 0\n    accu_total2 = 0\n    time_total1 = 0\n    time_total2 = 0\n    for epoch in range(n_epoch):\n        data_source_iter = iter(dataloader_source)\n        data_target_iter = iter(dataloader_target)\n        i = 0\n        while i < len_dataloader:\n            data_target = data_target_iter.next()\n            (t_img, t_label) = data_target\n            my_net.zero_grad()\n            loss = 0\n            batch_size = len(t_label)\n            input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n            class_label = torch.LongTensor(batch_size)\n            domain_label = torch.ones(batch_size)\n            domain_label = domain_label.long()\n            if cuda:\n                t_img = t_img.cuda()\n                t_label = t_label.cuda()\n                input_img = input_img.cuda()\n                class_label = class_label.cuda()\n                domain_label = domain_label.cuda()\n            input_img.resize_as_(t_img).copy_(t_img)\n            class_label.resize_as_(t_label).copy_(t_label)\n            target_inputv_img = Variable(input_img)\n            target_classv_label = Variable(class_label)\n            target_domainv_label = Variable(domain_label)\n            if current_step > active_domain_loss_step:\n                p = float(i + (epoch - dann_epoch) * len_dataloader / (n_epoch - dann_epoch) / len_dataloader)\n                p = 2.0 / (1.0 + np.exp(-10 * p)) - 1\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all', p=p)\n                (target_private_coda, target_share_coda, target_domain_label, target_rec_code) = result\n                target_dann = gamma_weight * loss_similarity(target_domain_label, target_domainv_label)\n                loss += target_dann\n            else:\n                if cuda:\n                    target_dann = Variable(torch.zeros(1).float().cuda())\n                else:\n                    target_dann = Variable(torch.zeros(1).float())\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all')\n                (target_private_coda, target_share_coda, _, target_rec_code) = result\n                target_diff = beta_weight * loss_diff(target_private_coda, target_share_coda, weight=0.05)\n                loss += target_diff\n                target_mse = alpha_weight * loss_recon1(target_rec_code, target_inputv_img)\n                loss += target_mse\n                target_simse = alpha_weight * loss_recon2(target_rec_code, target_inputv_img)\n                loss += target_mse\n                loss.backward()\n                optimizer.step()\n                data_source = data_source_iter.next()\n                (s_img, s_label) = data_source\n                my_net.zero_grad()\n                batch_size = len(s_label)\n                input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n                class_label = torch.LongTensor(batch_size)\n                domain_label = torch.zeros(batch_size)\n                damain_label = domain_label.long()\n                loss = 0\n                if cuda:\n                    s_img = s_img.cuda()\n                    s_label = s_label.cuda()\n                    input_img = input_img.cuda()\n                    class_label = class_label.cuda()\n                    domain_label = domain_label.cuda()\n                input_img.resize_as_(input_img).copy_(s_img)\n                class_label.resize_as_(s_label).copy_(s_label)\n                source_inputv_img = Variable(input_img)\n                source_classv_label = Variable(class_label)\n                source_domainv_label = Variable(domain_label)\n                if current_step > active_domain_loss_step:\n                    result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all', p=p)\n                    (source_private_code, source_share_code, source_domain_label, source_classv_label, source_rec_code) = result\n                    source_dann = gamma_weight * loss_similarity(source_domain_label, source_classv_label)\n                    loss += source_dann\n                else:\n                    if cuda:\n                        source_dann = Variable(torch.zeros(1).float().cuda())\n                    else:\n                        if cuda:\n                            source_dann = Variable(torch.zeros(1).float().cuda())\n                        else:\n                            source_dann = Variable(torch.zeros(1).float())\n                        result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all')\n                        (source_private_code, source_share_code, _, source_class_label, source_rec_code) = result\n                    source_classification = loss_classfication(source_class_label, source_classv_label)\n                    loss += source_classification\n                    source_diff = beta_weight * loss_diff(source_private_code, source_share_code, weight=0.05)\n                    loss += source_diff\n                    source_mse = alpha_weight * loss_recon1(source_rec_code, source_inputv_img)\n                    loss += source_mse\n                    source_simse = gamma_weight * loss_recon2(source_rec_code, source_inputv_img)\n                    loss += source_simse\n                    loss.backward()\n                    optimizer.step()\n                    i += 1\n                    current_step += 1\n                    start1 = time.time()\n                    accu1 = test(epoch=epoch, name='dataset1')\n                    end1 = time.time()\n                    curr1 = end1 - start1\n                    time_total1 += curr1\n                    accu_total1 += accu1\n                    start2 = time.time()\n                    accu2 = test(epoch=epoch, name='dataset2')\n                    end2 = time.time()\n                    curr2 = end2 - start2\n                    time_total2 += curr2\n                    accu_total2 += accu2\n    model_index = epoch\n    model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    while os.path.exists(model_path):\n        model_index = model_index + 1\n        model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    torch.save(my_net.state_dict(), model_path)\n    average_accu1 = accu_total1 / (len_dataloader * n_epoch)\n    average_accu2 = accu_total2 / (len_dataloader * n_epoch)\n    print(round(float(average_accu1), 3))\n    print(round(float(average_accu2), 3))\n    print(round(float(time_total1), 3))\n    print(round(float(time_total2), 3))\n    return result",
        "mutated": [
            "def run(net_str):\n    if False:\n        i = 10\n    net_str = os.path.join('D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2', net_str)\n    source_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset1')\n    target_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset2')\n    target = 'dataset2'\n    p = str(8)\n    model_root = 'dataset1' + p + 'dataset2'\n    if not os.path.exists(model_root):\n        os.mkdir(model_root)\n    if not os.path.exists(model_root):\n        os.makedirs(model_root)\n    log_path = os.path.join(model_root, 'train.txt')\n    sys.stdout = Logger(log_path)\n    cuda = False\n    cudnn.benchmark = True\n    lr = 0.01\n    batch_size = 16\n    image_size = 28\n    n_epoch = 1\n    step_decay_weight = 0.95\n    lr_decay_step = 20000\n    active_domain_loss_step = 10000\n    weight_decay = 1e-06\n    alpha_weight = 0.01\n    beta_weight = 0.075\n    gamma_weight = 0.25\n    momentum = 0.9\n    manual_seed = random.randint(1, 10000)\n    random.seed(manual_seed)\n    torch.manual_seed(manual_seed)\n    img_transform_source = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.1307,), std=(0.3081,))])\n    img_transform_target = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n    source_list = os.path.join(source_image_root, 'dataset1_train_labels.txt')\n    dataset_source = GetLoader(data_root=os.path.join(source_image_root, 'dataset1_train'), data_list=source_list, transform=img_transform_target)\n    dataloader_source = torch.utils.data.DataLoader(dataset=dataset_source, batch_size=batch_size, shuffle=True, num_workers=0)\n    target_list = os.path.join(target_image_root, 'dataset2_train_labels.txt')\n    dataset_target = GetLoader(data_root=os.path.join(target_image_root, 'dataset2_train'), data_list=target_list, transform=img_transform_target)\n    dataloader_target = torch.utils.data.DataLoader(dataset=dataset_target, batch_size=batch_size, shuffle=True, num_workers=0)\n    my_net = DSN()\n    my_net.load_state_dict(torch.load(net_str))\n\n    def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n        current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n        if step % lr_decay_step == 0:\n            print('learning rate is set to %f' % current_lr)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = current_lr\n        return optimizer\n    optimizer = optim.SGD(my_net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    loss_classfication = torch.nn.CrossEntropyLoss()\n    loss_recon1 = MSE()\n    loss_recon2 = SIMSE()\n    loss_diff = DiffLoss_tfTrans()\n    loss_similarity = torch.nn.CrossEntropyLoss()\n    if cuda:\n        my_net = my_net.cuda()\n        loss_classification = loss_classification.cuda()\n        loss_recon1 = loss_recon1.cuda()\n        loss_recon2 = loss_recon2.cuda()\n        loss_diff = loss_diff.cuda()\n        loss_similarity = loss_similarity.cuda()\n    for p in my_net.parameters():\n        p.requires_grad = True\n    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n    dann_epoch = np.floor(active_domain_loss_step / len_dataloader * 1.0)\n    current_step = 0\n    accu_total1 = 0\n    accu_total2 = 0\n    time_total1 = 0\n    time_total2 = 0\n    for epoch in range(n_epoch):\n        data_source_iter = iter(dataloader_source)\n        data_target_iter = iter(dataloader_target)\n        i = 0\n        while i < len_dataloader:\n            data_target = data_target_iter.next()\n            (t_img, t_label) = data_target\n            my_net.zero_grad()\n            loss = 0\n            batch_size = len(t_label)\n            input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n            class_label = torch.LongTensor(batch_size)\n            domain_label = torch.ones(batch_size)\n            domain_label = domain_label.long()\n            if cuda:\n                t_img = t_img.cuda()\n                t_label = t_label.cuda()\n                input_img = input_img.cuda()\n                class_label = class_label.cuda()\n                domain_label = domain_label.cuda()\n            input_img.resize_as_(t_img).copy_(t_img)\n            class_label.resize_as_(t_label).copy_(t_label)\n            target_inputv_img = Variable(input_img)\n            target_classv_label = Variable(class_label)\n            target_domainv_label = Variable(domain_label)\n            if current_step > active_domain_loss_step:\n                p = float(i + (epoch - dann_epoch) * len_dataloader / (n_epoch - dann_epoch) / len_dataloader)\n                p = 2.0 / (1.0 + np.exp(-10 * p)) - 1\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all', p=p)\n                (target_private_coda, target_share_coda, target_domain_label, target_rec_code) = result\n                target_dann = gamma_weight * loss_similarity(target_domain_label, target_domainv_label)\n                loss += target_dann\n            else:\n                if cuda:\n                    target_dann = Variable(torch.zeros(1).float().cuda())\n                else:\n                    target_dann = Variable(torch.zeros(1).float())\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all')\n                (target_private_coda, target_share_coda, _, target_rec_code) = result\n                target_diff = beta_weight * loss_diff(target_private_coda, target_share_coda, weight=0.05)\n                loss += target_diff\n                target_mse = alpha_weight * loss_recon1(target_rec_code, target_inputv_img)\n                loss += target_mse\n                target_simse = alpha_weight * loss_recon2(target_rec_code, target_inputv_img)\n                loss += target_mse\n                loss.backward()\n                optimizer.step()\n                data_source = data_source_iter.next()\n                (s_img, s_label) = data_source\n                my_net.zero_grad()\n                batch_size = len(s_label)\n                input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n                class_label = torch.LongTensor(batch_size)\n                domain_label = torch.zeros(batch_size)\n                damain_label = domain_label.long()\n                loss = 0\n                if cuda:\n                    s_img = s_img.cuda()\n                    s_label = s_label.cuda()\n                    input_img = input_img.cuda()\n                    class_label = class_label.cuda()\n                    domain_label = domain_label.cuda()\n                input_img.resize_as_(input_img).copy_(s_img)\n                class_label.resize_as_(s_label).copy_(s_label)\n                source_inputv_img = Variable(input_img)\n                source_classv_label = Variable(class_label)\n                source_domainv_label = Variable(domain_label)\n                if current_step > active_domain_loss_step:\n                    result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all', p=p)\n                    (source_private_code, source_share_code, source_domain_label, source_classv_label, source_rec_code) = result\n                    source_dann = gamma_weight * loss_similarity(source_domain_label, source_classv_label)\n                    loss += source_dann\n                else:\n                    if cuda:\n                        source_dann = Variable(torch.zeros(1).float().cuda())\n                    else:\n                        if cuda:\n                            source_dann = Variable(torch.zeros(1).float().cuda())\n                        else:\n                            source_dann = Variable(torch.zeros(1).float())\n                        result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all')\n                        (source_private_code, source_share_code, _, source_class_label, source_rec_code) = result\n                    source_classification = loss_classfication(source_class_label, source_classv_label)\n                    loss += source_classification\n                    source_diff = beta_weight * loss_diff(source_private_code, source_share_code, weight=0.05)\n                    loss += source_diff\n                    source_mse = alpha_weight * loss_recon1(source_rec_code, source_inputv_img)\n                    loss += source_mse\n                    source_simse = gamma_weight * loss_recon2(source_rec_code, source_inputv_img)\n                    loss += source_simse\n                    loss.backward()\n                    optimizer.step()\n                    i += 1\n                    current_step += 1\n                    start1 = time.time()\n                    accu1 = test(epoch=epoch, name='dataset1')\n                    end1 = time.time()\n                    curr1 = end1 - start1\n                    time_total1 += curr1\n                    accu_total1 += accu1\n                    start2 = time.time()\n                    accu2 = test(epoch=epoch, name='dataset2')\n                    end2 = time.time()\n                    curr2 = end2 - start2\n                    time_total2 += curr2\n                    accu_total2 += accu2\n    model_index = epoch\n    model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    while os.path.exists(model_path):\n        model_index = model_index + 1\n        model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    torch.save(my_net.state_dict(), model_path)\n    average_accu1 = accu_total1 / (len_dataloader * n_epoch)\n    average_accu2 = accu_total2 / (len_dataloader * n_epoch)\n    print(round(float(average_accu1), 3))\n    print(round(float(average_accu2), 3))\n    print(round(float(time_total1), 3))\n    print(round(float(time_total2), 3))\n    return result",
            "def run(net_str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    net_str = os.path.join('D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2', net_str)\n    source_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset1')\n    target_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset2')\n    target = 'dataset2'\n    p = str(8)\n    model_root = 'dataset1' + p + 'dataset2'\n    if not os.path.exists(model_root):\n        os.mkdir(model_root)\n    if not os.path.exists(model_root):\n        os.makedirs(model_root)\n    log_path = os.path.join(model_root, 'train.txt')\n    sys.stdout = Logger(log_path)\n    cuda = False\n    cudnn.benchmark = True\n    lr = 0.01\n    batch_size = 16\n    image_size = 28\n    n_epoch = 1\n    step_decay_weight = 0.95\n    lr_decay_step = 20000\n    active_domain_loss_step = 10000\n    weight_decay = 1e-06\n    alpha_weight = 0.01\n    beta_weight = 0.075\n    gamma_weight = 0.25\n    momentum = 0.9\n    manual_seed = random.randint(1, 10000)\n    random.seed(manual_seed)\n    torch.manual_seed(manual_seed)\n    img_transform_source = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.1307,), std=(0.3081,))])\n    img_transform_target = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n    source_list = os.path.join(source_image_root, 'dataset1_train_labels.txt')\n    dataset_source = GetLoader(data_root=os.path.join(source_image_root, 'dataset1_train'), data_list=source_list, transform=img_transform_target)\n    dataloader_source = torch.utils.data.DataLoader(dataset=dataset_source, batch_size=batch_size, shuffle=True, num_workers=0)\n    target_list = os.path.join(target_image_root, 'dataset2_train_labels.txt')\n    dataset_target = GetLoader(data_root=os.path.join(target_image_root, 'dataset2_train'), data_list=target_list, transform=img_transform_target)\n    dataloader_target = torch.utils.data.DataLoader(dataset=dataset_target, batch_size=batch_size, shuffle=True, num_workers=0)\n    my_net = DSN()\n    my_net.load_state_dict(torch.load(net_str))\n\n    def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n        current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n        if step % lr_decay_step == 0:\n            print('learning rate is set to %f' % current_lr)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = current_lr\n        return optimizer\n    optimizer = optim.SGD(my_net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    loss_classfication = torch.nn.CrossEntropyLoss()\n    loss_recon1 = MSE()\n    loss_recon2 = SIMSE()\n    loss_diff = DiffLoss_tfTrans()\n    loss_similarity = torch.nn.CrossEntropyLoss()\n    if cuda:\n        my_net = my_net.cuda()\n        loss_classification = loss_classification.cuda()\n        loss_recon1 = loss_recon1.cuda()\n        loss_recon2 = loss_recon2.cuda()\n        loss_diff = loss_diff.cuda()\n        loss_similarity = loss_similarity.cuda()\n    for p in my_net.parameters():\n        p.requires_grad = True\n    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n    dann_epoch = np.floor(active_domain_loss_step / len_dataloader * 1.0)\n    current_step = 0\n    accu_total1 = 0\n    accu_total2 = 0\n    time_total1 = 0\n    time_total2 = 0\n    for epoch in range(n_epoch):\n        data_source_iter = iter(dataloader_source)\n        data_target_iter = iter(dataloader_target)\n        i = 0\n        while i < len_dataloader:\n            data_target = data_target_iter.next()\n            (t_img, t_label) = data_target\n            my_net.zero_grad()\n            loss = 0\n            batch_size = len(t_label)\n            input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n            class_label = torch.LongTensor(batch_size)\n            domain_label = torch.ones(batch_size)\n            domain_label = domain_label.long()\n            if cuda:\n                t_img = t_img.cuda()\n                t_label = t_label.cuda()\n                input_img = input_img.cuda()\n                class_label = class_label.cuda()\n                domain_label = domain_label.cuda()\n            input_img.resize_as_(t_img).copy_(t_img)\n            class_label.resize_as_(t_label).copy_(t_label)\n            target_inputv_img = Variable(input_img)\n            target_classv_label = Variable(class_label)\n            target_domainv_label = Variable(domain_label)\n            if current_step > active_domain_loss_step:\n                p = float(i + (epoch - dann_epoch) * len_dataloader / (n_epoch - dann_epoch) / len_dataloader)\n                p = 2.0 / (1.0 + np.exp(-10 * p)) - 1\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all', p=p)\n                (target_private_coda, target_share_coda, target_domain_label, target_rec_code) = result\n                target_dann = gamma_weight * loss_similarity(target_domain_label, target_domainv_label)\n                loss += target_dann\n            else:\n                if cuda:\n                    target_dann = Variable(torch.zeros(1).float().cuda())\n                else:\n                    target_dann = Variable(torch.zeros(1).float())\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all')\n                (target_private_coda, target_share_coda, _, target_rec_code) = result\n                target_diff = beta_weight * loss_diff(target_private_coda, target_share_coda, weight=0.05)\n                loss += target_diff\n                target_mse = alpha_weight * loss_recon1(target_rec_code, target_inputv_img)\n                loss += target_mse\n                target_simse = alpha_weight * loss_recon2(target_rec_code, target_inputv_img)\n                loss += target_mse\n                loss.backward()\n                optimizer.step()\n                data_source = data_source_iter.next()\n                (s_img, s_label) = data_source\n                my_net.zero_grad()\n                batch_size = len(s_label)\n                input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n                class_label = torch.LongTensor(batch_size)\n                domain_label = torch.zeros(batch_size)\n                damain_label = domain_label.long()\n                loss = 0\n                if cuda:\n                    s_img = s_img.cuda()\n                    s_label = s_label.cuda()\n                    input_img = input_img.cuda()\n                    class_label = class_label.cuda()\n                    domain_label = domain_label.cuda()\n                input_img.resize_as_(input_img).copy_(s_img)\n                class_label.resize_as_(s_label).copy_(s_label)\n                source_inputv_img = Variable(input_img)\n                source_classv_label = Variable(class_label)\n                source_domainv_label = Variable(domain_label)\n                if current_step > active_domain_loss_step:\n                    result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all', p=p)\n                    (source_private_code, source_share_code, source_domain_label, source_classv_label, source_rec_code) = result\n                    source_dann = gamma_weight * loss_similarity(source_domain_label, source_classv_label)\n                    loss += source_dann\n                else:\n                    if cuda:\n                        source_dann = Variable(torch.zeros(1).float().cuda())\n                    else:\n                        if cuda:\n                            source_dann = Variable(torch.zeros(1).float().cuda())\n                        else:\n                            source_dann = Variable(torch.zeros(1).float())\n                        result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all')\n                        (source_private_code, source_share_code, _, source_class_label, source_rec_code) = result\n                    source_classification = loss_classfication(source_class_label, source_classv_label)\n                    loss += source_classification\n                    source_diff = beta_weight * loss_diff(source_private_code, source_share_code, weight=0.05)\n                    loss += source_diff\n                    source_mse = alpha_weight * loss_recon1(source_rec_code, source_inputv_img)\n                    loss += source_mse\n                    source_simse = gamma_weight * loss_recon2(source_rec_code, source_inputv_img)\n                    loss += source_simse\n                    loss.backward()\n                    optimizer.step()\n                    i += 1\n                    current_step += 1\n                    start1 = time.time()\n                    accu1 = test(epoch=epoch, name='dataset1')\n                    end1 = time.time()\n                    curr1 = end1 - start1\n                    time_total1 += curr1\n                    accu_total1 += accu1\n                    start2 = time.time()\n                    accu2 = test(epoch=epoch, name='dataset2')\n                    end2 = time.time()\n                    curr2 = end2 - start2\n                    time_total2 += curr2\n                    accu_total2 += accu2\n    model_index = epoch\n    model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    while os.path.exists(model_path):\n        model_index = model_index + 1\n        model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    torch.save(my_net.state_dict(), model_path)\n    average_accu1 = accu_total1 / (len_dataloader * n_epoch)\n    average_accu2 = accu_total2 / (len_dataloader * n_epoch)\n    print(round(float(average_accu1), 3))\n    print(round(float(average_accu2), 3))\n    print(round(float(time_total1), 3))\n    print(round(float(time_total2), 3))\n    return result",
            "def run(net_str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    net_str = os.path.join('D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2', net_str)\n    source_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset1')\n    target_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset2')\n    target = 'dataset2'\n    p = str(8)\n    model_root = 'dataset1' + p + 'dataset2'\n    if not os.path.exists(model_root):\n        os.mkdir(model_root)\n    if not os.path.exists(model_root):\n        os.makedirs(model_root)\n    log_path = os.path.join(model_root, 'train.txt')\n    sys.stdout = Logger(log_path)\n    cuda = False\n    cudnn.benchmark = True\n    lr = 0.01\n    batch_size = 16\n    image_size = 28\n    n_epoch = 1\n    step_decay_weight = 0.95\n    lr_decay_step = 20000\n    active_domain_loss_step = 10000\n    weight_decay = 1e-06\n    alpha_weight = 0.01\n    beta_weight = 0.075\n    gamma_weight = 0.25\n    momentum = 0.9\n    manual_seed = random.randint(1, 10000)\n    random.seed(manual_seed)\n    torch.manual_seed(manual_seed)\n    img_transform_source = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.1307,), std=(0.3081,))])\n    img_transform_target = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n    source_list = os.path.join(source_image_root, 'dataset1_train_labels.txt')\n    dataset_source = GetLoader(data_root=os.path.join(source_image_root, 'dataset1_train'), data_list=source_list, transform=img_transform_target)\n    dataloader_source = torch.utils.data.DataLoader(dataset=dataset_source, batch_size=batch_size, shuffle=True, num_workers=0)\n    target_list = os.path.join(target_image_root, 'dataset2_train_labels.txt')\n    dataset_target = GetLoader(data_root=os.path.join(target_image_root, 'dataset2_train'), data_list=target_list, transform=img_transform_target)\n    dataloader_target = torch.utils.data.DataLoader(dataset=dataset_target, batch_size=batch_size, shuffle=True, num_workers=0)\n    my_net = DSN()\n    my_net.load_state_dict(torch.load(net_str))\n\n    def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n        current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n        if step % lr_decay_step == 0:\n            print('learning rate is set to %f' % current_lr)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = current_lr\n        return optimizer\n    optimizer = optim.SGD(my_net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    loss_classfication = torch.nn.CrossEntropyLoss()\n    loss_recon1 = MSE()\n    loss_recon2 = SIMSE()\n    loss_diff = DiffLoss_tfTrans()\n    loss_similarity = torch.nn.CrossEntropyLoss()\n    if cuda:\n        my_net = my_net.cuda()\n        loss_classification = loss_classification.cuda()\n        loss_recon1 = loss_recon1.cuda()\n        loss_recon2 = loss_recon2.cuda()\n        loss_diff = loss_diff.cuda()\n        loss_similarity = loss_similarity.cuda()\n    for p in my_net.parameters():\n        p.requires_grad = True\n    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n    dann_epoch = np.floor(active_domain_loss_step / len_dataloader * 1.0)\n    current_step = 0\n    accu_total1 = 0\n    accu_total2 = 0\n    time_total1 = 0\n    time_total2 = 0\n    for epoch in range(n_epoch):\n        data_source_iter = iter(dataloader_source)\n        data_target_iter = iter(dataloader_target)\n        i = 0\n        while i < len_dataloader:\n            data_target = data_target_iter.next()\n            (t_img, t_label) = data_target\n            my_net.zero_grad()\n            loss = 0\n            batch_size = len(t_label)\n            input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n            class_label = torch.LongTensor(batch_size)\n            domain_label = torch.ones(batch_size)\n            domain_label = domain_label.long()\n            if cuda:\n                t_img = t_img.cuda()\n                t_label = t_label.cuda()\n                input_img = input_img.cuda()\n                class_label = class_label.cuda()\n                domain_label = domain_label.cuda()\n            input_img.resize_as_(t_img).copy_(t_img)\n            class_label.resize_as_(t_label).copy_(t_label)\n            target_inputv_img = Variable(input_img)\n            target_classv_label = Variable(class_label)\n            target_domainv_label = Variable(domain_label)\n            if current_step > active_domain_loss_step:\n                p = float(i + (epoch - dann_epoch) * len_dataloader / (n_epoch - dann_epoch) / len_dataloader)\n                p = 2.0 / (1.0 + np.exp(-10 * p)) - 1\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all', p=p)\n                (target_private_coda, target_share_coda, target_domain_label, target_rec_code) = result\n                target_dann = gamma_weight * loss_similarity(target_domain_label, target_domainv_label)\n                loss += target_dann\n            else:\n                if cuda:\n                    target_dann = Variable(torch.zeros(1).float().cuda())\n                else:\n                    target_dann = Variable(torch.zeros(1).float())\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all')\n                (target_private_coda, target_share_coda, _, target_rec_code) = result\n                target_diff = beta_weight * loss_diff(target_private_coda, target_share_coda, weight=0.05)\n                loss += target_diff\n                target_mse = alpha_weight * loss_recon1(target_rec_code, target_inputv_img)\n                loss += target_mse\n                target_simse = alpha_weight * loss_recon2(target_rec_code, target_inputv_img)\n                loss += target_mse\n                loss.backward()\n                optimizer.step()\n                data_source = data_source_iter.next()\n                (s_img, s_label) = data_source\n                my_net.zero_grad()\n                batch_size = len(s_label)\n                input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n                class_label = torch.LongTensor(batch_size)\n                domain_label = torch.zeros(batch_size)\n                damain_label = domain_label.long()\n                loss = 0\n                if cuda:\n                    s_img = s_img.cuda()\n                    s_label = s_label.cuda()\n                    input_img = input_img.cuda()\n                    class_label = class_label.cuda()\n                    domain_label = domain_label.cuda()\n                input_img.resize_as_(input_img).copy_(s_img)\n                class_label.resize_as_(s_label).copy_(s_label)\n                source_inputv_img = Variable(input_img)\n                source_classv_label = Variable(class_label)\n                source_domainv_label = Variable(domain_label)\n                if current_step > active_domain_loss_step:\n                    result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all', p=p)\n                    (source_private_code, source_share_code, source_domain_label, source_classv_label, source_rec_code) = result\n                    source_dann = gamma_weight * loss_similarity(source_domain_label, source_classv_label)\n                    loss += source_dann\n                else:\n                    if cuda:\n                        source_dann = Variable(torch.zeros(1).float().cuda())\n                    else:\n                        if cuda:\n                            source_dann = Variable(torch.zeros(1).float().cuda())\n                        else:\n                            source_dann = Variable(torch.zeros(1).float())\n                        result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all')\n                        (source_private_code, source_share_code, _, source_class_label, source_rec_code) = result\n                    source_classification = loss_classfication(source_class_label, source_classv_label)\n                    loss += source_classification\n                    source_diff = beta_weight * loss_diff(source_private_code, source_share_code, weight=0.05)\n                    loss += source_diff\n                    source_mse = alpha_weight * loss_recon1(source_rec_code, source_inputv_img)\n                    loss += source_mse\n                    source_simse = gamma_weight * loss_recon2(source_rec_code, source_inputv_img)\n                    loss += source_simse\n                    loss.backward()\n                    optimizer.step()\n                    i += 1\n                    current_step += 1\n                    start1 = time.time()\n                    accu1 = test(epoch=epoch, name='dataset1')\n                    end1 = time.time()\n                    curr1 = end1 - start1\n                    time_total1 += curr1\n                    accu_total1 += accu1\n                    start2 = time.time()\n                    accu2 = test(epoch=epoch, name='dataset2')\n                    end2 = time.time()\n                    curr2 = end2 - start2\n                    time_total2 += curr2\n                    accu_total2 += accu2\n    model_index = epoch\n    model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    while os.path.exists(model_path):\n        model_index = model_index + 1\n        model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    torch.save(my_net.state_dict(), model_path)\n    average_accu1 = accu_total1 / (len_dataloader * n_epoch)\n    average_accu2 = accu_total2 / (len_dataloader * n_epoch)\n    print(round(float(average_accu1), 3))\n    print(round(float(average_accu2), 3))\n    print(round(float(time_total1), 3))\n    print(round(float(time_total2), 3))\n    return result",
            "def run(net_str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    net_str = os.path.join('D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2', net_str)\n    source_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset1')\n    target_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset2')\n    target = 'dataset2'\n    p = str(8)\n    model_root = 'dataset1' + p + 'dataset2'\n    if not os.path.exists(model_root):\n        os.mkdir(model_root)\n    if not os.path.exists(model_root):\n        os.makedirs(model_root)\n    log_path = os.path.join(model_root, 'train.txt')\n    sys.stdout = Logger(log_path)\n    cuda = False\n    cudnn.benchmark = True\n    lr = 0.01\n    batch_size = 16\n    image_size = 28\n    n_epoch = 1\n    step_decay_weight = 0.95\n    lr_decay_step = 20000\n    active_domain_loss_step = 10000\n    weight_decay = 1e-06\n    alpha_weight = 0.01\n    beta_weight = 0.075\n    gamma_weight = 0.25\n    momentum = 0.9\n    manual_seed = random.randint(1, 10000)\n    random.seed(manual_seed)\n    torch.manual_seed(manual_seed)\n    img_transform_source = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.1307,), std=(0.3081,))])\n    img_transform_target = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n    source_list = os.path.join(source_image_root, 'dataset1_train_labels.txt')\n    dataset_source = GetLoader(data_root=os.path.join(source_image_root, 'dataset1_train'), data_list=source_list, transform=img_transform_target)\n    dataloader_source = torch.utils.data.DataLoader(dataset=dataset_source, batch_size=batch_size, shuffle=True, num_workers=0)\n    target_list = os.path.join(target_image_root, 'dataset2_train_labels.txt')\n    dataset_target = GetLoader(data_root=os.path.join(target_image_root, 'dataset2_train'), data_list=target_list, transform=img_transform_target)\n    dataloader_target = torch.utils.data.DataLoader(dataset=dataset_target, batch_size=batch_size, shuffle=True, num_workers=0)\n    my_net = DSN()\n    my_net.load_state_dict(torch.load(net_str))\n\n    def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n        current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n        if step % lr_decay_step == 0:\n            print('learning rate is set to %f' % current_lr)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = current_lr\n        return optimizer\n    optimizer = optim.SGD(my_net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    loss_classfication = torch.nn.CrossEntropyLoss()\n    loss_recon1 = MSE()\n    loss_recon2 = SIMSE()\n    loss_diff = DiffLoss_tfTrans()\n    loss_similarity = torch.nn.CrossEntropyLoss()\n    if cuda:\n        my_net = my_net.cuda()\n        loss_classification = loss_classification.cuda()\n        loss_recon1 = loss_recon1.cuda()\n        loss_recon2 = loss_recon2.cuda()\n        loss_diff = loss_diff.cuda()\n        loss_similarity = loss_similarity.cuda()\n    for p in my_net.parameters():\n        p.requires_grad = True\n    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n    dann_epoch = np.floor(active_domain_loss_step / len_dataloader * 1.0)\n    current_step = 0\n    accu_total1 = 0\n    accu_total2 = 0\n    time_total1 = 0\n    time_total2 = 0\n    for epoch in range(n_epoch):\n        data_source_iter = iter(dataloader_source)\n        data_target_iter = iter(dataloader_target)\n        i = 0\n        while i < len_dataloader:\n            data_target = data_target_iter.next()\n            (t_img, t_label) = data_target\n            my_net.zero_grad()\n            loss = 0\n            batch_size = len(t_label)\n            input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n            class_label = torch.LongTensor(batch_size)\n            domain_label = torch.ones(batch_size)\n            domain_label = domain_label.long()\n            if cuda:\n                t_img = t_img.cuda()\n                t_label = t_label.cuda()\n                input_img = input_img.cuda()\n                class_label = class_label.cuda()\n                domain_label = domain_label.cuda()\n            input_img.resize_as_(t_img).copy_(t_img)\n            class_label.resize_as_(t_label).copy_(t_label)\n            target_inputv_img = Variable(input_img)\n            target_classv_label = Variable(class_label)\n            target_domainv_label = Variable(domain_label)\n            if current_step > active_domain_loss_step:\n                p = float(i + (epoch - dann_epoch) * len_dataloader / (n_epoch - dann_epoch) / len_dataloader)\n                p = 2.0 / (1.0 + np.exp(-10 * p)) - 1\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all', p=p)\n                (target_private_coda, target_share_coda, target_domain_label, target_rec_code) = result\n                target_dann = gamma_weight * loss_similarity(target_domain_label, target_domainv_label)\n                loss += target_dann\n            else:\n                if cuda:\n                    target_dann = Variable(torch.zeros(1).float().cuda())\n                else:\n                    target_dann = Variable(torch.zeros(1).float())\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all')\n                (target_private_coda, target_share_coda, _, target_rec_code) = result\n                target_diff = beta_weight * loss_diff(target_private_coda, target_share_coda, weight=0.05)\n                loss += target_diff\n                target_mse = alpha_weight * loss_recon1(target_rec_code, target_inputv_img)\n                loss += target_mse\n                target_simse = alpha_weight * loss_recon2(target_rec_code, target_inputv_img)\n                loss += target_mse\n                loss.backward()\n                optimizer.step()\n                data_source = data_source_iter.next()\n                (s_img, s_label) = data_source\n                my_net.zero_grad()\n                batch_size = len(s_label)\n                input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n                class_label = torch.LongTensor(batch_size)\n                domain_label = torch.zeros(batch_size)\n                damain_label = domain_label.long()\n                loss = 0\n                if cuda:\n                    s_img = s_img.cuda()\n                    s_label = s_label.cuda()\n                    input_img = input_img.cuda()\n                    class_label = class_label.cuda()\n                    domain_label = domain_label.cuda()\n                input_img.resize_as_(input_img).copy_(s_img)\n                class_label.resize_as_(s_label).copy_(s_label)\n                source_inputv_img = Variable(input_img)\n                source_classv_label = Variable(class_label)\n                source_domainv_label = Variable(domain_label)\n                if current_step > active_domain_loss_step:\n                    result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all', p=p)\n                    (source_private_code, source_share_code, source_domain_label, source_classv_label, source_rec_code) = result\n                    source_dann = gamma_weight * loss_similarity(source_domain_label, source_classv_label)\n                    loss += source_dann\n                else:\n                    if cuda:\n                        source_dann = Variable(torch.zeros(1).float().cuda())\n                    else:\n                        if cuda:\n                            source_dann = Variable(torch.zeros(1).float().cuda())\n                        else:\n                            source_dann = Variable(torch.zeros(1).float())\n                        result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all')\n                        (source_private_code, source_share_code, _, source_class_label, source_rec_code) = result\n                    source_classification = loss_classfication(source_class_label, source_classv_label)\n                    loss += source_classification\n                    source_diff = beta_weight * loss_diff(source_private_code, source_share_code, weight=0.05)\n                    loss += source_diff\n                    source_mse = alpha_weight * loss_recon1(source_rec_code, source_inputv_img)\n                    loss += source_mse\n                    source_simse = gamma_weight * loss_recon2(source_rec_code, source_inputv_img)\n                    loss += source_simse\n                    loss.backward()\n                    optimizer.step()\n                    i += 1\n                    current_step += 1\n                    start1 = time.time()\n                    accu1 = test(epoch=epoch, name='dataset1')\n                    end1 = time.time()\n                    curr1 = end1 - start1\n                    time_total1 += curr1\n                    accu_total1 += accu1\n                    start2 = time.time()\n                    accu2 = test(epoch=epoch, name='dataset2')\n                    end2 = time.time()\n                    curr2 = end2 - start2\n                    time_total2 += curr2\n                    accu_total2 += accu2\n    model_index = epoch\n    model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    while os.path.exists(model_path):\n        model_index = model_index + 1\n        model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    torch.save(my_net.state_dict(), model_path)\n    average_accu1 = accu_total1 / (len_dataloader * n_epoch)\n    average_accu2 = accu_total2 / (len_dataloader * n_epoch)\n    print(round(float(average_accu1), 3))\n    print(round(float(average_accu2), 3))\n    print(round(float(time_total1), 3))\n    print(round(float(time_total2), 3))\n    return result",
            "def run(net_str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    net_str = os.path.join('D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2', net_str)\n    source_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset1')\n    target_image_root = os.path.join('D:\\\\', 'study', 'graduation_project', 'grdaution_project', 'instru_identify', 'dataset', 'dataset2')\n    target = 'dataset2'\n    p = str(8)\n    model_root = 'dataset1' + p + 'dataset2'\n    if not os.path.exists(model_root):\n        os.mkdir(model_root)\n    if not os.path.exists(model_root):\n        os.makedirs(model_root)\n    log_path = os.path.join(model_root, 'train.txt')\n    sys.stdout = Logger(log_path)\n    cuda = False\n    cudnn.benchmark = True\n    lr = 0.01\n    batch_size = 16\n    image_size = 28\n    n_epoch = 1\n    step_decay_weight = 0.95\n    lr_decay_step = 20000\n    active_domain_loss_step = 10000\n    weight_decay = 1e-06\n    alpha_weight = 0.01\n    beta_weight = 0.075\n    gamma_weight = 0.25\n    momentum = 0.9\n    manual_seed = random.randint(1, 10000)\n    random.seed(manual_seed)\n    torch.manual_seed(manual_seed)\n    img_transform_source = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.1307,), std=(0.3081,))])\n    img_transform_target = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n    source_list = os.path.join(source_image_root, 'dataset1_train_labels.txt')\n    dataset_source = GetLoader(data_root=os.path.join(source_image_root, 'dataset1_train'), data_list=source_list, transform=img_transform_target)\n    dataloader_source = torch.utils.data.DataLoader(dataset=dataset_source, batch_size=batch_size, shuffle=True, num_workers=0)\n    target_list = os.path.join(target_image_root, 'dataset2_train_labels.txt')\n    dataset_target = GetLoader(data_root=os.path.join(target_image_root, 'dataset2_train'), data_list=target_list, transform=img_transform_target)\n    dataloader_target = torch.utils.data.DataLoader(dataset=dataset_target, batch_size=batch_size, shuffle=True, num_workers=0)\n    my_net = DSN()\n    my_net.load_state_dict(torch.load(net_str))\n\n    def exp_lr_scheduler(optimizer, step, init_lr=lr, lr_decay_step=lr_decay_step, step_decay_weight=step_decay_weight):\n        current_lr = init_lr * step_decay_weight ** (step / lr_decay_step)\n        if step % lr_decay_step == 0:\n            print('learning rate is set to %f' % current_lr)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = current_lr\n        return optimizer\n    optimizer = optim.SGD(my_net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    loss_classfication = torch.nn.CrossEntropyLoss()\n    loss_recon1 = MSE()\n    loss_recon2 = SIMSE()\n    loss_diff = DiffLoss_tfTrans()\n    loss_similarity = torch.nn.CrossEntropyLoss()\n    if cuda:\n        my_net = my_net.cuda()\n        loss_classification = loss_classification.cuda()\n        loss_recon1 = loss_recon1.cuda()\n        loss_recon2 = loss_recon2.cuda()\n        loss_diff = loss_diff.cuda()\n        loss_similarity = loss_similarity.cuda()\n    for p in my_net.parameters():\n        p.requires_grad = True\n    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n    dann_epoch = np.floor(active_domain_loss_step / len_dataloader * 1.0)\n    current_step = 0\n    accu_total1 = 0\n    accu_total2 = 0\n    time_total1 = 0\n    time_total2 = 0\n    for epoch in range(n_epoch):\n        data_source_iter = iter(dataloader_source)\n        data_target_iter = iter(dataloader_target)\n        i = 0\n        while i < len_dataloader:\n            data_target = data_target_iter.next()\n            (t_img, t_label) = data_target\n            my_net.zero_grad()\n            loss = 0\n            batch_size = len(t_label)\n            input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n            class_label = torch.LongTensor(batch_size)\n            domain_label = torch.ones(batch_size)\n            domain_label = domain_label.long()\n            if cuda:\n                t_img = t_img.cuda()\n                t_label = t_label.cuda()\n                input_img = input_img.cuda()\n                class_label = class_label.cuda()\n                domain_label = domain_label.cuda()\n            input_img.resize_as_(t_img).copy_(t_img)\n            class_label.resize_as_(t_label).copy_(t_label)\n            target_inputv_img = Variable(input_img)\n            target_classv_label = Variable(class_label)\n            target_domainv_label = Variable(domain_label)\n            if current_step > active_domain_loss_step:\n                p = float(i + (epoch - dann_epoch) * len_dataloader / (n_epoch - dann_epoch) / len_dataloader)\n                p = 2.0 / (1.0 + np.exp(-10 * p)) - 1\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all', p=p)\n                (target_private_coda, target_share_coda, target_domain_label, target_rec_code) = result\n                target_dann = gamma_weight * loss_similarity(target_domain_label, target_domainv_label)\n                loss += target_dann\n            else:\n                if cuda:\n                    target_dann = Variable(torch.zeros(1).float().cuda())\n                else:\n                    target_dann = Variable(torch.zeros(1).float())\n                result = my_net(input_data=target_inputv_img, mode='target', rec_scheme='all')\n                (target_private_coda, target_share_coda, _, target_rec_code) = result\n                target_diff = beta_weight * loss_diff(target_private_coda, target_share_coda, weight=0.05)\n                loss += target_diff\n                target_mse = alpha_weight * loss_recon1(target_rec_code, target_inputv_img)\n                loss += target_mse\n                target_simse = alpha_weight * loss_recon2(target_rec_code, target_inputv_img)\n                loss += target_mse\n                loss.backward()\n                optimizer.step()\n                data_source = data_source_iter.next()\n                (s_img, s_label) = data_source\n                my_net.zero_grad()\n                batch_size = len(s_label)\n                input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n                class_label = torch.LongTensor(batch_size)\n                domain_label = torch.zeros(batch_size)\n                damain_label = domain_label.long()\n                loss = 0\n                if cuda:\n                    s_img = s_img.cuda()\n                    s_label = s_label.cuda()\n                    input_img = input_img.cuda()\n                    class_label = class_label.cuda()\n                    domain_label = domain_label.cuda()\n                input_img.resize_as_(input_img).copy_(s_img)\n                class_label.resize_as_(s_label).copy_(s_label)\n                source_inputv_img = Variable(input_img)\n                source_classv_label = Variable(class_label)\n                source_domainv_label = Variable(domain_label)\n                if current_step > active_domain_loss_step:\n                    result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all', p=p)\n                    (source_private_code, source_share_code, source_domain_label, source_classv_label, source_rec_code) = result\n                    source_dann = gamma_weight * loss_similarity(source_domain_label, source_classv_label)\n                    loss += source_dann\n                else:\n                    if cuda:\n                        source_dann = Variable(torch.zeros(1).float().cuda())\n                    else:\n                        if cuda:\n                            source_dann = Variable(torch.zeros(1).float().cuda())\n                        else:\n                            source_dann = Variable(torch.zeros(1).float())\n                        result = my_net(input_data=source_inputv_img, mode='source', rec_scheme='all')\n                        (source_private_code, source_share_code, _, source_class_label, source_rec_code) = result\n                    source_classification = loss_classfication(source_class_label, source_classv_label)\n                    loss += source_classification\n                    source_diff = beta_weight * loss_diff(source_private_code, source_share_code, weight=0.05)\n                    loss += source_diff\n                    source_mse = alpha_weight * loss_recon1(source_rec_code, source_inputv_img)\n                    loss += source_mse\n                    source_simse = gamma_weight * loss_recon2(source_rec_code, source_inputv_img)\n                    loss += source_simse\n                    loss.backward()\n                    optimizer.step()\n                    i += 1\n                    current_step += 1\n                    start1 = time.time()\n                    accu1 = test(epoch=epoch, name='dataset1')\n                    end1 = time.time()\n                    curr1 = end1 - start1\n                    time_total1 += curr1\n                    accu_total1 += accu1\n                    start2 = time.time()\n                    accu2 = test(epoch=epoch, name='dataset2')\n                    end2 = time.time()\n                    curr2 = end2 - start2\n                    time_total2 += curr2\n                    accu_total2 += accu2\n    model_index = epoch\n    model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    while os.path.exists(model_path):\n        model_index = model_index + 1\n        model_path = 'D:\\\\study\\\\graduation_project\\\\grdaution_project\\\\instru_identify\\\\dataset18dataset2' + '\\\\dsn_epoch_' + str(model_index) + '.pth'\n    torch.save(my_net.state_dict(), model_path)\n    average_accu1 = accu_total1 / (len_dataloader * n_epoch)\n    average_accu2 = accu_total2 / (len_dataloader * n_epoch)\n    print(round(float(average_accu1), 3))\n    print(round(float(average_accu2), 3))\n    print(round(float(time_total1), 3))\n    print(round(float(time_total2), 3))\n    return result"
        ]
    }
]