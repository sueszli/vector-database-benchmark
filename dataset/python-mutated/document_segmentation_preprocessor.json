[
    {
        "func_name": "__init__",
        "original": "def __init__(self, model_dir: str, model_max_length: int, mode: str=ModeKeys.INFERENCE, question_column_name='labels', context_column_name='sentences', example_id_column_name='example_id', label_list=['B-EOP', 'O']):\n    \"\"\"The preprocessor for document segmentation task, based on transformers' tokenizer.\n\n        Args:\n            model_dir: The model dir containing the essential files to build the tokenizer.\n            model_max_length: The max length the model supported.\n            mode: The mode for this preprocessor.\n            question_column_name: The key for the question column, default `labels`.\n            context_column_name: The key for the context column, default `sentences`.\n            example_id_column_name: The key for the example id column, default `example_id`.\n            label_list: The label list, default `['B-EOP', 'O']`\n        \"\"\"\n    super().__init__(mode)\n    from transformers import BertTokenizerFast\n    self.tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n    self.question_column_name = question_column_name\n    self.context_column_name = context_column_name\n    self.example_id_column_name = example_id_column_name\n    self.label_list = label_list\n    self.label_to_id = {label: id for (id, label) in enumerate(self.label_list)}\n    self.target_specical_ids = set()\n    self.target_specical_ids.add(self.tokenizer.eos_token_id)\n    self.max_seq_length = model_max_length",
        "mutated": [
            "def __init__(self, model_dir: str, model_max_length: int, mode: str=ModeKeys.INFERENCE, question_column_name='labels', context_column_name='sentences', example_id_column_name='example_id', label_list=['B-EOP', 'O']):\n    if False:\n        i = 10\n    \"The preprocessor for document segmentation task, based on transformers' tokenizer.\\n\\n        Args:\\n            model_dir: The model dir containing the essential files to build the tokenizer.\\n            model_max_length: The max length the model supported.\\n            mode: The mode for this preprocessor.\\n            question_column_name: The key for the question column, default `labels`.\\n            context_column_name: The key for the context column, default `sentences`.\\n            example_id_column_name: The key for the example id column, default `example_id`.\\n            label_list: The label list, default `['B-EOP', 'O']`\\n        \"\n    super().__init__(mode)\n    from transformers import BertTokenizerFast\n    self.tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n    self.question_column_name = question_column_name\n    self.context_column_name = context_column_name\n    self.example_id_column_name = example_id_column_name\n    self.label_list = label_list\n    self.label_to_id = {label: id for (id, label) in enumerate(self.label_list)}\n    self.target_specical_ids = set()\n    self.target_specical_ids.add(self.tokenizer.eos_token_id)\n    self.max_seq_length = model_max_length",
            "def __init__(self, model_dir: str, model_max_length: int, mode: str=ModeKeys.INFERENCE, question_column_name='labels', context_column_name='sentences', example_id_column_name='example_id', label_list=['B-EOP', 'O']):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"The preprocessor for document segmentation task, based on transformers' tokenizer.\\n\\n        Args:\\n            model_dir: The model dir containing the essential files to build the tokenizer.\\n            model_max_length: The max length the model supported.\\n            mode: The mode for this preprocessor.\\n            question_column_name: The key for the question column, default `labels`.\\n            context_column_name: The key for the context column, default `sentences`.\\n            example_id_column_name: The key for the example id column, default `example_id`.\\n            label_list: The label list, default `['B-EOP', 'O']`\\n        \"\n    super().__init__(mode)\n    from transformers import BertTokenizerFast\n    self.tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n    self.question_column_name = question_column_name\n    self.context_column_name = context_column_name\n    self.example_id_column_name = example_id_column_name\n    self.label_list = label_list\n    self.label_to_id = {label: id for (id, label) in enumerate(self.label_list)}\n    self.target_specical_ids = set()\n    self.target_specical_ids.add(self.tokenizer.eos_token_id)\n    self.max_seq_length = model_max_length",
            "def __init__(self, model_dir: str, model_max_length: int, mode: str=ModeKeys.INFERENCE, question_column_name='labels', context_column_name='sentences', example_id_column_name='example_id', label_list=['B-EOP', 'O']):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"The preprocessor for document segmentation task, based on transformers' tokenizer.\\n\\n        Args:\\n            model_dir: The model dir containing the essential files to build the tokenizer.\\n            model_max_length: The max length the model supported.\\n            mode: The mode for this preprocessor.\\n            question_column_name: The key for the question column, default `labels`.\\n            context_column_name: The key for the context column, default `sentences`.\\n            example_id_column_name: The key for the example id column, default `example_id`.\\n            label_list: The label list, default `['B-EOP', 'O']`\\n        \"\n    super().__init__(mode)\n    from transformers import BertTokenizerFast\n    self.tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n    self.question_column_name = question_column_name\n    self.context_column_name = context_column_name\n    self.example_id_column_name = example_id_column_name\n    self.label_list = label_list\n    self.label_to_id = {label: id for (id, label) in enumerate(self.label_list)}\n    self.target_specical_ids = set()\n    self.target_specical_ids.add(self.tokenizer.eos_token_id)\n    self.max_seq_length = model_max_length",
            "def __init__(self, model_dir: str, model_max_length: int, mode: str=ModeKeys.INFERENCE, question_column_name='labels', context_column_name='sentences', example_id_column_name='example_id', label_list=['B-EOP', 'O']):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"The preprocessor for document segmentation task, based on transformers' tokenizer.\\n\\n        Args:\\n            model_dir: The model dir containing the essential files to build the tokenizer.\\n            model_max_length: The max length the model supported.\\n            mode: The mode for this preprocessor.\\n            question_column_name: The key for the question column, default `labels`.\\n            context_column_name: The key for the context column, default `sentences`.\\n            example_id_column_name: The key for the example id column, default `example_id`.\\n            label_list: The label list, default `['B-EOP', 'O']`\\n        \"\n    super().__init__(mode)\n    from transformers import BertTokenizerFast\n    self.tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n    self.question_column_name = question_column_name\n    self.context_column_name = context_column_name\n    self.example_id_column_name = example_id_column_name\n    self.label_list = label_list\n    self.label_to_id = {label: id for (id, label) in enumerate(self.label_list)}\n    self.target_specical_ids = set()\n    self.target_specical_ids.add(self.tokenizer.eos_token_id)\n    self.max_seq_length = model_max_length",
            "def __init__(self, model_dir: str, model_max_length: int, mode: str=ModeKeys.INFERENCE, question_column_name='labels', context_column_name='sentences', example_id_column_name='example_id', label_list=['B-EOP', 'O']):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"The preprocessor for document segmentation task, based on transformers' tokenizer.\\n\\n        Args:\\n            model_dir: The model dir containing the essential files to build the tokenizer.\\n            model_max_length: The max length the model supported.\\n            mode: The mode for this preprocessor.\\n            question_column_name: The key for the question column, default `labels`.\\n            context_column_name: The key for the context column, default `sentences`.\\n            example_id_column_name: The key for the example id column, default `example_id`.\\n            label_list: The label list, default `['B-EOP', 'O']`\\n        \"\n    super().__init__(mode)\n    from transformers import BertTokenizerFast\n    self.tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n    self.question_column_name = question_column_name\n    self.context_column_name = context_column_name\n    self.example_id_column_name = example_id_column_name\n    self.label_list = label_list\n    self.label_to_id = {label: id for (id, label) in enumerate(self.label_list)}\n    self.target_specical_ids = set()\n    self.target_specical_ids.add(self.tokenizer.eos_token_id)\n    self.max_seq_length = model_max_length"
        ]
    },
    {
        "func_name": "__call__",
        "original": "def __call__(self, examples, model_cfg=None) -> Dict[str, Any]:\n    questions = examples[self.question_column_name]\n    contexts = examples[self.context_column_name]\n    example_ids = examples[self.example_id_column_name]\n    num_examples = len(questions)\n    sentences = []\n    for sentence_list in contexts:\n        sentence_list = [_ + '[EOS]' for _ in sentence_list]\n        sentences.append(sentence_list)\n    try:\n        tokenized_examples = self.tokenizer(sentences, is_split_into_words=True, add_special_tokens=False, return_token_type_ids=True, return_attention_mask=True)\n    except Exception as e:\n        logger.error(e)\n        return {}\n    segment_ids = []\n    token_seq_labels = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_labels = questions[example_index]\n        example_labels = [self.label_to_id[_] if _ in self.label_to_id else -100 for _ in example_labels]\n        example_token_labels = []\n        segment_id = []\n        cur_seg_id = 1\n        para_segment_id = []\n        cut_para_seg_id = 1\n        for token_index in range(len(example_input_ids)):\n            if example_input_ids[token_index] in self.target_specical_ids:\n                example_token_labels.append(example_labels[cur_seg_id - 1])\n                segment_id.append(cur_seg_id)\n                cur_seg_id += 1\n            else:\n                example_token_labels.append(-100)\n                segment_id.append(cur_seg_id)\n            if example_token_labels[token_index] != -100:\n                para_segment_id.append(cut_para_seg_id)\n                cut_para_seg_id += 1\n            else:\n                para_segment_id.append(cut_para_seg_id)\n        if model_cfg is not None and model_cfg['type'] == 'ponet' and (model_cfg['level'] == 'topic'):\n            segment_ids.append(para_segment_id)\n        else:\n            segment_ids.append(segment_id)\n        token_seq_labels.append(example_token_labels)\n    tokenized_examples['segment_ids'] = segment_ids\n    tokenized_examples['token_seq_labels'] = token_seq_labels\n    new_segment_ids = []\n    new_token_seq_labels = []\n    new_input_ids = []\n    new_token_type_ids = []\n    new_attention_mask = []\n    new_example_ids = []\n    new_sentences = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_token_type_ids = tokenized_examples['token_type_ids'][example_index]\n        example_attention_mask = tokenized_examples['attention_mask'][example_index]\n        example_segment_ids = tokenized_examples['segment_ids'][example_index]\n        example_token_seq_labels = tokenized_examples['token_seq_labels'][example_index]\n        example_sentences = contexts[example_index]\n        example_id = example_ids[example_index]\n        example_total_num_sentences = len(questions[example_index])\n        example_total_num_tokens = len(tokenized_examples['input_ids'][example_index])\n        accumulate_length = [i for (i, x) in enumerate(tokenized_examples['input_ids'][example_index]) if x == self.tokenizer.eos_token_id]\n        samples_boundary = []\n        left_index = 0\n        sent_left_index = 0\n        sent_i = 0\n        while sent_i < len(accumulate_length):\n            length = accumulate_length[sent_i]\n            right_index = length + 1\n            sent_right_index = sent_i + 1\n            if right_index - left_index >= self.max_seq_length - 1 or right_index == example_total_num_tokens:\n                samples_boundary.append([left_index, right_index])\n                sample_input_ids = [self.tokenizer.cls_token_id] + example_input_ids[left_index:right_index]\n                sample_input_ids = sample_input_ids[:self.max_seq_length]\n                sample_token_type_ids = [0] + example_token_type_ids[left_index:right_index]\n                sample_token_type_ids = sample_token_type_ids[:self.max_seq_length]\n                sample_attention_mask = [1] + example_attention_mask[left_index:right_index]\n                sample_attention_mask = sample_attention_mask[:self.max_seq_length]\n                sample_segment_ids = [0] + example_segment_ids[left_index:right_index]\n                sample_segment_ids = sample_segment_ids[:self.max_seq_length]\n                sample_token_seq_labels = [-100] + example_token_seq_labels[left_index:right_index]\n                sample_token_seq_labels = sample_token_seq_labels[:self.max_seq_length]\n                if sent_right_index - 1 == sent_left_index:\n                    left_index = right_index\n                    sample_input_ids[-1] = self.tokenizer.eos_token_id\n                    sample_token_seq_labels[-1] = -100\n                else:\n                    left_index = accumulate_length[sent_i - 1] + 1\n                    if sample_token_seq_labels[-1] != -100:\n                        sample_token_seq_labels[-1] = -100\n                if sent_right_index - 1 == sent_left_index or right_index == example_total_num_tokens:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index]\n                    sent_left_index = sent_right_index\n                    sent_i += 1\n                else:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index - 1]\n                    sent_left_index = sent_right_index - 1\n                if len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences) - 1 and len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences):\n                    tmp = []\n                    for (w_i, w, l) in zip(sample_input_ids, self.tokenizer.decode(sample_input_ids).split(' '), sample_token_seq_labels):\n                        tmp.append((w_i, w, l))\n                while len(sample_input_ids) < self.max_seq_length:\n                    sample_input_ids.append(self.tokenizer.pad_token_id)\n                    sample_token_type_ids.append(0)\n                    sample_attention_mask.append(0)\n                    sample_segment_ids.append(example_total_num_sentences + 1)\n                    sample_token_seq_labels.append(-100)\n                new_input_ids.append(sample_input_ids)\n                new_token_type_ids.append(sample_token_type_ids)\n                new_attention_mask.append(sample_attention_mask)\n                new_segment_ids.append(sample_segment_ids)\n                new_token_seq_labels.append(sample_token_seq_labels)\n                new_example_ids.append(example_id)\n                new_sentences.append(sample_sentences)\n            else:\n                sent_i += 1\n                continue\n    output_samples = {}\n    output_samples['input_ids'] = new_input_ids\n    output_samples['token_type_ids'] = new_token_type_ids\n    output_samples['attention_mask'] = new_attention_mask\n    output_samples['segment_ids'] = new_segment_ids\n    output_samples['example_id'] = new_example_ids\n    output_samples['labels'] = new_token_seq_labels\n    output_samples['sentences'] = new_sentences\n    return output_samples",
        "mutated": [
            "def __call__(self, examples, model_cfg=None) -> Dict[str, Any]:\n    if False:\n        i = 10\n    questions = examples[self.question_column_name]\n    contexts = examples[self.context_column_name]\n    example_ids = examples[self.example_id_column_name]\n    num_examples = len(questions)\n    sentences = []\n    for sentence_list in contexts:\n        sentence_list = [_ + '[EOS]' for _ in sentence_list]\n        sentences.append(sentence_list)\n    try:\n        tokenized_examples = self.tokenizer(sentences, is_split_into_words=True, add_special_tokens=False, return_token_type_ids=True, return_attention_mask=True)\n    except Exception as e:\n        logger.error(e)\n        return {}\n    segment_ids = []\n    token_seq_labels = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_labels = questions[example_index]\n        example_labels = [self.label_to_id[_] if _ in self.label_to_id else -100 for _ in example_labels]\n        example_token_labels = []\n        segment_id = []\n        cur_seg_id = 1\n        para_segment_id = []\n        cut_para_seg_id = 1\n        for token_index in range(len(example_input_ids)):\n            if example_input_ids[token_index] in self.target_specical_ids:\n                example_token_labels.append(example_labels[cur_seg_id - 1])\n                segment_id.append(cur_seg_id)\n                cur_seg_id += 1\n            else:\n                example_token_labels.append(-100)\n                segment_id.append(cur_seg_id)\n            if example_token_labels[token_index] != -100:\n                para_segment_id.append(cut_para_seg_id)\n                cut_para_seg_id += 1\n            else:\n                para_segment_id.append(cut_para_seg_id)\n        if model_cfg is not None and model_cfg['type'] == 'ponet' and (model_cfg['level'] == 'topic'):\n            segment_ids.append(para_segment_id)\n        else:\n            segment_ids.append(segment_id)\n        token_seq_labels.append(example_token_labels)\n    tokenized_examples['segment_ids'] = segment_ids\n    tokenized_examples['token_seq_labels'] = token_seq_labels\n    new_segment_ids = []\n    new_token_seq_labels = []\n    new_input_ids = []\n    new_token_type_ids = []\n    new_attention_mask = []\n    new_example_ids = []\n    new_sentences = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_token_type_ids = tokenized_examples['token_type_ids'][example_index]\n        example_attention_mask = tokenized_examples['attention_mask'][example_index]\n        example_segment_ids = tokenized_examples['segment_ids'][example_index]\n        example_token_seq_labels = tokenized_examples['token_seq_labels'][example_index]\n        example_sentences = contexts[example_index]\n        example_id = example_ids[example_index]\n        example_total_num_sentences = len(questions[example_index])\n        example_total_num_tokens = len(tokenized_examples['input_ids'][example_index])\n        accumulate_length = [i for (i, x) in enumerate(tokenized_examples['input_ids'][example_index]) if x == self.tokenizer.eos_token_id]\n        samples_boundary = []\n        left_index = 0\n        sent_left_index = 0\n        sent_i = 0\n        while sent_i < len(accumulate_length):\n            length = accumulate_length[sent_i]\n            right_index = length + 1\n            sent_right_index = sent_i + 1\n            if right_index - left_index >= self.max_seq_length - 1 or right_index == example_total_num_tokens:\n                samples_boundary.append([left_index, right_index])\n                sample_input_ids = [self.tokenizer.cls_token_id] + example_input_ids[left_index:right_index]\n                sample_input_ids = sample_input_ids[:self.max_seq_length]\n                sample_token_type_ids = [0] + example_token_type_ids[left_index:right_index]\n                sample_token_type_ids = sample_token_type_ids[:self.max_seq_length]\n                sample_attention_mask = [1] + example_attention_mask[left_index:right_index]\n                sample_attention_mask = sample_attention_mask[:self.max_seq_length]\n                sample_segment_ids = [0] + example_segment_ids[left_index:right_index]\n                sample_segment_ids = sample_segment_ids[:self.max_seq_length]\n                sample_token_seq_labels = [-100] + example_token_seq_labels[left_index:right_index]\n                sample_token_seq_labels = sample_token_seq_labels[:self.max_seq_length]\n                if sent_right_index - 1 == sent_left_index:\n                    left_index = right_index\n                    sample_input_ids[-1] = self.tokenizer.eos_token_id\n                    sample_token_seq_labels[-1] = -100\n                else:\n                    left_index = accumulate_length[sent_i - 1] + 1\n                    if sample_token_seq_labels[-1] != -100:\n                        sample_token_seq_labels[-1] = -100\n                if sent_right_index - 1 == sent_left_index or right_index == example_total_num_tokens:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index]\n                    sent_left_index = sent_right_index\n                    sent_i += 1\n                else:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index - 1]\n                    sent_left_index = sent_right_index - 1\n                if len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences) - 1 and len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences):\n                    tmp = []\n                    for (w_i, w, l) in zip(sample_input_ids, self.tokenizer.decode(sample_input_ids).split(' '), sample_token_seq_labels):\n                        tmp.append((w_i, w, l))\n                while len(sample_input_ids) < self.max_seq_length:\n                    sample_input_ids.append(self.tokenizer.pad_token_id)\n                    sample_token_type_ids.append(0)\n                    sample_attention_mask.append(0)\n                    sample_segment_ids.append(example_total_num_sentences + 1)\n                    sample_token_seq_labels.append(-100)\n                new_input_ids.append(sample_input_ids)\n                new_token_type_ids.append(sample_token_type_ids)\n                new_attention_mask.append(sample_attention_mask)\n                new_segment_ids.append(sample_segment_ids)\n                new_token_seq_labels.append(sample_token_seq_labels)\n                new_example_ids.append(example_id)\n                new_sentences.append(sample_sentences)\n            else:\n                sent_i += 1\n                continue\n    output_samples = {}\n    output_samples['input_ids'] = new_input_ids\n    output_samples['token_type_ids'] = new_token_type_ids\n    output_samples['attention_mask'] = new_attention_mask\n    output_samples['segment_ids'] = new_segment_ids\n    output_samples['example_id'] = new_example_ids\n    output_samples['labels'] = new_token_seq_labels\n    output_samples['sentences'] = new_sentences\n    return output_samples",
            "def __call__(self, examples, model_cfg=None) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    questions = examples[self.question_column_name]\n    contexts = examples[self.context_column_name]\n    example_ids = examples[self.example_id_column_name]\n    num_examples = len(questions)\n    sentences = []\n    for sentence_list in contexts:\n        sentence_list = [_ + '[EOS]' for _ in sentence_list]\n        sentences.append(sentence_list)\n    try:\n        tokenized_examples = self.tokenizer(sentences, is_split_into_words=True, add_special_tokens=False, return_token_type_ids=True, return_attention_mask=True)\n    except Exception as e:\n        logger.error(e)\n        return {}\n    segment_ids = []\n    token_seq_labels = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_labels = questions[example_index]\n        example_labels = [self.label_to_id[_] if _ in self.label_to_id else -100 for _ in example_labels]\n        example_token_labels = []\n        segment_id = []\n        cur_seg_id = 1\n        para_segment_id = []\n        cut_para_seg_id = 1\n        for token_index in range(len(example_input_ids)):\n            if example_input_ids[token_index] in self.target_specical_ids:\n                example_token_labels.append(example_labels[cur_seg_id - 1])\n                segment_id.append(cur_seg_id)\n                cur_seg_id += 1\n            else:\n                example_token_labels.append(-100)\n                segment_id.append(cur_seg_id)\n            if example_token_labels[token_index] != -100:\n                para_segment_id.append(cut_para_seg_id)\n                cut_para_seg_id += 1\n            else:\n                para_segment_id.append(cut_para_seg_id)\n        if model_cfg is not None and model_cfg['type'] == 'ponet' and (model_cfg['level'] == 'topic'):\n            segment_ids.append(para_segment_id)\n        else:\n            segment_ids.append(segment_id)\n        token_seq_labels.append(example_token_labels)\n    tokenized_examples['segment_ids'] = segment_ids\n    tokenized_examples['token_seq_labels'] = token_seq_labels\n    new_segment_ids = []\n    new_token_seq_labels = []\n    new_input_ids = []\n    new_token_type_ids = []\n    new_attention_mask = []\n    new_example_ids = []\n    new_sentences = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_token_type_ids = tokenized_examples['token_type_ids'][example_index]\n        example_attention_mask = tokenized_examples['attention_mask'][example_index]\n        example_segment_ids = tokenized_examples['segment_ids'][example_index]\n        example_token_seq_labels = tokenized_examples['token_seq_labels'][example_index]\n        example_sentences = contexts[example_index]\n        example_id = example_ids[example_index]\n        example_total_num_sentences = len(questions[example_index])\n        example_total_num_tokens = len(tokenized_examples['input_ids'][example_index])\n        accumulate_length = [i for (i, x) in enumerate(tokenized_examples['input_ids'][example_index]) if x == self.tokenizer.eos_token_id]\n        samples_boundary = []\n        left_index = 0\n        sent_left_index = 0\n        sent_i = 0\n        while sent_i < len(accumulate_length):\n            length = accumulate_length[sent_i]\n            right_index = length + 1\n            sent_right_index = sent_i + 1\n            if right_index - left_index >= self.max_seq_length - 1 or right_index == example_total_num_tokens:\n                samples_boundary.append([left_index, right_index])\n                sample_input_ids = [self.tokenizer.cls_token_id] + example_input_ids[left_index:right_index]\n                sample_input_ids = sample_input_ids[:self.max_seq_length]\n                sample_token_type_ids = [0] + example_token_type_ids[left_index:right_index]\n                sample_token_type_ids = sample_token_type_ids[:self.max_seq_length]\n                sample_attention_mask = [1] + example_attention_mask[left_index:right_index]\n                sample_attention_mask = sample_attention_mask[:self.max_seq_length]\n                sample_segment_ids = [0] + example_segment_ids[left_index:right_index]\n                sample_segment_ids = sample_segment_ids[:self.max_seq_length]\n                sample_token_seq_labels = [-100] + example_token_seq_labels[left_index:right_index]\n                sample_token_seq_labels = sample_token_seq_labels[:self.max_seq_length]\n                if sent_right_index - 1 == sent_left_index:\n                    left_index = right_index\n                    sample_input_ids[-1] = self.tokenizer.eos_token_id\n                    sample_token_seq_labels[-1] = -100\n                else:\n                    left_index = accumulate_length[sent_i - 1] + 1\n                    if sample_token_seq_labels[-1] != -100:\n                        sample_token_seq_labels[-1] = -100\n                if sent_right_index - 1 == sent_left_index or right_index == example_total_num_tokens:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index]\n                    sent_left_index = sent_right_index\n                    sent_i += 1\n                else:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index - 1]\n                    sent_left_index = sent_right_index - 1\n                if len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences) - 1 and len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences):\n                    tmp = []\n                    for (w_i, w, l) in zip(sample_input_ids, self.tokenizer.decode(sample_input_ids).split(' '), sample_token_seq_labels):\n                        tmp.append((w_i, w, l))\n                while len(sample_input_ids) < self.max_seq_length:\n                    sample_input_ids.append(self.tokenizer.pad_token_id)\n                    sample_token_type_ids.append(0)\n                    sample_attention_mask.append(0)\n                    sample_segment_ids.append(example_total_num_sentences + 1)\n                    sample_token_seq_labels.append(-100)\n                new_input_ids.append(sample_input_ids)\n                new_token_type_ids.append(sample_token_type_ids)\n                new_attention_mask.append(sample_attention_mask)\n                new_segment_ids.append(sample_segment_ids)\n                new_token_seq_labels.append(sample_token_seq_labels)\n                new_example_ids.append(example_id)\n                new_sentences.append(sample_sentences)\n            else:\n                sent_i += 1\n                continue\n    output_samples = {}\n    output_samples['input_ids'] = new_input_ids\n    output_samples['token_type_ids'] = new_token_type_ids\n    output_samples['attention_mask'] = new_attention_mask\n    output_samples['segment_ids'] = new_segment_ids\n    output_samples['example_id'] = new_example_ids\n    output_samples['labels'] = new_token_seq_labels\n    output_samples['sentences'] = new_sentences\n    return output_samples",
            "def __call__(self, examples, model_cfg=None) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    questions = examples[self.question_column_name]\n    contexts = examples[self.context_column_name]\n    example_ids = examples[self.example_id_column_name]\n    num_examples = len(questions)\n    sentences = []\n    for sentence_list in contexts:\n        sentence_list = [_ + '[EOS]' for _ in sentence_list]\n        sentences.append(sentence_list)\n    try:\n        tokenized_examples = self.tokenizer(sentences, is_split_into_words=True, add_special_tokens=False, return_token_type_ids=True, return_attention_mask=True)\n    except Exception as e:\n        logger.error(e)\n        return {}\n    segment_ids = []\n    token_seq_labels = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_labels = questions[example_index]\n        example_labels = [self.label_to_id[_] if _ in self.label_to_id else -100 for _ in example_labels]\n        example_token_labels = []\n        segment_id = []\n        cur_seg_id = 1\n        para_segment_id = []\n        cut_para_seg_id = 1\n        for token_index in range(len(example_input_ids)):\n            if example_input_ids[token_index] in self.target_specical_ids:\n                example_token_labels.append(example_labels[cur_seg_id - 1])\n                segment_id.append(cur_seg_id)\n                cur_seg_id += 1\n            else:\n                example_token_labels.append(-100)\n                segment_id.append(cur_seg_id)\n            if example_token_labels[token_index] != -100:\n                para_segment_id.append(cut_para_seg_id)\n                cut_para_seg_id += 1\n            else:\n                para_segment_id.append(cut_para_seg_id)\n        if model_cfg is not None and model_cfg['type'] == 'ponet' and (model_cfg['level'] == 'topic'):\n            segment_ids.append(para_segment_id)\n        else:\n            segment_ids.append(segment_id)\n        token_seq_labels.append(example_token_labels)\n    tokenized_examples['segment_ids'] = segment_ids\n    tokenized_examples['token_seq_labels'] = token_seq_labels\n    new_segment_ids = []\n    new_token_seq_labels = []\n    new_input_ids = []\n    new_token_type_ids = []\n    new_attention_mask = []\n    new_example_ids = []\n    new_sentences = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_token_type_ids = tokenized_examples['token_type_ids'][example_index]\n        example_attention_mask = tokenized_examples['attention_mask'][example_index]\n        example_segment_ids = tokenized_examples['segment_ids'][example_index]\n        example_token_seq_labels = tokenized_examples['token_seq_labels'][example_index]\n        example_sentences = contexts[example_index]\n        example_id = example_ids[example_index]\n        example_total_num_sentences = len(questions[example_index])\n        example_total_num_tokens = len(tokenized_examples['input_ids'][example_index])\n        accumulate_length = [i for (i, x) in enumerate(tokenized_examples['input_ids'][example_index]) if x == self.tokenizer.eos_token_id]\n        samples_boundary = []\n        left_index = 0\n        sent_left_index = 0\n        sent_i = 0\n        while sent_i < len(accumulate_length):\n            length = accumulate_length[sent_i]\n            right_index = length + 1\n            sent_right_index = sent_i + 1\n            if right_index - left_index >= self.max_seq_length - 1 or right_index == example_total_num_tokens:\n                samples_boundary.append([left_index, right_index])\n                sample_input_ids = [self.tokenizer.cls_token_id] + example_input_ids[left_index:right_index]\n                sample_input_ids = sample_input_ids[:self.max_seq_length]\n                sample_token_type_ids = [0] + example_token_type_ids[left_index:right_index]\n                sample_token_type_ids = sample_token_type_ids[:self.max_seq_length]\n                sample_attention_mask = [1] + example_attention_mask[left_index:right_index]\n                sample_attention_mask = sample_attention_mask[:self.max_seq_length]\n                sample_segment_ids = [0] + example_segment_ids[left_index:right_index]\n                sample_segment_ids = sample_segment_ids[:self.max_seq_length]\n                sample_token_seq_labels = [-100] + example_token_seq_labels[left_index:right_index]\n                sample_token_seq_labels = sample_token_seq_labels[:self.max_seq_length]\n                if sent_right_index - 1 == sent_left_index:\n                    left_index = right_index\n                    sample_input_ids[-1] = self.tokenizer.eos_token_id\n                    sample_token_seq_labels[-1] = -100\n                else:\n                    left_index = accumulate_length[sent_i - 1] + 1\n                    if sample_token_seq_labels[-1] != -100:\n                        sample_token_seq_labels[-1] = -100\n                if sent_right_index - 1 == sent_left_index or right_index == example_total_num_tokens:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index]\n                    sent_left_index = sent_right_index\n                    sent_i += 1\n                else:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index - 1]\n                    sent_left_index = sent_right_index - 1\n                if len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences) - 1 and len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences):\n                    tmp = []\n                    for (w_i, w, l) in zip(sample_input_ids, self.tokenizer.decode(sample_input_ids).split(' '), sample_token_seq_labels):\n                        tmp.append((w_i, w, l))\n                while len(sample_input_ids) < self.max_seq_length:\n                    sample_input_ids.append(self.tokenizer.pad_token_id)\n                    sample_token_type_ids.append(0)\n                    sample_attention_mask.append(0)\n                    sample_segment_ids.append(example_total_num_sentences + 1)\n                    sample_token_seq_labels.append(-100)\n                new_input_ids.append(sample_input_ids)\n                new_token_type_ids.append(sample_token_type_ids)\n                new_attention_mask.append(sample_attention_mask)\n                new_segment_ids.append(sample_segment_ids)\n                new_token_seq_labels.append(sample_token_seq_labels)\n                new_example_ids.append(example_id)\n                new_sentences.append(sample_sentences)\n            else:\n                sent_i += 1\n                continue\n    output_samples = {}\n    output_samples['input_ids'] = new_input_ids\n    output_samples['token_type_ids'] = new_token_type_ids\n    output_samples['attention_mask'] = new_attention_mask\n    output_samples['segment_ids'] = new_segment_ids\n    output_samples['example_id'] = new_example_ids\n    output_samples['labels'] = new_token_seq_labels\n    output_samples['sentences'] = new_sentences\n    return output_samples",
            "def __call__(self, examples, model_cfg=None) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    questions = examples[self.question_column_name]\n    contexts = examples[self.context_column_name]\n    example_ids = examples[self.example_id_column_name]\n    num_examples = len(questions)\n    sentences = []\n    for sentence_list in contexts:\n        sentence_list = [_ + '[EOS]' for _ in sentence_list]\n        sentences.append(sentence_list)\n    try:\n        tokenized_examples = self.tokenizer(sentences, is_split_into_words=True, add_special_tokens=False, return_token_type_ids=True, return_attention_mask=True)\n    except Exception as e:\n        logger.error(e)\n        return {}\n    segment_ids = []\n    token_seq_labels = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_labels = questions[example_index]\n        example_labels = [self.label_to_id[_] if _ in self.label_to_id else -100 for _ in example_labels]\n        example_token_labels = []\n        segment_id = []\n        cur_seg_id = 1\n        para_segment_id = []\n        cut_para_seg_id = 1\n        for token_index in range(len(example_input_ids)):\n            if example_input_ids[token_index] in self.target_specical_ids:\n                example_token_labels.append(example_labels[cur_seg_id - 1])\n                segment_id.append(cur_seg_id)\n                cur_seg_id += 1\n            else:\n                example_token_labels.append(-100)\n                segment_id.append(cur_seg_id)\n            if example_token_labels[token_index] != -100:\n                para_segment_id.append(cut_para_seg_id)\n                cut_para_seg_id += 1\n            else:\n                para_segment_id.append(cut_para_seg_id)\n        if model_cfg is not None and model_cfg['type'] == 'ponet' and (model_cfg['level'] == 'topic'):\n            segment_ids.append(para_segment_id)\n        else:\n            segment_ids.append(segment_id)\n        token_seq_labels.append(example_token_labels)\n    tokenized_examples['segment_ids'] = segment_ids\n    tokenized_examples['token_seq_labels'] = token_seq_labels\n    new_segment_ids = []\n    new_token_seq_labels = []\n    new_input_ids = []\n    new_token_type_ids = []\n    new_attention_mask = []\n    new_example_ids = []\n    new_sentences = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_token_type_ids = tokenized_examples['token_type_ids'][example_index]\n        example_attention_mask = tokenized_examples['attention_mask'][example_index]\n        example_segment_ids = tokenized_examples['segment_ids'][example_index]\n        example_token_seq_labels = tokenized_examples['token_seq_labels'][example_index]\n        example_sentences = contexts[example_index]\n        example_id = example_ids[example_index]\n        example_total_num_sentences = len(questions[example_index])\n        example_total_num_tokens = len(tokenized_examples['input_ids'][example_index])\n        accumulate_length = [i for (i, x) in enumerate(tokenized_examples['input_ids'][example_index]) if x == self.tokenizer.eos_token_id]\n        samples_boundary = []\n        left_index = 0\n        sent_left_index = 0\n        sent_i = 0\n        while sent_i < len(accumulate_length):\n            length = accumulate_length[sent_i]\n            right_index = length + 1\n            sent_right_index = sent_i + 1\n            if right_index - left_index >= self.max_seq_length - 1 or right_index == example_total_num_tokens:\n                samples_boundary.append([left_index, right_index])\n                sample_input_ids = [self.tokenizer.cls_token_id] + example_input_ids[left_index:right_index]\n                sample_input_ids = sample_input_ids[:self.max_seq_length]\n                sample_token_type_ids = [0] + example_token_type_ids[left_index:right_index]\n                sample_token_type_ids = sample_token_type_ids[:self.max_seq_length]\n                sample_attention_mask = [1] + example_attention_mask[left_index:right_index]\n                sample_attention_mask = sample_attention_mask[:self.max_seq_length]\n                sample_segment_ids = [0] + example_segment_ids[left_index:right_index]\n                sample_segment_ids = sample_segment_ids[:self.max_seq_length]\n                sample_token_seq_labels = [-100] + example_token_seq_labels[left_index:right_index]\n                sample_token_seq_labels = sample_token_seq_labels[:self.max_seq_length]\n                if sent_right_index - 1 == sent_left_index:\n                    left_index = right_index\n                    sample_input_ids[-1] = self.tokenizer.eos_token_id\n                    sample_token_seq_labels[-1] = -100\n                else:\n                    left_index = accumulate_length[sent_i - 1] + 1\n                    if sample_token_seq_labels[-1] != -100:\n                        sample_token_seq_labels[-1] = -100\n                if sent_right_index - 1 == sent_left_index or right_index == example_total_num_tokens:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index]\n                    sent_left_index = sent_right_index\n                    sent_i += 1\n                else:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index - 1]\n                    sent_left_index = sent_right_index - 1\n                if len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences) - 1 and len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences):\n                    tmp = []\n                    for (w_i, w, l) in zip(sample_input_ids, self.tokenizer.decode(sample_input_ids).split(' '), sample_token_seq_labels):\n                        tmp.append((w_i, w, l))\n                while len(sample_input_ids) < self.max_seq_length:\n                    sample_input_ids.append(self.tokenizer.pad_token_id)\n                    sample_token_type_ids.append(0)\n                    sample_attention_mask.append(0)\n                    sample_segment_ids.append(example_total_num_sentences + 1)\n                    sample_token_seq_labels.append(-100)\n                new_input_ids.append(sample_input_ids)\n                new_token_type_ids.append(sample_token_type_ids)\n                new_attention_mask.append(sample_attention_mask)\n                new_segment_ids.append(sample_segment_ids)\n                new_token_seq_labels.append(sample_token_seq_labels)\n                new_example_ids.append(example_id)\n                new_sentences.append(sample_sentences)\n            else:\n                sent_i += 1\n                continue\n    output_samples = {}\n    output_samples['input_ids'] = new_input_ids\n    output_samples['token_type_ids'] = new_token_type_ids\n    output_samples['attention_mask'] = new_attention_mask\n    output_samples['segment_ids'] = new_segment_ids\n    output_samples['example_id'] = new_example_ids\n    output_samples['labels'] = new_token_seq_labels\n    output_samples['sentences'] = new_sentences\n    return output_samples",
            "def __call__(self, examples, model_cfg=None) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    questions = examples[self.question_column_name]\n    contexts = examples[self.context_column_name]\n    example_ids = examples[self.example_id_column_name]\n    num_examples = len(questions)\n    sentences = []\n    for sentence_list in contexts:\n        sentence_list = [_ + '[EOS]' for _ in sentence_list]\n        sentences.append(sentence_list)\n    try:\n        tokenized_examples = self.tokenizer(sentences, is_split_into_words=True, add_special_tokens=False, return_token_type_ids=True, return_attention_mask=True)\n    except Exception as e:\n        logger.error(e)\n        return {}\n    segment_ids = []\n    token_seq_labels = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_labels = questions[example_index]\n        example_labels = [self.label_to_id[_] if _ in self.label_to_id else -100 for _ in example_labels]\n        example_token_labels = []\n        segment_id = []\n        cur_seg_id = 1\n        para_segment_id = []\n        cut_para_seg_id = 1\n        for token_index in range(len(example_input_ids)):\n            if example_input_ids[token_index] in self.target_specical_ids:\n                example_token_labels.append(example_labels[cur_seg_id - 1])\n                segment_id.append(cur_seg_id)\n                cur_seg_id += 1\n            else:\n                example_token_labels.append(-100)\n                segment_id.append(cur_seg_id)\n            if example_token_labels[token_index] != -100:\n                para_segment_id.append(cut_para_seg_id)\n                cut_para_seg_id += 1\n            else:\n                para_segment_id.append(cut_para_seg_id)\n        if model_cfg is not None and model_cfg['type'] == 'ponet' and (model_cfg['level'] == 'topic'):\n            segment_ids.append(para_segment_id)\n        else:\n            segment_ids.append(segment_id)\n        token_seq_labels.append(example_token_labels)\n    tokenized_examples['segment_ids'] = segment_ids\n    tokenized_examples['token_seq_labels'] = token_seq_labels\n    new_segment_ids = []\n    new_token_seq_labels = []\n    new_input_ids = []\n    new_token_type_ids = []\n    new_attention_mask = []\n    new_example_ids = []\n    new_sentences = []\n    for example_index in range(num_examples):\n        example_input_ids = tokenized_examples['input_ids'][example_index]\n        example_token_type_ids = tokenized_examples['token_type_ids'][example_index]\n        example_attention_mask = tokenized_examples['attention_mask'][example_index]\n        example_segment_ids = tokenized_examples['segment_ids'][example_index]\n        example_token_seq_labels = tokenized_examples['token_seq_labels'][example_index]\n        example_sentences = contexts[example_index]\n        example_id = example_ids[example_index]\n        example_total_num_sentences = len(questions[example_index])\n        example_total_num_tokens = len(tokenized_examples['input_ids'][example_index])\n        accumulate_length = [i for (i, x) in enumerate(tokenized_examples['input_ids'][example_index]) if x == self.tokenizer.eos_token_id]\n        samples_boundary = []\n        left_index = 0\n        sent_left_index = 0\n        sent_i = 0\n        while sent_i < len(accumulate_length):\n            length = accumulate_length[sent_i]\n            right_index = length + 1\n            sent_right_index = sent_i + 1\n            if right_index - left_index >= self.max_seq_length - 1 or right_index == example_total_num_tokens:\n                samples_boundary.append([left_index, right_index])\n                sample_input_ids = [self.tokenizer.cls_token_id] + example_input_ids[left_index:right_index]\n                sample_input_ids = sample_input_ids[:self.max_seq_length]\n                sample_token_type_ids = [0] + example_token_type_ids[left_index:right_index]\n                sample_token_type_ids = sample_token_type_ids[:self.max_seq_length]\n                sample_attention_mask = [1] + example_attention_mask[left_index:right_index]\n                sample_attention_mask = sample_attention_mask[:self.max_seq_length]\n                sample_segment_ids = [0] + example_segment_ids[left_index:right_index]\n                sample_segment_ids = sample_segment_ids[:self.max_seq_length]\n                sample_token_seq_labels = [-100] + example_token_seq_labels[left_index:right_index]\n                sample_token_seq_labels = sample_token_seq_labels[:self.max_seq_length]\n                if sent_right_index - 1 == sent_left_index:\n                    left_index = right_index\n                    sample_input_ids[-1] = self.tokenizer.eos_token_id\n                    sample_token_seq_labels[-1] = -100\n                else:\n                    left_index = accumulate_length[sent_i - 1] + 1\n                    if sample_token_seq_labels[-1] != -100:\n                        sample_token_seq_labels[-1] = -100\n                if sent_right_index - 1 == sent_left_index or right_index == example_total_num_tokens:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index]\n                    sent_left_index = sent_right_index\n                    sent_i += 1\n                else:\n                    sample_sentences = example_sentences[sent_left_index:sent_right_index - 1]\n                    sent_left_index = sent_right_index - 1\n                if len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences) - 1 and len([_ for _ in sample_token_seq_labels if _ != -100]) != len(sample_sentences):\n                    tmp = []\n                    for (w_i, w, l) in zip(sample_input_ids, self.tokenizer.decode(sample_input_ids).split(' '), sample_token_seq_labels):\n                        tmp.append((w_i, w, l))\n                while len(sample_input_ids) < self.max_seq_length:\n                    sample_input_ids.append(self.tokenizer.pad_token_id)\n                    sample_token_type_ids.append(0)\n                    sample_attention_mask.append(0)\n                    sample_segment_ids.append(example_total_num_sentences + 1)\n                    sample_token_seq_labels.append(-100)\n                new_input_ids.append(sample_input_ids)\n                new_token_type_ids.append(sample_token_type_ids)\n                new_attention_mask.append(sample_attention_mask)\n                new_segment_ids.append(sample_segment_ids)\n                new_token_seq_labels.append(sample_token_seq_labels)\n                new_example_ids.append(example_id)\n                new_sentences.append(sample_sentences)\n            else:\n                sent_i += 1\n                continue\n    output_samples = {}\n    output_samples['input_ids'] = new_input_ids\n    output_samples['token_type_ids'] = new_token_type_ids\n    output_samples['attention_mask'] = new_attention_mask\n    output_samples['segment_ids'] = new_segment_ids\n    output_samples['example_id'] = new_example_ids\n    output_samples['labels'] = new_token_seq_labels\n    output_samples['sentences'] = new_sentences\n    return output_samples"
        ]
    }
]