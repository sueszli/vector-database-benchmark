[
    {
        "func_name": "__init__",
        "original": "def __init__(self, weights_fpath, device: Union[str, torch.device]=None, verbose=True):\n    \"\"\"\n        :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\").\n        If None, defaults to cuda if it is available on your machine, otherwise the model will\n        run on cpu. Outputs are always returned on the cpu, as numpy arrays.\n        \"\"\"\n    super().__init__()\n    self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)\n    self.linear = nn.Linear(model_hidden_size, model_embedding_size)\n    self.relu = nn.ReLU()\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    elif isinstance(device, str):\n        device = torch.device(device)\n    self.device = device\n    start = timer()\n    checkpoint = load_fsspec(weights_fpath, map_location='cpu')\n    self.load_state_dict(checkpoint['model_state'], strict=False)\n    self.to(device)\n    if verbose:\n        print('Loaded the voice encoder model on %s in %.2f seconds.' % (device.type, timer() - start))",
        "mutated": [
            "def __init__(self, weights_fpath, device: Union[str, torch.device]=None, verbose=True):\n    if False:\n        i = 10\n    '\\n        :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\").\\n        If None, defaults to cuda if it is available on your machine, otherwise the model will\\n        run on cpu. Outputs are always returned on the cpu, as numpy arrays.\\n        '\n    super().__init__()\n    self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)\n    self.linear = nn.Linear(model_hidden_size, model_embedding_size)\n    self.relu = nn.ReLU()\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    elif isinstance(device, str):\n        device = torch.device(device)\n    self.device = device\n    start = timer()\n    checkpoint = load_fsspec(weights_fpath, map_location='cpu')\n    self.load_state_dict(checkpoint['model_state'], strict=False)\n    self.to(device)\n    if verbose:\n        print('Loaded the voice encoder model on %s in %.2f seconds.' % (device.type, timer() - start))",
            "def __init__(self, weights_fpath, device: Union[str, torch.device]=None, verbose=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\").\\n        If None, defaults to cuda if it is available on your machine, otherwise the model will\\n        run on cpu. Outputs are always returned on the cpu, as numpy arrays.\\n        '\n    super().__init__()\n    self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)\n    self.linear = nn.Linear(model_hidden_size, model_embedding_size)\n    self.relu = nn.ReLU()\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    elif isinstance(device, str):\n        device = torch.device(device)\n    self.device = device\n    start = timer()\n    checkpoint = load_fsspec(weights_fpath, map_location='cpu')\n    self.load_state_dict(checkpoint['model_state'], strict=False)\n    self.to(device)\n    if verbose:\n        print('Loaded the voice encoder model on %s in %.2f seconds.' % (device.type, timer() - start))",
            "def __init__(self, weights_fpath, device: Union[str, torch.device]=None, verbose=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\").\\n        If None, defaults to cuda if it is available on your machine, otherwise the model will\\n        run on cpu. Outputs are always returned on the cpu, as numpy arrays.\\n        '\n    super().__init__()\n    self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)\n    self.linear = nn.Linear(model_hidden_size, model_embedding_size)\n    self.relu = nn.ReLU()\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    elif isinstance(device, str):\n        device = torch.device(device)\n    self.device = device\n    start = timer()\n    checkpoint = load_fsspec(weights_fpath, map_location='cpu')\n    self.load_state_dict(checkpoint['model_state'], strict=False)\n    self.to(device)\n    if verbose:\n        print('Loaded the voice encoder model on %s in %.2f seconds.' % (device.type, timer() - start))",
            "def __init__(self, weights_fpath, device: Union[str, torch.device]=None, verbose=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\").\\n        If None, defaults to cuda if it is available on your machine, otherwise the model will\\n        run on cpu. Outputs are always returned on the cpu, as numpy arrays.\\n        '\n    super().__init__()\n    self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)\n    self.linear = nn.Linear(model_hidden_size, model_embedding_size)\n    self.relu = nn.ReLU()\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    elif isinstance(device, str):\n        device = torch.device(device)\n    self.device = device\n    start = timer()\n    checkpoint = load_fsspec(weights_fpath, map_location='cpu')\n    self.load_state_dict(checkpoint['model_state'], strict=False)\n    self.to(device)\n    if verbose:\n        print('Loaded the voice encoder model on %s in %.2f seconds.' % (device.type, timer() - start))",
            "def __init__(self, weights_fpath, device: Union[str, torch.device]=None, verbose=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\").\\n        If None, defaults to cuda if it is available on your machine, otherwise the model will\\n        run on cpu. Outputs are always returned on the cpu, as numpy arrays.\\n        '\n    super().__init__()\n    self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)\n    self.linear = nn.Linear(model_hidden_size, model_embedding_size)\n    self.relu = nn.ReLU()\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    elif isinstance(device, str):\n        device = torch.device(device)\n    self.device = device\n    start = timer()\n    checkpoint = load_fsspec(weights_fpath, map_location='cpu')\n    self.load_state_dict(checkpoint['model_state'], strict=False)\n    self.to(device)\n    if verbose:\n        print('Loaded the voice encoder model on %s in %.2f seconds.' % (device.type, timer() - start))"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, mels: torch.FloatTensor):\n    \"\"\"\n        Computes the embeddings of a batch of utterance spectrograms.\n        :param mels: a batch of mel spectrograms of same duration as a float32 tensor of shape\n        (batch_size, n_frames, n_channels)\n        :return: the embeddings as a float 32 tensor of shape (batch_size, embedding_size).\n        Embeddings are positive and L2-normed, thus they lay in the range [0, 1].\n        \"\"\"\n    (_, (hidden, _)) = self.lstm(mels)\n    embeds_raw = self.relu(self.linear(hidden[-1]))\n    return embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)",
        "mutated": [
            "def forward(self, mels: torch.FloatTensor):\n    if False:\n        i = 10\n    '\\n        Computes the embeddings of a batch of utterance spectrograms.\\n        :param mels: a batch of mel spectrograms of same duration as a float32 tensor of shape\\n        (batch_size, n_frames, n_channels)\\n        :return: the embeddings as a float 32 tensor of shape (batch_size, embedding_size).\\n        Embeddings are positive and L2-normed, thus they lay in the range [0, 1].\\n        '\n    (_, (hidden, _)) = self.lstm(mels)\n    embeds_raw = self.relu(self.linear(hidden[-1]))\n    return embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)",
            "def forward(self, mels: torch.FloatTensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Computes the embeddings of a batch of utterance spectrograms.\\n        :param mels: a batch of mel spectrograms of same duration as a float32 tensor of shape\\n        (batch_size, n_frames, n_channels)\\n        :return: the embeddings as a float 32 tensor of shape (batch_size, embedding_size).\\n        Embeddings are positive and L2-normed, thus they lay in the range [0, 1].\\n        '\n    (_, (hidden, _)) = self.lstm(mels)\n    embeds_raw = self.relu(self.linear(hidden[-1]))\n    return embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)",
            "def forward(self, mels: torch.FloatTensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Computes the embeddings of a batch of utterance spectrograms.\\n        :param mels: a batch of mel spectrograms of same duration as a float32 tensor of shape\\n        (batch_size, n_frames, n_channels)\\n        :return: the embeddings as a float 32 tensor of shape (batch_size, embedding_size).\\n        Embeddings are positive and L2-normed, thus they lay in the range [0, 1].\\n        '\n    (_, (hidden, _)) = self.lstm(mels)\n    embeds_raw = self.relu(self.linear(hidden[-1]))\n    return embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)",
            "def forward(self, mels: torch.FloatTensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Computes the embeddings of a batch of utterance spectrograms.\\n        :param mels: a batch of mel spectrograms of same duration as a float32 tensor of shape\\n        (batch_size, n_frames, n_channels)\\n        :return: the embeddings as a float 32 tensor of shape (batch_size, embedding_size).\\n        Embeddings are positive and L2-normed, thus they lay in the range [0, 1].\\n        '\n    (_, (hidden, _)) = self.lstm(mels)\n    embeds_raw = self.relu(self.linear(hidden[-1]))\n    return embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)",
            "def forward(self, mels: torch.FloatTensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Computes the embeddings of a batch of utterance spectrograms.\\n        :param mels: a batch of mel spectrograms of same duration as a float32 tensor of shape\\n        (batch_size, n_frames, n_channels)\\n        :return: the embeddings as a float 32 tensor of shape (batch_size, embedding_size).\\n        Embeddings are positive and L2-normed, thus they lay in the range [0, 1].\\n        '\n    (_, (hidden, _)) = self.lstm(mels)\n    embeds_raw = self.relu(self.linear(hidden[-1]))\n    return embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)"
        ]
    },
    {
        "func_name": "compute_partial_slices",
        "original": "@staticmethod\ndef compute_partial_slices(n_samples: int, rate, min_coverage):\n    \"\"\"\n        Computes where to split an utterance waveform and its corresponding mel spectrogram to\n        obtain partial utterances of <partials_n_frames> each. Both the waveform and the\n        mel spectrogram slices are returned, so as to make each partial utterance waveform\n        correspond to its spectrogram.\n\n        The returned ranges may be indexing further than the length of the waveform. It is\n        recommended that you pad the waveform with zeros up to wav_slices[-1].stop.\n\n        :param n_samples: the number of samples in the waveform\n        :param rate: how many partial utterances should occur per second. Partial utterances must\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\n        the minimum rate is thus 0.625.\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\n        it will be discarded. If there aren't enough frames for one partial utterance,\n        this parameter is ignored so that the function always returns at least one slice.\n        :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\n        respectively the waveform and the mel spectrogram with these slices to obtain the partial\n        utterances.\n        \"\"\"\n    assert 0 < min_coverage <= 1\n    samples_per_frame = int(sampling_rate * mel_window_step / 1000)\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = int(np.round(sampling_rate / rate / samples_per_frame))\n    assert 0 < frame_step, 'The rate is too high'\n    assert frame_step <= partials_n_frames, 'The rate is too low, it should be %f at least' % (sampling_rate / (samples_per_frame * partials_n_frames))\n    (wav_slices, mel_slices) = ([], [])\n    steps = max(1, n_frames - partials_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partials_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    return (wav_slices, mel_slices)",
        "mutated": [
            "@staticmethod\ndef compute_partial_slices(n_samples: int, rate, min_coverage):\n    if False:\n        i = 10\n    \"\\n        Computes where to split an utterance waveform and its corresponding mel spectrogram to\\n        obtain partial utterances of <partials_n_frames> each. Both the waveform and the\\n        mel spectrogram slices are returned, so as to make each partial utterance waveform\\n        correspond to its spectrogram.\\n\\n        The returned ranges may be indexing further than the length of the waveform. It is\\n        recommended that you pad the waveform with zeros up to wav_slices[-1].stop.\\n\\n        :param n_samples: the number of samples in the waveform\\n        :param rate: how many partial utterances should occur per second. Partial utterances must\\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\\n        the minimum rate is thus 0.625.\\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\\n        it will be discarded. If there aren't enough frames for one partial utterance,\\n        this parameter is ignored so that the function always returns at least one slice.\\n        :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\\n        respectively the waveform and the mel spectrogram with these slices to obtain the partial\\n        utterances.\\n        \"\n    assert 0 < min_coverage <= 1\n    samples_per_frame = int(sampling_rate * mel_window_step / 1000)\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = int(np.round(sampling_rate / rate / samples_per_frame))\n    assert 0 < frame_step, 'The rate is too high'\n    assert frame_step <= partials_n_frames, 'The rate is too low, it should be %f at least' % (sampling_rate / (samples_per_frame * partials_n_frames))\n    (wav_slices, mel_slices) = ([], [])\n    steps = max(1, n_frames - partials_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partials_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    return (wav_slices, mel_slices)",
            "@staticmethod\ndef compute_partial_slices(n_samples: int, rate, min_coverage):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Computes where to split an utterance waveform and its corresponding mel spectrogram to\\n        obtain partial utterances of <partials_n_frames> each. Both the waveform and the\\n        mel spectrogram slices are returned, so as to make each partial utterance waveform\\n        correspond to its spectrogram.\\n\\n        The returned ranges may be indexing further than the length of the waveform. It is\\n        recommended that you pad the waveform with zeros up to wav_slices[-1].stop.\\n\\n        :param n_samples: the number of samples in the waveform\\n        :param rate: how many partial utterances should occur per second. Partial utterances must\\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\\n        the minimum rate is thus 0.625.\\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\\n        it will be discarded. If there aren't enough frames for one partial utterance,\\n        this parameter is ignored so that the function always returns at least one slice.\\n        :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\\n        respectively the waveform and the mel spectrogram with these slices to obtain the partial\\n        utterances.\\n        \"\n    assert 0 < min_coverage <= 1\n    samples_per_frame = int(sampling_rate * mel_window_step / 1000)\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = int(np.round(sampling_rate / rate / samples_per_frame))\n    assert 0 < frame_step, 'The rate is too high'\n    assert frame_step <= partials_n_frames, 'The rate is too low, it should be %f at least' % (sampling_rate / (samples_per_frame * partials_n_frames))\n    (wav_slices, mel_slices) = ([], [])\n    steps = max(1, n_frames - partials_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partials_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    return (wav_slices, mel_slices)",
            "@staticmethod\ndef compute_partial_slices(n_samples: int, rate, min_coverage):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Computes where to split an utterance waveform and its corresponding mel spectrogram to\\n        obtain partial utterances of <partials_n_frames> each. Both the waveform and the\\n        mel spectrogram slices are returned, so as to make each partial utterance waveform\\n        correspond to its spectrogram.\\n\\n        The returned ranges may be indexing further than the length of the waveform. It is\\n        recommended that you pad the waveform with zeros up to wav_slices[-1].stop.\\n\\n        :param n_samples: the number of samples in the waveform\\n        :param rate: how many partial utterances should occur per second. Partial utterances must\\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\\n        the minimum rate is thus 0.625.\\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\\n        it will be discarded. If there aren't enough frames for one partial utterance,\\n        this parameter is ignored so that the function always returns at least one slice.\\n        :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\\n        respectively the waveform and the mel spectrogram with these slices to obtain the partial\\n        utterances.\\n        \"\n    assert 0 < min_coverage <= 1\n    samples_per_frame = int(sampling_rate * mel_window_step / 1000)\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = int(np.round(sampling_rate / rate / samples_per_frame))\n    assert 0 < frame_step, 'The rate is too high'\n    assert frame_step <= partials_n_frames, 'The rate is too low, it should be %f at least' % (sampling_rate / (samples_per_frame * partials_n_frames))\n    (wav_slices, mel_slices) = ([], [])\n    steps = max(1, n_frames - partials_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partials_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    return (wav_slices, mel_slices)",
            "@staticmethod\ndef compute_partial_slices(n_samples: int, rate, min_coverage):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Computes where to split an utterance waveform and its corresponding mel spectrogram to\\n        obtain partial utterances of <partials_n_frames> each. Both the waveform and the\\n        mel spectrogram slices are returned, so as to make each partial utterance waveform\\n        correspond to its spectrogram.\\n\\n        The returned ranges may be indexing further than the length of the waveform. It is\\n        recommended that you pad the waveform with zeros up to wav_slices[-1].stop.\\n\\n        :param n_samples: the number of samples in the waveform\\n        :param rate: how many partial utterances should occur per second. Partial utterances must\\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\\n        the minimum rate is thus 0.625.\\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\\n        it will be discarded. If there aren't enough frames for one partial utterance,\\n        this parameter is ignored so that the function always returns at least one slice.\\n        :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\\n        respectively the waveform and the mel spectrogram with these slices to obtain the partial\\n        utterances.\\n        \"\n    assert 0 < min_coverage <= 1\n    samples_per_frame = int(sampling_rate * mel_window_step / 1000)\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = int(np.round(sampling_rate / rate / samples_per_frame))\n    assert 0 < frame_step, 'The rate is too high'\n    assert frame_step <= partials_n_frames, 'The rate is too low, it should be %f at least' % (sampling_rate / (samples_per_frame * partials_n_frames))\n    (wav_slices, mel_slices) = ([], [])\n    steps = max(1, n_frames - partials_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partials_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    return (wav_slices, mel_slices)",
            "@staticmethod\ndef compute_partial_slices(n_samples: int, rate, min_coverage):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Computes where to split an utterance waveform and its corresponding mel spectrogram to\\n        obtain partial utterances of <partials_n_frames> each. Both the waveform and the\\n        mel spectrogram slices are returned, so as to make each partial utterance waveform\\n        correspond to its spectrogram.\\n\\n        The returned ranges may be indexing further than the length of the waveform. It is\\n        recommended that you pad the waveform with zeros up to wav_slices[-1].stop.\\n\\n        :param n_samples: the number of samples in the waveform\\n        :param rate: how many partial utterances should occur per second. Partial utterances must\\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\\n        the minimum rate is thus 0.625.\\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\\n        it will be discarded. If there aren't enough frames for one partial utterance,\\n        this parameter is ignored so that the function always returns at least one slice.\\n        :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\\n        respectively the waveform and the mel spectrogram with these slices to obtain the partial\\n        utterances.\\n        \"\n    assert 0 < min_coverage <= 1\n    samples_per_frame = int(sampling_rate * mel_window_step / 1000)\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = int(np.round(sampling_rate / rate / samples_per_frame))\n    assert 0 < frame_step, 'The rate is too high'\n    assert frame_step <= partials_n_frames, 'The rate is too low, it should be %f at least' % (sampling_rate / (samples_per_frame * partials_n_frames))\n    (wav_slices, mel_slices) = ([], [])\n    steps = max(1, n_frames - partials_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partials_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    return (wav_slices, mel_slices)"
        ]
    },
    {
        "func_name": "embed_utterance",
        "original": "def embed_utterance(self, wav: np.ndarray, return_partials=False, rate=1.3, min_coverage=0.75):\n    \"\"\"\n        Computes an embedding for a single utterance. The utterance is divided in partial\n        utterances and an embedding is computed for each. The complete utterance embedding is the\n        L2-normed average embedding of the partial utterances.\n\n        TODO: independent batched version of this function\n\n        :param wav: a preprocessed utterance waveform as a numpy array of float32\n        :param return_partials: if True, the partial embeddings will also be returned along with\n        the wav slices corresponding to each partial utterance.\n        :param rate: how many partial utterances should occur per second. Partial utterances must\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\n        the minimum rate is thus 0.625.\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\n        it will be discarded. If there aren't enough frames for one partial utterance,\n        this parameter is ignored so that the function always returns at least one slice.\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If\n        <return_partials> is True, the partial utterances as a numpy array of float32 of shape\n        (n_partials, model_embedding_size) and the wav partials as a list of slices will also be\n        returned.\n        \"\"\"\n    (wav_slices, mel_slices) = self.compute_partial_slices(len(wav), rate, min_coverage)\n    max_wave_length = wav_slices[-1].stop\n    if max_wave_length >= len(wav):\n        wav = np.pad(wav, (0, max_wave_length - len(wav)), 'constant')\n    mel = audio.wav_to_mel_spectrogram(wav)\n    mels = np.array([mel[s] for s in mel_slices])\n    with torch.no_grad():\n        mels = torch.from_numpy(mels).to(self.device)\n        partial_embeds = self(mels).cpu().numpy()\n    raw_embed = np.mean(partial_embeds, axis=0)\n    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n    if return_partials:\n        return (embed, partial_embeds, wav_slices)\n    return embed",
        "mutated": [
            "def embed_utterance(self, wav: np.ndarray, return_partials=False, rate=1.3, min_coverage=0.75):\n    if False:\n        i = 10\n    \"\\n        Computes an embedding for a single utterance. The utterance is divided in partial\\n        utterances and an embedding is computed for each. The complete utterance embedding is the\\n        L2-normed average embedding of the partial utterances.\\n\\n        TODO: independent batched version of this function\\n\\n        :param wav: a preprocessed utterance waveform as a numpy array of float32\\n        :param return_partials: if True, the partial embeddings will also be returned along with\\n        the wav slices corresponding to each partial utterance.\\n        :param rate: how many partial utterances should occur per second. Partial utterances must\\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\\n        the minimum rate is thus 0.625.\\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\\n        it will be discarded. If there aren't enough frames for one partial utterance,\\n        this parameter is ignored so that the function always returns at least one slice.\\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If\\n        <return_partials> is True, the partial utterances as a numpy array of float32 of shape\\n        (n_partials, model_embedding_size) and the wav partials as a list of slices will also be\\n        returned.\\n        \"\n    (wav_slices, mel_slices) = self.compute_partial_slices(len(wav), rate, min_coverage)\n    max_wave_length = wav_slices[-1].stop\n    if max_wave_length >= len(wav):\n        wav = np.pad(wav, (0, max_wave_length - len(wav)), 'constant')\n    mel = audio.wav_to_mel_spectrogram(wav)\n    mels = np.array([mel[s] for s in mel_slices])\n    with torch.no_grad():\n        mels = torch.from_numpy(mels).to(self.device)\n        partial_embeds = self(mels).cpu().numpy()\n    raw_embed = np.mean(partial_embeds, axis=0)\n    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n    if return_partials:\n        return (embed, partial_embeds, wav_slices)\n    return embed",
            "def embed_utterance(self, wav: np.ndarray, return_partials=False, rate=1.3, min_coverage=0.75):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Computes an embedding for a single utterance. The utterance is divided in partial\\n        utterances and an embedding is computed for each. The complete utterance embedding is the\\n        L2-normed average embedding of the partial utterances.\\n\\n        TODO: independent batched version of this function\\n\\n        :param wav: a preprocessed utterance waveform as a numpy array of float32\\n        :param return_partials: if True, the partial embeddings will also be returned along with\\n        the wav slices corresponding to each partial utterance.\\n        :param rate: how many partial utterances should occur per second. Partial utterances must\\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\\n        the minimum rate is thus 0.625.\\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\\n        it will be discarded. If there aren't enough frames for one partial utterance,\\n        this parameter is ignored so that the function always returns at least one slice.\\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If\\n        <return_partials> is True, the partial utterances as a numpy array of float32 of shape\\n        (n_partials, model_embedding_size) and the wav partials as a list of slices will also be\\n        returned.\\n        \"\n    (wav_slices, mel_slices) = self.compute_partial_slices(len(wav), rate, min_coverage)\n    max_wave_length = wav_slices[-1].stop\n    if max_wave_length >= len(wav):\n        wav = np.pad(wav, (0, max_wave_length - len(wav)), 'constant')\n    mel = audio.wav_to_mel_spectrogram(wav)\n    mels = np.array([mel[s] for s in mel_slices])\n    with torch.no_grad():\n        mels = torch.from_numpy(mels).to(self.device)\n        partial_embeds = self(mels).cpu().numpy()\n    raw_embed = np.mean(partial_embeds, axis=0)\n    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n    if return_partials:\n        return (embed, partial_embeds, wav_slices)\n    return embed",
            "def embed_utterance(self, wav: np.ndarray, return_partials=False, rate=1.3, min_coverage=0.75):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Computes an embedding for a single utterance. The utterance is divided in partial\\n        utterances and an embedding is computed for each. The complete utterance embedding is the\\n        L2-normed average embedding of the partial utterances.\\n\\n        TODO: independent batched version of this function\\n\\n        :param wav: a preprocessed utterance waveform as a numpy array of float32\\n        :param return_partials: if True, the partial embeddings will also be returned along with\\n        the wav slices corresponding to each partial utterance.\\n        :param rate: how many partial utterances should occur per second. Partial utterances must\\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\\n        the minimum rate is thus 0.625.\\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\\n        it will be discarded. If there aren't enough frames for one partial utterance,\\n        this parameter is ignored so that the function always returns at least one slice.\\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If\\n        <return_partials> is True, the partial utterances as a numpy array of float32 of shape\\n        (n_partials, model_embedding_size) and the wav partials as a list of slices will also be\\n        returned.\\n        \"\n    (wav_slices, mel_slices) = self.compute_partial_slices(len(wav), rate, min_coverage)\n    max_wave_length = wav_slices[-1].stop\n    if max_wave_length >= len(wav):\n        wav = np.pad(wav, (0, max_wave_length - len(wav)), 'constant')\n    mel = audio.wav_to_mel_spectrogram(wav)\n    mels = np.array([mel[s] for s in mel_slices])\n    with torch.no_grad():\n        mels = torch.from_numpy(mels).to(self.device)\n        partial_embeds = self(mels).cpu().numpy()\n    raw_embed = np.mean(partial_embeds, axis=0)\n    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n    if return_partials:\n        return (embed, partial_embeds, wav_slices)\n    return embed",
            "def embed_utterance(self, wav: np.ndarray, return_partials=False, rate=1.3, min_coverage=0.75):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Computes an embedding for a single utterance. The utterance is divided in partial\\n        utterances and an embedding is computed for each. The complete utterance embedding is the\\n        L2-normed average embedding of the partial utterances.\\n\\n        TODO: independent batched version of this function\\n\\n        :param wav: a preprocessed utterance waveform as a numpy array of float32\\n        :param return_partials: if True, the partial embeddings will also be returned along with\\n        the wav slices corresponding to each partial utterance.\\n        :param rate: how many partial utterances should occur per second. Partial utterances must\\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\\n        the minimum rate is thus 0.625.\\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\\n        it will be discarded. If there aren't enough frames for one partial utterance,\\n        this parameter is ignored so that the function always returns at least one slice.\\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If\\n        <return_partials> is True, the partial utterances as a numpy array of float32 of shape\\n        (n_partials, model_embedding_size) and the wav partials as a list of slices will also be\\n        returned.\\n        \"\n    (wav_slices, mel_slices) = self.compute_partial_slices(len(wav), rate, min_coverage)\n    max_wave_length = wav_slices[-1].stop\n    if max_wave_length >= len(wav):\n        wav = np.pad(wav, (0, max_wave_length - len(wav)), 'constant')\n    mel = audio.wav_to_mel_spectrogram(wav)\n    mels = np.array([mel[s] for s in mel_slices])\n    with torch.no_grad():\n        mels = torch.from_numpy(mels).to(self.device)\n        partial_embeds = self(mels).cpu().numpy()\n    raw_embed = np.mean(partial_embeds, axis=0)\n    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n    if return_partials:\n        return (embed, partial_embeds, wav_slices)\n    return embed",
            "def embed_utterance(self, wav: np.ndarray, return_partials=False, rate=1.3, min_coverage=0.75):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Computes an embedding for a single utterance. The utterance is divided in partial\\n        utterances and an embedding is computed for each. The complete utterance embedding is the\\n        L2-normed average embedding of the partial utterances.\\n\\n        TODO: independent batched version of this function\\n\\n        :param wav: a preprocessed utterance waveform as a numpy array of float32\\n        :param return_partials: if True, the partial embeddings will also be returned along with\\n        the wav slices corresponding to each partial utterance.\\n        :param rate: how many partial utterances should occur per second. Partial utterances must\\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\\n        the minimum rate is thus 0.625.\\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\\n        it will be discarded. If there aren't enough frames for one partial utterance,\\n        this parameter is ignored so that the function always returns at least one slice.\\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If\\n        <return_partials> is True, the partial utterances as a numpy array of float32 of shape\\n        (n_partials, model_embedding_size) and the wav partials as a list of slices will also be\\n        returned.\\n        \"\n    (wav_slices, mel_slices) = self.compute_partial_slices(len(wav), rate, min_coverage)\n    max_wave_length = wav_slices[-1].stop\n    if max_wave_length >= len(wav):\n        wav = np.pad(wav, (0, max_wave_length - len(wav)), 'constant')\n    mel = audio.wav_to_mel_spectrogram(wav)\n    mels = np.array([mel[s] for s in mel_slices])\n    with torch.no_grad():\n        mels = torch.from_numpy(mels).to(self.device)\n        partial_embeds = self(mels).cpu().numpy()\n    raw_embed = np.mean(partial_embeds, axis=0)\n    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n    if return_partials:\n        return (embed, partial_embeds, wav_slices)\n    return embed"
        ]
    },
    {
        "func_name": "embed_speaker",
        "original": "def embed_speaker(self, wavs: List[np.ndarray], **kwargs):\n    \"\"\"\n        Compute the embedding of a collection of wavs (presumably from the same speaker) by\n        averaging their embedding and L2-normalizing it.\n\n        :param wavs: list of wavs a numpy arrays of float32.\n        :param kwargs: extra arguments to embed_utterance()\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,).\n        \"\"\"\n    raw_embed = np.mean([self.embed_utterance(wav, return_partials=False, **kwargs) for wav in wavs], axis=0)\n    return raw_embed / np.linalg.norm(raw_embed, 2)",
        "mutated": [
            "def embed_speaker(self, wavs: List[np.ndarray], **kwargs):\n    if False:\n        i = 10\n    '\\n        Compute the embedding of a collection of wavs (presumably from the same speaker) by\\n        averaging their embedding and L2-normalizing it.\\n\\n        :param wavs: list of wavs a numpy arrays of float32.\\n        :param kwargs: extra arguments to embed_utterance()\\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,).\\n        '\n    raw_embed = np.mean([self.embed_utterance(wav, return_partials=False, **kwargs) for wav in wavs], axis=0)\n    return raw_embed / np.linalg.norm(raw_embed, 2)",
            "def embed_speaker(self, wavs: List[np.ndarray], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Compute the embedding of a collection of wavs (presumably from the same speaker) by\\n        averaging their embedding and L2-normalizing it.\\n\\n        :param wavs: list of wavs a numpy arrays of float32.\\n        :param kwargs: extra arguments to embed_utterance()\\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,).\\n        '\n    raw_embed = np.mean([self.embed_utterance(wav, return_partials=False, **kwargs) for wav in wavs], axis=0)\n    return raw_embed / np.linalg.norm(raw_embed, 2)",
            "def embed_speaker(self, wavs: List[np.ndarray], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Compute the embedding of a collection of wavs (presumably from the same speaker) by\\n        averaging their embedding and L2-normalizing it.\\n\\n        :param wavs: list of wavs a numpy arrays of float32.\\n        :param kwargs: extra arguments to embed_utterance()\\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,).\\n        '\n    raw_embed = np.mean([self.embed_utterance(wav, return_partials=False, **kwargs) for wav in wavs], axis=0)\n    return raw_embed / np.linalg.norm(raw_embed, 2)",
            "def embed_speaker(self, wavs: List[np.ndarray], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Compute the embedding of a collection of wavs (presumably from the same speaker) by\\n        averaging their embedding and L2-normalizing it.\\n\\n        :param wavs: list of wavs a numpy arrays of float32.\\n        :param kwargs: extra arguments to embed_utterance()\\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,).\\n        '\n    raw_embed = np.mean([self.embed_utterance(wav, return_partials=False, **kwargs) for wav in wavs], axis=0)\n    return raw_embed / np.linalg.norm(raw_embed, 2)",
            "def embed_speaker(self, wavs: List[np.ndarray], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Compute the embedding of a collection of wavs (presumably from the same speaker) by\\n        averaging their embedding and L2-normalizing it.\\n\\n        :param wavs: list of wavs a numpy arrays of float32.\\n        :param kwargs: extra arguments to embed_utterance()\\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,).\\n        '\n    raw_embed = np.mean([self.embed_utterance(wav, return_partials=False, **kwargs) for wav in wavs], axis=0)\n    return raw_embed / np.linalg.norm(raw_embed, 2)"
        ]
    }
]