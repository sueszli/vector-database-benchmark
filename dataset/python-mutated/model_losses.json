[
    {
        "func_name": "create_dis_loss",
        "original": "def create_dis_loss(fake_predictions, real_predictions, targets_present):\n    \"\"\"Compute Discriminator loss across real/fake.\"\"\"\n    missing = tf.cast(targets_present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    real_labels = tf.ones([FLAGS.batch_size, FLAGS.sequence_length])\n    dis_loss_real = tf.losses.sigmoid_cross_entropy(real_labels, real_predictions, weights=missing)\n    dis_loss_fake = tf.losses.sigmoid_cross_entropy(targets_present, fake_predictions, weights=missing)\n    dis_loss = (dis_loss_fake + dis_loss_real) / 2.0\n    return (dis_loss, dis_loss_fake, dis_loss_real)",
        "mutated": [
            "def create_dis_loss(fake_predictions, real_predictions, targets_present):\n    if False:\n        i = 10\n    'Compute Discriminator loss across real/fake.'\n    missing = tf.cast(targets_present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    real_labels = tf.ones([FLAGS.batch_size, FLAGS.sequence_length])\n    dis_loss_real = tf.losses.sigmoid_cross_entropy(real_labels, real_predictions, weights=missing)\n    dis_loss_fake = tf.losses.sigmoid_cross_entropy(targets_present, fake_predictions, weights=missing)\n    dis_loss = (dis_loss_fake + dis_loss_real) / 2.0\n    return (dis_loss, dis_loss_fake, dis_loss_real)",
            "def create_dis_loss(fake_predictions, real_predictions, targets_present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute Discriminator loss across real/fake.'\n    missing = tf.cast(targets_present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    real_labels = tf.ones([FLAGS.batch_size, FLAGS.sequence_length])\n    dis_loss_real = tf.losses.sigmoid_cross_entropy(real_labels, real_predictions, weights=missing)\n    dis_loss_fake = tf.losses.sigmoid_cross_entropy(targets_present, fake_predictions, weights=missing)\n    dis_loss = (dis_loss_fake + dis_loss_real) / 2.0\n    return (dis_loss, dis_loss_fake, dis_loss_real)",
            "def create_dis_loss(fake_predictions, real_predictions, targets_present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute Discriminator loss across real/fake.'\n    missing = tf.cast(targets_present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    real_labels = tf.ones([FLAGS.batch_size, FLAGS.sequence_length])\n    dis_loss_real = tf.losses.sigmoid_cross_entropy(real_labels, real_predictions, weights=missing)\n    dis_loss_fake = tf.losses.sigmoid_cross_entropy(targets_present, fake_predictions, weights=missing)\n    dis_loss = (dis_loss_fake + dis_loss_real) / 2.0\n    return (dis_loss, dis_loss_fake, dis_loss_real)",
            "def create_dis_loss(fake_predictions, real_predictions, targets_present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute Discriminator loss across real/fake.'\n    missing = tf.cast(targets_present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    real_labels = tf.ones([FLAGS.batch_size, FLAGS.sequence_length])\n    dis_loss_real = tf.losses.sigmoid_cross_entropy(real_labels, real_predictions, weights=missing)\n    dis_loss_fake = tf.losses.sigmoid_cross_entropy(targets_present, fake_predictions, weights=missing)\n    dis_loss = (dis_loss_fake + dis_loss_real) / 2.0\n    return (dis_loss, dis_loss_fake, dis_loss_real)",
            "def create_dis_loss(fake_predictions, real_predictions, targets_present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute Discriminator loss across real/fake.'\n    missing = tf.cast(targets_present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    real_labels = tf.ones([FLAGS.batch_size, FLAGS.sequence_length])\n    dis_loss_real = tf.losses.sigmoid_cross_entropy(real_labels, real_predictions, weights=missing)\n    dis_loss_fake = tf.losses.sigmoid_cross_entropy(targets_present, fake_predictions, weights=missing)\n    dis_loss = (dis_loss_fake + dis_loss_real) / 2.0\n    return (dis_loss, dis_loss_fake, dis_loss_real)"
        ]
    },
    {
        "func_name": "create_critic_loss",
        "original": "def create_critic_loss(cumulative_rewards, estimated_values, present):\n    \"\"\"Compute Critic loss in estimating the value function.  This should be an\n  estimate only for the missing elements.\"\"\"\n    missing = tf.cast(present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    loss = tf.losses.mean_squared_error(labels=cumulative_rewards, predictions=estimated_values, weights=missing)\n    return loss",
        "mutated": [
            "def create_critic_loss(cumulative_rewards, estimated_values, present):\n    if False:\n        i = 10\n    'Compute Critic loss in estimating the value function.  This should be an\\n  estimate only for the missing elements.'\n    missing = tf.cast(present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    loss = tf.losses.mean_squared_error(labels=cumulative_rewards, predictions=estimated_values, weights=missing)\n    return loss",
            "def create_critic_loss(cumulative_rewards, estimated_values, present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute Critic loss in estimating the value function.  This should be an\\n  estimate only for the missing elements.'\n    missing = tf.cast(present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    loss = tf.losses.mean_squared_error(labels=cumulative_rewards, predictions=estimated_values, weights=missing)\n    return loss",
            "def create_critic_loss(cumulative_rewards, estimated_values, present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute Critic loss in estimating the value function.  This should be an\\n  estimate only for the missing elements.'\n    missing = tf.cast(present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    loss = tf.losses.mean_squared_error(labels=cumulative_rewards, predictions=estimated_values, weights=missing)\n    return loss",
            "def create_critic_loss(cumulative_rewards, estimated_values, present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute Critic loss in estimating the value function.  This should be an\\n  estimate only for the missing elements.'\n    missing = tf.cast(present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    loss = tf.losses.mean_squared_error(labels=cumulative_rewards, predictions=estimated_values, weights=missing)\n    return loss",
            "def create_critic_loss(cumulative_rewards, estimated_values, present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute Critic loss in estimating the value function.  This should be an\\n  estimate only for the missing elements.'\n    missing = tf.cast(present, tf.int32)\n    missing = 1 - missing\n    missing = tf.cast(missing, tf.bool)\n    loss = tf.losses.mean_squared_error(labels=cumulative_rewards, predictions=estimated_values, weights=missing)\n    return loss"
        ]
    },
    {
        "func_name": "create_masked_cross_entropy_loss",
        "original": "def create_masked_cross_entropy_loss(targets, present, logits):\n    \"\"\"Calculate the cross entropy loss matrices for the masked tokens.\"\"\"\n    cross_entropy_losses = losses.cross_entropy_loss_matrix(targets, logits)\n    zeros_losses = tf.zeros(shape=[FLAGS.batch_size, FLAGS.sequence_length], dtype=tf.float32)\n    missing_ce_loss = tf.where(present, zeros_losses, cross_entropy_losses)\n    return missing_ce_loss",
        "mutated": [
            "def create_masked_cross_entropy_loss(targets, present, logits):\n    if False:\n        i = 10\n    'Calculate the cross entropy loss matrices for the masked tokens.'\n    cross_entropy_losses = losses.cross_entropy_loss_matrix(targets, logits)\n    zeros_losses = tf.zeros(shape=[FLAGS.batch_size, FLAGS.sequence_length], dtype=tf.float32)\n    missing_ce_loss = tf.where(present, zeros_losses, cross_entropy_losses)\n    return missing_ce_loss",
            "def create_masked_cross_entropy_loss(targets, present, logits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate the cross entropy loss matrices for the masked tokens.'\n    cross_entropy_losses = losses.cross_entropy_loss_matrix(targets, logits)\n    zeros_losses = tf.zeros(shape=[FLAGS.batch_size, FLAGS.sequence_length], dtype=tf.float32)\n    missing_ce_loss = tf.where(present, zeros_losses, cross_entropy_losses)\n    return missing_ce_loss",
            "def create_masked_cross_entropy_loss(targets, present, logits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate the cross entropy loss matrices for the masked tokens.'\n    cross_entropy_losses = losses.cross_entropy_loss_matrix(targets, logits)\n    zeros_losses = tf.zeros(shape=[FLAGS.batch_size, FLAGS.sequence_length], dtype=tf.float32)\n    missing_ce_loss = tf.where(present, zeros_losses, cross_entropy_losses)\n    return missing_ce_loss",
            "def create_masked_cross_entropy_loss(targets, present, logits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate the cross entropy loss matrices for the masked tokens.'\n    cross_entropy_losses = losses.cross_entropy_loss_matrix(targets, logits)\n    zeros_losses = tf.zeros(shape=[FLAGS.batch_size, FLAGS.sequence_length], dtype=tf.float32)\n    missing_ce_loss = tf.where(present, zeros_losses, cross_entropy_losses)\n    return missing_ce_loss",
            "def create_masked_cross_entropy_loss(targets, present, logits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate the cross entropy loss matrices for the masked tokens.'\n    cross_entropy_losses = losses.cross_entropy_loss_matrix(targets, logits)\n    zeros_losses = tf.zeros(shape=[FLAGS.batch_size, FLAGS.sequence_length], dtype=tf.float32)\n    missing_ce_loss = tf.where(present, zeros_losses, cross_entropy_losses)\n    return missing_ce_loss"
        ]
    },
    {
        "func_name": "calculate_reinforce_objective",
        "original": "def calculate_reinforce_objective(hparams, log_probs, dis_predictions, present, estimated_values=None):\n    \"\"\"Calculate the REINFORCE objectives.  The REINFORCE objective should\n  only be on the tokens that were missing.  Specifically, the final Generator\n  reward should be based on the Discriminator predictions on missing tokens.\n  The log probaibilities should be only for missing tokens and the baseline\n  should be calculated only on the missing tokens.\n\n  For this model, we optimize the reward is the log of the *conditional*\n  probability the Discriminator assigns to the distribution.  Specifically, for\n  a Discriminator D which outputs probability of real, given the past context,\n\n    r_t = log D(x_t|x_0,x_1,...x_{t-1})\n\n  And the policy for Generator G is the log-probability of taking action x2\n  given the past context.\n\n\n  Args:\n    hparams:  MaskGAN hyperparameters.\n    log_probs:  tf.float32 Tensor of log probailities of the tokens selected by\n      the Generator.  Shape [batch_size, sequence_length].\n    dis_predictions:  tf.float32 Tensor of the predictions from the\n      Discriminator.  Shape [batch_size, sequence_length].\n    present:  tf.bool Tensor indicating which tokens are present.  Shape\n      [batch_size, sequence_length].\n    estimated_values:  tf.float32 Tensor of estimated state values of tokens.\n      Shape [batch_size, sequence_length]\n\n  Returns:\n    final_gen_objective:  Final REINFORCE objective for the sequence.\n    rewards:  tf.float32 Tensor of rewards for sequence of shape [batch_size,\n      sequence_length]\n    advantages: tf.float32 Tensor of advantages for sequence of shape\n      [batch_size, sequence_length]\n    baselines:  tf.float32 Tensor of baselines for sequence of shape\n      [batch_size, sequence_length]\n    maintain_averages_op:  ExponentialMovingAverage apply average op to\n      maintain the baseline.\n  \"\"\"\n    final_gen_objective = 0.0\n    gamma = hparams.rl_discount_rate\n    eps = 1e-07\n    eps = tf.constant(1e-07, tf.float32)\n    dis_predictions = tf.nn.sigmoid(dis_predictions)\n    rewards = tf.log(dis_predictions + eps)\n    zeros = tf.zeros_like(present, dtype=tf.float32)\n    log_probs = tf.where(present, zeros, log_probs)\n    rewards = tf.where(present, zeros, rewards)\n    rewards_list = tf.unstack(rewards, axis=1)\n    log_probs_list = tf.unstack(log_probs, axis=1)\n    missing = 1.0 - tf.cast(present, tf.float32)\n    missing_list = tf.unstack(missing, axis=1)\n    cumulative_rewards = []\n    for t in xrange(FLAGS.sequence_length):\n        cum_value = tf.zeros(shape=[FLAGS.batch_size])\n        for s in xrange(t, FLAGS.sequence_length):\n            cum_value += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n        cumulative_rewards.append(cum_value)\n    cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n    if FLAGS.baseline_method == 'critic':\n        critic_loss = create_critic_loss(cumulative_rewards, estimated_values, present)\n        baselines = tf.unstack(estimated_values, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cum_advantage -= baselines[t]\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'dis_batch':\n        [rewards_half, baseline_half] = tf.split(rewards, num_or_size_splits=2, axis=0)\n        [log_probs_half, _] = tf.split(log_probs, num_or_size_splits=2, axis=0)\n        [reward_present_half, baseline_present_half] = tf.split(present, num_or_size_splits=2, axis=0)\n        baseline_list = tf.unstack(baseline_half, axis=1)\n        baseline_missing = 1.0 - tf.cast(baseline_present_half, tf.float32)\n        baseline_missing_list = tf.unstack(baseline_missing, axis=1)\n        baselines = []\n        for t in xrange(FLAGS.sequence_length):\n            num_missing = tf.reduce_sum(baseline_missing_list[t])\n            avg_baseline = tf.reduce_sum(baseline_missing_list[t] * baseline_list[t], keep_dims=True) / (num_missing + eps)\n            baseline = tf.tile(avg_baseline, multiples=[FLAGS.batch_size / 2])\n            baselines.append(baseline)\n        rewards_list = tf.unstack(rewards_half, axis=1)\n        log_probs_list = tf.unstack(log_probs_half, axis=1)\n        reward_missing = 1.0 - tf.cast(reward_present_half, tf.float32)\n        reward_missing_list = tf.unstack(reward_missing, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += reward_missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(reward_missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, reward_missing_list[t] * tf.stop_gradient(cum_advantage))\n        cumulative_rewards = []\n        for t in xrange(FLAGS.sequence_length):\n            cum_value = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_value += reward_missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cumulative_rewards.append(cum_value)\n        cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n        rewards = rewards_half\n        critic_loss = None\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'ema':\n        ema = tf.train.ExponentialMovingAverage(decay=hparams.baseline_decay)\n        maintain_averages_op = ema.apply(rewards_list)\n        baselines = []\n        for r in rewards_list:\n            baselines.append(ema.average(r))\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        critic_loss = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method is None:\n        num_missing = tf.reduce_sum(missing)\n        final_gen_objective += tf.reduce_sum(rewards) / (num_missing + eps)\n        baselines = tf.zeros_like(rewards)\n        critic_loss = None\n        maintain_averages_op = None\n        advantages = cumulative_rewards\n    else:\n        raise NotImplementedError\n    return [final_gen_objective, log_probs, rewards, advantages, baselines, maintain_averages_op, critic_loss, cumulative_rewards]",
        "mutated": [
            "def calculate_reinforce_objective(hparams, log_probs, dis_predictions, present, estimated_values=None):\n    if False:\n        i = 10\n    'Calculate the REINFORCE objectives.  The REINFORCE objective should\\n  only be on the tokens that were missing.  Specifically, the final Generator\\n  reward should be based on the Discriminator predictions on missing tokens.\\n  The log probaibilities should be only for missing tokens and the baseline\\n  should be calculated only on the missing tokens.\\n\\n  For this model, we optimize the reward is the log of the *conditional*\\n  probability the Discriminator assigns to the distribution.  Specifically, for\\n  a Discriminator D which outputs probability of real, given the past context,\\n\\n    r_t = log D(x_t|x_0,x_1,...x_{t-1})\\n\\n  And the policy for Generator G is the log-probability of taking action x2\\n  given the past context.\\n\\n\\n  Args:\\n    hparams:  MaskGAN hyperparameters.\\n    log_probs:  tf.float32 Tensor of log probailities of the tokens selected by\\n      the Generator.  Shape [batch_size, sequence_length].\\n    dis_predictions:  tf.float32 Tensor of the predictions from the\\n      Discriminator.  Shape [batch_size, sequence_length].\\n    present:  tf.bool Tensor indicating which tokens are present.  Shape\\n      [batch_size, sequence_length].\\n    estimated_values:  tf.float32 Tensor of estimated state values of tokens.\\n      Shape [batch_size, sequence_length]\\n\\n  Returns:\\n    final_gen_objective:  Final REINFORCE objective for the sequence.\\n    rewards:  tf.float32 Tensor of rewards for sequence of shape [batch_size,\\n      sequence_length]\\n    advantages: tf.float32 Tensor of advantages for sequence of shape\\n      [batch_size, sequence_length]\\n    baselines:  tf.float32 Tensor of baselines for sequence of shape\\n      [batch_size, sequence_length]\\n    maintain_averages_op:  ExponentialMovingAverage apply average op to\\n      maintain the baseline.\\n  '\n    final_gen_objective = 0.0\n    gamma = hparams.rl_discount_rate\n    eps = 1e-07\n    eps = tf.constant(1e-07, tf.float32)\n    dis_predictions = tf.nn.sigmoid(dis_predictions)\n    rewards = tf.log(dis_predictions + eps)\n    zeros = tf.zeros_like(present, dtype=tf.float32)\n    log_probs = tf.where(present, zeros, log_probs)\n    rewards = tf.where(present, zeros, rewards)\n    rewards_list = tf.unstack(rewards, axis=1)\n    log_probs_list = tf.unstack(log_probs, axis=1)\n    missing = 1.0 - tf.cast(present, tf.float32)\n    missing_list = tf.unstack(missing, axis=1)\n    cumulative_rewards = []\n    for t in xrange(FLAGS.sequence_length):\n        cum_value = tf.zeros(shape=[FLAGS.batch_size])\n        for s in xrange(t, FLAGS.sequence_length):\n            cum_value += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n        cumulative_rewards.append(cum_value)\n    cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n    if FLAGS.baseline_method == 'critic':\n        critic_loss = create_critic_loss(cumulative_rewards, estimated_values, present)\n        baselines = tf.unstack(estimated_values, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cum_advantage -= baselines[t]\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'dis_batch':\n        [rewards_half, baseline_half] = tf.split(rewards, num_or_size_splits=2, axis=0)\n        [log_probs_half, _] = tf.split(log_probs, num_or_size_splits=2, axis=0)\n        [reward_present_half, baseline_present_half] = tf.split(present, num_or_size_splits=2, axis=0)\n        baseline_list = tf.unstack(baseline_half, axis=1)\n        baseline_missing = 1.0 - tf.cast(baseline_present_half, tf.float32)\n        baseline_missing_list = tf.unstack(baseline_missing, axis=1)\n        baselines = []\n        for t in xrange(FLAGS.sequence_length):\n            num_missing = tf.reduce_sum(baseline_missing_list[t])\n            avg_baseline = tf.reduce_sum(baseline_missing_list[t] * baseline_list[t], keep_dims=True) / (num_missing + eps)\n            baseline = tf.tile(avg_baseline, multiples=[FLAGS.batch_size / 2])\n            baselines.append(baseline)\n        rewards_list = tf.unstack(rewards_half, axis=1)\n        log_probs_list = tf.unstack(log_probs_half, axis=1)\n        reward_missing = 1.0 - tf.cast(reward_present_half, tf.float32)\n        reward_missing_list = tf.unstack(reward_missing, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += reward_missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(reward_missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, reward_missing_list[t] * tf.stop_gradient(cum_advantage))\n        cumulative_rewards = []\n        for t in xrange(FLAGS.sequence_length):\n            cum_value = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_value += reward_missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cumulative_rewards.append(cum_value)\n        cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n        rewards = rewards_half\n        critic_loss = None\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'ema':\n        ema = tf.train.ExponentialMovingAverage(decay=hparams.baseline_decay)\n        maintain_averages_op = ema.apply(rewards_list)\n        baselines = []\n        for r in rewards_list:\n            baselines.append(ema.average(r))\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        critic_loss = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method is None:\n        num_missing = tf.reduce_sum(missing)\n        final_gen_objective += tf.reduce_sum(rewards) / (num_missing + eps)\n        baselines = tf.zeros_like(rewards)\n        critic_loss = None\n        maintain_averages_op = None\n        advantages = cumulative_rewards\n    else:\n        raise NotImplementedError\n    return [final_gen_objective, log_probs, rewards, advantages, baselines, maintain_averages_op, critic_loss, cumulative_rewards]",
            "def calculate_reinforce_objective(hparams, log_probs, dis_predictions, present, estimated_values=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate the REINFORCE objectives.  The REINFORCE objective should\\n  only be on the tokens that were missing.  Specifically, the final Generator\\n  reward should be based on the Discriminator predictions on missing tokens.\\n  The log probaibilities should be only for missing tokens and the baseline\\n  should be calculated only on the missing tokens.\\n\\n  For this model, we optimize the reward is the log of the *conditional*\\n  probability the Discriminator assigns to the distribution.  Specifically, for\\n  a Discriminator D which outputs probability of real, given the past context,\\n\\n    r_t = log D(x_t|x_0,x_1,...x_{t-1})\\n\\n  And the policy for Generator G is the log-probability of taking action x2\\n  given the past context.\\n\\n\\n  Args:\\n    hparams:  MaskGAN hyperparameters.\\n    log_probs:  tf.float32 Tensor of log probailities of the tokens selected by\\n      the Generator.  Shape [batch_size, sequence_length].\\n    dis_predictions:  tf.float32 Tensor of the predictions from the\\n      Discriminator.  Shape [batch_size, sequence_length].\\n    present:  tf.bool Tensor indicating which tokens are present.  Shape\\n      [batch_size, sequence_length].\\n    estimated_values:  tf.float32 Tensor of estimated state values of tokens.\\n      Shape [batch_size, sequence_length]\\n\\n  Returns:\\n    final_gen_objective:  Final REINFORCE objective for the sequence.\\n    rewards:  tf.float32 Tensor of rewards for sequence of shape [batch_size,\\n      sequence_length]\\n    advantages: tf.float32 Tensor of advantages for sequence of shape\\n      [batch_size, sequence_length]\\n    baselines:  tf.float32 Tensor of baselines for sequence of shape\\n      [batch_size, sequence_length]\\n    maintain_averages_op:  ExponentialMovingAverage apply average op to\\n      maintain the baseline.\\n  '\n    final_gen_objective = 0.0\n    gamma = hparams.rl_discount_rate\n    eps = 1e-07\n    eps = tf.constant(1e-07, tf.float32)\n    dis_predictions = tf.nn.sigmoid(dis_predictions)\n    rewards = tf.log(dis_predictions + eps)\n    zeros = tf.zeros_like(present, dtype=tf.float32)\n    log_probs = tf.where(present, zeros, log_probs)\n    rewards = tf.where(present, zeros, rewards)\n    rewards_list = tf.unstack(rewards, axis=1)\n    log_probs_list = tf.unstack(log_probs, axis=1)\n    missing = 1.0 - tf.cast(present, tf.float32)\n    missing_list = tf.unstack(missing, axis=1)\n    cumulative_rewards = []\n    for t in xrange(FLAGS.sequence_length):\n        cum_value = tf.zeros(shape=[FLAGS.batch_size])\n        for s in xrange(t, FLAGS.sequence_length):\n            cum_value += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n        cumulative_rewards.append(cum_value)\n    cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n    if FLAGS.baseline_method == 'critic':\n        critic_loss = create_critic_loss(cumulative_rewards, estimated_values, present)\n        baselines = tf.unstack(estimated_values, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cum_advantage -= baselines[t]\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'dis_batch':\n        [rewards_half, baseline_half] = tf.split(rewards, num_or_size_splits=2, axis=0)\n        [log_probs_half, _] = tf.split(log_probs, num_or_size_splits=2, axis=0)\n        [reward_present_half, baseline_present_half] = tf.split(present, num_or_size_splits=2, axis=0)\n        baseline_list = tf.unstack(baseline_half, axis=1)\n        baseline_missing = 1.0 - tf.cast(baseline_present_half, tf.float32)\n        baseline_missing_list = tf.unstack(baseline_missing, axis=1)\n        baselines = []\n        for t in xrange(FLAGS.sequence_length):\n            num_missing = tf.reduce_sum(baseline_missing_list[t])\n            avg_baseline = tf.reduce_sum(baseline_missing_list[t] * baseline_list[t], keep_dims=True) / (num_missing + eps)\n            baseline = tf.tile(avg_baseline, multiples=[FLAGS.batch_size / 2])\n            baselines.append(baseline)\n        rewards_list = tf.unstack(rewards_half, axis=1)\n        log_probs_list = tf.unstack(log_probs_half, axis=1)\n        reward_missing = 1.0 - tf.cast(reward_present_half, tf.float32)\n        reward_missing_list = tf.unstack(reward_missing, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += reward_missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(reward_missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, reward_missing_list[t] * tf.stop_gradient(cum_advantage))\n        cumulative_rewards = []\n        for t in xrange(FLAGS.sequence_length):\n            cum_value = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_value += reward_missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cumulative_rewards.append(cum_value)\n        cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n        rewards = rewards_half\n        critic_loss = None\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'ema':\n        ema = tf.train.ExponentialMovingAverage(decay=hparams.baseline_decay)\n        maintain_averages_op = ema.apply(rewards_list)\n        baselines = []\n        for r in rewards_list:\n            baselines.append(ema.average(r))\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        critic_loss = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method is None:\n        num_missing = tf.reduce_sum(missing)\n        final_gen_objective += tf.reduce_sum(rewards) / (num_missing + eps)\n        baselines = tf.zeros_like(rewards)\n        critic_loss = None\n        maintain_averages_op = None\n        advantages = cumulative_rewards\n    else:\n        raise NotImplementedError\n    return [final_gen_objective, log_probs, rewards, advantages, baselines, maintain_averages_op, critic_loss, cumulative_rewards]",
            "def calculate_reinforce_objective(hparams, log_probs, dis_predictions, present, estimated_values=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate the REINFORCE objectives.  The REINFORCE objective should\\n  only be on the tokens that were missing.  Specifically, the final Generator\\n  reward should be based on the Discriminator predictions on missing tokens.\\n  The log probaibilities should be only for missing tokens and the baseline\\n  should be calculated only on the missing tokens.\\n\\n  For this model, we optimize the reward is the log of the *conditional*\\n  probability the Discriminator assigns to the distribution.  Specifically, for\\n  a Discriminator D which outputs probability of real, given the past context,\\n\\n    r_t = log D(x_t|x_0,x_1,...x_{t-1})\\n\\n  And the policy for Generator G is the log-probability of taking action x2\\n  given the past context.\\n\\n\\n  Args:\\n    hparams:  MaskGAN hyperparameters.\\n    log_probs:  tf.float32 Tensor of log probailities of the tokens selected by\\n      the Generator.  Shape [batch_size, sequence_length].\\n    dis_predictions:  tf.float32 Tensor of the predictions from the\\n      Discriminator.  Shape [batch_size, sequence_length].\\n    present:  tf.bool Tensor indicating which tokens are present.  Shape\\n      [batch_size, sequence_length].\\n    estimated_values:  tf.float32 Tensor of estimated state values of tokens.\\n      Shape [batch_size, sequence_length]\\n\\n  Returns:\\n    final_gen_objective:  Final REINFORCE objective for the sequence.\\n    rewards:  tf.float32 Tensor of rewards for sequence of shape [batch_size,\\n      sequence_length]\\n    advantages: tf.float32 Tensor of advantages for sequence of shape\\n      [batch_size, sequence_length]\\n    baselines:  tf.float32 Tensor of baselines for sequence of shape\\n      [batch_size, sequence_length]\\n    maintain_averages_op:  ExponentialMovingAverage apply average op to\\n      maintain the baseline.\\n  '\n    final_gen_objective = 0.0\n    gamma = hparams.rl_discount_rate\n    eps = 1e-07\n    eps = tf.constant(1e-07, tf.float32)\n    dis_predictions = tf.nn.sigmoid(dis_predictions)\n    rewards = tf.log(dis_predictions + eps)\n    zeros = tf.zeros_like(present, dtype=tf.float32)\n    log_probs = tf.where(present, zeros, log_probs)\n    rewards = tf.where(present, zeros, rewards)\n    rewards_list = tf.unstack(rewards, axis=1)\n    log_probs_list = tf.unstack(log_probs, axis=1)\n    missing = 1.0 - tf.cast(present, tf.float32)\n    missing_list = tf.unstack(missing, axis=1)\n    cumulative_rewards = []\n    for t in xrange(FLAGS.sequence_length):\n        cum_value = tf.zeros(shape=[FLAGS.batch_size])\n        for s in xrange(t, FLAGS.sequence_length):\n            cum_value += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n        cumulative_rewards.append(cum_value)\n    cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n    if FLAGS.baseline_method == 'critic':\n        critic_loss = create_critic_loss(cumulative_rewards, estimated_values, present)\n        baselines = tf.unstack(estimated_values, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cum_advantage -= baselines[t]\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'dis_batch':\n        [rewards_half, baseline_half] = tf.split(rewards, num_or_size_splits=2, axis=0)\n        [log_probs_half, _] = tf.split(log_probs, num_or_size_splits=2, axis=0)\n        [reward_present_half, baseline_present_half] = tf.split(present, num_or_size_splits=2, axis=0)\n        baseline_list = tf.unstack(baseline_half, axis=1)\n        baseline_missing = 1.0 - tf.cast(baseline_present_half, tf.float32)\n        baseline_missing_list = tf.unstack(baseline_missing, axis=1)\n        baselines = []\n        for t in xrange(FLAGS.sequence_length):\n            num_missing = tf.reduce_sum(baseline_missing_list[t])\n            avg_baseline = tf.reduce_sum(baseline_missing_list[t] * baseline_list[t], keep_dims=True) / (num_missing + eps)\n            baseline = tf.tile(avg_baseline, multiples=[FLAGS.batch_size / 2])\n            baselines.append(baseline)\n        rewards_list = tf.unstack(rewards_half, axis=1)\n        log_probs_list = tf.unstack(log_probs_half, axis=1)\n        reward_missing = 1.0 - tf.cast(reward_present_half, tf.float32)\n        reward_missing_list = tf.unstack(reward_missing, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += reward_missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(reward_missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, reward_missing_list[t] * tf.stop_gradient(cum_advantage))\n        cumulative_rewards = []\n        for t in xrange(FLAGS.sequence_length):\n            cum_value = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_value += reward_missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cumulative_rewards.append(cum_value)\n        cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n        rewards = rewards_half\n        critic_loss = None\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'ema':\n        ema = tf.train.ExponentialMovingAverage(decay=hparams.baseline_decay)\n        maintain_averages_op = ema.apply(rewards_list)\n        baselines = []\n        for r in rewards_list:\n            baselines.append(ema.average(r))\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        critic_loss = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method is None:\n        num_missing = tf.reduce_sum(missing)\n        final_gen_objective += tf.reduce_sum(rewards) / (num_missing + eps)\n        baselines = tf.zeros_like(rewards)\n        critic_loss = None\n        maintain_averages_op = None\n        advantages = cumulative_rewards\n    else:\n        raise NotImplementedError\n    return [final_gen_objective, log_probs, rewards, advantages, baselines, maintain_averages_op, critic_loss, cumulative_rewards]",
            "def calculate_reinforce_objective(hparams, log_probs, dis_predictions, present, estimated_values=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate the REINFORCE objectives.  The REINFORCE objective should\\n  only be on the tokens that were missing.  Specifically, the final Generator\\n  reward should be based on the Discriminator predictions on missing tokens.\\n  The log probaibilities should be only for missing tokens and the baseline\\n  should be calculated only on the missing tokens.\\n\\n  For this model, we optimize the reward is the log of the *conditional*\\n  probability the Discriminator assigns to the distribution.  Specifically, for\\n  a Discriminator D which outputs probability of real, given the past context,\\n\\n    r_t = log D(x_t|x_0,x_1,...x_{t-1})\\n\\n  And the policy for Generator G is the log-probability of taking action x2\\n  given the past context.\\n\\n\\n  Args:\\n    hparams:  MaskGAN hyperparameters.\\n    log_probs:  tf.float32 Tensor of log probailities of the tokens selected by\\n      the Generator.  Shape [batch_size, sequence_length].\\n    dis_predictions:  tf.float32 Tensor of the predictions from the\\n      Discriminator.  Shape [batch_size, sequence_length].\\n    present:  tf.bool Tensor indicating which tokens are present.  Shape\\n      [batch_size, sequence_length].\\n    estimated_values:  tf.float32 Tensor of estimated state values of tokens.\\n      Shape [batch_size, sequence_length]\\n\\n  Returns:\\n    final_gen_objective:  Final REINFORCE objective for the sequence.\\n    rewards:  tf.float32 Tensor of rewards for sequence of shape [batch_size,\\n      sequence_length]\\n    advantages: tf.float32 Tensor of advantages for sequence of shape\\n      [batch_size, sequence_length]\\n    baselines:  tf.float32 Tensor of baselines for sequence of shape\\n      [batch_size, sequence_length]\\n    maintain_averages_op:  ExponentialMovingAverage apply average op to\\n      maintain the baseline.\\n  '\n    final_gen_objective = 0.0\n    gamma = hparams.rl_discount_rate\n    eps = 1e-07\n    eps = tf.constant(1e-07, tf.float32)\n    dis_predictions = tf.nn.sigmoid(dis_predictions)\n    rewards = tf.log(dis_predictions + eps)\n    zeros = tf.zeros_like(present, dtype=tf.float32)\n    log_probs = tf.where(present, zeros, log_probs)\n    rewards = tf.where(present, zeros, rewards)\n    rewards_list = tf.unstack(rewards, axis=1)\n    log_probs_list = tf.unstack(log_probs, axis=1)\n    missing = 1.0 - tf.cast(present, tf.float32)\n    missing_list = tf.unstack(missing, axis=1)\n    cumulative_rewards = []\n    for t in xrange(FLAGS.sequence_length):\n        cum_value = tf.zeros(shape=[FLAGS.batch_size])\n        for s in xrange(t, FLAGS.sequence_length):\n            cum_value += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n        cumulative_rewards.append(cum_value)\n    cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n    if FLAGS.baseline_method == 'critic':\n        critic_loss = create_critic_loss(cumulative_rewards, estimated_values, present)\n        baselines = tf.unstack(estimated_values, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cum_advantage -= baselines[t]\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'dis_batch':\n        [rewards_half, baseline_half] = tf.split(rewards, num_or_size_splits=2, axis=0)\n        [log_probs_half, _] = tf.split(log_probs, num_or_size_splits=2, axis=0)\n        [reward_present_half, baseline_present_half] = tf.split(present, num_or_size_splits=2, axis=0)\n        baseline_list = tf.unstack(baseline_half, axis=1)\n        baseline_missing = 1.0 - tf.cast(baseline_present_half, tf.float32)\n        baseline_missing_list = tf.unstack(baseline_missing, axis=1)\n        baselines = []\n        for t in xrange(FLAGS.sequence_length):\n            num_missing = tf.reduce_sum(baseline_missing_list[t])\n            avg_baseline = tf.reduce_sum(baseline_missing_list[t] * baseline_list[t], keep_dims=True) / (num_missing + eps)\n            baseline = tf.tile(avg_baseline, multiples=[FLAGS.batch_size / 2])\n            baselines.append(baseline)\n        rewards_list = tf.unstack(rewards_half, axis=1)\n        log_probs_list = tf.unstack(log_probs_half, axis=1)\n        reward_missing = 1.0 - tf.cast(reward_present_half, tf.float32)\n        reward_missing_list = tf.unstack(reward_missing, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += reward_missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(reward_missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, reward_missing_list[t] * tf.stop_gradient(cum_advantage))\n        cumulative_rewards = []\n        for t in xrange(FLAGS.sequence_length):\n            cum_value = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_value += reward_missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cumulative_rewards.append(cum_value)\n        cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n        rewards = rewards_half\n        critic_loss = None\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'ema':\n        ema = tf.train.ExponentialMovingAverage(decay=hparams.baseline_decay)\n        maintain_averages_op = ema.apply(rewards_list)\n        baselines = []\n        for r in rewards_list:\n            baselines.append(ema.average(r))\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        critic_loss = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method is None:\n        num_missing = tf.reduce_sum(missing)\n        final_gen_objective += tf.reduce_sum(rewards) / (num_missing + eps)\n        baselines = tf.zeros_like(rewards)\n        critic_loss = None\n        maintain_averages_op = None\n        advantages = cumulative_rewards\n    else:\n        raise NotImplementedError\n    return [final_gen_objective, log_probs, rewards, advantages, baselines, maintain_averages_op, critic_loss, cumulative_rewards]",
            "def calculate_reinforce_objective(hparams, log_probs, dis_predictions, present, estimated_values=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate the REINFORCE objectives.  The REINFORCE objective should\\n  only be on the tokens that were missing.  Specifically, the final Generator\\n  reward should be based on the Discriminator predictions on missing tokens.\\n  The log probaibilities should be only for missing tokens and the baseline\\n  should be calculated only on the missing tokens.\\n\\n  For this model, we optimize the reward is the log of the *conditional*\\n  probability the Discriminator assigns to the distribution.  Specifically, for\\n  a Discriminator D which outputs probability of real, given the past context,\\n\\n    r_t = log D(x_t|x_0,x_1,...x_{t-1})\\n\\n  And the policy for Generator G is the log-probability of taking action x2\\n  given the past context.\\n\\n\\n  Args:\\n    hparams:  MaskGAN hyperparameters.\\n    log_probs:  tf.float32 Tensor of log probailities of the tokens selected by\\n      the Generator.  Shape [batch_size, sequence_length].\\n    dis_predictions:  tf.float32 Tensor of the predictions from the\\n      Discriminator.  Shape [batch_size, sequence_length].\\n    present:  tf.bool Tensor indicating which tokens are present.  Shape\\n      [batch_size, sequence_length].\\n    estimated_values:  tf.float32 Tensor of estimated state values of tokens.\\n      Shape [batch_size, sequence_length]\\n\\n  Returns:\\n    final_gen_objective:  Final REINFORCE objective for the sequence.\\n    rewards:  tf.float32 Tensor of rewards for sequence of shape [batch_size,\\n      sequence_length]\\n    advantages: tf.float32 Tensor of advantages for sequence of shape\\n      [batch_size, sequence_length]\\n    baselines:  tf.float32 Tensor of baselines for sequence of shape\\n      [batch_size, sequence_length]\\n    maintain_averages_op:  ExponentialMovingAverage apply average op to\\n      maintain the baseline.\\n  '\n    final_gen_objective = 0.0\n    gamma = hparams.rl_discount_rate\n    eps = 1e-07\n    eps = tf.constant(1e-07, tf.float32)\n    dis_predictions = tf.nn.sigmoid(dis_predictions)\n    rewards = tf.log(dis_predictions + eps)\n    zeros = tf.zeros_like(present, dtype=tf.float32)\n    log_probs = tf.where(present, zeros, log_probs)\n    rewards = tf.where(present, zeros, rewards)\n    rewards_list = tf.unstack(rewards, axis=1)\n    log_probs_list = tf.unstack(log_probs, axis=1)\n    missing = 1.0 - tf.cast(present, tf.float32)\n    missing_list = tf.unstack(missing, axis=1)\n    cumulative_rewards = []\n    for t in xrange(FLAGS.sequence_length):\n        cum_value = tf.zeros(shape=[FLAGS.batch_size])\n        for s in xrange(t, FLAGS.sequence_length):\n            cum_value += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n        cumulative_rewards.append(cum_value)\n    cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n    if FLAGS.baseline_method == 'critic':\n        critic_loss = create_critic_loss(cumulative_rewards, estimated_values, present)\n        baselines = tf.unstack(estimated_values, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cum_advantage -= baselines[t]\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'dis_batch':\n        [rewards_half, baseline_half] = tf.split(rewards, num_or_size_splits=2, axis=0)\n        [log_probs_half, _] = tf.split(log_probs, num_or_size_splits=2, axis=0)\n        [reward_present_half, baseline_present_half] = tf.split(present, num_or_size_splits=2, axis=0)\n        baseline_list = tf.unstack(baseline_half, axis=1)\n        baseline_missing = 1.0 - tf.cast(baseline_present_half, tf.float32)\n        baseline_missing_list = tf.unstack(baseline_missing, axis=1)\n        baselines = []\n        for t in xrange(FLAGS.sequence_length):\n            num_missing = tf.reduce_sum(baseline_missing_list[t])\n            avg_baseline = tf.reduce_sum(baseline_missing_list[t] * baseline_list[t], keep_dims=True) / (num_missing + eps)\n            baseline = tf.tile(avg_baseline, multiples=[FLAGS.batch_size / 2])\n            baselines.append(baseline)\n        rewards_list = tf.unstack(rewards_half, axis=1)\n        log_probs_list = tf.unstack(log_probs_half, axis=1)\n        reward_missing = 1.0 - tf.cast(reward_present_half, tf.float32)\n        reward_missing_list = tf.unstack(reward_missing, axis=1)\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += reward_missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(reward_missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, reward_missing_list[t] * tf.stop_gradient(cum_advantage))\n        cumulative_rewards = []\n        for t in xrange(FLAGS.sequence_length):\n            cum_value = tf.zeros(shape=[FLAGS.batch_size / 2])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_value += reward_missing_list[s] * np.power(gamma, s - t) * rewards_list[s]\n            cumulative_rewards.append(cum_value)\n        cumulative_rewards = tf.stack(cumulative_rewards, axis=1)\n        rewards = rewards_half\n        critic_loss = None\n        maintain_averages_op = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method == 'ema':\n        ema = tf.train.ExponentialMovingAverage(decay=hparams.baseline_decay)\n        maintain_averages_op = ema.apply(rewards_list)\n        baselines = []\n        for r in rewards_list:\n            baselines.append(ema.average(r))\n        advantages = []\n        for t in xrange(FLAGS.sequence_length):\n            log_probability = log_probs_list[t]\n            cum_advantage = tf.zeros(shape=[FLAGS.batch_size])\n            for s in xrange(t, FLAGS.sequence_length):\n                cum_advantage += missing_list[s] * np.power(gamma, s - t) * (rewards_list[s] - baselines[s])\n            cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping, FLAGS.advantage_clipping)\n            advantages.append(missing_list[t] * cum_advantage)\n            final_gen_objective += tf.multiply(log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))\n        critic_loss = None\n        baselines = tf.stack(baselines, axis=1)\n        advantages = tf.stack(advantages, axis=1)\n    elif FLAGS.baseline_method is None:\n        num_missing = tf.reduce_sum(missing)\n        final_gen_objective += tf.reduce_sum(rewards) / (num_missing + eps)\n        baselines = tf.zeros_like(rewards)\n        critic_loss = None\n        maintain_averages_op = None\n        advantages = cumulative_rewards\n    else:\n        raise NotImplementedError\n    return [final_gen_objective, log_probs, rewards, advantages, baselines, maintain_averages_op, critic_loss, cumulative_rewards]"
        ]
    },
    {
        "func_name": "calculate_log_perplexity",
        "original": "def calculate_log_perplexity(logits, targets, present):\n    \"\"\"Calculate the average log perplexity per *missing* token.\n\n  Args:\n    logits:  tf.float32 Tensor of the logits of shape [batch_size,\n      sequence_length, vocab_size].\n    targets:  tf.int32 Tensor of the sequence target of shape [batch_size,\n      sequence_length].\n    present:  tf.bool Tensor indicating the presence or absence of the token\n      of shape [batch_size, sequence_length].\n\n  Returns:\n    avg_log_perplexity:  Scalar indicating the average log perplexity per\n      missing token in the batch.\n  \"\"\"\n    eps = 1e-12\n    logits = tf.reshape(logits, [-1, FLAGS.vocab_size])\n    weights = tf.cast(present, tf.float32)\n    weights = 1.0 - weights\n    weights = tf.reshape(weights, [-1])\n    num_missing = tf.reduce_sum(weights)\n    log_perplexity = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(targets, [-1])], [weights])\n    avg_log_perplexity = tf.reduce_sum(log_perplexity) / (num_missing + eps)\n    return avg_log_perplexity",
        "mutated": [
            "def calculate_log_perplexity(logits, targets, present):\n    if False:\n        i = 10\n    'Calculate the average log perplexity per *missing* token.\\n\\n  Args:\\n    logits:  tf.float32 Tensor of the logits of shape [batch_size,\\n      sequence_length, vocab_size].\\n    targets:  tf.int32 Tensor of the sequence target of shape [batch_size,\\n      sequence_length].\\n    present:  tf.bool Tensor indicating the presence or absence of the token\\n      of shape [batch_size, sequence_length].\\n\\n  Returns:\\n    avg_log_perplexity:  Scalar indicating the average log perplexity per\\n      missing token in the batch.\\n  '\n    eps = 1e-12\n    logits = tf.reshape(logits, [-1, FLAGS.vocab_size])\n    weights = tf.cast(present, tf.float32)\n    weights = 1.0 - weights\n    weights = tf.reshape(weights, [-1])\n    num_missing = tf.reduce_sum(weights)\n    log_perplexity = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(targets, [-1])], [weights])\n    avg_log_perplexity = tf.reduce_sum(log_perplexity) / (num_missing + eps)\n    return avg_log_perplexity",
            "def calculate_log_perplexity(logits, targets, present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate the average log perplexity per *missing* token.\\n\\n  Args:\\n    logits:  tf.float32 Tensor of the logits of shape [batch_size,\\n      sequence_length, vocab_size].\\n    targets:  tf.int32 Tensor of the sequence target of shape [batch_size,\\n      sequence_length].\\n    present:  tf.bool Tensor indicating the presence or absence of the token\\n      of shape [batch_size, sequence_length].\\n\\n  Returns:\\n    avg_log_perplexity:  Scalar indicating the average log perplexity per\\n      missing token in the batch.\\n  '\n    eps = 1e-12\n    logits = tf.reshape(logits, [-1, FLAGS.vocab_size])\n    weights = tf.cast(present, tf.float32)\n    weights = 1.0 - weights\n    weights = tf.reshape(weights, [-1])\n    num_missing = tf.reduce_sum(weights)\n    log_perplexity = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(targets, [-1])], [weights])\n    avg_log_perplexity = tf.reduce_sum(log_perplexity) / (num_missing + eps)\n    return avg_log_perplexity",
            "def calculate_log_perplexity(logits, targets, present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate the average log perplexity per *missing* token.\\n\\n  Args:\\n    logits:  tf.float32 Tensor of the logits of shape [batch_size,\\n      sequence_length, vocab_size].\\n    targets:  tf.int32 Tensor of the sequence target of shape [batch_size,\\n      sequence_length].\\n    present:  tf.bool Tensor indicating the presence or absence of the token\\n      of shape [batch_size, sequence_length].\\n\\n  Returns:\\n    avg_log_perplexity:  Scalar indicating the average log perplexity per\\n      missing token in the batch.\\n  '\n    eps = 1e-12\n    logits = tf.reshape(logits, [-1, FLAGS.vocab_size])\n    weights = tf.cast(present, tf.float32)\n    weights = 1.0 - weights\n    weights = tf.reshape(weights, [-1])\n    num_missing = tf.reduce_sum(weights)\n    log_perplexity = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(targets, [-1])], [weights])\n    avg_log_perplexity = tf.reduce_sum(log_perplexity) / (num_missing + eps)\n    return avg_log_perplexity",
            "def calculate_log_perplexity(logits, targets, present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate the average log perplexity per *missing* token.\\n\\n  Args:\\n    logits:  tf.float32 Tensor of the logits of shape [batch_size,\\n      sequence_length, vocab_size].\\n    targets:  tf.int32 Tensor of the sequence target of shape [batch_size,\\n      sequence_length].\\n    present:  tf.bool Tensor indicating the presence or absence of the token\\n      of shape [batch_size, sequence_length].\\n\\n  Returns:\\n    avg_log_perplexity:  Scalar indicating the average log perplexity per\\n      missing token in the batch.\\n  '\n    eps = 1e-12\n    logits = tf.reshape(logits, [-1, FLAGS.vocab_size])\n    weights = tf.cast(present, tf.float32)\n    weights = 1.0 - weights\n    weights = tf.reshape(weights, [-1])\n    num_missing = tf.reduce_sum(weights)\n    log_perplexity = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(targets, [-1])], [weights])\n    avg_log_perplexity = tf.reduce_sum(log_perplexity) / (num_missing + eps)\n    return avg_log_perplexity",
            "def calculate_log_perplexity(logits, targets, present):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate the average log perplexity per *missing* token.\\n\\n  Args:\\n    logits:  tf.float32 Tensor of the logits of shape [batch_size,\\n      sequence_length, vocab_size].\\n    targets:  tf.int32 Tensor of the sequence target of shape [batch_size,\\n      sequence_length].\\n    present:  tf.bool Tensor indicating the presence or absence of the token\\n      of shape [batch_size, sequence_length].\\n\\n  Returns:\\n    avg_log_perplexity:  Scalar indicating the average log perplexity per\\n      missing token in the batch.\\n  '\n    eps = 1e-12\n    logits = tf.reshape(logits, [-1, FLAGS.vocab_size])\n    weights = tf.cast(present, tf.float32)\n    weights = 1.0 - weights\n    weights = tf.reshape(weights, [-1])\n    num_missing = tf.reduce_sum(weights)\n    log_perplexity = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(targets, [-1])], [weights])\n    avg_log_perplexity = tf.reduce_sum(log_perplexity) / (num_missing + eps)\n    return avg_log_perplexity"
        ]
    }
]