[
    {
        "func_name": "__init__",
        "original": "def __init__(self, learning_rate, momentum, lars_coeff=0.001, lars_weight_decay=0.0005, parameter_list=None, regularization=None, grad_clip=None, name=None, exclude_from_weight_decay=None, epsilon=0, multi_precision=False, rescale_grad=1.0):\n    assert learning_rate is not None\n    assert momentum is not None\n    super().__init__(learning_rate=learning_rate, parameters=parameter_list, weight_decay=regularization, grad_clip=grad_clip, name=name)\n    self.type = 'lars_momentum'\n    self._momentum = momentum\n    self._lars_coeff = float(lars_coeff)\n    self._lars_weight_decay = float(lars_weight_decay)\n    self._epsilon = float(epsilon)\n    if exclude_from_weight_decay is None:\n        self._exclude_from_weight_decay = []\n    else:\n        self._exclude_from_weight_decay = exclude_from_weight_decay\n    self._multi_precision = multi_precision\n    self._rescale_grad = float(rescale_grad)\n    self._master_weights = {}",
        "mutated": [
            "def __init__(self, learning_rate, momentum, lars_coeff=0.001, lars_weight_decay=0.0005, parameter_list=None, regularization=None, grad_clip=None, name=None, exclude_from_weight_decay=None, epsilon=0, multi_precision=False, rescale_grad=1.0):\n    if False:\n        i = 10\n    assert learning_rate is not None\n    assert momentum is not None\n    super().__init__(learning_rate=learning_rate, parameters=parameter_list, weight_decay=regularization, grad_clip=grad_clip, name=name)\n    self.type = 'lars_momentum'\n    self._momentum = momentum\n    self._lars_coeff = float(lars_coeff)\n    self._lars_weight_decay = float(lars_weight_decay)\n    self._epsilon = float(epsilon)\n    if exclude_from_weight_decay is None:\n        self._exclude_from_weight_decay = []\n    else:\n        self._exclude_from_weight_decay = exclude_from_weight_decay\n    self._multi_precision = multi_precision\n    self._rescale_grad = float(rescale_grad)\n    self._master_weights = {}",
            "def __init__(self, learning_rate, momentum, lars_coeff=0.001, lars_weight_decay=0.0005, parameter_list=None, regularization=None, grad_clip=None, name=None, exclude_from_weight_decay=None, epsilon=0, multi_precision=False, rescale_grad=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert learning_rate is not None\n    assert momentum is not None\n    super().__init__(learning_rate=learning_rate, parameters=parameter_list, weight_decay=regularization, grad_clip=grad_clip, name=name)\n    self.type = 'lars_momentum'\n    self._momentum = momentum\n    self._lars_coeff = float(lars_coeff)\n    self._lars_weight_decay = float(lars_weight_decay)\n    self._epsilon = float(epsilon)\n    if exclude_from_weight_decay is None:\n        self._exclude_from_weight_decay = []\n    else:\n        self._exclude_from_weight_decay = exclude_from_weight_decay\n    self._multi_precision = multi_precision\n    self._rescale_grad = float(rescale_grad)\n    self._master_weights = {}",
            "def __init__(self, learning_rate, momentum, lars_coeff=0.001, lars_weight_decay=0.0005, parameter_list=None, regularization=None, grad_clip=None, name=None, exclude_from_weight_decay=None, epsilon=0, multi_precision=False, rescale_grad=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert learning_rate is not None\n    assert momentum is not None\n    super().__init__(learning_rate=learning_rate, parameters=parameter_list, weight_decay=regularization, grad_clip=grad_clip, name=name)\n    self.type = 'lars_momentum'\n    self._momentum = momentum\n    self._lars_coeff = float(lars_coeff)\n    self._lars_weight_decay = float(lars_weight_decay)\n    self._epsilon = float(epsilon)\n    if exclude_from_weight_decay is None:\n        self._exclude_from_weight_decay = []\n    else:\n        self._exclude_from_weight_decay = exclude_from_weight_decay\n    self._multi_precision = multi_precision\n    self._rescale_grad = float(rescale_grad)\n    self._master_weights = {}",
            "def __init__(self, learning_rate, momentum, lars_coeff=0.001, lars_weight_decay=0.0005, parameter_list=None, regularization=None, grad_clip=None, name=None, exclude_from_weight_decay=None, epsilon=0, multi_precision=False, rescale_grad=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert learning_rate is not None\n    assert momentum is not None\n    super().__init__(learning_rate=learning_rate, parameters=parameter_list, weight_decay=regularization, grad_clip=grad_clip, name=name)\n    self.type = 'lars_momentum'\n    self._momentum = momentum\n    self._lars_coeff = float(lars_coeff)\n    self._lars_weight_decay = float(lars_weight_decay)\n    self._epsilon = float(epsilon)\n    if exclude_from_weight_decay is None:\n        self._exclude_from_weight_decay = []\n    else:\n        self._exclude_from_weight_decay = exclude_from_weight_decay\n    self._multi_precision = multi_precision\n    self._rescale_grad = float(rescale_grad)\n    self._master_weights = {}",
            "def __init__(self, learning_rate, momentum, lars_coeff=0.001, lars_weight_decay=0.0005, parameter_list=None, regularization=None, grad_clip=None, name=None, exclude_from_weight_decay=None, epsilon=0, multi_precision=False, rescale_grad=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert learning_rate is not None\n    assert momentum is not None\n    super().__init__(learning_rate=learning_rate, parameters=parameter_list, weight_decay=regularization, grad_clip=grad_clip, name=name)\n    self.type = 'lars_momentum'\n    self._momentum = momentum\n    self._lars_coeff = float(lars_coeff)\n    self._lars_weight_decay = float(lars_weight_decay)\n    self._epsilon = float(epsilon)\n    if exclude_from_weight_decay is None:\n        self._exclude_from_weight_decay = []\n    else:\n        self._exclude_from_weight_decay = exclude_from_weight_decay\n    self._multi_precision = multi_precision\n    self._rescale_grad = float(rescale_grad)\n    self._master_weights = {}"
        ]
    },
    {
        "func_name": "_create_accumulators",
        "original": "def _create_accumulators(self, block, parameters):\n    assert isinstance(block, framework.Block)\n    for p in parameters:\n        if self._multi_precision and self._is_dtype_fp16_or_bf16(p.dtype):\n            master_p = self._create_master_weight(p)\n            self._add_accumulator(self._velocity_acc_str, master_p)\n            continue\n        if self._is_dtype_fp16_or_bf16(p.dtype) and (not self._multi_precision):\n            warnings.warn('Accumulating with FP16/BF16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Lars optimizer.')\n        self._add_accumulator(self._velocity_acc_str, p)",
        "mutated": [
            "def _create_accumulators(self, block, parameters):\n    if False:\n        i = 10\n    assert isinstance(block, framework.Block)\n    for p in parameters:\n        if self._multi_precision and self._is_dtype_fp16_or_bf16(p.dtype):\n            master_p = self._create_master_weight(p)\n            self._add_accumulator(self._velocity_acc_str, master_p)\n            continue\n        if self._is_dtype_fp16_or_bf16(p.dtype) and (not self._multi_precision):\n            warnings.warn('Accumulating with FP16/BF16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Lars optimizer.')\n        self._add_accumulator(self._velocity_acc_str, p)",
            "def _create_accumulators(self, block, parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert isinstance(block, framework.Block)\n    for p in parameters:\n        if self._multi_precision and self._is_dtype_fp16_or_bf16(p.dtype):\n            master_p = self._create_master_weight(p)\n            self._add_accumulator(self._velocity_acc_str, master_p)\n            continue\n        if self._is_dtype_fp16_or_bf16(p.dtype) and (not self._multi_precision):\n            warnings.warn('Accumulating with FP16/BF16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Lars optimizer.')\n        self._add_accumulator(self._velocity_acc_str, p)",
            "def _create_accumulators(self, block, parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert isinstance(block, framework.Block)\n    for p in parameters:\n        if self._multi_precision and self._is_dtype_fp16_or_bf16(p.dtype):\n            master_p = self._create_master_weight(p)\n            self._add_accumulator(self._velocity_acc_str, master_p)\n            continue\n        if self._is_dtype_fp16_or_bf16(p.dtype) and (not self._multi_precision):\n            warnings.warn('Accumulating with FP16/BF16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Lars optimizer.')\n        self._add_accumulator(self._velocity_acc_str, p)",
            "def _create_accumulators(self, block, parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert isinstance(block, framework.Block)\n    for p in parameters:\n        if self._multi_precision and self._is_dtype_fp16_or_bf16(p.dtype):\n            master_p = self._create_master_weight(p)\n            self._add_accumulator(self._velocity_acc_str, master_p)\n            continue\n        if self._is_dtype_fp16_or_bf16(p.dtype) and (not self._multi_precision):\n            warnings.warn('Accumulating with FP16/BF16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Lars optimizer.')\n        self._add_accumulator(self._velocity_acc_str, p)",
            "def _create_accumulators(self, block, parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert isinstance(block, framework.Block)\n    for p in parameters:\n        if self._multi_precision and self._is_dtype_fp16_or_bf16(p.dtype):\n            master_p = self._create_master_weight(p)\n            self._add_accumulator(self._velocity_acc_str, master_p)\n            continue\n        if self._is_dtype_fp16_or_bf16(p.dtype) and (not self._multi_precision):\n            warnings.warn('Accumulating with FP16/BF16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Lars optimizer.')\n        self._add_accumulator(self._velocity_acc_str, p)"
        ]
    },
    {
        "func_name": "_append_optimize_op",
        "original": "def _append_optimize_op(self, block, param_and_grad):\n    assert isinstance(block, framework.Block)\n    _lars_weight_decay = self._lars_weight_decay\n    param_name = param_and_grad[0].name\n    if len(self._exclude_from_weight_decay) > 0:\n        for name in self._exclude_from_weight_decay:\n            if name in param_name:\n                _lars_weight_decay = 0.0\n                break\n    velocity_acc = self._get_accumulator_master(self._velocity_acc_str, param_and_grad[0])\n    lr = self._create_param_lr(param_and_grad)\n    find_master = self._multi_precision and self._is_dtype_fp16_or_bf16(param_and_grad[0].dtype)\n    master_weight = self._master_weights[param_and_grad[0].name] if find_master else None\n    attrs = {'mu': self._momentum, 'lars_coeff': self._lars_coeff, 'lars_weight_decay': [_lars_weight_decay], 'multi_precision': find_master, 'epsilon': self._epsilon, 'rescale_grad': self._rescale_grad}\n    inputs = {'Param': param_and_grad[0], 'Grad': param_and_grad[1], 'Velocity': velocity_acc, 'LearningRate': lr}\n    outputs = {'ParamOut': param_and_grad[0], 'VelocityOut': velocity_acc}\n    if find_master:\n        inputs['MasterParam'] = master_weight\n        outputs['MasterParamOut'] = master_weight\n    if in_dygraph_mode():\n        (tmp, tmp2) = _legacy_C_ops.lars_momentum([param_and_grad[0]], [param_and_grad[1]], [velocity_acc], [lr], [param_and_grad[0]], [velocity_acc], 'mu', self._momentum, 'lars_coeff', self._lars_coeff, 'lars_weight_decay', [_lars_weight_decay], 'multi_precision', find_master, 'epsilon', self._epsilon, 'rescale_grad', self._rescale_grad)\n    else:\n        momentum_op = block.append_op(type=self.type, inputs=inputs, outputs=outputs, attrs=attrs, stop_gradient=True)\n        return momentum_op",
        "mutated": [
            "def _append_optimize_op(self, block, param_and_grad):\n    if False:\n        i = 10\n    assert isinstance(block, framework.Block)\n    _lars_weight_decay = self._lars_weight_decay\n    param_name = param_and_grad[0].name\n    if len(self._exclude_from_weight_decay) > 0:\n        for name in self._exclude_from_weight_decay:\n            if name in param_name:\n                _lars_weight_decay = 0.0\n                break\n    velocity_acc = self._get_accumulator_master(self._velocity_acc_str, param_and_grad[0])\n    lr = self._create_param_lr(param_and_grad)\n    find_master = self._multi_precision and self._is_dtype_fp16_or_bf16(param_and_grad[0].dtype)\n    master_weight = self._master_weights[param_and_grad[0].name] if find_master else None\n    attrs = {'mu': self._momentum, 'lars_coeff': self._lars_coeff, 'lars_weight_decay': [_lars_weight_decay], 'multi_precision': find_master, 'epsilon': self._epsilon, 'rescale_grad': self._rescale_grad}\n    inputs = {'Param': param_and_grad[0], 'Grad': param_and_grad[1], 'Velocity': velocity_acc, 'LearningRate': lr}\n    outputs = {'ParamOut': param_and_grad[0], 'VelocityOut': velocity_acc}\n    if find_master:\n        inputs['MasterParam'] = master_weight\n        outputs['MasterParamOut'] = master_weight\n    if in_dygraph_mode():\n        (tmp, tmp2) = _legacy_C_ops.lars_momentum([param_and_grad[0]], [param_and_grad[1]], [velocity_acc], [lr], [param_and_grad[0]], [velocity_acc], 'mu', self._momentum, 'lars_coeff', self._lars_coeff, 'lars_weight_decay', [_lars_weight_decay], 'multi_precision', find_master, 'epsilon', self._epsilon, 'rescale_grad', self._rescale_grad)\n    else:\n        momentum_op = block.append_op(type=self.type, inputs=inputs, outputs=outputs, attrs=attrs, stop_gradient=True)\n        return momentum_op",
            "def _append_optimize_op(self, block, param_and_grad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert isinstance(block, framework.Block)\n    _lars_weight_decay = self._lars_weight_decay\n    param_name = param_and_grad[0].name\n    if len(self._exclude_from_weight_decay) > 0:\n        for name in self._exclude_from_weight_decay:\n            if name in param_name:\n                _lars_weight_decay = 0.0\n                break\n    velocity_acc = self._get_accumulator_master(self._velocity_acc_str, param_and_grad[0])\n    lr = self._create_param_lr(param_and_grad)\n    find_master = self._multi_precision and self._is_dtype_fp16_or_bf16(param_and_grad[0].dtype)\n    master_weight = self._master_weights[param_and_grad[0].name] if find_master else None\n    attrs = {'mu': self._momentum, 'lars_coeff': self._lars_coeff, 'lars_weight_decay': [_lars_weight_decay], 'multi_precision': find_master, 'epsilon': self._epsilon, 'rescale_grad': self._rescale_grad}\n    inputs = {'Param': param_and_grad[0], 'Grad': param_and_grad[1], 'Velocity': velocity_acc, 'LearningRate': lr}\n    outputs = {'ParamOut': param_and_grad[0], 'VelocityOut': velocity_acc}\n    if find_master:\n        inputs['MasterParam'] = master_weight\n        outputs['MasterParamOut'] = master_weight\n    if in_dygraph_mode():\n        (tmp, tmp2) = _legacy_C_ops.lars_momentum([param_and_grad[0]], [param_and_grad[1]], [velocity_acc], [lr], [param_and_grad[0]], [velocity_acc], 'mu', self._momentum, 'lars_coeff', self._lars_coeff, 'lars_weight_decay', [_lars_weight_decay], 'multi_precision', find_master, 'epsilon', self._epsilon, 'rescale_grad', self._rescale_grad)\n    else:\n        momentum_op = block.append_op(type=self.type, inputs=inputs, outputs=outputs, attrs=attrs, stop_gradient=True)\n        return momentum_op",
            "def _append_optimize_op(self, block, param_and_grad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert isinstance(block, framework.Block)\n    _lars_weight_decay = self._lars_weight_decay\n    param_name = param_and_grad[0].name\n    if len(self._exclude_from_weight_decay) > 0:\n        for name in self._exclude_from_weight_decay:\n            if name in param_name:\n                _lars_weight_decay = 0.0\n                break\n    velocity_acc = self._get_accumulator_master(self._velocity_acc_str, param_and_grad[0])\n    lr = self._create_param_lr(param_and_grad)\n    find_master = self._multi_precision and self._is_dtype_fp16_or_bf16(param_and_grad[0].dtype)\n    master_weight = self._master_weights[param_and_grad[0].name] if find_master else None\n    attrs = {'mu': self._momentum, 'lars_coeff': self._lars_coeff, 'lars_weight_decay': [_lars_weight_decay], 'multi_precision': find_master, 'epsilon': self._epsilon, 'rescale_grad': self._rescale_grad}\n    inputs = {'Param': param_and_grad[0], 'Grad': param_and_grad[1], 'Velocity': velocity_acc, 'LearningRate': lr}\n    outputs = {'ParamOut': param_and_grad[0], 'VelocityOut': velocity_acc}\n    if find_master:\n        inputs['MasterParam'] = master_weight\n        outputs['MasterParamOut'] = master_weight\n    if in_dygraph_mode():\n        (tmp, tmp2) = _legacy_C_ops.lars_momentum([param_and_grad[0]], [param_and_grad[1]], [velocity_acc], [lr], [param_and_grad[0]], [velocity_acc], 'mu', self._momentum, 'lars_coeff', self._lars_coeff, 'lars_weight_decay', [_lars_weight_decay], 'multi_precision', find_master, 'epsilon', self._epsilon, 'rescale_grad', self._rescale_grad)\n    else:\n        momentum_op = block.append_op(type=self.type, inputs=inputs, outputs=outputs, attrs=attrs, stop_gradient=True)\n        return momentum_op",
            "def _append_optimize_op(self, block, param_and_grad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert isinstance(block, framework.Block)\n    _lars_weight_decay = self._lars_weight_decay\n    param_name = param_and_grad[0].name\n    if len(self._exclude_from_weight_decay) > 0:\n        for name in self._exclude_from_weight_decay:\n            if name in param_name:\n                _lars_weight_decay = 0.0\n                break\n    velocity_acc = self._get_accumulator_master(self._velocity_acc_str, param_and_grad[0])\n    lr = self._create_param_lr(param_and_grad)\n    find_master = self._multi_precision and self._is_dtype_fp16_or_bf16(param_and_grad[0].dtype)\n    master_weight = self._master_weights[param_and_grad[0].name] if find_master else None\n    attrs = {'mu': self._momentum, 'lars_coeff': self._lars_coeff, 'lars_weight_decay': [_lars_weight_decay], 'multi_precision': find_master, 'epsilon': self._epsilon, 'rescale_grad': self._rescale_grad}\n    inputs = {'Param': param_and_grad[0], 'Grad': param_and_grad[1], 'Velocity': velocity_acc, 'LearningRate': lr}\n    outputs = {'ParamOut': param_and_grad[0], 'VelocityOut': velocity_acc}\n    if find_master:\n        inputs['MasterParam'] = master_weight\n        outputs['MasterParamOut'] = master_weight\n    if in_dygraph_mode():\n        (tmp, tmp2) = _legacy_C_ops.lars_momentum([param_and_grad[0]], [param_and_grad[1]], [velocity_acc], [lr], [param_and_grad[0]], [velocity_acc], 'mu', self._momentum, 'lars_coeff', self._lars_coeff, 'lars_weight_decay', [_lars_weight_decay], 'multi_precision', find_master, 'epsilon', self._epsilon, 'rescale_grad', self._rescale_grad)\n    else:\n        momentum_op = block.append_op(type=self.type, inputs=inputs, outputs=outputs, attrs=attrs, stop_gradient=True)\n        return momentum_op",
            "def _append_optimize_op(self, block, param_and_grad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert isinstance(block, framework.Block)\n    _lars_weight_decay = self._lars_weight_decay\n    param_name = param_and_grad[0].name\n    if len(self._exclude_from_weight_decay) > 0:\n        for name in self._exclude_from_weight_decay:\n            if name in param_name:\n                _lars_weight_decay = 0.0\n                break\n    velocity_acc = self._get_accumulator_master(self._velocity_acc_str, param_and_grad[0])\n    lr = self._create_param_lr(param_and_grad)\n    find_master = self._multi_precision and self._is_dtype_fp16_or_bf16(param_and_grad[0].dtype)\n    master_weight = self._master_weights[param_and_grad[0].name] if find_master else None\n    attrs = {'mu': self._momentum, 'lars_coeff': self._lars_coeff, 'lars_weight_decay': [_lars_weight_decay], 'multi_precision': find_master, 'epsilon': self._epsilon, 'rescale_grad': self._rescale_grad}\n    inputs = {'Param': param_and_grad[0], 'Grad': param_and_grad[1], 'Velocity': velocity_acc, 'LearningRate': lr}\n    outputs = {'ParamOut': param_and_grad[0], 'VelocityOut': velocity_acc}\n    if find_master:\n        inputs['MasterParam'] = master_weight\n        outputs['MasterParamOut'] = master_weight\n    if in_dygraph_mode():\n        (tmp, tmp2) = _legacy_C_ops.lars_momentum([param_and_grad[0]], [param_and_grad[1]], [velocity_acc], [lr], [param_and_grad[0]], [velocity_acc], 'mu', self._momentum, 'lars_coeff', self._lars_coeff, 'lars_weight_decay', [_lars_weight_decay], 'multi_precision', find_master, 'epsilon', self._epsilon, 'rescale_grad', self._rescale_grad)\n    else:\n        momentum_op = block.append_op(type=self.type, inputs=inputs, outputs=outputs, attrs=attrs, stop_gradient=True)\n        return momentum_op"
        ]
    }
]