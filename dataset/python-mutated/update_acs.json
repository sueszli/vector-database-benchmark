[
    {
        "func_name": "search_client_from_config",
        "original": "def search_client_from_config(acs_config: dict, credential):\n    return SearchClient(endpoint=acs_config['endpoint'], index_name=acs_config['index_name'], credential=credential, api_version=acs_config.get('api_version', '2023-07-01-preview'), user_agent=acs_user_agent)",
        "mutated": [
            "def search_client_from_config(acs_config: dict, credential):\n    if False:\n        i = 10\n    return SearchClient(endpoint=acs_config['endpoint'], index_name=acs_config['index_name'], credential=credential, api_version=acs_config.get('api_version', '2023-07-01-preview'), user_agent=acs_user_agent)",
            "def search_client_from_config(acs_config: dict, credential):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return SearchClient(endpoint=acs_config['endpoint'], index_name=acs_config['index_name'], credential=credential, api_version=acs_config.get('api_version', '2023-07-01-preview'), user_agent=acs_user_agent)",
            "def search_client_from_config(acs_config: dict, credential):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return SearchClient(endpoint=acs_config['endpoint'], index_name=acs_config['index_name'], credential=credential, api_version=acs_config.get('api_version', '2023-07-01-preview'), user_agent=acs_user_agent)",
            "def search_client_from_config(acs_config: dict, credential):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return SearchClient(endpoint=acs_config['endpoint'], index_name=acs_config['index_name'], credential=credential, api_version=acs_config.get('api_version', '2023-07-01-preview'), user_agent=acs_user_agent)",
            "def search_client_from_config(acs_config: dict, credential):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return SearchClient(endpoint=acs_config['endpoint'], index_name=acs_config['index_name'], credential=credential, api_version=acs_config.get('api_version', '2023-07-01-preview'), user_agent=acs_user_agent)"
        ]
    },
    {
        "func_name": "create_search_index_sdk",
        "original": "def create_search_index_sdk(acs_config: dict, credential, embeddings: Optional[EmbeddingsContainer]=None):\n    \"\"\"\n    Create a search index using the Azure Search SDK.\n\n    Args:\n    ----\n        acs_config (dict): ACS configuration dictionary. Expected to contain:\n            - endpoint: ACS endpoint\n            - index_name: ACS index name\n            - field_mapping: Mappings from a set of fields understoon by MLIndex (refer to MLIndex.INDEX_FIELD_MAPPING_TYPES) to ACS field names.\n        credential (TokenCredential): Azure credential to use for authentication.\n        embeddings (EmbeddingsContainer): EmbeddingsContainer to use for creating the index. If provided, the index will be configured to support vector search.\n    \"\"\"\n    logger.info(f\"Ensuring search index {acs_config['index_name']} exists\")\n    index_client = SearchIndexClient(endpoint=acs_config['endpoint'], credential=credential, user_agent=acs_user_agent)\n    if acs_config['index_name'] not in index_client.list_index_names():\n        current_version = pkg_version.parse(azure_documents_search_version)\n        fields = []\n        for (field_type, field_name) in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY].items():\n            if field_type == 'content':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String, analyzer_name='standard'))\n            elif field_type == 'url' or field_type == 'filename':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'title':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'metadata':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'embedding':\n                if current_version >= pkg_version.parse('11.4.0b11'):\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_profile=f'{field_name}_config'))\n                else:\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_configuration=f'{field_name}_config'))\n            else:\n                logger.warning(f'Unknown field type will be ignored and not included in index: {field_type}')\n        fields.append(SimpleField(name='id', type=SearchFieldDataType.String, key=True))\n        if 'content' not in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n            raise RuntimeError(f\"ACS index must have a 'content' field. Please specify a 'content' field in the {MLIndex.INDEX_FIELD_MAPPING_KEY} config.\")\n        vector_search_args = {}\n        if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and embeddings and (embeddings.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]):\n            if current_version >= pkg_version.parse('11.4.0b11'):\n                from azure.search.documents.indexes.models import HnswParameters, HnswVectorSearchAlgorithmConfiguration, VectorSearch, VectorSearchAlgorithmKind, VectorSearchProfile\n                vector_config_name = f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\"\n                hnsw_name = 'azureml_default_hnsw_config'\n                vector_search_args['vector_search'] = VectorSearch(algorithms=[HnswVectorSearchAlgorithmConfiguration(name=hnsw_name, kind=VectorSearchAlgorithmKind.HNSW, parameters=HnswParameters(m=4, ef_construction=400, ef_search=500, metric='cosine'))], profiles=[VectorSearchProfile(name=vector_config_name, algorithm=hnsw_name)])\n            elif current_version >= pkg_version.parse('11.4.0b8'):\n                from azure.search.documents.indexes.models import HnswVectorSearchAlgorithmConfiguration, VectorSearch\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[HnswVectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            elif current_version >= pkg_version.parse('11.4.0b4'):\n                from azure.search.documents.indexes.models import VectorSearch, VectorSearchAlgorithmConfiguration\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[VectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", kind='hnsw', hnsw_parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            else:\n                raise RuntimeError(f'azure-search-documents version {azure_documents_search_version} is not supported when using embeddings. Please upgrade to 11.4.0b4 or later.')\n        semantic_config = SemanticConfiguration(name='azureml-default', prioritized_fields=PrioritizedFields(title_field=SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']) if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] else None, prioritized_content_fields=[SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content'])]))\n        index = SearchIndex(name=acs_config['index_name'], fields=fields, semantic_settings=SemanticSettings(configurations=[semantic_config]), **vector_search_args)\n        logger.info(f\"Creating {acs_config['index_name']} search index\")\n        result = index_client.create_or_update_index(index)\n        logger.info(f'Created {result.name} search index')\n    else:\n        logger.info(f\"Search index {acs_config['index_name']} already exists\")",
        "mutated": [
            "def create_search_index_sdk(acs_config: dict, credential, embeddings: Optional[EmbeddingsContainer]=None):\n    if False:\n        i = 10\n    '\\n    Create a search index using the Azure Search SDK.\\n\\n    Args:\\n    ----\\n        acs_config (dict): ACS configuration dictionary. Expected to contain:\\n            - endpoint: ACS endpoint\\n            - index_name: ACS index name\\n            - field_mapping: Mappings from a set of fields understoon by MLIndex (refer to MLIndex.INDEX_FIELD_MAPPING_TYPES) to ACS field names.\\n        credential (TokenCredential): Azure credential to use for authentication.\\n        embeddings (EmbeddingsContainer): EmbeddingsContainer to use for creating the index. If provided, the index will be configured to support vector search.\\n    '\n    logger.info(f\"Ensuring search index {acs_config['index_name']} exists\")\n    index_client = SearchIndexClient(endpoint=acs_config['endpoint'], credential=credential, user_agent=acs_user_agent)\n    if acs_config['index_name'] not in index_client.list_index_names():\n        current_version = pkg_version.parse(azure_documents_search_version)\n        fields = []\n        for (field_type, field_name) in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY].items():\n            if field_type == 'content':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String, analyzer_name='standard'))\n            elif field_type == 'url' or field_type == 'filename':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'title':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'metadata':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'embedding':\n                if current_version >= pkg_version.parse('11.4.0b11'):\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_profile=f'{field_name}_config'))\n                else:\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_configuration=f'{field_name}_config'))\n            else:\n                logger.warning(f'Unknown field type will be ignored and not included in index: {field_type}')\n        fields.append(SimpleField(name='id', type=SearchFieldDataType.String, key=True))\n        if 'content' not in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n            raise RuntimeError(f\"ACS index must have a 'content' field. Please specify a 'content' field in the {MLIndex.INDEX_FIELD_MAPPING_KEY} config.\")\n        vector_search_args = {}\n        if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and embeddings and (embeddings.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]):\n            if current_version >= pkg_version.parse('11.4.0b11'):\n                from azure.search.documents.indexes.models import HnswParameters, HnswVectorSearchAlgorithmConfiguration, VectorSearch, VectorSearchAlgorithmKind, VectorSearchProfile\n                vector_config_name = f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\"\n                hnsw_name = 'azureml_default_hnsw_config'\n                vector_search_args['vector_search'] = VectorSearch(algorithms=[HnswVectorSearchAlgorithmConfiguration(name=hnsw_name, kind=VectorSearchAlgorithmKind.HNSW, parameters=HnswParameters(m=4, ef_construction=400, ef_search=500, metric='cosine'))], profiles=[VectorSearchProfile(name=vector_config_name, algorithm=hnsw_name)])\n            elif current_version >= pkg_version.parse('11.4.0b8'):\n                from azure.search.documents.indexes.models import HnswVectorSearchAlgorithmConfiguration, VectorSearch\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[HnswVectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            elif current_version >= pkg_version.parse('11.4.0b4'):\n                from azure.search.documents.indexes.models import VectorSearch, VectorSearchAlgorithmConfiguration\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[VectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", kind='hnsw', hnsw_parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            else:\n                raise RuntimeError(f'azure-search-documents version {azure_documents_search_version} is not supported when using embeddings. Please upgrade to 11.4.0b4 or later.')\n        semantic_config = SemanticConfiguration(name='azureml-default', prioritized_fields=PrioritizedFields(title_field=SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']) if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] else None, prioritized_content_fields=[SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content'])]))\n        index = SearchIndex(name=acs_config['index_name'], fields=fields, semantic_settings=SemanticSettings(configurations=[semantic_config]), **vector_search_args)\n        logger.info(f\"Creating {acs_config['index_name']} search index\")\n        result = index_client.create_or_update_index(index)\n        logger.info(f'Created {result.name} search index')\n    else:\n        logger.info(f\"Search index {acs_config['index_name']} already exists\")",
            "def create_search_index_sdk(acs_config: dict, credential, embeddings: Optional[EmbeddingsContainer]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Create a search index using the Azure Search SDK.\\n\\n    Args:\\n    ----\\n        acs_config (dict): ACS configuration dictionary. Expected to contain:\\n            - endpoint: ACS endpoint\\n            - index_name: ACS index name\\n            - field_mapping: Mappings from a set of fields understoon by MLIndex (refer to MLIndex.INDEX_FIELD_MAPPING_TYPES) to ACS field names.\\n        credential (TokenCredential): Azure credential to use for authentication.\\n        embeddings (EmbeddingsContainer): EmbeddingsContainer to use for creating the index. If provided, the index will be configured to support vector search.\\n    '\n    logger.info(f\"Ensuring search index {acs_config['index_name']} exists\")\n    index_client = SearchIndexClient(endpoint=acs_config['endpoint'], credential=credential, user_agent=acs_user_agent)\n    if acs_config['index_name'] not in index_client.list_index_names():\n        current_version = pkg_version.parse(azure_documents_search_version)\n        fields = []\n        for (field_type, field_name) in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY].items():\n            if field_type == 'content':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String, analyzer_name='standard'))\n            elif field_type == 'url' or field_type == 'filename':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'title':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'metadata':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'embedding':\n                if current_version >= pkg_version.parse('11.4.0b11'):\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_profile=f'{field_name}_config'))\n                else:\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_configuration=f'{field_name}_config'))\n            else:\n                logger.warning(f'Unknown field type will be ignored and not included in index: {field_type}')\n        fields.append(SimpleField(name='id', type=SearchFieldDataType.String, key=True))\n        if 'content' not in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n            raise RuntimeError(f\"ACS index must have a 'content' field. Please specify a 'content' field in the {MLIndex.INDEX_FIELD_MAPPING_KEY} config.\")\n        vector_search_args = {}\n        if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and embeddings and (embeddings.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]):\n            if current_version >= pkg_version.parse('11.4.0b11'):\n                from azure.search.documents.indexes.models import HnswParameters, HnswVectorSearchAlgorithmConfiguration, VectorSearch, VectorSearchAlgorithmKind, VectorSearchProfile\n                vector_config_name = f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\"\n                hnsw_name = 'azureml_default_hnsw_config'\n                vector_search_args['vector_search'] = VectorSearch(algorithms=[HnswVectorSearchAlgorithmConfiguration(name=hnsw_name, kind=VectorSearchAlgorithmKind.HNSW, parameters=HnswParameters(m=4, ef_construction=400, ef_search=500, metric='cosine'))], profiles=[VectorSearchProfile(name=vector_config_name, algorithm=hnsw_name)])\n            elif current_version >= pkg_version.parse('11.4.0b8'):\n                from azure.search.documents.indexes.models import HnswVectorSearchAlgorithmConfiguration, VectorSearch\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[HnswVectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            elif current_version >= pkg_version.parse('11.4.0b4'):\n                from azure.search.documents.indexes.models import VectorSearch, VectorSearchAlgorithmConfiguration\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[VectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", kind='hnsw', hnsw_parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            else:\n                raise RuntimeError(f'azure-search-documents version {azure_documents_search_version} is not supported when using embeddings. Please upgrade to 11.4.0b4 or later.')\n        semantic_config = SemanticConfiguration(name='azureml-default', prioritized_fields=PrioritizedFields(title_field=SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']) if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] else None, prioritized_content_fields=[SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content'])]))\n        index = SearchIndex(name=acs_config['index_name'], fields=fields, semantic_settings=SemanticSettings(configurations=[semantic_config]), **vector_search_args)\n        logger.info(f\"Creating {acs_config['index_name']} search index\")\n        result = index_client.create_or_update_index(index)\n        logger.info(f'Created {result.name} search index')\n    else:\n        logger.info(f\"Search index {acs_config['index_name']} already exists\")",
            "def create_search_index_sdk(acs_config: dict, credential, embeddings: Optional[EmbeddingsContainer]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Create a search index using the Azure Search SDK.\\n\\n    Args:\\n    ----\\n        acs_config (dict): ACS configuration dictionary. Expected to contain:\\n            - endpoint: ACS endpoint\\n            - index_name: ACS index name\\n            - field_mapping: Mappings from a set of fields understoon by MLIndex (refer to MLIndex.INDEX_FIELD_MAPPING_TYPES) to ACS field names.\\n        credential (TokenCredential): Azure credential to use for authentication.\\n        embeddings (EmbeddingsContainer): EmbeddingsContainer to use for creating the index. If provided, the index will be configured to support vector search.\\n    '\n    logger.info(f\"Ensuring search index {acs_config['index_name']} exists\")\n    index_client = SearchIndexClient(endpoint=acs_config['endpoint'], credential=credential, user_agent=acs_user_agent)\n    if acs_config['index_name'] not in index_client.list_index_names():\n        current_version = pkg_version.parse(azure_documents_search_version)\n        fields = []\n        for (field_type, field_name) in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY].items():\n            if field_type == 'content':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String, analyzer_name='standard'))\n            elif field_type == 'url' or field_type == 'filename':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'title':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'metadata':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'embedding':\n                if current_version >= pkg_version.parse('11.4.0b11'):\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_profile=f'{field_name}_config'))\n                else:\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_configuration=f'{field_name}_config'))\n            else:\n                logger.warning(f'Unknown field type will be ignored and not included in index: {field_type}')\n        fields.append(SimpleField(name='id', type=SearchFieldDataType.String, key=True))\n        if 'content' not in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n            raise RuntimeError(f\"ACS index must have a 'content' field. Please specify a 'content' field in the {MLIndex.INDEX_FIELD_MAPPING_KEY} config.\")\n        vector_search_args = {}\n        if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and embeddings and (embeddings.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]):\n            if current_version >= pkg_version.parse('11.4.0b11'):\n                from azure.search.documents.indexes.models import HnswParameters, HnswVectorSearchAlgorithmConfiguration, VectorSearch, VectorSearchAlgorithmKind, VectorSearchProfile\n                vector_config_name = f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\"\n                hnsw_name = 'azureml_default_hnsw_config'\n                vector_search_args['vector_search'] = VectorSearch(algorithms=[HnswVectorSearchAlgorithmConfiguration(name=hnsw_name, kind=VectorSearchAlgorithmKind.HNSW, parameters=HnswParameters(m=4, ef_construction=400, ef_search=500, metric='cosine'))], profiles=[VectorSearchProfile(name=vector_config_name, algorithm=hnsw_name)])\n            elif current_version >= pkg_version.parse('11.4.0b8'):\n                from azure.search.documents.indexes.models import HnswVectorSearchAlgorithmConfiguration, VectorSearch\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[HnswVectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            elif current_version >= pkg_version.parse('11.4.0b4'):\n                from azure.search.documents.indexes.models import VectorSearch, VectorSearchAlgorithmConfiguration\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[VectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", kind='hnsw', hnsw_parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            else:\n                raise RuntimeError(f'azure-search-documents version {azure_documents_search_version} is not supported when using embeddings. Please upgrade to 11.4.0b4 or later.')\n        semantic_config = SemanticConfiguration(name='azureml-default', prioritized_fields=PrioritizedFields(title_field=SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']) if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] else None, prioritized_content_fields=[SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content'])]))\n        index = SearchIndex(name=acs_config['index_name'], fields=fields, semantic_settings=SemanticSettings(configurations=[semantic_config]), **vector_search_args)\n        logger.info(f\"Creating {acs_config['index_name']} search index\")\n        result = index_client.create_or_update_index(index)\n        logger.info(f'Created {result.name} search index')\n    else:\n        logger.info(f\"Search index {acs_config['index_name']} already exists\")",
            "def create_search_index_sdk(acs_config: dict, credential, embeddings: Optional[EmbeddingsContainer]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Create a search index using the Azure Search SDK.\\n\\n    Args:\\n    ----\\n        acs_config (dict): ACS configuration dictionary. Expected to contain:\\n            - endpoint: ACS endpoint\\n            - index_name: ACS index name\\n            - field_mapping: Mappings from a set of fields understoon by MLIndex (refer to MLIndex.INDEX_FIELD_MAPPING_TYPES) to ACS field names.\\n        credential (TokenCredential): Azure credential to use for authentication.\\n        embeddings (EmbeddingsContainer): EmbeddingsContainer to use for creating the index. If provided, the index will be configured to support vector search.\\n    '\n    logger.info(f\"Ensuring search index {acs_config['index_name']} exists\")\n    index_client = SearchIndexClient(endpoint=acs_config['endpoint'], credential=credential, user_agent=acs_user_agent)\n    if acs_config['index_name'] not in index_client.list_index_names():\n        current_version = pkg_version.parse(azure_documents_search_version)\n        fields = []\n        for (field_type, field_name) in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY].items():\n            if field_type == 'content':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String, analyzer_name='standard'))\n            elif field_type == 'url' or field_type == 'filename':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'title':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'metadata':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'embedding':\n                if current_version >= pkg_version.parse('11.4.0b11'):\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_profile=f'{field_name}_config'))\n                else:\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_configuration=f'{field_name}_config'))\n            else:\n                logger.warning(f'Unknown field type will be ignored and not included in index: {field_type}')\n        fields.append(SimpleField(name='id', type=SearchFieldDataType.String, key=True))\n        if 'content' not in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n            raise RuntimeError(f\"ACS index must have a 'content' field. Please specify a 'content' field in the {MLIndex.INDEX_FIELD_MAPPING_KEY} config.\")\n        vector_search_args = {}\n        if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and embeddings and (embeddings.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]):\n            if current_version >= pkg_version.parse('11.4.0b11'):\n                from azure.search.documents.indexes.models import HnswParameters, HnswVectorSearchAlgorithmConfiguration, VectorSearch, VectorSearchAlgorithmKind, VectorSearchProfile\n                vector_config_name = f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\"\n                hnsw_name = 'azureml_default_hnsw_config'\n                vector_search_args['vector_search'] = VectorSearch(algorithms=[HnswVectorSearchAlgorithmConfiguration(name=hnsw_name, kind=VectorSearchAlgorithmKind.HNSW, parameters=HnswParameters(m=4, ef_construction=400, ef_search=500, metric='cosine'))], profiles=[VectorSearchProfile(name=vector_config_name, algorithm=hnsw_name)])\n            elif current_version >= pkg_version.parse('11.4.0b8'):\n                from azure.search.documents.indexes.models import HnswVectorSearchAlgorithmConfiguration, VectorSearch\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[HnswVectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            elif current_version >= pkg_version.parse('11.4.0b4'):\n                from azure.search.documents.indexes.models import VectorSearch, VectorSearchAlgorithmConfiguration\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[VectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", kind='hnsw', hnsw_parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            else:\n                raise RuntimeError(f'azure-search-documents version {azure_documents_search_version} is not supported when using embeddings. Please upgrade to 11.4.0b4 or later.')\n        semantic_config = SemanticConfiguration(name='azureml-default', prioritized_fields=PrioritizedFields(title_field=SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']) if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] else None, prioritized_content_fields=[SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content'])]))\n        index = SearchIndex(name=acs_config['index_name'], fields=fields, semantic_settings=SemanticSettings(configurations=[semantic_config]), **vector_search_args)\n        logger.info(f\"Creating {acs_config['index_name']} search index\")\n        result = index_client.create_or_update_index(index)\n        logger.info(f'Created {result.name} search index')\n    else:\n        logger.info(f\"Search index {acs_config['index_name']} already exists\")",
            "def create_search_index_sdk(acs_config: dict, credential, embeddings: Optional[EmbeddingsContainer]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Create a search index using the Azure Search SDK.\\n\\n    Args:\\n    ----\\n        acs_config (dict): ACS configuration dictionary. Expected to contain:\\n            - endpoint: ACS endpoint\\n            - index_name: ACS index name\\n            - field_mapping: Mappings from a set of fields understoon by MLIndex (refer to MLIndex.INDEX_FIELD_MAPPING_TYPES) to ACS field names.\\n        credential (TokenCredential): Azure credential to use for authentication.\\n        embeddings (EmbeddingsContainer): EmbeddingsContainer to use for creating the index. If provided, the index will be configured to support vector search.\\n    '\n    logger.info(f\"Ensuring search index {acs_config['index_name']} exists\")\n    index_client = SearchIndexClient(endpoint=acs_config['endpoint'], credential=credential, user_agent=acs_user_agent)\n    if acs_config['index_name'] not in index_client.list_index_names():\n        current_version = pkg_version.parse(azure_documents_search_version)\n        fields = []\n        for (field_type, field_name) in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY].items():\n            if field_type == 'content':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String, analyzer_name='standard'))\n            elif field_type == 'url' or field_type == 'filename':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'title':\n                fields.append(SearchableField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'metadata':\n                fields.append(SimpleField(name=field_name, type=SearchFieldDataType.String))\n            elif field_type == 'embedding':\n                if current_version >= pkg_version.parse('11.4.0b11'):\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_profile=f'{field_name}_config'))\n                else:\n                    fields.append(SearchField(name=field_name, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=embeddings.get_embedding_dimensions(), vector_search_configuration=f'{field_name}_config'))\n            else:\n                logger.warning(f'Unknown field type will be ignored and not included in index: {field_type}')\n        fields.append(SimpleField(name='id', type=SearchFieldDataType.String, key=True))\n        if 'content' not in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n            raise RuntimeError(f\"ACS index must have a 'content' field. Please specify a 'content' field in the {MLIndex.INDEX_FIELD_MAPPING_KEY} config.\")\n        vector_search_args = {}\n        if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and embeddings and (embeddings.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]):\n            if current_version >= pkg_version.parse('11.4.0b11'):\n                from azure.search.documents.indexes.models import HnswParameters, HnswVectorSearchAlgorithmConfiguration, VectorSearch, VectorSearchAlgorithmKind, VectorSearchProfile\n                vector_config_name = f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\"\n                hnsw_name = 'azureml_default_hnsw_config'\n                vector_search_args['vector_search'] = VectorSearch(algorithms=[HnswVectorSearchAlgorithmConfiguration(name=hnsw_name, kind=VectorSearchAlgorithmKind.HNSW, parameters=HnswParameters(m=4, ef_construction=400, ef_search=500, metric='cosine'))], profiles=[VectorSearchProfile(name=vector_config_name, algorithm=hnsw_name)])\n            elif current_version >= pkg_version.parse('11.4.0b8'):\n                from azure.search.documents.indexes.models import HnswVectorSearchAlgorithmConfiguration, VectorSearch\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[HnswVectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            elif current_version >= pkg_version.parse('11.4.0b4'):\n                from azure.search.documents.indexes.models import VectorSearch, VectorSearchAlgorithmConfiguration\n                vector_search_args['vector_search'] = VectorSearch(algorithm_configurations=[VectorSearchAlgorithmConfiguration(name=f\"{acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']}_config\", kind='hnsw', hnsw_parameters={'m': 4, 'efConstruction': 400, 'efSearch': 500, 'metric': 'cosine'})])\n            else:\n                raise RuntimeError(f'azure-search-documents version {azure_documents_search_version} is not supported when using embeddings. Please upgrade to 11.4.0b4 or later.')\n        semantic_config = SemanticConfiguration(name='azureml-default', prioritized_fields=PrioritizedFields(title_field=SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']) if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] else None, prioritized_content_fields=[SemanticField(field_name=acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content'])]))\n        index = SearchIndex(name=acs_config['index_name'], fields=fields, semantic_settings=SemanticSettings(configurations=[semantic_config]), **vector_search_args)\n        logger.info(f\"Creating {acs_config['index_name']} search index\")\n        result = index_client.create_or_update_index(index)\n        logger.info(f'Created {result.name} search index')\n    else:\n        logger.info(f\"Search index {acs_config['index_name']} already exists\")"
        ]
    },
    {
        "func_name": "process_upload_results",
        "original": "def process_upload_results(results, start_time):\n    succeeded = []\n    failed = []\n    for r in results:\n        if isinstance(r, dict):\n            if r['status'] is False:\n                failed.append(r)\n            else:\n                succeeded.append(r)\n        elif r.succeeded:\n            succeeded.append(r)\n        else:\n            failed.append(r)\n    duration = time.time() - start_time\n    logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n    activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n    if len(failed) > 0:\n        for r in failed:\n            error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n            logger.error(f'Failed document reason: {error}')\n        return failed\n    return []",
        "mutated": [
            "def process_upload_results(results, start_time):\n    if False:\n        i = 10\n    succeeded = []\n    failed = []\n    for r in results:\n        if isinstance(r, dict):\n            if r['status'] is False:\n                failed.append(r)\n            else:\n                succeeded.append(r)\n        elif r.succeeded:\n            succeeded.append(r)\n        else:\n            failed.append(r)\n    duration = time.time() - start_time\n    logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n    activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n    if len(failed) > 0:\n        for r in failed:\n            error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n            logger.error(f'Failed document reason: {error}')\n        return failed\n    return []",
            "def process_upload_results(results, start_time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    succeeded = []\n    failed = []\n    for r in results:\n        if isinstance(r, dict):\n            if r['status'] is False:\n                failed.append(r)\n            else:\n                succeeded.append(r)\n        elif r.succeeded:\n            succeeded.append(r)\n        else:\n            failed.append(r)\n    duration = time.time() - start_time\n    logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n    activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n    if len(failed) > 0:\n        for r in failed:\n            error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n            logger.error(f'Failed document reason: {error}')\n        return failed\n    return []",
            "def process_upload_results(results, start_time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    succeeded = []\n    failed = []\n    for r in results:\n        if isinstance(r, dict):\n            if r['status'] is False:\n                failed.append(r)\n            else:\n                succeeded.append(r)\n        elif r.succeeded:\n            succeeded.append(r)\n        else:\n            failed.append(r)\n    duration = time.time() - start_time\n    logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n    activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n    if len(failed) > 0:\n        for r in failed:\n            error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n            logger.error(f'Failed document reason: {error}')\n        return failed\n    return []",
            "def process_upload_results(results, start_time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    succeeded = []\n    failed = []\n    for r in results:\n        if isinstance(r, dict):\n            if r['status'] is False:\n                failed.append(r)\n            else:\n                succeeded.append(r)\n        elif r.succeeded:\n            succeeded.append(r)\n        else:\n            failed.append(r)\n    duration = time.time() - start_time\n    logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n    activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n    if len(failed) > 0:\n        for r in failed:\n            error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n            logger.error(f'Failed document reason: {error}')\n        return failed\n    return []",
            "def process_upload_results(results, start_time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    succeeded = []\n    failed = []\n    for r in results:\n        if isinstance(r, dict):\n            if r['status'] is False:\n                failed.append(r)\n            else:\n                succeeded.append(r)\n        elif r.succeeded:\n            succeeded.append(r)\n        else:\n            failed.append(r)\n    duration = time.time() - start_time\n    logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n    activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n    if len(failed) > 0:\n        for r in failed:\n            error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n            logger.error(f'Failed document reason: {error}')\n        return failed\n    return []"
        ]
    },
    {
        "func_name": "batched_docs_to_delete",
        "original": "def batched_docs_to_delete(embeddings_container) -> List[str]:\n    num_deleted_ids = 0\n    deleted_ids = []\n    for (source_id, source) in emb._deleted_sources.items():\n        logger.info(f'Deleting all documents from source: {source_id}')\n        for doc_id in source.document_ids:\n            num_deleted_ids += 1\n            deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n            if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                yield deleted_ids\n                deleted_ids = []\n            if verbosity > 1:\n                logger.info(f'Marked document for deletion: {doc_id}')\n    logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n    for doc_id in emb._deleted_documents:\n        num_deleted_ids += 1\n        deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n        if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n            yield deleted_ids\n            deleted_ids = []\n        if verbosity > 1:\n            logger.info(f'Marked document for deletion: {doc_id}')\n    if len(deleted_ids) > 0:\n        yield deleted_ids\n    logger.info(f'Total {num_deleted_ids} documents marked for deletion')",
        "mutated": [
            "def batched_docs_to_delete(embeddings_container) -> List[str]:\n    if False:\n        i = 10\n    num_deleted_ids = 0\n    deleted_ids = []\n    for (source_id, source) in emb._deleted_sources.items():\n        logger.info(f'Deleting all documents from source: {source_id}')\n        for doc_id in source.document_ids:\n            num_deleted_ids += 1\n            deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n            if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                yield deleted_ids\n                deleted_ids = []\n            if verbosity > 1:\n                logger.info(f'Marked document for deletion: {doc_id}')\n    logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n    for doc_id in emb._deleted_documents:\n        num_deleted_ids += 1\n        deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n        if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n            yield deleted_ids\n            deleted_ids = []\n        if verbosity > 1:\n            logger.info(f'Marked document for deletion: {doc_id}')\n    if len(deleted_ids) > 0:\n        yield deleted_ids\n    logger.info(f'Total {num_deleted_ids} documents marked for deletion')",
            "def batched_docs_to_delete(embeddings_container) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    num_deleted_ids = 0\n    deleted_ids = []\n    for (source_id, source) in emb._deleted_sources.items():\n        logger.info(f'Deleting all documents from source: {source_id}')\n        for doc_id in source.document_ids:\n            num_deleted_ids += 1\n            deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n            if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                yield deleted_ids\n                deleted_ids = []\n            if verbosity > 1:\n                logger.info(f'Marked document for deletion: {doc_id}')\n    logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n    for doc_id in emb._deleted_documents:\n        num_deleted_ids += 1\n        deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n        if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n            yield deleted_ids\n            deleted_ids = []\n        if verbosity > 1:\n            logger.info(f'Marked document for deletion: {doc_id}')\n    if len(deleted_ids) > 0:\n        yield deleted_ids\n    logger.info(f'Total {num_deleted_ids} documents marked for deletion')",
            "def batched_docs_to_delete(embeddings_container) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    num_deleted_ids = 0\n    deleted_ids = []\n    for (source_id, source) in emb._deleted_sources.items():\n        logger.info(f'Deleting all documents from source: {source_id}')\n        for doc_id in source.document_ids:\n            num_deleted_ids += 1\n            deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n            if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                yield deleted_ids\n                deleted_ids = []\n            if verbosity > 1:\n                logger.info(f'Marked document for deletion: {doc_id}')\n    logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n    for doc_id in emb._deleted_documents:\n        num_deleted_ids += 1\n        deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n        if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n            yield deleted_ids\n            deleted_ids = []\n        if verbosity > 1:\n            logger.info(f'Marked document for deletion: {doc_id}')\n    if len(deleted_ids) > 0:\n        yield deleted_ids\n    logger.info(f'Total {num_deleted_ids} documents marked for deletion')",
            "def batched_docs_to_delete(embeddings_container) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    num_deleted_ids = 0\n    deleted_ids = []\n    for (source_id, source) in emb._deleted_sources.items():\n        logger.info(f'Deleting all documents from source: {source_id}')\n        for doc_id in source.document_ids:\n            num_deleted_ids += 1\n            deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n            if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                yield deleted_ids\n                deleted_ids = []\n            if verbosity > 1:\n                logger.info(f'Marked document for deletion: {doc_id}')\n    logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n    for doc_id in emb._deleted_documents:\n        num_deleted_ids += 1\n        deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n        if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n            yield deleted_ids\n            deleted_ids = []\n        if verbosity > 1:\n            logger.info(f'Marked document for deletion: {doc_id}')\n    if len(deleted_ids) > 0:\n        yield deleted_ids\n    logger.info(f'Total {num_deleted_ids} documents marked for deletion')",
            "def batched_docs_to_delete(embeddings_container) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    num_deleted_ids = 0\n    deleted_ids = []\n    for (source_id, source) in emb._deleted_sources.items():\n        logger.info(f'Deleting all documents from source: {source_id}')\n        for doc_id in source.document_ids:\n            num_deleted_ids += 1\n            deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n            if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                yield deleted_ids\n                deleted_ids = []\n            if verbosity > 1:\n                logger.info(f'Marked document for deletion: {doc_id}')\n    logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n    for doc_id in emb._deleted_documents:\n        num_deleted_ids += 1\n        deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n        if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n            yield deleted_ids\n            deleted_ids = []\n        if verbosity > 1:\n            logger.info(f'Marked document for deletion: {doc_id}')\n    if len(deleted_ids) > 0:\n        yield deleted_ids\n    logger.info(f'Total {num_deleted_ids} documents marked for deletion')"
        ]
    },
    {
        "func_name": "create_index_from_raw_embeddings",
        "original": "def create_index_from_raw_embeddings(emb: EmbeddingsContainer, acs_config={}, connection={}, output_path: Optional[str]=None, credential: Optional[TokenCredential]=None, verbosity: int=1) -> MLIndex:\n    \"\"\"Upload an EmbeddingsContainer to Azure Cognitive Search and return an MLIndex.\"\"\"\n    with track_activity(logger, 'update_acs', custom_dimensions={'num_documents': len(emb._document_embeddings)}) as activity_logger:\n        logger.info('Updating ACS index')\n        connection_credential = get_connection_credential(connection, credential=credential)\n        if MLIndex.INDEX_FIELD_MAPPING_KEY not in acs_config:\n            acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] = {'content': 'content', 'url': 'url', 'filename': 'filepath', 'title': 'title', 'metadata': 'meta_json_string'}\n            if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none'):\n                acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding'] = 'contentVector'\n        logger.info(f'Using Index fields: {json.dumps(acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], indent=2)}')\n        create_search_index_sdk(acs_config, connection_credential, emb)\n        search_client = search_client_from_config(acs_config, connection_credential)\n        batch_size = acs_config['batch_size'] if 'batch_size' in acs_config else 100\n\n        def process_upload_results(results, start_time):\n            succeeded = []\n            failed = []\n            for r in results:\n                if isinstance(r, dict):\n                    if r['status'] is False:\n                        failed.append(r)\n                    else:\n                        succeeded.append(r)\n                elif r.succeeded:\n                    succeeded.append(r)\n                else:\n                    failed.append(r)\n            duration = time.time() - start_time\n            logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n            activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n            if len(failed) > 0:\n                for r in failed:\n                    error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n                    logger.error(f'Failed document reason: {error}')\n                return failed\n            return []\n\n        def batched_docs_to_delete(embeddings_container) -> List[str]:\n            num_deleted_ids = 0\n            deleted_ids = []\n            for (source_id, source) in emb._deleted_sources.items():\n                logger.info(f'Deleting all documents from source: {source_id}')\n                for doc_id in source.document_ids:\n                    num_deleted_ids += 1\n                    deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                    if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                        yield deleted_ids\n                        deleted_ids = []\n                    if verbosity > 1:\n                        logger.info(f'Marked document for deletion: {doc_id}')\n            logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n            for doc_id in emb._deleted_documents:\n                num_deleted_ids += 1\n                deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                    yield deleted_ids\n                    deleted_ids = []\n                if verbosity > 1:\n                    logger.info(f'Marked document for deletion: {doc_id}')\n            if len(deleted_ids) > 0:\n                yield deleted_ids\n            logger.info(f'Total {num_deleted_ids} documents marked for deletion')\n        for delete_batch in batched_docs_to_delete(emb):\n            logger.info(f'Deleting {len(delete_batch)} documents from ACS')\n            start_time = time.time()\n            results = search_client.delete_documents(delete_batch)\n            logger.info(f'First delete result: {vars(results[0])}')\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.delete_documents([doc for doc in delete_batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to delete {len(failed)} documents.')\n        include_embeddings = str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY])\n        logger.info(f'Documents include embeddings: {include_embeddings}')\n        t1 = time.time()\n        num_source_docs = 0\n        num_skipped_documents = 0\n        batch = []\n        syncing_index = acs_config.get('sync_index', acs_config.get('full_sync', False))\n        last_doc_prefix = None\n        doc_prefix_count = 0\n        skipped_prefix_documents = 0\n        for (doc_id, emb_doc) in emb._document_embeddings.items():\n            doc_prefix = doc_id.split('.')[0]\n            if doc_prefix != last_doc_prefix:\n                if doc_prefix_count > 0:\n                    logger.info(f'Processed source: {last_doc_prefix}\\nTotal Documents: {doc_prefix_count}\\nSkipped: {skipped_prefix_documents}\\nAdded: {doc_prefix_count - skipped_prefix_documents}')\n                    num_skipped_documents += skipped_prefix_documents\n                doc_prefix_count = 1\n                skipped_prefix_documents = 0\n                logger.info(f'Processing documents from: {doc_prefix}')\n                last_doc_prefix = doc_prefix\n            else:\n                doc_prefix_count += 1\n            if syncing_index and isinstance(emb_doc, ReferenceEmbeddedDocument) and (not emb_doc.is_local):\n                skipped_prefix_documents += 1\n                num_source_docs += 1\n                if verbosity > 2:\n                    logger.info(f'Skipping document as it should already be in index: {doc_id}')\n                continue\n            elif verbosity > 2:\n                logger.info(f'Pushing document to index: {doc_id}')\n            doc_source = emb_doc.metadata.get('source', {})\n            if isinstance(doc_source, str):\n                doc_source = {'url': doc_source, 'filename': emb_doc.metadata.get('filename', doc_source), 'title': emb_doc.metadata.get('title', doc_source)}\n            acs_doc = {'@search.action': 'mergeOrUpload', 'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'), acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content']: emb_doc.get_data()}\n            if 'url' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['url']] = doc_source['url']\n            if 'filename' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['filename']] = doc_source['filename']\n            if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']] = doc_source.get('title', emb_doc.metadata.get('title'))\n            if 'metadata' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['metadata']] = json.dumps(emb_doc.metadata)\n            if include_embeddings:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']] = emb_doc.get_embeddings()\n            batch.append(acs_doc)\n            if len(batch) % batch_size == 0:\n                logger.info(f'Sending {len(batch)} documents to ACS')\n                start_time = time.time()\n                results = search_client.upload_documents(batch)\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    logger.info(f'Retrying {len(failed)} documents')\n                    failed_ids = [fail['key'] for fail in failed]\n                    results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                    failed = process_upload_results(results, start_time)\n                    if len(failed) > 0:\n                        raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n                batch = []\n                num_source_docs += batch_size\n        if len(batch) > 0:\n            logger.info(f'Sending {len(batch)} documents to ACS')\n            start_time = time.time()\n            results = search_client.upload_documents(batch)\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n            num_source_docs += len(batch)\n        duration = time.time() - t1\n        logger.info(f'Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds')\n        activity_logger.info('Built index', extra={'properties': {'num_source_docs': num_source_docs, 'duration': duration}})\n        activity_logger.activity_info['num_source_docs'] = num_source_docs\n        logger.info('Writing MLIndex yaml')\n        mlindex_config = {'embeddings': emb.get_metadata()}\n        mlindex_config['index'] = {'kind': 'acs', 'engine': 'azure-sdk', 'index': acs_config['index_name'], 'api_version': acs_config.get('api_version', '2023-07-01-preview'), 'field_mapping': acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], 'semantic_configuration_name': 'azureml-default'}\n        if not isinstance(connection, DefaultAzureCredential):\n            mlindex_config['index'] = {**mlindex_config['index'], **connection}\n        mlindex_config['index']['endpoint'] = acs_config['endpoint']\n        if output_path is not None:\n            output = Path(output_path)\n            output.mkdir(parents=True, exist_ok=True)\n            with open(output / 'MLIndex', 'w') as f:\n                yaml.dump(mlindex_config, f)\n    mlindex = MLIndex(uri=output_path, mlindex_config=mlindex_config)\n    return mlindex",
        "mutated": [
            "def create_index_from_raw_embeddings(emb: EmbeddingsContainer, acs_config={}, connection={}, output_path: Optional[str]=None, credential: Optional[TokenCredential]=None, verbosity: int=1) -> MLIndex:\n    if False:\n        i = 10\n    'Upload an EmbeddingsContainer to Azure Cognitive Search and return an MLIndex.'\n    with track_activity(logger, 'update_acs', custom_dimensions={'num_documents': len(emb._document_embeddings)}) as activity_logger:\n        logger.info('Updating ACS index')\n        connection_credential = get_connection_credential(connection, credential=credential)\n        if MLIndex.INDEX_FIELD_MAPPING_KEY not in acs_config:\n            acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] = {'content': 'content', 'url': 'url', 'filename': 'filepath', 'title': 'title', 'metadata': 'meta_json_string'}\n            if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none'):\n                acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding'] = 'contentVector'\n        logger.info(f'Using Index fields: {json.dumps(acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], indent=2)}')\n        create_search_index_sdk(acs_config, connection_credential, emb)\n        search_client = search_client_from_config(acs_config, connection_credential)\n        batch_size = acs_config['batch_size'] if 'batch_size' in acs_config else 100\n\n        def process_upload_results(results, start_time):\n            succeeded = []\n            failed = []\n            for r in results:\n                if isinstance(r, dict):\n                    if r['status'] is False:\n                        failed.append(r)\n                    else:\n                        succeeded.append(r)\n                elif r.succeeded:\n                    succeeded.append(r)\n                else:\n                    failed.append(r)\n            duration = time.time() - start_time\n            logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n            activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n            if len(failed) > 0:\n                for r in failed:\n                    error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n                    logger.error(f'Failed document reason: {error}')\n                return failed\n            return []\n\n        def batched_docs_to_delete(embeddings_container) -> List[str]:\n            num_deleted_ids = 0\n            deleted_ids = []\n            for (source_id, source) in emb._deleted_sources.items():\n                logger.info(f'Deleting all documents from source: {source_id}')\n                for doc_id in source.document_ids:\n                    num_deleted_ids += 1\n                    deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                    if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                        yield deleted_ids\n                        deleted_ids = []\n                    if verbosity > 1:\n                        logger.info(f'Marked document for deletion: {doc_id}')\n            logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n            for doc_id in emb._deleted_documents:\n                num_deleted_ids += 1\n                deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                    yield deleted_ids\n                    deleted_ids = []\n                if verbosity > 1:\n                    logger.info(f'Marked document for deletion: {doc_id}')\n            if len(deleted_ids) > 0:\n                yield deleted_ids\n            logger.info(f'Total {num_deleted_ids} documents marked for deletion')\n        for delete_batch in batched_docs_to_delete(emb):\n            logger.info(f'Deleting {len(delete_batch)} documents from ACS')\n            start_time = time.time()\n            results = search_client.delete_documents(delete_batch)\n            logger.info(f'First delete result: {vars(results[0])}')\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.delete_documents([doc for doc in delete_batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to delete {len(failed)} documents.')\n        include_embeddings = str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY])\n        logger.info(f'Documents include embeddings: {include_embeddings}')\n        t1 = time.time()\n        num_source_docs = 0\n        num_skipped_documents = 0\n        batch = []\n        syncing_index = acs_config.get('sync_index', acs_config.get('full_sync', False))\n        last_doc_prefix = None\n        doc_prefix_count = 0\n        skipped_prefix_documents = 0\n        for (doc_id, emb_doc) in emb._document_embeddings.items():\n            doc_prefix = doc_id.split('.')[0]\n            if doc_prefix != last_doc_prefix:\n                if doc_prefix_count > 0:\n                    logger.info(f'Processed source: {last_doc_prefix}\\nTotal Documents: {doc_prefix_count}\\nSkipped: {skipped_prefix_documents}\\nAdded: {doc_prefix_count - skipped_prefix_documents}')\n                    num_skipped_documents += skipped_prefix_documents\n                doc_prefix_count = 1\n                skipped_prefix_documents = 0\n                logger.info(f'Processing documents from: {doc_prefix}')\n                last_doc_prefix = doc_prefix\n            else:\n                doc_prefix_count += 1\n            if syncing_index and isinstance(emb_doc, ReferenceEmbeddedDocument) and (not emb_doc.is_local):\n                skipped_prefix_documents += 1\n                num_source_docs += 1\n                if verbosity > 2:\n                    logger.info(f'Skipping document as it should already be in index: {doc_id}')\n                continue\n            elif verbosity > 2:\n                logger.info(f'Pushing document to index: {doc_id}')\n            doc_source = emb_doc.metadata.get('source', {})\n            if isinstance(doc_source, str):\n                doc_source = {'url': doc_source, 'filename': emb_doc.metadata.get('filename', doc_source), 'title': emb_doc.metadata.get('title', doc_source)}\n            acs_doc = {'@search.action': 'mergeOrUpload', 'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'), acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content']: emb_doc.get_data()}\n            if 'url' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['url']] = doc_source['url']\n            if 'filename' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['filename']] = doc_source['filename']\n            if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']] = doc_source.get('title', emb_doc.metadata.get('title'))\n            if 'metadata' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['metadata']] = json.dumps(emb_doc.metadata)\n            if include_embeddings:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']] = emb_doc.get_embeddings()\n            batch.append(acs_doc)\n            if len(batch) % batch_size == 0:\n                logger.info(f'Sending {len(batch)} documents to ACS')\n                start_time = time.time()\n                results = search_client.upload_documents(batch)\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    logger.info(f'Retrying {len(failed)} documents')\n                    failed_ids = [fail['key'] for fail in failed]\n                    results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                    failed = process_upload_results(results, start_time)\n                    if len(failed) > 0:\n                        raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n                batch = []\n                num_source_docs += batch_size\n        if len(batch) > 0:\n            logger.info(f'Sending {len(batch)} documents to ACS')\n            start_time = time.time()\n            results = search_client.upload_documents(batch)\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n            num_source_docs += len(batch)\n        duration = time.time() - t1\n        logger.info(f'Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds')\n        activity_logger.info('Built index', extra={'properties': {'num_source_docs': num_source_docs, 'duration': duration}})\n        activity_logger.activity_info['num_source_docs'] = num_source_docs\n        logger.info('Writing MLIndex yaml')\n        mlindex_config = {'embeddings': emb.get_metadata()}\n        mlindex_config['index'] = {'kind': 'acs', 'engine': 'azure-sdk', 'index': acs_config['index_name'], 'api_version': acs_config.get('api_version', '2023-07-01-preview'), 'field_mapping': acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], 'semantic_configuration_name': 'azureml-default'}\n        if not isinstance(connection, DefaultAzureCredential):\n            mlindex_config['index'] = {**mlindex_config['index'], **connection}\n        mlindex_config['index']['endpoint'] = acs_config['endpoint']\n        if output_path is not None:\n            output = Path(output_path)\n            output.mkdir(parents=True, exist_ok=True)\n            with open(output / 'MLIndex', 'w') as f:\n                yaml.dump(mlindex_config, f)\n    mlindex = MLIndex(uri=output_path, mlindex_config=mlindex_config)\n    return mlindex",
            "def create_index_from_raw_embeddings(emb: EmbeddingsContainer, acs_config={}, connection={}, output_path: Optional[str]=None, credential: Optional[TokenCredential]=None, verbosity: int=1) -> MLIndex:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Upload an EmbeddingsContainer to Azure Cognitive Search and return an MLIndex.'\n    with track_activity(logger, 'update_acs', custom_dimensions={'num_documents': len(emb._document_embeddings)}) as activity_logger:\n        logger.info('Updating ACS index')\n        connection_credential = get_connection_credential(connection, credential=credential)\n        if MLIndex.INDEX_FIELD_MAPPING_KEY not in acs_config:\n            acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] = {'content': 'content', 'url': 'url', 'filename': 'filepath', 'title': 'title', 'metadata': 'meta_json_string'}\n            if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none'):\n                acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding'] = 'contentVector'\n        logger.info(f'Using Index fields: {json.dumps(acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], indent=2)}')\n        create_search_index_sdk(acs_config, connection_credential, emb)\n        search_client = search_client_from_config(acs_config, connection_credential)\n        batch_size = acs_config['batch_size'] if 'batch_size' in acs_config else 100\n\n        def process_upload_results(results, start_time):\n            succeeded = []\n            failed = []\n            for r in results:\n                if isinstance(r, dict):\n                    if r['status'] is False:\n                        failed.append(r)\n                    else:\n                        succeeded.append(r)\n                elif r.succeeded:\n                    succeeded.append(r)\n                else:\n                    failed.append(r)\n            duration = time.time() - start_time\n            logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n            activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n            if len(failed) > 0:\n                for r in failed:\n                    error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n                    logger.error(f'Failed document reason: {error}')\n                return failed\n            return []\n\n        def batched_docs_to_delete(embeddings_container) -> List[str]:\n            num_deleted_ids = 0\n            deleted_ids = []\n            for (source_id, source) in emb._deleted_sources.items():\n                logger.info(f'Deleting all documents from source: {source_id}')\n                for doc_id in source.document_ids:\n                    num_deleted_ids += 1\n                    deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                    if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                        yield deleted_ids\n                        deleted_ids = []\n                    if verbosity > 1:\n                        logger.info(f'Marked document for deletion: {doc_id}')\n            logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n            for doc_id in emb._deleted_documents:\n                num_deleted_ids += 1\n                deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                    yield deleted_ids\n                    deleted_ids = []\n                if verbosity > 1:\n                    logger.info(f'Marked document for deletion: {doc_id}')\n            if len(deleted_ids) > 0:\n                yield deleted_ids\n            logger.info(f'Total {num_deleted_ids} documents marked for deletion')\n        for delete_batch in batched_docs_to_delete(emb):\n            logger.info(f'Deleting {len(delete_batch)} documents from ACS')\n            start_time = time.time()\n            results = search_client.delete_documents(delete_batch)\n            logger.info(f'First delete result: {vars(results[0])}')\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.delete_documents([doc for doc in delete_batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to delete {len(failed)} documents.')\n        include_embeddings = str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY])\n        logger.info(f'Documents include embeddings: {include_embeddings}')\n        t1 = time.time()\n        num_source_docs = 0\n        num_skipped_documents = 0\n        batch = []\n        syncing_index = acs_config.get('sync_index', acs_config.get('full_sync', False))\n        last_doc_prefix = None\n        doc_prefix_count = 0\n        skipped_prefix_documents = 0\n        for (doc_id, emb_doc) in emb._document_embeddings.items():\n            doc_prefix = doc_id.split('.')[0]\n            if doc_prefix != last_doc_prefix:\n                if doc_prefix_count > 0:\n                    logger.info(f'Processed source: {last_doc_prefix}\\nTotal Documents: {doc_prefix_count}\\nSkipped: {skipped_prefix_documents}\\nAdded: {doc_prefix_count - skipped_prefix_documents}')\n                    num_skipped_documents += skipped_prefix_documents\n                doc_prefix_count = 1\n                skipped_prefix_documents = 0\n                logger.info(f'Processing documents from: {doc_prefix}')\n                last_doc_prefix = doc_prefix\n            else:\n                doc_prefix_count += 1\n            if syncing_index and isinstance(emb_doc, ReferenceEmbeddedDocument) and (not emb_doc.is_local):\n                skipped_prefix_documents += 1\n                num_source_docs += 1\n                if verbosity > 2:\n                    logger.info(f'Skipping document as it should already be in index: {doc_id}')\n                continue\n            elif verbosity > 2:\n                logger.info(f'Pushing document to index: {doc_id}')\n            doc_source = emb_doc.metadata.get('source', {})\n            if isinstance(doc_source, str):\n                doc_source = {'url': doc_source, 'filename': emb_doc.metadata.get('filename', doc_source), 'title': emb_doc.metadata.get('title', doc_source)}\n            acs_doc = {'@search.action': 'mergeOrUpload', 'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'), acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content']: emb_doc.get_data()}\n            if 'url' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['url']] = doc_source['url']\n            if 'filename' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['filename']] = doc_source['filename']\n            if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']] = doc_source.get('title', emb_doc.metadata.get('title'))\n            if 'metadata' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['metadata']] = json.dumps(emb_doc.metadata)\n            if include_embeddings:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']] = emb_doc.get_embeddings()\n            batch.append(acs_doc)\n            if len(batch) % batch_size == 0:\n                logger.info(f'Sending {len(batch)} documents to ACS')\n                start_time = time.time()\n                results = search_client.upload_documents(batch)\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    logger.info(f'Retrying {len(failed)} documents')\n                    failed_ids = [fail['key'] for fail in failed]\n                    results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                    failed = process_upload_results(results, start_time)\n                    if len(failed) > 0:\n                        raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n                batch = []\n                num_source_docs += batch_size\n        if len(batch) > 0:\n            logger.info(f'Sending {len(batch)} documents to ACS')\n            start_time = time.time()\n            results = search_client.upload_documents(batch)\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n            num_source_docs += len(batch)\n        duration = time.time() - t1\n        logger.info(f'Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds')\n        activity_logger.info('Built index', extra={'properties': {'num_source_docs': num_source_docs, 'duration': duration}})\n        activity_logger.activity_info['num_source_docs'] = num_source_docs\n        logger.info('Writing MLIndex yaml')\n        mlindex_config = {'embeddings': emb.get_metadata()}\n        mlindex_config['index'] = {'kind': 'acs', 'engine': 'azure-sdk', 'index': acs_config['index_name'], 'api_version': acs_config.get('api_version', '2023-07-01-preview'), 'field_mapping': acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], 'semantic_configuration_name': 'azureml-default'}\n        if not isinstance(connection, DefaultAzureCredential):\n            mlindex_config['index'] = {**mlindex_config['index'], **connection}\n        mlindex_config['index']['endpoint'] = acs_config['endpoint']\n        if output_path is not None:\n            output = Path(output_path)\n            output.mkdir(parents=True, exist_ok=True)\n            with open(output / 'MLIndex', 'w') as f:\n                yaml.dump(mlindex_config, f)\n    mlindex = MLIndex(uri=output_path, mlindex_config=mlindex_config)\n    return mlindex",
            "def create_index_from_raw_embeddings(emb: EmbeddingsContainer, acs_config={}, connection={}, output_path: Optional[str]=None, credential: Optional[TokenCredential]=None, verbosity: int=1) -> MLIndex:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Upload an EmbeddingsContainer to Azure Cognitive Search and return an MLIndex.'\n    with track_activity(logger, 'update_acs', custom_dimensions={'num_documents': len(emb._document_embeddings)}) as activity_logger:\n        logger.info('Updating ACS index')\n        connection_credential = get_connection_credential(connection, credential=credential)\n        if MLIndex.INDEX_FIELD_MAPPING_KEY not in acs_config:\n            acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] = {'content': 'content', 'url': 'url', 'filename': 'filepath', 'title': 'title', 'metadata': 'meta_json_string'}\n            if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none'):\n                acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding'] = 'contentVector'\n        logger.info(f'Using Index fields: {json.dumps(acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], indent=2)}')\n        create_search_index_sdk(acs_config, connection_credential, emb)\n        search_client = search_client_from_config(acs_config, connection_credential)\n        batch_size = acs_config['batch_size'] if 'batch_size' in acs_config else 100\n\n        def process_upload_results(results, start_time):\n            succeeded = []\n            failed = []\n            for r in results:\n                if isinstance(r, dict):\n                    if r['status'] is False:\n                        failed.append(r)\n                    else:\n                        succeeded.append(r)\n                elif r.succeeded:\n                    succeeded.append(r)\n                else:\n                    failed.append(r)\n            duration = time.time() - start_time\n            logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n            activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n            if len(failed) > 0:\n                for r in failed:\n                    error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n                    logger.error(f'Failed document reason: {error}')\n                return failed\n            return []\n\n        def batched_docs_to_delete(embeddings_container) -> List[str]:\n            num_deleted_ids = 0\n            deleted_ids = []\n            for (source_id, source) in emb._deleted_sources.items():\n                logger.info(f'Deleting all documents from source: {source_id}')\n                for doc_id in source.document_ids:\n                    num_deleted_ids += 1\n                    deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                    if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                        yield deleted_ids\n                        deleted_ids = []\n                    if verbosity > 1:\n                        logger.info(f'Marked document for deletion: {doc_id}')\n            logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n            for doc_id in emb._deleted_documents:\n                num_deleted_ids += 1\n                deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                    yield deleted_ids\n                    deleted_ids = []\n                if verbosity > 1:\n                    logger.info(f'Marked document for deletion: {doc_id}')\n            if len(deleted_ids) > 0:\n                yield deleted_ids\n            logger.info(f'Total {num_deleted_ids} documents marked for deletion')\n        for delete_batch in batched_docs_to_delete(emb):\n            logger.info(f'Deleting {len(delete_batch)} documents from ACS')\n            start_time = time.time()\n            results = search_client.delete_documents(delete_batch)\n            logger.info(f'First delete result: {vars(results[0])}')\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.delete_documents([doc for doc in delete_batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to delete {len(failed)} documents.')\n        include_embeddings = str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY])\n        logger.info(f'Documents include embeddings: {include_embeddings}')\n        t1 = time.time()\n        num_source_docs = 0\n        num_skipped_documents = 0\n        batch = []\n        syncing_index = acs_config.get('sync_index', acs_config.get('full_sync', False))\n        last_doc_prefix = None\n        doc_prefix_count = 0\n        skipped_prefix_documents = 0\n        for (doc_id, emb_doc) in emb._document_embeddings.items():\n            doc_prefix = doc_id.split('.')[0]\n            if doc_prefix != last_doc_prefix:\n                if doc_prefix_count > 0:\n                    logger.info(f'Processed source: {last_doc_prefix}\\nTotal Documents: {doc_prefix_count}\\nSkipped: {skipped_prefix_documents}\\nAdded: {doc_prefix_count - skipped_prefix_documents}')\n                    num_skipped_documents += skipped_prefix_documents\n                doc_prefix_count = 1\n                skipped_prefix_documents = 0\n                logger.info(f'Processing documents from: {doc_prefix}')\n                last_doc_prefix = doc_prefix\n            else:\n                doc_prefix_count += 1\n            if syncing_index and isinstance(emb_doc, ReferenceEmbeddedDocument) and (not emb_doc.is_local):\n                skipped_prefix_documents += 1\n                num_source_docs += 1\n                if verbosity > 2:\n                    logger.info(f'Skipping document as it should already be in index: {doc_id}')\n                continue\n            elif verbosity > 2:\n                logger.info(f'Pushing document to index: {doc_id}')\n            doc_source = emb_doc.metadata.get('source', {})\n            if isinstance(doc_source, str):\n                doc_source = {'url': doc_source, 'filename': emb_doc.metadata.get('filename', doc_source), 'title': emb_doc.metadata.get('title', doc_source)}\n            acs_doc = {'@search.action': 'mergeOrUpload', 'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'), acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content']: emb_doc.get_data()}\n            if 'url' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['url']] = doc_source['url']\n            if 'filename' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['filename']] = doc_source['filename']\n            if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']] = doc_source.get('title', emb_doc.metadata.get('title'))\n            if 'metadata' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['metadata']] = json.dumps(emb_doc.metadata)\n            if include_embeddings:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']] = emb_doc.get_embeddings()\n            batch.append(acs_doc)\n            if len(batch) % batch_size == 0:\n                logger.info(f'Sending {len(batch)} documents to ACS')\n                start_time = time.time()\n                results = search_client.upload_documents(batch)\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    logger.info(f'Retrying {len(failed)} documents')\n                    failed_ids = [fail['key'] for fail in failed]\n                    results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                    failed = process_upload_results(results, start_time)\n                    if len(failed) > 0:\n                        raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n                batch = []\n                num_source_docs += batch_size\n        if len(batch) > 0:\n            logger.info(f'Sending {len(batch)} documents to ACS')\n            start_time = time.time()\n            results = search_client.upload_documents(batch)\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n            num_source_docs += len(batch)\n        duration = time.time() - t1\n        logger.info(f'Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds')\n        activity_logger.info('Built index', extra={'properties': {'num_source_docs': num_source_docs, 'duration': duration}})\n        activity_logger.activity_info['num_source_docs'] = num_source_docs\n        logger.info('Writing MLIndex yaml')\n        mlindex_config = {'embeddings': emb.get_metadata()}\n        mlindex_config['index'] = {'kind': 'acs', 'engine': 'azure-sdk', 'index': acs_config['index_name'], 'api_version': acs_config.get('api_version', '2023-07-01-preview'), 'field_mapping': acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], 'semantic_configuration_name': 'azureml-default'}\n        if not isinstance(connection, DefaultAzureCredential):\n            mlindex_config['index'] = {**mlindex_config['index'], **connection}\n        mlindex_config['index']['endpoint'] = acs_config['endpoint']\n        if output_path is not None:\n            output = Path(output_path)\n            output.mkdir(parents=True, exist_ok=True)\n            with open(output / 'MLIndex', 'w') as f:\n                yaml.dump(mlindex_config, f)\n    mlindex = MLIndex(uri=output_path, mlindex_config=mlindex_config)\n    return mlindex",
            "def create_index_from_raw_embeddings(emb: EmbeddingsContainer, acs_config={}, connection={}, output_path: Optional[str]=None, credential: Optional[TokenCredential]=None, verbosity: int=1) -> MLIndex:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Upload an EmbeddingsContainer to Azure Cognitive Search and return an MLIndex.'\n    with track_activity(logger, 'update_acs', custom_dimensions={'num_documents': len(emb._document_embeddings)}) as activity_logger:\n        logger.info('Updating ACS index')\n        connection_credential = get_connection_credential(connection, credential=credential)\n        if MLIndex.INDEX_FIELD_MAPPING_KEY not in acs_config:\n            acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] = {'content': 'content', 'url': 'url', 'filename': 'filepath', 'title': 'title', 'metadata': 'meta_json_string'}\n            if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none'):\n                acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding'] = 'contentVector'\n        logger.info(f'Using Index fields: {json.dumps(acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], indent=2)}')\n        create_search_index_sdk(acs_config, connection_credential, emb)\n        search_client = search_client_from_config(acs_config, connection_credential)\n        batch_size = acs_config['batch_size'] if 'batch_size' in acs_config else 100\n\n        def process_upload_results(results, start_time):\n            succeeded = []\n            failed = []\n            for r in results:\n                if isinstance(r, dict):\n                    if r['status'] is False:\n                        failed.append(r)\n                    else:\n                        succeeded.append(r)\n                elif r.succeeded:\n                    succeeded.append(r)\n                else:\n                    failed.append(r)\n            duration = time.time() - start_time\n            logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n            activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n            if len(failed) > 0:\n                for r in failed:\n                    error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n                    logger.error(f'Failed document reason: {error}')\n                return failed\n            return []\n\n        def batched_docs_to_delete(embeddings_container) -> List[str]:\n            num_deleted_ids = 0\n            deleted_ids = []\n            for (source_id, source) in emb._deleted_sources.items():\n                logger.info(f'Deleting all documents from source: {source_id}')\n                for doc_id in source.document_ids:\n                    num_deleted_ids += 1\n                    deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                    if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                        yield deleted_ids\n                        deleted_ids = []\n                    if verbosity > 1:\n                        logger.info(f'Marked document for deletion: {doc_id}')\n            logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n            for doc_id in emb._deleted_documents:\n                num_deleted_ids += 1\n                deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                    yield deleted_ids\n                    deleted_ids = []\n                if verbosity > 1:\n                    logger.info(f'Marked document for deletion: {doc_id}')\n            if len(deleted_ids) > 0:\n                yield deleted_ids\n            logger.info(f'Total {num_deleted_ids} documents marked for deletion')\n        for delete_batch in batched_docs_to_delete(emb):\n            logger.info(f'Deleting {len(delete_batch)} documents from ACS')\n            start_time = time.time()\n            results = search_client.delete_documents(delete_batch)\n            logger.info(f'First delete result: {vars(results[0])}')\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.delete_documents([doc for doc in delete_batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to delete {len(failed)} documents.')\n        include_embeddings = str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY])\n        logger.info(f'Documents include embeddings: {include_embeddings}')\n        t1 = time.time()\n        num_source_docs = 0\n        num_skipped_documents = 0\n        batch = []\n        syncing_index = acs_config.get('sync_index', acs_config.get('full_sync', False))\n        last_doc_prefix = None\n        doc_prefix_count = 0\n        skipped_prefix_documents = 0\n        for (doc_id, emb_doc) in emb._document_embeddings.items():\n            doc_prefix = doc_id.split('.')[0]\n            if doc_prefix != last_doc_prefix:\n                if doc_prefix_count > 0:\n                    logger.info(f'Processed source: {last_doc_prefix}\\nTotal Documents: {doc_prefix_count}\\nSkipped: {skipped_prefix_documents}\\nAdded: {doc_prefix_count - skipped_prefix_documents}')\n                    num_skipped_documents += skipped_prefix_documents\n                doc_prefix_count = 1\n                skipped_prefix_documents = 0\n                logger.info(f'Processing documents from: {doc_prefix}')\n                last_doc_prefix = doc_prefix\n            else:\n                doc_prefix_count += 1\n            if syncing_index and isinstance(emb_doc, ReferenceEmbeddedDocument) and (not emb_doc.is_local):\n                skipped_prefix_documents += 1\n                num_source_docs += 1\n                if verbosity > 2:\n                    logger.info(f'Skipping document as it should already be in index: {doc_id}')\n                continue\n            elif verbosity > 2:\n                logger.info(f'Pushing document to index: {doc_id}')\n            doc_source = emb_doc.metadata.get('source', {})\n            if isinstance(doc_source, str):\n                doc_source = {'url': doc_source, 'filename': emb_doc.metadata.get('filename', doc_source), 'title': emb_doc.metadata.get('title', doc_source)}\n            acs_doc = {'@search.action': 'mergeOrUpload', 'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'), acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content']: emb_doc.get_data()}\n            if 'url' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['url']] = doc_source['url']\n            if 'filename' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['filename']] = doc_source['filename']\n            if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']] = doc_source.get('title', emb_doc.metadata.get('title'))\n            if 'metadata' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['metadata']] = json.dumps(emb_doc.metadata)\n            if include_embeddings:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']] = emb_doc.get_embeddings()\n            batch.append(acs_doc)\n            if len(batch) % batch_size == 0:\n                logger.info(f'Sending {len(batch)} documents to ACS')\n                start_time = time.time()\n                results = search_client.upload_documents(batch)\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    logger.info(f'Retrying {len(failed)} documents')\n                    failed_ids = [fail['key'] for fail in failed]\n                    results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                    failed = process_upload_results(results, start_time)\n                    if len(failed) > 0:\n                        raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n                batch = []\n                num_source_docs += batch_size\n        if len(batch) > 0:\n            logger.info(f'Sending {len(batch)} documents to ACS')\n            start_time = time.time()\n            results = search_client.upload_documents(batch)\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n            num_source_docs += len(batch)\n        duration = time.time() - t1\n        logger.info(f'Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds')\n        activity_logger.info('Built index', extra={'properties': {'num_source_docs': num_source_docs, 'duration': duration}})\n        activity_logger.activity_info['num_source_docs'] = num_source_docs\n        logger.info('Writing MLIndex yaml')\n        mlindex_config = {'embeddings': emb.get_metadata()}\n        mlindex_config['index'] = {'kind': 'acs', 'engine': 'azure-sdk', 'index': acs_config['index_name'], 'api_version': acs_config.get('api_version', '2023-07-01-preview'), 'field_mapping': acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], 'semantic_configuration_name': 'azureml-default'}\n        if not isinstance(connection, DefaultAzureCredential):\n            mlindex_config['index'] = {**mlindex_config['index'], **connection}\n        mlindex_config['index']['endpoint'] = acs_config['endpoint']\n        if output_path is not None:\n            output = Path(output_path)\n            output.mkdir(parents=True, exist_ok=True)\n            with open(output / 'MLIndex', 'w') as f:\n                yaml.dump(mlindex_config, f)\n    mlindex = MLIndex(uri=output_path, mlindex_config=mlindex_config)\n    return mlindex",
            "def create_index_from_raw_embeddings(emb: EmbeddingsContainer, acs_config={}, connection={}, output_path: Optional[str]=None, credential: Optional[TokenCredential]=None, verbosity: int=1) -> MLIndex:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Upload an EmbeddingsContainer to Azure Cognitive Search and return an MLIndex.'\n    with track_activity(logger, 'update_acs', custom_dimensions={'num_documents': len(emb._document_embeddings)}) as activity_logger:\n        logger.info('Updating ACS index')\n        connection_credential = get_connection_credential(connection, credential=credential)\n        if MLIndex.INDEX_FIELD_MAPPING_KEY not in acs_config:\n            acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY] = {'content': 'content', 'url': 'url', 'filename': 'filepath', 'title': 'title', 'metadata': 'meta_json_string'}\n            if str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none'):\n                acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding'] = 'contentVector'\n        logger.info(f'Using Index fields: {json.dumps(acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], indent=2)}')\n        create_search_index_sdk(acs_config, connection_credential, emb)\n        search_client = search_client_from_config(acs_config, connection_credential)\n        batch_size = acs_config['batch_size'] if 'batch_size' in acs_config else 100\n\n        def process_upload_results(results, start_time):\n            succeeded = []\n            failed = []\n            for r in results:\n                if isinstance(r, dict):\n                    if r['status'] is False:\n                        failed.append(r)\n                    else:\n                        succeeded.append(r)\n                elif r.succeeded:\n                    succeeded.append(r)\n                else:\n                    failed.append(r)\n            duration = time.time() - start_time\n            logger.info(f'Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed')\n            activity_logger.info('Uploaded documents', extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})\n            if len(failed) > 0:\n                for r in failed:\n                    error = r['errorMessage'] if isinstance(r, dict) else r.error_message\n                    logger.error(f'Failed document reason: {error}')\n                return failed\n            return []\n\n        def batched_docs_to_delete(embeddings_container) -> List[str]:\n            num_deleted_ids = 0\n            deleted_ids = []\n            for (source_id, source) in emb._deleted_sources.items():\n                logger.info(f'Deleting all documents from source: {source_id}')\n                for doc_id in source.document_ids:\n                    num_deleted_ids += 1\n                    deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                    if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                        yield deleted_ids\n                        deleted_ids = []\n                    if verbosity > 1:\n                        logger.info(f'Marked document for deletion: {doc_id}')\n            logger.info(f'{len(deleted_ids)} documents from sources marked for deletion, adding individual documents marked for deletion')\n            for doc_id in emb._deleted_documents:\n                num_deleted_ids += 1\n                deleted_ids.append({'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8')})\n                if len(deleted_ids) == AZURE_SEARCH_DOCUMENT_ACTION_BATCH_LIMIT:\n                    yield deleted_ids\n                    deleted_ids = []\n                if verbosity > 1:\n                    logger.info(f'Marked document for deletion: {doc_id}')\n            if len(deleted_ids) > 0:\n                yield deleted_ids\n            logger.info(f'Total {num_deleted_ids} documents marked for deletion')\n        for delete_batch in batched_docs_to_delete(emb):\n            logger.info(f'Deleting {len(delete_batch)} documents from ACS')\n            start_time = time.time()\n            results = search_client.delete_documents(delete_batch)\n            logger.info(f'First delete result: {vars(results[0])}')\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.delete_documents([doc for doc in delete_batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to delete {len(failed)} documents.')\n        include_embeddings = str(acs_config.get('push_embeddings', 'true')).lower() == 'true' and emb and (emb.kind != 'none') and ('embedding' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY])\n        logger.info(f'Documents include embeddings: {include_embeddings}')\n        t1 = time.time()\n        num_source_docs = 0\n        num_skipped_documents = 0\n        batch = []\n        syncing_index = acs_config.get('sync_index', acs_config.get('full_sync', False))\n        last_doc_prefix = None\n        doc_prefix_count = 0\n        skipped_prefix_documents = 0\n        for (doc_id, emb_doc) in emb._document_embeddings.items():\n            doc_prefix = doc_id.split('.')[0]\n            if doc_prefix != last_doc_prefix:\n                if doc_prefix_count > 0:\n                    logger.info(f'Processed source: {last_doc_prefix}\\nTotal Documents: {doc_prefix_count}\\nSkipped: {skipped_prefix_documents}\\nAdded: {doc_prefix_count - skipped_prefix_documents}')\n                    num_skipped_documents += skipped_prefix_documents\n                doc_prefix_count = 1\n                skipped_prefix_documents = 0\n                logger.info(f'Processing documents from: {doc_prefix}')\n                last_doc_prefix = doc_prefix\n            else:\n                doc_prefix_count += 1\n            if syncing_index and isinstance(emb_doc, ReferenceEmbeddedDocument) and (not emb_doc.is_local):\n                skipped_prefix_documents += 1\n                num_source_docs += 1\n                if verbosity > 2:\n                    logger.info(f'Skipping document as it should already be in index: {doc_id}')\n                continue\n            elif verbosity > 2:\n                logger.info(f'Pushing document to index: {doc_id}')\n            doc_source = emb_doc.metadata.get('source', {})\n            if isinstance(doc_source, str):\n                doc_source = {'url': doc_source, 'filename': emb_doc.metadata.get('filename', doc_source), 'title': emb_doc.metadata.get('title', doc_source)}\n            acs_doc = {'@search.action': 'mergeOrUpload', 'id': base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'), acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['content']: emb_doc.get_data()}\n            if 'url' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['url']] = doc_source['url']\n            if 'filename' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['filename']] = doc_source['filename']\n            if 'title' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['title']] = doc_source.get('title', emb_doc.metadata.get('title'))\n            if 'metadata' in acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['metadata']] = json.dumps(emb_doc.metadata)\n            if include_embeddings:\n                acs_doc[acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY]['embedding']] = emb_doc.get_embeddings()\n            batch.append(acs_doc)\n            if len(batch) % batch_size == 0:\n                logger.info(f'Sending {len(batch)} documents to ACS')\n                start_time = time.time()\n                results = search_client.upload_documents(batch)\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    logger.info(f'Retrying {len(failed)} documents')\n                    failed_ids = [fail['key'] for fail in failed]\n                    results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                    failed = process_upload_results(results, start_time)\n                    if len(failed) > 0:\n                        raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n                batch = []\n                num_source_docs += batch_size\n        if len(batch) > 0:\n            logger.info(f'Sending {len(batch)} documents to ACS')\n            start_time = time.time()\n            results = search_client.upload_documents(batch)\n            failed = process_upload_results(results, start_time)\n            if len(failed) > 0:\n                logger.info(f'Retrying {len(failed)} documents')\n                failed_ids = [fail['key'] for fail in failed]\n                results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])\n                failed = process_upload_results(results, start_time)\n                if len(failed) > 0:\n                    raise RuntimeError(f'Failed to upload {len(failed)} documents.')\n            num_source_docs += len(batch)\n        duration = time.time() - t1\n        logger.info(f'Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds')\n        activity_logger.info('Built index', extra={'properties': {'num_source_docs': num_source_docs, 'duration': duration}})\n        activity_logger.activity_info['num_source_docs'] = num_source_docs\n        logger.info('Writing MLIndex yaml')\n        mlindex_config = {'embeddings': emb.get_metadata()}\n        mlindex_config['index'] = {'kind': 'acs', 'engine': 'azure-sdk', 'index': acs_config['index_name'], 'api_version': acs_config.get('api_version', '2023-07-01-preview'), 'field_mapping': acs_config[MLIndex.INDEX_FIELD_MAPPING_KEY], 'semantic_configuration_name': 'azureml-default'}\n        if not isinstance(connection, DefaultAzureCredential):\n            mlindex_config['index'] = {**mlindex_config['index'], **connection}\n        mlindex_config['index']['endpoint'] = acs_config['endpoint']\n        if output_path is not None:\n            output = Path(output_path)\n            output.mkdir(parents=True, exist_ok=True)\n            with open(output / 'MLIndex', 'w') as f:\n                yaml.dump(mlindex_config, f)\n    mlindex = MLIndex(uri=output_path, mlindex_config=mlindex_config)\n    return mlindex"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(args, logger, activity_logger):\n    try:\n        try:\n            acs_config = json.loads(args.acs_config)\n        except Exception as e:\n            logger.error(f'Failed to parse acs_config as json: {e}')\n            activity_logger.error('Failed to parse acs_config as json')\n            raise\n        connection_args = {}\n        if args.connection_id is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            connection_args['connection'] = {'id': args.connection_id}\n            from azure.ai.generative.index._utils.connections import get_connection_by_id_v2, get_metadata_from_connection, get_target_from_connection\n            connection = get_connection_by_id_v2(args.connection_id)\n            acs_config['endpoint'] = get_target_from_connection(connection)\n            acs_config['api_version'] = get_metadata_from_connection(connection).get('apiVersion', '2023-07-01-preview')\n        elif 'endpoint_key_name' in acs_config:\n            connection_args['connection_type'] = 'workspace_keyvault'\n            from azureml.core import Run\n            run = Run.get_context()\n            ws = run.experiment.workspace\n            connection_args['connection'] = {'key': acs_config['endpoint_key_name'], 'subscription': ws.subscription_id, 'resource_group': ws.resource_group, 'workspace': ws.name}\n        raw_embeddings_uri = args.embeddings\n        logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')\n        splits = raw_embeddings_uri.split('/')\n        embeddings_dir_name = splits.pop(len(splits) - 2)\n        logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')\n        parent = '/'.join(splits)\n        logger.info(f'extracted embeddings container path: {parent}')\n        from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount\n        mnt_options = MountOptions(default_permission=365, allow_other=False, read_only=True)\n        logger.info(f'mounting embeddings container from: \\n{parent} \\n   to: \\n{os.getcwd()}/embeddings_mount')\n        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:\n            emb = EmbeddingsContainer.load(embeddings_dir_name, mount_context.mount_point)\n            create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output, verbosity=args.verbosity)\n    except Exception as e:\n        logger.error('Failed to update ACS index')\n        exception_str = str(e)\n        if 'Floats quota has been exceeded for this service.' in exception_str:\n            logger.error('Floats quota exceeded on Azure Cognitive Search Service. For more information check these docs: https://github.com/Azure/cognitive-search-vector-pr#storage-and-vector-index-size-limits')\n            logger.error('The usage statistic of an index can be checked using this REST API: https://learn.microsoft.com/en-us/rest/api/searchservice/get-index-statistics ')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Floats quota has been exceeded for this service.'\n        elif 'Cannot find nested property' in exception_str:\n            logger.error(f'''The vector index provided \"{acs_config['index_name']}\" has a different schema than outlined in this components description. This can happen if a different embedding model was used previously when updating this index.''')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Cannot find nested property'\n        elif 'Failed to upload' in exception_str:\n            activity_logger.activity_info['error'] = exception_str\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        else:\n            activity_logger.activity_info['error'] = str(e.__class__.__name__)\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        raise e\n    logger.info('Updated ACS index')",
        "mutated": [
            "def main(args, logger, activity_logger):\n    if False:\n        i = 10\n    try:\n        try:\n            acs_config = json.loads(args.acs_config)\n        except Exception as e:\n            logger.error(f'Failed to parse acs_config as json: {e}')\n            activity_logger.error('Failed to parse acs_config as json')\n            raise\n        connection_args = {}\n        if args.connection_id is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            connection_args['connection'] = {'id': args.connection_id}\n            from azure.ai.generative.index._utils.connections import get_connection_by_id_v2, get_metadata_from_connection, get_target_from_connection\n            connection = get_connection_by_id_v2(args.connection_id)\n            acs_config['endpoint'] = get_target_from_connection(connection)\n            acs_config['api_version'] = get_metadata_from_connection(connection).get('apiVersion', '2023-07-01-preview')\n        elif 'endpoint_key_name' in acs_config:\n            connection_args['connection_type'] = 'workspace_keyvault'\n            from azureml.core import Run\n            run = Run.get_context()\n            ws = run.experiment.workspace\n            connection_args['connection'] = {'key': acs_config['endpoint_key_name'], 'subscription': ws.subscription_id, 'resource_group': ws.resource_group, 'workspace': ws.name}\n        raw_embeddings_uri = args.embeddings\n        logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')\n        splits = raw_embeddings_uri.split('/')\n        embeddings_dir_name = splits.pop(len(splits) - 2)\n        logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')\n        parent = '/'.join(splits)\n        logger.info(f'extracted embeddings container path: {parent}')\n        from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount\n        mnt_options = MountOptions(default_permission=365, allow_other=False, read_only=True)\n        logger.info(f'mounting embeddings container from: \\n{parent} \\n   to: \\n{os.getcwd()}/embeddings_mount')\n        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:\n            emb = EmbeddingsContainer.load(embeddings_dir_name, mount_context.mount_point)\n            create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output, verbosity=args.verbosity)\n    except Exception as e:\n        logger.error('Failed to update ACS index')\n        exception_str = str(e)\n        if 'Floats quota has been exceeded for this service.' in exception_str:\n            logger.error('Floats quota exceeded on Azure Cognitive Search Service. For more information check these docs: https://github.com/Azure/cognitive-search-vector-pr#storage-and-vector-index-size-limits')\n            logger.error('The usage statistic of an index can be checked using this REST API: https://learn.microsoft.com/en-us/rest/api/searchservice/get-index-statistics ')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Floats quota has been exceeded for this service.'\n        elif 'Cannot find nested property' in exception_str:\n            logger.error(f'''The vector index provided \"{acs_config['index_name']}\" has a different schema than outlined in this components description. This can happen if a different embedding model was used previously when updating this index.''')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Cannot find nested property'\n        elif 'Failed to upload' in exception_str:\n            activity_logger.activity_info['error'] = exception_str\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        else:\n            activity_logger.activity_info['error'] = str(e.__class__.__name__)\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        raise e\n    logger.info('Updated ACS index')",
            "def main(args, logger, activity_logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    try:\n        try:\n            acs_config = json.loads(args.acs_config)\n        except Exception as e:\n            logger.error(f'Failed to parse acs_config as json: {e}')\n            activity_logger.error('Failed to parse acs_config as json')\n            raise\n        connection_args = {}\n        if args.connection_id is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            connection_args['connection'] = {'id': args.connection_id}\n            from azure.ai.generative.index._utils.connections import get_connection_by_id_v2, get_metadata_from_connection, get_target_from_connection\n            connection = get_connection_by_id_v2(args.connection_id)\n            acs_config['endpoint'] = get_target_from_connection(connection)\n            acs_config['api_version'] = get_metadata_from_connection(connection).get('apiVersion', '2023-07-01-preview')\n        elif 'endpoint_key_name' in acs_config:\n            connection_args['connection_type'] = 'workspace_keyvault'\n            from azureml.core import Run\n            run = Run.get_context()\n            ws = run.experiment.workspace\n            connection_args['connection'] = {'key': acs_config['endpoint_key_name'], 'subscription': ws.subscription_id, 'resource_group': ws.resource_group, 'workspace': ws.name}\n        raw_embeddings_uri = args.embeddings\n        logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')\n        splits = raw_embeddings_uri.split('/')\n        embeddings_dir_name = splits.pop(len(splits) - 2)\n        logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')\n        parent = '/'.join(splits)\n        logger.info(f'extracted embeddings container path: {parent}')\n        from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount\n        mnt_options = MountOptions(default_permission=365, allow_other=False, read_only=True)\n        logger.info(f'mounting embeddings container from: \\n{parent} \\n   to: \\n{os.getcwd()}/embeddings_mount')\n        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:\n            emb = EmbeddingsContainer.load(embeddings_dir_name, mount_context.mount_point)\n            create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output, verbosity=args.verbosity)\n    except Exception as e:\n        logger.error('Failed to update ACS index')\n        exception_str = str(e)\n        if 'Floats quota has been exceeded for this service.' in exception_str:\n            logger.error('Floats quota exceeded on Azure Cognitive Search Service. For more information check these docs: https://github.com/Azure/cognitive-search-vector-pr#storage-and-vector-index-size-limits')\n            logger.error('The usage statistic of an index can be checked using this REST API: https://learn.microsoft.com/en-us/rest/api/searchservice/get-index-statistics ')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Floats quota has been exceeded for this service.'\n        elif 'Cannot find nested property' in exception_str:\n            logger.error(f'''The vector index provided \"{acs_config['index_name']}\" has a different schema than outlined in this components description. This can happen if a different embedding model was used previously when updating this index.''')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Cannot find nested property'\n        elif 'Failed to upload' in exception_str:\n            activity_logger.activity_info['error'] = exception_str\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        else:\n            activity_logger.activity_info['error'] = str(e.__class__.__name__)\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        raise e\n    logger.info('Updated ACS index')",
            "def main(args, logger, activity_logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    try:\n        try:\n            acs_config = json.loads(args.acs_config)\n        except Exception as e:\n            logger.error(f'Failed to parse acs_config as json: {e}')\n            activity_logger.error('Failed to parse acs_config as json')\n            raise\n        connection_args = {}\n        if args.connection_id is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            connection_args['connection'] = {'id': args.connection_id}\n            from azure.ai.generative.index._utils.connections import get_connection_by_id_v2, get_metadata_from_connection, get_target_from_connection\n            connection = get_connection_by_id_v2(args.connection_id)\n            acs_config['endpoint'] = get_target_from_connection(connection)\n            acs_config['api_version'] = get_metadata_from_connection(connection).get('apiVersion', '2023-07-01-preview')\n        elif 'endpoint_key_name' in acs_config:\n            connection_args['connection_type'] = 'workspace_keyvault'\n            from azureml.core import Run\n            run = Run.get_context()\n            ws = run.experiment.workspace\n            connection_args['connection'] = {'key': acs_config['endpoint_key_name'], 'subscription': ws.subscription_id, 'resource_group': ws.resource_group, 'workspace': ws.name}\n        raw_embeddings_uri = args.embeddings\n        logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')\n        splits = raw_embeddings_uri.split('/')\n        embeddings_dir_name = splits.pop(len(splits) - 2)\n        logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')\n        parent = '/'.join(splits)\n        logger.info(f'extracted embeddings container path: {parent}')\n        from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount\n        mnt_options = MountOptions(default_permission=365, allow_other=False, read_only=True)\n        logger.info(f'mounting embeddings container from: \\n{parent} \\n   to: \\n{os.getcwd()}/embeddings_mount')\n        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:\n            emb = EmbeddingsContainer.load(embeddings_dir_name, mount_context.mount_point)\n            create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output, verbosity=args.verbosity)\n    except Exception as e:\n        logger.error('Failed to update ACS index')\n        exception_str = str(e)\n        if 'Floats quota has been exceeded for this service.' in exception_str:\n            logger.error('Floats quota exceeded on Azure Cognitive Search Service. For more information check these docs: https://github.com/Azure/cognitive-search-vector-pr#storage-and-vector-index-size-limits')\n            logger.error('The usage statistic of an index can be checked using this REST API: https://learn.microsoft.com/en-us/rest/api/searchservice/get-index-statistics ')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Floats quota has been exceeded for this service.'\n        elif 'Cannot find nested property' in exception_str:\n            logger.error(f'''The vector index provided \"{acs_config['index_name']}\" has a different schema than outlined in this components description. This can happen if a different embedding model was used previously when updating this index.''')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Cannot find nested property'\n        elif 'Failed to upload' in exception_str:\n            activity_logger.activity_info['error'] = exception_str\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        else:\n            activity_logger.activity_info['error'] = str(e.__class__.__name__)\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        raise e\n    logger.info('Updated ACS index')",
            "def main(args, logger, activity_logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    try:\n        try:\n            acs_config = json.loads(args.acs_config)\n        except Exception as e:\n            logger.error(f'Failed to parse acs_config as json: {e}')\n            activity_logger.error('Failed to parse acs_config as json')\n            raise\n        connection_args = {}\n        if args.connection_id is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            connection_args['connection'] = {'id': args.connection_id}\n            from azure.ai.generative.index._utils.connections import get_connection_by_id_v2, get_metadata_from_connection, get_target_from_connection\n            connection = get_connection_by_id_v2(args.connection_id)\n            acs_config['endpoint'] = get_target_from_connection(connection)\n            acs_config['api_version'] = get_metadata_from_connection(connection).get('apiVersion', '2023-07-01-preview')\n        elif 'endpoint_key_name' in acs_config:\n            connection_args['connection_type'] = 'workspace_keyvault'\n            from azureml.core import Run\n            run = Run.get_context()\n            ws = run.experiment.workspace\n            connection_args['connection'] = {'key': acs_config['endpoint_key_name'], 'subscription': ws.subscription_id, 'resource_group': ws.resource_group, 'workspace': ws.name}\n        raw_embeddings_uri = args.embeddings\n        logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')\n        splits = raw_embeddings_uri.split('/')\n        embeddings_dir_name = splits.pop(len(splits) - 2)\n        logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')\n        parent = '/'.join(splits)\n        logger.info(f'extracted embeddings container path: {parent}')\n        from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount\n        mnt_options = MountOptions(default_permission=365, allow_other=False, read_only=True)\n        logger.info(f'mounting embeddings container from: \\n{parent} \\n   to: \\n{os.getcwd()}/embeddings_mount')\n        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:\n            emb = EmbeddingsContainer.load(embeddings_dir_name, mount_context.mount_point)\n            create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output, verbosity=args.verbosity)\n    except Exception as e:\n        logger.error('Failed to update ACS index')\n        exception_str = str(e)\n        if 'Floats quota has been exceeded for this service.' in exception_str:\n            logger.error('Floats quota exceeded on Azure Cognitive Search Service. For more information check these docs: https://github.com/Azure/cognitive-search-vector-pr#storage-and-vector-index-size-limits')\n            logger.error('The usage statistic of an index can be checked using this REST API: https://learn.microsoft.com/en-us/rest/api/searchservice/get-index-statistics ')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Floats quota has been exceeded for this service.'\n        elif 'Cannot find nested property' in exception_str:\n            logger.error(f'''The vector index provided \"{acs_config['index_name']}\" has a different schema than outlined in this components description. This can happen if a different embedding model was used previously when updating this index.''')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Cannot find nested property'\n        elif 'Failed to upload' in exception_str:\n            activity_logger.activity_info['error'] = exception_str\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        else:\n            activity_logger.activity_info['error'] = str(e.__class__.__name__)\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        raise e\n    logger.info('Updated ACS index')",
            "def main(args, logger, activity_logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    try:\n        try:\n            acs_config = json.loads(args.acs_config)\n        except Exception as e:\n            logger.error(f'Failed to parse acs_config as json: {e}')\n            activity_logger.error('Failed to parse acs_config as json')\n            raise\n        connection_args = {}\n        if args.connection_id is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            connection_args['connection'] = {'id': args.connection_id}\n            from azure.ai.generative.index._utils.connections import get_connection_by_id_v2, get_metadata_from_connection, get_target_from_connection\n            connection = get_connection_by_id_v2(args.connection_id)\n            acs_config['endpoint'] = get_target_from_connection(connection)\n            acs_config['api_version'] = get_metadata_from_connection(connection).get('apiVersion', '2023-07-01-preview')\n        elif 'endpoint_key_name' in acs_config:\n            connection_args['connection_type'] = 'workspace_keyvault'\n            from azureml.core import Run\n            run = Run.get_context()\n            ws = run.experiment.workspace\n            connection_args['connection'] = {'key': acs_config['endpoint_key_name'], 'subscription': ws.subscription_id, 'resource_group': ws.resource_group, 'workspace': ws.name}\n        raw_embeddings_uri = args.embeddings\n        logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')\n        splits = raw_embeddings_uri.split('/')\n        embeddings_dir_name = splits.pop(len(splits) - 2)\n        logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')\n        parent = '/'.join(splits)\n        logger.info(f'extracted embeddings container path: {parent}')\n        from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount\n        mnt_options = MountOptions(default_permission=365, allow_other=False, read_only=True)\n        logger.info(f'mounting embeddings container from: \\n{parent} \\n   to: \\n{os.getcwd()}/embeddings_mount')\n        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:\n            emb = EmbeddingsContainer.load(embeddings_dir_name, mount_context.mount_point)\n            create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output, verbosity=args.verbosity)\n    except Exception as e:\n        logger.error('Failed to update ACS index')\n        exception_str = str(e)\n        if 'Floats quota has been exceeded for this service.' in exception_str:\n            logger.error('Floats quota exceeded on Azure Cognitive Search Service. For more information check these docs: https://github.com/Azure/cognitive-search-vector-pr#storage-and-vector-index-size-limits')\n            logger.error('The usage statistic of an index can be checked using this REST API: https://learn.microsoft.com/en-us/rest/api/searchservice/get-index-statistics ')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Floats quota has been exceeded for this service.'\n        elif 'Cannot find nested property' in exception_str:\n            logger.error(f'''The vector index provided \"{acs_config['index_name']}\" has a different schema than outlined in this components description. This can happen if a different embedding model was used previously when updating this index.''')\n            activity_logger.activity_info['error_classification'] = 'UserError'\n            activity_logger.activity_info['error'] = f'{e.__class__.__name__}: Cannot find nested property'\n        elif 'Failed to upload' in exception_str:\n            activity_logger.activity_info['error'] = exception_str\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        else:\n            activity_logger.activity_info['error'] = str(e.__class__.__name__)\n            activity_logger.activity_info['error_classification'] = 'SystemError'\n        raise e\n    logger.info('Updated ACS index')"
        ]
    },
    {
        "func_name": "main_wrapper",
        "original": "def main_wrapper(args, logger):\n    with track_activity(logger, 'update_acs') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception:\n            activity_logger.error(f'update_acs failed with exception: {traceback.format_exc()}')\n            raise",
        "mutated": [
            "def main_wrapper(args, logger):\n    if False:\n        i = 10\n    with track_activity(logger, 'update_acs') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception:\n            activity_logger.error(f'update_acs failed with exception: {traceback.format_exc()}')\n            raise",
            "def main_wrapper(args, logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with track_activity(logger, 'update_acs') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception:\n            activity_logger.error(f'update_acs failed with exception: {traceback.format_exc()}')\n            raise",
            "def main_wrapper(args, logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with track_activity(logger, 'update_acs') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception:\n            activity_logger.error(f'update_acs failed with exception: {traceback.format_exc()}')\n            raise",
            "def main_wrapper(args, logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with track_activity(logger, 'update_acs') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception:\n            activity_logger.error(f'update_acs failed with exception: {traceback.format_exc()}')\n            raise",
            "def main_wrapper(args, logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with track_activity(logger, 'update_acs') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception:\n            activity_logger.error(f'update_acs failed with exception: {traceback.format_exc()}')\n            raise"
        ]
    }
]