[
    {
        "func_name": "_lm_robust",
        "original": "def _lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    \"\"\"general formula for score/LM test\n\n    generalized score or lagrange multiplier test for implicit constraints\n\n    `r(params) = 0`, with gradient `R = d r / d params`\n\n    linear constraints are given by `R params - q = 0`\n\n    It is assumed that all arrays are evaluated at the constrained estimates.\n\n\n    Parameters\n    ----------\n    score : ndarray, 1-D\n        derivative of objective function at estimated parameters\n        of constrained model\n    constraint_matrix R : ndarray\n        Linear restriction matrix or Jacobian of nonlinear constraints\n    score_deriv_inv, Ainv : ndarray, symmetric, square\n        inverse of second derivative of objective function\n        TODO: could be inverse of OPG or any other estimator if information\n        matrix equality holds\n    cov_score B :  ndarray, symmetric, square\n        covariance matrix of the score. This is the inner part of a sandwich\n        estimator.\n    cov_params V :  ndarray, symmetric, square\n        covariance of full parameter vector evaluated at constrained parameter\n        estimate. This can be specified instead of cov_score B.\n\n    Returns\n    -------\n    lm_stat : float\n        score/lagrange multiplier statistic\n    p-value : float\n        p-value of the LM test based on chisquare distribution\n\n    Notes\n    -----\n\n    \"\"\"\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    k_constraints = np.linalg.matrix_rank(R)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval, k_constraints)",
        "mutated": [
            "def _lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    if False:\n        i = 10\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for implicit constraints\\n\\n    `r(params) = 0`, with gradient `R = d r / d params`\\n\\n    linear constraints are given by `R params - q = 0`\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    constraint_matrix R : ndarray\\n        Linear restriction matrix or Jacobian of nonlinear constraints\\n    score_deriv_inv, Ainv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be inverse of OPG or any other estimator if information\\n        matrix equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n\\n    '\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    k_constraints = np.linalg.matrix_rank(R)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval, k_constraints)",
            "def _lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for implicit constraints\\n\\n    `r(params) = 0`, with gradient `R = d r / d params`\\n\\n    linear constraints are given by `R params - q = 0`\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    constraint_matrix R : ndarray\\n        Linear restriction matrix or Jacobian of nonlinear constraints\\n    score_deriv_inv, Ainv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be inverse of OPG or any other estimator if information\\n        matrix equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n\\n    '\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    k_constraints = np.linalg.matrix_rank(R)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval, k_constraints)",
            "def _lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for implicit constraints\\n\\n    `r(params) = 0`, with gradient `R = d r / d params`\\n\\n    linear constraints are given by `R params - q = 0`\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    constraint_matrix R : ndarray\\n        Linear restriction matrix or Jacobian of nonlinear constraints\\n    score_deriv_inv, Ainv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be inverse of OPG or any other estimator if information\\n        matrix equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n\\n    '\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    k_constraints = np.linalg.matrix_rank(R)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval, k_constraints)",
            "def _lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for implicit constraints\\n\\n    `r(params) = 0`, with gradient `R = d r / d params`\\n\\n    linear constraints are given by `R params - q = 0`\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    constraint_matrix R : ndarray\\n        Linear restriction matrix or Jacobian of nonlinear constraints\\n    score_deriv_inv, Ainv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be inverse of OPG or any other estimator if information\\n        matrix equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n\\n    '\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    k_constraints = np.linalg.matrix_rank(R)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval, k_constraints)",
            "def _lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for implicit constraints\\n\\n    `r(params) = 0`, with gradient `R = d r / d params`\\n\\n    linear constraints are given by `R params - q = 0`\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    constraint_matrix R : ndarray\\n        Linear restriction matrix or Jacobian of nonlinear constraints\\n    score_deriv_inv, Ainv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be inverse of OPG or any other estimator if information\\n        matrix equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n\\n    '\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    k_constraints = np.linalg.matrix_rank(R)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval, k_constraints)"
        ]
    },
    {
        "func_name": "score_test",
        "original": "def score_test(self, exog_extra=None, params_constrained=None, hypothesis='joint', cov_type=None, cov_kwds=None, k_constraints=None, r_matrix=None, scale=None, observed=True):\n    \"\"\"score test for restrictions or for omitted variables\n\n    Null Hypothesis : constraints are satisfied\n\n    Alternative Hypothesis : at least one of the constraints does not hold\n\n    This allows to specify restricted and unrestricted model properties in\n    three different ways\n\n    - fit_constrained result: model contains score and hessian function for\n      the full, unrestricted model, but the parameter estimate in the results\n      instance is for the restricted model. This is the case if the model\n      was estimated with fit_constrained.\n    - restricted model with variable addition: If exog_extra is not None, then\n      it is assumed that the current model is a model with zero restrictions\n      and the unrestricted model is given by adding exog_extra as additional\n      explanatory variables.\n    - unrestricted model with restricted parameters explicitly provided. If\n      params_constrained is not None, then the model is assumed to be for the\n      unrestricted model, but the provided parameters are for the restricted\n      model.\n      TODO: This case will currently only work for `nonrobust` cov_type,\n      otherwise we will also need the restriction matrix provided by the user.\n\n\n    Parameters\n    ----------\n    exog_extra : None or array_like\n        Explanatory variables that are jointly tested for inclusion in the\n        model, i.e. omitted variables.\n    params_constrained : array_like\n        estimated parameter of the restricted model. This can be the\n        parameter estimate for the current when testing for omitted\n        variables.\n    hypothesis : str, 'joint' (default) or 'separate'\n        If hypothesis is 'joint', then the chisquare test results for the\n        joint hypothesis that all constraints hold is returned.\n        If hypothesis is 'joint', then z-test results for each constraint\n        is returned.\n        This is currently only implemented for cov_type=\"nonrobust\".\n    cov_type : str\n        Warning: only partially implemented so far, currently only \"nonrobust\"\n        and \"HC0\" are supported.\n        If cov_type is None, then the cov_type specified in fit for the Wald\n        tests is used.\n        If the cov_type argument is not None, then it will be used instead of\n        the Wald cov_type given in fit.\n    k_constraints : int or None\n        Number of constraints that were used in the estimation of params\n        restricted relative to the number of exog in the model.\n        This must be provided if no exog_extra are given. If exog_extra is\n        not None, then k_constraints is assumed to be zero if it is None.\n    observed : bool\n        If True, then the observed Hessian is used in calculating the\n        covariance matrix of the score. If false then the expected\n        information matrix is used. This currently only applies to GLM where\n        EIM is available.\n        Warning: This option might still change.\n\n    Returns\n    -------\n    chi2_stat : float\n        chisquare statistic for the score test\n    p-value : float\n        P-value of the score test based on the chisquare distribution.\n    df : int\n        Degrees of freedom used in the p-value calculation. This is equal\n        to the number of constraints.\n\n    Notes\n    -----\n    Status: experimental, several options are not implemented yet or are not\n    verified yet. Currently available ptions might also still change.\n\n    cov_type is 'nonrobust':\n\n    The covariance matrix for the score is based on the Hessian, i.e.\n    observed information matrix or optionally on the expected information\n    matrix.\n\n    cov_type is 'HC0'\n\n    The covariance matrix of the score is the simple empirical covariance of\n    score_obs without degrees of freedom correction.\n    \"\"\"\n    if hasattr(self, '_results'):\n        self = self._results\n    model = self.model\n    nobs = model.endog.shape[0]\n    if params_constrained is None:\n        params_constrained = self.params\n    cov_type = cov_type if cov_type is not None else self.cov_type\n    if observed is False:\n        hess_kwd = {'observed': False}\n    else:\n        hess_kwd = {}\n    if exog_extra is None:\n        if hasattr(self, 'constraints'):\n            if isinstance(self.constraints, tuple):\n                r_matrix = self.constraints[0]\n            else:\n                r_matrix = self.constraints.coefs\n            k_constraints = r_matrix.shape[0]\n        elif k_constraints is None:\n            raise ValueError('if exog_extra is None, then k_constraintsneeds to be given')\n        if scale is not None:\n            score_kwd = {'scale': scale}\n            hess_kwd['scale'] = scale\n        else:\n            score_kwd = {}\n        score = model.score(params_constrained, **score_kwd)\n        score_obs = model.score_obs(params_constrained, **score_kwd)\n        hessian = model.hessian(params_constrained, **hess_kwd)\n    else:\n        if cov_type == 'V':\n            raise ValueError('if exog_extra is not None, then cov_type cannot be V')\n        if hasattr(self, 'constraints'):\n            raise NotImplementedError('if exog_extra is not None, then selfshould not be a constrained fit result')\n        if isinstance(exog_extra, tuple):\n            sh = _scorehess_extra(self, params_constrained, *exog_extra, hess_kwds=hess_kwd)\n            (score_obs, hessian, k_constraints, r_matrix) = sh\n            score = score_obs.sum(0)\n        else:\n            exog_extra = np.asarray(exog_extra)\n            k_constraints = 0\n            ex = np.column_stack((model.exog, exog_extra))\n            k_constraints += ex.shape[1] - model.exog.shape[1]\n            r_matrix = np.eye(len(self.params) + k_constraints)[-k_constraints:]\n            score_factor = model.score_factor(params_constrained)\n            if score_factor.ndim == 1:\n                score_obs = score_factor[:, None] * ex\n            else:\n                sf = score_factor\n                score_obs = np.column_stack((sf[:, :1] * ex, sf[:, 1:]))\n            score = score_obs.sum(0)\n            hessian_factor = model.hessian_factor(params_constrained, **hess_kwd)\n            from statsmodels.genmod.generalized_linear_model import GLM\n            if isinstance(model, GLM):\n                hessian_factor *= -1\n            hessian = np.dot(ex.T * hessian_factor, ex)\n    if cov_type == 'nonrobust':\n        cov_score_test = -hessian\n    elif cov_type.upper() == 'HC0':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        lm = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=None)\n        return lm\n    elif cov_type.upper() == 'V':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        V = self.cov_params_default\n        chi2stat = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=V)\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval)\n    else:\n        msg = 'Only cov_type \"nonrobust\" and \"HC0\" are available.'\n        raise NotImplementedError(msg)\n    if hypothesis == 'joint':\n        chi2stat = score.dot(np.linalg.solve(cov_score_test, score[:, None]))\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval, k_constraints)\n    elif hypothesis == 'separate':\n        diff = score\n        bse = np.sqrt(np.diag(cov_score_test))\n        stat = diff / bse\n        pval = stats.norm.sf(np.abs(stat)) * 2\n        return (stat, pval)\n    else:\n        raise NotImplementedError('only hypothesis \"joint\" is available')",
        "mutated": [
            "def score_test(self, exog_extra=None, params_constrained=None, hypothesis='joint', cov_type=None, cov_kwds=None, k_constraints=None, r_matrix=None, scale=None, observed=True):\n    if False:\n        i = 10\n    'score test for restrictions or for omitted variables\\n\\n    Null Hypothesis : constraints are satisfied\\n\\n    Alternative Hypothesis : at least one of the constraints does not hold\\n\\n    This allows to specify restricted and unrestricted model properties in\\n    three different ways\\n\\n    - fit_constrained result: model contains score and hessian function for\\n      the full, unrestricted model, but the parameter estimate in the results\\n      instance is for the restricted model. This is the case if the model\\n      was estimated with fit_constrained.\\n    - restricted model with variable addition: If exog_extra is not None, then\\n      it is assumed that the current model is a model with zero restrictions\\n      and the unrestricted model is given by adding exog_extra as additional\\n      explanatory variables.\\n    - unrestricted model with restricted parameters explicitly provided. If\\n      params_constrained is not None, then the model is assumed to be for the\\n      unrestricted model, but the provided parameters are for the restricted\\n      model.\\n      TODO: This case will currently only work for `nonrobust` cov_type,\\n      otherwise we will also need the restriction matrix provided by the user.\\n\\n\\n    Parameters\\n    ----------\\n    exog_extra : None or array_like\\n        Explanatory variables that are jointly tested for inclusion in the\\n        model, i.e. omitted variables.\\n    params_constrained : array_like\\n        estimated parameter of the restricted model. This can be the\\n        parameter estimate for the current when testing for omitted\\n        variables.\\n    hypothesis : str, \\'joint\\' (default) or \\'separate\\'\\n        If hypothesis is \\'joint\\', then the chisquare test results for the\\n        joint hypothesis that all constraints hold is returned.\\n        If hypothesis is \\'joint\\', then z-test results for each constraint\\n        is returned.\\n        This is currently only implemented for cov_type=\"nonrobust\".\\n    cov_type : str\\n        Warning: only partially implemented so far, currently only \"nonrobust\"\\n        and \"HC0\" are supported.\\n        If cov_type is None, then the cov_type specified in fit for the Wald\\n        tests is used.\\n        If the cov_type argument is not None, then it will be used instead of\\n        the Wald cov_type given in fit.\\n    k_constraints : int or None\\n        Number of constraints that were used in the estimation of params\\n        restricted relative to the number of exog in the model.\\n        This must be provided if no exog_extra are given. If exog_extra is\\n        not None, then k_constraints is assumed to be zero if it is None.\\n    observed : bool\\n        If True, then the observed Hessian is used in calculating the\\n        covariance matrix of the score. If false then the expected\\n        information matrix is used. This currently only applies to GLM where\\n        EIM is available.\\n        Warning: This option might still change.\\n\\n    Returns\\n    -------\\n    chi2_stat : float\\n        chisquare statistic for the score test\\n    p-value : float\\n        P-value of the score test based on the chisquare distribution.\\n    df : int\\n        Degrees of freedom used in the p-value calculation. This is equal\\n        to the number of constraints.\\n\\n    Notes\\n    -----\\n    Status: experimental, several options are not implemented yet or are not\\n    verified yet. Currently available ptions might also still change.\\n\\n    cov_type is \\'nonrobust\\':\\n\\n    The covariance matrix for the score is based on the Hessian, i.e.\\n    observed information matrix or optionally on the expected information\\n    matrix.\\n\\n    cov_type is \\'HC0\\'\\n\\n    The covariance matrix of the score is the simple empirical covariance of\\n    score_obs without degrees of freedom correction.\\n    '\n    if hasattr(self, '_results'):\n        self = self._results\n    model = self.model\n    nobs = model.endog.shape[0]\n    if params_constrained is None:\n        params_constrained = self.params\n    cov_type = cov_type if cov_type is not None else self.cov_type\n    if observed is False:\n        hess_kwd = {'observed': False}\n    else:\n        hess_kwd = {}\n    if exog_extra is None:\n        if hasattr(self, 'constraints'):\n            if isinstance(self.constraints, tuple):\n                r_matrix = self.constraints[0]\n            else:\n                r_matrix = self.constraints.coefs\n            k_constraints = r_matrix.shape[0]\n        elif k_constraints is None:\n            raise ValueError('if exog_extra is None, then k_constraintsneeds to be given')\n        if scale is not None:\n            score_kwd = {'scale': scale}\n            hess_kwd['scale'] = scale\n        else:\n            score_kwd = {}\n        score = model.score(params_constrained, **score_kwd)\n        score_obs = model.score_obs(params_constrained, **score_kwd)\n        hessian = model.hessian(params_constrained, **hess_kwd)\n    else:\n        if cov_type == 'V':\n            raise ValueError('if exog_extra is not None, then cov_type cannot be V')\n        if hasattr(self, 'constraints'):\n            raise NotImplementedError('if exog_extra is not None, then selfshould not be a constrained fit result')\n        if isinstance(exog_extra, tuple):\n            sh = _scorehess_extra(self, params_constrained, *exog_extra, hess_kwds=hess_kwd)\n            (score_obs, hessian, k_constraints, r_matrix) = sh\n            score = score_obs.sum(0)\n        else:\n            exog_extra = np.asarray(exog_extra)\n            k_constraints = 0\n            ex = np.column_stack((model.exog, exog_extra))\n            k_constraints += ex.shape[1] - model.exog.shape[1]\n            r_matrix = np.eye(len(self.params) + k_constraints)[-k_constraints:]\n            score_factor = model.score_factor(params_constrained)\n            if score_factor.ndim == 1:\n                score_obs = score_factor[:, None] * ex\n            else:\n                sf = score_factor\n                score_obs = np.column_stack((sf[:, :1] * ex, sf[:, 1:]))\n            score = score_obs.sum(0)\n            hessian_factor = model.hessian_factor(params_constrained, **hess_kwd)\n            from statsmodels.genmod.generalized_linear_model import GLM\n            if isinstance(model, GLM):\n                hessian_factor *= -1\n            hessian = np.dot(ex.T * hessian_factor, ex)\n    if cov_type == 'nonrobust':\n        cov_score_test = -hessian\n    elif cov_type.upper() == 'HC0':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        lm = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=None)\n        return lm\n    elif cov_type.upper() == 'V':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        V = self.cov_params_default\n        chi2stat = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=V)\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval)\n    else:\n        msg = 'Only cov_type \"nonrobust\" and \"HC0\" are available.'\n        raise NotImplementedError(msg)\n    if hypothesis == 'joint':\n        chi2stat = score.dot(np.linalg.solve(cov_score_test, score[:, None]))\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval, k_constraints)\n    elif hypothesis == 'separate':\n        diff = score\n        bse = np.sqrt(np.diag(cov_score_test))\n        stat = diff / bse\n        pval = stats.norm.sf(np.abs(stat)) * 2\n        return (stat, pval)\n    else:\n        raise NotImplementedError('only hypothesis \"joint\" is available')",
            "def score_test(self, exog_extra=None, params_constrained=None, hypothesis='joint', cov_type=None, cov_kwds=None, k_constraints=None, r_matrix=None, scale=None, observed=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'score test for restrictions or for omitted variables\\n\\n    Null Hypothesis : constraints are satisfied\\n\\n    Alternative Hypothesis : at least one of the constraints does not hold\\n\\n    This allows to specify restricted and unrestricted model properties in\\n    three different ways\\n\\n    - fit_constrained result: model contains score and hessian function for\\n      the full, unrestricted model, but the parameter estimate in the results\\n      instance is for the restricted model. This is the case if the model\\n      was estimated with fit_constrained.\\n    - restricted model with variable addition: If exog_extra is not None, then\\n      it is assumed that the current model is a model with zero restrictions\\n      and the unrestricted model is given by adding exog_extra as additional\\n      explanatory variables.\\n    - unrestricted model with restricted parameters explicitly provided. If\\n      params_constrained is not None, then the model is assumed to be for the\\n      unrestricted model, but the provided parameters are for the restricted\\n      model.\\n      TODO: This case will currently only work for `nonrobust` cov_type,\\n      otherwise we will also need the restriction matrix provided by the user.\\n\\n\\n    Parameters\\n    ----------\\n    exog_extra : None or array_like\\n        Explanatory variables that are jointly tested for inclusion in the\\n        model, i.e. omitted variables.\\n    params_constrained : array_like\\n        estimated parameter of the restricted model. This can be the\\n        parameter estimate for the current when testing for omitted\\n        variables.\\n    hypothesis : str, \\'joint\\' (default) or \\'separate\\'\\n        If hypothesis is \\'joint\\', then the chisquare test results for the\\n        joint hypothesis that all constraints hold is returned.\\n        If hypothesis is \\'joint\\', then z-test results for each constraint\\n        is returned.\\n        This is currently only implemented for cov_type=\"nonrobust\".\\n    cov_type : str\\n        Warning: only partially implemented so far, currently only \"nonrobust\"\\n        and \"HC0\" are supported.\\n        If cov_type is None, then the cov_type specified in fit for the Wald\\n        tests is used.\\n        If the cov_type argument is not None, then it will be used instead of\\n        the Wald cov_type given in fit.\\n    k_constraints : int or None\\n        Number of constraints that were used in the estimation of params\\n        restricted relative to the number of exog in the model.\\n        This must be provided if no exog_extra are given. If exog_extra is\\n        not None, then k_constraints is assumed to be zero if it is None.\\n    observed : bool\\n        If True, then the observed Hessian is used in calculating the\\n        covariance matrix of the score. If false then the expected\\n        information matrix is used. This currently only applies to GLM where\\n        EIM is available.\\n        Warning: This option might still change.\\n\\n    Returns\\n    -------\\n    chi2_stat : float\\n        chisquare statistic for the score test\\n    p-value : float\\n        P-value of the score test based on the chisquare distribution.\\n    df : int\\n        Degrees of freedom used in the p-value calculation. This is equal\\n        to the number of constraints.\\n\\n    Notes\\n    -----\\n    Status: experimental, several options are not implemented yet or are not\\n    verified yet. Currently available ptions might also still change.\\n\\n    cov_type is \\'nonrobust\\':\\n\\n    The covariance matrix for the score is based on the Hessian, i.e.\\n    observed information matrix or optionally on the expected information\\n    matrix.\\n\\n    cov_type is \\'HC0\\'\\n\\n    The covariance matrix of the score is the simple empirical covariance of\\n    score_obs without degrees of freedom correction.\\n    '\n    if hasattr(self, '_results'):\n        self = self._results\n    model = self.model\n    nobs = model.endog.shape[0]\n    if params_constrained is None:\n        params_constrained = self.params\n    cov_type = cov_type if cov_type is not None else self.cov_type\n    if observed is False:\n        hess_kwd = {'observed': False}\n    else:\n        hess_kwd = {}\n    if exog_extra is None:\n        if hasattr(self, 'constraints'):\n            if isinstance(self.constraints, tuple):\n                r_matrix = self.constraints[0]\n            else:\n                r_matrix = self.constraints.coefs\n            k_constraints = r_matrix.shape[0]\n        elif k_constraints is None:\n            raise ValueError('if exog_extra is None, then k_constraintsneeds to be given')\n        if scale is not None:\n            score_kwd = {'scale': scale}\n            hess_kwd['scale'] = scale\n        else:\n            score_kwd = {}\n        score = model.score(params_constrained, **score_kwd)\n        score_obs = model.score_obs(params_constrained, **score_kwd)\n        hessian = model.hessian(params_constrained, **hess_kwd)\n    else:\n        if cov_type == 'V':\n            raise ValueError('if exog_extra is not None, then cov_type cannot be V')\n        if hasattr(self, 'constraints'):\n            raise NotImplementedError('if exog_extra is not None, then selfshould not be a constrained fit result')\n        if isinstance(exog_extra, tuple):\n            sh = _scorehess_extra(self, params_constrained, *exog_extra, hess_kwds=hess_kwd)\n            (score_obs, hessian, k_constraints, r_matrix) = sh\n            score = score_obs.sum(0)\n        else:\n            exog_extra = np.asarray(exog_extra)\n            k_constraints = 0\n            ex = np.column_stack((model.exog, exog_extra))\n            k_constraints += ex.shape[1] - model.exog.shape[1]\n            r_matrix = np.eye(len(self.params) + k_constraints)[-k_constraints:]\n            score_factor = model.score_factor(params_constrained)\n            if score_factor.ndim == 1:\n                score_obs = score_factor[:, None] * ex\n            else:\n                sf = score_factor\n                score_obs = np.column_stack((sf[:, :1] * ex, sf[:, 1:]))\n            score = score_obs.sum(0)\n            hessian_factor = model.hessian_factor(params_constrained, **hess_kwd)\n            from statsmodels.genmod.generalized_linear_model import GLM\n            if isinstance(model, GLM):\n                hessian_factor *= -1\n            hessian = np.dot(ex.T * hessian_factor, ex)\n    if cov_type == 'nonrobust':\n        cov_score_test = -hessian\n    elif cov_type.upper() == 'HC0':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        lm = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=None)\n        return lm\n    elif cov_type.upper() == 'V':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        V = self.cov_params_default\n        chi2stat = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=V)\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval)\n    else:\n        msg = 'Only cov_type \"nonrobust\" and \"HC0\" are available.'\n        raise NotImplementedError(msg)\n    if hypothesis == 'joint':\n        chi2stat = score.dot(np.linalg.solve(cov_score_test, score[:, None]))\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval, k_constraints)\n    elif hypothesis == 'separate':\n        diff = score\n        bse = np.sqrt(np.diag(cov_score_test))\n        stat = diff / bse\n        pval = stats.norm.sf(np.abs(stat)) * 2\n        return (stat, pval)\n    else:\n        raise NotImplementedError('only hypothesis \"joint\" is available')",
            "def score_test(self, exog_extra=None, params_constrained=None, hypothesis='joint', cov_type=None, cov_kwds=None, k_constraints=None, r_matrix=None, scale=None, observed=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'score test for restrictions or for omitted variables\\n\\n    Null Hypothesis : constraints are satisfied\\n\\n    Alternative Hypothesis : at least one of the constraints does not hold\\n\\n    This allows to specify restricted and unrestricted model properties in\\n    three different ways\\n\\n    - fit_constrained result: model contains score and hessian function for\\n      the full, unrestricted model, but the parameter estimate in the results\\n      instance is for the restricted model. This is the case if the model\\n      was estimated with fit_constrained.\\n    - restricted model with variable addition: If exog_extra is not None, then\\n      it is assumed that the current model is a model with zero restrictions\\n      and the unrestricted model is given by adding exog_extra as additional\\n      explanatory variables.\\n    - unrestricted model with restricted parameters explicitly provided. If\\n      params_constrained is not None, then the model is assumed to be for the\\n      unrestricted model, but the provided parameters are for the restricted\\n      model.\\n      TODO: This case will currently only work for `nonrobust` cov_type,\\n      otherwise we will also need the restriction matrix provided by the user.\\n\\n\\n    Parameters\\n    ----------\\n    exog_extra : None or array_like\\n        Explanatory variables that are jointly tested for inclusion in the\\n        model, i.e. omitted variables.\\n    params_constrained : array_like\\n        estimated parameter of the restricted model. This can be the\\n        parameter estimate for the current when testing for omitted\\n        variables.\\n    hypothesis : str, \\'joint\\' (default) or \\'separate\\'\\n        If hypothesis is \\'joint\\', then the chisquare test results for the\\n        joint hypothesis that all constraints hold is returned.\\n        If hypothesis is \\'joint\\', then z-test results for each constraint\\n        is returned.\\n        This is currently only implemented for cov_type=\"nonrobust\".\\n    cov_type : str\\n        Warning: only partially implemented so far, currently only \"nonrobust\"\\n        and \"HC0\" are supported.\\n        If cov_type is None, then the cov_type specified in fit for the Wald\\n        tests is used.\\n        If the cov_type argument is not None, then it will be used instead of\\n        the Wald cov_type given in fit.\\n    k_constraints : int or None\\n        Number of constraints that were used in the estimation of params\\n        restricted relative to the number of exog in the model.\\n        This must be provided if no exog_extra are given. If exog_extra is\\n        not None, then k_constraints is assumed to be zero if it is None.\\n    observed : bool\\n        If True, then the observed Hessian is used in calculating the\\n        covariance matrix of the score. If false then the expected\\n        information matrix is used. This currently only applies to GLM where\\n        EIM is available.\\n        Warning: This option might still change.\\n\\n    Returns\\n    -------\\n    chi2_stat : float\\n        chisquare statistic for the score test\\n    p-value : float\\n        P-value of the score test based on the chisquare distribution.\\n    df : int\\n        Degrees of freedom used in the p-value calculation. This is equal\\n        to the number of constraints.\\n\\n    Notes\\n    -----\\n    Status: experimental, several options are not implemented yet or are not\\n    verified yet. Currently available ptions might also still change.\\n\\n    cov_type is \\'nonrobust\\':\\n\\n    The covariance matrix for the score is based on the Hessian, i.e.\\n    observed information matrix or optionally on the expected information\\n    matrix.\\n\\n    cov_type is \\'HC0\\'\\n\\n    The covariance matrix of the score is the simple empirical covariance of\\n    score_obs without degrees of freedom correction.\\n    '\n    if hasattr(self, '_results'):\n        self = self._results\n    model = self.model\n    nobs = model.endog.shape[0]\n    if params_constrained is None:\n        params_constrained = self.params\n    cov_type = cov_type if cov_type is not None else self.cov_type\n    if observed is False:\n        hess_kwd = {'observed': False}\n    else:\n        hess_kwd = {}\n    if exog_extra is None:\n        if hasattr(self, 'constraints'):\n            if isinstance(self.constraints, tuple):\n                r_matrix = self.constraints[0]\n            else:\n                r_matrix = self.constraints.coefs\n            k_constraints = r_matrix.shape[0]\n        elif k_constraints is None:\n            raise ValueError('if exog_extra is None, then k_constraintsneeds to be given')\n        if scale is not None:\n            score_kwd = {'scale': scale}\n            hess_kwd['scale'] = scale\n        else:\n            score_kwd = {}\n        score = model.score(params_constrained, **score_kwd)\n        score_obs = model.score_obs(params_constrained, **score_kwd)\n        hessian = model.hessian(params_constrained, **hess_kwd)\n    else:\n        if cov_type == 'V':\n            raise ValueError('if exog_extra is not None, then cov_type cannot be V')\n        if hasattr(self, 'constraints'):\n            raise NotImplementedError('if exog_extra is not None, then selfshould not be a constrained fit result')\n        if isinstance(exog_extra, tuple):\n            sh = _scorehess_extra(self, params_constrained, *exog_extra, hess_kwds=hess_kwd)\n            (score_obs, hessian, k_constraints, r_matrix) = sh\n            score = score_obs.sum(0)\n        else:\n            exog_extra = np.asarray(exog_extra)\n            k_constraints = 0\n            ex = np.column_stack((model.exog, exog_extra))\n            k_constraints += ex.shape[1] - model.exog.shape[1]\n            r_matrix = np.eye(len(self.params) + k_constraints)[-k_constraints:]\n            score_factor = model.score_factor(params_constrained)\n            if score_factor.ndim == 1:\n                score_obs = score_factor[:, None] * ex\n            else:\n                sf = score_factor\n                score_obs = np.column_stack((sf[:, :1] * ex, sf[:, 1:]))\n            score = score_obs.sum(0)\n            hessian_factor = model.hessian_factor(params_constrained, **hess_kwd)\n            from statsmodels.genmod.generalized_linear_model import GLM\n            if isinstance(model, GLM):\n                hessian_factor *= -1\n            hessian = np.dot(ex.T * hessian_factor, ex)\n    if cov_type == 'nonrobust':\n        cov_score_test = -hessian\n    elif cov_type.upper() == 'HC0':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        lm = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=None)\n        return lm\n    elif cov_type.upper() == 'V':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        V = self.cov_params_default\n        chi2stat = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=V)\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval)\n    else:\n        msg = 'Only cov_type \"nonrobust\" and \"HC0\" are available.'\n        raise NotImplementedError(msg)\n    if hypothesis == 'joint':\n        chi2stat = score.dot(np.linalg.solve(cov_score_test, score[:, None]))\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval, k_constraints)\n    elif hypothesis == 'separate':\n        diff = score\n        bse = np.sqrt(np.diag(cov_score_test))\n        stat = diff / bse\n        pval = stats.norm.sf(np.abs(stat)) * 2\n        return (stat, pval)\n    else:\n        raise NotImplementedError('only hypothesis \"joint\" is available')",
            "def score_test(self, exog_extra=None, params_constrained=None, hypothesis='joint', cov_type=None, cov_kwds=None, k_constraints=None, r_matrix=None, scale=None, observed=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'score test for restrictions or for omitted variables\\n\\n    Null Hypothesis : constraints are satisfied\\n\\n    Alternative Hypothesis : at least one of the constraints does not hold\\n\\n    This allows to specify restricted and unrestricted model properties in\\n    three different ways\\n\\n    - fit_constrained result: model contains score and hessian function for\\n      the full, unrestricted model, but the parameter estimate in the results\\n      instance is for the restricted model. This is the case if the model\\n      was estimated with fit_constrained.\\n    - restricted model with variable addition: If exog_extra is not None, then\\n      it is assumed that the current model is a model with zero restrictions\\n      and the unrestricted model is given by adding exog_extra as additional\\n      explanatory variables.\\n    - unrestricted model with restricted parameters explicitly provided. If\\n      params_constrained is not None, then the model is assumed to be for the\\n      unrestricted model, but the provided parameters are for the restricted\\n      model.\\n      TODO: This case will currently only work for `nonrobust` cov_type,\\n      otherwise we will also need the restriction matrix provided by the user.\\n\\n\\n    Parameters\\n    ----------\\n    exog_extra : None or array_like\\n        Explanatory variables that are jointly tested for inclusion in the\\n        model, i.e. omitted variables.\\n    params_constrained : array_like\\n        estimated parameter of the restricted model. This can be the\\n        parameter estimate for the current when testing for omitted\\n        variables.\\n    hypothesis : str, \\'joint\\' (default) or \\'separate\\'\\n        If hypothesis is \\'joint\\', then the chisquare test results for the\\n        joint hypothesis that all constraints hold is returned.\\n        If hypothesis is \\'joint\\', then z-test results for each constraint\\n        is returned.\\n        This is currently only implemented for cov_type=\"nonrobust\".\\n    cov_type : str\\n        Warning: only partially implemented so far, currently only \"nonrobust\"\\n        and \"HC0\" are supported.\\n        If cov_type is None, then the cov_type specified in fit for the Wald\\n        tests is used.\\n        If the cov_type argument is not None, then it will be used instead of\\n        the Wald cov_type given in fit.\\n    k_constraints : int or None\\n        Number of constraints that were used in the estimation of params\\n        restricted relative to the number of exog in the model.\\n        This must be provided if no exog_extra are given. If exog_extra is\\n        not None, then k_constraints is assumed to be zero if it is None.\\n    observed : bool\\n        If True, then the observed Hessian is used in calculating the\\n        covariance matrix of the score. If false then the expected\\n        information matrix is used. This currently only applies to GLM where\\n        EIM is available.\\n        Warning: This option might still change.\\n\\n    Returns\\n    -------\\n    chi2_stat : float\\n        chisquare statistic for the score test\\n    p-value : float\\n        P-value of the score test based on the chisquare distribution.\\n    df : int\\n        Degrees of freedom used in the p-value calculation. This is equal\\n        to the number of constraints.\\n\\n    Notes\\n    -----\\n    Status: experimental, several options are not implemented yet or are not\\n    verified yet. Currently available ptions might also still change.\\n\\n    cov_type is \\'nonrobust\\':\\n\\n    The covariance matrix for the score is based on the Hessian, i.e.\\n    observed information matrix or optionally on the expected information\\n    matrix.\\n\\n    cov_type is \\'HC0\\'\\n\\n    The covariance matrix of the score is the simple empirical covariance of\\n    score_obs without degrees of freedom correction.\\n    '\n    if hasattr(self, '_results'):\n        self = self._results\n    model = self.model\n    nobs = model.endog.shape[0]\n    if params_constrained is None:\n        params_constrained = self.params\n    cov_type = cov_type if cov_type is not None else self.cov_type\n    if observed is False:\n        hess_kwd = {'observed': False}\n    else:\n        hess_kwd = {}\n    if exog_extra is None:\n        if hasattr(self, 'constraints'):\n            if isinstance(self.constraints, tuple):\n                r_matrix = self.constraints[0]\n            else:\n                r_matrix = self.constraints.coefs\n            k_constraints = r_matrix.shape[0]\n        elif k_constraints is None:\n            raise ValueError('if exog_extra is None, then k_constraintsneeds to be given')\n        if scale is not None:\n            score_kwd = {'scale': scale}\n            hess_kwd['scale'] = scale\n        else:\n            score_kwd = {}\n        score = model.score(params_constrained, **score_kwd)\n        score_obs = model.score_obs(params_constrained, **score_kwd)\n        hessian = model.hessian(params_constrained, **hess_kwd)\n    else:\n        if cov_type == 'V':\n            raise ValueError('if exog_extra is not None, then cov_type cannot be V')\n        if hasattr(self, 'constraints'):\n            raise NotImplementedError('if exog_extra is not None, then selfshould not be a constrained fit result')\n        if isinstance(exog_extra, tuple):\n            sh = _scorehess_extra(self, params_constrained, *exog_extra, hess_kwds=hess_kwd)\n            (score_obs, hessian, k_constraints, r_matrix) = sh\n            score = score_obs.sum(0)\n        else:\n            exog_extra = np.asarray(exog_extra)\n            k_constraints = 0\n            ex = np.column_stack((model.exog, exog_extra))\n            k_constraints += ex.shape[1] - model.exog.shape[1]\n            r_matrix = np.eye(len(self.params) + k_constraints)[-k_constraints:]\n            score_factor = model.score_factor(params_constrained)\n            if score_factor.ndim == 1:\n                score_obs = score_factor[:, None] * ex\n            else:\n                sf = score_factor\n                score_obs = np.column_stack((sf[:, :1] * ex, sf[:, 1:]))\n            score = score_obs.sum(0)\n            hessian_factor = model.hessian_factor(params_constrained, **hess_kwd)\n            from statsmodels.genmod.generalized_linear_model import GLM\n            if isinstance(model, GLM):\n                hessian_factor *= -1\n            hessian = np.dot(ex.T * hessian_factor, ex)\n    if cov_type == 'nonrobust':\n        cov_score_test = -hessian\n    elif cov_type.upper() == 'HC0':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        lm = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=None)\n        return lm\n    elif cov_type.upper() == 'V':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        V = self.cov_params_default\n        chi2stat = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=V)\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval)\n    else:\n        msg = 'Only cov_type \"nonrobust\" and \"HC0\" are available.'\n        raise NotImplementedError(msg)\n    if hypothesis == 'joint':\n        chi2stat = score.dot(np.linalg.solve(cov_score_test, score[:, None]))\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval, k_constraints)\n    elif hypothesis == 'separate':\n        diff = score\n        bse = np.sqrt(np.diag(cov_score_test))\n        stat = diff / bse\n        pval = stats.norm.sf(np.abs(stat)) * 2\n        return (stat, pval)\n    else:\n        raise NotImplementedError('only hypothesis \"joint\" is available')",
            "def score_test(self, exog_extra=None, params_constrained=None, hypothesis='joint', cov_type=None, cov_kwds=None, k_constraints=None, r_matrix=None, scale=None, observed=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'score test for restrictions or for omitted variables\\n\\n    Null Hypothesis : constraints are satisfied\\n\\n    Alternative Hypothesis : at least one of the constraints does not hold\\n\\n    This allows to specify restricted and unrestricted model properties in\\n    three different ways\\n\\n    - fit_constrained result: model contains score and hessian function for\\n      the full, unrestricted model, but the parameter estimate in the results\\n      instance is for the restricted model. This is the case if the model\\n      was estimated with fit_constrained.\\n    - restricted model with variable addition: If exog_extra is not None, then\\n      it is assumed that the current model is a model with zero restrictions\\n      and the unrestricted model is given by adding exog_extra as additional\\n      explanatory variables.\\n    - unrestricted model with restricted parameters explicitly provided. If\\n      params_constrained is not None, then the model is assumed to be for the\\n      unrestricted model, but the provided parameters are for the restricted\\n      model.\\n      TODO: This case will currently only work for `nonrobust` cov_type,\\n      otherwise we will also need the restriction matrix provided by the user.\\n\\n\\n    Parameters\\n    ----------\\n    exog_extra : None or array_like\\n        Explanatory variables that are jointly tested for inclusion in the\\n        model, i.e. omitted variables.\\n    params_constrained : array_like\\n        estimated parameter of the restricted model. This can be the\\n        parameter estimate for the current when testing for omitted\\n        variables.\\n    hypothesis : str, \\'joint\\' (default) or \\'separate\\'\\n        If hypothesis is \\'joint\\', then the chisquare test results for the\\n        joint hypothesis that all constraints hold is returned.\\n        If hypothesis is \\'joint\\', then z-test results for each constraint\\n        is returned.\\n        This is currently only implemented for cov_type=\"nonrobust\".\\n    cov_type : str\\n        Warning: only partially implemented so far, currently only \"nonrobust\"\\n        and \"HC0\" are supported.\\n        If cov_type is None, then the cov_type specified in fit for the Wald\\n        tests is used.\\n        If the cov_type argument is not None, then it will be used instead of\\n        the Wald cov_type given in fit.\\n    k_constraints : int or None\\n        Number of constraints that were used in the estimation of params\\n        restricted relative to the number of exog in the model.\\n        This must be provided if no exog_extra are given. If exog_extra is\\n        not None, then k_constraints is assumed to be zero if it is None.\\n    observed : bool\\n        If True, then the observed Hessian is used in calculating the\\n        covariance matrix of the score. If false then the expected\\n        information matrix is used. This currently only applies to GLM where\\n        EIM is available.\\n        Warning: This option might still change.\\n\\n    Returns\\n    -------\\n    chi2_stat : float\\n        chisquare statistic for the score test\\n    p-value : float\\n        P-value of the score test based on the chisquare distribution.\\n    df : int\\n        Degrees of freedom used in the p-value calculation. This is equal\\n        to the number of constraints.\\n\\n    Notes\\n    -----\\n    Status: experimental, several options are not implemented yet or are not\\n    verified yet. Currently available ptions might also still change.\\n\\n    cov_type is \\'nonrobust\\':\\n\\n    The covariance matrix for the score is based on the Hessian, i.e.\\n    observed information matrix or optionally on the expected information\\n    matrix.\\n\\n    cov_type is \\'HC0\\'\\n\\n    The covariance matrix of the score is the simple empirical covariance of\\n    score_obs without degrees of freedom correction.\\n    '\n    if hasattr(self, '_results'):\n        self = self._results\n    model = self.model\n    nobs = model.endog.shape[0]\n    if params_constrained is None:\n        params_constrained = self.params\n    cov_type = cov_type if cov_type is not None else self.cov_type\n    if observed is False:\n        hess_kwd = {'observed': False}\n    else:\n        hess_kwd = {}\n    if exog_extra is None:\n        if hasattr(self, 'constraints'):\n            if isinstance(self.constraints, tuple):\n                r_matrix = self.constraints[0]\n            else:\n                r_matrix = self.constraints.coefs\n            k_constraints = r_matrix.shape[0]\n        elif k_constraints is None:\n            raise ValueError('if exog_extra is None, then k_constraintsneeds to be given')\n        if scale is not None:\n            score_kwd = {'scale': scale}\n            hess_kwd['scale'] = scale\n        else:\n            score_kwd = {}\n        score = model.score(params_constrained, **score_kwd)\n        score_obs = model.score_obs(params_constrained, **score_kwd)\n        hessian = model.hessian(params_constrained, **hess_kwd)\n    else:\n        if cov_type == 'V':\n            raise ValueError('if exog_extra is not None, then cov_type cannot be V')\n        if hasattr(self, 'constraints'):\n            raise NotImplementedError('if exog_extra is not None, then selfshould not be a constrained fit result')\n        if isinstance(exog_extra, tuple):\n            sh = _scorehess_extra(self, params_constrained, *exog_extra, hess_kwds=hess_kwd)\n            (score_obs, hessian, k_constraints, r_matrix) = sh\n            score = score_obs.sum(0)\n        else:\n            exog_extra = np.asarray(exog_extra)\n            k_constraints = 0\n            ex = np.column_stack((model.exog, exog_extra))\n            k_constraints += ex.shape[1] - model.exog.shape[1]\n            r_matrix = np.eye(len(self.params) + k_constraints)[-k_constraints:]\n            score_factor = model.score_factor(params_constrained)\n            if score_factor.ndim == 1:\n                score_obs = score_factor[:, None] * ex\n            else:\n                sf = score_factor\n                score_obs = np.column_stack((sf[:, :1] * ex, sf[:, 1:]))\n            score = score_obs.sum(0)\n            hessian_factor = model.hessian_factor(params_constrained, **hess_kwd)\n            from statsmodels.genmod.generalized_linear_model import GLM\n            if isinstance(model, GLM):\n                hessian_factor *= -1\n            hessian = np.dot(ex.T * hessian_factor, ex)\n    if cov_type == 'nonrobust':\n        cov_score_test = -hessian\n    elif cov_type.upper() == 'HC0':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        lm = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=None)\n        return lm\n    elif cov_type.upper() == 'V':\n        hinv = -np.linalg.inv(hessian)\n        cov_score = nobs * np.cov(score_obs.T)\n        V = self.cov_params_default\n        chi2stat = _lm_robust(score, r_matrix, hinv, cov_score, cov_params=V)\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval)\n    else:\n        msg = 'Only cov_type \"nonrobust\" and \"HC0\" are available.'\n        raise NotImplementedError(msg)\n    if hypothesis == 'joint':\n        chi2stat = score.dot(np.linalg.solve(cov_score_test, score[:, None]))\n        pval = stats.chi2.sf(chi2stat, k_constraints)\n        return (chi2stat, pval, k_constraints)\n    elif hypothesis == 'separate':\n        diff = score\n        bse = np.sqrt(np.diag(cov_score_test))\n        stat = diff / bse\n        pval = stats.norm.sf(np.abs(stat)) * 2\n        return (stat, pval)\n    else:\n        raise NotImplementedError('only hypothesis \"joint\" is available')"
        ]
    },
    {
        "func_name": "_scorehess_extra",
        "original": "def _scorehess_extra(self, params=None, exog_extra=None, exog2_extra=None, hess_kwds=None):\n    \"\"\"Experimental helper function for variable addition score test.\n\n    This uses score and hessian factor at the params which should be the\n    params of the restricted model.\n\n    \"\"\"\n    if hess_kwds is None:\n        hess_kwds = {}\n    model = self.model\n    if params is None:\n        params = self.params\n    (exog_o1, exog_o2) = model._get_exogs()\n    if exog_o2 is None:\n        exog_o2 = np.ones((exog_o1.shape[0], 1))\n    k_mean = exog_o1.shape[1]\n    k_prec = exog_o2.shape[1]\n    if exog_extra is not None:\n        exog = np.column_stack((exog_o1, exog_extra))\n    else:\n        exog = exog_o1\n    if exog2_extra is not None:\n        exog2 = np.column_stack((exog_o2, exog2_extra))\n    else:\n        exog2 = exog_o2\n    k_mean_new = exog.shape[1]\n    k_prec_new = exog2.shape[1]\n    k_cm = k_mean_new - k_mean\n    k_cp = k_prec_new - k_prec\n    k_constraints = k_cm + k_cp\n    index_mean = np.arange(k_mean, k_mean_new)\n    index_prec = np.arange(k_mean_new + k_prec, k_mean_new + k_prec_new)\n    r_matrix = np.zeros((k_constraints, len(params) + k_constraints))\n    r_matrix[:k_cm, index_mean] = np.eye(k_cm)\n    r_matrix[k_cm:k_cm + k_cp, index_prec] = np.eye(k_cp)\n    if hasattr(model, 'score_hessian_factor'):\n        (sf, hf) = model.score_hessian_factor(params, return_hessian=True, **hess_kwds)\n    else:\n        sf = model.score_factor(params)\n        hf = model.hessian_factor(params, **hess_kwds)\n    (sf1, sf2) = sf\n    (hf11, hf12, hf22) = hf\n    d1 = sf1[:, None] * exog\n    d2 = sf2[:, None] * exog2\n    score_obs = np.column_stack((d1, d2))\n    d11 = (exog.T * hf11).dot(exog)\n    d12 = (exog.T * hf12).dot(exog2)\n    d22 = (exog2.T * hf22).dot(exog2)\n    hessian = np.block([[d11, d12], [d12.T, d22]])\n    return (score_obs, hessian, k_constraints, r_matrix)",
        "mutated": [
            "def _scorehess_extra(self, params=None, exog_extra=None, exog2_extra=None, hess_kwds=None):\n    if False:\n        i = 10\n    'Experimental helper function for variable addition score test.\\n\\n    This uses score and hessian factor at the params which should be the\\n    params of the restricted model.\\n\\n    '\n    if hess_kwds is None:\n        hess_kwds = {}\n    model = self.model\n    if params is None:\n        params = self.params\n    (exog_o1, exog_o2) = model._get_exogs()\n    if exog_o2 is None:\n        exog_o2 = np.ones((exog_o1.shape[0], 1))\n    k_mean = exog_o1.shape[1]\n    k_prec = exog_o2.shape[1]\n    if exog_extra is not None:\n        exog = np.column_stack((exog_o1, exog_extra))\n    else:\n        exog = exog_o1\n    if exog2_extra is not None:\n        exog2 = np.column_stack((exog_o2, exog2_extra))\n    else:\n        exog2 = exog_o2\n    k_mean_new = exog.shape[1]\n    k_prec_new = exog2.shape[1]\n    k_cm = k_mean_new - k_mean\n    k_cp = k_prec_new - k_prec\n    k_constraints = k_cm + k_cp\n    index_mean = np.arange(k_mean, k_mean_new)\n    index_prec = np.arange(k_mean_new + k_prec, k_mean_new + k_prec_new)\n    r_matrix = np.zeros((k_constraints, len(params) + k_constraints))\n    r_matrix[:k_cm, index_mean] = np.eye(k_cm)\n    r_matrix[k_cm:k_cm + k_cp, index_prec] = np.eye(k_cp)\n    if hasattr(model, 'score_hessian_factor'):\n        (sf, hf) = model.score_hessian_factor(params, return_hessian=True, **hess_kwds)\n    else:\n        sf = model.score_factor(params)\n        hf = model.hessian_factor(params, **hess_kwds)\n    (sf1, sf2) = sf\n    (hf11, hf12, hf22) = hf\n    d1 = sf1[:, None] * exog\n    d2 = sf2[:, None] * exog2\n    score_obs = np.column_stack((d1, d2))\n    d11 = (exog.T * hf11).dot(exog)\n    d12 = (exog.T * hf12).dot(exog2)\n    d22 = (exog2.T * hf22).dot(exog2)\n    hessian = np.block([[d11, d12], [d12.T, d22]])\n    return (score_obs, hessian, k_constraints, r_matrix)",
            "def _scorehess_extra(self, params=None, exog_extra=None, exog2_extra=None, hess_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Experimental helper function for variable addition score test.\\n\\n    This uses score and hessian factor at the params which should be the\\n    params of the restricted model.\\n\\n    '\n    if hess_kwds is None:\n        hess_kwds = {}\n    model = self.model\n    if params is None:\n        params = self.params\n    (exog_o1, exog_o2) = model._get_exogs()\n    if exog_o2 is None:\n        exog_o2 = np.ones((exog_o1.shape[0], 1))\n    k_mean = exog_o1.shape[1]\n    k_prec = exog_o2.shape[1]\n    if exog_extra is not None:\n        exog = np.column_stack((exog_o1, exog_extra))\n    else:\n        exog = exog_o1\n    if exog2_extra is not None:\n        exog2 = np.column_stack((exog_o2, exog2_extra))\n    else:\n        exog2 = exog_o2\n    k_mean_new = exog.shape[1]\n    k_prec_new = exog2.shape[1]\n    k_cm = k_mean_new - k_mean\n    k_cp = k_prec_new - k_prec\n    k_constraints = k_cm + k_cp\n    index_mean = np.arange(k_mean, k_mean_new)\n    index_prec = np.arange(k_mean_new + k_prec, k_mean_new + k_prec_new)\n    r_matrix = np.zeros((k_constraints, len(params) + k_constraints))\n    r_matrix[:k_cm, index_mean] = np.eye(k_cm)\n    r_matrix[k_cm:k_cm + k_cp, index_prec] = np.eye(k_cp)\n    if hasattr(model, 'score_hessian_factor'):\n        (sf, hf) = model.score_hessian_factor(params, return_hessian=True, **hess_kwds)\n    else:\n        sf = model.score_factor(params)\n        hf = model.hessian_factor(params, **hess_kwds)\n    (sf1, sf2) = sf\n    (hf11, hf12, hf22) = hf\n    d1 = sf1[:, None] * exog\n    d2 = sf2[:, None] * exog2\n    score_obs = np.column_stack((d1, d2))\n    d11 = (exog.T * hf11).dot(exog)\n    d12 = (exog.T * hf12).dot(exog2)\n    d22 = (exog2.T * hf22).dot(exog2)\n    hessian = np.block([[d11, d12], [d12.T, d22]])\n    return (score_obs, hessian, k_constraints, r_matrix)",
            "def _scorehess_extra(self, params=None, exog_extra=None, exog2_extra=None, hess_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Experimental helper function for variable addition score test.\\n\\n    This uses score and hessian factor at the params which should be the\\n    params of the restricted model.\\n\\n    '\n    if hess_kwds is None:\n        hess_kwds = {}\n    model = self.model\n    if params is None:\n        params = self.params\n    (exog_o1, exog_o2) = model._get_exogs()\n    if exog_o2 is None:\n        exog_o2 = np.ones((exog_o1.shape[0], 1))\n    k_mean = exog_o1.shape[1]\n    k_prec = exog_o2.shape[1]\n    if exog_extra is not None:\n        exog = np.column_stack((exog_o1, exog_extra))\n    else:\n        exog = exog_o1\n    if exog2_extra is not None:\n        exog2 = np.column_stack((exog_o2, exog2_extra))\n    else:\n        exog2 = exog_o2\n    k_mean_new = exog.shape[1]\n    k_prec_new = exog2.shape[1]\n    k_cm = k_mean_new - k_mean\n    k_cp = k_prec_new - k_prec\n    k_constraints = k_cm + k_cp\n    index_mean = np.arange(k_mean, k_mean_new)\n    index_prec = np.arange(k_mean_new + k_prec, k_mean_new + k_prec_new)\n    r_matrix = np.zeros((k_constraints, len(params) + k_constraints))\n    r_matrix[:k_cm, index_mean] = np.eye(k_cm)\n    r_matrix[k_cm:k_cm + k_cp, index_prec] = np.eye(k_cp)\n    if hasattr(model, 'score_hessian_factor'):\n        (sf, hf) = model.score_hessian_factor(params, return_hessian=True, **hess_kwds)\n    else:\n        sf = model.score_factor(params)\n        hf = model.hessian_factor(params, **hess_kwds)\n    (sf1, sf2) = sf\n    (hf11, hf12, hf22) = hf\n    d1 = sf1[:, None] * exog\n    d2 = sf2[:, None] * exog2\n    score_obs = np.column_stack((d1, d2))\n    d11 = (exog.T * hf11).dot(exog)\n    d12 = (exog.T * hf12).dot(exog2)\n    d22 = (exog2.T * hf22).dot(exog2)\n    hessian = np.block([[d11, d12], [d12.T, d22]])\n    return (score_obs, hessian, k_constraints, r_matrix)",
            "def _scorehess_extra(self, params=None, exog_extra=None, exog2_extra=None, hess_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Experimental helper function for variable addition score test.\\n\\n    This uses score and hessian factor at the params which should be the\\n    params of the restricted model.\\n\\n    '\n    if hess_kwds is None:\n        hess_kwds = {}\n    model = self.model\n    if params is None:\n        params = self.params\n    (exog_o1, exog_o2) = model._get_exogs()\n    if exog_o2 is None:\n        exog_o2 = np.ones((exog_o1.shape[0], 1))\n    k_mean = exog_o1.shape[1]\n    k_prec = exog_o2.shape[1]\n    if exog_extra is not None:\n        exog = np.column_stack((exog_o1, exog_extra))\n    else:\n        exog = exog_o1\n    if exog2_extra is not None:\n        exog2 = np.column_stack((exog_o2, exog2_extra))\n    else:\n        exog2 = exog_o2\n    k_mean_new = exog.shape[1]\n    k_prec_new = exog2.shape[1]\n    k_cm = k_mean_new - k_mean\n    k_cp = k_prec_new - k_prec\n    k_constraints = k_cm + k_cp\n    index_mean = np.arange(k_mean, k_mean_new)\n    index_prec = np.arange(k_mean_new + k_prec, k_mean_new + k_prec_new)\n    r_matrix = np.zeros((k_constraints, len(params) + k_constraints))\n    r_matrix[:k_cm, index_mean] = np.eye(k_cm)\n    r_matrix[k_cm:k_cm + k_cp, index_prec] = np.eye(k_cp)\n    if hasattr(model, 'score_hessian_factor'):\n        (sf, hf) = model.score_hessian_factor(params, return_hessian=True, **hess_kwds)\n    else:\n        sf = model.score_factor(params)\n        hf = model.hessian_factor(params, **hess_kwds)\n    (sf1, sf2) = sf\n    (hf11, hf12, hf22) = hf\n    d1 = sf1[:, None] * exog\n    d2 = sf2[:, None] * exog2\n    score_obs = np.column_stack((d1, d2))\n    d11 = (exog.T * hf11).dot(exog)\n    d12 = (exog.T * hf12).dot(exog2)\n    d22 = (exog2.T * hf22).dot(exog2)\n    hessian = np.block([[d11, d12], [d12.T, d22]])\n    return (score_obs, hessian, k_constraints, r_matrix)",
            "def _scorehess_extra(self, params=None, exog_extra=None, exog2_extra=None, hess_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Experimental helper function for variable addition score test.\\n\\n    This uses score and hessian factor at the params which should be the\\n    params of the restricted model.\\n\\n    '\n    if hess_kwds is None:\n        hess_kwds = {}\n    model = self.model\n    if params is None:\n        params = self.params\n    (exog_o1, exog_o2) = model._get_exogs()\n    if exog_o2 is None:\n        exog_o2 = np.ones((exog_o1.shape[0], 1))\n    k_mean = exog_o1.shape[1]\n    k_prec = exog_o2.shape[1]\n    if exog_extra is not None:\n        exog = np.column_stack((exog_o1, exog_extra))\n    else:\n        exog = exog_o1\n    if exog2_extra is not None:\n        exog2 = np.column_stack((exog_o2, exog2_extra))\n    else:\n        exog2 = exog_o2\n    k_mean_new = exog.shape[1]\n    k_prec_new = exog2.shape[1]\n    k_cm = k_mean_new - k_mean\n    k_cp = k_prec_new - k_prec\n    k_constraints = k_cm + k_cp\n    index_mean = np.arange(k_mean, k_mean_new)\n    index_prec = np.arange(k_mean_new + k_prec, k_mean_new + k_prec_new)\n    r_matrix = np.zeros((k_constraints, len(params) + k_constraints))\n    r_matrix[:k_cm, index_mean] = np.eye(k_cm)\n    r_matrix[k_cm:k_cm + k_cp, index_prec] = np.eye(k_cp)\n    if hasattr(model, 'score_hessian_factor'):\n        (sf, hf) = model.score_hessian_factor(params, return_hessian=True, **hess_kwds)\n    else:\n        sf = model.score_factor(params)\n        hf = model.hessian_factor(params, **hess_kwds)\n    (sf1, sf2) = sf\n    (hf11, hf12, hf22) = hf\n    d1 = sf1[:, None] * exog\n    d2 = sf2[:, None] * exog2\n    score_obs = np.column_stack((d1, d2))\n    d11 = (exog.T * hf11).dot(exog)\n    d12 = (exog.T * hf12).dot(exog2)\n    d22 = (exog2.T * hf22).dot(exog2)\n    hessian = np.block([[d11, d12], [d12.T, d22]])\n    return (score_obs, hessian, k_constraints, r_matrix)"
        ]
    },
    {
        "func_name": "im_ratio",
        "original": "def im_ratio(results):\n    res = getattr(results, '_results', results)\n    hess = res.model.hessian(res.params)\n    if res.cov_type == 'nonrobust':\n        score_obs = res.model.score_obs(res.params)\n        cov_score = score_obs.T @ score_obs\n        hessneg_inv = np.linalg.inv(-hess)\n        im_ratio = hessneg_inv @ cov_score\n    else:\n        im_ratio = res.cov_params() @ -hess\n    return im_ratio",
        "mutated": [
            "def im_ratio(results):\n    if False:\n        i = 10\n    res = getattr(results, '_results', results)\n    hess = res.model.hessian(res.params)\n    if res.cov_type == 'nonrobust':\n        score_obs = res.model.score_obs(res.params)\n        cov_score = score_obs.T @ score_obs\n        hessneg_inv = np.linalg.inv(-hess)\n        im_ratio = hessneg_inv @ cov_score\n    else:\n        im_ratio = res.cov_params() @ -hess\n    return im_ratio",
            "def im_ratio(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    res = getattr(results, '_results', results)\n    hess = res.model.hessian(res.params)\n    if res.cov_type == 'nonrobust':\n        score_obs = res.model.score_obs(res.params)\n        cov_score = score_obs.T @ score_obs\n        hessneg_inv = np.linalg.inv(-hess)\n        im_ratio = hessneg_inv @ cov_score\n    else:\n        im_ratio = res.cov_params() @ -hess\n    return im_ratio",
            "def im_ratio(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    res = getattr(results, '_results', results)\n    hess = res.model.hessian(res.params)\n    if res.cov_type == 'nonrobust':\n        score_obs = res.model.score_obs(res.params)\n        cov_score = score_obs.T @ score_obs\n        hessneg_inv = np.linalg.inv(-hess)\n        im_ratio = hessneg_inv @ cov_score\n    else:\n        im_ratio = res.cov_params() @ -hess\n    return im_ratio",
            "def im_ratio(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    res = getattr(results, '_results', results)\n    hess = res.model.hessian(res.params)\n    if res.cov_type == 'nonrobust':\n        score_obs = res.model.score_obs(res.params)\n        cov_score = score_obs.T @ score_obs\n        hessneg_inv = np.linalg.inv(-hess)\n        im_ratio = hessneg_inv @ cov_score\n    else:\n        im_ratio = res.cov_params() @ -hess\n    return im_ratio",
            "def im_ratio(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    res = getattr(results, '_results', results)\n    hess = res.model.hessian(res.params)\n    if res.cov_type == 'nonrobust':\n        score_obs = res.model.score_obs(res.params)\n        cov_score = score_obs.T @ score_obs\n        hessneg_inv = np.linalg.inv(-hess)\n        im_ratio = hessneg_inv @ cov_score\n    else:\n        im_ratio = res.cov_params() @ -hess\n    return im_ratio"
        ]
    },
    {
        "func_name": "tic",
        "original": "def tic(results):\n    \"\"\"Takeuchi information criterion for misspecified models\n\n    \"\"\"\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    tic = -2 * results.llf + 2 * np.trace(imr)\n    return tic",
        "mutated": [
            "def tic(results):\n    if False:\n        i = 10\n    'Takeuchi information criterion for misspecified models\\n\\n    '\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    tic = -2 * results.llf + 2 * np.trace(imr)\n    return tic",
            "def tic(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Takeuchi information criterion for misspecified models\\n\\n    '\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    tic = -2 * results.llf + 2 * np.trace(imr)\n    return tic",
            "def tic(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Takeuchi information criterion for misspecified models\\n\\n    '\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    tic = -2 * results.llf + 2 * np.trace(imr)\n    return tic",
            "def tic(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Takeuchi information criterion for misspecified models\\n\\n    '\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    tic = -2 * results.llf + 2 * np.trace(imr)\n    return tic",
            "def tic(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Takeuchi information criterion for misspecified models\\n\\n    '\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    tic = -2 * results.llf + 2 * np.trace(imr)\n    return tic"
        ]
    },
    {
        "func_name": "gbic",
        "original": "def gbic(results, gbicp=False):\n    \"\"\"generalized BIC for misspecified models\n\n    References\n    ----------\n    Lv, Jinchi, and Jun S. Liu. 2014. \"Model Selection Principles in\n    Misspecified Models.\" Journal of the Royal Statistical Society.\n    Series B (Statistical Methodology) 76 (1): 141\u201367.\n\n    \"\"\"\n    self = getattr(results, '_results', results)\n    k_params = self.df_model + 1\n    nobs = k_params + self.df_resid\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    imr_logdet = np.linalg.slogdet(imr)[1]\n    gbic = -2 * self.llf + k_params * np.log(nobs) - imr_logdet\n    gbicp = gbic + np.trace(imr)\n    return (gbic, gbicp)",
        "mutated": [
            "def gbic(results, gbicp=False):\n    if False:\n        i = 10\n    'generalized BIC for misspecified models\\n\\n    References\\n    ----------\\n    Lv, Jinchi, and Jun S. Liu. 2014. \"Model Selection Principles in\\n    Misspecified Models.\" Journal of the Royal Statistical Society.\\n    Series B (Statistical Methodology) 76 (1): 141\u201367.\\n\\n    '\n    self = getattr(results, '_results', results)\n    k_params = self.df_model + 1\n    nobs = k_params + self.df_resid\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    imr_logdet = np.linalg.slogdet(imr)[1]\n    gbic = -2 * self.llf + k_params * np.log(nobs) - imr_logdet\n    gbicp = gbic + np.trace(imr)\n    return (gbic, gbicp)",
            "def gbic(results, gbicp=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'generalized BIC for misspecified models\\n\\n    References\\n    ----------\\n    Lv, Jinchi, and Jun S. Liu. 2014. \"Model Selection Principles in\\n    Misspecified Models.\" Journal of the Royal Statistical Society.\\n    Series B (Statistical Methodology) 76 (1): 141\u201367.\\n\\n    '\n    self = getattr(results, '_results', results)\n    k_params = self.df_model + 1\n    nobs = k_params + self.df_resid\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    imr_logdet = np.linalg.slogdet(imr)[1]\n    gbic = -2 * self.llf + k_params * np.log(nobs) - imr_logdet\n    gbicp = gbic + np.trace(imr)\n    return (gbic, gbicp)",
            "def gbic(results, gbicp=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'generalized BIC for misspecified models\\n\\n    References\\n    ----------\\n    Lv, Jinchi, and Jun S. Liu. 2014. \"Model Selection Principles in\\n    Misspecified Models.\" Journal of the Royal Statistical Society.\\n    Series B (Statistical Methodology) 76 (1): 141\u201367.\\n\\n    '\n    self = getattr(results, '_results', results)\n    k_params = self.df_model + 1\n    nobs = k_params + self.df_resid\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    imr_logdet = np.linalg.slogdet(imr)[1]\n    gbic = -2 * self.llf + k_params * np.log(nobs) - imr_logdet\n    gbicp = gbic + np.trace(imr)\n    return (gbic, gbicp)",
            "def gbic(results, gbicp=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'generalized BIC for misspecified models\\n\\n    References\\n    ----------\\n    Lv, Jinchi, and Jun S. Liu. 2014. \"Model Selection Principles in\\n    Misspecified Models.\" Journal of the Royal Statistical Society.\\n    Series B (Statistical Methodology) 76 (1): 141\u201367.\\n\\n    '\n    self = getattr(results, '_results', results)\n    k_params = self.df_model + 1\n    nobs = k_params + self.df_resid\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    imr_logdet = np.linalg.slogdet(imr)[1]\n    gbic = -2 * self.llf + k_params * np.log(nobs) - imr_logdet\n    gbicp = gbic + np.trace(imr)\n    return (gbic, gbicp)",
            "def gbic(results, gbicp=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'generalized BIC for misspecified models\\n\\n    References\\n    ----------\\n    Lv, Jinchi, and Jun S. Liu. 2014. \"Model Selection Principles in\\n    Misspecified Models.\" Journal of the Royal Statistical Society.\\n    Series B (Statistical Methodology) 76 (1): 141\u201367.\\n\\n    '\n    self = getattr(results, '_results', results)\n    k_params = self.df_model + 1\n    nobs = k_params + self.df_resid\n    imr = getattr(results, 'im_ratio', im_ratio(results))\n    imr_logdet = np.linalg.slogdet(imr)[1]\n    gbic = -2 * self.llf + k_params * np.log(nobs) - imr_logdet\n    gbicp = gbic + np.trace(imr)\n    return (gbic, gbicp)"
        ]
    }
]