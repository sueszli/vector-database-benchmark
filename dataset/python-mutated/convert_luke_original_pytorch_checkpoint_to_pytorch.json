[
    {
        "func_name": "convert_luke_checkpoint",
        "original": "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    entity_vocab = load_entity_vocab(entity_vocab_path)\n    tokenizer = RobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, LukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[tokenizer.convert_tokens_to_ids(['@'])[0]].unsqueeze(0)\n    ent2_emb = word_emb[tokenizer.convert_tokens_to_ids(['#'])[0]].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_emb[entity_vocab['[MASK2]']] = entity_emb[entity_vocab['[MASK]']]\n    model = LukeModel(config=config).eval()\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    if not (len(missing_keys) == 1 and missing_keys[0] == 'embeddings.position_ids'):\n        raise ValueError(f\"Missing keys {', '.join(missing_keys)}. Expected only missing embeddings.position_ids\")\n    if not all((key.startswith('entity_predictions') or key.startswith('lm_head') for key in unexpected_keys)):\n        raise ValueError(f\"Unexpected keys {', '.join([key for key in unexpected_keys if not (key.startswith('entity_predictions') or key.startswith('lm_head'))])}\")\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'Top seed Ana Ivanovic said on Thursday she could hardly believe her luck as a fortuitous netcord helped the new world number one avoid a humiliating second- round exit at Wimbledon .'\n    span = (39, 42)\n    encoding = tokenizer(text, entity_spans=[span], add_prefix_space=True, return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 42, 1024))\n        expected_slice = torch.tensor([[0.0133, 0.0865, 0.0095], [0.3093, -0.2576, -0.7418], [-0.172, -0.2117, -0.2869]])\n    else:\n        expected_shape = torch.Size((1, 42, 768))\n        expected_slice = torch.tensor([[0.0037, 0.1368, -0.0091], [0.1099, 0.3329, -0.1095], [0.0765, 0.5335, 0.1179]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 1, 1024))\n        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[0.1457, 0.1044, 0.0174]])\n    if not outputs.entity_last_hidden_state.shape != expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)",
        "mutated": [
            "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    if False:\n        i = 10\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    entity_vocab = load_entity_vocab(entity_vocab_path)\n    tokenizer = RobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, LukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[tokenizer.convert_tokens_to_ids(['@'])[0]].unsqueeze(0)\n    ent2_emb = word_emb[tokenizer.convert_tokens_to_ids(['#'])[0]].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_emb[entity_vocab['[MASK2]']] = entity_emb[entity_vocab['[MASK]']]\n    model = LukeModel(config=config).eval()\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    if not (len(missing_keys) == 1 and missing_keys[0] == 'embeddings.position_ids'):\n        raise ValueError(f\"Missing keys {', '.join(missing_keys)}. Expected only missing embeddings.position_ids\")\n    if not all((key.startswith('entity_predictions') or key.startswith('lm_head') for key in unexpected_keys)):\n        raise ValueError(f\"Unexpected keys {', '.join([key for key in unexpected_keys if not (key.startswith('entity_predictions') or key.startswith('lm_head'))])}\")\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'Top seed Ana Ivanovic said on Thursday she could hardly believe her luck as a fortuitous netcord helped the new world number one avoid a humiliating second- round exit at Wimbledon .'\n    span = (39, 42)\n    encoding = tokenizer(text, entity_spans=[span], add_prefix_space=True, return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 42, 1024))\n        expected_slice = torch.tensor([[0.0133, 0.0865, 0.0095], [0.3093, -0.2576, -0.7418], [-0.172, -0.2117, -0.2869]])\n    else:\n        expected_shape = torch.Size((1, 42, 768))\n        expected_slice = torch.tensor([[0.0037, 0.1368, -0.0091], [0.1099, 0.3329, -0.1095], [0.0765, 0.5335, 0.1179]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 1, 1024))\n        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[0.1457, 0.1044, 0.0174]])\n    if not outputs.entity_last_hidden_state.shape != expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    entity_vocab = load_entity_vocab(entity_vocab_path)\n    tokenizer = RobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, LukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[tokenizer.convert_tokens_to_ids(['@'])[0]].unsqueeze(0)\n    ent2_emb = word_emb[tokenizer.convert_tokens_to_ids(['#'])[0]].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_emb[entity_vocab['[MASK2]']] = entity_emb[entity_vocab['[MASK]']]\n    model = LukeModel(config=config).eval()\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    if not (len(missing_keys) == 1 and missing_keys[0] == 'embeddings.position_ids'):\n        raise ValueError(f\"Missing keys {', '.join(missing_keys)}. Expected only missing embeddings.position_ids\")\n    if not all((key.startswith('entity_predictions') or key.startswith('lm_head') for key in unexpected_keys)):\n        raise ValueError(f\"Unexpected keys {', '.join([key for key in unexpected_keys if not (key.startswith('entity_predictions') or key.startswith('lm_head'))])}\")\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'Top seed Ana Ivanovic said on Thursday she could hardly believe her luck as a fortuitous netcord helped the new world number one avoid a humiliating second- round exit at Wimbledon .'\n    span = (39, 42)\n    encoding = tokenizer(text, entity_spans=[span], add_prefix_space=True, return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 42, 1024))\n        expected_slice = torch.tensor([[0.0133, 0.0865, 0.0095], [0.3093, -0.2576, -0.7418], [-0.172, -0.2117, -0.2869]])\n    else:\n        expected_shape = torch.Size((1, 42, 768))\n        expected_slice = torch.tensor([[0.0037, 0.1368, -0.0091], [0.1099, 0.3329, -0.1095], [0.0765, 0.5335, 0.1179]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 1, 1024))\n        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[0.1457, 0.1044, 0.0174]])\n    if not outputs.entity_last_hidden_state.shape != expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    entity_vocab = load_entity_vocab(entity_vocab_path)\n    tokenizer = RobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, LukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[tokenizer.convert_tokens_to_ids(['@'])[0]].unsqueeze(0)\n    ent2_emb = word_emb[tokenizer.convert_tokens_to_ids(['#'])[0]].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_emb[entity_vocab['[MASK2]']] = entity_emb[entity_vocab['[MASK]']]\n    model = LukeModel(config=config).eval()\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    if not (len(missing_keys) == 1 and missing_keys[0] == 'embeddings.position_ids'):\n        raise ValueError(f\"Missing keys {', '.join(missing_keys)}. Expected only missing embeddings.position_ids\")\n    if not all((key.startswith('entity_predictions') or key.startswith('lm_head') for key in unexpected_keys)):\n        raise ValueError(f\"Unexpected keys {', '.join([key for key in unexpected_keys if not (key.startswith('entity_predictions') or key.startswith('lm_head'))])}\")\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'Top seed Ana Ivanovic said on Thursday she could hardly believe her luck as a fortuitous netcord helped the new world number one avoid a humiliating second- round exit at Wimbledon .'\n    span = (39, 42)\n    encoding = tokenizer(text, entity_spans=[span], add_prefix_space=True, return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 42, 1024))\n        expected_slice = torch.tensor([[0.0133, 0.0865, 0.0095], [0.3093, -0.2576, -0.7418], [-0.172, -0.2117, -0.2869]])\n    else:\n        expected_shape = torch.Size((1, 42, 768))\n        expected_slice = torch.tensor([[0.0037, 0.1368, -0.0091], [0.1099, 0.3329, -0.1095], [0.0765, 0.5335, 0.1179]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 1, 1024))\n        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[0.1457, 0.1044, 0.0174]])\n    if not outputs.entity_last_hidden_state.shape != expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    entity_vocab = load_entity_vocab(entity_vocab_path)\n    tokenizer = RobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, LukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[tokenizer.convert_tokens_to_ids(['@'])[0]].unsqueeze(0)\n    ent2_emb = word_emb[tokenizer.convert_tokens_to_ids(['#'])[0]].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_emb[entity_vocab['[MASK2]']] = entity_emb[entity_vocab['[MASK]']]\n    model = LukeModel(config=config).eval()\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    if not (len(missing_keys) == 1 and missing_keys[0] == 'embeddings.position_ids'):\n        raise ValueError(f\"Missing keys {', '.join(missing_keys)}. Expected only missing embeddings.position_ids\")\n    if not all((key.startswith('entity_predictions') or key.startswith('lm_head') for key in unexpected_keys)):\n        raise ValueError(f\"Unexpected keys {', '.join([key for key in unexpected_keys if not (key.startswith('entity_predictions') or key.startswith('lm_head'))])}\")\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'Top seed Ana Ivanovic said on Thursday she could hardly believe her luck as a fortuitous netcord helped the new world number one avoid a humiliating second- round exit at Wimbledon .'\n    span = (39, 42)\n    encoding = tokenizer(text, entity_spans=[span], add_prefix_space=True, return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 42, 1024))\n        expected_slice = torch.tensor([[0.0133, 0.0865, 0.0095], [0.3093, -0.2576, -0.7418], [-0.172, -0.2117, -0.2869]])\n    else:\n        expected_shape = torch.Size((1, 42, 768))\n        expected_slice = torch.tensor([[0.0037, 0.1368, -0.0091], [0.1099, 0.3329, -0.1095], [0.0765, 0.5335, 0.1179]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 1, 1024))\n        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[0.1457, 0.1044, 0.0174]])\n    if not outputs.entity_last_hidden_state.shape != expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    entity_vocab = load_entity_vocab(entity_vocab_path)\n    tokenizer = RobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, LukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[tokenizer.convert_tokens_to_ids(['@'])[0]].unsqueeze(0)\n    ent2_emb = word_emb[tokenizer.convert_tokens_to_ids(['#'])[0]].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_emb[entity_vocab['[MASK2]']] = entity_emb[entity_vocab['[MASK]']]\n    model = LukeModel(config=config).eval()\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    if not (len(missing_keys) == 1 and missing_keys[0] == 'embeddings.position_ids'):\n        raise ValueError(f\"Missing keys {', '.join(missing_keys)}. Expected only missing embeddings.position_ids\")\n    if not all((key.startswith('entity_predictions') or key.startswith('lm_head') for key in unexpected_keys)):\n        raise ValueError(f\"Unexpected keys {', '.join([key for key in unexpected_keys if not (key.startswith('entity_predictions') or key.startswith('lm_head'))])}\")\n    tokenizer = LukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'Top seed Ana Ivanovic said on Thursday she could hardly believe her luck as a fortuitous netcord helped the new world number one avoid a humiliating second- round exit at Wimbledon .'\n    span = (39, 42)\n    encoding = tokenizer(text, entity_spans=[span], add_prefix_space=True, return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 42, 1024))\n        expected_slice = torch.tensor([[0.0133, 0.0865, 0.0095], [0.3093, -0.2576, -0.7418], [-0.172, -0.2117, -0.2869]])\n    else:\n        expected_shape = torch.Size((1, 42, 768))\n        expected_slice = torch.tensor([[0.0037, 0.1368, -0.0091], [0.1099, 0.3329, -0.1095], [0.0765, 0.5335, 0.1179]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        expected_shape = torch.Size((1, 1, 1024))\n        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[0.1457, 0.1044, 0.0174]])\n    if not outputs.entity_last_hidden_state.shape != expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)"
        ]
    },
    {
        "func_name": "load_entity_vocab",
        "original": "def load_entity_vocab(entity_vocab_path):\n    entity_vocab = {}\n    with open(entity_vocab_path, 'r', encoding='utf-8') as f:\n        for (index, line) in enumerate(f):\n            (title, _) = line.rstrip().split('\\t')\n            entity_vocab[title] = index\n    return entity_vocab",
        "mutated": [
            "def load_entity_vocab(entity_vocab_path):\n    if False:\n        i = 10\n    entity_vocab = {}\n    with open(entity_vocab_path, 'r', encoding='utf-8') as f:\n        for (index, line) in enumerate(f):\n            (title, _) = line.rstrip().split('\\t')\n            entity_vocab[title] = index\n    return entity_vocab",
            "def load_entity_vocab(entity_vocab_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    entity_vocab = {}\n    with open(entity_vocab_path, 'r', encoding='utf-8') as f:\n        for (index, line) in enumerate(f):\n            (title, _) = line.rstrip().split('\\t')\n            entity_vocab[title] = index\n    return entity_vocab",
            "def load_entity_vocab(entity_vocab_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    entity_vocab = {}\n    with open(entity_vocab_path, 'r', encoding='utf-8') as f:\n        for (index, line) in enumerate(f):\n            (title, _) = line.rstrip().split('\\t')\n            entity_vocab[title] = index\n    return entity_vocab",
            "def load_entity_vocab(entity_vocab_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    entity_vocab = {}\n    with open(entity_vocab_path, 'r', encoding='utf-8') as f:\n        for (index, line) in enumerate(f):\n            (title, _) = line.rstrip().split('\\t')\n            entity_vocab[title] = index\n    return entity_vocab",
            "def load_entity_vocab(entity_vocab_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    entity_vocab = {}\n    with open(entity_vocab_path, 'r', encoding='utf-8') as f:\n        for (index, line) in enumerate(f):\n            (title, _) = line.rstrip().split('\\t')\n            entity_vocab[title] = index\n    return entity_vocab"
        ]
    }
]