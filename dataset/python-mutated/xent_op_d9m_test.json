[
    {
        "func_name": "testExceptionThrowing",
        "original": "@test_util.run_gpu_only\n@test_util.run_in_graph_and_eager_modes\ndef testExceptionThrowing(self):\n    with self.session(), test_util.force_gpu():\n        for dtype in [dtypes.float16, dtypes.float32, dtypes.float64]:\n            features = constant_op.constant([[0.3, 0.5], [0.5, 0.6]], dtype=dtype)\n            labels = constant_op.constant([[0.2, 0.4], [0.1, 0.2]], dtype=dtype)\n            with self.assertRaisesRegex(errors_impl.UnimplementedError, 'The GPU implementation of SoftmaxCrossEntropyWithLogits that ' + 'would have been executed is not deterministic. Note that the ' + 'Python API uses an alternative, deterministic, GPU-accelerated ' + 'path when determinism is enabled.'):\n                result = gen_nn_ops.softmax_cross_entropy_with_logits(features=features, labels=labels)\n                self.evaluate(result)",
        "mutated": [
            "@test_util.run_gpu_only\n@test_util.run_in_graph_and_eager_modes\ndef testExceptionThrowing(self):\n    if False:\n        i = 10\n    with self.session(), test_util.force_gpu():\n        for dtype in [dtypes.float16, dtypes.float32, dtypes.float64]:\n            features = constant_op.constant([[0.3, 0.5], [0.5, 0.6]], dtype=dtype)\n            labels = constant_op.constant([[0.2, 0.4], [0.1, 0.2]], dtype=dtype)\n            with self.assertRaisesRegex(errors_impl.UnimplementedError, 'The GPU implementation of SoftmaxCrossEntropyWithLogits that ' + 'would have been executed is not deterministic. Note that the ' + 'Python API uses an alternative, deterministic, GPU-accelerated ' + 'path when determinism is enabled.'):\n                result = gen_nn_ops.softmax_cross_entropy_with_logits(features=features, labels=labels)\n                self.evaluate(result)",
            "@test_util.run_gpu_only\n@test_util.run_in_graph_and_eager_modes\ndef testExceptionThrowing(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with self.session(), test_util.force_gpu():\n        for dtype in [dtypes.float16, dtypes.float32, dtypes.float64]:\n            features = constant_op.constant([[0.3, 0.5], [0.5, 0.6]], dtype=dtype)\n            labels = constant_op.constant([[0.2, 0.4], [0.1, 0.2]], dtype=dtype)\n            with self.assertRaisesRegex(errors_impl.UnimplementedError, 'The GPU implementation of SoftmaxCrossEntropyWithLogits that ' + 'would have been executed is not deterministic. Note that the ' + 'Python API uses an alternative, deterministic, GPU-accelerated ' + 'path when determinism is enabled.'):\n                result = gen_nn_ops.softmax_cross_entropy_with_logits(features=features, labels=labels)\n                self.evaluate(result)",
            "@test_util.run_gpu_only\n@test_util.run_in_graph_and_eager_modes\ndef testExceptionThrowing(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with self.session(), test_util.force_gpu():\n        for dtype in [dtypes.float16, dtypes.float32, dtypes.float64]:\n            features = constant_op.constant([[0.3, 0.5], [0.5, 0.6]], dtype=dtype)\n            labels = constant_op.constant([[0.2, 0.4], [0.1, 0.2]], dtype=dtype)\n            with self.assertRaisesRegex(errors_impl.UnimplementedError, 'The GPU implementation of SoftmaxCrossEntropyWithLogits that ' + 'would have been executed is not deterministic. Note that the ' + 'Python API uses an alternative, deterministic, GPU-accelerated ' + 'path when determinism is enabled.'):\n                result = gen_nn_ops.softmax_cross_entropy_with_logits(features=features, labels=labels)\n                self.evaluate(result)",
            "@test_util.run_gpu_only\n@test_util.run_in_graph_and_eager_modes\ndef testExceptionThrowing(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with self.session(), test_util.force_gpu():\n        for dtype in [dtypes.float16, dtypes.float32, dtypes.float64]:\n            features = constant_op.constant([[0.3, 0.5], [0.5, 0.6]], dtype=dtype)\n            labels = constant_op.constant([[0.2, 0.4], [0.1, 0.2]], dtype=dtype)\n            with self.assertRaisesRegex(errors_impl.UnimplementedError, 'The GPU implementation of SoftmaxCrossEntropyWithLogits that ' + 'would have been executed is not deterministic. Note that the ' + 'Python API uses an alternative, deterministic, GPU-accelerated ' + 'path when determinism is enabled.'):\n                result = gen_nn_ops.softmax_cross_entropy_with_logits(features=features, labels=labels)\n                self.evaluate(result)",
            "@test_util.run_gpu_only\n@test_util.run_in_graph_and_eager_modes\ndef testExceptionThrowing(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with self.session(), test_util.force_gpu():\n        for dtype in [dtypes.float16, dtypes.float32, dtypes.float64]:\n            features = constant_op.constant([[0.3, 0.5], [0.5, 0.6]], dtype=dtype)\n            labels = constant_op.constant([[0.2, 0.4], [0.1, 0.2]], dtype=dtype)\n            with self.assertRaisesRegex(errors_impl.UnimplementedError, 'The GPU implementation of SoftmaxCrossEntropyWithLogits that ' + 'would have been executed is not deterministic. Note that the ' + 'Python API uses an alternative, deterministic, GPU-accelerated ' + 'path when determinism is enabled.'):\n                result = gen_nn_ops.softmax_cross_entropy_with_logits(features=features, labels=labels)\n                self.evaluate(result)"
        ]
    },
    {
        "func_name": "normalize",
        "original": "def normalize(row):\n    return row / row.sum()",
        "mutated": [
            "def normalize(row):\n    if False:\n        i = 10\n    return row / row.sum()",
            "def normalize(row):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return row / row.sum()",
            "def normalize(row):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return row / row.sum()",
            "def normalize(row):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return row / row.sum()",
            "def normalize(row):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return row / row.sum()"
        ]
    },
    {
        "func_name": "_randomFloats",
        "original": "def _randomFloats(self, shape, dtype, normalized_rows=False):\n    a = (2 * np.random.random_sample(shape) - 1).astype(dtype)\n    if normalized_rows:\n\n        def normalize(row):\n            return row / row.sum()\n        a = np.apply_along_axis(normalize, 1, a)\n    return constant_op.constant(a)",
        "mutated": [
            "def _randomFloats(self, shape, dtype, normalized_rows=False):\n    if False:\n        i = 10\n    a = (2 * np.random.random_sample(shape) - 1).astype(dtype)\n    if normalized_rows:\n\n        def normalize(row):\n            return row / row.sum()\n        a = np.apply_along_axis(normalize, 1, a)\n    return constant_op.constant(a)",
            "def _randomFloats(self, shape, dtype, normalized_rows=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    a = (2 * np.random.random_sample(shape) - 1).astype(dtype)\n    if normalized_rows:\n\n        def normalize(row):\n            return row / row.sum()\n        a = np.apply_along_axis(normalize, 1, a)\n    return constant_op.constant(a)",
            "def _randomFloats(self, shape, dtype, normalized_rows=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    a = (2 * np.random.random_sample(shape) - 1).astype(dtype)\n    if normalized_rows:\n\n        def normalize(row):\n            return row / row.sum()\n        a = np.apply_along_axis(normalize, 1, a)\n    return constant_op.constant(a)",
            "def _randomFloats(self, shape, dtype, normalized_rows=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    a = (2 * np.random.random_sample(shape) - 1).astype(dtype)\n    if normalized_rows:\n\n        def normalize(row):\n            return row / row.sum()\n        a = np.apply_along_axis(normalize, 1, a)\n    return constant_op.constant(a)",
            "def _randomFloats(self, shape, dtype, normalized_rows=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    a = (2 * np.random.random_sample(shape) - 1).astype(dtype)\n    if normalized_rows:\n\n        def normalize(row):\n            return row / row.sum()\n        a = np.apply_along_axis(normalize, 1, a)\n    return constant_op.constant(a)"
        ]
    },
    {
        "func_name": "_generateInputs",
        "original": "def _generateInputs(self, dtype, seed=123, forward_not_backward=False):\n    batch_size = 1024\n    if forward_not_backward and dtype == np.float16:\n        classes_count = 20000\n    else:\n        classes_count = 3000\n    shape = (batch_size, classes_count)\n    np.random.seed(seed)\n    labels = self._randomFloats(shape, dtype, normalized_rows=True)\n    logits = self._randomFloats(shape, dtype)\n    return (labels, logits)",
        "mutated": [
            "def _generateInputs(self, dtype, seed=123, forward_not_backward=False):\n    if False:\n        i = 10\n    batch_size = 1024\n    if forward_not_backward and dtype == np.float16:\n        classes_count = 20000\n    else:\n        classes_count = 3000\n    shape = (batch_size, classes_count)\n    np.random.seed(seed)\n    labels = self._randomFloats(shape, dtype, normalized_rows=True)\n    logits = self._randomFloats(shape, dtype)\n    return (labels, logits)",
            "def _generateInputs(self, dtype, seed=123, forward_not_backward=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    batch_size = 1024\n    if forward_not_backward and dtype == np.float16:\n        classes_count = 20000\n    else:\n        classes_count = 3000\n    shape = (batch_size, classes_count)\n    np.random.seed(seed)\n    labels = self._randomFloats(shape, dtype, normalized_rows=True)\n    logits = self._randomFloats(shape, dtype)\n    return (labels, logits)",
            "def _generateInputs(self, dtype, seed=123, forward_not_backward=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    batch_size = 1024\n    if forward_not_backward and dtype == np.float16:\n        classes_count = 20000\n    else:\n        classes_count = 3000\n    shape = (batch_size, classes_count)\n    np.random.seed(seed)\n    labels = self._randomFloats(shape, dtype, normalized_rows=True)\n    logits = self._randomFloats(shape, dtype)\n    return (labels, logits)",
            "def _generateInputs(self, dtype, seed=123, forward_not_backward=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    batch_size = 1024\n    if forward_not_backward and dtype == np.float16:\n        classes_count = 20000\n    else:\n        classes_count = 3000\n    shape = (batch_size, classes_count)\n    np.random.seed(seed)\n    labels = self._randomFloats(shape, dtype, normalized_rows=True)\n    logits = self._randomFloats(shape, dtype)\n    return (labels, logits)",
            "def _generateInputs(self, dtype, seed=123, forward_not_backward=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    batch_size = 1024\n    if forward_not_backward and dtype == np.float16:\n        classes_count = 20000\n    else:\n        classes_count = 3000\n    shape = (batch_size, classes_count)\n    np.random.seed(seed)\n    labels = self._randomFloats(shape, dtype, normalized_rows=True)\n    logits = self._randomFloats(shape, dtype)\n    return (labels, logits)"
        ]
    },
    {
        "func_name": "testForward",
        "original": "@test_util.run_in_graph_and_eager_modes\ndef testForward(self):\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            for trial in range(5):\n                seed = 123 + trial\n                (labels, logits) = self._generateInputs(dtype, seed=seed, forward_not_backward=True)\n                result_a = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                result_b = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                self.assertAllEqual(result_a, result_b)",
        "mutated": [
            "@test_util.run_in_graph_and_eager_modes\ndef testForward(self):\n    if False:\n        i = 10\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            for trial in range(5):\n                seed = 123 + trial\n                (labels, logits) = self._generateInputs(dtype, seed=seed, forward_not_backward=True)\n                result_a = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                result_b = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                self.assertAllEqual(result_a, result_b)",
            "@test_util.run_in_graph_and_eager_modes\ndef testForward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            for trial in range(5):\n                seed = 123 + trial\n                (labels, logits) = self._generateInputs(dtype, seed=seed, forward_not_backward=True)\n                result_a = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                result_b = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                self.assertAllEqual(result_a, result_b)",
            "@test_util.run_in_graph_and_eager_modes\ndef testForward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            for trial in range(5):\n                seed = 123 + trial\n                (labels, logits) = self._generateInputs(dtype, seed=seed, forward_not_backward=True)\n                result_a = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                result_b = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                self.assertAllEqual(result_a, result_b)",
            "@test_util.run_in_graph_and_eager_modes\ndef testForward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            for trial in range(5):\n                seed = 123 + trial\n                (labels, logits) = self._generateInputs(dtype, seed=seed, forward_not_backward=True)\n                result_a = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                result_b = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                self.assertAllEqual(result_a, result_b)",
            "@test_util.run_in_graph_and_eager_modes\ndef testForward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            for trial in range(5):\n                seed = 123 + trial\n                (labels, logits) = self._generateInputs(dtype, seed=seed, forward_not_backward=True)\n                result_a = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                result_b = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                self.assertAllEqual(result_a, result_b)"
        ]
    },
    {
        "func_name": "gradients",
        "original": "def gradients(seed):\n    np.random.seed(seed)\n    upstream_gradients = self._randomFloats(output_shape, dtype)\n    with backprop.GradientTape(persistent=True) as tape:\n        tape.watch(labels)\n        tape.watch(logits)\n        op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n        gradient_injector_output = op_output * upstream_gradients\n    return tape.gradient(gradient_injector_output, [labels, logits])",
        "mutated": [
            "def gradients(seed):\n    if False:\n        i = 10\n    np.random.seed(seed)\n    upstream_gradients = self._randomFloats(output_shape, dtype)\n    with backprop.GradientTape(persistent=True) as tape:\n        tape.watch(labels)\n        tape.watch(logits)\n        op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n        gradient_injector_output = op_output * upstream_gradients\n    return tape.gradient(gradient_injector_output, [labels, logits])",
            "def gradients(seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(seed)\n    upstream_gradients = self._randomFloats(output_shape, dtype)\n    with backprop.GradientTape(persistent=True) as tape:\n        tape.watch(labels)\n        tape.watch(logits)\n        op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n        gradient_injector_output = op_output * upstream_gradients\n    return tape.gradient(gradient_injector_output, [labels, logits])",
            "def gradients(seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(seed)\n    upstream_gradients = self._randomFloats(output_shape, dtype)\n    with backprop.GradientTape(persistent=True) as tape:\n        tape.watch(labels)\n        tape.watch(logits)\n        op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n        gradient_injector_output = op_output * upstream_gradients\n    return tape.gradient(gradient_injector_output, [labels, logits])",
            "def gradients(seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(seed)\n    upstream_gradients = self._randomFloats(output_shape, dtype)\n    with backprop.GradientTape(persistent=True) as tape:\n        tape.watch(labels)\n        tape.watch(logits)\n        op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n        gradient_injector_output = op_output * upstream_gradients\n    return tape.gradient(gradient_injector_output, [labels, logits])",
            "def gradients(seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(seed)\n    upstream_gradients = self._randomFloats(output_shape, dtype)\n    with backprop.GradientTape(persistent=True) as tape:\n        tape.watch(labels)\n        tape.watch(logits)\n        op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n        gradient_injector_output = op_output * upstream_gradients\n    return tape.gradient(gradient_injector_output, [labels, logits])"
        ]
    },
    {
        "func_name": "testBackward",
        "original": "@test_util.run_in_graph_and_eager_modes\ndef testBackward(self):\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            (labels, logits) = self._generateInputs(dtype, seed=456)\n            output_shape = labels.shape[0]\n\n            def gradients(seed):\n                np.random.seed(seed)\n                upstream_gradients = self._randomFloats(output_shape, dtype)\n                with backprop.GradientTape(persistent=True) as tape:\n                    tape.watch(labels)\n                    tape.watch(logits)\n                    op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                    gradient_injector_output = op_output * upstream_gradients\n                return tape.gradient(gradient_injector_output, [labels, logits])\n            for trial in range(5):\n                seed = 456 + trial\n                (labels_grad_a, logits_grad_a) = gradients(seed=seed)\n                (labels_grad_b, logits_grad_b) = gradients(seed=seed)\n                self.assertAllEqual(labels_grad_a, labels_grad_b)\n                self.assertAllEqual(logits_grad_a, logits_grad_b)",
        "mutated": [
            "@test_util.run_in_graph_and_eager_modes\ndef testBackward(self):\n    if False:\n        i = 10\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            (labels, logits) = self._generateInputs(dtype, seed=456)\n            output_shape = labels.shape[0]\n\n            def gradients(seed):\n                np.random.seed(seed)\n                upstream_gradients = self._randomFloats(output_shape, dtype)\n                with backprop.GradientTape(persistent=True) as tape:\n                    tape.watch(labels)\n                    tape.watch(logits)\n                    op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                    gradient_injector_output = op_output * upstream_gradients\n                return tape.gradient(gradient_injector_output, [labels, logits])\n            for trial in range(5):\n                seed = 456 + trial\n                (labels_grad_a, logits_grad_a) = gradients(seed=seed)\n                (labels_grad_b, logits_grad_b) = gradients(seed=seed)\n                self.assertAllEqual(labels_grad_a, labels_grad_b)\n                self.assertAllEqual(logits_grad_a, logits_grad_b)",
            "@test_util.run_in_graph_and_eager_modes\ndef testBackward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            (labels, logits) = self._generateInputs(dtype, seed=456)\n            output_shape = labels.shape[0]\n\n            def gradients(seed):\n                np.random.seed(seed)\n                upstream_gradients = self._randomFloats(output_shape, dtype)\n                with backprop.GradientTape(persistent=True) as tape:\n                    tape.watch(labels)\n                    tape.watch(logits)\n                    op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                    gradient_injector_output = op_output * upstream_gradients\n                return tape.gradient(gradient_injector_output, [labels, logits])\n            for trial in range(5):\n                seed = 456 + trial\n                (labels_grad_a, logits_grad_a) = gradients(seed=seed)\n                (labels_grad_b, logits_grad_b) = gradients(seed=seed)\n                self.assertAllEqual(labels_grad_a, labels_grad_b)\n                self.assertAllEqual(logits_grad_a, logits_grad_b)",
            "@test_util.run_in_graph_and_eager_modes\ndef testBackward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            (labels, logits) = self._generateInputs(dtype, seed=456)\n            output_shape = labels.shape[0]\n\n            def gradients(seed):\n                np.random.seed(seed)\n                upstream_gradients = self._randomFloats(output_shape, dtype)\n                with backprop.GradientTape(persistent=True) as tape:\n                    tape.watch(labels)\n                    tape.watch(logits)\n                    op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                    gradient_injector_output = op_output * upstream_gradients\n                return tape.gradient(gradient_injector_output, [labels, logits])\n            for trial in range(5):\n                seed = 456 + trial\n                (labels_grad_a, logits_grad_a) = gradients(seed=seed)\n                (labels_grad_b, logits_grad_b) = gradients(seed=seed)\n                self.assertAllEqual(labels_grad_a, labels_grad_b)\n                self.assertAllEqual(logits_grad_a, logits_grad_b)",
            "@test_util.run_in_graph_and_eager_modes\ndef testBackward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            (labels, logits) = self._generateInputs(dtype, seed=456)\n            output_shape = labels.shape[0]\n\n            def gradients(seed):\n                np.random.seed(seed)\n                upstream_gradients = self._randomFloats(output_shape, dtype)\n                with backprop.GradientTape(persistent=True) as tape:\n                    tape.watch(labels)\n                    tape.watch(logits)\n                    op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                    gradient_injector_output = op_output * upstream_gradients\n                return tape.gradient(gradient_injector_output, [labels, logits])\n            for trial in range(5):\n                seed = 456 + trial\n                (labels_grad_a, logits_grad_a) = gradients(seed=seed)\n                (labels_grad_b, logits_grad_b) = gradients(seed=seed)\n                self.assertAllEqual(labels_grad_a, labels_grad_b)\n                self.assertAllEqual(logits_grad_a, logits_grad_b)",
            "@test_util.run_in_graph_and_eager_modes\ndef testBackward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with self.cached_session():\n        for dtype in [np.float16, np.float32, np.float64, dtypes.bfloat16.as_numpy_dtype]:\n            (labels, logits) = self._generateInputs(dtype, seed=456)\n            output_shape = labels.shape[0]\n\n            def gradients(seed):\n                np.random.seed(seed)\n                upstream_gradients = self._randomFloats(output_shape, dtype)\n                with backprop.GradientTape(persistent=True) as tape:\n                    tape.watch(labels)\n                    tape.watch(logits)\n                    op_output = nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n                    gradient_injector_output = op_output * upstream_gradients\n                return tape.gradient(gradient_injector_output, [labels, logits])\n            for trial in range(5):\n                seed = 456 + trial\n                (labels_grad_a, logits_grad_a) = gradients(seed=seed)\n                (labels_grad_b, logits_grad_b) = gradients(seed=seed)\n                self.assertAllEqual(labels_grad_a, labels_grad_b)\n                self.assertAllEqual(logits_grad_a, logits_grad_b)"
        ]
    },
    {
        "func_name": "testSingleClass",
        "original": "def testSingleClass(self):\n    \"\"\"Modify testing of gradient for single-class case.\n\n    The deterministic implementation does not produce the gradients expected by\n    the original test (for the nondeterministic functionality) when the labels\n    vector is not a valid probability distribution.\n\n    labels: [[-1.], [0.], [1.], [1.]]\n    logits: [[1.], [-1.], [0.], [1.]]\n\n                   nondeterministic               deterministic\n    dloss/dlogits: [[2.0], [1.0], [0.0], [0.0]]   [[0.0], [0.0], [0.0], [0.0]]\n\n    Note that only the second two label vectors are valid probability\n    distributions (as required by the API) and that the gradient matches for\n    those cases.\n\n    TODO(duncanriach): Further investigate the source of the difference in\n                       the gradients for this case.\n    \"\"\"\n    self._testSingleClass(expected_gradient=[[0.0], [0.0], [0.0], [0.0]])",
        "mutated": [
            "def testSingleClass(self):\n    if False:\n        i = 10\n    'Modify testing of gradient for single-class case.\\n\\n    The deterministic implementation does not produce the gradients expected by\\n    the original test (for the nondeterministic functionality) when the labels\\n    vector is not a valid probability distribution.\\n\\n    labels: [[-1.], [0.], [1.], [1.]]\\n    logits: [[1.], [-1.], [0.], [1.]]\\n\\n                   nondeterministic               deterministic\\n    dloss/dlogits: [[2.0], [1.0], [0.0], [0.0]]   [[0.0], [0.0], [0.0], [0.0]]\\n\\n    Note that only the second two label vectors are valid probability\\n    distributions (as required by the API) and that the gradient matches for\\n    those cases.\\n\\n    TODO(duncanriach): Further investigate the source of the difference in\\n                       the gradients for this case.\\n    '\n    self._testSingleClass(expected_gradient=[[0.0], [0.0], [0.0], [0.0]])",
            "def testSingleClass(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Modify testing of gradient for single-class case.\\n\\n    The deterministic implementation does not produce the gradients expected by\\n    the original test (for the nondeterministic functionality) when the labels\\n    vector is not a valid probability distribution.\\n\\n    labels: [[-1.], [0.], [1.], [1.]]\\n    logits: [[1.], [-1.], [0.], [1.]]\\n\\n                   nondeterministic               deterministic\\n    dloss/dlogits: [[2.0], [1.0], [0.0], [0.0]]   [[0.0], [0.0], [0.0], [0.0]]\\n\\n    Note that only the second two label vectors are valid probability\\n    distributions (as required by the API) and that the gradient matches for\\n    those cases.\\n\\n    TODO(duncanriach): Further investigate the source of the difference in\\n                       the gradients for this case.\\n    '\n    self._testSingleClass(expected_gradient=[[0.0], [0.0], [0.0], [0.0]])",
            "def testSingleClass(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Modify testing of gradient for single-class case.\\n\\n    The deterministic implementation does not produce the gradients expected by\\n    the original test (for the nondeterministic functionality) when the labels\\n    vector is not a valid probability distribution.\\n\\n    labels: [[-1.], [0.], [1.], [1.]]\\n    logits: [[1.], [-1.], [0.], [1.]]\\n\\n                   nondeterministic               deterministic\\n    dloss/dlogits: [[2.0], [1.0], [0.0], [0.0]]   [[0.0], [0.0], [0.0], [0.0]]\\n\\n    Note that only the second two label vectors are valid probability\\n    distributions (as required by the API) and that the gradient matches for\\n    those cases.\\n\\n    TODO(duncanriach): Further investigate the source of the difference in\\n                       the gradients for this case.\\n    '\n    self._testSingleClass(expected_gradient=[[0.0], [0.0], [0.0], [0.0]])",
            "def testSingleClass(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Modify testing of gradient for single-class case.\\n\\n    The deterministic implementation does not produce the gradients expected by\\n    the original test (for the nondeterministic functionality) when the labels\\n    vector is not a valid probability distribution.\\n\\n    labels: [[-1.], [0.], [1.], [1.]]\\n    logits: [[1.], [-1.], [0.], [1.]]\\n\\n                   nondeterministic               deterministic\\n    dloss/dlogits: [[2.0], [1.0], [0.0], [0.0]]   [[0.0], [0.0], [0.0], [0.0]]\\n\\n    Note that only the second two label vectors are valid probability\\n    distributions (as required by the API) and that the gradient matches for\\n    those cases.\\n\\n    TODO(duncanriach): Further investigate the source of the difference in\\n                       the gradients for this case.\\n    '\n    self._testSingleClass(expected_gradient=[[0.0], [0.0], [0.0], [0.0]])",
            "def testSingleClass(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Modify testing of gradient for single-class case.\\n\\n    The deterministic implementation does not produce the gradients expected by\\n    the original test (for the nondeterministic functionality) when the labels\\n    vector is not a valid probability distribution.\\n\\n    labels: [[-1.], [0.], [1.], [1.]]\\n    logits: [[1.], [-1.], [0.], [1.]]\\n\\n                   nondeterministic               deterministic\\n    dloss/dlogits: [[2.0], [1.0], [0.0], [0.0]]   [[0.0], [0.0], [0.0], [0.0]]\\n\\n    Note that only the second two label vectors are valid probability\\n    distributions (as required by the API) and that the gradient matches for\\n    those cases.\\n\\n    TODO(duncanriach): Further investigate the source of the difference in\\n                       the gradients for this case.\\n    '\n    self._testSingleClass(expected_gradient=[[0.0], [0.0], [0.0], [0.0]])"
        ]
    },
    {
        "func_name": "testLabelsBroadcast",
        "original": "def testLabelsBroadcast(self):\n    \"\"\"Modify testing of gradient for labels-broadcast case.\n\n    The deterministic implementation does not produce the gradients expected by\n    the original test (for the nondeterministic functionality) when the labels\n    vector (after broadcasting) is not a valid probability distribution.\n\n    labels: [[0.], [2.], [0.25]]\n    logits: [[1., 1., 1., 1.],\n             [1., 2., 3., 4.],\n             [1., 2., 3., 4.]]\n\n    dloss/dlogits (nondeterministic):\n        [[ 0.25 ,  0.25 ,  0.25 ,  0.25 ],\n         [-1.968, -1.913, -1.763, -1.355],\n         [-0.218, -0.163, -0.013,  0.394]]\n\n    dloss/dlogits (determinsitic):\n        [[ 0.   ,  0.   ,  0.   ,  0.   ],\n         [-1.743, -1.303, -0.105,  3.150],\n         [-0.218, -0.163, -0.013,  0.394]]\n\n    Note that neither of the first two broadcast label vectors is a valid\n    probability distribution (as required by the API) and that these are the\n    cases that yield different gradients for nondeterministic vs determinsitic\n    implementations.\n\n    TODO(duncanriach): Further investigate the source of the difference in\n                       the gradient for this case.\n    \"\"\"\n    self._testLabelsBroadcast(uniform_labels_gradient=[[0.0, 0.0, 0.0, 0.0], [-1.743, -1.303, -0.105, 3.15], [-0.218, -0.163, -0.013, 0.394]])",
        "mutated": [
            "def testLabelsBroadcast(self):\n    if False:\n        i = 10\n    'Modify testing of gradient for labels-broadcast case.\\n\\n    The deterministic implementation does not produce the gradients expected by\\n    the original test (for the nondeterministic functionality) when the labels\\n    vector (after broadcasting) is not a valid probability distribution.\\n\\n    labels: [[0.], [2.], [0.25]]\\n    logits: [[1., 1., 1., 1.],\\n             [1., 2., 3., 4.],\\n             [1., 2., 3., 4.]]\\n\\n    dloss/dlogits (nondeterministic):\\n        [[ 0.25 ,  0.25 ,  0.25 ,  0.25 ],\\n         [-1.968, -1.913, -1.763, -1.355],\\n         [-0.218, -0.163, -0.013,  0.394]]\\n\\n    dloss/dlogits (determinsitic):\\n        [[ 0.   ,  0.   ,  0.   ,  0.   ],\\n         [-1.743, -1.303, -0.105,  3.150],\\n         [-0.218, -0.163, -0.013,  0.394]]\\n\\n    Note that neither of the first two broadcast label vectors is a valid\\n    probability distribution (as required by the API) and that these are the\\n    cases that yield different gradients for nondeterministic vs determinsitic\\n    implementations.\\n\\n    TODO(duncanriach): Further investigate the source of the difference in\\n                       the gradient for this case.\\n    '\n    self._testLabelsBroadcast(uniform_labels_gradient=[[0.0, 0.0, 0.0, 0.0], [-1.743, -1.303, -0.105, 3.15], [-0.218, -0.163, -0.013, 0.394]])",
            "def testLabelsBroadcast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Modify testing of gradient for labels-broadcast case.\\n\\n    The deterministic implementation does not produce the gradients expected by\\n    the original test (for the nondeterministic functionality) when the labels\\n    vector (after broadcasting) is not a valid probability distribution.\\n\\n    labels: [[0.], [2.], [0.25]]\\n    logits: [[1., 1., 1., 1.],\\n             [1., 2., 3., 4.],\\n             [1., 2., 3., 4.]]\\n\\n    dloss/dlogits (nondeterministic):\\n        [[ 0.25 ,  0.25 ,  0.25 ,  0.25 ],\\n         [-1.968, -1.913, -1.763, -1.355],\\n         [-0.218, -0.163, -0.013,  0.394]]\\n\\n    dloss/dlogits (determinsitic):\\n        [[ 0.   ,  0.   ,  0.   ,  0.   ],\\n         [-1.743, -1.303, -0.105,  3.150],\\n         [-0.218, -0.163, -0.013,  0.394]]\\n\\n    Note that neither of the first two broadcast label vectors is a valid\\n    probability distribution (as required by the API) and that these are the\\n    cases that yield different gradients for nondeterministic vs determinsitic\\n    implementations.\\n\\n    TODO(duncanriach): Further investigate the source of the difference in\\n                       the gradient for this case.\\n    '\n    self._testLabelsBroadcast(uniform_labels_gradient=[[0.0, 0.0, 0.0, 0.0], [-1.743, -1.303, -0.105, 3.15], [-0.218, -0.163, -0.013, 0.394]])",
            "def testLabelsBroadcast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Modify testing of gradient for labels-broadcast case.\\n\\n    The deterministic implementation does not produce the gradients expected by\\n    the original test (for the nondeterministic functionality) when the labels\\n    vector (after broadcasting) is not a valid probability distribution.\\n\\n    labels: [[0.], [2.], [0.25]]\\n    logits: [[1., 1., 1., 1.],\\n             [1., 2., 3., 4.],\\n             [1., 2., 3., 4.]]\\n\\n    dloss/dlogits (nondeterministic):\\n        [[ 0.25 ,  0.25 ,  0.25 ,  0.25 ],\\n         [-1.968, -1.913, -1.763, -1.355],\\n         [-0.218, -0.163, -0.013,  0.394]]\\n\\n    dloss/dlogits (determinsitic):\\n        [[ 0.   ,  0.   ,  0.   ,  0.   ],\\n         [-1.743, -1.303, -0.105,  3.150],\\n         [-0.218, -0.163, -0.013,  0.394]]\\n\\n    Note that neither of the first two broadcast label vectors is a valid\\n    probability distribution (as required by the API) and that these are the\\n    cases that yield different gradients for nondeterministic vs determinsitic\\n    implementations.\\n\\n    TODO(duncanriach): Further investigate the source of the difference in\\n                       the gradient for this case.\\n    '\n    self._testLabelsBroadcast(uniform_labels_gradient=[[0.0, 0.0, 0.0, 0.0], [-1.743, -1.303, -0.105, 3.15], [-0.218, -0.163, -0.013, 0.394]])",
            "def testLabelsBroadcast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Modify testing of gradient for labels-broadcast case.\\n\\n    The deterministic implementation does not produce the gradients expected by\\n    the original test (for the nondeterministic functionality) when the labels\\n    vector (after broadcasting) is not a valid probability distribution.\\n\\n    labels: [[0.], [2.], [0.25]]\\n    logits: [[1., 1., 1., 1.],\\n             [1., 2., 3., 4.],\\n             [1., 2., 3., 4.]]\\n\\n    dloss/dlogits (nondeterministic):\\n        [[ 0.25 ,  0.25 ,  0.25 ,  0.25 ],\\n         [-1.968, -1.913, -1.763, -1.355],\\n         [-0.218, -0.163, -0.013,  0.394]]\\n\\n    dloss/dlogits (determinsitic):\\n        [[ 0.   ,  0.   ,  0.   ,  0.   ],\\n         [-1.743, -1.303, -0.105,  3.150],\\n         [-0.218, -0.163, -0.013,  0.394]]\\n\\n    Note that neither of the first two broadcast label vectors is a valid\\n    probability distribution (as required by the API) and that these are the\\n    cases that yield different gradients for nondeterministic vs determinsitic\\n    implementations.\\n\\n    TODO(duncanriach): Further investigate the source of the difference in\\n                       the gradient for this case.\\n    '\n    self._testLabelsBroadcast(uniform_labels_gradient=[[0.0, 0.0, 0.0, 0.0], [-1.743, -1.303, -0.105, 3.15], [-0.218, -0.163, -0.013, 0.394]])",
            "def testLabelsBroadcast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Modify testing of gradient for labels-broadcast case.\\n\\n    The deterministic implementation does not produce the gradients expected by\\n    the original test (for the nondeterministic functionality) when the labels\\n    vector (after broadcasting) is not a valid probability distribution.\\n\\n    labels: [[0.], [2.], [0.25]]\\n    logits: [[1., 1., 1., 1.],\\n             [1., 2., 3., 4.],\\n             [1., 2., 3., 4.]]\\n\\n    dloss/dlogits (nondeterministic):\\n        [[ 0.25 ,  0.25 ,  0.25 ,  0.25 ],\\n         [-1.968, -1.913, -1.763, -1.355],\\n         [-0.218, -0.163, -0.013,  0.394]]\\n\\n    dloss/dlogits (determinsitic):\\n        [[ 0.   ,  0.   ,  0.   ,  0.   ],\\n         [-1.743, -1.303, -0.105,  3.150],\\n         [-0.218, -0.163, -0.013,  0.394]]\\n\\n    Note that neither of the first two broadcast label vectors is a valid\\n    probability distribution (as required by the API) and that these are the\\n    cases that yield different gradients for nondeterministic vs determinsitic\\n    implementations.\\n\\n    TODO(duncanriach): Further investigate the source of the difference in\\n                       the gradient for this case.\\n    '\n    self._testLabelsBroadcast(uniform_labels_gradient=[[0.0, 0.0, 0.0, 0.0], [-1.743, -1.303, -0.105, 3.15], [-0.218, -0.163, -0.013, 0.394]])"
        ]
    }
]