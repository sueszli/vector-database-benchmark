[
    {
        "func_name": "_endwith",
        "original": "def _endwith(s: str, suffixes: List[str]):\n    return any((s.endswith(suffix) for suffix in suffixes))",
        "mutated": [
            "def _endwith(s: str, suffixes: List[str]):\n    if False:\n        i = 10\n    return any((s.endswith(suffix) for suffix in suffixes))",
            "def _endwith(s: str, suffixes: List[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return any((s.endswith(suffix) for suffix in suffixes))",
            "def _endwith(s: str, suffixes: List[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return any((s.endswith(suffix) for suffix in suffixes))",
            "def _endwith(s: str, suffixes: List[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return any((s.endswith(suffix) for suffix in suffixes))",
            "def _endwith(s: str, suffixes: List[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return any((s.endswith(suffix) for suffix in suffixes))"
        ]
    },
    {
        "func_name": "_prune_head_idxs",
        "original": "def _prune_head_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    head_mask = mask.reshape([num_heads, -1]).sum(-1) == 0.0\n    return torch.arange(len(head_mask))[head_mask].long().tolist()",
        "mutated": [
            "def _prune_head_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    if False:\n        i = 10\n    head_mask = mask.reshape([num_heads, -1]).sum(-1) == 0.0\n    return torch.arange(len(head_mask))[head_mask].long().tolist()",
            "def _prune_head_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    head_mask = mask.reshape([num_heads, -1]).sum(-1) == 0.0\n    return torch.arange(len(head_mask))[head_mask].long().tolist()",
            "def _prune_head_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    head_mask = mask.reshape([num_heads, -1]).sum(-1) == 0.0\n    return torch.arange(len(head_mask))[head_mask].long().tolist()",
            "def _prune_head_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    head_mask = mask.reshape([num_heads, -1]).sum(-1) == 0.0\n    return torch.arange(len(head_mask))[head_mask].long().tolist()",
            "def _prune_head_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    head_mask = mask.reshape([num_heads, -1]).sum(-1) == 0.0\n    return torch.arange(len(head_mask))[head_mask].long().tolist()"
        ]
    },
    {
        "func_name": "_remained_idxs",
        "original": "def _remained_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    repeats = mask.shape[0] // num_heads\n    remained = (mask.reshape([num_heads, -1]).sum(-1) != 0.0).repeat_interleave(repeats)\n    return torch.arange(len(mask))[remained].long().tolist()",
        "mutated": [
            "def _remained_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    if False:\n        i = 10\n    repeats = mask.shape[0] // num_heads\n    remained = (mask.reshape([num_heads, -1]).sum(-1) != 0.0).repeat_interleave(repeats)\n    return torch.arange(len(mask))[remained].long().tolist()",
            "def _remained_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    repeats = mask.shape[0] // num_heads\n    remained = (mask.reshape([num_heads, -1]).sum(-1) != 0.0).repeat_interleave(repeats)\n    return torch.arange(len(mask))[remained].long().tolist()",
            "def _remained_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    repeats = mask.shape[0] // num_heads\n    remained = (mask.reshape([num_heads, -1]).sum(-1) != 0.0).repeat_interleave(repeats)\n    return torch.arange(len(mask))[remained].long().tolist()",
            "def _remained_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    repeats = mask.shape[0] // num_heads\n    remained = (mask.reshape([num_heads, -1]).sum(-1) != 0.0).repeat_interleave(repeats)\n    return torch.arange(len(mask))[remained].long().tolist()",
            "def _remained_idxs(mask: torch.Tensor, num_heads: int) -> List[int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    repeats = mask.shape[0] // num_heads\n    remained = (mask.reshape([num_heads, -1]).sum(-1) != 0.0).repeat_interleave(repeats)\n    return torch.arange(len(mask))[remained].long().tolist()"
        ]
    },
    {
        "func_name": "_fill_one_on_dims",
        "original": "def _fill_one_on_dims(mask: torch.Tensor, dims: int | List[int]) -> torch.Tensor:\n    dims = dims if isinstance(dims, list) else [dims]\n    dims = [d if d >= 0 else d + len(mask.shape) for d in dims]\n    new_mask = torch.ones_like(mask)\n    for i in range(len(mask.shape)):\n        if i in dims:\n            continue\n        dim_mask = mask.sum([_ for _ in range(len(mask.shape)) if _ != i]) == 0.0\n        new_mask = new_mask.transpose(0, i)\n        new_mask[torch.arange(len(dim_mask), device=new_mask.device)[dim_mask].long().tolist()] = 0.0\n        new_mask = new_mask.transpose(0, i)\n    return new_mask",
        "mutated": [
            "def _fill_one_on_dims(mask: torch.Tensor, dims: int | List[int]) -> torch.Tensor:\n    if False:\n        i = 10\n    dims = dims if isinstance(dims, list) else [dims]\n    dims = [d if d >= 0 else d + len(mask.shape) for d in dims]\n    new_mask = torch.ones_like(mask)\n    for i in range(len(mask.shape)):\n        if i in dims:\n            continue\n        dim_mask = mask.sum([_ for _ in range(len(mask.shape)) if _ != i]) == 0.0\n        new_mask = new_mask.transpose(0, i)\n        new_mask[torch.arange(len(dim_mask), device=new_mask.device)[dim_mask].long().tolist()] = 0.0\n        new_mask = new_mask.transpose(0, i)\n    return new_mask",
            "def _fill_one_on_dims(mask: torch.Tensor, dims: int | List[int]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dims = dims if isinstance(dims, list) else [dims]\n    dims = [d if d >= 0 else d + len(mask.shape) for d in dims]\n    new_mask = torch.ones_like(mask)\n    for i in range(len(mask.shape)):\n        if i in dims:\n            continue\n        dim_mask = mask.sum([_ for _ in range(len(mask.shape)) if _ != i]) == 0.0\n        new_mask = new_mask.transpose(0, i)\n        new_mask[torch.arange(len(dim_mask), device=new_mask.device)[dim_mask].long().tolist()] = 0.0\n        new_mask = new_mask.transpose(0, i)\n    return new_mask",
            "def _fill_one_on_dims(mask: torch.Tensor, dims: int | List[int]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dims = dims if isinstance(dims, list) else [dims]\n    dims = [d if d >= 0 else d + len(mask.shape) for d in dims]\n    new_mask = torch.ones_like(mask)\n    for i in range(len(mask.shape)):\n        if i in dims:\n            continue\n        dim_mask = mask.sum([_ for _ in range(len(mask.shape)) if _ != i]) == 0.0\n        new_mask = new_mask.transpose(0, i)\n        new_mask[torch.arange(len(dim_mask), device=new_mask.device)[dim_mask].long().tolist()] = 0.0\n        new_mask = new_mask.transpose(0, i)\n    return new_mask",
            "def _fill_one_on_dims(mask: torch.Tensor, dims: int | List[int]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dims = dims if isinstance(dims, list) else [dims]\n    dims = [d if d >= 0 else d + len(mask.shape) for d in dims]\n    new_mask = torch.ones_like(mask)\n    for i in range(len(mask.shape)):\n        if i in dims:\n            continue\n        dim_mask = mask.sum([_ for _ in range(len(mask.shape)) if _ != i]) == 0.0\n        new_mask = new_mask.transpose(0, i)\n        new_mask[torch.arange(len(dim_mask), device=new_mask.device)[dim_mask].long().tolist()] = 0.0\n        new_mask = new_mask.transpose(0, i)\n    return new_mask",
            "def _fill_one_on_dims(mask: torch.Tensor, dims: int | List[int]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dims = dims if isinstance(dims, list) else [dims]\n    dims = [d if d >= 0 else d + len(mask.shape) for d in dims]\n    new_mask = torch.ones_like(mask)\n    for i in range(len(mask.shape)):\n        if i in dims:\n            continue\n        dim_mask = mask.sum([_ for _ in range(len(mask.shape)) if _ != i]) == 0.0\n        new_mask = new_mask.transpose(0, i)\n        new_mask[torch.arange(len(dim_mask), device=new_mask.device)[dim_mask].long().tolist()] = 0.0\n        new_mask = new_mask.transpose(0, i)\n    return new_mask"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, model: torch.nn.Module, parser: HuggingfaceModelParser | None=None):\n    self.parser = parser_factory(model) if parser is None else parser\n    if self.parser is None:\n        err_msg = f'Can not get the model parser of {type(model)}'\n        raise RuntimeError(err_msg)",
        "mutated": [
            "def __init__(self, model: torch.nn.Module, parser: HuggingfaceModelParser | None=None):\n    if False:\n        i = 10\n    self.parser = parser_factory(model) if parser is None else parser\n    if self.parser is None:\n        err_msg = f'Can not get the model parser of {type(model)}'\n        raise RuntimeError(err_msg)",
            "def __init__(self, model: torch.nn.Module, parser: HuggingfaceModelParser | None=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.parser = parser_factory(model) if parser is None else parser\n    if self.parser is None:\n        err_msg = f'Can not get the model parser of {type(model)}'\n        raise RuntimeError(err_msg)",
            "def __init__(self, model: torch.nn.Module, parser: HuggingfaceModelParser | None=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.parser = parser_factory(model) if parser is None else parser\n    if self.parser is None:\n        err_msg = f'Can not get the model parser of {type(model)}'\n        raise RuntimeError(err_msg)",
            "def __init__(self, model: torch.nn.Module, parser: HuggingfaceModelParser | None=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.parser = parser_factory(model) if parser is None else parser\n    if self.parser is None:\n        err_msg = f'Can not get the model parser of {type(model)}'\n        raise RuntimeError(err_msg)",
            "def __init__(self, model: torch.nn.Module, parser: HuggingfaceModelParser | None=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.parser = parser_factory(model) if parser is None else parser\n    if self.parser is None:\n        err_msg = f'Can not get the model parser of {type(model)}'\n        raise RuntimeError(err_msg)"
        ]
    },
    {
        "func_name": "replace_modules",
        "original": "def replace_modules(self, speedup: 'ModelSpeedup'):\n    attention_name_dict = defaultdict(list)\n    attention_patterns = [self.parser.TRANSFORMER_PREFIX + att_p for att_p in self.parser.ATTENTION]\n    target2node = {}\n    for (node, node_info) in speedup.node_infos.items():\n        if node.op == 'call_module' and self.parser.is_attention(node.target):\n            target2node[node.target] = node\n            for attention_pattern in attention_patterns:\n                attention_layer_name = re.findall(attention_pattern, node.target)[0]\n                attention_name_dict[attention_layer_name].append(node.target)\n    for (attention_layer_name, qkvo_names) in attention_name_dict.items():\n        qkvo_flatten_head_mask: torch.Tensor | None = None\n        for name in qkvo_names:\n            if _endwith(name, self.parser.QKVO):\n                info_msg = f'Find QKVO layer `{name}`, try to prune head.'\n                _logger.info(info_msg)\n                node = target2node[name]\n                node_info = speedup.node_infos[node]\n                if _endwith(name, self.parser.QKV):\n                    out_masks = node_info.output_masks\n                    flatten_head_mask = (torch.sum(out_masks, dim=[_ for _ in range(len(out_masks.shape) - 1)]).detach() > 0.0).float()\n                else:\n                    in_masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                    flatten_head_mask = (torch.sum(in_masks[0], dim=[_ for _ in range(len(in_masks[0].shape) - 1)]).detach() > 0.0).float()\n                if qkvo_flatten_head_mask is not None:\n                    qkvo_flatten_head_mask *= flatten_head_mask\n                else:\n                    qkvo_flatten_head_mask = flatten_head_mask\n        if qkvo_flatten_head_mask is not None:\n            original_num_heads = self.parser.get_num_heads(attention_layer_name, speedup.bound_model)\n            head_idxs = _prune_head_idxs(qkvo_flatten_head_mask, original_num_heads)\n            info_msg = f'Prune {attention_layer_name} head {head_idxs}'\n            _logger.info(info_msg)\n            attention_layer = get_nested_attr(speedup.bound_model, attention_layer_name)\n            attention_layer.prune_heads(head_idxs)\n            remained_idxs = _remained_idxs(qkvo_flatten_head_mask, original_num_heads)\n            for name in qkvo_names:\n                if _endwith(name, self.parser.QKVO):\n                    node = target2node[name]\n                    node_info = speedup.node_infos[node]\n                    if _endwith(name, self.parser.QKV):\n                        mask = node_info.param_masks['weight'][remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 0)\n                        mask = node_info.output_masks.transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        node_info.output_masks = _fill_one_on_dims(mask, -1)\n                    else:\n                        mask = node_info.param_masks['weight'][:, remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 1)\n                        masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                        mask = masks[0].transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        for n in node.args:\n                            speedup.node_infos[n].output_masks = _fill_one_on_dims(mask, -1)",
        "mutated": [
            "def replace_modules(self, speedup: 'ModelSpeedup'):\n    if False:\n        i = 10\n    attention_name_dict = defaultdict(list)\n    attention_patterns = [self.parser.TRANSFORMER_PREFIX + att_p for att_p in self.parser.ATTENTION]\n    target2node = {}\n    for (node, node_info) in speedup.node_infos.items():\n        if node.op == 'call_module' and self.parser.is_attention(node.target):\n            target2node[node.target] = node\n            for attention_pattern in attention_patterns:\n                attention_layer_name = re.findall(attention_pattern, node.target)[0]\n                attention_name_dict[attention_layer_name].append(node.target)\n    for (attention_layer_name, qkvo_names) in attention_name_dict.items():\n        qkvo_flatten_head_mask: torch.Tensor | None = None\n        for name in qkvo_names:\n            if _endwith(name, self.parser.QKVO):\n                info_msg = f'Find QKVO layer `{name}`, try to prune head.'\n                _logger.info(info_msg)\n                node = target2node[name]\n                node_info = speedup.node_infos[node]\n                if _endwith(name, self.parser.QKV):\n                    out_masks = node_info.output_masks\n                    flatten_head_mask = (torch.sum(out_masks, dim=[_ for _ in range(len(out_masks.shape) - 1)]).detach() > 0.0).float()\n                else:\n                    in_masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                    flatten_head_mask = (torch.sum(in_masks[0], dim=[_ for _ in range(len(in_masks[0].shape) - 1)]).detach() > 0.0).float()\n                if qkvo_flatten_head_mask is not None:\n                    qkvo_flatten_head_mask *= flatten_head_mask\n                else:\n                    qkvo_flatten_head_mask = flatten_head_mask\n        if qkvo_flatten_head_mask is not None:\n            original_num_heads = self.parser.get_num_heads(attention_layer_name, speedup.bound_model)\n            head_idxs = _prune_head_idxs(qkvo_flatten_head_mask, original_num_heads)\n            info_msg = f'Prune {attention_layer_name} head {head_idxs}'\n            _logger.info(info_msg)\n            attention_layer = get_nested_attr(speedup.bound_model, attention_layer_name)\n            attention_layer.prune_heads(head_idxs)\n            remained_idxs = _remained_idxs(qkvo_flatten_head_mask, original_num_heads)\n            for name in qkvo_names:\n                if _endwith(name, self.parser.QKVO):\n                    node = target2node[name]\n                    node_info = speedup.node_infos[node]\n                    if _endwith(name, self.parser.QKV):\n                        mask = node_info.param_masks['weight'][remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 0)\n                        mask = node_info.output_masks.transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        node_info.output_masks = _fill_one_on_dims(mask, -1)\n                    else:\n                        mask = node_info.param_masks['weight'][:, remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 1)\n                        masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                        mask = masks[0].transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        for n in node.args:\n                            speedup.node_infos[n].output_masks = _fill_one_on_dims(mask, -1)",
            "def replace_modules(self, speedup: 'ModelSpeedup'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    attention_name_dict = defaultdict(list)\n    attention_patterns = [self.parser.TRANSFORMER_PREFIX + att_p for att_p in self.parser.ATTENTION]\n    target2node = {}\n    for (node, node_info) in speedup.node_infos.items():\n        if node.op == 'call_module' and self.parser.is_attention(node.target):\n            target2node[node.target] = node\n            for attention_pattern in attention_patterns:\n                attention_layer_name = re.findall(attention_pattern, node.target)[0]\n                attention_name_dict[attention_layer_name].append(node.target)\n    for (attention_layer_name, qkvo_names) in attention_name_dict.items():\n        qkvo_flatten_head_mask: torch.Tensor | None = None\n        for name in qkvo_names:\n            if _endwith(name, self.parser.QKVO):\n                info_msg = f'Find QKVO layer `{name}`, try to prune head.'\n                _logger.info(info_msg)\n                node = target2node[name]\n                node_info = speedup.node_infos[node]\n                if _endwith(name, self.parser.QKV):\n                    out_masks = node_info.output_masks\n                    flatten_head_mask = (torch.sum(out_masks, dim=[_ for _ in range(len(out_masks.shape) - 1)]).detach() > 0.0).float()\n                else:\n                    in_masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                    flatten_head_mask = (torch.sum(in_masks[0], dim=[_ for _ in range(len(in_masks[0].shape) - 1)]).detach() > 0.0).float()\n                if qkvo_flatten_head_mask is not None:\n                    qkvo_flatten_head_mask *= flatten_head_mask\n                else:\n                    qkvo_flatten_head_mask = flatten_head_mask\n        if qkvo_flatten_head_mask is not None:\n            original_num_heads = self.parser.get_num_heads(attention_layer_name, speedup.bound_model)\n            head_idxs = _prune_head_idxs(qkvo_flatten_head_mask, original_num_heads)\n            info_msg = f'Prune {attention_layer_name} head {head_idxs}'\n            _logger.info(info_msg)\n            attention_layer = get_nested_attr(speedup.bound_model, attention_layer_name)\n            attention_layer.prune_heads(head_idxs)\n            remained_idxs = _remained_idxs(qkvo_flatten_head_mask, original_num_heads)\n            for name in qkvo_names:\n                if _endwith(name, self.parser.QKVO):\n                    node = target2node[name]\n                    node_info = speedup.node_infos[node]\n                    if _endwith(name, self.parser.QKV):\n                        mask = node_info.param_masks['weight'][remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 0)\n                        mask = node_info.output_masks.transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        node_info.output_masks = _fill_one_on_dims(mask, -1)\n                    else:\n                        mask = node_info.param_masks['weight'][:, remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 1)\n                        masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                        mask = masks[0].transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        for n in node.args:\n                            speedup.node_infos[n].output_masks = _fill_one_on_dims(mask, -1)",
            "def replace_modules(self, speedup: 'ModelSpeedup'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    attention_name_dict = defaultdict(list)\n    attention_patterns = [self.parser.TRANSFORMER_PREFIX + att_p for att_p in self.parser.ATTENTION]\n    target2node = {}\n    for (node, node_info) in speedup.node_infos.items():\n        if node.op == 'call_module' and self.parser.is_attention(node.target):\n            target2node[node.target] = node\n            for attention_pattern in attention_patterns:\n                attention_layer_name = re.findall(attention_pattern, node.target)[0]\n                attention_name_dict[attention_layer_name].append(node.target)\n    for (attention_layer_name, qkvo_names) in attention_name_dict.items():\n        qkvo_flatten_head_mask: torch.Tensor | None = None\n        for name in qkvo_names:\n            if _endwith(name, self.parser.QKVO):\n                info_msg = f'Find QKVO layer `{name}`, try to prune head.'\n                _logger.info(info_msg)\n                node = target2node[name]\n                node_info = speedup.node_infos[node]\n                if _endwith(name, self.parser.QKV):\n                    out_masks = node_info.output_masks\n                    flatten_head_mask = (torch.sum(out_masks, dim=[_ for _ in range(len(out_masks.shape) - 1)]).detach() > 0.0).float()\n                else:\n                    in_masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                    flatten_head_mask = (torch.sum(in_masks[0], dim=[_ for _ in range(len(in_masks[0].shape) - 1)]).detach() > 0.0).float()\n                if qkvo_flatten_head_mask is not None:\n                    qkvo_flatten_head_mask *= flatten_head_mask\n                else:\n                    qkvo_flatten_head_mask = flatten_head_mask\n        if qkvo_flatten_head_mask is not None:\n            original_num_heads = self.parser.get_num_heads(attention_layer_name, speedup.bound_model)\n            head_idxs = _prune_head_idxs(qkvo_flatten_head_mask, original_num_heads)\n            info_msg = f'Prune {attention_layer_name} head {head_idxs}'\n            _logger.info(info_msg)\n            attention_layer = get_nested_attr(speedup.bound_model, attention_layer_name)\n            attention_layer.prune_heads(head_idxs)\n            remained_idxs = _remained_idxs(qkvo_flatten_head_mask, original_num_heads)\n            for name in qkvo_names:\n                if _endwith(name, self.parser.QKVO):\n                    node = target2node[name]\n                    node_info = speedup.node_infos[node]\n                    if _endwith(name, self.parser.QKV):\n                        mask = node_info.param_masks['weight'][remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 0)\n                        mask = node_info.output_masks.transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        node_info.output_masks = _fill_one_on_dims(mask, -1)\n                    else:\n                        mask = node_info.param_masks['weight'][:, remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 1)\n                        masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                        mask = masks[0].transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        for n in node.args:\n                            speedup.node_infos[n].output_masks = _fill_one_on_dims(mask, -1)",
            "def replace_modules(self, speedup: 'ModelSpeedup'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    attention_name_dict = defaultdict(list)\n    attention_patterns = [self.parser.TRANSFORMER_PREFIX + att_p for att_p in self.parser.ATTENTION]\n    target2node = {}\n    for (node, node_info) in speedup.node_infos.items():\n        if node.op == 'call_module' and self.parser.is_attention(node.target):\n            target2node[node.target] = node\n            for attention_pattern in attention_patterns:\n                attention_layer_name = re.findall(attention_pattern, node.target)[0]\n                attention_name_dict[attention_layer_name].append(node.target)\n    for (attention_layer_name, qkvo_names) in attention_name_dict.items():\n        qkvo_flatten_head_mask: torch.Tensor | None = None\n        for name in qkvo_names:\n            if _endwith(name, self.parser.QKVO):\n                info_msg = f'Find QKVO layer `{name}`, try to prune head.'\n                _logger.info(info_msg)\n                node = target2node[name]\n                node_info = speedup.node_infos[node]\n                if _endwith(name, self.parser.QKV):\n                    out_masks = node_info.output_masks\n                    flatten_head_mask = (torch.sum(out_masks, dim=[_ for _ in range(len(out_masks.shape) - 1)]).detach() > 0.0).float()\n                else:\n                    in_masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                    flatten_head_mask = (torch.sum(in_masks[0], dim=[_ for _ in range(len(in_masks[0].shape) - 1)]).detach() > 0.0).float()\n                if qkvo_flatten_head_mask is not None:\n                    qkvo_flatten_head_mask *= flatten_head_mask\n                else:\n                    qkvo_flatten_head_mask = flatten_head_mask\n        if qkvo_flatten_head_mask is not None:\n            original_num_heads = self.parser.get_num_heads(attention_layer_name, speedup.bound_model)\n            head_idxs = _prune_head_idxs(qkvo_flatten_head_mask, original_num_heads)\n            info_msg = f'Prune {attention_layer_name} head {head_idxs}'\n            _logger.info(info_msg)\n            attention_layer = get_nested_attr(speedup.bound_model, attention_layer_name)\n            attention_layer.prune_heads(head_idxs)\n            remained_idxs = _remained_idxs(qkvo_flatten_head_mask, original_num_heads)\n            for name in qkvo_names:\n                if _endwith(name, self.parser.QKVO):\n                    node = target2node[name]\n                    node_info = speedup.node_infos[node]\n                    if _endwith(name, self.parser.QKV):\n                        mask = node_info.param_masks['weight'][remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 0)\n                        mask = node_info.output_masks.transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        node_info.output_masks = _fill_one_on_dims(mask, -1)\n                    else:\n                        mask = node_info.param_masks['weight'][:, remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 1)\n                        masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                        mask = masks[0].transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        for n in node.args:\n                            speedup.node_infos[n].output_masks = _fill_one_on_dims(mask, -1)",
            "def replace_modules(self, speedup: 'ModelSpeedup'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    attention_name_dict = defaultdict(list)\n    attention_patterns = [self.parser.TRANSFORMER_PREFIX + att_p for att_p in self.parser.ATTENTION]\n    target2node = {}\n    for (node, node_info) in speedup.node_infos.items():\n        if node.op == 'call_module' and self.parser.is_attention(node.target):\n            target2node[node.target] = node\n            for attention_pattern in attention_patterns:\n                attention_layer_name = re.findall(attention_pattern, node.target)[0]\n                attention_name_dict[attention_layer_name].append(node.target)\n    for (attention_layer_name, qkvo_names) in attention_name_dict.items():\n        qkvo_flatten_head_mask: torch.Tensor | None = None\n        for name in qkvo_names:\n            if _endwith(name, self.parser.QKVO):\n                info_msg = f'Find QKVO layer `{name}`, try to prune head.'\n                _logger.info(info_msg)\n                node = target2node[name]\n                node_info = speedup.node_infos[node]\n                if _endwith(name, self.parser.QKV):\n                    out_masks = node_info.output_masks\n                    flatten_head_mask = (torch.sum(out_masks, dim=[_ for _ in range(len(out_masks.shape) - 1)]).detach() > 0.0).float()\n                else:\n                    in_masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                    flatten_head_mask = (torch.sum(in_masks[0], dim=[_ for _ in range(len(in_masks[0].shape) - 1)]).detach() > 0.0).float()\n                if qkvo_flatten_head_mask is not None:\n                    qkvo_flatten_head_mask *= flatten_head_mask\n                else:\n                    qkvo_flatten_head_mask = flatten_head_mask\n        if qkvo_flatten_head_mask is not None:\n            original_num_heads = self.parser.get_num_heads(attention_layer_name, speedup.bound_model)\n            head_idxs = _prune_head_idxs(qkvo_flatten_head_mask, original_num_heads)\n            info_msg = f'Prune {attention_layer_name} head {head_idxs}'\n            _logger.info(info_msg)\n            attention_layer = get_nested_attr(speedup.bound_model, attention_layer_name)\n            attention_layer.prune_heads(head_idxs)\n            remained_idxs = _remained_idxs(qkvo_flatten_head_mask, original_num_heads)\n            for name in qkvo_names:\n                if _endwith(name, self.parser.QKVO):\n                    node = target2node[name]\n                    node_info = speedup.node_infos[node]\n                    if _endwith(name, self.parser.QKV):\n                        mask = node_info.param_masks['weight'][remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 0)\n                        mask = node_info.output_masks.transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        node_info.output_masks = _fill_one_on_dims(mask, -1)\n                    else:\n                        mask = node_info.param_masks['weight'][:, remained_idxs]\n                        node_info.param_masks['weight'] = _fill_one_on_dims(mask, 1)\n                        masks = tree_map(lambda n: speedup.node_infos[n].output_masks, node.args)\n                        mask = masks[0].transpose(0, -1)[remained_idxs].transpose(0, -1)\n                        for n in node.args:\n                            speedup.node_infos[n].output_masks = _fill_one_on_dims(mask, -1)"
        ]
    }
]