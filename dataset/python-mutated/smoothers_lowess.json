[
    {
        "func_name": "lowess",
        "original": "def lowess(endog, exog, frac=2.0 / 3.0, it=3, delta=0.0, xvals=None, is_sorted=False, missing='drop', return_sorted=True):\n    \"\"\"LOWESS (Locally Weighted Scatterplot Smoothing)\n\n    A lowess function that outs smoothed estimates of endog\n    at the given exog values from points (exog, endog)\n\n    Parameters\n    ----------\n    endog : 1-D numpy array\n        The y-values of the observed points\n    exog : 1-D numpy array\n        The x-values of the observed points\n    frac : float\n        Between 0 and 1. The fraction of the data used\n        when estimating each y-value.\n    it : int\n        The number of residual-based reweightings\n        to perform.\n    delta : float\n        Distance within which to use linear-interpolation\n        instead of weighted regression.\n    xvals: 1-D numpy array\n        Values of the exogenous variable at which to evaluate the regression.\n        If supplied, cannot use delta.\n    is_sorted : bool\n        If False (default), then the data will be sorted by exog before\n        calculating lowess. If True, then it is assumed that the data is\n        already sorted by exog. If xvals is specified, then it too must be\n        sorted if is_sorted is True.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'drop'.\n    return_sorted : bool\n        If True (default), then the returned array is sorted by exog and has\n        missing (nan or infinite) observations removed.\n        If False, then the returned array is in the same length and the same\n        sequence of observations as the input array.\n\n    Returns\n    -------\n    out : {ndarray, float}\n        The returned array is two-dimensional if return_sorted is True, and\n        one dimensional if return_sorted is False.\n        If return_sorted is True, then a numpy array with two columns. The\n        first column contains the sorted x (exog) values and the second column\n        the associated estimated y (endog) values.\n        If return_sorted is False, then only the fitted values are returned,\n        and the observations will be in the same order as the input arrays.\n        If xvals is provided, then return_sorted is ignored and the returned\n        array is always one dimensional, containing the y values fitted at\n        the x values provided by xvals.\n\n    Notes\n    -----\n    This lowess function implements the algorithm given in the\n    reference below using local linear estimates.\n\n    Suppose the input data has N points. The algorithm works by\n    estimating the `smooth` y_i by taking the frac*N closest points\n    to (x_i,y_i) based on their x values and estimating y_i\n    using a weighted linear regression. The weight for (x_j,y_j)\n    is tricube function applied to abs(x_i-x_j).\n\n    If it > 1, then further weighted local linear regressions\n    are performed, where the weights are the same as above\n    times the _lowess_bisquare function of the residuals. Each iteration\n    takes approximately the same amount of time as the original fit,\n    so these iterations are expensive. They are most useful when\n    the noise has extremely heavy tails, such as Cauchy noise.\n    Noise with less heavy-tails, such as t-distributions with df>2,\n    are less problematic. The weights downgrade the influence of\n    points with large residuals. In the extreme case, points whose\n    residuals are larger than 6 times the median absolute residual\n    are given weight 0.\n\n    `delta` can be used to save computations. For each `x_i`, regressions\n    are skipped for points closer than `delta`. The next regression is\n    fit for the farthest point within delta of `x_i` and all points in\n    between are estimated by linearly interpolating between the two\n    regression fits.\n\n    Judicious choice of delta can cut computation time considerably\n    for large data (N > 5000). A good choice is ``delta = 0.01 * range(exog)``.\n\n    If `xvals` is provided, the regression is then computed at those points\n    and the fit values are returned. Otherwise, the regression is run\n    at points of `exog`.\n\n    Some experimentation is likely required to find a good\n    choice of `frac` and `iter` for a particular dataset.\n\n    References\n    ----------\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\n    and Smoothing Scatterplots\". Journal of the American Statistical\n    Association 74 (368): 829-836.\n\n    Examples\n    --------\n    The below allows a comparison between how different the fits from\n    lowess for different values of frac can be.\n\n    >>> import numpy as np\n    >>> import statsmodels.api as sm\n    >>> lowess = sm.nonparametric.lowess\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\n    >>> z = lowess(y, x)\n    >>> w = lowess(y, x, frac=1./3)\n\n    This gives a similar comparison for when it is 0 vs not.\n\n    >>> import numpy as np\n    >>> import scipy.stats as stats\n    >>> import statsmodels.api as sm\n    >>> lowess = sm.nonparametric.lowess\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\n    >>> z = lowess(y, x, frac= 1./3, it=0)\n    >>> w = lowess(y, x, frac=1./3)\n\n    \"\"\"\n    endog = np.asarray(endog, float)\n    exog = np.asarray(exog, float)\n    given_xvals = xvals is not None\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != exog.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    if xvals is not None:\n        xvals = np.ascontiguousarray(xvals)\n        if xvals.ndim != 1:\n            raise ValueError('exog_predict must be a vector')\n    if missing in ['drop', 'raise']:\n        mask_valid = np.isfinite(exog) & np.isfinite(endog)\n        all_valid = np.all(mask_valid)\n        if all_valid:\n            y = endog\n            x = exog\n        elif missing == 'drop':\n            x = exog[mask_valid]\n            y = endog[mask_valid]\n        else:\n            raise ValueError('nan or inf found in data')\n    elif missing == 'none':\n        y = endog\n        x = exog\n        all_valid = True\n    else:\n        raise ValueError(\"missing can only be 'none', 'drop' or 'raise'\")\n    if not is_sorted:\n        sort_index = np.argsort(x)\n        x = np.array(x[sort_index])\n        y = np.array(y[sort_index])\n    if not given_xvals:\n        xvals = exog\n        xvalues = x\n        xvals_all_valid = all_valid\n        if missing == 'drop':\n            xvals_mask_valid = mask_valid\n    else:\n        if delta != 0.0:\n            raise ValueError(\"Cannot have non-zero 'delta' and 'xvals' values\")\n        mask_valid = np.isfinite(xvals)\n        if missing == 'raise':\n            raise ValueError(\"NaN values in xvals with missing='raise'\")\n        elif missing == 'drop':\n            xvals_mask_valid = mask_valid\n        xvalues = xvals\n        xvals_all_valid = True if missing == 'none' else np.all(mask_valid)\n        return_sorted = False\n        if missing in ['drop', 'raise']:\n            xvals_mask_valid = np.isfinite(xvals)\n            xvals_all_valid = np.all(xvals_mask_valid)\n            if xvals_all_valid:\n                xvalues = xvals\n            elif missing == 'drop':\n                xvalues = xvals[xvals_mask_valid]\n            else:\n                raise ValueError('nan or inf found in xvals')\n        if not is_sorted:\n            sort_index = np.argsort(xvalues)\n            xvalues = np.array(xvalues[sort_index])\n        else:\n            xvals_all_valid = True\n    y = np.ascontiguousarray(y)\n    x = np.ascontiguousarray(x)\n    if not given_xvals:\n        (res, _) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it, delta=delta, given_xvals=False)\n    else:\n        if it > 0:\n            (_, weights) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it - 1, delta=delta, given_xvals=False)\n        else:\n            weights = np.ones_like(x)\n        xvalues = np.ascontiguousarray(xvalues, dtype=float)\n        (res, _) = _lowess(y, x, xvalues, weights, frac=frac, it=0, delta=delta, given_xvals=True)\n    (_, yfitted) = res.T\n    if return_sorted:\n        return res\n    else:\n        if not is_sorted:\n            yfitted_ = np.empty_like(xvalues)\n            yfitted_.fill(np.nan)\n            yfitted_[sort_index] = yfitted\n            yfitted = yfitted_\n        else:\n            yfitted = yfitted\n        if not xvals_all_valid:\n            yfitted_ = np.empty_like(xvals)\n            yfitted_.fill(np.nan)\n            yfitted_[xvals_mask_valid] = yfitted\n            yfitted = yfitted_\n        return yfitted",
        "mutated": [
            "def lowess(endog, exog, frac=2.0 / 3.0, it=3, delta=0.0, xvals=None, is_sorted=False, missing='drop', return_sorted=True):\n    if False:\n        i = 10\n    'LOWESS (Locally Weighted Scatterplot Smoothing)\\n\\n    A lowess function that outs smoothed estimates of endog\\n    at the given exog values from points (exog, endog)\\n\\n    Parameters\\n    ----------\\n    endog : 1-D numpy array\\n        The y-values of the observed points\\n    exog : 1-D numpy array\\n        The x-values of the observed points\\n    frac : float\\n        Between 0 and 1. The fraction of the data used\\n        when estimating each y-value.\\n    it : int\\n        The number of residual-based reweightings\\n        to perform.\\n    delta : float\\n        Distance within which to use linear-interpolation\\n        instead of weighted regression.\\n    xvals: 1-D numpy array\\n        Values of the exogenous variable at which to evaluate the regression.\\n        If supplied, cannot use delta.\\n    is_sorted : bool\\n        If False (default), then the data will be sorted by exog before\\n        calculating lowess. If True, then it is assumed that the data is\\n        already sorted by exog. If xvals is specified, then it too must be\\n        sorted if is_sorted is True.\\n    missing : str\\n        Available options are \\'none\\', \\'drop\\', and \\'raise\\'. If \\'none\\', no nan\\n        checking is done. If \\'drop\\', any observations with nans are dropped.\\n        If \\'raise\\', an error is raised. Default is \\'drop\\'.\\n    return_sorted : bool\\n        If True (default), then the returned array is sorted by exog and has\\n        missing (nan or infinite) observations removed.\\n        If False, then the returned array is in the same length and the same\\n        sequence of observations as the input array.\\n\\n    Returns\\n    -------\\n    out : {ndarray, float}\\n        The returned array is two-dimensional if return_sorted is True, and\\n        one dimensional if return_sorted is False.\\n        If return_sorted is True, then a numpy array with two columns. The\\n        first column contains the sorted x (exog) values and the second column\\n        the associated estimated y (endog) values.\\n        If return_sorted is False, then only the fitted values are returned,\\n        and the observations will be in the same order as the input arrays.\\n        If xvals is provided, then return_sorted is ignored and the returned\\n        array is always one dimensional, containing the y values fitted at\\n        the x values provided by xvals.\\n\\n    Notes\\n    -----\\n    This lowess function implements the algorithm given in the\\n    reference below using local linear estimates.\\n\\n    Suppose the input data has N points. The algorithm works by\\n    estimating the `smooth` y_i by taking the frac*N closest points\\n    to (x_i,y_i) based on their x values and estimating y_i\\n    using a weighted linear regression. The weight for (x_j,y_j)\\n    is tricube function applied to abs(x_i-x_j).\\n\\n    If it > 1, then further weighted local linear regressions\\n    are performed, where the weights are the same as above\\n    times the _lowess_bisquare function of the residuals. Each iteration\\n    takes approximately the same amount of time as the original fit,\\n    so these iterations are expensive. They are most useful when\\n    the noise has extremely heavy tails, such as Cauchy noise.\\n    Noise with less heavy-tails, such as t-distributions with df>2,\\n    are less problematic. The weights downgrade the influence of\\n    points with large residuals. In the extreme case, points whose\\n    residuals are larger than 6 times the median absolute residual\\n    are given weight 0.\\n\\n    `delta` can be used to save computations. For each `x_i`, regressions\\n    are skipped for points closer than `delta`. The next regression is\\n    fit for the farthest point within delta of `x_i` and all points in\\n    between are estimated by linearly interpolating between the two\\n    regression fits.\\n\\n    Judicious choice of delta can cut computation time considerably\\n    for large data (N > 5000). A good choice is ``delta = 0.01 * range(exog)``.\\n\\n    If `xvals` is provided, the regression is then computed at those points\\n    and the fit values are returned. Otherwise, the regression is run\\n    at points of `exog`.\\n\\n    Some experimentation is likely required to find a good\\n    choice of `frac` and `iter` for a particular dataset.\\n\\n    References\\n    ----------\\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\\n    and Smoothing Scatterplots\". Journal of the American Statistical\\n    Association 74 (368): 829-836.\\n\\n    Examples\\n    --------\\n    The below allows a comparison between how different the fits from\\n    lowess for different values of frac can be.\\n\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\\n    >>> z = lowess(y, x)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    This gives a similar comparison for when it is 0 vs not.\\n\\n    >>> import numpy as np\\n    >>> import scipy.stats as stats\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\\n    >>> z = lowess(y, x, frac= 1./3, it=0)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    '\n    endog = np.asarray(endog, float)\n    exog = np.asarray(exog, float)\n    given_xvals = xvals is not None\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != exog.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    if xvals is not None:\n        xvals = np.ascontiguousarray(xvals)\n        if xvals.ndim != 1:\n            raise ValueError('exog_predict must be a vector')\n    if missing in ['drop', 'raise']:\n        mask_valid = np.isfinite(exog) & np.isfinite(endog)\n        all_valid = np.all(mask_valid)\n        if all_valid:\n            y = endog\n            x = exog\n        elif missing == 'drop':\n            x = exog[mask_valid]\n            y = endog[mask_valid]\n        else:\n            raise ValueError('nan or inf found in data')\n    elif missing == 'none':\n        y = endog\n        x = exog\n        all_valid = True\n    else:\n        raise ValueError(\"missing can only be 'none', 'drop' or 'raise'\")\n    if not is_sorted:\n        sort_index = np.argsort(x)\n        x = np.array(x[sort_index])\n        y = np.array(y[sort_index])\n    if not given_xvals:\n        xvals = exog\n        xvalues = x\n        xvals_all_valid = all_valid\n        if missing == 'drop':\n            xvals_mask_valid = mask_valid\n    else:\n        if delta != 0.0:\n            raise ValueError(\"Cannot have non-zero 'delta' and 'xvals' values\")\n        mask_valid = np.isfinite(xvals)\n        if missing == 'raise':\n            raise ValueError(\"NaN values in xvals with missing='raise'\")\n        elif missing == 'drop':\n            xvals_mask_valid = mask_valid\n        xvalues = xvals\n        xvals_all_valid = True if missing == 'none' else np.all(mask_valid)\n        return_sorted = False\n        if missing in ['drop', 'raise']:\n            xvals_mask_valid = np.isfinite(xvals)\n            xvals_all_valid = np.all(xvals_mask_valid)\n            if xvals_all_valid:\n                xvalues = xvals\n            elif missing == 'drop':\n                xvalues = xvals[xvals_mask_valid]\n            else:\n                raise ValueError('nan or inf found in xvals')\n        if not is_sorted:\n            sort_index = np.argsort(xvalues)\n            xvalues = np.array(xvalues[sort_index])\n        else:\n            xvals_all_valid = True\n    y = np.ascontiguousarray(y)\n    x = np.ascontiguousarray(x)\n    if not given_xvals:\n        (res, _) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it, delta=delta, given_xvals=False)\n    else:\n        if it > 0:\n            (_, weights) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it - 1, delta=delta, given_xvals=False)\n        else:\n            weights = np.ones_like(x)\n        xvalues = np.ascontiguousarray(xvalues, dtype=float)\n        (res, _) = _lowess(y, x, xvalues, weights, frac=frac, it=0, delta=delta, given_xvals=True)\n    (_, yfitted) = res.T\n    if return_sorted:\n        return res\n    else:\n        if not is_sorted:\n            yfitted_ = np.empty_like(xvalues)\n            yfitted_.fill(np.nan)\n            yfitted_[sort_index] = yfitted\n            yfitted = yfitted_\n        else:\n            yfitted = yfitted\n        if not xvals_all_valid:\n            yfitted_ = np.empty_like(xvals)\n            yfitted_.fill(np.nan)\n            yfitted_[xvals_mask_valid] = yfitted\n            yfitted = yfitted_\n        return yfitted",
            "def lowess(endog, exog, frac=2.0 / 3.0, it=3, delta=0.0, xvals=None, is_sorted=False, missing='drop', return_sorted=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'LOWESS (Locally Weighted Scatterplot Smoothing)\\n\\n    A lowess function that outs smoothed estimates of endog\\n    at the given exog values from points (exog, endog)\\n\\n    Parameters\\n    ----------\\n    endog : 1-D numpy array\\n        The y-values of the observed points\\n    exog : 1-D numpy array\\n        The x-values of the observed points\\n    frac : float\\n        Between 0 and 1. The fraction of the data used\\n        when estimating each y-value.\\n    it : int\\n        The number of residual-based reweightings\\n        to perform.\\n    delta : float\\n        Distance within which to use linear-interpolation\\n        instead of weighted regression.\\n    xvals: 1-D numpy array\\n        Values of the exogenous variable at which to evaluate the regression.\\n        If supplied, cannot use delta.\\n    is_sorted : bool\\n        If False (default), then the data will be sorted by exog before\\n        calculating lowess. If True, then it is assumed that the data is\\n        already sorted by exog. If xvals is specified, then it too must be\\n        sorted if is_sorted is True.\\n    missing : str\\n        Available options are \\'none\\', \\'drop\\', and \\'raise\\'. If \\'none\\', no nan\\n        checking is done. If \\'drop\\', any observations with nans are dropped.\\n        If \\'raise\\', an error is raised. Default is \\'drop\\'.\\n    return_sorted : bool\\n        If True (default), then the returned array is sorted by exog and has\\n        missing (nan or infinite) observations removed.\\n        If False, then the returned array is in the same length and the same\\n        sequence of observations as the input array.\\n\\n    Returns\\n    -------\\n    out : {ndarray, float}\\n        The returned array is two-dimensional if return_sorted is True, and\\n        one dimensional if return_sorted is False.\\n        If return_sorted is True, then a numpy array with two columns. The\\n        first column contains the sorted x (exog) values and the second column\\n        the associated estimated y (endog) values.\\n        If return_sorted is False, then only the fitted values are returned,\\n        and the observations will be in the same order as the input arrays.\\n        If xvals is provided, then return_sorted is ignored and the returned\\n        array is always one dimensional, containing the y values fitted at\\n        the x values provided by xvals.\\n\\n    Notes\\n    -----\\n    This lowess function implements the algorithm given in the\\n    reference below using local linear estimates.\\n\\n    Suppose the input data has N points. The algorithm works by\\n    estimating the `smooth` y_i by taking the frac*N closest points\\n    to (x_i,y_i) based on their x values and estimating y_i\\n    using a weighted linear regression. The weight for (x_j,y_j)\\n    is tricube function applied to abs(x_i-x_j).\\n\\n    If it > 1, then further weighted local linear regressions\\n    are performed, where the weights are the same as above\\n    times the _lowess_bisquare function of the residuals. Each iteration\\n    takes approximately the same amount of time as the original fit,\\n    so these iterations are expensive. They are most useful when\\n    the noise has extremely heavy tails, such as Cauchy noise.\\n    Noise with less heavy-tails, such as t-distributions with df>2,\\n    are less problematic. The weights downgrade the influence of\\n    points with large residuals. In the extreme case, points whose\\n    residuals are larger than 6 times the median absolute residual\\n    are given weight 0.\\n\\n    `delta` can be used to save computations. For each `x_i`, regressions\\n    are skipped for points closer than `delta`. The next regression is\\n    fit for the farthest point within delta of `x_i` and all points in\\n    between are estimated by linearly interpolating between the two\\n    regression fits.\\n\\n    Judicious choice of delta can cut computation time considerably\\n    for large data (N > 5000). A good choice is ``delta = 0.01 * range(exog)``.\\n\\n    If `xvals` is provided, the regression is then computed at those points\\n    and the fit values are returned. Otherwise, the regression is run\\n    at points of `exog`.\\n\\n    Some experimentation is likely required to find a good\\n    choice of `frac` and `iter` for a particular dataset.\\n\\n    References\\n    ----------\\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\\n    and Smoothing Scatterplots\". Journal of the American Statistical\\n    Association 74 (368): 829-836.\\n\\n    Examples\\n    --------\\n    The below allows a comparison between how different the fits from\\n    lowess for different values of frac can be.\\n\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\\n    >>> z = lowess(y, x)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    This gives a similar comparison for when it is 0 vs not.\\n\\n    >>> import numpy as np\\n    >>> import scipy.stats as stats\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\\n    >>> z = lowess(y, x, frac= 1./3, it=0)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    '\n    endog = np.asarray(endog, float)\n    exog = np.asarray(exog, float)\n    given_xvals = xvals is not None\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != exog.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    if xvals is not None:\n        xvals = np.ascontiguousarray(xvals)\n        if xvals.ndim != 1:\n            raise ValueError('exog_predict must be a vector')\n    if missing in ['drop', 'raise']:\n        mask_valid = np.isfinite(exog) & np.isfinite(endog)\n        all_valid = np.all(mask_valid)\n        if all_valid:\n            y = endog\n            x = exog\n        elif missing == 'drop':\n            x = exog[mask_valid]\n            y = endog[mask_valid]\n        else:\n            raise ValueError('nan or inf found in data')\n    elif missing == 'none':\n        y = endog\n        x = exog\n        all_valid = True\n    else:\n        raise ValueError(\"missing can only be 'none', 'drop' or 'raise'\")\n    if not is_sorted:\n        sort_index = np.argsort(x)\n        x = np.array(x[sort_index])\n        y = np.array(y[sort_index])\n    if not given_xvals:\n        xvals = exog\n        xvalues = x\n        xvals_all_valid = all_valid\n        if missing == 'drop':\n            xvals_mask_valid = mask_valid\n    else:\n        if delta != 0.0:\n            raise ValueError(\"Cannot have non-zero 'delta' and 'xvals' values\")\n        mask_valid = np.isfinite(xvals)\n        if missing == 'raise':\n            raise ValueError(\"NaN values in xvals with missing='raise'\")\n        elif missing == 'drop':\n            xvals_mask_valid = mask_valid\n        xvalues = xvals\n        xvals_all_valid = True if missing == 'none' else np.all(mask_valid)\n        return_sorted = False\n        if missing in ['drop', 'raise']:\n            xvals_mask_valid = np.isfinite(xvals)\n            xvals_all_valid = np.all(xvals_mask_valid)\n            if xvals_all_valid:\n                xvalues = xvals\n            elif missing == 'drop':\n                xvalues = xvals[xvals_mask_valid]\n            else:\n                raise ValueError('nan or inf found in xvals')\n        if not is_sorted:\n            sort_index = np.argsort(xvalues)\n            xvalues = np.array(xvalues[sort_index])\n        else:\n            xvals_all_valid = True\n    y = np.ascontiguousarray(y)\n    x = np.ascontiguousarray(x)\n    if not given_xvals:\n        (res, _) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it, delta=delta, given_xvals=False)\n    else:\n        if it > 0:\n            (_, weights) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it - 1, delta=delta, given_xvals=False)\n        else:\n            weights = np.ones_like(x)\n        xvalues = np.ascontiguousarray(xvalues, dtype=float)\n        (res, _) = _lowess(y, x, xvalues, weights, frac=frac, it=0, delta=delta, given_xvals=True)\n    (_, yfitted) = res.T\n    if return_sorted:\n        return res\n    else:\n        if not is_sorted:\n            yfitted_ = np.empty_like(xvalues)\n            yfitted_.fill(np.nan)\n            yfitted_[sort_index] = yfitted\n            yfitted = yfitted_\n        else:\n            yfitted = yfitted\n        if not xvals_all_valid:\n            yfitted_ = np.empty_like(xvals)\n            yfitted_.fill(np.nan)\n            yfitted_[xvals_mask_valid] = yfitted\n            yfitted = yfitted_\n        return yfitted",
            "def lowess(endog, exog, frac=2.0 / 3.0, it=3, delta=0.0, xvals=None, is_sorted=False, missing='drop', return_sorted=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'LOWESS (Locally Weighted Scatterplot Smoothing)\\n\\n    A lowess function that outs smoothed estimates of endog\\n    at the given exog values from points (exog, endog)\\n\\n    Parameters\\n    ----------\\n    endog : 1-D numpy array\\n        The y-values of the observed points\\n    exog : 1-D numpy array\\n        The x-values of the observed points\\n    frac : float\\n        Between 0 and 1. The fraction of the data used\\n        when estimating each y-value.\\n    it : int\\n        The number of residual-based reweightings\\n        to perform.\\n    delta : float\\n        Distance within which to use linear-interpolation\\n        instead of weighted regression.\\n    xvals: 1-D numpy array\\n        Values of the exogenous variable at which to evaluate the regression.\\n        If supplied, cannot use delta.\\n    is_sorted : bool\\n        If False (default), then the data will be sorted by exog before\\n        calculating lowess. If True, then it is assumed that the data is\\n        already sorted by exog. If xvals is specified, then it too must be\\n        sorted if is_sorted is True.\\n    missing : str\\n        Available options are \\'none\\', \\'drop\\', and \\'raise\\'. If \\'none\\', no nan\\n        checking is done. If \\'drop\\', any observations with nans are dropped.\\n        If \\'raise\\', an error is raised. Default is \\'drop\\'.\\n    return_sorted : bool\\n        If True (default), then the returned array is sorted by exog and has\\n        missing (nan or infinite) observations removed.\\n        If False, then the returned array is in the same length and the same\\n        sequence of observations as the input array.\\n\\n    Returns\\n    -------\\n    out : {ndarray, float}\\n        The returned array is two-dimensional if return_sorted is True, and\\n        one dimensional if return_sorted is False.\\n        If return_sorted is True, then a numpy array with two columns. The\\n        first column contains the sorted x (exog) values and the second column\\n        the associated estimated y (endog) values.\\n        If return_sorted is False, then only the fitted values are returned,\\n        and the observations will be in the same order as the input arrays.\\n        If xvals is provided, then return_sorted is ignored and the returned\\n        array is always one dimensional, containing the y values fitted at\\n        the x values provided by xvals.\\n\\n    Notes\\n    -----\\n    This lowess function implements the algorithm given in the\\n    reference below using local linear estimates.\\n\\n    Suppose the input data has N points. The algorithm works by\\n    estimating the `smooth` y_i by taking the frac*N closest points\\n    to (x_i,y_i) based on their x values and estimating y_i\\n    using a weighted linear regression. The weight for (x_j,y_j)\\n    is tricube function applied to abs(x_i-x_j).\\n\\n    If it > 1, then further weighted local linear regressions\\n    are performed, where the weights are the same as above\\n    times the _lowess_bisquare function of the residuals. Each iteration\\n    takes approximately the same amount of time as the original fit,\\n    so these iterations are expensive. They are most useful when\\n    the noise has extremely heavy tails, such as Cauchy noise.\\n    Noise with less heavy-tails, such as t-distributions with df>2,\\n    are less problematic. The weights downgrade the influence of\\n    points with large residuals. In the extreme case, points whose\\n    residuals are larger than 6 times the median absolute residual\\n    are given weight 0.\\n\\n    `delta` can be used to save computations. For each `x_i`, regressions\\n    are skipped for points closer than `delta`. The next regression is\\n    fit for the farthest point within delta of `x_i` and all points in\\n    between are estimated by linearly interpolating between the two\\n    regression fits.\\n\\n    Judicious choice of delta can cut computation time considerably\\n    for large data (N > 5000). A good choice is ``delta = 0.01 * range(exog)``.\\n\\n    If `xvals` is provided, the regression is then computed at those points\\n    and the fit values are returned. Otherwise, the regression is run\\n    at points of `exog`.\\n\\n    Some experimentation is likely required to find a good\\n    choice of `frac` and `iter` for a particular dataset.\\n\\n    References\\n    ----------\\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\\n    and Smoothing Scatterplots\". Journal of the American Statistical\\n    Association 74 (368): 829-836.\\n\\n    Examples\\n    --------\\n    The below allows a comparison between how different the fits from\\n    lowess for different values of frac can be.\\n\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\\n    >>> z = lowess(y, x)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    This gives a similar comparison for when it is 0 vs not.\\n\\n    >>> import numpy as np\\n    >>> import scipy.stats as stats\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\\n    >>> z = lowess(y, x, frac= 1./3, it=0)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    '\n    endog = np.asarray(endog, float)\n    exog = np.asarray(exog, float)\n    given_xvals = xvals is not None\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != exog.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    if xvals is not None:\n        xvals = np.ascontiguousarray(xvals)\n        if xvals.ndim != 1:\n            raise ValueError('exog_predict must be a vector')\n    if missing in ['drop', 'raise']:\n        mask_valid = np.isfinite(exog) & np.isfinite(endog)\n        all_valid = np.all(mask_valid)\n        if all_valid:\n            y = endog\n            x = exog\n        elif missing == 'drop':\n            x = exog[mask_valid]\n            y = endog[mask_valid]\n        else:\n            raise ValueError('nan or inf found in data')\n    elif missing == 'none':\n        y = endog\n        x = exog\n        all_valid = True\n    else:\n        raise ValueError(\"missing can only be 'none', 'drop' or 'raise'\")\n    if not is_sorted:\n        sort_index = np.argsort(x)\n        x = np.array(x[sort_index])\n        y = np.array(y[sort_index])\n    if not given_xvals:\n        xvals = exog\n        xvalues = x\n        xvals_all_valid = all_valid\n        if missing == 'drop':\n            xvals_mask_valid = mask_valid\n    else:\n        if delta != 0.0:\n            raise ValueError(\"Cannot have non-zero 'delta' and 'xvals' values\")\n        mask_valid = np.isfinite(xvals)\n        if missing == 'raise':\n            raise ValueError(\"NaN values in xvals with missing='raise'\")\n        elif missing == 'drop':\n            xvals_mask_valid = mask_valid\n        xvalues = xvals\n        xvals_all_valid = True if missing == 'none' else np.all(mask_valid)\n        return_sorted = False\n        if missing in ['drop', 'raise']:\n            xvals_mask_valid = np.isfinite(xvals)\n            xvals_all_valid = np.all(xvals_mask_valid)\n            if xvals_all_valid:\n                xvalues = xvals\n            elif missing == 'drop':\n                xvalues = xvals[xvals_mask_valid]\n            else:\n                raise ValueError('nan or inf found in xvals')\n        if not is_sorted:\n            sort_index = np.argsort(xvalues)\n            xvalues = np.array(xvalues[sort_index])\n        else:\n            xvals_all_valid = True\n    y = np.ascontiguousarray(y)\n    x = np.ascontiguousarray(x)\n    if not given_xvals:\n        (res, _) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it, delta=delta, given_xvals=False)\n    else:\n        if it > 0:\n            (_, weights) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it - 1, delta=delta, given_xvals=False)\n        else:\n            weights = np.ones_like(x)\n        xvalues = np.ascontiguousarray(xvalues, dtype=float)\n        (res, _) = _lowess(y, x, xvalues, weights, frac=frac, it=0, delta=delta, given_xvals=True)\n    (_, yfitted) = res.T\n    if return_sorted:\n        return res\n    else:\n        if not is_sorted:\n            yfitted_ = np.empty_like(xvalues)\n            yfitted_.fill(np.nan)\n            yfitted_[sort_index] = yfitted\n            yfitted = yfitted_\n        else:\n            yfitted = yfitted\n        if not xvals_all_valid:\n            yfitted_ = np.empty_like(xvals)\n            yfitted_.fill(np.nan)\n            yfitted_[xvals_mask_valid] = yfitted\n            yfitted = yfitted_\n        return yfitted",
            "def lowess(endog, exog, frac=2.0 / 3.0, it=3, delta=0.0, xvals=None, is_sorted=False, missing='drop', return_sorted=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'LOWESS (Locally Weighted Scatterplot Smoothing)\\n\\n    A lowess function that outs smoothed estimates of endog\\n    at the given exog values from points (exog, endog)\\n\\n    Parameters\\n    ----------\\n    endog : 1-D numpy array\\n        The y-values of the observed points\\n    exog : 1-D numpy array\\n        The x-values of the observed points\\n    frac : float\\n        Between 0 and 1. The fraction of the data used\\n        when estimating each y-value.\\n    it : int\\n        The number of residual-based reweightings\\n        to perform.\\n    delta : float\\n        Distance within which to use linear-interpolation\\n        instead of weighted regression.\\n    xvals: 1-D numpy array\\n        Values of the exogenous variable at which to evaluate the regression.\\n        If supplied, cannot use delta.\\n    is_sorted : bool\\n        If False (default), then the data will be sorted by exog before\\n        calculating lowess. If True, then it is assumed that the data is\\n        already sorted by exog. If xvals is specified, then it too must be\\n        sorted if is_sorted is True.\\n    missing : str\\n        Available options are \\'none\\', \\'drop\\', and \\'raise\\'. If \\'none\\', no nan\\n        checking is done. If \\'drop\\', any observations with nans are dropped.\\n        If \\'raise\\', an error is raised. Default is \\'drop\\'.\\n    return_sorted : bool\\n        If True (default), then the returned array is sorted by exog and has\\n        missing (nan or infinite) observations removed.\\n        If False, then the returned array is in the same length and the same\\n        sequence of observations as the input array.\\n\\n    Returns\\n    -------\\n    out : {ndarray, float}\\n        The returned array is two-dimensional if return_sorted is True, and\\n        one dimensional if return_sorted is False.\\n        If return_sorted is True, then a numpy array with two columns. The\\n        first column contains the sorted x (exog) values and the second column\\n        the associated estimated y (endog) values.\\n        If return_sorted is False, then only the fitted values are returned,\\n        and the observations will be in the same order as the input arrays.\\n        If xvals is provided, then return_sorted is ignored and the returned\\n        array is always one dimensional, containing the y values fitted at\\n        the x values provided by xvals.\\n\\n    Notes\\n    -----\\n    This lowess function implements the algorithm given in the\\n    reference below using local linear estimates.\\n\\n    Suppose the input data has N points. The algorithm works by\\n    estimating the `smooth` y_i by taking the frac*N closest points\\n    to (x_i,y_i) based on their x values and estimating y_i\\n    using a weighted linear regression. The weight for (x_j,y_j)\\n    is tricube function applied to abs(x_i-x_j).\\n\\n    If it > 1, then further weighted local linear regressions\\n    are performed, where the weights are the same as above\\n    times the _lowess_bisquare function of the residuals. Each iteration\\n    takes approximately the same amount of time as the original fit,\\n    so these iterations are expensive. They are most useful when\\n    the noise has extremely heavy tails, such as Cauchy noise.\\n    Noise with less heavy-tails, such as t-distributions with df>2,\\n    are less problematic. The weights downgrade the influence of\\n    points with large residuals. In the extreme case, points whose\\n    residuals are larger than 6 times the median absolute residual\\n    are given weight 0.\\n\\n    `delta` can be used to save computations. For each `x_i`, regressions\\n    are skipped for points closer than `delta`. The next regression is\\n    fit for the farthest point within delta of `x_i` and all points in\\n    between are estimated by linearly interpolating between the two\\n    regression fits.\\n\\n    Judicious choice of delta can cut computation time considerably\\n    for large data (N > 5000). A good choice is ``delta = 0.01 * range(exog)``.\\n\\n    If `xvals` is provided, the regression is then computed at those points\\n    and the fit values are returned. Otherwise, the regression is run\\n    at points of `exog`.\\n\\n    Some experimentation is likely required to find a good\\n    choice of `frac` and `iter` for a particular dataset.\\n\\n    References\\n    ----------\\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\\n    and Smoothing Scatterplots\". Journal of the American Statistical\\n    Association 74 (368): 829-836.\\n\\n    Examples\\n    --------\\n    The below allows a comparison between how different the fits from\\n    lowess for different values of frac can be.\\n\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\\n    >>> z = lowess(y, x)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    This gives a similar comparison for when it is 0 vs not.\\n\\n    >>> import numpy as np\\n    >>> import scipy.stats as stats\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\\n    >>> z = lowess(y, x, frac= 1./3, it=0)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    '\n    endog = np.asarray(endog, float)\n    exog = np.asarray(exog, float)\n    given_xvals = xvals is not None\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != exog.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    if xvals is not None:\n        xvals = np.ascontiguousarray(xvals)\n        if xvals.ndim != 1:\n            raise ValueError('exog_predict must be a vector')\n    if missing in ['drop', 'raise']:\n        mask_valid = np.isfinite(exog) & np.isfinite(endog)\n        all_valid = np.all(mask_valid)\n        if all_valid:\n            y = endog\n            x = exog\n        elif missing == 'drop':\n            x = exog[mask_valid]\n            y = endog[mask_valid]\n        else:\n            raise ValueError('nan or inf found in data')\n    elif missing == 'none':\n        y = endog\n        x = exog\n        all_valid = True\n    else:\n        raise ValueError(\"missing can only be 'none', 'drop' or 'raise'\")\n    if not is_sorted:\n        sort_index = np.argsort(x)\n        x = np.array(x[sort_index])\n        y = np.array(y[sort_index])\n    if not given_xvals:\n        xvals = exog\n        xvalues = x\n        xvals_all_valid = all_valid\n        if missing == 'drop':\n            xvals_mask_valid = mask_valid\n    else:\n        if delta != 0.0:\n            raise ValueError(\"Cannot have non-zero 'delta' and 'xvals' values\")\n        mask_valid = np.isfinite(xvals)\n        if missing == 'raise':\n            raise ValueError(\"NaN values in xvals with missing='raise'\")\n        elif missing == 'drop':\n            xvals_mask_valid = mask_valid\n        xvalues = xvals\n        xvals_all_valid = True if missing == 'none' else np.all(mask_valid)\n        return_sorted = False\n        if missing in ['drop', 'raise']:\n            xvals_mask_valid = np.isfinite(xvals)\n            xvals_all_valid = np.all(xvals_mask_valid)\n            if xvals_all_valid:\n                xvalues = xvals\n            elif missing == 'drop':\n                xvalues = xvals[xvals_mask_valid]\n            else:\n                raise ValueError('nan or inf found in xvals')\n        if not is_sorted:\n            sort_index = np.argsort(xvalues)\n            xvalues = np.array(xvalues[sort_index])\n        else:\n            xvals_all_valid = True\n    y = np.ascontiguousarray(y)\n    x = np.ascontiguousarray(x)\n    if not given_xvals:\n        (res, _) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it, delta=delta, given_xvals=False)\n    else:\n        if it > 0:\n            (_, weights) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it - 1, delta=delta, given_xvals=False)\n        else:\n            weights = np.ones_like(x)\n        xvalues = np.ascontiguousarray(xvalues, dtype=float)\n        (res, _) = _lowess(y, x, xvalues, weights, frac=frac, it=0, delta=delta, given_xvals=True)\n    (_, yfitted) = res.T\n    if return_sorted:\n        return res\n    else:\n        if not is_sorted:\n            yfitted_ = np.empty_like(xvalues)\n            yfitted_.fill(np.nan)\n            yfitted_[sort_index] = yfitted\n            yfitted = yfitted_\n        else:\n            yfitted = yfitted\n        if not xvals_all_valid:\n            yfitted_ = np.empty_like(xvals)\n            yfitted_.fill(np.nan)\n            yfitted_[xvals_mask_valid] = yfitted\n            yfitted = yfitted_\n        return yfitted",
            "def lowess(endog, exog, frac=2.0 / 3.0, it=3, delta=0.0, xvals=None, is_sorted=False, missing='drop', return_sorted=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'LOWESS (Locally Weighted Scatterplot Smoothing)\\n\\n    A lowess function that outs smoothed estimates of endog\\n    at the given exog values from points (exog, endog)\\n\\n    Parameters\\n    ----------\\n    endog : 1-D numpy array\\n        The y-values of the observed points\\n    exog : 1-D numpy array\\n        The x-values of the observed points\\n    frac : float\\n        Between 0 and 1. The fraction of the data used\\n        when estimating each y-value.\\n    it : int\\n        The number of residual-based reweightings\\n        to perform.\\n    delta : float\\n        Distance within which to use linear-interpolation\\n        instead of weighted regression.\\n    xvals: 1-D numpy array\\n        Values of the exogenous variable at which to evaluate the regression.\\n        If supplied, cannot use delta.\\n    is_sorted : bool\\n        If False (default), then the data will be sorted by exog before\\n        calculating lowess. If True, then it is assumed that the data is\\n        already sorted by exog. If xvals is specified, then it too must be\\n        sorted if is_sorted is True.\\n    missing : str\\n        Available options are \\'none\\', \\'drop\\', and \\'raise\\'. If \\'none\\', no nan\\n        checking is done. If \\'drop\\', any observations with nans are dropped.\\n        If \\'raise\\', an error is raised. Default is \\'drop\\'.\\n    return_sorted : bool\\n        If True (default), then the returned array is sorted by exog and has\\n        missing (nan or infinite) observations removed.\\n        If False, then the returned array is in the same length and the same\\n        sequence of observations as the input array.\\n\\n    Returns\\n    -------\\n    out : {ndarray, float}\\n        The returned array is two-dimensional if return_sorted is True, and\\n        one dimensional if return_sorted is False.\\n        If return_sorted is True, then a numpy array with two columns. The\\n        first column contains the sorted x (exog) values and the second column\\n        the associated estimated y (endog) values.\\n        If return_sorted is False, then only the fitted values are returned,\\n        and the observations will be in the same order as the input arrays.\\n        If xvals is provided, then return_sorted is ignored and the returned\\n        array is always one dimensional, containing the y values fitted at\\n        the x values provided by xvals.\\n\\n    Notes\\n    -----\\n    This lowess function implements the algorithm given in the\\n    reference below using local linear estimates.\\n\\n    Suppose the input data has N points. The algorithm works by\\n    estimating the `smooth` y_i by taking the frac*N closest points\\n    to (x_i,y_i) based on their x values and estimating y_i\\n    using a weighted linear regression. The weight for (x_j,y_j)\\n    is tricube function applied to abs(x_i-x_j).\\n\\n    If it > 1, then further weighted local linear regressions\\n    are performed, where the weights are the same as above\\n    times the _lowess_bisquare function of the residuals. Each iteration\\n    takes approximately the same amount of time as the original fit,\\n    so these iterations are expensive. They are most useful when\\n    the noise has extremely heavy tails, such as Cauchy noise.\\n    Noise with less heavy-tails, such as t-distributions with df>2,\\n    are less problematic. The weights downgrade the influence of\\n    points with large residuals. In the extreme case, points whose\\n    residuals are larger than 6 times the median absolute residual\\n    are given weight 0.\\n\\n    `delta` can be used to save computations. For each `x_i`, regressions\\n    are skipped for points closer than `delta`. The next regression is\\n    fit for the farthest point within delta of `x_i` and all points in\\n    between are estimated by linearly interpolating between the two\\n    regression fits.\\n\\n    Judicious choice of delta can cut computation time considerably\\n    for large data (N > 5000). A good choice is ``delta = 0.01 * range(exog)``.\\n\\n    If `xvals` is provided, the regression is then computed at those points\\n    and the fit values are returned. Otherwise, the regression is run\\n    at points of `exog`.\\n\\n    Some experimentation is likely required to find a good\\n    choice of `frac` and `iter` for a particular dataset.\\n\\n    References\\n    ----------\\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\\n    and Smoothing Scatterplots\". Journal of the American Statistical\\n    Association 74 (368): 829-836.\\n\\n    Examples\\n    --------\\n    The below allows a comparison between how different the fits from\\n    lowess for different values of frac can be.\\n\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\\n    >>> z = lowess(y, x)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    This gives a similar comparison for when it is 0 vs not.\\n\\n    >>> import numpy as np\\n    >>> import scipy.stats as stats\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\\n    >>> z = lowess(y, x, frac= 1./3, it=0)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    '\n    endog = np.asarray(endog, float)\n    exog = np.asarray(exog, float)\n    given_xvals = xvals is not None\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != exog.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    if xvals is not None:\n        xvals = np.ascontiguousarray(xvals)\n        if xvals.ndim != 1:\n            raise ValueError('exog_predict must be a vector')\n    if missing in ['drop', 'raise']:\n        mask_valid = np.isfinite(exog) & np.isfinite(endog)\n        all_valid = np.all(mask_valid)\n        if all_valid:\n            y = endog\n            x = exog\n        elif missing == 'drop':\n            x = exog[mask_valid]\n            y = endog[mask_valid]\n        else:\n            raise ValueError('nan or inf found in data')\n    elif missing == 'none':\n        y = endog\n        x = exog\n        all_valid = True\n    else:\n        raise ValueError(\"missing can only be 'none', 'drop' or 'raise'\")\n    if not is_sorted:\n        sort_index = np.argsort(x)\n        x = np.array(x[sort_index])\n        y = np.array(y[sort_index])\n    if not given_xvals:\n        xvals = exog\n        xvalues = x\n        xvals_all_valid = all_valid\n        if missing == 'drop':\n            xvals_mask_valid = mask_valid\n    else:\n        if delta != 0.0:\n            raise ValueError(\"Cannot have non-zero 'delta' and 'xvals' values\")\n        mask_valid = np.isfinite(xvals)\n        if missing == 'raise':\n            raise ValueError(\"NaN values in xvals with missing='raise'\")\n        elif missing == 'drop':\n            xvals_mask_valid = mask_valid\n        xvalues = xvals\n        xvals_all_valid = True if missing == 'none' else np.all(mask_valid)\n        return_sorted = False\n        if missing in ['drop', 'raise']:\n            xvals_mask_valid = np.isfinite(xvals)\n            xvals_all_valid = np.all(xvals_mask_valid)\n            if xvals_all_valid:\n                xvalues = xvals\n            elif missing == 'drop':\n                xvalues = xvals[xvals_mask_valid]\n            else:\n                raise ValueError('nan or inf found in xvals')\n        if not is_sorted:\n            sort_index = np.argsort(xvalues)\n            xvalues = np.array(xvalues[sort_index])\n        else:\n            xvals_all_valid = True\n    y = np.ascontiguousarray(y)\n    x = np.ascontiguousarray(x)\n    if not given_xvals:\n        (res, _) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it, delta=delta, given_xvals=False)\n    else:\n        if it > 0:\n            (_, weights) = _lowess(y, x, x, np.ones_like(x), frac=frac, it=it - 1, delta=delta, given_xvals=False)\n        else:\n            weights = np.ones_like(x)\n        xvalues = np.ascontiguousarray(xvalues, dtype=float)\n        (res, _) = _lowess(y, x, xvalues, weights, frac=frac, it=0, delta=delta, given_xvals=True)\n    (_, yfitted) = res.T\n    if return_sorted:\n        return res\n    else:\n        if not is_sorted:\n            yfitted_ = np.empty_like(xvalues)\n            yfitted_.fill(np.nan)\n            yfitted_[sort_index] = yfitted\n            yfitted = yfitted_\n        else:\n            yfitted = yfitted\n        if not xvals_all_valid:\n            yfitted_ = np.empty_like(xvals)\n            yfitted_.fill(np.nan)\n            yfitted_[xvals_mask_valid] = yfitted\n            yfitted = yfitted_\n        return yfitted"
        ]
    }
]