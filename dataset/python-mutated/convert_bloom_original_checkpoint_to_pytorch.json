[
    {
        "func_name": "layer_name_mapping",
        "original": "def layer_name_mapping(key, file):\n    \"\"\"Convert Megatron-DeepSpeed TP/PP weights mapping in transformers PP only\"\"\"\n    layer_rename_map = {'word_embeddings.weight': 'word_embeddings.weight', 'word_embeddings.norm.weight': 'word_embeddings_layernorm.weight', 'word_embeddings.norm.bias': 'word_embeddings_layernorm.bias', 'weight': 'ln_f.weight', 'bias': 'ln_f.bias'}\n    if key in layer_rename_map:\n        return layer_rename_map[key]\n    layer_number = int(re.match('.*layer_(\\\\d*).*', file)[1])\n    layer_number -= 3\n    return f'h.{layer_number}.' + key",
        "mutated": [
            "def layer_name_mapping(key, file):\n    if False:\n        i = 10\n    'Convert Megatron-DeepSpeed TP/PP weights mapping in transformers PP only'\n    layer_rename_map = {'word_embeddings.weight': 'word_embeddings.weight', 'word_embeddings.norm.weight': 'word_embeddings_layernorm.weight', 'word_embeddings.norm.bias': 'word_embeddings_layernorm.bias', 'weight': 'ln_f.weight', 'bias': 'ln_f.bias'}\n    if key in layer_rename_map:\n        return layer_rename_map[key]\n    layer_number = int(re.match('.*layer_(\\\\d*).*', file)[1])\n    layer_number -= 3\n    return f'h.{layer_number}.' + key",
            "def layer_name_mapping(key, file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convert Megatron-DeepSpeed TP/PP weights mapping in transformers PP only'\n    layer_rename_map = {'word_embeddings.weight': 'word_embeddings.weight', 'word_embeddings.norm.weight': 'word_embeddings_layernorm.weight', 'word_embeddings.norm.bias': 'word_embeddings_layernorm.bias', 'weight': 'ln_f.weight', 'bias': 'ln_f.bias'}\n    if key in layer_rename_map:\n        return layer_rename_map[key]\n    layer_number = int(re.match('.*layer_(\\\\d*).*', file)[1])\n    layer_number -= 3\n    return f'h.{layer_number}.' + key",
            "def layer_name_mapping(key, file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convert Megatron-DeepSpeed TP/PP weights mapping in transformers PP only'\n    layer_rename_map = {'word_embeddings.weight': 'word_embeddings.weight', 'word_embeddings.norm.weight': 'word_embeddings_layernorm.weight', 'word_embeddings.norm.bias': 'word_embeddings_layernorm.bias', 'weight': 'ln_f.weight', 'bias': 'ln_f.bias'}\n    if key in layer_rename_map:\n        return layer_rename_map[key]\n    layer_number = int(re.match('.*layer_(\\\\d*).*', file)[1])\n    layer_number -= 3\n    return f'h.{layer_number}.' + key",
            "def layer_name_mapping(key, file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convert Megatron-DeepSpeed TP/PP weights mapping in transformers PP only'\n    layer_rename_map = {'word_embeddings.weight': 'word_embeddings.weight', 'word_embeddings.norm.weight': 'word_embeddings_layernorm.weight', 'word_embeddings.norm.bias': 'word_embeddings_layernorm.bias', 'weight': 'ln_f.weight', 'bias': 'ln_f.bias'}\n    if key in layer_rename_map:\n        return layer_rename_map[key]\n    layer_number = int(re.match('.*layer_(\\\\d*).*', file)[1])\n    layer_number -= 3\n    return f'h.{layer_number}.' + key",
            "def layer_name_mapping(key, file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convert Megatron-DeepSpeed TP/PP weights mapping in transformers PP only'\n    layer_rename_map = {'word_embeddings.weight': 'word_embeddings.weight', 'word_embeddings.norm.weight': 'word_embeddings_layernorm.weight', 'word_embeddings.norm.bias': 'word_embeddings_layernorm.bias', 'weight': 'ln_f.weight', 'bias': 'ln_f.bias'}\n    if key in layer_rename_map:\n        return layer_rename_map[key]\n    layer_number = int(re.match('.*layer_(\\\\d*).*', file)[1])\n    layer_number -= 3\n    return f'h.{layer_number}.' + key"
        ]
    },
    {
        "func_name": "get_dtype_size",
        "original": "def get_dtype_size(dtype):\n    if dtype == torch.bool:\n        return 1 / 8\n    bit_search = re.search('[^\\\\d](\\\\d+)$', str(dtype))\n    if bit_search is None:\n        raise ValueError(f'`dtype` is not a valid dtype: {dtype}.')\n    bit_size = int(bit_search.groups()[0])\n    return bit_size // 8",
        "mutated": [
            "def get_dtype_size(dtype):\n    if False:\n        i = 10\n    if dtype == torch.bool:\n        return 1 / 8\n    bit_search = re.search('[^\\\\d](\\\\d+)$', str(dtype))\n    if bit_search is None:\n        raise ValueError(f'`dtype` is not a valid dtype: {dtype}.')\n    bit_size = int(bit_search.groups()[0])\n    return bit_size // 8",
            "def get_dtype_size(dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if dtype == torch.bool:\n        return 1 / 8\n    bit_search = re.search('[^\\\\d](\\\\d+)$', str(dtype))\n    if bit_search is None:\n        raise ValueError(f'`dtype` is not a valid dtype: {dtype}.')\n    bit_size = int(bit_search.groups()[0])\n    return bit_size // 8",
            "def get_dtype_size(dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if dtype == torch.bool:\n        return 1 / 8\n    bit_search = re.search('[^\\\\d](\\\\d+)$', str(dtype))\n    if bit_search is None:\n        raise ValueError(f'`dtype` is not a valid dtype: {dtype}.')\n    bit_size = int(bit_search.groups()[0])\n    return bit_size // 8",
            "def get_dtype_size(dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if dtype == torch.bool:\n        return 1 / 8\n    bit_search = re.search('[^\\\\d](\\\\d+)$', str(dtype))\n    if bit_search is None:\n        raise ValueError(f'`dtype` is not a valid dtype: {dtype}.')\n    bit_size = int(bit_search.groups()[0])\n    return bit_size // 8",
            "def get_dtype_size(dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if dtype == torch.bool:\n        return 1 / 8\n    bit_search = re.search('[^\\\\d](\\\\d+)$', str(dtype))\n    if bit_search is None:\n        raise ValueError(f'`dtype` is not a valid dtype: {dtype}.')\n    bit_size = int(bit_search.groups()[0])\n    return bit_size // 8"
        ]
    },
    {
        "func_name": "convert_bloom_checkpoint_to_pytorch",
        "original": "def convert_bloom_checkpoint_to_pytorch(bloom_checkpoint_path, bloom_config_file, pytorch_dump_folder_path, shard_model, pretraining_tp):\n    if bloom_config_file == '':\n        config = BloomConfig()\n    else:\n        config = BloomConfig.from_json_file(bloom_config_file)\n    if shard_model:\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        index_dict = {'weight_map': {}, 'metadata': {}}\n        total_size = 0\n        missing_keys = None\n        config = BloomConfig()\n        for (j, file) in enumerate(file_names):\n            print('Processing file: {}'.format(file))\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            torch.save(tensors, os.path.join(pytorch_dump_folder_path, 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))))\n            for key in tensors.keys():\n                value = tensors[key]\n                total_size += value.numel() * get_dtype_size(value.dtype)\n                if key not in index_dict['weight_map']:\n                    index_dict['weight_map'][key] = 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))\n        config = BloomConfig()\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        index_dict['metadata']['total_size'] = total_size\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())\n        with open(os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME + '.index.json'), 'w', encoding='utf-8') as f:\n            json_config = json.dumps(index_dict, indent=2, sort_keys=True) + '\\n'\n            f.write(json_config)\n    else:\n        model = BloomModel(config)\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        missing_keys = None\n        for (i, file) in enumerate(file_names):\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            other_keys = model.load_state_dict(tensors, strict=False)\n            assert not other_keys.unexpected_keys, f'The keys {other_keys.unexpected_keys} are unexpected'\n            if missing_keys is None:\n                missing_keys = set(other_keys.missing_keys)\n            else:\n                missing_keys = missing_keys.intersection(set(other_keys.missing_keys))\n        assert not missing_keys, f'The keys {missing_keys} are missing'\n        os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n        pytorch_weights_dump_path = pytorch_dump_folder_path + '/' + WEIGHTS_NAME\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        print(f'Save PyTorch model to {pytorch_weights_dump_path} with dtype {config.torch_dtype}')\n        if config.torch_dtype is not None:\n            model = model.to(config.torch_dtype)\n        torch.save(model.state_dict(), pytorch_weights_dump_path)\n        print(f'Save configuration file to {pytorch_config_dump_path}')\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())",
        "mutated": [
            "def convert_bloom_checkpoint_to_pytorch(bloom_checkpoint_path, bloom_config_file, pytorch_dump_folder_path, shard_model, pretraining_tp):\n    if False:\n        i = 10\n    if bloom_config_file == '':\n        config = BloomConfig()\n    else:\n        config = BloomConfig.from_json_file(bloom_config_file)\n    if shard_model:\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        index_dict = {'weight_map': {}, 'metadata': {}}\n        total_size = 0\n        missing_keys = None\n        config = BloomConfig()\n        for (j, file) in enumerate(file_names):\n            print('Processing file: {}'.format(file))\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            torch.save(tensors, os.path.join(pytorch_dump_folder_path, 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))))\n            for key in tensors.keys():\n                value = tensors[key]\n                total_size += value.numel() * get_dtype_size(value.dtype)\n                if key not in index_dict['weight_map']:\n                    index_dict['weight_map'][key] = 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))\n        config = BloomConfig()\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        index_dict['metadata']['total_size'] = total_size\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())\n        with open(os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME + '.index.json'), 'w', encoding='utf-8') as f:\n            json_config = json.dumps(index_dict, indent=2, sort_keys=True) + '\\n'\n            f.write(json_config)\n    else:\n        model = BloomModel(config)\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        missing_keys = None\n        for (i, file) in enumerate(file_names):\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            other_keys = model.load_state_dict(tensors, strict=False)\n            assert not other_keys.unexpected_keys, f'The keys {other_keys.unexpected_keys} are unexpected'\n            if missing_keys is None:\n                missing_keys = set(other_keys.missing_keys)\n            else:\n                missing_keys = missing_keys.intersection(set(other_keys.missing_keys))\n        assert not missing_keys, f'The keys {missing_keys} are missing'\n        os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n        pytorch_weights_dump_path = pytorch_dump_folder_path + '/' + WEIGHTS_NAME\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        print(f'Save PyTorch model to {pytorch_weights_dump_path} with dtype {config.torch_dtype}')\n        if config.torch_dtype is not None:\n            model = model.to(config.torch_dtype)\n        torch.save(model.state_dict(), pytorch_weights_dump_path)\n        print(f'Save configuration file to {pytorch_config_dump_path}')\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())",
            "def convert_bloom_checkpoint_to_pytorch(bloom_checkpoint_path, bloom_config_file, pytorch_dump_folder_path, shard_model, pretraining_tp):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if bloom_config_file == '':\n        config = BloomConfig()\n    else:\n        config = BloomConfig.from_json_file(bloom_config_file)\n    if shard_model:\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        index_dict = {'weight_map': {}, 'metadata': {}}\n        total_size = 0\n        missing_keys = None\n        config = BloomConfig()\n        for (j, file) in enumerate(file_names):\n            print('Processing file: {}'.format(file))\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            torch.save(tensors, os.path.join(pytorch_dump_folder_path, 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))))\n            for key in tensors.keys():\n                value = tensors[key]\n                total_size += value.numel() * get_dtype_size(value.dtype)\n                if key not in index_dict['weight_map']:\n                    index_dict['weight_map'][key] = 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))\n        config = BloomConfig()\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        index_dict['metadata']['total_size'] = total_size\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())\n        with open(os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME + '.index.json'), 'w', encoding='utf-8') as f:\n            json_config = json.dumps(index_dict, indent=2, sort_keys=True) + '\\n'\n            f.write(json_config)\n    else:\n        model = BloomModel(config)\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        missing_keys = None\n        for (i, file) in enumerate(file_names):\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            other_keys = model.load_state_dict(tensors, strict=False)\n            assert not other_keys.unexpected_keys, f'The keys {other_keys.unexpected_keys} are unexpected'\n            if missing_keys is None:\n                missing_keys = set(other_keys.missing_keys)\n            else:\n                missing_keys = missing_keys.intersection(set(other_keys.missing_keys))\n        assert not missing_keys, f'The keys {missing_keys} are missing'\n        os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n        pytorch_weights_dump_path = pytorch_dump_folder_path + '/' + WEIGHTS_NAME\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        print(f'Save PyTorch model to {pytorch_weights_dump_path} with dtype {config.torch_dtype}')\n        if config.torch_dtype is not None:\n            model = model.to(config.torch_dtype)\n        torch.save(model.state_dict(), pytorch_weights_dump_path)\n        print(f'Save configuration file to {pytorch_config_dump_path}')\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())",
            "def convert_bloom_checkpoint_to_pytorch(bloom_checkpoint_path, bloom_config_file, pytorch_dump_folder_path, shard_model, pretraining_tp):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if bloom_config_file == '':\n        config = BloomConfig()\n    else:\n        config = BloomConfig.from_json_file(bloom_config_file)\n    if shard_model:\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        index_dict = {'weight_map': {}, 'metadata': {}}\n        total_size = 0\n        missing_keys = None\n        config = BloomConfig()\n        for (j, file) in enumerate(file_names):\n            print('Processing file: {}'.format(file))\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            torch.save(tensors, os.path.join(pytorch_dump_folder_path, 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))))\n            for key in tensors.keys():\n                value = tensors[key]\n                total_size += value.numel() * get_dtype_size(value.dtype)\n                if key not in index_dict['weight_map']:\n                    index_dict['weight_map'][key] = 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))\n        config = BloomConfig()\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        index_dict['metadata']['total_size'] = total_size\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())\n        with open(os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME + '.index.json'), 'w', encoding='utf-8') as f:\n            json_config = json.dumps(index_dict, indent=2, sort_keys=True) + '\\n'\n            f.write(json_config)\n    else:\n        model = BloomModel(config)\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        missing_keys = None\n        for (i, file) in enumerate(file_names):\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            other_keys = model.load_state_dict(tensors, strict=False)\n            assert not other_keys.unexpected_keys, f'The keys {other_keys.unexpected_keys} are unexpected'\n            if missing_keys is None:\n                missing_keys = set(other_keys.missing_keys)\n            else:\n                missing_keys = missing_keys.intersection(set(other_keys.missing_keys))\n        assert not missing_keys, f'The keys {missing_keys} are missing'\n        os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n        pytorch_weights_dump_path = pytorch_dump_folder_path + '/' + WEIGHTS_NAME\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        print(f'Save PyTorch model to {pytorch_weights_dump_path} with dtype {config.torch_dtype}')\n        if config.torch_dtype is not None:\n            model = model.to(config.torch_dtype)\n        torch.save(model.state_dict(), pytorch_weights_dump_path)\n        print(f'Save configuration file to {pytorch_config_dump_path}')\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())",
            "def convert_bloom_checkpoint_to_pytorch(bloom_checkpoint_path, bloom_config_file, pytorch_dump_folder_path, shard_model, pretraining_tp):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if bloom_config_file == '':\n        config = BloomConfig()\n    else:\n        config = BloomConfig.from_json_file(bloom_config_file)\n    if shard_model:\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        index_dict = {'weight_map': {}, 'metadata': {}}\n        total_size = 0\n        missing_keys = None\n        config = BloomConfig()\n        for (j, file) in enumerate(file_names):\n            print('Processing file: {}'.format(file))\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            torch.save(tensors, os.path.join(pytorch_dump_folder_path, 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))))\n            for key in tensors.keys():\n                value = tensors[key]\n                total_size += value.numel() * get_dtype_size(value.dtype)\n                if key not in index_dict['weight_map']:\n                    index_dict['weight_map'][key] = 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))\n        config = BloomConfig()\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        index_dict['metadata']['total_size'] = total_size\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())\n        with open(os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME + '.index.json'), 'w', encoding='utf-8') as f:\n            json_config = json.dumps(index_dict, indent=2, sort_keys=True) + '\\n'\n            f.write(json_config)\n    else:\n        model = BloomModel(config)\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        missing_keys = None\n        for (i, file) in enumerate(file_names):\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            other_keys = model.load_state_dict(tensors, strict=False)\n            assert not other_keys.unexpected_keys, f'The keys {other_keys.unexpected_keys} are unexpected'\n            if missing_keys is None:\n                missing_keys = set(other_keys.missing_keys)\n            else:\n                missing_keys = missing_keys.intersection(set(other_keys.missing_keys))\n        assert not missing_keys, f'The keys {missing_keys} are missing'\n        os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n        pytorch_weights_dump_path = pytorch_dump_folder_path + '/' + WEIGHTS_NAME\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        print(f'Save PyTorch model to {pytorch_weights_dump_path} with dtype {config.torch_dtype}')\n        if config.torch_dtype is not None:\n            model = model.to(config.torch_dtype)\n        torch.save(model.state_dict(), pytorch_weights_dump_path)\n        print(f'Save configuration file to {pytorch_config_dump_path}')\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())",
            "def convert_bloom_checkpoint_to_pytorch(bloom_checkpoint_path, bloom_config_file, pytorch_dump_folder_path, shard_model, pretraining_tp):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if bloom_config_file == '':\n        config = BloomConfig()\n    else:\n        config = BloomConfig.from_json_file(bloom_config_file)\n    if shard_model:\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        index_dict = {'weight_map': {}, 'metadata': {}}\n        total_size = 0\n        missing_keys = None\n        config = BloomConfig()\n        for (j, file) in enumerate(file_names):\n            print('Processing file: {}'.format(file))\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            torch.save(tensors, os.path.join(pytorch_dump_folder_path, 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))))\n            for key in tensors.keys():\n                value = tensors[key]\n                total_size += value.numel() * get_dtype_size(value.dtype)\n                if key not in index_dict['weight_map']:\n                    index_dict['weight_map'][key] = 'pytorch_model_{}-of-{}.bin'.format(str(j + 1).zfill(5), str(len(file_names)).zfill(5))\n        config = BloomConfig()\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        index_dict['metadata']['total_size'] = total_size\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())\n        with open(os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME + '.index.json'), 'w', encoding='utf-8') as f:\n            json_config = json.dumps(index_dict, indent=2, sort_keys=True) + '\\n'\n            f.write(json_config)\n    else:\n        model = BloomModel(config)\n        file_names = os.listdir(bloom_checkpoint_path)\n        file_names = sorted(filter(lambda s: s.startswith('layer') and 'model_00' in s, file_names))\n        missing_keys = None\n        for (i, file) in enumerate(file_names):\n            tensors = None\n            for i in range(pretraining_tp):\n                f_name = file.replace('model_00', f'model_0{i}')\n                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location='cpu')\n                keys = list(temp.keys())\n                for key in keys:\n                    temp[layer_name_mapping(key, file)] = temp.pop(key)\n                if tensors is None:\n                    tensors = temp\n                else:\n                    for key in tensors.keys():\n                        if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                            tensors[key] += temp[key]\n                        else:\n                            cat_dim = 1 if any((text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN)) else 0\n                            tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n            for key in tensors.keys():\n                if any((key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH)):\n                    tensors[key] = tensors[key] / pretraining_tp\n            other_keys = model.load_state_dict(tensors, strict=False)\n            assert not other_keys.unexpected_keys, f'The keys {other_keys.unexpected_keys} are unexpected'\n            if missing_keys is None:\n                missing_keys = set(other_keys.missing_keys)\n            else:\n                missing_keys = missing_keys.intersection(set(other_keys.missing_keys))\n        assert not missing_keys, f'The keys {missing_keys} are missing'\n        os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n        pytorch_weights_dump_path = pytorch_dump_folder_path + '/' + WEIGHTS_NAME\n        pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n        print(f'Save PyTorch model to {pytorch_weights_dump_path} with dtype {config.torch_dtype}')\n        if config.torch_dtype is not None:\n            model = model.to(config.torch_dtype)\n        torch.save(model.state_dict(), pytorch_weights_dump_path)\n        print(f'Save configuration file to {pytorch_config_dump_path}')\n        with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:\n            f.write(config.to_json_string())"
        ]
    }
]