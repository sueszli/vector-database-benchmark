[
    {
        "func_name": "__init__",
        "original": "def __init__(self, classifier: PyTorchClassifier, attack: EvasionAttack, beta: float):\n    \"\"\"\n        Create an :class:`.AdversarialTrainerTRADESPyTorch` instance.\n\n        :param classifier: Model to train adversarially.\n        :param attack: attack to use for data augmentation in adversarial training\n        :param beta: The scaling factor controlling tradeoff between clean loss and adversarial loss\n        \"\"\"\n    super().__init__(classifier, attack, beta)\n    self._classifier: PyTorchClassifier\n    self._attack: EvasionAttack\n    self._beta: float",
        "mutated": [
            "def __init__(self, classifier: PyTorchClassifier, attack: EvasionAttack, beta: float):\n    if False:\n        i = 10\n    '\\n        Create an :class:`.AdversarialTrainerTRADESPyTorch` instance.\\n\\n        :param classifier: Model to train adversarially.\\n        :param attack: attack to use for data augmentation in adversarial training\\n        :param beta: The scaling factor controlling tradeoff between clean loss and adversarial loss\\n        '\n    super().__init__(classifier, attack, beta)\n    self._classifier: PyTorchClassifier\n    self._attack: EvasionAttack\n    self._beta: float",
            "def __init__(self, classifier: PyTorchClassifier, attack: EvasionAttack, beta: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create an :class:`.AdversarialTrainerTRADESPyTorch` instance.\\n\\n        :param classifier: Model to train adversarially.\\n        :param attack: attack to use for data augmentation in adversarial training\\n        :param beta: The scaling factor controlling tradeoff between clean loss and adversarial loss\\n        '\n    super().__init__(classifier, attack, beta)\n    self._classifier: PyTorchClassifier\n    self._attack: EvasionAttack\n    self._beta: float",
            "def __init__(self, classifier: PyTorchClassifier, attack: EvasionAttack, beta: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create an :class:`.AdversarialTrainerTRADESPyTorch` instance.\\n\\n        :param classifier: Model to train adversarially.\\n        :param attack: attack to use for data augmentation in adversarial training\\n        :param beta: The scaling factor controlling tradeoff between clean loss and adversarial loss\\n        '\n    super().__init__(classifier, attack, beta)\n    self._classifier: PyTorchClassifier\n    self._attack: EvasionAttack\n    self._beta: float",
            "def __init__(self, classifier: PyTorchClassifier, attack: EvasionAttack, beta: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create an :class:`.AdversarialTrainerTRADESPyTorch` instance.\\n\\n        :param classifier: Model to train adversarially.\\n        :param attack: attack to use for data augmentation in adversarial training\\n        :param beta: The scaling factor controlling tradeoff between clean loss and adversarial loss\\n        '\n    super().__init__(classifier, attack, beta)\n    self._classifier: PyTorchClassifier\n    self._attack: EvasionAttack\n    self._beta: float",
            "def __init__(self, classifier: PyTorchClassifier, attack: EvasionAttack, beta: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create an :class:`.AdversarialTrainerTRADESPyTorch` instance.\\n\\n        :param classifier: Model to train adversarially.\\n        :param attack: attack to use for data augmentation in adversarial training\\n        :param beta: The scaling factor controlling tradeoff between clean loss and adversarial loss\\n        '\n    super().__init__(classifier, attack, beta)\n    self._classifier: PyTorchClassifier\n    self._attack: EvasionAttack\n    self._beta: float"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, x: np.ndarray, y: np.ndarray, validation_data: Optional[Tuple[np.ndarray, np.ndarray]]=None, batch_size: int=128, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    \"\"\"\n        Train a model adversarially with TRADES protocol.\n        See class documentation for more information on the exact procedure.\n\n        :param x: Training set.\n        :param y: Labels for the training set.\n        :param validation_data: Tuple consisting of validation data, (x_val, y_val)\n        :param batch_size: Size of batches.\n        :param nb_epochs: Number of epochs to use for trainings.\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\n                                  the target classifier.\n        \"\"\"\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    nb_batches = int(np.ceil(len(x) / batch_size))\n    ind = np.arange(len(x))\n    logger.info('Adversarial Training TRADES')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    if validation_data is not None:\n        (x_test, y_test) = validation_data\n        y_test = check_and_transform_label_format(y_test, nb_classes=self.classifier.nb_classes)\n        (x_preprocessed_test, y_preprocessed_test) = self._classifier._apply_preprocessing(x_test, y_test, fit=True)\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        np.random.shuffle(ind)\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            x_batch = x[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]].copy()\n            y_batch = y[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]]\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        if validation_data is not None:\n            output = np.argmax(self.predict(x_preprocessed_test), axis=1)\n            nb_correct_pred = np.sum(output == np.argmax(y_preprocessed_test, axis=1))\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc(tr): %.4f acc(val): %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n, nb_correct_pred / x_test.shape[0])\n        else:\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)",
        "mutated": [
            "def fit(self, x: np.ndarray, y: np.ndarray, validation_data: Optional[Tuple[np.ndarray, np.ndarray]]=None, batch_size: int=128, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    if False:\n        i = 10\n    '\\n        Train a model adversarially with TRADES protocol.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param x: Training set.\\n        :param y: Labels for the training set.\\n        :param validation_data: Tuple consisting of validation data, (x_val, y_val)\\n        :param batch_size: Size of batches.\\n        :param nb_epochs: Number of epochs to use for trainings.\\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\\n                                  the target classifier.\\n        '\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    nb_batches = int(np.ceil(len(x) / batch_size))\n    ind = np.arange(len(x))\n    logger.info('Adversarial Training TRADES')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    if validation_data is not None:\n        (x_test, y_test) = validation_data\n        y_test = check_and_transform_label_format(y_test, nb_classes=self.classifier.nb_classes)\n        (x_preprocessed_test, y_preprocessed_test) = self._classifier._apply_preprocessing(x_test, y_test, fit=True)\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        np.random.shuffle(ind)\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            x_batch = x[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]].copy()\n            y_batch = y[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]]\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        if validation_data is not None:\n            output = np.argmax(self.predict(x_preprocessed_test), axis=1)\n            nb_correct_pred = np.sum(output == np.argmax(y_preprocessed_test, axis=1))\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc(tr): %.4f acc(val): %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n, nb_correct_pred / x_test.shape[0])\n        else:\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)",
            "def fit(self, x: np.ndarray, y: np.ndarray, validation_data: Optional[Tuple[np.ndarray, np.ndarray]]=None, batch_size: int=128, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Train a model adversarially with TRADES protocol.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param x: Training set.\\n        :param y: Labels for the training set.\\n        :param validation_data: Tuple consisting of validation data, (x_val, y_val)\\n        :param batch_size: Size of batches.\\n        :param nb_epochs: Number of epochs to use for trainings.\\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\\n                                  the target classifier.\\n        '\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    nb_batches = int(np.ceil(len(x) / batch_size))\n    ind = np.arange(len(x))\n    logger.info('Adversarial Training TRADES')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    if validation_data is not None:\n        (x_test, y_test) = validation_data\n        y_test = check_and_transform_label_format(y_test, nb_classes=self.classifier.nb_classes)\n        (x_preprocessed_test, y_preprocessed_test) = self._classifier._apply_preprocessing(x_test, y_test, fit=True)\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        np.random.shuffle(ind)\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            x_batch = x[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]].copy()\n            y_batch = y[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]]\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        if validation_data is not None:\n            output = np.argmax(self.predict(x_preprocessed_test), axis=1)\n            nb_correct_pred = np.sum(output == np.argmax(y_preprocessed_test, axis=1))\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc(tr): %.4f acc(val): %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n, nb_correct_pred / x_test.shape[0])\n        else:\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)",
            "def fit(self, x: np.ndarray, y: np.ndarray, validation_data: Optional[Tuple[np.ndarray, np.ndarray]]=None, batch_size: int=128, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Train a model adversarially with TRADES protocol.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param x: Training set.\\n        :param y: Labels for the training set.\\n        :param validation_data: Tuple consisting of validation data, (x_val, y_val)\\n        :param batch_size: Size of batches.\\n        :param nb_epochs: Number of epochs to use for trainings.\\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\\n                                  the target classifier.\\n        '\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    nb_batches = int(np.ceil(len(x) / batch_size))\n    ind = np.arange(len(x))\n    logger.info('Adversarial Training TRADES')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    if validation_data is not None:\n        (x_test, y_test) = validation_data\n        y_test = check_and_transform_label_format(y_test, nb_classes=self.classifier.nb_classes)\n        (x_preprocessed_test, y_preprocessed_test) = self._classifier._apply_preprocessing(x_test, y_test, fit=True)\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        np.random.shuffle(ind)\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            x_batch = x[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]].copy()\n            y_batch = y[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]]\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        if validation_data is not None:\n            output = np.argmax(self.predict(x_preprocessed_test), axis=1)\n            nb_correct_pred = np.sum(output == np.argmax(y_preprocessed_test, axis=1))\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc(tr): %.4f acc(val): %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n, nb_correct_pred / x_test.shape[0])\n        else:\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)",
            "def fit(self, x: np.ndarray, y: np.ndarray, validation_data: Optional[Tuple[np.ndarray, np.ndarray]]=None, batch_size: int=128, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Train a model adversarially with TRADES protocol.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param x: Training set.\\n        :param y: Labels for the training set.\\n        :param validation_data: Tuple consisting of validation data, (x_val, y_val)\\n        :param batch_size: Size of batches.\\n        :param nb_epochs: Number of epochs to use for trainings.\\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\\n                                  the target classifier.\\n        '\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    nb_batches = int(np.ceil(len(x) / batch_size))\n    ind = np.arange(len(x))\n    logger.info('Adversarial Training TRADES')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    if validation_data is not None:\n        (x_test, y_test) = validation_data\n        y_test = check_and_transform_label_format(y_test, nb_classes=self.classifier.nb_classes)\n        (x_preprocessed_test, y_preprocessed_test) = self._classifier._apply_preprocessing(x_test, y_test, fit=True)\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        np.random.shuffle(ind)\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            x_batch = x[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]].copy()\n            y_batch = y[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]]\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        if validation_data is not None:\n            output = np.argmax(self.predict(x_preprocessed_test), axis=1)\n            nb_correct_pred = np.sum(output == np.argmax(y_preprocessed_test, axis=1))\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc(tr): %.4f acc(val): %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n, nb_correct_pred / x_test.shape[0])\n        else:\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)",
            "def fit(self, x: np.ndarray, y: np.ndarray, validation_data: Optional[Tuple[np.ndarray, np.ndarray]]=None, batch_size: int=128, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Train a model adversarially with TRADES protocol.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param x: Training set.\\n        :param y: Labels for the training set.\\n        :param validation_data: Tuple consisting of validation data, (x_val, y_val)\\n        :param batch_size: Size of batches.\\n        :param nb_epochs: Number of epochs to use for trainings.\\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\\n                                  the target classifier.\\n        '\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    nb_batches = int(np.ceil(len(x) / batch_size))\n    ind = np.arange(len(x))\n    logger.info('Adversarial Training TRADES')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    if validation_data is not None:\n        (x_test, y_test) = validation_data\n        y_test = check_and_transform_label_format(y_test, nb_classes=self.classifier.nb_classes)\n        (x_preprocessed_test, y_preprocessed_test) = self._classifier._apply_preprocessing(x_test, y_test, fit=True)\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        np.random.shuffle(ind)\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            x_batch = x[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]].copy()\n            y_batch = y[ind[batch_id * batch_size:min((batch_id + 1) * batch_size, x.shape[0])]]\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        if validation_data is not None:\n            output = np.argmax(self.predict(x_preprocessed_test), axis=1)\n            nb_correct_pred = np.sum(output == np.argmax(y_preprocessed_test, axis=1))\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc(tr): %.4f acc(val): %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n, nb_correct_pred / x_test.shape[0])\n        else:\n            logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)"
        ]
    },
    {
        "func_name": "fit_generator",
        "original": "def fit_generator(self, generator: DataGenerator, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    \"\"\"\n        Train a model adversarially with TRADES protocol using a data generator.\n        See class documentation for more information on the exact procedure.\n\n        :param generator: Data generator.\n        :param nb_epochs: Number of epochs to use for trainings.\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\n                                  the target classifier.\n        \"\"\"\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    size = generator.size\n    batch_size = generator.batch_size\n    if size is not None:\n        nb_batches = int(np.ceil(size / batch_size))\n    else:\n        raise ValueError('Size is None.')\n    logger.info('Adversarial Training TRADES')\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            (x_batch, y_batch) = generator.get_batch()\n            x_batch = x_batch.copy()\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)",
        "mutated": [
            "def fit_generator(self, generator: DataGenerator, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    if False:\n        i = 10\n    '\\n        Train a model adversarially with TRADES protocol using a data generator.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param generator: Data generator.\\n        :param nb_epochs: Number of epochs to use for trainings.\\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\\n                                  the target classifier.\\n        '\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    size = generator.size\n    batch_size = generator.batch_size\n    if size is not None:\n        nb_batches = int(np.ceil(size / batch_size))\n    else:\n        raise ValueError('Size is None.')\n    logger.info('Adversarial Training TRADES')\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            (x_batch, y_batch) = generator.get_batch()\n            x_batch = x_batch.copy()\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)",
            "def fit_generator(self, generator: DataGenerator, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Train a model adversarially with TRADES protocol using a data generator.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param generator: Data generator.\\n        :param nb_epochs: Number of epochs to use for trainings.\\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\\n                                  the target classifier.\\n        '\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    size = generator.size\n    batch_size = generator.batch_size\n    if size is not None:\n        nb_batches = int(np.ceil(size / batch_size))\n    else:\n        raise ValueError('Size is None.')\n    logger.info('Adversarial Training TRADES')\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            (x_batch, y_batch) = generator.get_batch()\n            x_batch = x_batch.copy()\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)",
            "def fit_generator(self, generator: DataGenerator, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Train a model adversarially with TRADES protocol using a data generator.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param generator: Data generator.\\n        :param nb_epochs: Number of epochs to use for trainings.\\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\\n                                  the target classifier.\\n        '\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    size = generator.size\n    batch_size = generator.batch_size\n    if size is not None:\n        nb_batches = int(np.ceil(size / batch_size))\n    else:\n        raise ValueError('Size is None.')\n    logger.info('Adversarial Training TRADES')\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            (x_batch, y_batch) = generator.get_batch()\n            x_batch = x_batch.copy()\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)",
            "def fit_generator(self, generator: DataGenerator, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Train a model adversarially with TRADES protocol using a data generator.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param generator: Data generator.\\n        :param nb_epochs: Number of epochs to use for trainings.\\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\\n                                  the target classifier.\\n        '\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    size = generator.size\n    batch_size = generator.batch_size\n    if size is not None:\n        nb_batches = int(np.ceil(size / batch_size))\n    else:\n        raise ValueError('Size is None.')\n    logger.info('Adversarial Training TRADES')\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            (x_batch, y_batch) = generator.get_batch()\n            x_batch = x_batch.copy()\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)",
            "def fit_generator(self, generator: DataGenerator, nb_epochs: int=20, scheduler: 'torch.optim.lr_scheduler._LRScheduler'=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Train a model adversarially with TRADES protocol using a data generator.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param generator: Data generator.\\n        :param nb_epochs: Number of epochs to use for trainings.\\n        :param scheduler: Learning rate scheduler to run at the end of every epoch.\\n        :param kwargs: Dictionary of framework-specific arguments. These will be passed as such to the `fit` function of\\n                                  the target classifier.\\n        '\n    import torch\n    logger.info('Performing adversarial training with TRADES protocol')\n    if scheduler is not None and (not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler)):\n        raise ValueError('Invalid Pytorch scheduler is provided for adversarial training.')\n    size = generator.size\n    batch_size = generator.batch_size\n    if size is not None:\n        nb_batches = int(np.ceil(size / batch_size))\n    else:\n        raise ValueError('Size is None.')\n    logger.info('Adversarial Training TRADES')\n    for i_epoch in trange(nb_epochs, desc='Adversarial Training TRADES - Epochs'):\n        start_time = time.time()\n        train_loss = 0.0\n        train_acc = 0.0\n        train_n = 0.0\n        for batch_id in range(nb_batches):\n            (x_batch, y_batch) = generator.get_batch()\n            x_batch = x_batch.copy()\n            (_train_loss, _train_acc, _train_n) = self._batch_process(x_batch, y_batch)\n            train_loss += _train_loss\n            train_acc += _train_acc\n            train_n += _train_n\n        if scheduler:\n            scheduler.step()\n        train_time = time.time()\n        logger.info('epoch: %s time(s): %.1f loss: %.4f acc: %.4f', i_epoch, train_time - start_time, train_loss / train_n, train_acc / train_n)"
        ]
    },
    {
        "func_name": "_batch_process",
        "original": "def _batch_process(self, x_batch: np.ndarray, y_batch: np.ndarray) -> Tuple[float, float, float]:\n    \"\"\"\n        Perform the operations of TRADES for a batch of data.\n        See class documentation for more information on the exact procedure.\n\n        :param x_batch: batch of x.\n        :param y_batch: batch of y.\n        :return: tuple containing batch data loss, batch data accuracy and number of samples in the batch\n        \"\"\"\n    import torch\n    from torch import nn\n    import torch.nn.functional as F\n    if self._classifier._optimizer is None:\n        raise ValueError('Optimizer of classifier is currently None, but is required for adversarial training.')\n    n = x_batch.shape[0]\n    self._classifier._model.train(mode=False)\n    x_batch_pert = self._attack.generate(x_batch, y=y_batch)\n    y_batch = check_and_transform_label_format(y_batch, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self._classifier._apply_preprocessing(x_batch, y_batch, fit=True)\n    (x_preprocessed_pert, _) = self._classifier._apply_preprocessing(x_batch_pert, y_batch, fit=True)\n    if self._classifier._reduce_labels:\n        y_preprocessed = np.argmax(y_preprocessed, axis=1)\n    i_batch = torch.from_numpy(x_preprocessed).to(self._classifier._device)\n    i_batch_pert = torch.from_numpy(x_preprocessed_pert).to(self._classifier._device)\n    o_batch = torch.from_numpy(y_preprocessed).to(self._classifier._device)\n    self._classifier._model.train(mode=True)\n    self._classifier._optimizer.zero_grad()\n    model_outputs = self._classifier._model(i_batch)\n    model_outputs_pert = self._classifier._model(i_batch_pert)\n    loss_clean = self._classifier._loss(model_outputs[-1], o_batch)\n    loss_kl = 1.0 / n * nn.KLDivLoss(reduction='sum')(F.log_softmax(model_outputs_pert[-1], dim=1), torch.clamp(F.softmax(model_outputs[-1], dim=1), min=EPS))\n    loss = loss_clean + self._beta * loss_kl\n    loss.backward()\n    self._classifier._optimizer.step()\n    train_loss = loss.item() * o_batch.size(0)\n    train_acc = (model_outputs_pert[0].max(1)[1] == o_batch).sum().item()\n    train_n = o_batch.size(0)\n    self._classifier._model.train(mode=False)\n    return (train_loss, train_acc, train_n)",
        "mutated": [
            "def _batch_process(self, x_batch: np.ndarray, y_batch: np.ndarray) -> Tuple[float, float, float]:\n    if False:\n        i = 10\n    '\\n        Perform the operations of TRADES for a batch of data.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param x_batch: batch of x.\\n        :param y_batch: batch of y.\\n        :return: tuple containing batch data loss, batch data accuracy and number of samples in the batch\\n        '\n    import torch\n    from torch import nn\n    import torch.nn.functional as F\n    if self._classifier._optimizer is None:\n        raise ValueError('Optimizer of classifier is currently None, but is required for adversarial training.')\n    n = x_batch.shape[0]\n    self._classifier._model.train(mode=False)\n    x_batch_pert = self._attack.generate(x_batch, y=y_batch)\n    y_batch = check_and_transform_label_format(y_batch, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self._classifier._apply_preprocessing(x_batch, y_batch, fit=True)\n    (x_preprocessed_pert, _) = self._classifier._apply_preprocessing(x_batch_pert, y_batch, fit=True)\n    if self._classifier._reduce_labels:\n        y_preprocessed = np.argmax(y_preprocessed, axis=1)\n    i_batch = torch.from_numpy(x_preprocessed).to(self._classifier._device)\n    i_batch_pert = torch.from_numpy(x_preprocessed_pert).to(self._classifier._device)\n    o_batch = torch.from_numpy(y_preprocessed).to(self._classifier._device)\n    self._classifier._model.train(mode=True)\n    self._classifier._optimizer.zero_grad()\n    model_outputs = self._classifier._model(i_batch)\n    model_outputs_pert = self._classifier._model(i_batch_pert)\n    loss_clean = self._classifier._loss(model_outputs[-1], o_batch)\n    loss_kl = 1.0 / n * nn.KLDivLoss(reduction='sum')(F.log_softmax(model_outputs_pert[-1], dim=1), torch.clamp(F.softmax(model_outputs[-1], dim=1), min=EPS))\n    loss = loss_clean + self._beta * loss_kl\n    loss.backward()\n    self._classifier._optimizer.step()\n    train_loss = loss.item() * o_batch.size(0)\n    train_acc = (model_outputs_pert[0].max(1)[1] == o_batch).sum().item()\n    train_n = o_batch.size(0)\n    self._classifier._model.train(mode=False)\n    return (train_loss, train_acc, train_n)",
            "def _batch_process(self, x_batch: np.ndarray, y_batch: np.ndarray) -> Tuple[float, float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Perform the operations of TRADES for a batch of data.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param x_batch: batch of x.\\n        :param y_batch: batch of y.\\n        :return: tuple containing batch data loss, batch data accuracy and number of samples in the batch\\n        '\n    import torch\n    from torch import nn\n    import torch.nn.functional as F\n    if self._classifier._optimizer is None:\n        raise ValueError('Optimizer of classifier is currently None, but is required for adversarial training.')\n    n = x_batch.shape[0]\n    self._classifier._model.train(mode=False)\n    x_batch_pert = self._attack.generate(x_batch, y=y_batch)\n    y_batch = check_and_transform_label_format(y_batch, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self._classifier._apply_preprocessing(x_batch, y_batch, fit=True)\n    (x_preprocessed_pert, _) = self._classifier._apply_preprocessing(x_batch_pert, y_batch, fit=True)\n    if self._classifier._reduce_labels:\n        y_preprocessed = np.argmax(y_preprocessed, axis=1)\n    i_batch = torch.from_numpy(x_preprocessed).to(self._classifier._device)\n    i_batch_pert = torch.from_numpy(x_preprocessed_pert).to(self._classifier._device)\n    o_batch = torch.from_numpy(y_preprocessed).to(self._classifier._device)\n    self._classifier._model.train(mode=True)\n    self._classifier._optimizer.zero_grad()\n    model_outputs = self._classifier._model(i_batch)\n    model_outputs_pert = self._classifier._model(i_batch_pert)\n    loss_clean = self._classifier._loss(model_outputs[-1], o_batch)\n    loss_kl = 1.0 / n * nn.KLDivLoss(reduction='sum')(F.log_softmax(model_outputs_pert[-1], dim=1), torch.clamp(F.softmax(model_outputs[-1], dim=1), min=EPS))\n    loss = loss_clean + self._beta * loss_kl\n    loss.backward()\n    self._classifier._optimizer.step()\n    train_loss = loss.item() * o_batch.size(0)\n    train_acc = (model_outputs_pert[0].max(1)[1] == o_batch).sum().item()\n    train_n = o_batch.size(0)\n    self._classifier._model.train(mode=False)\n    return (train_loss, train_acc, train_n)",
            "def _batch_process(self, x_batch: np.ndarray, y_batch: np.ndarray) -> Tuple[float, float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Perform the operations of TRADES for a batch of data.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param x_batch: batch of x.\\n        :param y_batch: batch of y.\\n        :return: tuple containing batch data loss, batch data accuracy and number of samples in the batch\\n        '\n    import torch\n    from torch import nn\n    import torch.nn.functional as F\n    if self._classifier._optimizer is None:\n        raise ValueError('Optimizer of classifier is currently None, but is required for adversarial training.')\n    n = x_batch.shape[0]\n    self._classifier._model.train(mode=False)\n    x_batch_pert = self._attack.generate(x_batch, y=y_batch)\n    y_batch = check_and_transform_label_format(y_batch, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self._classifier._apply_preprocessing(x_batch, y_batch, fit=True)\n    (x_preprocessed_pert, _) = self._classifier._apply_preprocessing(x_batch_pert, y_batch, fit=True)\n    if self._classifier._reduce_labels:\n        y_preprocessed = np.argmax(y_preprocessed, axis=1)\n    i_batch = torch.from_numpy(x_preprocessed).to(self._classifier._device)\n    i_batch_pert = torch.from_numpy(x_preprocessed_pert).to(self._classifier._device)\n    o_batch = torch.from_numpy(y_preprocessed).to(self._classifier._device)\n    self._classifier._model.train(mode=True)\n    self._classifier._optimizer.zero_grad()\n    model_outputs = self._classifier._model(i_batch)\n    model_outputs_pert = self._classifier._model(i_batch_pert)\n    loss_clean = self._classifier._loss(model_outputs[-1], o_batch)\n    loss_kl = 1.0 / n * nn.KLDivLoss(reduction='sum')(F.log_softmax(model_outputs_pert[-1], dim=1), torch.clamp(F.softmax(model_outputs[-1], dim=1), min=EPS))\n    loss = loss_clean + self._beta * loss_kl\n    loss.backward()\n    self._classifier._optimizer.step()\n    train_loss = loss.item() * o_batch.size(0)\n    train_acc = (model_outputs_pert[0].max(1)[1] == o_batch).sum().item()\n    train_n = o_batch.size(0)\n    self._classifier._model.train(mode=False)\n    return (train_loss, train_acc, train_n)",
            "def _batch_process(self, x_batch: np.ndarray, y_batch: np.ndarray) -> Tuple[float, float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Perform the operations of TRADES for a batch of data.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param x_batch: batch of x.\\n        :param y_batch: batch of y.\\n        :return: tuple containing batch data loss, batch data accuracy and number of samples in the batch\\n        '\n    import torch\n    from torch import nn\n    import torch.nn.functional as F\n    if self._classifier._optimizer is None:\n        raise ValueError('Optimizer of classifier is currently None, but is required for adversarial training.')\n    n = x_batch.shape[0]\n    self._classifier._model.train(mode=False)\n    x_batch_pert = self._attack.generate(x_batch, y=y_batch)\n    y_batch = check_and_transform_label_format(y_batch, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self._classifier._apply_preprocessing(x_batch, y_batch, fit=True)\n    (x_preprocessed_pert, _) = self._classifier._apply_preprocessing(x_batch_pert, y_batch, fit=True)\n    if self._classifier._reduce_labels:\n        y_preprocessed = np.argmax(y_preprocessed, axis=1)\n    i_batch = torch.from_numpy(x_preprocessed).to(self._classifier._device)\n    i_batch_pert = torch.from_numpy(x_preprocessed_pert).to(self._classifier._device)\n    o_batch = torch.from_numpy(y_preprocessed).to(self._classifier._device)\n    self._classifier._model.train(mode=True)\n    self._classifier._optimizer.zero_grad()\n    model_outputs = self._classifier._model(i_batch)\n    model_outputs_pert = self._classifier._model(i_batch_pert)\n    loss_clean = self._classifier._loss(model_outputs[-1], o_batch)\n    loss_kl = 1.0 / n * nn.KLDivLoss(reduction='sum')(F.log_softmax(model_outputs_pert[-1], dim=1), torch.clamp(F.softmax(model_outputs[-1], dim=1), min=EPS))\n    loss = loss_clean + self._beta * loss_kl\n    loss.backward()\n    self._classifier._optimizer.step()\n    train_loss = loss.item() * o_batch.size(0)\n    train_acc = (model_outputs_pert[0].max(1)[1] == o_batch).sum().item()\n    train_n = o_batch.size(0)\n    self._classifier._model.train(mode=False)\n    return (train_loss, train_acc, train_n)",
            "def _batch_process(self, x_batch: np.ndarray, y_batch: np.ndarray) -> Tuple[float, float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Perform the operations of TRADES for a batch of data.\\n        See class documentation for more information on the exact procedure.\\n\\n        :param x_batch: batch of x.\\n        :param y_batch: batch of y.\\n        :return: tuple containing batch data loss, batch data accuracy and number of samples in the batch\\n        '\n    import torch\n    from torch import nn\n    import torch.nn.functional as F\n    if self._classifier._optimizer is None:\n        raise ValueError('Optimizer of classifier is currently None, but is required for adversarial training.')\n    n = x_batch.shape[0]\n    self._classifier._model.train(mode=False)\n    x_batch_pert = self._attack.generate(x_batch, y=y_batch)\n    y_batch = check_and_transform_label_format(y_batch, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self._classifier._apply_preprocessing(x_batch, y_batch, fit=True)\n    (x_preprocessed_pert, _) = self._classifier._apply_preprocessing(x_batch_pert, y_batch, fit=True)\n    if self._classifier._reduce_labels:\n        y_preprocessed = np.argmax(y_preprocessed, axis=1)\n    i_batch = torch.from_numpy(x_preprocessed).to(self._classifier._device)\n    i_batch_pert = torch.from_numpy(x_preprocessed_pert).to(self._classifier._device)\n    o_batch = torch.from_numpy(y_preprocessed).to(self._classifier._device)\n    self._classifier._model.train(mode=True)\n    self._classifier._optimizer.zero_grad()\n    model_outputs = self._classifier._model(i_batch)\n    model_outputs_pert = self._classifier._model(i_batch_pert)\n    loss_clean = self._classifier._loss(model_outputs[-1], o_batch)\n    loss_kl = 1.0 / n * nn.KLDivLoss(reduction='sum')(F.log_softmax(model_outputs_pert[-1], dim=1), torch.clamp(F.softmax(model_outputs[-1], dim=1), min=EPS))\n    loss = loss_clean + self._beta * loss_kl\n    loss.backward()\n    self._classifier._optimizer.step()\n    train_loss = loss.item() * o_batch.size(0)\n    train_acc = (model_outputs_pert[0].max(1)[1] == o_batch).sum().item()\n    train_n = o_batch.size(0)\n    self._classifier._model.train(mode=False)\n    return (train_loss, train_acc, train_n)"
        ]
    }
]