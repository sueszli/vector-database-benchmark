[
    {
        "func_name": "__init__",
        "original": "def __init__(self, seed_value=None):\n    super(SelfPlayTraining, self).__init__()\n    self.envs = None\n    self.eval_envs = None\n    self.evalsave_callbacks = None\n    self.archives = None\n    self.models = None\n    self.opponent_selection_callbacks = None\n    self.wandb_callbacks = None\n    self.seed_value = seed_value\n    self.deterministic = False\n    self.THREADED = THREADED",
        "mutated": [
            "def __init__(self, seed_value=None):\n    if False:\n        i = 10\n    super(SelfPlayTraining, self).__init__()\n    self.envs = None\n    self.eval_envs = None\n    self.evalsave_callbacks = None\n    self.archives = None\n    self.models = None\n    self.opponent_selection_callbacks = None\n    self.wandb_callbacks = None\n    self.seed_value = seed_value\n    self.deterministic = False\n    self.THREADED = THREADED",
            "def __init__(self, seed_value=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(SelfPlayTraining, self).__init__()\n    self.envs = None\n    self.eval_envs = None\n    self.evalsave_callbacks = None\n    self.archives = None\n    self.models = None\n    self.opponent_selection_callbacks = None\n    self.wandb_callbacks = None\n    self.seed_value = seed_value\n    self.deterministic = False\n    self.THREADED = THREADED",
            "def __init__(self, seed_value=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(SelfPlayTraining, self).__init__()\n    self.envs = None\n    self.eval_envs = None\n    self.evalsave_callbacks = None\n    self.archives = None\n    self.models = None\n    self.opponent_selection_callbacks = None\n    self.wandb_callbacks = None\n    self.seed_value = seed_value\n    self.deterministic = False\n    self.THREADED = THREADED",
            "def __init__(self, seed_value=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(SelfPlayTraining, self).__init__()\n    self.envs = None\n    self.eval_envs = None\n    self.evalsave_callbacks = None\n    self.archives = None\n    self.models = None\n    self.opponent_selection_callbacks = None\n    self.wandb_callbacks = None\n    self.seed_value = seed_value\n    self.deterministic = False\n    self.THREADED = THREADED",
            "def __init__(self, seed_value=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(SelfPlayTraining, self).__init__()\n    self.envs = None\n    self.eval_envs = None\n    self.evalsave_callbacks = None\n    self.archives = None\n    self.models = None\n    self.opponent_selection_callbacks = None\n    self.wandb_callbacks = None\n    self.seed_value = seed_value\n    self.deterministic = False\n    self.THREADED = THREADED"
        ]
    },
    {
        "func_name": "_init_argparse",
        "original": "def _init_argparse(self):\n    super(SelfPlayTraining, self)._init_argparse(description='Self-play experiment training script', help='The experiemnt configuration file path and name which the experiment should be loaded')",
        "mutated": [
            "def _init_argparse(self):\n    if False:\n        i = 10\n    super(SelfPlayTraining, self)._init_argparse(description='Self-play experiment training script', help='The experiemnt configuration file path and name which the experiment should be loaded')",
            "def _init_argparse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(SelfPlayTraining, self)._init_argparse(description='Self-play experiment training script', help='The experiemnt configuration file path and name which the experiment should be loaded')",
            "def _init_argparse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(SelfPlayTraining, self)._init_argparse(description='Self-play experiment training script', help='The experiemnt configuration file path and name which the experiment should be loaded')",
            "def _init_argparse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(SelfPlayTraining, self)._init_argparse(description='Self-play experiment training script', help='The experiemnt configuration file path and name which the experiment should be loaded')",
            "def _init_argparse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(SelfPlayTraining, self)._init_argparse(description='Self-play experiment training script', help='The experiemnt configuration file path and name which the experiment should be loaded')"
        ]
    },
    {
        "func_name": "_generate_log_dir",
        "original": "def _generate_log_dir(self):\n    return super(SelfPlayTraining, self)._generate_log_dir(dir_postfix='train')",
        "mutated": [
            "def _generate_log_dir(self):\n    if False:\n        i = 10\n    return super(SelfPlayTraining, self)._generate_log_dir(dir_postfix='train')",
            "def _generate_log_dir(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return super(SelfPlayTraining, self)._generate_log_dir(dir_postfix='train')",
            "def _generate_log_dir(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return super(SelfPlayTraining, self)._generate_log_dir(dir_postfix='train')",
            "def _generate_log_dir(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return super(SelfPlayTraining, self)._generate_log_dir(dir_postfix='train')",
            "def _generate_log_dir(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return super(SelfPlayTraining, self)._generate_log_dir(dir_postfix='train')"
        ]
    },
    {
        "func_name": "_init_archives",
        "original": "def _init_archives(self):\n    self.archives = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        eval_opponent_selection = agent_configs['eval_opponent_selection']\n        opponent_selection = agent_configs['opponent_selection']\n        self.archives[agent_name] = Archive(sorting_keys=[eval_opponent_selection, opponent_selection], sorting=True, moving_least_freq_flag=False, save_path=os.path.join(self.log_dir, agent_name), delta=agent_configs.get('delta_latest', 0) * population_size)",
        "mutated": [
            "def _init_archives(self):\n    if False:\n        i = 10\n    self.archives = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        eval_opponent_selection = agent_configs['eval_opponent_selection']\n        opponent_selection = agent_configs['opponent_selection']\n        self.archives[agent_name] = Archive(sorting_keys=[eval_opponent_selection, opponent_selection], sorting=True, moving_least_freq_flag=False, save_path=os.path.join(self.log_dir, agent_name), delta=agent_configs.get('delta_latest', 0) * population_size)",
            "def _init_archives(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.archives = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        eval_opponent_selection = agent_configs['eval_opponent_selection']\n        opponent_selection = agent_configs['opponent_selection']\n        self.archives[agent_name] = Archive(sorting_keys=[eval_opponent_selection, opponent_selection], sorting=True, moving_least_freq_flag=False, save_path=os.path.join(self.log_dir, agent_name), delta=agent_configs.get('delta_latest', 0) * population_size)",
            "def _init_archives(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.archives = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        eval_opponent_selection = agent_configs['eval_opponent_selection']\n        opponent_selection = agent_configs['opponent_selection']\n        self.archives[agent_name] = Archive(sorting_keys=[eval_opponent_selection, opponent_selection], sorting=True, moving_least_freq_flag=False, save_path=os.path.join(self.log_dir, agent_name), delta=agent_configs.get('delta_latest', 0) * population_size)",
            "def _init_archives(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.archives = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        eval_opponent_selection = agent_configs['eval_opponent_selection']\n        opponent_selection = agent_configs['opponent_selection']\n        self.archives[agent_name] = Archive(sorting_keys=[eval_opponent_selection, opponent_selection], sorting=True, moving_least_freq_flag=False, save_path=os.path.join(self.log_dir, agent_name), delta=agent_configs.get('delta_latest', 0) * population_size)",
            "def _init_archives(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.archives = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        eval_opponent_selection = agent_configs['eval_opponent_selection']\n        opponent_selection = agent_configs['opponent_selection']\n        self.archives[agent_name] = Archive(sorting_keys=[eval_opponent_selection, opponent_selection], sorting=True, moving_least_freq_flag=False, save_path=os.path.join(self.log_dir, agent_name), delta=agent_configs.get('delta_latest', 0) * population_size)"
        ]
    },
    {
        "func_name": "_init_envs",
        "original": "def _init_envs(self):\n    self.envs = {}\n    self.eval_envs = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.envs[agent_name] = []\n        self.eval_envs[agent_name] = []\n        for population_num in range(population_size):\n            opponent_name = agent_configs['opponent_name']\n            opponent_archive = self.archives[opponent_name]\n            sampling_parameters = {'opponent_selection': agent_configs['opponent_selection'], 'sample_path': os.path.join(self.log_dir, opponent_name), 'randomly_reseed_sampling': agent_configs.get('randomly_reseed_sampling', False)}\n            algorithm_class = None\n            opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n            if opponent_algorithm_class_cfg == 'PPO':\n                algorithm_class = PPO\n            elif opponent_algorithm_class_cfg == 'SAC':\n                algorithm_class = SAC\n            self.envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Training', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=agent_configs['sample_after_reset'], sampling_parameters=sampling_parameters, seed_value=self.seed_value + population_num))\n            self.eval_envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Evaluation', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=False, sampling_parameters=None, seed_value=self.seed_value + population_num))",
        "mutated": [
            "def _init_envs(self):\n    if False:\n        i = 10\n    self.envs = {}\n    self.eval_envs = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.envs[agent_name] = []\n        self.eval_envs[agent_name] = []\n        for population_num in range(population_size):\n            opponent_name = agent_configs['opponent_name']\n            opponent_archive = self.archives[opponent_name]\n            sampling_parameters = {'opponent_selection': agent_configs['opponent_selection'], 'sample_path': os.path.join(self.log_dir, opponent_name), 'randomly_reseed_sampling': agent_configs.get('randomly_reseed_sampling', False)}\n            algorithm_class = None\n            opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n            if opponent_algorithm_class_cfg == 'PPO':\n                algorithm_class = PPO\n            elif opponent_algorithm_class_cfg == 'SAC':\n                algorithm_class = SAC\n            self.envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Training', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=agent_configs['sample_after_reset'], sampling_parameters=sampling_parameters, seed_value=self.seed_value + population_num))\n            self.eval_envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Evaluation', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=False, sampling_parameters=None, seed_value=self.seed_value + population_num))",
            "def _init_envs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.envs = {}\n    self.eval_envs = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.envs[agent_name] = []\n        self.eval_envs[agent_name] = []\n        for population_num in range(population_size):\n            opponent_name = agent_configs['opponent_name']\n            opponent_archive = self.archives[opponent_name]\n            sampling_parameters = {'opponent_selection': agent_configs['opponent_selection'], 'sample_path': os.path.join(self.log_dir, opponent_name), 'randomly_reseed_sampling': agent_configs.get('randomly_reseed_sampling', False)}\n            algorithm_class = None\n            opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n            if opponent_algorithm_class_cfg == 'PPO':\n                algorithm_class = PPO\n            elif opponent_algorithm_class_cfg == 'SAC':\n                algorithm_class = SAC\n            self.envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Training', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=agent_configs['sample_after_reset'], sampling_parameters=sampling_parameters, seed_value=self.seed_value + population_num))\n            self.eval_envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Evaluation', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=False, sampling_parameters=None, seed_value=self.seed_value + population_num))",
            "def _init_envs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.envs = {}\n    self.eval_envs = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.envs[agent_name] = []\n        self.eval_envs[agent_name] = []\n        for population_num in range(population_size):\n            opponent_name = agent_configs['opponent_name']\n            opponent_archive = self.archives[opponent_name]\n            sampling_parameters = {'opponent_selection': agent_configs['opponent_selection'], 'sample_path': os.path.join(self.log_dir, opponent_name), 'randomly_reseed_sampling': agent_configs.get('randomly_reseed_sampling', False)}\n            algorithm_class = None\n            opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n            if opponent_algorithm_class_cfg == 'PPO':\n                algorithm_class = PPO\n            elif opponent_algorithm_class_cfg == 'SAC':\n                algorithm_class = SAC\n            self.envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Training', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=agent_configs['sample_after_reset'], sampling_parameters=sampling_parameters, seed_value=self.seed_value + population_num))\n            self.eval_envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Evaluation', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=False, sampling_parameters=None, seed_value=self.seed_value + population_num))",
            "def _init_envs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.envs = {}\n    self.eval_envs = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.envs[agent_name] = []\n        self.eval_envs[agent_name] = []\n        for population_num in range(population_size):\n            opponent_name = agent_configs['opponent_name']\n            opponent_archive = self.archives[opponent_name]\n            sampling_parameters = {'opponent_selection': agent_configs['opponent_selection'], 'sample_path': os.path.join(self.log_dir, opponent_name), 'randomly_reseed_sampling': agent_configs.get('randomly_reseed_sampling', False)}\n            algorithm_class = None\n            opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n            if opponent_algorithm_class_cfg == 'PPO':\n                algorithm_class = PPO\n            elif opponent_algorithm_class_cfg == 'SAC':\n                algorithm_class = SAC\n            self.envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Training', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=agent_configs['sample_after_reset'], sampling_parameters=sampling_parameters, seed_value=self.seed_value + population_num))\n            self.eval_envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Evaluation', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=False, sampling_parameters=None, seed_value=self.seed_value + population_num))",
            "def _init_envs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.envs = {}\n    self.eval_envs = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.envs[agent_name] = []\n        self.eval_envs[agent_name] = []\n        for population_num in range(population_size):\n            opponent_name = agent_configs['opponent_name']\n            opponent_archive = self.archives[opponent_name]\n            sampling_parameters = {'opponent_selection': agent_configs['opponent_selection'], 'sample_path': os.path.join(self.log_dir, opponent_name), 'randomly_reseed_sampling': agent_configs.get('randomly_reseed_sampling', False)}\n            algorithm_class = None\n            opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n            if opponent_algorithm_class_cfg == 'PPO':\n                algorithm_class = PPO\n            elif opponent_algorithm_class_cfg == 'SAC':\n                algorithm_class = SAC\n            self.envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Training', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=agent_configs['sample_after_reset'], sampling_parameters=sampling_parameters, seed_value=self.seed_value + population_num))\n            self.eval_envs[agent_name].append(super(SelfPlayTraining, self).create_env(key=k, name='Evaluation', algorithm_class=algorithm_class, opponent_archive=opponent_archive, sample_after_reset=False, sampling_parameters=None, seed_value=self.seed_value + population_num))"
        ]
    },
    {
        "func_name": "_init_models",
        "original": "def _init_models(self):\n    self.models = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.models[agent_name] = []\n        for population_num in range(population_size):\n            policy_kwargs = get_policy_arch(str(agent_configs.get('policy_arch', None)))\n            policy = None\n            if agent_configs['rl_algorithm'] == 'PPO':\n                policy = PPO(agent_configs['policy'], self.envs[agent_name][population_num], clip_range=agent_configs['clip_range'], ent_coef=agent_configs['ent_coef'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=2, tensorboard_log=os.path.join(self.log_dir, agent_name), n_epochs=agent_configs['n_epochs'], n_steps=agent_configs.get('n_steps', 2048), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            elif agent_configs['rl_algorithm'] == 'SAC':\n                policy = SAC(agent_configs['policy'], self.envs[agent_name][population_num], buffer_size=agent_configs['buffer_size'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=agent_configs.get('verbose', 2), tensorboard_log=os.path.join(self.log_dir, agent_name), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            self.models[agent_name].append(policy)",
        "mutated": [
            "def _init_models(self):\n    if False:\n        i = 10\n    self.models = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.models[agent_name] = []\n        for population_num in range(population_size):\n            policy_kwargs = get_policy_arch(str(agent_configs.get('policy_arch', None)))\n            policy = None\n            if agent_configs['rl_algorithm'] == 'PPO':\n                policy = PPO(agent_configs['policy'], self.envs[agent_name][population_num], clip_range=agent_configs['clip_range'], ent_coef=agent_configs['ent_coef'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=2, tensorboard_log=os.path.join(self.log_dir, agent_name), n_epochs=agent_configs['n_epochs'], n_steps=agent_configs.get('n_steps', 2048), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            elif agent_configs['rl_algorithm'] == 'SAC':\n                policy = SAC(agent_configs['policy'], self.envs[agent_name][population_num], buffer_size=agent_configs['buffer_size'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=agent_configs.get('verbose', 2), tensorboard_log=os.path.join(self.log_dir, agent_name), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            self.models[agent_name].append(policy)",
            "def _init_models(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.models = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.models[agent_name] = []\n        for population_num in range(population_size):\n            policy_kwargs = get_policy_arch(str(agent_configs.get('policy_arch', None)))\n            policy = None\n            if agent_configs['rl_algorithm'] == 'PPO':\n                policy = PPO(agent_configs['policy'], self.envs[agent_name][population_num], clip_range=agent_configs['clip_range'], ent_coef=agent_configs['ent_coef'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=2, tensorboard_log=os.path.join(self.log_dir, agent_name), n_epochs=agent_configs['n_epochs'], n_steps=agent_configs.get('n_steps', 2048), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            elif agent_configs['rl_algorithm'] == 'SAC':\n                policy = SAC(agent_configs['policy'], self.envs[agent_name][population_num], buffer_size=agent_configs['buffer_size'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=agent_configs.get('verbose', 2), tensorboard_log=os.path.join(self.log_dir, agent_name), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            self.models[agent_name].append(policy)",
            "def _init_models(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.models = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.models[agent_name] = []\n        for population_num in range(population_size):\n            policy_kwargs = get_policy_arch(str(agent_configs.get('policy_arch', None)))\n            policy = None\n            if agent_configs['rl_algorithm'] == 'PPO':\n                policy = PPO(agent_configs['policy'], self.envs[agent_name][population_num], clip_range=agent_configs['clip_range'], ent_coef=agent_configs['ent_coef'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=2, tensorboard_log=os.path.join(self.log_dir, agent_name), n_epochs=agent_configs['n_epochs'], n_steps=agent_configs.get('n_steps', 2048), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            elif agent_configs['rl_algorithm'] == 'SAC':\n                policy = SAC(agent_configs['policy'], self.envs[agent_name][population_num], buffer_size=agent_configs['buffer_size'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=agent_configs.get('verbose', 2), tensorboard_log=os.path.join(self.log_dir, agent_name), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            self.models[agent_name].append(policy)",
            "def _init_models(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.models = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.models[agent_name] = []\n        for population_num in range(population_size):\n            policy_kwargs = get_policy_arch(str(agent_configs.get('policy_arch', None)))\n            policy = None\n            if agent_configs['rl_algorithm'] == 'PPO':\n                policy = PPO(agent_configs['policy'], self.envs[agent_name][population_num], clip_range=agent_configs['clip_range'], ent_coef=agent_configs['ent_coef'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=2, tensorboard_log=os.path.join(self.log_dir, agent_name), n_epochs=agent_configs['n_epochs'], n_steps=agent_configs.get('n_steps', 2048), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            elif agent_configs['rl_algorithm'] == 'SAC':\n                policy = SAC(agent_configs['policy'], self.envs[agent_name][population_num], buffer_size=agent_configs['buffer_size'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=agent_configs.get('verbose', 2), tensorboard_log=os.path.join(self.log_dir, agent_name), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            self.models[agent_name].append(policy)",
            "def _init_models(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.models = {}\n    population_size = self.experiment_configs['population_size']\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        self.models[agent_name] = []\n        for population_num in range(population_size):\n            policy_kwargs = get_policy_arch(str(agent_configs.get('policy_arch', None)))\n            policy = None\n            if agent_configs['rl_algorithm'] == 'PPO':\n                policy = PPO(agent_configs['policy'], self.envs[agent_name][population_num], clip_range=agent_configs['clip_range'], ent_coef=agent_configs['ent_coef'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=2, tensorboard_log=os.path.join(self.log_dir, agent_name), n_epochs=agent_configs['n_epochs'], n_steps=agent_configs.get('n_steps', 2048), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            elif agent_configs['rl_algorithm'] == 'SAC':\n                policy = SAC(agent_configs['policy'], self.envs[agent_name][population_num], buffer_size=agent_configs['buffer_size'], learning_rate=agent_configs['lr'], batch_size=agent_configs['batch_size'], gamma=agent_configs['gamma'], verbose=agent_configs.get('verbose', 2), tensorboard_log=os.path.join(self.log_dir, agent_name), seed=self.seed_value + population_num, policy_kwargs=policy_kwargs)\n            self.models[agent_name].append(policy)"
        ]
    },
    {
        "func_name": "_init_callbacks",
        "original": "def _init_callbacks(self):\n    self.opponent_selection_callbacks = {}\n    self.evalsave_callbacks = {}\n    self.wandb_callbacks = {}\n    population_size = self.experiment_configs['population_size']\n    self.eval_matrix_method = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        opponent_name = agent_configs['opponent_name']\n        opponent_sample_path = os.path.join(self.log_dir, opponent_name)\n        agent_path = os.path.join(self.log_dir, agent_name)\n        self.eval_matrix_method[agent_name] = agent_configs.get('eval_matrix_method', 'reward')\n        self.evalsave_callbacks[agent_name] = []\n        self.opponent_selection_callbacks[agent_name] = []\n        for population_num in range(population_size):\n            enable_evaluation_matrix = True\n            self.evalsave_callbacks[agent_name].append(EvalSaveCallback(eval_env=self.eval_envs[agent_name][population_num], log_path=agent_path, eval_freq=int(agent_configs['eval_freq']), n_eval_episodes=agent_configs['num_eval_episodes'], deterministic=self.deterministic, save_path=agent_path, eval_metric=agent_configs['eval_metric'], eval_opponent_selection=agent_configs['eval_opponent_selection'], eval_sample_path=opponent_sample_path, save_freq=int(agent_configs['save_freq']), archive={'self': self.archives[agent_name], 'opponent': self.archives[opponent_name]}, agent_name=agent_name, num_rounds=self.experiment_configs['num_rounds'], seed_value=self.seed_value, enable_evaluation_matrix=enable_evaluation_matrix, randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False), eval_matrix_method=self.eval_matrix_method[agent_name]))\n            self.evalsave_callbacks[agent_name][-1].population_idx = population_num\n            self.opponent_selection_callbacks[agent_name].append(TrainingOpponentSelectionCallback(sample_path=opponent_sample_path, env=self.envs[agent_name][population_num], opponent_selection=agent_configs['opponent_selection'], sample_after_rollout=agent_configs['sample_after_rollout'], sample_after_reset=agent_configs['sample_after_reset'], num_sampled_per_round=agent_configs['num_sampled_opponent_per_round'], archive=self.archives[opponent_name], randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False)))\n        self.wandb_callbacks[agent_name] = WandbCallback()",
        "mutated": [
            "def _init_callbacks(self):\n    if False:\n        i = 10\n    self.opponent_selection_callbacks = {}\n    self.evalsave_callbacks = {}\n    self.wandb_callbacks = {}\n    population_size = self.experiment_configs['population_size']\n    self.eval_matrix_method = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        opponent_name = agent_configs['opponent_name']\n        opponent_sample_path = os.path.join(self.log_dir, opponent_name)\n        agent_path = os.path.join(self.log_dir, agent_name)\n        self.eval_matrix_method[agent_name] = agent_configs.get('eval_matrix_method', 'reward')\n        self.evalsave_callbacks[agent_name] = []\n        self.opponent_selection_callbacks[agent_name] = []\n        for population_num in range(population_size):\n            enable_evaluation_matrix = True\n            self.evalsave_callbacks[agent_name].append(EvalSaveCallback(eval_env=self.eval_envs[agent_name][population_num], log_path=agent_path, eval_freq=int(agent_configs['eval_freq']), n_eval_episodes=agent_configs['num_eval_episodes'], deterministic=self.deterministic, save_path=agent_path, eval_metric=agent_configs['eval_metric'], eval_opponent_selection=agent_configs['eval_opponent_selection'], eval_sample_path=opponent_sample_path, save_freq=int(agent_configs['save_freq']), archive={'self': self.archives[agent_name], 'opponent': self.archives[opponent_name]}, agent_name=agent_name, num_rounds=self.experiment_configs['num_rounds'], seed_value=self.seed_value, enable_evaluation_matrix=enable_evaluation_matrix, randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False), eval_matrix_method=self.eval_matrix_method[agent_name]))\n            self.evalsave_callbacks[agent_name][-1].population_idx = population_num\n            self.opponent_selection_callbacks[agent_name].append(TrainingOpponentSelectionCallback(sample_path=opponent_sample_path, env=self.envs[agent_name][population_num], opponent_selection=agent_configs['opponent_selection'], sample_after_rollout=agent_configs['sample_after_rollout'], sample_after_reset=agent_configs['sample_after_reset'], num_sampled_per_round=agent_configs['num_sampled_opponent_per_round'], archive=self.archives[opponent_name], randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False)))\n        self.wandb_callbacks[agent_name] = WandbCallback()",
            "def _init_callbacks(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.opponent_selection_callbacks = {}\n    self.evalsave_callbacks = {}\n    self.wandb_callbacks = {}\n    population_size = self.experiment_configs['population_size']\n    self.eval_matrix_method = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        opponent_name = agent_configs['opponent_name']\n        opponent_sample_path = os.path.join(self.log_dir, opponent_name)\n        agent_path = os.path.join(self.log_dir, agent_name)\n        self.eval_matrix_method[agent_name] = agent_configs.get('eval_matrix_method', 'reward')\n        self.evalsave_callbacks[agent_name] = []\n        self.opponent_selection_callbacks[agent_name] = []\n        for population_num in range(population_size):\n            enable_evaluation_matrix = True\n            self.evalsave_callbacks[agent_name].append(EvalSaveCallback(eval_env=self.eval_envs[agent_name][population_num], log_path=agent_path, eval_freq=int(agent_configs['eval_freq']), n_eval_episodes=agent_configs['num_eval_episodes'], deterministic=self.deterministic, save_path=agent_path, eval_metric=agent_configs['eval_metric'], eval_opponent_selection=agent_configs['eval_opponent_selection'], eval_sample_path=opponent_sample_path, save_freq=int(agent_configs['save_freq']), archive={'self': self.archives[agent_name], 'opponent': self.archives[opponent_name]}, agent_name=agent_name, num_rounds=self.experiment_configs['num_rounds'], seed_value=self.seed_value, enable_evaluation_matrix=enable_evaluation_matrix, randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False), eval_matrix_method=self.eval_matrix_method[agent_name]))\n            self.evalsave_callbacks[agent_name][-1].population_idx = population_num\n            self.opponent_selection_callbacks[agent_name].append(TrainingOpponentSelectionCallback(sample_path=opponent_sample_path, env=self.envs[agent_name][population_num], opponent_selection=agent_configs['opponent_selection'], sample_after_rollout=agent_configs['sample_after_rollout'], sample_after_reset=agent_configs['sample_after_reset'], num_sampled_per_round=agent_configs['num_sampled_opponent_per_round'], archive=self.archives[opponent_name], randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False)))\n        self.wandb_callbacks[agent_name] = WandbCallback()",
            "def _init_callbacks(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.opponent_selection_callbacks = {}\n    self.evalsave_callbacks = {}\n    self.wandb_callbacks = {}\n    population_size = self.experiment_configs['population_size']\n    self.eval_matrix_method = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        opponent_name = agent_configs['opponent_name']\n        opponent_sample_path = os.path.join(self.log_dir, opponent_name)\n        agent_path = os.path.join(self.log_dir, agent_name)\n        self.eval_matrix_method[agent_name] = agent_configs.get('eval_matrix_method', 'reward')\n        self.evalsave_callbacks[agent_name] = []\n        self.opponent_selection_callbacks[agent_name] = []\n        for population_num in range(population_size):\n            enable_evaluation_matrix = True\n            self.evalsave_callbacks[agent_name].append(EvalSaveCallback(eval_env=self.eval_envs[agent_name][population_num], log_path=agent_path, eval_freq=int(agent_configs['eval_freq']), n_eval_episodes=agent_configs['num_eval_episodes'], deterministic=self.deterministic, save_path=agent_path, eval_metric=agent_configs['eval_metric'], eval_opponent_selection=agent_configs['eval_opponent_selection'], eval_sample_path=opponent_sample_path, save_freq=int(agent_configs['save_freq']), archive={'self': self.archives[agent_name], 'opponent': self.archives[opponent_name]}, agent_name=agent_name, num_rounds=self.experiment_configs['num_rounds'], seed_value=self.seed_value, enable_evaluation_matrix=enable_evaluation_matrix, randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False), eval_matrix_method=self.eval_matrix_method[agent_name]))\n            self.evalsave_callbacks[agent_name][-1].population_idx = population_num\n            self.opponent_selection_callbacks[agent_name].append(TrainingOpponentSelectionCallback(sample_path=opponent_sample_path, env=self.envs[agent_name][population_num], opponent_selection=agent_configs['opponent_selection'], sample_after_rollout=agent_configs['sample_after_rollout'], sample_after_reset=agent_configs['sample_after_reset'], num_sampled_per_round=agent_configs['num_sampled_opponent_per_round'], archive=self.archives[opponent_name], randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False)))\n        self.wandb_callbacks[agent_name] = WandbCallback()",
            "def _init_callbacks(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.opponent_selection_callbacks = {}\n    self.evalsave_callbacks = {}\n    self.wandb_callbacks = {}\n    population_size = self.experiment_configs['population_size']\n    self.eval_matrix_method = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        opponent_name = agent_configs['opponent_name']\n        opponent_sample_path = os.path.join(self.log_dir, opponent_name)\n        agent_path = os.path.join(self.log_dir, agent_name)\n        self.eval_matrix_method[agent_name] = agent_configs.get('eval_matrix_method', 'reward')\n        self.evalsave_callbacks[agent_name] = []\n        self.opponent_selection_callbacks[agent_name] = []\n        for population_num in range(population_size):\n            enable_evaluation_matrix = True\n            self.evalsave_callbacks[agent_name].append(EvalSaveCallback(eval_env=self.eval_envs[agent_name][population_num], log_path=agent_path, eval_freq=int(agent_configs['eval_freq']), n_eval_episodes=agent_configs['num_eval_episodes'], deterministic=self.deterministic, save_path=agent_path, eval_metric=agent_configs['eval_metric'], eval_opponent_selection=agent_configs['eval_opponent_selection'], eval_sample_path=opponent_sample_path, save_freq=int(agent_configs['save_freq']), archive={'self': self.archives[agent_name], 'opponent': self.archives[opponent_name]}, agent_name=agent_name, num_rounds=self.experiment_configs['num_rounds'], seed_value=self.seed_value, enable_evaluation_matrix=enable_evaluation_matrix, randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False), eval_matrix_method=self.eval_matrix_method[agent_name]))\n            self.evalsave_callbacks[agent_name][-1].population_idx = population_num\n            self.opponent_selection_callbacks[agent_name].append(TrainingOpponentSelectionCallback(sample_path=opponent_sample_path, env=self.envs[agent_name][population_num], opponent_selection=agent_configs['opponent_selection'], sample_after_rollout=agent_configs['sample_after_rollout'], sample_after_reset=agent_configs['sample_after_reset'], num_sampled_per_round=agent_configs['num_sampled_opponent_per_round'], archive=self.archives[opponent_name], randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False)))\n        self.wandb_callbacks[agent_name] = WandbCallback()",
            "def _init_callbacks(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.opponent_selection_callbacks = {}\n    self.evalsave_callbacks = {}\n    self.wandb_callbacks = {}\n    population_size = self.experiment_configs['population_size']\n    self.eval_matrix_method = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        opponent_name = agent_configs['opponent_name']\n        opponent_sample_path = os.path.join(self.log_dir, opponent_name)\n        agent_path = os.path.join(self.log_dir, agent_name)\n        self.eval_matrix_method[agent_name] = agent_configs.get('eval_matrix_method', 'reward')\n        self.evalsave_callbacks[agent_name] = []\n        self.opponent_selection_callbacks[agent_name] = []\n        for population_num in range(population_size):\n            enable_evaluation_matrix = True\n            self.evalsave_callbacks[agent_name].append(EvalSaveCallback(eval_env=self.eval_envs[agent_name][population_num], log_path=agent_path, eval_freq=int(agent_configs['eval_freq']), n_eval_episodes=agent_configs['num_eval_episodes'], deterministic=self.deterministic, save_path=agent_path, eval_metric=agent_configs['eval_metric'], eval_opponent_selection=agent_configs['eval_opponent_selection'], eval_sample_path=opponent_sample_path, save_freq=int(agent_configs['save_freq']), archive={'self': self.archives[agent_name], 'opponent': self.archives[opponent_name]}, agent_name=agent_name, num_rounds=self.experiment_configs['num_rounds'], seed_value=self.seed_value, enable_evaluation_matrix=enable_evaluation_matrix, randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False), eval_matrix_method=self.eval_matrix_method[agent_name]))\n            self.evalsave_callbacks[agent_name][-1].population_idx = population_num\n            self.opponent_selection_callbacks[agent_name].append(TrainingOpponentSelectionCallback(sample_path=opponent_sample_path, env=self.envs[agent_name][population_num], opponent_selection=agent_configs['opponent_selection'], sample_after_rollout=agent_configs['sample_after_rollout'], sample_after_reset=agent_configs['sample_after_reset'], num_sampled_per_round=agent_configs['num_sampled_opponent_per_round'], archive=self.archives[opponent_name], randomly_reseed_sampling=agent_configs.get('randomly_reseed_sampling', False)))\n        self.wandb_callbacks[agent_name] = WandbCallback()"
        ]
    },
    {
        "func_name": "_init_training",
        "original": "def _init_training(self, experiment_filename):\n    super(SelfPlayTraining, self)._init_exp(experiment_filename, True, True)\n    self.clilog.info(f'----- Initialize archives, envs, models, callbacks')\n    self._init_archives()\n    self._init_envs()\n    self._init_models()\n    self._init_callbacks()",
        "mutated": [
            "def _init_training(self, experiment_filename):\n    if False:\n        i = 10\n    super(SelfPlayTraining, self)._init_exp(experiment_filename, True, True)\n    self.clilog.info(f'----- Initialize archives, envs, models, callbacks')\n    self._init_archives()\n    self._init_envs()\n    self._init_models()\n    self._init_callbacks()",
            "def _init_training(self, experiment_filename):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(SelfPlayTraining, self)._init_exp(experiment_filename, True, True)\n    self.clilog.info(f'----- Initialize archives, envs, models, callbacks')\n    self._init_archives()\n    self._init_envs()\n    self._init_models()\n    self._init_callbacks()",
            "def _init_training(self, experiment_filename):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(SelfPlayTraining, self)._init_exp(experiment_filename, True, True)\n    self.clilog.info(f'----- Initialize archives, envs, models, callbacks')\n    self._init_archives()\n    self._init_envs()\n    self._init_models()\n    self._init_callbacks()",
            "def _init_training(self, experiment_filename):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(SelfPlayTraining, self)._init_exp(experiment_filename, True, True)\n    self.clilog.info(f'----- Initialize archives, envs, models, callbacks')\n    self._init_archives()\n    self._init_envs()\n    self._init_models()\n    self._init_callbacks()",
            "def _init_training(self, experiment_filename):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(SelfPlayTraining, self)._init_exp(experiment_filename, True, True)\n    self.clilog.info(f'----- Initialize archives, envs, models, callbacks')\n    self._init_archives()\n    self._init_envs()\n    self._init_models()\n    self._init_callbacks()"
        ]
    },
    {
        "func_name": "_create_agents_names_list",
        "original": "def _create_agents_names_list(self):\n    agents_order = self.experiment_configs['agents_order']\n    agents_names_list = [None for i in range(len(agents_order.keys()))]\n    for (k, v) in agents_order.items():\n        agents_names_list[int(k)] = v\n    return agents_names_list",
        "mutated": [
            "def _create_agents_names_list(self):\n    if False:\n        i = 10\n    agents_order = self.experiment_configs['agents_order']\n    agents_names_list = [None for i in range(len(agents_order.keys()))]\n    for (k, v) in agents_order.items():\n        agents_names_list[int(k)] = v\n    return agents_names_list",
            "def _create_agents_names_list(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    agents_order = self.experiment_configs['agents_order']\n    agents_names_list = [None for i in range(len(agents_order.keys()))]\n    for (k, v) in agents_order.items():\n        agents_names_list[int(k)] = v\n    return agents_names_list",
            "def _create_agents_names_list(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    agents_order = self.experiment_configs['agents_order']\n    agents_names_list = [None for i in range(len(agents_order.keys()))]\n    for (k, v) in agents_order.items():\n        agents_names_list[int(k)] = v\n    return agents_names_list",
            "def _create_agents_names_list(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    agents_order = self.experiment_configs['agents_order']\n    agents_names_list = [None for i in range(len(agents_order.keys()))]\n    for (k, v) in agents_order.items():\n        agents_names_list[int(k)] = v\n    return agents_names_list",
            "def _create_agents_names_list(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    agents_order = self.experiment_configs['agents_order']\n    agents_names_list = [None for i in range(len(agents_order.keys()))]\n    for (k, v) in agents_order.items():\n        agents_names_list[int(k)] = v\n    return agents_names_list"
        ]
    },
    {
        "func_name": "_change_archives",
        "original": "def _change_archives(self, agent_name, archive):\n    self.archives[agent_name].change_archive_core(archive)",
        "mutated": [
            "def _change_archives(self, agent_name, archive):\n    if False:\n        i = 10\n    self.archives[agent_name].change_archive_core(archive)",
            "def _change_archives(self, agent_name, archive):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.archives[agent_name].change_archive_core(archive)",
            "def _change_archives(self, agent_name, archive):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.archives[agent_name].change_archive_core(archive)",
            "def _change_archives(self, agent_name, archive):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.archives[agent_name].change_archive_core(archive)",
            "def _change_archives(self, agent_name, archive):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.archives[agent_name].change_archive_core(archive)"
        ]
    },
    {
        "func_name": "_population_thread_func",
        "original": "def _population_thread_func(self, agent_name, population_num):\n    self.models[agent_name][population_num].learn(total_timesteps=int(self.agents_configs[agent_name]['num_timesteps']), callback=[self.opponent_selection_callbacks[agent_name][population_num], self.evalsave_callbacks[agent_name][population_num], self.wandb_callbacks[agent_name]], reset_num_timesteps=False)",
        "mutated": [
            "def _population_thread_func(self, agent_name, population_num):\n    if False:\n        i = 10\n    self.models[agent_name][population_num].learn(total_timesteps=int(self.agents_configs[agent_name]['num_timesteps']), callback=[self.opponent_selection_callbacks[agent_name][population_num], self.evalsave_callbacks[agent_name][population_num], self.wandb_callbacks[agent_name]], reset_num_timesteps=False)",
            "def _population_thread_func(self, agent_name, population_num):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.models[agent_name][population_num].learn(total_timesteps=int(self.agents_configs[agent_name]['num_timesteps']), callback=[self.opponent_selection_callbacks[agent_name][population_num], self.evalsave_callbacks[agent_name][population_num], self.wandb_callbacks[agent_name]], reset_num_timesteps=False)",
            "def _population_thread_func(self, agent_name, population_num):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.models[agent_name][population_num].learn(total_timesteps=int(self.agents_configs[agent_name]['num_timesteps']), callback=[self.opponent_selection_callbacks[agent_name][population_num], self.evalsave_callbacks[agent_name][population_num], self.wandb_callbacks[agent_name]], reset_num_timesteps=False)",
            "def _population_thread_func(self, agent_name, population_num):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.models[agent_name][population_num].learn(total_timesteps=int(self.agents_configs[agent_name]['num_timesteps']), callback=[self.opponent_selection_callbacks[agent_name][population_num], self.evalsave_callbacks[agent_name][population_num], self.wandb_callbacks[agent_name]], reset_num_timesteps=False)",
            "def _population_thread_func(self, agent_name, population_num):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.models[agent_name][population_num].learn(total_timesteps=int(self.agents_configs[agent_name]['num_timesteps']), callback=[self.opponent_selection_callbacks[agent_name][population_num], self.evalsave_callbacks[agent_name][population_num], self.wandb_callbacks[agent_name]], reset_num_timesteps=False)"
        ]
    },
    {
        "func_name": "_agent_thread_func",
        "original": "def _agent_thread_func(self, agent_name, population_size, round_num):\n    opponent_name = self.agents_configs[agent_name]['opponent_name']\n    if self.experiment_configs.get('parallel_alternate_training', True):\n        self.archives[opponent_name].change_archive_core(self.old_archives[opponent_name])\n    threads = []\n    for population_num in range(population_size):\n        self.clilog.info(f'------------------- Train {agent_name}, round: {round_num},  population: {population_num}--------------------')\n        self.clilog.debug(f'Model mem id: {self.models[agent_name][population_num]}')\n        if self.THREADED:\n            thread = threading.Thread(target=self._population_thread_func, args=(agent_name, population_num))\n            threads.append(thread)\n            thread.start()\n        else:\n            self._population_thread_func(agent_name, population_num)\n    for (e, thread) in enumerate(threads):\n        thread.join()\n    self.new_archives[agent_name] = deepcopy(self.archives[agent_name])",
        "mutated": [
            "def _agent_thread_func(self, agent_name, population_size, round_num):\n    if False:\n        i = 10\n    opponent_name = self.agents_configs[agent_name]['opponent_name']\n    if self.experiment_configs.get('parallel_alternate_training', True):\n        self.archives[opponent_name].change_archive_core(self.old_archives[opponent_name])\n    threads = []\n    for population_num in range(population_size):\n        self.clilog.info(f'------------------- Train {agent_name}, round: {round_num},  population: {population_num}--------------------')\n        self.clilog.debug(f'Model mem id: {self.models[agent_name][population_num]}')\n        if self.THREADED:\n            thread = threading.Thread(target=self._population_thread_func, args=(agent_name, population_num))\n            threads.append(thread)\n            thread.start()\n        else:\n            self._population_thread_func(agent_name, population_num)\n    for (e, thread) in enumerate(threads):\n        thread.join()\n    self.new_archives[agent_name] = deepcopy(self.archives[agent_name])",
            "def _agent_thread_func(self, agent_name, population_size, round_num):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    opponent_name = self.agents_configs[agent_name]['opponent_name']\n    if self.experiment_configs.get('parallel_alternate_training', True):\n        self.archives[opponent_name].change_archive_core(self.old_archives[opponent_name])\n    threads = []\n    for population_num in range(population_size):\n        self.clilog.info(f'------------------- Train {agent_name}, round: {round_num},  population: {population_num}--------------------')\n        self.clilog.debug(f'Model mem id: {self.models[agent_name][population_num]}')\n        if self.THREADED:\n            thread = threading.Thread(target=self._population_thread_func, args=(agent_name, population_num))\n            threads.append(thread)\n            thread.start()\n        else:\n            self._population_thread_func(agent_name, population_num)\n    for (e, thread) in enumerate(threads):\n        thread.join()\n    self.new_archives[agent_name] = deepcopy(self.archives[agent_name])",
            "def _agent_thread_func(self, agent_name, population_size, round_num):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    opponent_name = self.agents_configs[agent_name]['opponent_name']\n    if self.experiment_configs.get('parallel_alternate_training', True):\n        self.archives[opponent_name].change_archive_core(self.old_archives[opponent_name])\n    threads = []\n    for population_num in range(population_size):\n        self.clilog.info(f'------------------- Train {agent_name}, round: {round_num},  population: {population_num}--------------------')\n        self.clilog.debug(f'Model mem id: {self.models[agent_name][population_num]}')\n        if self.THREADED:\n            thread = threading.Thread(target=self._population_thread_func, args=(agent_name, population_num))\n            threads.append(thread)\n            thread.start()\n        else:\n            self._population_thread_func(agent_name, population_num)\n    for (e, thread) in enumerate(threads):\n        thread.join()\n    self.new_archives[agent_name] = deepcopy(self.archives[agent_name])",
            "def _agent_thread_func(self, agent_name, population_size, round_num):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    opponent_name = self.agents_configs[agent_name]['opponent_name']\n    if self.experiment_configs.get('parallel_alternate_training', True):\n        self.archives[opponent_name].change_archive_core(self.old_archives[opponent_name])\n    threads = []\n    for population_num in range(population_size):\n        self.clilog.info(f'------------------- Train {agent_name}, round: {round_num},  population: {population_num}--------------------')\n        self.clilog.debug(f'Model mem id: {self.models[agent_name][population_num]}')\n        if self.THREADED:\n            thread = threading.Thread(target=self._population_thread_func, args=(agent_name, population_num))\n            threads.append(thread)\n            thread.start()\n        else:\n            self._population_thread_func(agent_name, population_num)\n    for (e, thread) in enumerate(threads):\n        thread.join()\n    self.new_archives[agent_name] = deepcopy(self.archives[agent_name])",
            "def _agent_thread_func(self, agent_name, population_size, round_num):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    opponent_name = self.agents_configs[agent_name]['opponent_name']\n    if self.experiment_configs.get('parallel_alternate_training', True):\n        self.archives[opponent_name].change_archive_core(self.old_archives[opponent_name])\n    threads = []\n    for population_num in range(population_size):\n        self.clilog.info(f'------------------- Train {agent_name}, round: {round_num},  population: {population_num}--------------------')\n        self.clilog.debug(f'Model mem id: {self.models[agent_name][population_num]}')\n        if self.THREADED:\n            thread = threading.Thread(target=self._population_thread_func, args=(agent_name, population_num))\n            threads.append(thread)\n            thread.start()\n        else:\n            self._population_thread_func(agent_name, population_num)\n    for (e, thread) in enumerate(threads):\n        thread.join()\n    self.new_archives[agent_name] = deepcopy(self.archives[agent_name])"
        ]
    },
    {
        "func_name": "train",
        "original": "def train(self, experiment_filename=None):\n    self._init_training(experiment_filename=experiment_filename)\n    num_rounds = self.experiment_configs['num_rounds']\n    population_size = self.experiment_configs['population_size']\n    agents_names_list = self._create_agents_names_list()\n    self.old_archives = {}\n    self.new_archives = {}\n    old_THREADED = deepcopy(self.THREADED)\n    self.THREADED = False\n    for round_num in range(num_rounds):\n        wandb.log({f'progress (round_num)': round_num})\n        for (i, agent_name) in enumerate(agents_names_list):\n            for population_num in range(population_size):\n                self.evalsave_callbacks[agent_name][population_num].set_name_prefix(f'history_{round_num}')\n            self.old_archives[agent_name] = deepcopy(self.archives[agent_name])\n        threads = []\n        for (agent_idx, agent_name) in enumerate(agents_names_list):\n            if self.THREADED:\n                thread = threading.Thread(target=self._agent_thread_func, args=(agent_name, population_size, round_num))\n                threads.append(thread)\n                thread.start()\n            else:\n                self._agent_thread_func(agent_name, population_size, round_num)\n        for (e, thread) in enumerate(threads):\n            thread.join()\n        if self.experiment_configs.get('parallel_alternate_training', True):\n            for agent_name in agents_names_list:\n                self.archives[agent_name].change_archive_core(self.new_archives[agent_name])\n        if old_THREADED:\n            self.THREADED = True\n        for (j, agent_name) in enumerate(agents_names_list):\n            agent_config = self.agents_configs[agent_name]\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            final_save_freq = agent_config['final_save_freq']\n            heatmap_log_freq = agent_config['heatmap_log_freq']\n            aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n            if aggregate_eval_matrix:\n                self.clilog.info('--------------------------------------------------------------')\n                self.clilog.info(f'Round: {round_num} -> Aggregate HeatMap Evaluation for current round version of {agent_name} vs {opponent_name}')\n                for population_num in range(population_size):\n                    if agent_config['rl_algorithm'] == 'PPO':\n                        algorithm_class = PPO\n                    elif agent_config['rl_algorithm'] == 'SAC':\n                        algorithm_class = SAC\n                    self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix_aggregate(prefix='history_', round_num=round_num, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name))\n            if aggregate_eval_matrix and (round_num % heatmap_log_freq == 0 or round_num == num_rounds - 1):\n                evaluation_matrices = []\n                for population_num in range(population_size):\n                    evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                    evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                    evaluation_matrices.append(evaluation_matrix)\n                mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n                std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n                if round_num == num_rounds - 1:\n                    wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=True)})\n                    wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=True)})\n                wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=False)})\n                wandb.log({f'{agent_name}/mid_eval/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=False)})\n                np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n                wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            if round_num % final_save_freq == 0 or round_num == num_rounds - 1:\n                self.clilog.info(f'------------------- Models saving freq --------------------')\n                for population_num in range(population_size):\n                    self.models[agent_name][population_num].save(os.path.join(self.log_dir, agent_name, f'final_model_pop{population_num}'))\n                self.models[agent_name][-1].save(os.path.join(self.log_dir, agent_name, 'final_model'))\n    for (j, agent_name) in enumerate(agents_names_list):\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        opponent_name = agent_config['opponent_name']\n        self.clilog.info(f'------------------- Prepare freq log used by {agent_name} ({opponent_name} archive) --------------------')\n        num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n        eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n        (freq_keys, freq_values) = self.archives[opponent_name].get_freq()\n        freq_dict = dict(zip(freq_keys, freq_values))\n        max_checkpoint_num = 1\n        for population_num in range(population_size):\n            max_checkpoint_num = max(max_checkpoint_num, self.evalsave_callbacks[opponent_name][population_num].max_checkpoint_num)\n        max_checkpoint_num = 1\n        freq_matrix = np.zeros((population_size, max_checkpoint_num * num_rounds))\n        sorted_keys = sort_steps(list(freq_keys))\n        axis = [[i for i in range(num_rounds)], [i for i in range(population_size)]]\n        for (i, key) in enumerate(sorted_keys):\n            population_num = population_key(key)\n            round_num = round_key(key)\n            checkpoint_num = checkpoint_key(key)\n            val = freq_dict[key]\n            freq_matrix[population_num, round_num] += val\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=True)})\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap_no_text': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=False)})\n        mean_freq_heatmap = np.mean(freq_matrix, axis=0)\n        std_freq_heatmap = np.std(freq_matrix, axis=0)\n        stat_freq_heatmap = np.vstack((mean_freq_heatmap, std_freq_heatmap))\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/stat_freq_heatmap': wandb.plots.HeatMap(axis[0], ['mean', 'std'], stat_freq_heatmap, show_text=True)})\n    self.evaluation_configs['log_dir'] = self.log_dir\n    self.evaluation_configs['log_main_dir'] = self.log_main_dir\n    for (j, agent_name) in enumerate(agents_names_list):\n        self.clilog.info(f' ----------- Evaluation for {agent_name} -----------')\n        if (j + 1) % 2:\n            self.clilog.warn('Note the score is inversed for length, it is not length but time elapsed')\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        if not aggregate_eval_matrix:\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n            maximize_indicator = True\n            evaluation_matrices = []\n            best_agents_population = {}\n            best_agent_search_radius = agent_config.get('best_agent_search_radius', num_rounds)\n            for population_num in range(population_size):\n                self.clilog.info(f'Full evaluation matrix for {agent_name} (population: {population_num})')\n                algorithm_class = None\n                if agent_config['rl_algorithm'] == 'PPO':\n                    algorithm_class = PPO\n                elif agent_config['rl_algorithm'] == 'SAC':\n                    algorithm_class = SAC\n                (axis, agent_names) = self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix(prefix='history_', num_rounds=num_rounds, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name), freq=eval_matrix_testing_freq, population_size=population_size, negative_indicator=(j + 1) % 2)\n                evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                evaluation_matrices.append(evaluation_matrix)\n                num_eval_rounds = len(axis[0])\n                best_agent_search_radius = min(best_agent_search_radius, num_eval_rounds)\n                mask = np.ones((num_eval_rounds, num_eval_rounds))\n                mask_initial_idx = num_eval_rounds - best_agent_search_radius\n                mask[mask_initial_idx:, :] = np.zeros((best_agent_search_radius, num_eval_rounds))\n                agent_names = np.array(agent_names)\n                eval_mask = mask if j % 2 == 0 else mask.T\n                shape = (best_agent_search_radius, num_eval_rounds)\n                masked_evaluation_matrix = evaluation_matrix[eval_mask == 0].reshape(shape)\n                masked_evaluation_matrix = masked_evaluation_matrix if j % 2 == 0 else masked_evaluation_matrix.T\n                agent_names = agent_names[mask[:, 0] == 0]\n                (best_agent_name, best_agent_score) = get_best_agent_from_eval_mat(masked_evaluation_matrix, agent_names, axis=j, maximize=maximize_indicator)\n                best_agents_population[best_agent_name] = best_agent_score\n            (best_agent_name, best_agent_score) = get_best_agent_from_vector(list(best_agents_population.values()), list(best_agents_population.keys()), maximize=maximize_indicator)\n            self.evaluation_configs[agent_name] = {'best_agent_name': best_agent_name, 'best_agent_score': best_agent_score, 'best_agent_method': self.eval_matrix_method[agent_name]}\n            self.clilog.info(f'Best agent for {agent_name} -> {best_agent_name}, score: {best_agent_score}')\n            mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n            std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n            wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=True)})\n            wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=False)})\n            wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap(axis[0], axis[1], std_evaluation_matrix, show_text=True)})\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x'), axis[0])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y'), axis[1])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y') + '.npy')\n        self.clilog.info('Save experiment configuration with ')\n        log_file = os.path.join(self.log_dir, 'experiment_config.json')\n        ExperimentParser.save(log_file, self.experiment_configs, self.agents_configs, self.evaluation_configs, self.testing_configs)\n        wandb.save(log_file)\n        post_eval_list = []\n        for population_num in range(population_size):\n            self.clilog.info('-----------------------------------------------------------------------')\n            self.clilog.info(f'Post Evaluation for {agent_name} (population: {population_num})')\n            self.clilog.info('-----------------------------------------------------------------------')\n            eval_return_list = self.evalsave_callbacks[agent_name][population_num].post_eval(opponents_path=os.path.join(self.log_dir, self.agents_configs[agent_name]['opponent_name']), population_size=population_size)\n            post_eval_list.append(eval_return_list)\n            self.envs[agent_name][population_num].close()\n            self.eval_envs[agent_name][population_num].close()\n        mean_post_eval = np.mean(post_eval_list, axis=0)\n        std_post_eval = np.std(post_eval_list, axis=0)\n        data = [[x, y] for (x, y) in zip([i for i in range(len(mean_post_eval))], mean_post_eval)]\n        table = wandb.Table(data=data, columns=['opponent idx', 'win-rate'])\n        std_data = [[x, y] for (x, y) in zip([i for i in range(len(std_post_eval))], std_post_eval)]\n        std_table = wandb.Table(data=std_data, columns=['opponent idx', 'win-rate'])\n        wandb.log({f'{agent_name}/post_eval/table': wandb.plot.line(table, 'opponent idx', 'win-rate', title=f'Post evaluation {agent_name}')})\n        wandb.log({f'{agent_name}/post_eval/std_table': wandb.plot.line(std_table, 'opponent idx', 'win-rate', title=f'Std Post evaluation {agent_name}')})",
        "mutated": [
            "def train(self, experiment_filename=None):\n    if False:\n        i = 10\n    self._init_training(experiment_filename=experiment_filename)\n    num_rounds = self.experiment_configs['num_rounds']\n    population_size = self.experiment_configs['population_size']\n    agents_names_list = self._create_agents_names_list()\n    self.old_archives = {}\n    self.new_archives = {}\n    old_THREADED = deepcopy(self.THREADED)\n    self.THREADED = False\n    for round_num in range(num_rounds):\n        wandb.log({f'progress (round_num)': round_num})\n        for (i, agent_name) in enumerate(agents_names_list):\n            for population_num in range(population_size):\n                self.evalsave_callbacks[agent_name][population_num].set_name_prefix(f'history_{round_num}')\n            self.old_archives[agent_name] = deepcopy(self.archives[agent_name])\n        threads = []\n        for (agent_idx, agent_name) in enumerate(agents_names_list):\n            if self.THREADED:\n                thread = threading.Thread(target=self._agent_thread_func, args=(agent_name, population_size, round_num))\n                threads.append(thread)\n                thread.start()\n            else:\n                self._agent_thread_func(agent_name, population_size, round_num)\n        for (e, thread) in enumerate(threads):\n            thread.join()\n        if self.experiment_configs.get('parallel_alternate_training', True):\n            for agent_name in agents_names_list:\n                self.archives[agent_name].change_archive_core(self.new_archives[agent_name])\n        if old_THREADED:\n            self.THREADED = True\n        for (j, agent_name) in enumerate(agents_names_list):\n            agent_config = self.agents_configs[agent_name]\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            final_save_freq = agent_config['final_save_freq']\n            heatmap_log_freq = agent_config['heatmap_log_freq']\n            aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n            if aggregate_eval_matrix:\n                self.clilog.info('--------------------------------------------------------------')\n                self.clilog.info(f'Round: {round_num} -> Aggregate HeatMap Evaluation for current round version of {agent_name} vs {opponent_name}')\n                for population_num in range(population_size):\n                    if agent_config['rl_algorithm'] == 'PPO':\n                        algorithm_class = PPO\n                    elif agent_config['rl_algorithm'] == 'SAC':\n                        algorithm_class = SAC\n                    self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix_aggregate(prefix='history_', round_num=round_num, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name))\n            if aggregate_eval_matrix and (round_num % heatmap_log_freq == 0 or round_num == num_rounds - 1):\n                evaluation_matrices = []\n                for population_num in range(population_size):\n                    evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                    evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                    evaluation_matrices.append(evaluation_matrix)\n                mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n                std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n                if round_num == num_rounds - 1:\n                    wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=True)})\n                    wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=True)})\n                wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=False)})\n                wandb.log({f'{agent_name}/mid_eval/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=False)})\n                np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n                wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            if round_num % final_save_freq == 0 or round_num == num_rounds - 1:\n                self.clilog.info(f'------------------- Models saving freq --------------------')\n                for population_num in range(population_size):\n                    self.models[agent_name][population_num].save(os.path.join(self.log_dir, agent_name, f'final_model_pop{population_num}'))\n                self.models[agent_name][-1].save(os.path.join(self.log_dir, agent_name, 'final_model'))\n    for (j, agent_name) in enumerate(agents_names_list):\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        opponent_name = agent_config['opponent_name']\n        self.clilog.info(f'------------------- Prepare freq log used by {agent_name} ({opponent_name} archive) --------------------')\n        num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n        eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n        (freq_keys, freq_values) = self.archives[opponent_name].get_freq()\n        freq_dict = dict(zip(freq_keys, freq_values))\n        max_checkpoint_num = 1\n        for population_num in range(population_size):\n            max_checkpoint_num = max(max_checkpoint_num, self.evalsave_callbacks[opponent_name][population_num].max_checkpoint_num)\n        max_checkpoint_num = 1\n        freq_matrix = np.zeros((population_size, max_checkpoint_num * num_rounds))\n        sorted_keys = sort_steps(list(freq_keys))\n        axis = [[i for i in range(num_rounds)], [i for i in range(population_size)]]\n        for (i, key) in enumerate(sorted_keys):\n            population_num = population_key(key)\n            round_num = round_key(key)\n            checkpoint_num = checkpoint_key(key)\n            val = freq_dict[key]\n            freq_matrix[population_num, round_num] += val\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=True)})\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap_no_text': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=False)})\n        mean_freq_heatmap = np.mean(freq_matrix, axis=0)\n        std_freq_heatmap = np.std(freq_matrix, axis=0)\n        stat_freq_heatmap = np.vstack((mean_freq_heatmap, std_freq_heatmap))\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/stat_freq_heatmap': wandb.plots.HeatMap(axis[0], ['mean', 'std'], stat_freq_heatmap, show_text=True)})\n    self.evaluation_configs['log_dir'] = self.log_dir\n    self.evaluation_configs['log_main_dir'] = self.log_main_dir\n    for (j, agent_name) in enumerate(agents_names_list):\n        self.clilog.info(f' ----------- Evaluation for {agent_name} -----------')\n        if (j + 1) % 2:\n            self.clilog.warn('Note the score is inversed for length, it is not length but time elapsed')\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        if not aggregate_eval_matrix:\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n            maximize_indicator = True\n            evaluation_matrices = []\n            best_agents_population = {}\n            best_agent_search_radius = agent_config.get('best_agent_search_radius', num_rounds)\n            for population_num in range(population_size):\n                self.clilog.info(f'Full evaluation matrix for {agent_name} (population: {population_num})')\n                algorithm_class = None\n                if agent_config['rl_algorithm'] == 'PPO':\n                    algorithm_class = PPO\n                elif agent_config['rl_algorithm'] == 'SAC':\n                    algorithm_class = SAC\n                (axis, agent_names) = self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix(prefix='history_', num_rounds=num_rounds, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name), freq=eval_matrix_testing_freq, population_size=population_size, negative_indicator=(j + 1) % 2)\n                evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                evaluation_matrices.append(evaluation_matrix)\n                num_eval_rounds = len(axis[0])\n                best_agent_search_radius = min(best_agent_search_radius, num_eval_rounds)\n                mask = np.ones((num_eval_rounds, num_eval_rounds))\n                mask_initial_idx = num_eval_rounds - best_agent_search_radius\n                mask[mask_initial_idx:, :] = np.zeros((best_agent_search_radius, num_eval_rounds))\n                agent_names = np.array(agent_names)\n                eval_mask = mask if j % 2 == 0 else mask.T\n                shape = (best_agent_search_radius, num_eval_rounds)\n                masked_evaluation_matrix = evaluation_matrix[eval_mask == 0].reshape(shape)\n                masked_evaluation_matrix = masked_evaluation_matrix if j % 2 == 0 else masked_evaluation_matrix.T\n                agent_names = agent_names[mask[:, 0] == 0]\n                (best_agent_name, best_agent_score) = get_best_agent_from_eval_mat(masked_evaluation_matrix, agent_names, axis=j, maximize=maximize_indicator)\n                best_agents_population[best_agent_name] = best_agent_score\n            (best_agent_name, best_agent_score) = get_best_agent_from_vector(list(best_agents_population.values()), list(best_agents_population.keys()), maximize=maximize_indicator)\n            self.evaluation_configs[agent_name] = {'best_agent_name': best_agent_name, 'best_agent_score': best_agent_score, 'best_agent_method': self.eval_matrix_method[agent_name]}\n            self.clilog.info(f'Best agent for {agent_name} -> {best_agent_name}, score: {best_agent_score}')\n            mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n            std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n            wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=True)})\n            wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=False)})\n            wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap(axis[0], axis[1], std_evaluation_matrix, show_text=True)})\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x'), axis[0])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y'), axis[1])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y') + '.npy')\n        self.clilog.info('Save experiment configuration with ')\n        log_file = os.path.join(self.log_dir, 'experiment_config.json')\n        ExperimentParser.save(log_file, self.experiment_configs, self.agents_configs, self.evaluation_configs, self.testing_configs)\n        wandb.save(log_file)\n        post_eval_list = []\n        for population_num in range(population_size):\n            self.clilog.info('-----------------------------------------------------------------------')\n            self.clilog.info(f'Post Evaluation for {agent_name} (population: {population_num})')\n            self.clilog.info('-----------------------------------------------------------------------')\n            eval_return_list = self.evalsave_callbacks[agent_name][population_num].post_eval(opponents_path=os.path.join(self.log_dir, self.agents_configs[agent_name]['opponent_name']), population_size=population_size)\n            post_eval_list.append(eval_return_list)\n            self.envs[agent_name][population_num].close()\n            self.eval_envs[agent_name][population_num].close()\n        mean_post_eval = np.mean(post_eval_list, axis=0)\n        std_post_eval = np.std(post_eval_list, axis=0)\n        data = [[x, y] for (x, y) in zip([i for i in range(len(mean_post_eval))], mean_post_eval)]\n        table = wandb.Table(data=data, columns=['opponent idx', 'win-rate'])\n        std_data = [[x, y] for (x, y) in zip([i for i in range(len(std_post_eval))], std_post_eval)]\n        std_table = wandb.Table(data=std_data, columns=['opponent idx', 'win-rate'])\n        wandb.log({f'{agent_name}/post_eval/table': wandb.plot.line(table, 'opponent idx', 'win-rate', title=f'Post evaluation {agent_name}')})\n        wandb.log({f'{agent_name}/post_eval/std_table': wandb.plot.line(std_table, 'opponent idx', 'win-rate', title=f'Std Post evaluation {agent_name}')})",
            "def train(self, experiment_filename=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self._init_training(experiment_filename=experiment_filename)\n    num_rounds = self.experiment_configs['num_rounds']\n    population_size = self.experiment_configs['population_size']\n    agents_names_list = self._create_agents_names_list()\n    self.old_archives = {}\n    self.new_archives = {}\n    old_THREADED = deepcopy(self.THREADED)\n    self.THREADED = False\n    for round_num in range(num_rounds):\n        wandb.log({f'progress (round_num)': round_num})\n        for (i, agent_name) in enumerate(agents_names_list):\n            for population_num in range(population_size):\n                self.evalsave_callbacks[agent_name][population_num].set_name_prefix(f'history_{round_num}')\n            self.old_archives[agent_name] = deepcopy(self.archives[agent_name])\n        threads = []\n        for (agent_idx, agent_name) in enumerate(agents_names_list):\n            if self.THREADED:\n                thread = threading.Thread(target=self._agent_thread_func, args=(agent_name, population_size, round_num))\n                threads.append(thread)\n                thread.start()\n            else:\n                self._agent_thread_func(agent_name, population_size, round_num)\n        for (e, thread) in enumerate(threads):\n            thread.join()\n        if self.experiment_configs.get('parallel_alternate_training', True):\n            for agent_name in agents_names_list:\n                self.archives[agent_name].change_archive_core(self.new_archives[agent_name])\n        if old_THREADED:\n            self.THREADED = True\n        for (j, agent_name) in enumerate(agents_names_list):\n            agent_config = self.agents_configs[agent_name]\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            final_save_freq = agent_config['final_save_freq']\n            heatmap_log_freq = agent_config['heatmap_log_freq']\n            aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n            if aggregate_eval_matrix:\n                self.clilog.info('--------------------------------------------------------------')\n                self.clilog.info(f'Round: {round_num} -> Aggregate HeatMap Evaluation for current round version of {agent_name} vs {opponent_name}')\n                for population_num in range(population_size):\n                    if agent_config['rl_algorithm'] == 'PPO':\n                        algorithm_class = PPO\n                    elif agent_config['rl_algorithm'] == 'SAC':\n                        algorithm_class = SAC\n                    self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix_aggregate(prefix='history_', round_num=round_num, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name))\n            if aggregate_eval_matrix and (round_num % heatmap_log_freq == 0 or round_num == num_rounds - 1):\n                evaluation_matrices = []\n                for population_num in range(population_size):\n                    evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                    evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                    evaluation_matrices.append(evaluation_matrix)\n                mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n                std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n                if round_num == num_rounds - 1:\n                    wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=True)})\n                    wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=True)})\n                wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=False)})\n                wandb.log({f'{agent_name}/mid_eval/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=False)})\n                np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n                wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            if round_num % final_save_freq == 0 or round_num == num_rounds - 1:\n                self.clilog.info(f'------------------- Models saving freq --------------------')\n                for population_num in range(population_size):\n                    self.models[agent_name][population_num].save(os.path.join(self.log_dir, agent_name, f'final_model_pop{population_num}'))\n                self.models[agent_name][-1].save(os.path.join(self.log_dir, agent_name, 'final_model'))\n    for (j, agent_name) in enumerate(agents_names_list):\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        opponent_name = agent_config['opponent_name']\n        self.clilog.info(f'------------------- Prepare freq log used by {agent_name} ({opponent_name} archive) --------------------')\n        num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n        eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n        (freq_keys, freq_values) = self.archives[opponent_name].get_freq()\n        freq_dict = dict(zip(freq_keys, freq_values))\n        max_checkpoint_num = 1\n        for population_num in range(population_size):\n            max_checkpoint_num = max(max_checkpoint_num, self.evalsave_callbacks[opponent_name][population_num].max_checkpoint_num)\n        max_checkpoint_num = 1\n        freq_matrix = np.zeros((population_size, max_checkpoint_num * num_rounds))\n        sorted_keys = sort_steps(list(freq_keys))\n        axis = [[i for i in range(num_rounds)], [i for i in range(population_size)]]\n        for (i, key) in enumerate(sorted_keys):\n            population_num = population_key(key)\n            round_num = round_key(key)\n            checkpoint_num = checkpoint_key(key)\n            val = freq_dict[key]\n            freq_matrix[population_num, round_num] += val\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=True)})\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap_no_text': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=False)})\n        mean_freq_heatmap = np.mean(freq_matrix, axis=0)\n        std_freq_heatmap = np.std(freq_matrix, axis=0)\n        stat_freq_heatmap = np.vstack((mean_freq_heatmap, std_freq_heatmap))\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/stat_freq_heatmap': wandb.plots.HeatMap(axis[0], ['mean', 'std'], stat_freq_heatmap, show_text=True)})\n    self.evaluation_configs['log_dir'] = self.log_dir\n    self.evaluation_configs['log_main_dir'] = self.log_main_dir\n    for (j, agent_name) in enumerate(agents_names_list):\n        self.clilog.info(f' ----------- Evaluation for {agent_name} -----------')\n        if (j + 1) % 2:\n            self.clilog.warn('Note the score is inversed for length, it is not length but time elapsed')\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        if not aggregate_eval_matrix:\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n            maximize_indicator = True\n            evaluation_matrices = []\n            best_agents_population = {}\n            best_agent_search_radius = agent_config.get('best_agent_search_radius', num_rounds)\n            for population_num in range(population_size):\n                self.clilog.info(f'Full evaluation matrix for {agent_name} (population: {population_num})')\n                algorithm_class = None\n                if agent_config['rl_algorithm'] == 'PPO':\n                    algorithm_class = PPO\n                elif agent_config['rl_algorithm'] == 'SAC':\n                    algorithm_class = SAC\n                (axis, agent_names) = self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix(prefix='history_', num_rounds=num_rounds, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name), freq=eval_matrix_testing_freq, population_size=population_size, negative_indicator=(j + 1) % 2)\n                evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                evaluation_matrices.append(evaluation_matrix)\n                num_eval_rounds = len(axis[0])\n                best_agent_search_radius = min(best_agent_search_radius, num_eval_rounds)\n                mask = np.ones((num_eval_rounds, num_eval_rounds))\n                mask_initial_idx = num_eval_rounds - best_agent_search_radius\n                mask[mask_initial_idx:, :] = np.zeros((best_agent_search_radius, num_eval_rounds))\n                agent_names = np.array(agent_names)\n                eval_mask = mask if j % 2 == 0 else mask.T\n                shape = (best_agent_search_radius, num_eval_rounds)\n                masked_evaluation_matrix = evaluation_matrix[eval_mask == 0].reshape(shape)\n                masked_evaluation_matrix = masked_evaluation_matrix if j % 2 == 0 else masked_evaluation_matrix.T\n                agent_names = agent_names[mask[:, 0] == 0]\n                (best_agent_name, best_agent_score) = get_best_agent_from_eval_mat(masked_evaluation_matrix, agent_names, axis=j, maximize=maximize_indicator)\n                best_agents_population[best_agent_name] = best_agent_score\n            (best_agent_name, best_agent_score) = get_best_agent_from_vector(list(best_agents_population.values()), list(best_agents_population.keys()), maximize=maximize_indicator)\n            self.evaluation_configs[agent_name] = {'best_agent_name': best_agent_name, 'best_agent_score': best_agent_score, 'best_agent_method': self.eval_matrix_method[agent_name]}\n            self.clilog.info(f'Best agent for {agent_name} -> {best_agent_name}, score: {best_agent_score}')\n            mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n            std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n            wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=True)})\n            wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=False)})\n            wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap(axis[0], axis[1], std_evaluation_matrix, show_text=True)})\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x'), axis[0])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y'), axis[1])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y') + '.npy')\n        self.clilog.info('Save experiment configuration with ')\n        log_file = os.path.join(self.log_dir, 'experiment_config.json')\n        ExperimentParser.save(log_file, self.experiment_configs, self.agents_configs, self.evaluation_configs, self.testing_configs)\n        wandb.save(log_file)\n        post_eval_list = []\n        for population_num in range(population_size):\n            self.clilog.info('-----------------------------------------------------------------------')\n            self.clilog.info(f'Post Evaluation for {agent_name} (population: {population_num})')\n            self.clilog.info('-----------------------------------------------------------------------')\n            eval_return_list = self.evalsave_callbacks[agent_name][population_num].post_eval(opponents_path=os.path.join(self.log_dir, self.agents_configs[agent_name]['opponent_name']), population_size=population_size)\n            post_eval_list.append(eval_return_list)\n            self.envs[agent_name][population_num].close()\n            self.eval_envs[agent_name][population_num].close()\n        mean_post_eval = np.mean(post_eval_list, axis=0)\n        std_post_eval = np.std(post_eval_list, axis=0)\n        data = [[x, y] for (x, y) in zip([i for i in range(len(mean_post_eval))], mean_post_eval)]\n        table = wandb.Table(data=data, columns=['opponent idx', 'win-rate'])\n        std_data = [[x, y] for (x, y) in zip([i for i in range(len(std_post_eval))], std_post_eval)]\n        std_table = wandb.Table(data=std_data, columns=['opponent idx', 'win-rate'])\n        wandb.log({f'{agent_name}/post_eval/table': wandb.plot.line(table, 'opponent idx', 'win-rate', title=f'Post evaluation {agent_name}')})\n        wandb.log({f'{agent_name}/post_eval/std_table': wandb.plot.line(std_table, 'opponent idx', 'win-rate', title=f'Std Post evaluation {agent_name}')})",
            "def train(self, experiment_filename=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self._init_training(experiment_filename=experiment_filename)\n    num_rounds = self.experiment_configs['num_rounds']\n    population_size = self.experiment_configs['population_size']\n    agents_names_list = self._create_agents_names_list()\n    self.old_archives = {}\n    self.new_archives = {}\n    old_THREADED = deepcopy(self.THREADED)\n    self.THREADED = False\n    for round_num in range(num_rounds):\n        wandb.log({f'progress (round_num)': round_num})\n        for (i, agent_name) in enumerate(agents_names_list):\n            for population_num in range(population_size):\n                self.evalsave_callbacks[agent_name][population_num].set_name_prefix(f'history_{round_num}')\n            self.old_archives[agent_name] = deepcopy(self.archives[agent_name])\n        threads = []\n        for (agent_idx, agent_name) in enumerate(agents_names_list):\n            if self.THREADED:\n                thread = threading.Thread(target=self._agent_thread_func, args=(agent_name, population_size, round_num))\n                threads.append(thread)\n                thread.start()\n            else:\n                self._agent_thread_func(agent_name, population_size, round_num)\n        for (e, thread) in enumerate(threads):\n            thread.join()\n        if self.experiment_configs.get('parallel_alternate_training', True):\n            for agent_name in agents_names_list:\n                self.archives[agent_name].change_archive_core(self.new_archives[agent_name])\n        if old_THREADED:\n            self.THREADED = True\n        for (j, agent_name) in enumerate(agents_names_list):\n            agent_config = self.agents_configs[agent_name]\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            final_save_freq = agent_config['final_save_freq']\n            heatmap_log_freq = agent_config['heatmap_log_freq']\n            aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n            if aggregate_eval_matrix:\n                self.clilog.info('--------------------------------------------------------------')\n                self.clilog.info(f'Round: {round_num} -> Aggregate HeatMap Evaluation for current round version of {agent_name} vs {opponent_name}')\n                for population_num in range(population_size):\n                    if agent_config['rl_algorithm'] == 'PPO':\n                        algorithm_class = PPO\n                    elif agent_config['rl_algorithm'] == 'SAC':\n                        algorithm_class = SAC\n                    self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix_aggregate(prefix='history_', round_num=round_num, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name))\n            if aggregate_eval_matrix and (round_num % heatmap_log_freq == 0 or round_num == num_rounds - 1):\n                evaluation_matrices = []\n                for population_num in range(population_size):\n                    evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                    evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                    evaluation_matrices.append(evaluation_matrix)\n                mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n                std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n                if round_num == num_rounds - 1:\n                    wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=True)})\n                    wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=True)})\n                wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=False)})\n                wandb.log({f'{agent_name}/mid_eval/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=False)})\n                np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n                wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            if round_num % final_save_freq == 0 or round_num == num_rounds - 1:\n                self.clilog.info(f'------------------- Models saving freq --------------------')\n                for population_num in range(population_size):\n                    self.models[agent_name][population_num].save(os.path.join(self.log_dir, agent_name, f'final_model_pop{population_num}'))\n                self.models[agent_name][-1].save(os.path.join(self.log_dir, agent_name, 'final_model'))\n    for (j, agent_name) in enumerate(agents_names_list):\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        opponent_name = agent_config['opponent_name']\n        self.clilog.info(f'------------------- Prepare freq log used by {agent_name} ({opponent_name} archive) --------------------')\n        num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n        eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n        (freq_keys, freq_values) = self.archives[opponent_name].get_freq()\n        freq_dict = dict(zip(freq_keys, freq_values))\n        max_checkpoint_num = 1\n        for population_num in range(population_size):\n            max_checkpoint_num = max(max_checkpoint_num, self.evalsave_callbacks[opponent_name][population_num].max_checkpoint_num)\n        max_checkpoint_num = 1\n        freq_matrix = np.zeros((population_size, max_checkpoint_num * num_rounds))\n        sorted_keys = sort_steps(list(freq_keys))\n        axis = [[i for i in range(num_rounds)], [i for i in range(population_size)]]\n        for (i, key) in enumerate(sorted_keys):\n            population_num = population_key(key)\n            round_num = round_key(key)\n            checkpoint_num = checkpoint_key(key)\n            val = freq_dict[key]\n            freq_matrix[population_num, round_num] += val\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=True)})\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap_no_text': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=False)})\n        mean_freq_heatmap = np.mean(freq_matrix, axis=0)\n        std_freq_heatmap = np.std(freq_matrix, axis=0)\n        stat_freq_heatmap = np.vstack((mean_freq_heatmap, std_freq_heatmap))\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/stat_freq_heatmap': wandb.plots.HeatMap(axis[0], ['mean', 'std'], stat_freq_heatmap, show_text=True)})\n    self.evaluation_configs['log_dir'] = self.log_dir\n    self.evaluation_configs['log_main_dir'] = self.log_main_dir\n    for (j, agent_name) in enumerate(agents_names_list):\n        self.clilog.info(f' ----------- Evaluation for {agent_name} -----------')\n        if (j + 1) % 2:\n            self.clilog.warn('Note the score is inversed for length, it is not length but time elapsed')\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        if not aggregate_eval_matrix:\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n            maximize_indicator = True\n            evaluation_matrices = []\n            best_agents_population = {}\n            best_agent_search_radius = agent_config.get('best_agent_search_radius', num_rounds)\n            for population_num in range(population_size):\n                self.clilog.info(f'Full evaluation matrix for {agent_name} (population: {population_num})')\n                algorithm_class = None\n                if agent_config['rl_algorithm'] == 'PPO':\n                    algorithm_class = PPO\n                elif agent_config['rl_algorithm'] == 'SAC':\n                    algorithm_class = SAC\n                (axis, agent_names) = self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix(prefix='history_', num_rounds=num_rounds, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name), freq=eval_matrix_testing_freq, population_size=population_size, negative_indicator=(j + 1) % 2)\n                evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                evaluation_matrices.append(evaluation_matrix)\n                num_eval_rounds = len(axis[0])\n                best_agent_search_radius = min(best_agent_search_radius, num_eval_rounds)\n                mask = np.ones((num_eval_rounds, num_eval_rounds))\n                mask_initial_idx = num_eval_rounds - best_agent_search_radius\n                mask[mask_initial_idx:, :] = np.zeros((best_agent_search_radius, num_eval_rounds))\n                agent_names = np.array(agent_names)\n                eval_mask = mask if j % 2 == 0 else mask.T\n                shape = (best_agent_search_radius, num_eval_rounds)\n                masked_evaluation_matrix = evaluation_matrix[eval_mask == 0].reshape(shape)\n                masked_evaluation_matrix = masked_evaluation_matrix if j % 2 == 0 else masked_evaluation_matrix.T\n                agent_names = agent_names[mask[:, 0] == 0]\n                (best_agent_name, best_agent_score) = get_best_agent_from_eval_mat(masked_evaluation_matrix, agent_names, axis=j, maximize=maximize_indicator)\n                best_agents_population[best_agent_name] = best_agent_score\n            (best_agent_name, best_agent_score) = get_best_agent_from_vector(list(best_agents_population.values()), list(best_agents_population.keys()), maximize=maximize_indicator)\n            self.evaluation_configs[agent_name] = {'best_agent_name': best_agent_name, 'best_agent_score': best_agent_score, 'best_agent_method': self.eval_matrix_method[agent_name]}\n            self.clilog.info(f'Best agent for {agent_name} -> {best_agent_name}, score: {best_agent_score}')\n            mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n            std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n            wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=True)})\n            wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=False)})\n            wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap(axis[0], axis[1], std_evaluation_matrix, show_text=True)})\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x'), axis[0])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y'), axis[1])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y') + '.npy')\n        self.clilog.info('Save experiment configuration with ')\n        log_file = os.path.join(self.log_dir, 'experiment_config.json')\n        ExperimentParser.save(log_file, self.experiment_configs, self.agents_configs, self.evaluation_configs, self.testing_configs)\n        wandb.save(log_file)\n        post_eval_list = []\n        for population_num in range(population_size):\n            self.clilog.info('-----------------------------------------------------------------------')\n            self.clilog.info(f'Post Evaluation for {agent_name} (population: {population_num})')\n            self.clilog.info('-----------------------------------------------------------------------')\n            eval_return_list = self.evalsave_callbacks[agent_name][population_num].post_eval(opponents_path=os.path.join(self.log_dir, self.agents_configs[agent_name]['opponent_name']), population_size=population_size)\n            post_eval_list.append(eval_return_list)\n            self.envs[agent_name][population_num].close()\n            self.eval_envs[agent_name][population_num].close()\n        mean_post_eval = np.mean(post_eval_list, axis=0)\n        std_post_eval = np.std(post_eval_list, axis=0)\n        data = [[x, y] for (x, y) in zip([i for i in range(len(mean_post_eval))], mean_post_eval)]\n        table = wandb.Table(data=data, columns=['opponent idx', 'win-rate'])\n        std_data = [[x, y] for (x, y) in zip([i for i in range(len(std_post_eval))], std_post_eval)]\n        std_table = wandb.Table(data=std_data, columns=['opponent idx', 'win-rate'])\n        wandb.log({f'{agent_name}/post_eval/table': wandb.plot.line(table, 'opponent idx', 'win-rate', title=f'Post evaluation {agent_name}')})\n        wandb.log({f'{agent_name}/post_eval/std_table': wandb.plot.line(std_table, 'opponent idx', 'win-rate', title=f'Std Post evaluation {agent_name}')})",
            "def train(self, experiment_filename=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self._init_training(experiment_filename=experiment_filename)\n    num_rounds = self.experiment_configs['num_rounds']\n    population_size = self.experiment_configs['population_size']\n    agents_names_list = self._create_agents_names_list()\n    self.old_archives = {}\n    self.new_archives = {}\n    old_THREADED = deepcopy(self.THREADED)\n    self.THREADED = False\n    for round_num in range(num_rounds):\n        wandb.log({f'progress (round_num)': round_num})\n        for (i, agent_name) in enumerate(agents_names_list):\n            for population_num in range(population_size):\n                self.evalsave_callbacks[agent_name][population_num].set_name_prefix(f'history_{round_num}')\n            self.old_archives[agent_name] = deepcopy(self.archives[agent_name])\n        threads = []\n        for (agent_idx, agent_name) in enumerate(agents_names_list):\n            if self.THREADED:\n                thread = threading.Thread(target=self._agent_thread_func, args=(agent_name, population_size, round_num))\n                threads.append(thread)\n                thread.start()\n            else:\n                self._agent_thread_func(agent_name, population_size, round_num)\n        for (e, thread) in enumerate(threads):\n            thread.join()\n        if self.experiment_configs.get('parallel_alternate_training', True):\n            for agent_name in agents_names_list:\n                self.archives[agent_name].change_archive_core(self.new_archives[agent_name])\n        if old_THREADED:\n            self.THREADED = True\n        for (j, agent_name) in enumerate(agents_names_list):\n            agent_config = self.agents_configs[agent_name]\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            final_save_freq = agent_config['final_save_freq']\n            heatmap_log_freq = agent_config['heatmap_log_freq']\n            aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n            if aggregate_eval_matrix:\n                self.clilog.info('--------------------------------------------------------------')\n                self.clilog.info(f'Round: {round_num} -> Aggregate HeatMap Evaluation for current round version of {agent_name} vs {opponent_name}')\n                for population_num in range(population_size):\n                    if agent_config['rl_algorithm'] == 'PPO':\n                        algorithm_class = PPO\n                    elif agent_config['rl_algorithm'] == 'SAC':\n                        algorithm_class = SAC\n                    self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix_aggregate(prefix='history_', round_num=round_num, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name))\n            if aggregate_eval_matrix and (round_num % heatmap_log_freq == 0 or round_num == num_rounds - 1):\n                evaluation_matrices = []\n                for population_num in range(population_size):\n                    evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                    evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                    evaluation_matrices.append(evaluation_matrix)\n                mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n                std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n                if round_num == num_rounds - 1:\n                    wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=True)})\n                    wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=True)})\n                wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=False)})\n                wandb.log({f'{agent_name}/mid_eval/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=False)})\n                np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n                wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            if round_num % final_save_freq == 0 or round_num == num_rounds - 1:\n                self.clilog.info(f'------------------- Models saving freq --------------------')\n                for population_num in range(population_size):\n                    self.models[agent_name][population_num].save(os.path.join(self.log_dir, agent_name, f'final_model_pop{population_num}'))\n                self.models[agent_name][-1].save(os.path.join(self.log_dir, agent_name, 'final_model'))\n    for (j, agent_name) in enumerate(agents_names_list):\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        opponent_name = agent_config['opponent_name']\n        self.clilog.info(f'------------------- Prepare freq log used by {agent_name} ({opponent_name} archive) --------------------')\n        num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n        eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n        (freq_keys, freq_values) = self.archives[opponent_name].get_freq()\n        freq_dict = dict(zip(freq_keys, freq_values))\n        max_checkpoint_num = 1\n        for population_num in range(population_size):\n            max_checkpoint_num = max(max_checkpoint_num, self.evalsave_callbacks[opponent_name][population_num].max_checkpoint_num)\n        max_checkpoint_num = 1\n        freq_matrix = np.zeros((population_size, max_checkpoint_num * num_rounds))\n        sorted_keys = sort_steps(list(freq_keys))\n        axis = [[i for i in range(num_rounds)], [i for i in range(population_size)]]\n        for (i, key) in enumerate(sorted_keys):\n            population_num = population_key(key)\n            round_num = round_key(key)\n            checkpoint_num = checkpoint_key(key)\n            val = freq_dict[key]\n            freq_matrix[population_num, round_num] += val\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=True)})\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap_no_text': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=False)})\n        mean_freq_heatmap = np.mean(freq_matrix, axis=0)\n        std_freq_heatmap = np.std(freq_matrix, axis=0)\n        stat_freq_heatmap = np.vstack((mean_freq_heatmap, std_freq_heatmap))\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/stat_freq_heatmap': wandb.plots.HeatMap(axis[0], ['mean', 'std'], stat_freq_heatmap, show_text=True)})\n    self.evaluation_configs['log_dir'] = self.log_dir\n    self.evaluation_configs['log_main_dir'] = self.log_main_dir\n    for (j, agent_name) in enumerate(agents_names_list):\n        self.clilog.info(f' ----------- Evaluation for {agent_name} -----------')\n        if (j + 1) % 2:\n            self.clilog.warn('Note the score is inversed for length, it is not length but time elapsed')\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        if not aggregate_eval_matrix:\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n            maximize_indicator = True\n            evaluation_matrices = []\n            best_agents_population = {}\n            best_agent_search_radius = agent_config.get('best_agent_search_radius', num_rounds)\n            for population_num in range(population_size):\n                self.clilog.info(f'Full evaluation matrix for {agent_name} (population: {population_num})')\n                algorithm_class = None\n                if agent_config['rl_algorithm'] == 'PPO':\n                    algorithm_class = PPO\n                elif agent_config['rl_algorithm'] == 'SAC':\n                    algorithm_class = SAC\n                (axis, agent_names) = self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix(prefix='history_', num_rounds=num_rounds, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name), freq=eval_matrix_testing_freq, population_size=population_size, negative_indicator=(j + 1) % 2)\n                evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                evaluation_matrices.append(evaluation_matrix)\n                num_eval_rounds = len(axis[0])\n                best_agent_search_radius = min(best_agent_search_radius, num_eval_rounds)\n                mask = np.ones((num_eval_rounds, num_eval_rounds))\n                mask_initial_idx = num_eval_rounds - best_agent_search_radius\n                mask[mask_initial_idx:, :] = np.zeros((best_agent_search_radius, num_eval_rounds))\n                agent_names = np.array(agent_names)\n                eval_mask = mask if j % 2 == 0 else mask.T\n                shape = (best_agent_search_radius, num_eval_rounds)\n                masked_evaluation_matrix = evaluation_matrix[eval_mask == 0].reshape(shape)\n                masked_evaluation_matrix = masked_evaluation_matrix if j % 2 == 0 else masked_evaluation_matrix.T\n                agent_names = agent_names[mask[:, 0] == 0]\n                (best_agent_name, best_agent_score) = get_best_agent_from_eval_mat(masked_evaluation_matrix, agent_names, axis=j, maximize=maximize_indicator)\n                best_agents_population[best_agent_name] = best_agent_score\n            (best_agent_name, best_agent_score) = get_best_agent_from_vector(list(best_agents_population.values()), list(best_agents_population.keys()), maximize=maximize_indicator)\n            self.evaluation_configs[agent_name] = {'best_agent_name': best_agent_name, 'best_agent_score': best_agent_score, 'best_agent_method': self.eval_matrix_method[agent_name]}\n            self.clilog.info(f'Best agent for {agent_name} -> {best_agent_name}, score: {best_agent_score}')\n            mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n            std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n            wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=True)})\n            wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=False)})\n            wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap(axis[0], axis[1], std_evaluation_matrix, show_text=True)})\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x'), axis[0])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y'), axis[1])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y') + '.npy')\n        self.clilog.info('Save experiment configuration with ')\n        log_file = os.path.join(self.log_dir, 'experiment_config.json')\n        ExperimentParser.save(log_file, self.experiment_configs, self.agents_configs, self.evaluation_configs, self.testing_configs)\n        wandb.save(log_file)\n        post_eval_list = []\n        for population_num in range(population_size):\n            self.clilog.info('-----------------------------------------------------------------------')\n            self.clilog.info(f'Post Evaluation for {agent_name} (population: {population_num})')\n            self.clilog.info('-----------------------------------------------------------------------')\n            eval_return_list = self.evalsave_callbacks[agent_name][population_num].post_eval(opponents_path=os.path.join(self.log_dir, self.agents_configs[agent_name]['opponent_name']), population_size=population_size)\n            post_eval_list.append(eval_return_list)\n            self.envs[agent_name][population_num].close()\n            self.eval_envs[agent_name][population_num].close()\n        mean_post_eval = np.mean(post_eval_list, axis=0)\n        std_post_eval = np.std(post_eval_list, axis=0)\n        data = [[x, y] for (x, y) in zip([i for i in range(len(mean_post_eval))], mean_post_eval)]\n        table = wandb.Table(data=data, columns=['opponent idx', 'win-rate'])\n        std_data = [[x, y] for (x, y) in zip([i for i in range(len(std_post_eval))], std_post_eval)]\n        std_table = wandb.Table(data=std_data, columns=['opponent idx', 'win-rate'])\n        wandb.log({f'{agent_name}/post_eval/table': wandb.plot.line(table, 'opponent idx', 'win-rate', title=f'Post evaluation {agent_name}')})\n        wandb.log({f'{agent_name}/post_eval/std_table': wandb.plot.line(std_table, 'opponent idx', 'win-rate', title=f'Std Post evaluation {agent_name}')})",
            "def train(self, experiment_filename=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self._init_training(experiment_filename=experiment_filename)\n    num_rounds = self.experiment_configs['num_rounds']\n    population_size = self.experiment_configs['population_size']\n    agents_names_list = self._create_agents_names_list()\n    self.old_archives = {}\n    self.new_archives = {}\n    old_THREADED = deepcopy(self.THREADED)\n    self.THREADED = False\n    for round_num in range(num_rounds):\n        wandb.log({f'progress (round_num)': round_num})\n        for (i, agent_name) in enumerate(agents_names_list):\n            for population_num in range(population_size):\n                self.evalsave_callbacks[agent_name][population_num].set_name_prefix(f'history_{round_num}')\n            self.old_archives[agent_name] = deepcopy(self.archives[agent_name])\n        threads = []\n        for (agent_idx, agent_name) in enumerate(agents_names_list):\n            if self.THREADED:\n                thread = threading.Thread(target=self._agent_thread_func, args=(agent_name, population_size, round_num))\n                threads.append(thread)\n                thread.start()\n            else:\n                self._agent_thread_func(agent_name, population_size, round_num)\n        for (e, thread) in enumerate(threads):\n            thread.join()\n        if self.experiment_configs.get('parallel_alternate_training', True):\n            for agent_name in agents_names_list:\n                self.archives[agent_name].change_archive_core(self.new_archives[agent_name])\n        if old_THREADED:\n            self.THREADED = True\n        for (j, agent_name) in enumerate(agents_names_list):\n            agent_config = self.agents_configs[agent_name]\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            final_save_freq = agent_config['final_save_freq']\n            heatmap_log_freq = agent_config['heatmap_log_freq']\n            aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n            if aggregate_eval_matrix:\n                self.clilog.info('--------------------------------------------------------------')\n                self.clilog.info(f'Round: {round_num} -> Aggregate HeatMap Evaluation for current round version of {agent_name} vs {opponent_name}')\n                for population_num in range(population_size):\n                    if agent_config['rl_algorithm'] == 'PPO':\n                        algorithm_class = PPO\n                    elif agent_config['rl_algorithm'] == 'SAC':\n                        algorithm_class = SAC\n                    self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix_aggregate(prefix='history_', round_num=round_num, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name))\n            if aggregate_eval_matrix and (round_num % heatmap_log_freq == 0 or round_num == num_rounds - 1):\n                evaluation_matrices = []\n                for population_num in range(population_size):\n                    evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                    evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                    evaluation_matrices.append(evaluation_matrix)\n                mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n                std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n                if round_num == num_rounds - 1:\n                    wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=True)})\n                    wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=True)})\n                wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], mean_evaluation_matrix, show_text=False)})\n                wandb.log({f'{agent_name}/mid_eval/std_heatmap': wandb.plots.HeatMap([i for i in range(num_rounds)], [i for i in range(num_rounds)], std_evaluation_matrix, show_text=False)})\n                np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n                wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            if round_num % final_save_freq == 0 or round_num == num_rounds - 1:\n                self.clilog.info(f'------------------- Models saving freq --------------------')\n                for population_num in range(population_size):\n                    self.models[agent_name][population_num].save(os.path.join(self.log_dir, agent_name, f'final_model_pop{population_num}'))\n                self.models[agent_name][-1].save(os.path.join(self.log_dir, agent_name, 'final_model'))\n    for (j, agent_name) in enumerate(agents_names_list):\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        opponent_name = agent_config['opponent_name']\n        self.clilog.info(f'------------------- Prepare freq log used by {agent_name} ({opponent_name} archive) --------------------')\n        num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n        eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n        (freq_keys, freq_values) = self.archives[opponent_name].get_freq()\n        freq_dict = dict(zip(freq_keys, freq_values))\n        max_checkpoint_num = 1\n        for population_num in range(population_size):\n            max_checkpoint_num = max(max_checkpoint_num, self.evalsave_callbacks[opponent_name][population_num].max_checkpoint_num)\n        max_checkpoint_num = 1\n        freq_matrix = np.zeros((population_size, max_checkpoint_num * num_rounds))\n        sorted_keys = sort_steps(list(freq_keys))\n        axis = [[i for i in range(num_rounds)], [i for i in range(population_size)]]\n        for (i, key) in enumerate(sorted_keys):\n            population_num = population_key(key)\n            round_num = round_key(key)\n            checkpoint_num = checkpoint_key(key)\n            val = freq_dict[key]\n            freq_matrix[population_num, round_num] += val\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=True)})\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/freq_heatmap_no_text': wandb.plots.HeatMap(axis[0], axis[1], freq_matrix, show_text=False)})\n        mean_freq_heatmap = np.mean(freq_matrix, axis=0)\n        std_freq_heatmap = np.std(freq_matrix, axis=0)\n        stat_freq_heatmap = np.vstack((mean_freq_heatmap, std_freq_heatmap))\n        wandb.log({f'{agent_name}vs({opponent_name}_archive)/stat_freq_heatmap': wandb.plots.HeatMap(axis[0], ['mean', 'std'], stat_freq_heatmap, show_text=True)})\n    self.evaluation_configs['log_dir'] = self.log_dir\n    self.evaluation_configs['log_main_dir'] = self.log_main_dir\n    for (j, agent_name) in enumerate(agents_names_list):\n        self.clilog.info(f' ----------- Evaluation for {agent_name} -----------')\n        if (j + 1) % 2:\n            self.clilog.warn('Note the score is inversed for length, it is not length but time elapsed')\n        agent_config = self.agents_configs[agent_name]\n        aggregate_eval_matrix = agent_config['aggregate_eval_matrix']\n        if not aggregate_eval_matrix:\n            opponent_name = agent_config['opponent_name']\n            num_heatmap_eval_episodes = agent_config['num_heatmap_eval_episodes']\n            eval_matrix_testing_freq = agent_config['eval_matrix_testing_freq']\n            maximize_indicator = True\n            evaluation_matrices = []\n            best_agents_population = {}\n            best_agent_search_radius = agent_config.get('best_agent_search_radius', num_rounds)\n            for population_num in range(population_size):\n                self.clilog.info(f'Full evaluation matrix for {agent_name} (population: {population_num})')\n                algorithm_class = None\n                if agent_config['rl_algorithm'] == 'PPO':\n                    algorithm_class = PPO\n                elif agent_config['rl_algorithm'] == 'SAC':\n                    algorithm_class = SAC\n                (axis, agent_names) = self.evalsave_callbacks[agent_name][population_num].compute_eval_matrix(prefix='history_', num_rounds=num_rounds, n_eval_rep=num_heatmap_eval_episodes, algorithm_class=algorithm_class, opponents_path=os.path.join(self.log_dir, opponent_name), agents_path=os.path.join(self.log_dir, agent_name), freq=eval_matrix_testing_freq, population_size=population_size, negative_indicator=(j + 1) % 2)\n                evaluation_matrix = self.evalsave_callbacks[agent_name][population_num].evaluation_matrix\n                evaluation_matrix = evaluation_matrix if j % 2 == 0 else evaluation_matrix.T\n                evaluation_matrices.append(evaluation_matrix)\n                num_eval_rounds = len(axis[0])\n                best_agent_search_radius = min(best_agent_search_radius, num_eval_rounds)\n                mask = np.ones((num_eval_rounds, num_eval_rounds))\n                mask_initial_idx = num_eval_rounds - best_agent_search_radius\n                mask[mask_initial_idx:, :] = np.zeros((best_agent_search_radius, num_eval_rounds))\n                agent_names = np.array(agent_names)\n                eval_mask = mask if j % 2 == 0 else mask.T\n                shape = (best_agent_search_radius, num_eval_rounds)\n                masked_evaluation_matrix = evaluation_matrix[eval_mask == 0].reshape(shape)\n                masked_evaluation_matrix = masked_evaluation_matrix if j % 2 == 0 else masked_evaluation_matrix.T\n                agent_names = agent_names[mask[:, 0] == 0]\n                (best_agent_name, best_agent_score) = get_best_agent_from_eval_mat(masked_evaluation_matrix, agent_names, axis=j, maximize=maximize_indicator)\n                best_agents_population[best_agent_name] = best_agent_score\n            (best_agent_name, best_agent_score) = get_best_agent_from_vector(list(best_agents_population.values()), list(best_agents_population.keys()), maximize=maximize_indicator)\n            self.evaluation_configs[agent_name] = {'best_agent_name': best_agent_name, 'best_agent_score': best_agent_score, 'best_agent_method': self.eval_matrix_method[agent_name]}\n            self.clilog.info(f'Best agent for {agent_name} -> {best_agent_name}, score: {best_agent_score}')\n            mean_evaluation_matrix = np.mean(evaluation_matrices, axis=0)\n            std_evaluation_matrix = np.std(evaluation_matrices, axis=0)\n            wandb.log({f'{agent_name}/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=True)})\n            wandb.log({f'{agent_name}/mid_eval/heatmap': wandb.plots.HeatMap(axis[0], axis[1], mean_evaluation_matrix, show_text=False)})\n            wandb.log({f'{agent_name}/std_heatmap': wandb.plots.HeatMap(axis[0], axis[1], std_evaluation_matrix, show_text=True)})\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x'), axis[0])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y'), axis[1])\n            np.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix'), mean_evaluation_matrix)\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_x') + '.npy')\n            wandb.save(os.path.join(self.log_dir, agent_name, 'evaluation_matrix_axis_y') + '.npy')\n        self.clilog.info('Save experiment configuration with ')\n        log_file = os.path.join(self.log_dir, 'experiment_config.json')\n        ExperimentParser.save(log_file, self.experiment_configs, self.agents_configs, self.evaluation_configs, self.testing_configs)\n        wandb.save(log_file)\n        post_eval_list = []\n        for population_num in range(population_size):\n            self.clilog.info('-----------------------------------------------------------------------')\n            self.clilog.info(f'Post Evaluation for {agent_name} (population: {population_num})')\n            self.clilog.info('-----------------------------------------------------------------------')\n            eval_return_list = self.evalsave_callbacks[agent_name][population_num].post_eval(opponents_path=os.path.join(self.log_dir, self.agents_configs[agent_name]['opponent_name']), population_size=population_size)\n            post_eval_list.append(eval_return_list)\n            self.envs[agent_name][population_num].close()\n            self.eval_envs[agent_name][population_num].close()\n        mean_post_eval = np.mean(post_eval_list, axis=0)\n        std_post_eval = np.std(post_eval_list, axis=0)\n        data = [[x, y] for (x, y) in zip([i for i in range(len(mean_post_eval))], mean_post_eval)]\n        table = wandb.Table(data=data, columns=['opponent idx', 'win-rate'])\n        std_data = [[x, y] for (x, y) in zip([i for i in range(len(std_post_eval))], std_post_eval)]\n        std_table = wandb.Table(data=std_data, columns=['opponent idx', 'win-rate'])\n        wandb.log({f'{agent_name}/post_eval/table': wandb.plot.line(table, 'opponent idx', 'win-rate', title=f'Post evaluation {agent_name}')})\n        wandb.log({f'{agent_name}/post_eval/std_table': wandb.plot.line(std_table, 'opponent idx', 'win-rate', title=f'Std Post evaluation {agent_name}')})"
        ]
    }
]