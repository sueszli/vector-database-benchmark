[
    {
        "func_name": "base_llm",
        "original": "def base_llm(messages):\n    \"\"\"\n        Returns a generator\n        \"\"\"\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        if interpreter.context_window and interpreter.max_tokens:\n            trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n            messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n        elif interpreter.context_window and (not interpreter.max_tokens):\n            messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            try:\n                messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n            except:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n    except TypeError as e:\n        if interpreter.vision and str(e) == 'expected string or buffer':\n            if interpreter.debug_mode:\n                print(\"Couldn't token trim image messages. Error:\", e)\n            messages = [{'role': 'system', 'content': system_message}] + messages\n        else:\n            raise\n    if interpreter.debug_mode:\n        print('Passing messages into LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.model == 'gpt-4-vision-preview':\n        return openai.ChatCompletion.create(**params)\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    return litellm.completion(**params)",
        "mutated": [
            "def base_llm(messages):\n    if False:\n        i = 10\n    '\\n        Returns a generator\\n        '\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        if interpreter.context_window and interpreter.max_tokens:\n            trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n            messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n        elif interpreter.context_window and (not interpreter.max_tokens):\n            messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            try:\n                messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n            except:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n    except TypeError as e:\n        if interpreter.vision and str(e) == 'expected string or buffer':\n            if interpreter.debug_mode:\n                print(\"Couldn't token trim image messages. Error:\", e)\n            messages = [{'role': 'system', 'content': system_message}] + messages\n        else:\n            raise\n    if interpreter.debug_mode:\n        print('Passing messages into LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.model == 'gpt-4-vision-preview':\n        return openai.ChatCompletion.create(**params)\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    return litellm.completion(**params)",
            "def base_llm(messages):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns a generator\\n        '\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        if interpreter.context_window and interpreter.max_tokens:\n            trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n            messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n        elif interpreter.context_window and (not interpreter.max_tokens):\n            messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            try:\n                messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n            except:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n    except TypeError as e:\n        if interpreter.vision and str(e) == 'expected string or buffer':\n            if interpreter.debug_mode:\n                print(\"Couldn't token trim image messages. Error:\", e)\n            messages = [{'role': 'system', 'content': system_message}] + messages\n        else:\n            raise\n    if interpreter.debug_mode:\n        print('Passing messages into LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.model == 'gpt-4-vision-preview':\n        return openai.ChatCompletion.create(**params)\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    return litellm.completion(**params)",
            "def base_llm(messages):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns a generator\\n        '\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        if interpreter.context_window and interpreter.max_tokens:\n            trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n            messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n        elif interpreter.context_window and (not interpreter.max_tokens):\n            messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            try:\n                messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n            except:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n    except TypeError as e:\n        if interpreter.vision and str(e) == 'expected string or buffer':\n            if interpreter.debug_mode:\n                print(\"Couldn't token trim image messages. Error:\", e)\n            messages = [{'role': 'system', 'content': system_message}] + messages\n        else:\n            raise\n    if interpreter.debug_mode:\n        print('Passing messages into LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.model == 'gpt-4-vision-preview':\n        return openai.ChatCompletion.create(**params)\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    return litellm.completion(**params)",
            "def base_llm(messages):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns a generator\\n        '\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        if interpreter.context_window and interpreter.max_tokens:\n            trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n            messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n        elif interpreter.context_window and (not interpreter.max_tokens):\n            messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            try:\n                messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n            except:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n    except TypeError as e:\n        if interpreter.vision and str(e) == 'expected string or buffer':\n            if interpreter.debug_mode:\n                print(\"Couldn't token trim image messages. Error:\", e)\n            messages = [{'role': 'system', 'content': system_message}] + messages\n        else:\n            raise\n    if interpreter.debug_mode:\n        print('Passing messages into LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.model == 'gpt-4-vision-preview':\n        return openai.ChatCompletion.create(**params)\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    return litellm.completion(**params)",
            "def base_llm(messages):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns a generator\\n        '\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        if interpreter.context_window and interpreter.max_tokens:\n            trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n            messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n        elif interpreter.context_window and (not interpreter.max_tokens):\n            messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            try:\n                messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n            except:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n    except TypeError as e:\n        if interpreter.vision and str(e) == 'expected string or buffer':\n            if interpreter.debug_mode:\n                print(\"Couldn't token trim image messages. Error:\", e)\n            messages = [{'role': 'system', 'content': system_message}] + messages\n        else:\n            raise\n    if interpreter.debug_mode:\n        print('Passing messages into LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.model == 'gpt-4-vision-preview':\n        return openai.ChatCompletion.create(**params)\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    return litellm.completion(**params)"
        ]
    },
    {
        "func_name": "setup_text_llm",
        "original": "def setup_text_llm(interpreter):\n    \"\"\"\n    Takes an Interpreter (which includes a ton of LLM settings),\n    returns a text LLM (an OpenAI-compatible chat LLM with baked-in settings. Only takes `messages`).\n    \"\"\"\n\n    def base_llm(messages):\n        \"\"\"\n        Returns a generator\n        \"\"\"\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            if interpreter.context_window and interpreter.max_tokens:\n                trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n                messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n            elif interpreter.context_window and (not interpreter.max_tokens):\n                messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                try:\n                    messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n                except:\n                    if len(messages) == 1:\n                        display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                    messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n        except TypeError as e:\n            if interpreter.vision and str(e) == 'expected string or buffer':\n                if interpreter.debug_mode:\n                    print(\"Couldn't token trim image messages. Error:\", e)\n                messages = [{'role': 'system', 'content': system_message}] + messages\n            else:\n                raise\n        if interpreter.debug_mode:\n            print('Passing messages into LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.model == 'gpt-4-vision-preview':\n            return openai.ChatCompletion.create(**params)\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        return litellm.completion(**params)\n    return base_llm",
        "mutated": [
            "def setup_text_llm(interpreter):\n    if False:\n        i = 10\n    '\\n    Takes an Interpreter (which includes a ton of LLM settings),\\n    returns a text LLM (an OpenAI-compatible chat LLM with baked-in settings. Only takes `messages`).\\n    '\n\n    def base_llm(messages):\n        \"\"\"\n        Returns a generator\n        \"\"\"\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            if interpreter.context_window and interpreter.max_tokens:\n                trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n                messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n            elif interpreter.context_window and (not interpreter.max_tokens):\n                messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                try:\n                    messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n                except:\n                    if len(messages) == 1:\n                        display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                    messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n        except TypeError as e:\n            if interpreter.vision and str(e) == 'expected string or buffer':\n                if interpreter.debug_mode:\n                    print(\"Couldn't token trim image messages. Error:\", e)\n                messages = [{'role': 'system', 'content': system_message}] + messages\n            else:\n                raise\n        if interpreter.debug_mode:\n            print('Passing messages into LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.model == 'gpt-4-vision-preview':\n            return openai.ChatCompletion.create(**params)\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        return litellm.completion(**params)\n    return base_llm",
            "def setup_text_llm(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Takes an Interpreter (which includes a ton of LLM settings),\\n    returns a text LLM (an OpenAI-compatible chat LLM with baked-in settings. Only takes `messages`).\\n    '\n\n    def base_llm(messages):\n        \"\"\"\n        Returns a generator\n        \"\"\"\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            if interpreter.context_window and interpreter.max_tokens:\n                trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n                messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n            elif interpreter.context_window and (not interpreter.max_tokens):\n                messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                try:\n                    messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n                except:\n                    if len(messages) == 1:\n                        display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                    messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n        except TypeError as e:\n            if interpreter.vision and str(e) == 'expected string or buffer':\n                if interpreter.debug_mode:\n                    print(\"Couldn't token trim image messages. Error:\", e)\n                messages = [{'role': 'system', 'content': system_message}] + messages\n            else:\n                raise\n        if interpreter.debug_mode:\n            print('Passing messages into LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.model == 'gpt-4-vision-preview':\n            return openai.ChatCompletion.create(**params)\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        return litellm.completion(**params)\n    return base_llm",
            "def setup_text_llm(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Takes an Interpreter (which includes a ton of LLM settings),\\n    returns a text LLM (an OpenAI-compatible chat LLM with baked-in settings. Only takes `messages`).\\n    '\n\n    def base_llm(messages):\n        \"\"\"\n        Returns a generator\n        \"\"\"\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            if interpreter.context_window and interpreter.max_tokens:\n                trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n                messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n            elif interpreter.context_window and (not interpreter.max_tokens):\n                messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                try:\n                    messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n                except:\n                    if len(messages) == 1:\n                        display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                    messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n        except TypeError as e:\n            if interpreter.vision and str(e) == 'expected string or buffer':\n                if interpreter.debug_mode:\n                    print(\"Couldn't token trim image messages. Error:\", e)\n                messages = [{'role': 'system', 'content': system_message}] + messages\n            else:\n                raise\n        if interpreter.debug_mode:\n            print('Passing messages into LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.model == 'gpt-4-vision-preview':\n            return openai.ChatCompletion.create(**params)\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        return litellm.completion(**params)\n    return base_llm",
            "def setup_text_llm(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Takes an Interpreter (which includes a ton of LLM settings),\\n    returns a text LLM (an OpenAI-compatible chat LLM with baked-in settings. Only takes `messages`).\\n    '\n\n    def base_llm(messages):\n        \"\"\"\n        Returns a generator\n        \"\"\"\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            if interpreter.context_window and interpreter.max_tokens:\n                trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n                messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n            elif interpreter.context_window and (not interpreter.max_tokens):\n                messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                try:\n                    messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n                except:\n                    if len(messages) == 1:\n                        display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                    messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n        except TypeError as e:\n            if interpreter.vision and str(e) == 'expected string or buffer':\n                if interpreter.debug_mode:\n                    print(\"Couldn't token trim image messages. Error:\", e)\n                messages = [{'role': 'system', 'content': system_message}] + messages\n            else:\n                raise\n        if interpreter.debug_mode:\n            print('Passing messages into LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.model == 'gpt-4-vision-preview':\n            return openai.ChatCompletion.create(**params)\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        return litellm.completion(**params)\n    return base_llm",
            "def setup_text_llm(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Takes an Interpreter (which includes a ton of LLM settings),\\n    returns a text LLM (an OpenAI-compatible chat LLM with baked-in settings. Only takes `messages`).\\n    '\n\n    def base_llm(messages):\n        \"\"\"\n        Returns a generator\n        \"\"\"\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            if interpreter.context_window and interpreter.max_tokens:\n                trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25\n                messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)\n            elif interpreter.context_window and (not interpreter.max_tokens):\n                messages = tt.trim(messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                try:\n                    messages = tt.trim(messages, system_message=system_message, model=interpreter.model)\n                except:\n                    if len(messages) == 1:\n                        display_markdown_message('\\n                        **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                        If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                        Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`\\n                        ')\n                    messages = tt.trim(messages, system_message=system_message, max_tokens=3000)\n        except TypeError as e:\n            if interpreter.vision and str(e) == 'expected string or buffer':\n                if interpreter.debug_mode:\n                    print(\"Couldn't token trim image messages. Error:\", e)\n                messages = [{'role': 'system', 'content': system_message}] + messages\n            else:\n                raise\n        if interpreter.debug_mode:\n            print('Passing messages into LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.model == 'gpt-4-vision-preview':\n            return openai.ChatCompletion.create(**params)\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        return litellm.completion(**params)\n    return base_llm"
        ]
    }
]