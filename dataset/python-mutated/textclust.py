from __future__ import annotations
import math
import numpy as np
import pandas as pd
from river import base
__all__ = ['TextClust']

class TextClust(base.Clusterer):
    """textClust, a clustering algorithm for text data.

    textClust [^1][^2] is a stream clustering algorithm for textual data that can identify and track topics
    over time in a stream of texts. The algorithm uses a widely popular two-phase clustering
    approach where the stream is first summarised in real-time.

    The result is many small preliminary clusters in the stream called `micro-clusters`.
    Micro-clusters maintain enough information to update and efficiently calculate the
    cosine similarity between them over time, based on the TF-IDF vector of their texts.
    Upon request, the miro-clusters can be reclustered to generate the final result
    using any distance-based clustering algorithm, such as hierarchical clustering.
    To keep the micro-clusters up-to-date, our algorithm applies a fading strategy where
    micro-clusters that are not updated regularly lose relevance and are eventually removed.

    Parameters
    ----------
    radius
        Distance threshold to merge two micro-clusters. Must be within the range `(0, 1]`
    fading_factor
        Fading factor of micro-clusters
    tgap
       Time between outlier removal
    term_fading
        Determines whether individual terms should also be faded
    real_time_fading
        Parameter that specifies whether natural time or the number of observations should be used
        for fading
    micro_distance
         Distance metric used for clustering macro-clusters
    macro_distance
        Distance metric used for clustering macro-clusters
    num_macro
        Number of macro clusters that should be identified during the reclustering phase
    min_weight
        Minimum weight of micro clusters to be used for reclustering
    auto_r
        Parameter that specifies if  `radius` should be automatically updated
    auto_merge
        Determines, if close observations shall be merged together
    sigma
        Parameter that influences the automated trheshold adaption technique

     Attributes
    ----------
    micro_clusters
        Micro-clusters generated by the algorithm. Micro-clusters are of type `textclust.microcluster`

    References
    ----------
    [^1]: Assenmacher, D. und Trautmann, H. (2022). Textual One-Pass Stream Clustering with
    Automated Distance Threshold Adaption. In: Asian Conference on Intelligent Information and
    Database Systems (Accepted)
    [^2]: Carnein, M., Assenmacher, D., Trautmann, H. (2017). Stream Clustering of Chat Messages with
    Applications to Twitch Streams. In: Advances in Conceptual Modeling. ER 2017.

    Examples
    --------

    >>> from river import compose
    >>> from river import feature_extraction
    >>> from river import metrics
    >>> from river import cluster

    >>> corpus = [
    ...    {"text":'This is the first document.',"idd":1, "cluster": 1, "cluster":1},
    ...    {"text":'This document is the second document.',"idd":2,"cluster": 1},
    ...    {"text":'And this is super unrelated.',"idd":3,"cluster": 2},
    ...    {"text":'Is this the first document?',"idd":4,"cluster": 1},
    ...    {"text":'This is super unrelated as well',"idd":5,"cluster": 2},
    ...    {"text":'Test text',"idd":6,"cluster": 5}
    ... ]

    >>> stopwords = [ 'stop', 'the', 'to', 'and', 'a', 'in', 'it', 'is', 'I']

    >>> metric = metrics.AdjustedRand()

    >>> model = compose.Pipeline(
    ...     feature_extraction.BagOfWords(lowercase=True, ngram_range=(1, 2), stop_words=stopwords),
    ...     cluster.TextClust(real_time_fading=False, fading_factor=0.001, tgap=100, auto_r=True,
    ...     radius=0.9)
    ... )

    >>> for x in corpus:
    ...     y_pred = model.predict_one(x["text"])
    ...     y = x["cluster"]
    ...     metric = metric.update(y,y_pred)
    ...     model = model.learn_one(x["text"])

    >>> print(metric)
    AdjustedRand: -0.17647058823529413


    """

    def __init__(self, radius=0.3, fading_factor=0.0005, tgap=100, term_fading=True, real_time_fading=True, micro_distance='tfidf_cosine_distance', macro_distance='tfidf_cosine_distance', num_macro=3, min_weight=0, auto_r=False, auto_merge=True, sigma=1):
        if False:
            i = 10
            return i + 15
        self.radius = radius
        self.fading_factor = fading_factor
        self.tgap = tgap
        self.term_fading = term_fading
        self.micro_distance = micro_distance
        self.macro_distance = macro_distance
        self.num_macro = num_macro
        self.real_time_fading = real_time_fading
        self.min_weight = min_weight
        self.auto_r = auto_r
        self.auto_merge = auto_merge
        self.sigma = sigma
        self.t = None
        self.last_cleanup = 0
        self.n = 1
        self.omega = 2 ** (-1 * self.fading_factor * self.tgap)
        self.micro_clusters = dict()
        self.microToMacro = None
        self.realtime = None
        self._clusterId = 0
        self._up_to_date = False
        self._dist_mean = 0
        self._num_merged_obs = 0
        self.micro_distance = self.distances(self.micro_distance)
        self.macro_distance = self.distances(self.macro_distance)

    def learn_one(self, x, t=None, sample_weight=None):
        if False:
            while True:
                i = 10
        localdict = {}
        for key in x.keys():
            new_key = key
            localdict[new_key] = {}
            localdict[new_key]['tf'] = x[key]
        ngrams = localdict
        ngrams = dict(ngrams)
        self._up_to_date = False
        if self.real_time_fading:
            self.t = t
        else:
            self.t = self.n
        if self.realtime is not None:
            self.realtime = self.realtime
        clusterId = None
        if len(ngrams) > 0:
            mc = self.microcluster(ngrams, self.t, 1, self.realtime, self._clusterId)
            idf = self._calculateIDF(self.micro_clusters.values())
            (clusterId, min_dist) = self._get_closest_mc(mc, idf, self.micro_distance)
            if clusterId is not None:
                self._num_merged_obs += 1
                self.micro_clusters[clusterId].n += 1
                self.micro_clusters[clusterId].merge(mc, self.t, self.omega, self.fading_factor, self.term_fading, self.realtime)
                self._dist_mean += min_dist
            else:
                self._dist_mean += min_dist
                clusterId = self._clusterId
                self.micro_clusters[clusterId] = mc
                self._clusterId += 1
        if self.last_cleanup is None or self.t - self.last_cleanup >= self.tgap:
            self._cleanup()
        self.n += 1
        return clusterId

    def predict_one(self, x, sample_weight=None, type='micro'):
        if False:
            print('Hello World!')
        localdict = {}
        for key in x.keys():
            new_key = key
            localdict[new_key] = {}
            localdict[new_key]['tf'] = x[key]
        ngrams = localdict
        ngrams = dict(ngrams)
        return self.get_assignment(ngrams, type=type)

    def _get_closest_mc(self, mc, idf, distance):
        if False:
            while True:
                i = 10
        'Return the closest micro cluster.'
        clusterId = None
        min_dist = 1
        smallest_key = None
        sumdist = 0
        squaresum = 0
        counter = 0
        for key in self.micro_clusters.keys():
            dist = distance.dist(mc, self.micro_clusters[key], idf)
            counter = counter + 1
            sumdist += dist
            squaresum += dist ** 2
            if dist < min_dist:
                min_dist = dist
                smallest_key = key
        if self.auto_r:
            if counter > 1:
                mu = (sumdist - min_dist) / (counter - 1)
                threshold = mu - self.sigma * math.sqrt(squaresum / (counter - 1) - mu ** 2)
                if min_dist < threshold:
                    clusterId = smallest_key
        elif min_dist < self.radius:
            clusterId = smallest_key
        return (clusterId, min_dist)

    def _calculateIDF(self, micro_clusters):
        if False:
            return 10
        result = {}
        for micro in micro_clusters:
            for k in list(micro.tf.keys()):
                if k not in result:
                    result[k] = 1
                else:
                    result[k] += 1
        for k in list(result.keys()):
            result[k] = 1 + math.log(len(micro_clusters) / result[k])
        return result

    def _updateweights(self):
        if False:
            while True:
                i = 10
        for micro in self.micro_clusters.values():
            micro.fade(self.t, self.omega, self.fading_factor, self.term_fading, self.realtime)
        for key in list(self.micro_clusters.keys()):
            if self.micro_clusters[key].weight <= self.omega or len(self.micro_clusters[key].tf) == 0:
                del self.micro_clusters[key]

    def _cleanup(self):
        if False:
            print('Hello World!')
        self.last_cleanup = self.t
        self._updateweights()
        for micro in self.micro_clusters.values():
            micro.deltaweight = micro.weight - micro.oldweight
            micro.oldweight = micro.weight
        if self.auto_merge:
            self._mergemicroclusters()
        self._dist_mean = 0
        self._num_merged_obs = 0

    def _mergemicroclusters(self):
        if False:
            while True:
                i = 10
        micro_keys = [*self.micro_clusters]
        idf = self._calculateIDF(self.micro_clusters.values())
        i = 0
        if self.auto_r:
            threshold = self._dist_mean / (self._num_merged_obs + 1)
        else:
            threshold = self.radius
        while i < len(self.micro_clusters):
            j = i + 1
            while j < len(self.micro_clusters):
                m_dist = self.micro_distance.dist(self.micro_clusters[micro_keys[i]], self.micro_clusters[micro_keys[j]], idf)
                if m_dist < threshold:
                    self.micro_clusters[micro_keys[i]].merge(self.micro_clusters[micro_keys[j]], self.t, self.omega, self.fading_factor, self.term_fading, self.realtime)
                    self.micro_clusters[micro_keys[i]].merged_ids.append(self.micro_clusters[micro_keys[j]].id)
                    del self.micro_clusters[micro_keys[j]]
                    del micro_keys[j]
                else:
                    j = j + 1
            i = i + 1

    def _get_distance_matrix(self, clusters):
        if False:
            for i in range(10):
                print('nop')
        idf = self._calculateIDF(clusters.values())
        numClusters = len(clusters)
        ids = list(clusters.keys())
        distances = pd.DataFrame(np.zeros((numClusters, numClusters)), columns=ids, index=ids)
        for (idx, row) in enumerate(ids):
            for col in ids[idx + 1:]:
                dist = self.macro_distance.dist(clusters[row], clusters[col], idf)
                distances.loc[row, col] = dist
                distances.loc[col, row] = dist
        return distances

    def _agglomerative_clustering(self, micros, k):
        if False:
            return 10
        clusters = []
        distm = self._get_distance_matrix(micros)
        indices = distm.index
        for i in range(0, len(micros)):
            clusters.append([indices[i]])
        while len(clusters) != k:
            min_dist = math.inf
            min_pair = ()
            for i in range(0, len(clusters) - 1):
                for j in range(i + 1, len(clusters)):
                    for c_i in clusters[i]:
                        for c_j in clusters[j]:
                            if distm[c_i][c_j] < min_dist:
                                min_dist = distm[c_i][c_j]
                                min_pair = (i, j)
            clusters[min_pair[0]] = clusters[min_pair[0]] + clusters[min_pair[1]]
            del clusters[min_pair[1]]
        return clusters

    def updateMacroClusters(self):
        if False:
            for i in range(10):
                print('nop')
        if not self._up_to_date:
            self._updateweights()
            micros = {key: value for (key, value) in self.micro_clusters.items() if value.weight > self.min_weight}
            numClusters = min([self.num_macro, len(micros)])
            if len(micros) > 1:
                assigned_clusters = self._agglomerative_clustering(micros, numClusters)
                self.microToMacro = {}
                for i in range(0, len(assigned_clusters)):
                    for x in assigned_clusters[i]:
                        self.microToMacro[x] = i
                self._up_to_date = True

    def get_macroclusters(self):
        if False:
            i = 10
            return i + 15
        self.updateMacroClusters()
        numClusters = min([self.num_macro, len(self.micro_clusters)])
        macros = {x: self.microcluster({}, self.t, 0, self.realtime, x) for x in range(numClusters)}
        for (key, value) in self.microToMacro.items():
            macros[value].merge(self.micro_clusters[key], self.t, self.omega, self.fading_factor, self.term_fading, self.realtime)
            macros[value].merged_ids.append(self.micro_clusters[key].id)
        return macros

    def showclusters(self, topn, num, type='micro'):
        if False:
            print('Hello World!')
        if type == 'micro':
            sortedmicro = sorted(self.micro_clusters.values(), key=lambda x: x.weight, reverse=True)
        else:
            sortedmicro = sorted(self.get_macroclusters().values(), key=lambda x: x.weight, reverse=True)
        print('-------------------------------------------')
        print('Summary of ' + type + ' clusters:')
        for micro in sortedmicro[0:topn]:
            print('----')
            print(type + ' cluster id ' + str(micro.id))
            print(type + ' cluster weight ' + str(micro.weight))
            if type != 'micro':
                print('merged micro clusters: ' + str(micro.merged_ids))
            indices = sorted(range(len([i['tf'] for i in micro.tf.values()])), key=[i['tf'] for i in micro.tf.values()].__getitem__, reverse=True)
            representatives = [(list(micro.tf.keys())[i], micro.tf[list(micro.tf.keys())[i]]['tf']) for i in indices[0:min(len(micro.tf.keys()), num)]]
            for rep in representatives:
                print('weight: ' + str(round(rep[1], 2)) + '\t token: ' + str(rep[0]).expandtabs(10))
        print('-------------------------------------------')

    def get_assignment(self, x, type):
        if False:
            for i in range(10):
                print('nop')
        self._updateweights()
        assignment = None
        idf = None
        idf = self._calculateIDF(self.micro_clusters.values())
        if len(x) > 0:
            mc = self.microcluster(x, 1, 1, self.realtime, None)
            dist = float('inf')
            closest = None
            for key in self.micro_clusters.keys():
                if self.micro_clusters[key].weight > self.min_weight:
                    cur_dist = self.micro_distance.dist(mc, self.micro_clusters[key], idf)
                    if cur_dist < dist:
                        dist = cur_dist
                        closest = key
            assignment = closest
            if type == 'micro':
                return assignment
            else:
                self.updateMacroClusters()
                return self.microToMacro[assignment] if assignment else None

    class tfcontainer:

        def __init__(self, tfvalue, ids):
            if False:
                for i in range(10):
                    print('nop')
            self.tfvalue = tfvalue
            self.ids = ids

    class microcluster:

        def __init__(self, tf, time, weight, realtime, clusterid):
            if False:
                i = 10
                return i + 15
            self.id = clusterid
            self.weight = weight
            self.time = time
            self.tf = tf
            self.oldweight = 0
            self.deltaweight = 0
            self.realtime = realtime
            self.n = 1
            self.merged_ids = []

        def fade(self, tnow, omega, fading_factor, term_fading, realtime):
            if False:
                for i in range(10):
                    print('nop')
            self.weight = self.weight * pow(2, -fading_factor * (tnow - self.time))
            if term_fading:
                for k in list(self.tf.keys()):
                    self.tf[k]['tf'] = self.tf[k]['tf'] * pow(2, -fading_factor * (tnow - self.time))
                    if self.tf[k]['tf'] <= omega:
                        del self.tf[k]
            self.time = tnow
            self.realtime = realtime

        def merge(self, microcluster, t, omega, fading_factor, term_fading, realtime):
            if False:
                return 10
            self.realtime = realtime
            self.weight = self.weight + microcluster.weight
            self.fade(t, omega, fading_factor, term_fading, realtime)
            microcluster.fade(t, omega, fading_factor, term_fading, realtime)
            self.time = t
            for k in list(microcluster.tf.keys()):
                if k in self.tf:
                    self.tf[k]['tf'] += microcluster.tf[k]['tf']
                else:
                    self.tf[k] = {}
                    self.tf[k]['tf'] = microcluster.tf[k]['tf']

    class distances:

        def __init__(self, type):
            if False:
                return 10
            self.type = type

        def dist(self, m1, m2, idf):
            if False:
                return 10
            return getattr(self, self.type, lambda : 'Invalid distance measure')(m1, m2, idf)

        def tfidf_cosine_distance(self, mc, microcluster, idf):
            if False:
                i = 10
                return i + 15
            sum = 0
            tfidflen = 0
            microtfidflen = 0
            for k in list(mc.tf.keys()):
                if k in idf:
                    if k in microcluster.tf:
                        sum += mc.tf[k]['tf'] * idf[k] * (microcluster.tf[k]['tf'] * idf[k])
                    tfidflen += mc.tf[k]['tf'] * idf[k] * mc.tf[k]['tf'] * idf[k]
            tfidflen = math.sqrt(tfidflen)
            for k in list(microcluster.tf.keys()):
                microtfidflen += microcluster.tf[k]['tf'] * idf[k] * microcluster.tf[k]['tf'] * idf[k]
            microtfidflen = math.sqrt(microtfidflen)
            if tfidflen == 0 or microtfidflen == 0:
                return 1
            else:
                return round(1 - sum / (tfidflen * microtfidflen), 10)