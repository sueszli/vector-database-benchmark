[
    {
        "func_name": "from_dlpack",
        "original": "def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n    \"\"\"from_dlpack(ext_tensor) -> Tensor\n\n    Converts a tensor from an external library into a ``torch.Tensor``.\n\n    The returned PyTorch tensor will share the memory with the input tensor\n    (which may have come from another library). Note that in-place operations\n    will therefore also affect the data of the input tensor. This may lead to\n    unexpected issues (e.g., other libraries may have read-only flags or\n    immutable data structures), so the user should only do this if they know\n    for sure that this is fine.\n\n    Args:\n        ext_tensor (object with ``__dlpack__`` attribute, or a DLPack capsule):\n            The tensor or DLPack capsule to convert.\n\n            If ``ext_tensor`` is a tensor (or ndarray) object, it must support\n            the ``__dlpack__`` protocol (i.e., have a ``ext_tensor.__dlpack__``\n            method). Otherwise ``ext_tensor`` may be a DLPack capsule, which is\n            an opaque ``PyCapsule`` instance, typically produced by a\n            ``to_dlpack`` function or method.\n\n    Examples::\n\n        >>> import torch.utils.dlpack\n        >>> t = torch.arange(4)\n\n        # Convert a tensor directly (supported in PyTorch >= 1.10)\n        >>> t2 = torch.from_dlpack(t)\n        >>> t2[:2] = -1  # show that memory is shared\n        >>> t2\n        tensor([-1, -1,  2,  3])\n        >>> t\n        tensor([-1, -1,  2,  3])\n\n        # The old-style DLPack usage, with an intermediate capsule object\n        >>> capsule = torch.utils.dlpack.to_dlpack(t)\n        >>> capsule\n        <capsule object \"dltensor\" at ...>\n        >>> t3 = torch.from_dlpack(capsule)\n        >>> t3\n        tensor([-1, -1,  2,  3])\n        >>> t3[0] = -9  # now we're sharing memory between 3 tensors\n        >>> t3\n        tensor([-9, -1,  2,  3])\n        >>> t2\n        tensor([-9, -1,  2,  3])\n        >>> t\n        tensor([-9, -1,  2,  3])\n\n    \"\"\"\n    if hasattr(ext_tensor, '__dlpack__'):\n        device = ext_tensor.__dlpack_device__()\n        if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n            is_cuda = device[0] == DLDeviceType.kDLGPU\n            stream_ptr = 1 if is_cuda and stream.cuda_stream == 0 else stream.cuda_stream\n            dlpack = ext_tensor.__dlpack__(stream=stream_ptr)\n        else:\n            dlpack = ext_tensor.__dlpack__()\n    else:\n        dlpack = ext_tensor\n    return _from_dlpack(dlpack)",
        "mutated": [
            "def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n    if False:\n        i = 10\n    'from_dlpack(ext_tensor) -> Tensor\\n\\n    Converts a tensor from an external library into a ``torch.Tensor``.\\n\\n    The returned PyTorch tensor will share the memory with the input tensor\\n    (which may have come from another library). Note that in-place operations\\n    will therefore also affect the data of the input tensor. This may lead to\\n    unexpected issues (e.g., other libraries may have read-only flags or\\n    immutable data structures), so the user should only do this if they know\\n    for sure that this is fine.\\n\\n    Args:\\n        ext_tensor (object with ``__dlpack__`` attribute, or a DLPack capsule):\\n            The tensor or DLPack capsule to convert.\\n\\n            If ``ext_tensor`` is a tensor (or ndarray) object, it must support\\n            the ``__dlpack__`` protocol (i.e., have a ``ext_tensor.__dlpack__``\\n            method). Otherwise ``ext_tensor`` may be a DLPack capsule, which is\\n            an opaque ``PyCapsule`` instance, typically produced by a\\n            ``to_dlpack`` function or method.\\n\\n    Examples::\\n\\n        >>> import torch.utils.dlpack\\n        >>> t = torch.arange(4)\\n\\n        # Convert a tensor directly (supported in PyTorch >= 1.10)\\n        >>> t2 = torch.from_dlpack(t)\\n        >>> t2[:2] = -1  # show that memory is shared\\n        >>> t2\\n        tensor([-1, -1,  2,  3])\\n        >>> t\\n        tensor([-1, -1,  2,  3])\\n\\n        # The old-style DLPack usage, with an intermediate capsule object\\n        >>> capsule = torch.utils.dlpack.to_dlpack(t)\\n        >>> capsule\\n        <capsule object \"dltensor\" at ...>\\n        >>> t3 = torch.from_dlpack(capsule)\\n        >>> t3\\n        tensor([-1, -1,  2,  3])\\n        >>> t3[0] = -9  # now we\\'re sharing memory between 3 tensors\\n        >>> t3\\n        tensor([-9, -1,  2,  3])\\n        >>> t2\\n        tensor([-9, -1,  2,  3])\\n        >>> t\\n        tensor([-9, -1,  2,  3])\\n\\n    '\n    if hasattr(ext_tensor, '__dlpack__'):\n        device = ext_tensor.__dlpack_device__()\n        if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n            is_cuda = device[0] == DLDeviceType.kDLGPU\n            stream_ptr = 1 if is_cuda and stream.cuda_stream == 0 else stream.cuda_stream\n            dlpack = ext_tensor.__dlpack__(stream=stream_ptr)\n        else:\n            dlpack = ext_tensor.__dlpack__()\n    else:\n        dlpack = ext_tensor\n    return _from_dlpack(dlpack)",
            "def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'from_dlpack(ext_tensor) -> Tensor\\n\\n    Converts a tensor from an external library into a ``torch.Tensor``.\\n\\n    The returned PyTorch tensor will share the memory with the input tensor\\n    (which may have come from another library). Note that in-place operations\\n    will therefore also affect the data of the input tensor. This may lead to\\n    unexpected issues (e.g., other libraries may have read-only flags or\\n    immutable data structures), so the user should only do this if they know\\n    for sure that this is fine.\\n\\n    Args:\\n        ext_tensor (object with ``__dlpack__`` attribute, or a DLPack capsule):\\n            The tensor or DLPack capsule to convert.\\n\\n            If ``ext_tensor`` is a tensor (or ndarray) object, it must support\\n            the ``__dlpack__`` protocol (i.e., have a ``ext_tensor.__dlpack__``\\n            method). Otherwise ``ext_tensor`` may be a DLPack capsule, which is\\n            an opaque ``PyCapsule`` instance, typically produced by a\\n            ``to_dlpack`` function or method.\\n\\n    Examples::\\n\\n        >>> import torch.utils.dlpack\\n        >>> t = torch.arange(4)\\n\\n        # Convert a tensor directly (supported in PyTorch >= 1.10)\\n        >>> t2 = torch.from_dlpack(t)\\n        >>> t2[:2] = -1  # show that memory is shared\\n        >>> t2\\n        tensor([-1, -1,  2,  3])\\n        >>> t\\n        tensor([-1, -1,  2,  3])\\n\\n        # The old-style DLPack usage, with an intermediate capsule object\\n        >>> capsule = torch.utils.dlpack.to_dlpack(t)\\n        >>> capsule\\n        <capsule object \"dltensor\" at ...>\\n        >>> t3 = torch.from_dlpack(capsule)\\n        >>> t3\\n        tensor([-1, -1,  2,  3])\\n        >>> t3[0] = -9  # now we\\'re sharing memory between 3 tensors\\n        >>> t3\\n        tensor([-9, -1,  2,  3])\\n        >>> t2\\n        tensor([-9, -1,  2,  3])\\n        >>> t\\n        tensor([-9, -1,  2,  3])\\n\\n    '\n    if hasattr(ext_tensor, '__dlpack__'):\n        device = ext_tensor.__dlpack_device__()\n        if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n            is_cuda = device[0] == DLDeviceType.kDLGPU\n            stream_ptr = 1 if is_cuda and stream.cuda_stream == 0 else stream.cuda_stream\n            dlpack = ext_tensor.__dlpack__(stream=stream_ptr)\n        else:\n            dlpack = ext_tensor.__dlpack__()\n    else:\n        dlpack = ext_tensor\n    return _from_dlpack(dlpack)",
            "def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'from_dlpack(ext_tensor) -> Tensor\\n\\n    Converts a tensor from an external library into a ``torch.Tensor``.\\n\\n    The returned PyTorch tensor will share the memory with the input tensor\\n    (which may have come from another library). Note that in-place operations\\n    will therefore also affect the data of the input tensor. This may lead to\\n    unexpected issues (e.g., other libraries may have read-only flags or\\n    immutable data structures), so the user should only do this if they know\\n    for sure that this is fine.\\n\\n    Args:\\n        ext_tensor (object with ``__dlpack__`` attribute, or a DLPack capsule):\\n            The tensor or DLPack capsule to convert.\\n\\n            If ``ext_tensor`` is a tensor (or ndarray) object, it must support\\n            the ``__dlpack__`` protocol (i.e., have a ``ext_tensor.__dlpack__``\\n            method). Otherwise ``ext_tensor`` may be a DLPack capsule, which is\\n            an opaque ``PyCapsule`` instance, typically produced by a\\n            ``to_dlpack`` function or method.\\n\\n    Examples::\\n\\n        >>> import torch.utils.dlpack\\n        >>> t = torch.arange(4)\\n\\n        # Convert a tensor directly (supported in PyTorch >= 1.10)\\n        >>> t2 = torch.from_dlpack(t)\\n        >>> t2[:2] = -1  # show that memory is shared\\n        >>> t2\\n        tensor([-1, -1,  2,  3])\\n        >>> t\\n        tensor([-1, -1,  2,  3])\\n\\n        # The old-style DLPack usage, with an intermediate capsule object\\n        >>> capsule = torch.utils.dlpack.to_dlpack(t)\\n        >>> capsule\\n        <capsule object \"dltensor\" at ...>\\n        >>> t3 = torch.from_dlpack(capsule)\\n        >>> t3\\n        tensor([-1, -1,  2,  3])\\n        >>> t3[0] = -9  # now we\\'re sharing memory between 3 tensors\\n        >>> t3\\n        tensor([-9, -1,  2,  3])\\n        >>> t2\\n        tensor([-9, -1,  2,  3])\\n        >>> t\\n        tensor([-9, -1,  2,  3])\\n\\n    '\n    if hasattr(ext_tensor, '__dlpack__'):\n        device = ext_tensor.__dlpack_device__()\n        if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n            is_cuda = device[0] == DLDeviceType.kDLGPU\n            stream_ptr = 1 if is_cuda and stream.cuda_stream == 0 else stream.cuda_stream\n            dlpack = ext_tensor.__dlpack__(stream=stream_ptr)\n        else:\n            dlpack = ext_tensor.__dlpack__()\n    else:\n        dlpack = ext_tensor\n    return _from_dlpack(dlpack)",
            "def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'from_dlpack(ext_tensor) -> Tensor\\n\\n    Converts a tensor from an external library into a ``torch.Tensor``.\\n\\n    The returned PyTorch tensor will share the memory with the input tensor\\n    (which may have come from another library). Note that in-place operations\\n    will therefore also affect the data of the input tensor. This may lead to\\n    unexpected issues (e.g., other libraries may have read-only flags or\\n    immutable data structures), so the user should only do this if they know\\n    for sure that this is fine.\\n\\n    Args:\\n        ext_tensor (object with ``__dlpack__`` attribute, or a DLPack capsule):\\n            The tensor or DLPack capsule to convert.\\n\\n            If ``ext_tensor`` is a tensor (or ndarray) object, it must support\\n            the ``__dlpack__`` protocol (i.e., have a ``ext_tensor.__dlpack__``\\n            method). Otherwise ``ext_tensor`` may be a DLPack capsule, which is\\n            an opaque ``PyCapsule`` instance, typically produced by a\\n            ``to_dlpack`` function or method.\\n\\n    Examples::\\n\\n        >>> import torch.utils.dlpack\\n        >>> t = torch.arange(4)\\n\\n        # Convert a tensor directly (supported in PyTorch >= 1.10)\\n        >>> t2 = torch.from_dlpack(t)\\n        >>> t2[:2] = -1  # show that memory is shared\\n        >>> t2\\n        tensor([-1, -1,  2,  3])\\n        >>> t\\n        tensor([-1, -1,  2,  3])\\n\\n        # The old-style DLPack usage, with an intermediate capsule object\\n        >>> capsule = torch.utils.dlpack.to_dlpack(t)\\n        >>> capsule\\n        <capsule object \"dltensor\" at ...>\\n        >>> t3 = torch.from_dlpack(capsule)\\n        >>> t3\\n        tensor([-1, -1,  2,  3])\\n        >>> t3[0] = -9  # now we\\'re sharing memory between 3 tensors\\n        >>> t3\\n        tensor([-9, -1,  2,  3])\\n        >>> t2\\n        tensor([-9, -1,  2,  3])\\n        >>> t\\n        tensor([-9, -1,  2,  3])\\n\\n    '\n    if hasattr(ext_tensor, '__dlpack__'):\n        device = ext_tensor.__dlpack_device__()\n        if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n            is_cuda = device[0] == DLDeviceType.kDLGPU\n            stream_ptr = 1 if is_cuda and stream.cuda_stream == 0 else stream.cuda_stream\n            dlpack = ext_tensor.__dlpack__(stream=stream_ptr)\n        else:\n            dlpack = ext_tensor.__dlpack__()\n    else:\n        dlpack = ext_tensor\n    return _from_dlpack(dlpack)",
            "def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'from_dlpack(ext_tensor) -> Tensor\\n\\n    Converts a tensor from an external library into a ``torch.Tensor``.\\n\\n    The returned PyTorch tensor will share the memory with the input tensor\\n    (which may have come from another library). Note that in-place operations\\n    will therefore also affect the data of the input tensor. This may lead to\\n    unexpected issues (e.g., other libraries may have read-only flags or\\n    immutable data structures), so the user should only do this if they know\\n    for sure that this is fine.\\n\\n    Args:\\n        ext_tensor (object with ``__dlpack__`` attribute, or a DLPack capsule):\\n            The tensor or DLPack capsule to convert.\\n\\n            If ``ext_tensor`` is a tensor (or ndarray) object, it must support\\n            the ``__dlpack__`` protocol (i.e., have a ``ext_tensor.__dlpack__``\\n            method). Otherwise ``ext_tensor`` may be a DLPack capsule, which is\\n            an opaque ``PyCapsule`` instance, typically produced by a\\n            ``to_dlpack`` function or method.\\n\\n    Examples::\\n\\n        >>> import torch.utils.dlpack\\n        >>> t = torch.arange(4)\\n\\n        # Convert a tensor directly (supported in PyTorch >= 1.10)\\n        >>> t2 = torch.from_dlpack(t)\\n        >>> t2[:2] = -1  # show that memory is shared\\n        >>> t2\\n        tensor([-1, -1,  2,  3])\\n        >>> t\\n        tensor([-1, -1,  2,  3])\\n\\n        # The old-style DLPack usage, with an intermediate capsule object\\n        >>> capsule = torch.utils.dlpack.to_dlpack(t)\\n        >>> capsule\\n        <capsule object \"dltensor\" at ...>\\n        >>> t3 = torch.from_dlpack(capsule)\\n        >>> t3\\n        tensor([-1, -1,  2,  3])\\n        >>> t3[0] = -9  # now we\\'re sharing memory between 3 tensors\\n        >>> t3\\n        tensor([-9, -1,  2,  3])\\n        >>> t2\\n        tensor([-9, -1,  2,  3])\\n        >>> t\\n        tensor([-9, -1,  2,  3])\\n\\n    '\n    if hasattr(ext_tensor, '__dlpack__'):\n        device = ext_tensor.__dlpack_device__()\n        if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n            is_cuda = device[0] == DLDeviceType.kDLGPU\n            stream_ptr = 1 if is_cuda and stream.cuda_stream == 0 else stream.cuda_stream\n            dlpack = ext_tensor.__dlpack__(stream=stream_ptr)\n        else:\n            dlpack = ext_tensor.__dlpack__()\n    else:\n        dlpack = ext_tensor\n    return _from_dlpack(dlpack)"
        ]
    }
]