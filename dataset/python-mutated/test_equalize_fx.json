[
    {
        "func_name": "channel_minmax",
        "original": "def channel_minmax(self, input, axis=1):\n    \"\"\" Finds the min/max of inputs associated with a specific channel\n        \"\"\"\n    size_of_tensor_dim = input.ndim\n    axis_list = list(range(size_of_tensor_dim))\n    axis_list.remove(axis)\n    axis_list.sort(reverse=True)\n    mins = input.copy()\n    maxs = input.copy()\n    for a in axis_list:\n        mins = mins.min(a)\n        maxs = maxs.max(a)\n    return (mins, maxs)",
        "mutated": [
            "def channel_minmax(self, input, axis=1):\n    if False:\n        i = 10\n    ' Finds the min/max of inputs associated with a specific channel\\n        '\n    size_of_tensor_dim = input.ndim\n    axis_list = list(range(size_of_tensor_dim))\n    axis_list.remove(axis)\n    axis_list.sort(reverse=True)\n    mins = input.copy()\n    maxs = input.copy()\n    for a in axis_list:\n        mins = mins.min(a)\n        maxs = maxs.max(a)\n    return (mins, maxs)",
            "def channel_minmax(self, input, axis=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Finds the min/max of inputs associated with a specific channel\\n        '\n    size_of_tensor_dim = input.ndim\n    axis_list = list(range(size_of_tensor_dim))\n    axis_list.remove(axis)\n    axis_list.sort(reverse=True)\n    mins = input.copy()\n    maxs = input.copy()\n    for a in axis_list:\n        mins = mins.min(a)\n        maxs = maxs.max(a)\n    return (mins, maxs)",
            "def channel_minmax(self, input, axis=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Finds the min/max of inputs associated with a specific channel\\n        '\n    size_of_tensor_dim = input.ndim\n    axis_list = list(range(size_of_tensor_dim))\n    axis_list.remove(axis)\n    axis_list.sort(reverse=True)\n    mins = input.copy()\n    maxs = input.copy()\n    for a in axis_list:\n        mins = mins.min(a)\n        maxs = maxs.max(a)\n    return (mins, maxs)",
            "def channel_minmax(self, input, axis=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Finds the min/max of inputs associated with a specific channel\\n        '\n    size_of_tensor_dim = input.ndim\n    axis_list = list(range(size_of_tensor_dim))\n    axis_list.remove(axis)\n    axis_list.sort(reverse=True)\n    mins = input.copy()\n    maxs = input.copy()\n    for a in axis_list:\n        mins = mins.min(a)\n        maxs = maxs.max(a)\n    return (mins, maxs)",
            "def channel_minmax(self, input, axis=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Finds the min/max of inputs associated with a specific channel\\n        '\n    size_of_tensor_dim = input.ndim\n    axis_list = list(range(size_of_tensor_dim))\n    axis_list.remove(axis)\n    axis_list.sort(reverse=True)\n    mins = input.copy()\n    maxs = input.copy()\n    for a in axis_list:\n        mins = mins.min(a)\n        maxs = maxs.max(a)\n    return (mins, maxs)"
        ]
    },
    {
        "func_name": "test_input_weight_eq_observer",
        "original": "@given(ndim=st.sampled_from((2, 3, 4, 5)), input_qdtype=st.sampled_from((torch.qint8, torch.quint8)), input_qscheme=st.sampled_from((torch.per_tensor_affine, torch.per_tensor_symmetric)), weight_qdtype=st.sampled_from((torch.qint8, torch.quint8)), weight_qscheme=st.sampled_from((torch.per_channel_affine, torch.per_channel_symmetric, torch.per_channel_affine_float_qparams)))\ndef test_input_weight_eq_observer(self, ndim, input_qdtype, input_qscheme, weight_qdtype, weight_qscheme):\n    sizes = []\n    for _ in range((ndim - 1) * 2):\n        sizes.append(np.random.randint(2, 10))\n    channel = np.random.randint(1, 10)\n    if ndim == 2:\n        x = np.random.random(size=(sizes[0], channel))\n        w = np.random.random(size=(sizes[1], channel))\n    elif ndim == 3:\n        x = np.random.random(size=(sizes[0], channel, sizes[1]))\n        w = np.random.random(size=(sizes[2], channel, sizes[3]))\n    elif ndim == 4:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2]))\n        w = np.random.random(size=(sizes[3], channel, sizes[4], sizes[5]))\n    elif ndim == 5:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2], sizes[3]))\n        w = np.random.random(size=(sizes[4], channel, sizes[5], sizes[6], sizes[7]))\n    x = (x * 10).round(decimals=2).astype(np.float32)\n    w = (w * 10).round(decimals=2).astype(np.float32)\n    input_eq_obs = _InputEqualizationObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    weight_eq_obs = _WeightEqualizationObserver(dtype=weight_qdtype, qscheme=weight_qscheme)\n    ret_x = input_eq_obs(torch.tensor(x))\n    ret_w = weight_eq_obs(torch.tensor(w))\n    self.assertEqual((ret_x, ret_w), (x, w))\n    (ref_min_inputs, ref_max_inputs) = self.channel_minmax(x)\n    (min_inputs, max_inputs) = input_eq_obs.get_input_minmax()\n    self.assertEqual(min_inputs, torch.tensor(ref_min_inputs, dtype=torch.float32))\n    self.assertEqual(max_inputs, torch.tensor(ref_max_inputs, dtype=torch.float32))\n    (ref_min_weights_col, ref_max_weights_col) = self.channel_minmax(w)\n    (min_weights_col, max_weights_col) = weight_eq_obs.get_weight_col_minmax()\n    self.assertEqual(min_weights_col, torch.tensor(ref_min_weights_col, dtype=torch.float32))\n    self.assertEqual(max_weights_col, torch.tensor(ref_max_weights_col, dtype=torch.float32))\n    equalization_scale = calculate_equalization_scale(input_eq_obs, weight_eq_obs)\n    ref_equalization_scale = np.sqrt((ref_max_weights_col - ref_min_weights_col) / (ref_max_inputs - ref_min_inputs))\n    self.assertEqual(equalization_scale, torch.tensor(ref_equalization_scale, dtype=torch.float32))\n    input_eq_obs.set_equalization_scale(equalization_scale)\n    weight_eq_obs.set_equalization_scale(equalization_scale)\n    (min_input_scaled, max_input_scaled) = input_eq_obs.calculate_scaled_minmax()\n    input_quant_obs = MinMaxObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    input_quant_obs.min_val = min_input_scaled\n    input_quant_obs.max_val = max_input_scaled\n    input_qparams = input_quant_obs.calculate_qparams()\n    ref_min_input_scaled = np.min(ref_min_inputs * ref_equalization_scale)\n    ref_min_input_scaled = min(0, ref_min_input_scaled)\n    ref_max_input_scaled = np.max(ref_max_inputs * ref_equalization_scale)\n    ref_max_input_scaled = max(0, ref_max_input_scaled)\n    if input_qscheme == torch.per_tensor_symmetric:\n        ref_scale = 2 * max(abs(ref_min_input_scaled), ref_max_input_scaled) / 255\n        ref_zero_point = 0 if input_qdtype is torch.qint8 else 128\n    else:\n        ref_scale = (ref_max_input_scaled - ref_min_input_scaled) / 255\n        quant_min = -128 if input_qdtype is torch.qint8 else 0\n        quant_max = 127 if input_qdtype is torch.qint8 else 255\n        ref_zero_point = quant_min - np.round(ref_min_input_scaled / ref_scale)\n        np.clip(ref_zero_point, quant_min, quant_max)\n    self.assertEqual(input_qparams[0].item(), ref_scale, atol=1e-05, rtol=0)\n    self.assertEqual(input_qparams[1].item(), ref_zero_point)\n    weight_quant_obs = PerChannelMinMaxObserver(ch_axis=1, dtype=weight_qdtype, qscheme=weight_qscheme)\n    new_shape = [1] * w.ndim\n    new_shape[1] = w.shape[1]\n    ref_w_scaled = w * np.reciprocal(ref_equalization_scale.reshape(tuple(new_shape)))\n    w = torch.tensor(w)\n    new_shape[1] = w.size(1)\n    w_scaled = torch.mul(w, torch.reciprocal(equalization_scale.view(new_shape)))\n    self.assertEqual(w_scaled, ref_w_scaled)\n    weight_quant_obs(w_scaled)\n    (ref_min_weights_scaled, ref_max_weights_scaled) = self.channel_minmax(ref_w_scaled)\n    self.assertEqual(weight_quant_obs.min_val, torch.tensor(ref_min_weights_scaled, dtype=torch.float32))\n    self.assertEqual(weight_quant_obs.max_val, torch.tensor(ref_max_weights_scaled, dtype=torch.float32))\n    weight_qparams = weight_quant_obs.calculate_qparams()\n    if weight_qscheme == torch.per_channel_symmetric:\n        ref_min_weights_scaled = np.minimum(np.zeros(ref_min_weights_scaled.shape), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros(ref_max_weights_scaled.shape), ref_max_weights_scaled)\n        ref_scales = 2 * np.maximum(np.abs(ref_min_weights_scaled), ref_max_weights_scaled) / 255\n        ref_zero_points = np.zeros_like(ref_scales) if weight_qdtype is torch.qint8 else np.ones_like(ref_scales) * 128\n    elif weight_qscheme == torch.per_channel_affine_float_qparams:\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_scales = np.where(ref_scales > 1e-07, ref_scales, np.ones_like(ref_scales))\n        ref_zero_points = -1 * ref_min_weights_scaled / ref_scales\n    else:\n        ref_min_weights_scaled = np.minimum(np.zeros_like(ref_min_weights_scaled), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros_like(ref_max_weights_scaled), ref_max_weights_scaled)\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_zero_points = -128 if weight_qdtype is torch.qint8 else 0\n        ref_zero_points = ref_zero_points - np.round(ref_min_weights_scaled / ref_scales)\n    self.assertEqual(weight_qparams[0], torch.tensor(ref_scales, dtype=weight_qparams[0].dtype), rtol=1e-05, atol=0.0001)\n    self.assertEqual(weight_qparams[1], torch.tensor(ref_zero_points, dtype=weight_qparams[1].dtype), rtol=1e-05, atol=1)",
        "mutated": [
            "@given(ndim=st.sampled_from((2, 3, 4, 5)), input_qdtype=st.sampled_from((torch.qint8, torch.quint8)), input_qscheme=st.sampled_from((torch.per_tensor_affine, torch.per_tensor_symmetric)), weight_qdtype=st.sampled_from((torch.qint8, torch.quint8)), weight_qscheme=st.sampled_from((torch.per_channel_affine, torch.per_channel_symmetric, torch.per_channel_affine_float_qparams)))\ndef test_input_weight_eq_observer(self, ndim, input_qdtype, input_qscheme, weight_qdtype, weight_qscheme):\n    if False:\n        i = 10\n    sizes = []\n    for _ in range((ndim - 1) * 2):\n        sizes.append(np.random.randint(2, 10))\n    channel = np.random.randint(1, 10)\n    if ndim == 2:\n        x = np.random.random(size=(sizes[0], channel))\n        w = np.random.random(size=(sizes[1], channel))\n    elif ndim == 3:\n        x = np.random.random(size=(sizes[0], channel, sizes[1]))\n        w = np.random.random(size=(sizes[2], channel, sizes[3]))\n    elif ndim == 4:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2]))\n        w = np.random.random(size=(sizes[3], channel, sizes[4], sizes[5]))\n    elif ndim == 5:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2], sizes[3]))\n        w = np.random.random(size=(sizes[4], channel, sizes[5], sizes[6], sizes[7]))\n    x = (x * 10).round(decimals=2).astype(np.float32)\n    w = (w * 10).round(decimals=2).astype(np.float32)\n    input_eq_obs = _InputEqualizationObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    weight_eq_obs = _WeightEqualizationObserver(dtype=weight_qdtype, qscheme=weight_qscheme)\n    ret_x = input_eq_obs(torch.tensor(x))\n    ret_w = weight_eq_obs(torch.tensor(w))\n    self.assertEqual((ret_x, ret_w), (x, w))\n    (ref_min_inputs, ref_max_inputs) = self.channel_minmax(x)\n    (min_inputs, max_inputs) = input_eq_obs.get_input_minmax()\n    self.assertEqual(min_inputs, torch.tensor(ref_min_inputs, dtype=torch.float32))\n    self.assertEqual(max_inputs, torch.tensor(ref_max_inputs, dtype=torch.float32))\n    (ref_min_weights_col, ref_max_weights_col) = self.channel_minmax(w)\n    (min_weights_col, max_weights_col) = weight_eq_obs.get_weight_col_minmax()\n    self.assertEqual(min_weights_col, torch.tensor(ref_min_weights_col, dtype=torch.float32))\n    self.assertEqual(max_weights_col, torch.tensor(ref_max_weights_col, dtype=torch.float32))\n    equalization_scale = calculate_equalization_scale(input_eq_obs, weight_eq_obs)\n    ref_equalization_scale = np.sqrt((ref_max_weights_col - ref_min_weights_col) / (ref_max_inputs - ref_min_inputs))\n    self.assertEqual(equalization_scale, torch.tensor(ref_equalization_scale, dtype=torch.float32))\n    input_eq_obs.set_equalization_scale(equalization_scale)\n    weight_eq_obs.set_equalization_scale(equalization_scale)\n    (min_input_scaled, max_input_scaled) = input_eq_obs.calculate_scaled_minmax()\n    input_quant_obs = MinMaxObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    input_quant_obs.min_val = min_input_scaled\n    input_quant_obs.max_val = max_input_scaled\n    input_qparams = input_quant_obs.calculate_qparams()\n    ref_min_input_scaled = np.min(ref_min_inputs * ref_equalization_scale)\n    ref_min_input_scaled = min(0, ref_min_input_scaled)\n    ref_max_input_scaled = np.max(ref_max_inputs * ref_equalization_scale)\n    ref_max_input_scaled = max(0, ref_max_input_scaled)\n    if input_qscheme == torch.per_tensor_symmetric:\n        ref_scale = 2 * max(abs(ref_min_input_scaled), ref_max_input_scaled) / 255\n        ref_zero_point = 0 if input_qdtype is torch.qint8 else 128\n    else:\n        ref_scale = (ref_max_input_scaled - ref_min_input_scaled) / 255\n        quant_min = -128 if input_qdtype is torch.qint8 else 0\n        quant_max = 127 if input_qdtype is torch.qint8 else 255\n        ref_zero_point = quant_min - np.round(ref_min_input_scaled / ref_scale)\n        np.clip(ref_zero_point, quant_min, quant_max)\n    self.assertEqual(input_qparams[0].item(), ref_scale, atol=1e-05, rtol=0)\n    self.assertEqual(input_qparams[1].item(), ref_zero_point)\n    weight_quant_obs = PerChannelMinMaxObserver(ch_axis=1, dtype=weight_qdtype, qscheme=weight_qscheme)\n    new_shape = [1] * w.ndim\n    new_shape[1] = w.shape[1]\n    ref_w_scaled = w * np.reciprocal(ref_equalization_scale.reshape(tuple(new_shape)))\n    w = torch.tensor(w)\n    new_shape[1] = w.size(1)\n    w_scaled = torch.mul(w, torch.reciprocal(equalization_scale.view(new_shape)))\n    self.assertEqual(w_scaled, ref_w_scaled)\n    weight_quant_obs(w_scaled)\n    (ref_min_weights_scaled, ref_max_weights_scaled) = self.channel_minmax(ref_w_scaled)\n    self.assertEqual(weight_quant_obs.min_val, torch.tensor(ref_min_weights_scaled, dtype=torch.float32))\n    self.assertEqual(weight_quant_obs.max_val, torch.tensor(ref_max_weights_scaled, dtype=torch.float32))\n    weight_qparams = weight_quant_obs.calculate_qparams()\n    if weight_qscheme == torch.per_channel_symmetric:\n        ref_min_weights_scaled = np.minimum(np.zeros(ref_min_weights_scaled.shape), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros(ref_max_weights_scaled.shape), ref_max_weights_scaled)\n        ref_scales = 2 * np.maximum(np.abs(ref_min_weights_scaled), ref_max_weights_scaled) / 255\n        ref_zero_points = np.zeros_like(ref_scales) if weight_qdtype is torch.qint8 else np.ones_like(ref_scales) * 128\n    elif weight_qscheme == torch.per_channel_affine_float_qparams:\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_scales = np.where(ref_scales > 1e-07, ref_scales, np.ones_like(ref_scales))\n        ref_zero_points = -1 * ref_min_weights_scaled / ref_scales\n    else:\n        ref_min_weights_scaled = np.minimum(np.zeros_like(ref_min_weights_scaled), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros_like(ref_max_weights_scaled), ref_max_weights_scaled)\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_zero_points = -128 if weight_qdtype is torch.qint8 else 0\n        ref_zero_points = ref_zero_points - np.round(ref_min_weights_scaled / ref_scales)\n    self.assertEqual(weight_qparams[0], torch.tensor(ref_scales, dtype=weight_qparams[0].dtype), rtol=1e-05, atol=0.0001)\n    self.assertEqual(weight_qparams[1], torch.tensor(ref_zero_points, dtype=weight_qparams[1].dtype), rtol=1e-05, atol=1)",
            "@given(ndim=st.sampled_from((2, 3, 4, 5)), input_qdtype=st.sampled_from((torch.qint8, torch.quint8)), input_qscheme=st.sampled_from((torch.per_tensor_affine, torch.per_tensor_symmetric)), weight_qdtype=st.sampled_from((torch.qint8, torch.quint8)), weight_qscheme=st.sampled_from((torch.per_channel_affine, torch.per_channel_symmetric, torch.per_channel_affine_float_qparams)))\ndef test_input_weight_eq_observer(self, ndim, input_qdtype, input_qscheme, weight_qdtype, weight_qscheme):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    sizes = []\n    for _ in range((ndim - 1) * 2):\n        sizes.append(np.random.randint(2, 10))\n    channel = np.random.randint(1, 10)\n    if ndim == 2:\n        x = np.random.random(size=(sizes[0], channel))\n        w = np.random.random(size=(sizes[1], channel))\n    elif ndim == 3:\n        x = np.random.random(size=(sizes[0], channel, sizes[1]))\n        w = np.random.random(size=(sizes[2], channel, sizes[3]))\n    elif ndim == 4:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2]))\n        w = np.random.random(size=(sizes[3], channel, sizes[4], sizes[5]))\n    elif ndim == 5:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2], sizes[3]))\n        w = np.random.random(size=(sizes[4], channel, sizes[5], sizes[6], sizes[7]))\n    x = (x * 10).round(decimals=2).astype(np.float32)\n    w = (w * 10).round(decimals=2).astype(np.float32)\n    input_eq_obs = _InputEqualizationObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    weight_eq_obs = _WeightEqualizationObserver(dtype=weight_qdtype, qscheme=weight_qscheme)\n    ret_x = input_eq_obs(torch.tensor(x))\n    ret_w = weight_eq_obs(torch.tensor(w))\n    self.assertEqual((ret_x, ret_w), (x, w))\n    (ref_min_inputs, ref_max_inputs) = self.channel_minmax(x)\n    (min_inputs, max_inputs) = input_eq_obs.get_input_minmax()\n    self.assertEqual(min_inputs, torch.tensor(ref_min_inputs, dtype=torch.float32))\n    self.assertEqual(max_inputs, torch.tensor(ref_max_inputs, dtype=torch.float32))\n    (ref_min_weights_col, ref_max_weights_col) = self.channel_minmax(w)\n    (min_weights_col, max_weights_col) = weight_eq_obs.get_weight_col_minmax()\n    self.assertEqual(min_weights_col, torch.tensor(ref_min_weights_col, dtype=torch.float32))\n    self.assertEqual(max_weights_col, torch.tensor(ref_max_weights_col, dtype=torch.float32))\n    equalization_scale = calculate_equalization_scale(input_eq_obs, weight_eq_obs)\n    ref_equalization_scale = np.sqrt((ref_max_weights_col - ref_min_weights_col) / (ref_max_inputs - ref_min_inputs))\n    self.assertEqual(equalization_scale, torch.tensor(ref_equalization_scale, dtype=torch.float32))\n    input_eq_obs.set_equalization_scale(equalization_scale)\n    weight_eq_obs.set_equalization_scale(equalization_scale)\n    (min_input_scaled, max_input_scaled) = input_eq_obs.calculate_scaled_minmax()\n    input_quant_obs = MinMaxObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    input_quant_obs.min_val = min_input_scaled\n    input_quant_obs.max_val = max_input_scaled\n    input_qparams = input_quant_obs.calculate_qparams()\n    ref_min_input_scaled = np.min(ref_min_inputs * ref_equalization_scale)\n    ref_min_input_scaled = min(0, ref_min_input_scaled)\n    ref_max_input_scaled = np.max(ref_max_inputs * ref_equalization_scale)\n    ref_max_input_scaled = max(0, ref_max_input_scaled)\n    if input_qscheme == torch.per_tensor_symmetric:\n        ref_scale = 2 * max(abs(ref_min_input_scaled), ref_max_input_scaled) / 255\n        ref_zero_point = 0 if input_qdtype is torch.qint8 else 128\n    else:\n        ref_scale = (ref_max_input_scaled - ref_min_input_scaled) / 255\n        quant_min = -128 if input_qdtype is torch.qint8 else 0\n        quant_max = 127 if input_qdtype is torch.qint8 else 255\n        ref_zero_point = quant_min - np.round(ref_min_input_scaled / ref_scale)\n        np.clip(ref_zero_point, quant_min, quant_max)\n    self.assertEqual(input_qparams[0].item(), ref_scale, atol=1e-05, rtol=0)\n    self.assertEqual(input_qparams[1].item(), ref_zero_point)\n    weight_quant_obs = PerChannelMinMaxObserver(ch_axis=1, dtype=weight_qdtype, qscheme=weight_qscheme)\n    new_shape = [1] * w.ndim\n    new_shape[1] = w.shape[1]\n    ref_w_scaled = w * np.reciprocal(ref_equalization_scale.reshape(tuple(new_shape)))\n    w = torch.tensor(w)\n    new_shape[1] = w.size(1)\n    w_scaled = torch.mul(w, torch.reciprocal(equalization_scale.view(new_shape)))\n    self.assertEqual(w_scaled, ref_w_scaled)\n    weight_quant_obs(w_scaled)\n    (ref_min_weights_scaled, ref_max_weights_scaled) = self.channel_minmax(ref_w_scaled)\n    self.assertEqual(weight_quant_obs.min_val, torch.tensor(ref_min_weights_scaled, dtype=torch.float32))\n    self.assertEqual(weight_quant_obs.max_val, torch.tensor(ref_max_weights_scaled, dtype=torch.float32))\n    weight_qparams = weight_quant_obs.calculate_qparams()\n    if weight_qscheme == torch.per_channel_symmetric:\n        ref_min_weights_scaled = np.minimum(np.zeros(ref_min_weights_scaled.shape), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros(ref_max_weights_scaled.shape), ref_max_weights_scaled)\n        ref_scales = 2 * np.maximum(np.abs(ref_min_weights_scaled), ref_max_weights_scaled) / 255\n        ref_zero_points = np.zeros_like(ref_scales) if weight_qdtype is torch.qint8 else np.ones_like(ref_scales) * 128\n    elif weight_qscheme == torch.per_channel_affine_float_qparams:\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_scales = np.where(ref_scales > 1e-07, ref_scales, np.ones_like(ref_scales))\n        ref_zero_points = -1 * ref_min_weights_scaled / ref_scales\n    else:\n        ref_min_weights_scaled = np.minimum(np.zeros_like(ref_min_weights_scaled), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros_like(ref_max_weights_scaled), ref_max_weights_scaled)\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_zero_points = -128 if weight_qdtype is torch.qint8 else 0\n        ref_zero_points = ref_zero_points - np.round(ref_min_weights_scaled / ref_scales)\n    self.assertEqual(weight_qparams[0], torch.tensor(ref_scales, dtype=weight_qparams[0].dtype), rtol=1e-05, atol=0.0001)\n    self.assertEqual(weight_qparams[1], torch.tensor(ref_zero_points, dtype=weight_qparams[1].dtype), rtol=1e-05, atol=1)",
            "@given(ndim=st.sampled_from((2, 3, 4, 5)), input_qdtype=st.sampled_from((torch.qint8, torch.quint8)), input_qscheme=st.sampled_from((torch.per_tensor_affine, torch.per_tensor_symmetric)), weight_qdtype=st.sampled_from((torch.qint8, torch.quint8)), weight_qscheme=st.sampled_from((torch.per_channel_affine, torch.per_channel_symmetric, torch.per_channel_affine_float_qparams)))\ndef test_input_weight_eq_observer(self, ndim, input_qdtype, input_qscheme, weight_qdtype, weight_qscheme):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    sizes = []\n    for _ in range((ndim - 1) * 2):\n        sizes.append(np.random.randint(2, 10))\n    channel = np.random.randint(1, 10)\n    if ndim == 2:\n        x = np.random.random(size=(sizes[0], channel))\n        w = np.random.random(size=(sizes[1], channel))\n    elif ndim == 3:\n        x = np.random.random(size=(sizes[0], channel, sizes[1]))\n        w = np.random.random(size=(sizes[2], channel, sizes[3]))\n    elif ndim == 4:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2]))\n        w = np.random.random(size=(sizes[3], channel, sizes[4], sizes[5]))\n    elif ndim == 5:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2], sizes[3]))\n        w = np.random.random(size=(sizes[4], channel, sizes[5], sizes[6], sizes[7]))\n    x = (x * 10).round(decimals=2).astype(np.float32)\n    w = (w * 10).round(decimals=2).astype(np.float32)\n    input_eq_obs = _InputEqualizationObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    weight_eq_obs = _WeightEqualizationObserver(dtype=weight_qdtype, qscheme=weight_qscheme)\n    ret_x = input_eq_obs(torch.tensor(x))\n    ret_w = weight_eq_obs(torch.tensor(w))\n    self.assertEqual((ret_x, ret_w), (x, w))\n    (ref_min_inputs, ref_max_inputs) = self.channel_minmax(x)\n    (min_inputs, max_inputs) = input_eq_obs.get_input_minmax()\n    self.assertEqual(min_inputs, torch.tensor(ref_min_inputs, dtype=torch.float32))\n    self.assertEqual(max_inputs, torch.tensor(ref_max_inputs, dtype=torch.float32))\n    (ref_min_weights_col, ref_max_weights_col) = self.channel_minmax(w)\n    (min_weights_col, max_weights_col) = weight_eq_obs.get_weight_col_minmax()\n    self.assertEqual(min_weights_col, torch.tensor(ref_min_weights_col, dtype=torch.float32))\n    self.assertEqual(max_weights_col, torch.tensor(ref_max_weights_col, dtype=torch.float32))\n    equalization_scale = calculate_equalization_scale(input_eq_obs, weight_eq_obs)\n    ref_equalization_scale = np.sqrt((ref_max_weights_col - ref_min_weights_col) / (ref_max_inputs - ref_min_inputs))\n    self.assertEqual(equalization_scale, torch.tensor(ref_equalization_scale, dtype=torch.float32))\n    input_eq_obs.set_equalization_scale(equalization_scale)\n    weight_eq_obs.set_equalization_scale(equalization_scale)\n    (min_input_scaled, max_input_scaled) = input_eq_obs.calculate_scaled_minmax()\n    input_quant_obs = MinMaxObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    input_quant_obs.min_val = min_input_scaled\n    input_quant_obs.max_val = max_input_scaled\n    input_qparams = input_quant_obs.calculate_qparams()\n    ref_min_input_scaled = np.min(ref_min_inputs * ref_equalization_scale)\n    ref_min_input_scaled = min(0, ref_min_input_scaled)\n    ref_max_input_scaled = np.max(ref_max_inputs * ref_equalization_scale)\n    ref_max_input_scaled = max(0, ref_max_input_scaled)\n    if input_qscheme == torch.per_tensor_symmetric:\n        ref_scale = 2 * max(abs(ref_min_input_scaled), ref_max_input_scaled) / 255\n        ref_zero_point = 0 if input_qdtype is torch.qint8 else 128\n    else:\n        ref_scale = (ref_max_input_scaled - ref_min_input_scaled) / 255\n        quant_min = -128 if input_qdtype is torch.qint8 else 0\n        quant_max = 127 if input_qdtype is torch.qint8 else 255\n        ref_zero_point = quant_min - np.round(ref_min_input_scaled / ref_scale)\n        np.clip(ref_zero_point, quant_min, quant_max)\n    self.assertEqual(input_qparams[0].item(), ref_scale, atol=1e-05, rtol=0)\n    self.assertEqual(input_qparams[1].item(), ref_zero_point)\n    weight_quant_obs = PerChannelMinMaxObserver(ch_axis=1, dtype=weight_qdtype, qscheme=weight_qscheme)\n    new_shape = [1] * w.ndim\n    new_shape[1] = w.shape[1]\n    ref_w_scaled = w * np.reciprocal(ref_equalization_scale.reshape(tuple(new_shape)))\n    w = torch.tensor(w)\n    new_shape[1] = w.size(1)\n    w_scaled = torch.mul(w, torch.reciprocal(equalization_scale.view(new_shape)))\n    self.assertEqual(w_scaled, ref_w_scaled)\n    weight_quant_obs(w_scaled)\n    (ref_min_weights_scaled, ref_max_weights_scaled) = self.channel_minmax(ref_w_scaled)\n    self.assertEqual(weight_quant_obs.min_val, torch.tensor(ref_min_weights_scaled, dtype=torch.float32))\n    self.assertEqual(weight_quant_obs.max_val, torch.tensor(ref_max_weights_scaled, dtype=torch.float32))\n    weight_qparams = weight_quant_obs.calculate_qparams()\n    if weight_qscheme == torch.per_channel_symmetric:\n        ref_min_weights_scaled = np.minimum(np.zeros(ref_min_weights_scaled.shape), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros(ref_max_weights_scaled.shape), ref_max_weights_scaled)\n        ref_scales = 2 * np.maximum(np.abs(ref_min_weights_scaled), ref_max_weights_scaled) / 255\n        ref_zero_points = np.zeros_like(ref_scales) if weight_qdtype is torch.qint8 else np.ones_like(ref_scales) * 128\n    elif weight_qscheme == torch.per_channel_affine_float_qparams:\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_scales = np.where(ref_scales > 1e-07, ref_scales, np.ones_like(ref_scales))\n        ref_zero_points = -1 * ref_min_weights_scaled / ref_scales\n    else:\n        ref_min_weights_scaled = np.minimum(np.zeros_like(ref_min_weights_scaled), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros_like(ref_max_weights_scaled), ref_max_weights_scaled)\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_zero_points = -128 if weight_qdtype is torch.qint8 else 0\n        ref_zero_points = ref_zero_points - np.round(ref_min_weights_scaled / ref_scales)\n    self.assertEqual(weight_qparams[0], torch.tensor(ref_scales, dtype=weight_qparams[0].dtype), rtol=1e-05, atol=0.0001)\n    self.assertEqual(weight_qparams[1], torch.tensor(ref_zero_points, dtype=weight_qparams[1].dtype), rtol=1e-05, atol=1)",
            "@given(ndim=st.sampled_from((2, 3, 4, 5)), input_qdtype=st.sampled_from((torch.qint8, torch.quint8)), input_qscheme=st.sampled_from((torch.per_tensor_affine, torch.per_tensor_symmetric)), weight_qdtype=st.sampled_from((torch.qint8, torch.quint8)), weight_qscheme=st.sampled_from((torch.per_channel_affine, torch.per_channel_symmetric, torch.per_channel_affine_float_qparams)))\ndef test_input_weight_eq_observer(self, ndim, input_qdtype, input_qscheme, weight_qdtype, weight_qscheme):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    sizes = []\n    for _ in range((ndim - 1) * 2):\n        sizes.append(np.random.randint(2, 10))\n    channel = np.random.randint(1, 10)\n    if ndim == 2:\n        x = np.random.random(size=(sizes[0], channel))\n        w = np.random.random(size=(sizes[1], channel))\n    elif ndim == 3:\n        x = np.random.random(size=(sizes[0], channel, sizes[1]))\n        w = np.random.random(size=(sizes[2], channel, sizes[3]))\n    elif ndim == 4:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2]))\n        w = np.random.random(size=(sizes[3], channel, sizes[4], sizes[5]))\n    elif ndim == 5:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2], sizes[3]))\n        w = np.random.random(size=(sizes[4], channel, sizes[5], sizes[6], sizes[7]))\n    x = (x * 10).round(decimals=2).astype(np.float32)\n    w = (w * 10).round(decimals=2).astype(np.float32)\n    input_eq_obs = _InputEqualizationObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    weight_eq_obs = _WeightEqualizationObserver(dtype=weight_qdtype, qscheme=weight_qscheme)\n    ret_x = input_eq_obs(torch.tensor(x))\n    ret_w = weight_eq_obs(torch.tensor(w))\n    self.assertEqual((ret_x, ret_w), (x, w))\n    (ref_min_inputs, ref_max_inputs) = self.channel_minmax(x)\n    (min_inputs, max_inputs) = input_eq_obs.get_input_minmax()\n    self.assertEqual(min_inputs, torch.tensor(ref_min_inputs, dtype=torch.float32))\n    self.assertEqual(max_inputs, torch.tensor(ref_max_inputs, dtype=torch.float32))\n    (ref_min_weights_col, ref_max_weights_col) = self.channel_minmax(w)\n    (min_weights_col, max_weights_col) = weight_eq_obs.get_weight_col_minmax()\n    self.assertEqual(min_weights_col, torch.tensor(ref_min_weights_col, dtype=torch.float32))\n    self.assertEqual(max_weights_col, torch.tensor(ref_max_weights_col, dtype=torch.float32))\n    equalization_scale = calculate_equalization_scale(input_eq_obs, weight_eq_obs)\n    ref_equalization_scale = np.sqrt((ref_max_weights_col - ref_min_weights_col) / (ref_max_inputs - ref_min_inputs))\n    self.assertEqual(equalization_scale, torch.tensor(ref_equalization_scale, dtype=torch.float32))\n    input_eq_obs.set_equalization_scale(equalization_scale)\n    weight_eq_obs.set_equalization_scale(equalization_scale)\n    (min_input_scaled, max_input_scaled) = input_eq_obs.calculate_scaled_minmax()\n    input_quant_obs = MinMaxObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    input_quant_obs.min_val = min_input_scaled\n    input_quant_obs.max_val = max_input_scaled\n    input_qparams = input_quant_obs.calculate_qparams()\n    ref_min_input_scaled = np.min(ref_min_inputs * ref_equalization_scale)\n    ref_min_input_scaled = min(0, ref_min_input_scaled)\n    ref_max_input_scaled = np.max(ref_max_inputs * ref_equalization_scale)\n    ref_max_input_scaled = max(0, ref_max_input_scaled)\n    if input_qscheme == torch.per_tensor_symmetric:\n        ref_scale = 2 * max(abs(ref_min_input_scaled), ref_max_input_scaled) / 255\n        ref_zero_point = 0 if input_qdtype is torch.qint8 else 128\n    else:\n        ref_scale = (ref_max_input_scaled - ref_min_input_scaled) / 255\n        quant_min = -128 if input_qdtype is torch.qint8 else 0\n        quant_max = 127 if input_qdtype is torch.qint8 else 255\n        ref_zero_point = quant_min - np.round(ref_min_input_scaled / ref_scale)\n        np.clip(ref_zero_point, quant_min, quant_max)\n    self.assertEqual(input_qparams[0].item(), ref_scale, atol=1e-05, rtol=0)\n    self.assertEqual(input_qparams[1].item(), ref_zero_point)\n    weight_quant_obs = PerChannelMinMaxObserver(ch_axis=1, dtype=weight_qdtype, qscheme=weight_qscheme)\n    new_shape = [1] * w.ndim\n    new_shape[1] = w.shape[1]\n    ref_w_scaled = w * np.reciprocal(ref_equalization_scale.reshape(tuple(new_shape)))\n    w = torch.tensor(w)\n    new_shape[1] = w.size(1)\n    w_scaled = torch.mul(w, torch.reciprocal(equalization_scale.view(new_shape)))\n    self.assertEqual(w_scaled, ref_w_scaled)\n    weight_quant_obs(w_scaled)\n    (ref_min_weights_scaled, ref_max_weights_scaled) = self.channel_minmax(ref_w_scaled)\n    self.assertEqual(weight_quant_obs.min_val, torch.tensor(ref_min_weights_scaled, dtype=torch.float32))\n    self.assertEqual(weight_quant_obs.max_val, torch.tensor(ref_max_weights_scaled, dtype=torch.float32))\n    weight_qparams = weight_quant_obs.calculate_qparams()\n    if weight_qscheme == torch.per_channel_symmetric:\n        ref_min_weights_scaled = np.minimum(np.zeros(ref_min_weights_scaled.shape), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros(ref_max_weights_scaled.shape), ref_max_weights_scaled)\n        ref_scales = 2 * np.maximum(np.abs(ref_min_weights_scaled), ref_max_weights_scaled) / 255\n        ref_zero_points = np.zeros_like(ref_scales) if weight_qdtype is torch.qint8 else np.ones_like(ref_scales) * 128\n    elif weight_qscheme == torch.per_channel_affine_float_qparams:\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_scales = np.where(ref_scales > 1e-07, ref_scales, np.ones_like(ref_scales))\n        ref_zero_points = -1 * ref_min_weights_scaled / ref_scales\n    else:\n        ref_min_weights_scaled = np.minimum(np.zeros_like(ref_min_weights_scaled), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros_like(ref_max_weights_scaled), ref_max_weights_scaled)\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_zero_points = -128 if weight_qdtype is torch.qint8 else 0\n        ref_zero_points = ref_zero_points - np.round(ref_min_weights_scaled / ref_scales)\n    self.assertEqual(weight_qparams[0], torch.tensor(ref_scales, dtype=weight_qparams[0].dtype), rtol=1e-05, atol=0.0001)\n    self.assertEqual(weight_qparams[1], torch.tensor(ref_zero_points, dtype=weight_qparams[1].dtype), rtol=1e-05, atol=1)",
            "@given(ndim=st.sampled_from((2, 3, 4, 5)), input_qdtype=st.sampled_from((torch.qint8, torch.quint8)), input_qscheme=st.sampled_from((torch.per_tensor_affine, torch.per_tensor_symmetric)), weight_qdtype=st.sampled_from((torch.qint8, torch.quint8)), weight_qscheme=st.sampled_from((torch.per_channel_affine, torch.per_channel_symmetric, torch.per_channel_affine_float_qparams)))\ndef test_input_weight_eq_observer(self, ndim, input_qdtype, input_qscheme, weight_qdtype, weight_qscheme):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    sizes = []\n    for _ in range((ndim - 1) * 2):\n        sizes.append(np.random.randint(2, 10))\n    channel = np.random.randint(1, 10)\n    if ndim == 2:\n        x = np.random.random(size=(sizes[0], channel))\n        w = np.random.random(size=(sizes[1], channel))\n    elif ndim == 3:\n        x = np.random.random(size=(sizes[0], channel, sizes[1]))\n        w = np.random.random(size=(sizes[2], channel, sizes[3]))\n    elif ndim == 4:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2]))\n        w = np.random.random(size=(sizes[3], channel, sizes[4], sizes[5]))\n    elif ndim == 5:\n        x = np.random.random(size=(sizes[0], channel, sizes[1], sizes[2], sizes[3]))\n        w = np.random.random(size=(sizes[4], channel, sizes[5], sizes[6], sizes[7]))\n    x = (x * 10).round(decimals=2).astype(np.float32)\n    w = (w * 10).round(decimals=2).astype(np.float32)\n    input_eq_obs = _InputEqualizationObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    weight_eq_obs = _WeightEqualizationObserver(dtype=weight_qdtype, qscheme=weight_qscheme)\n    ret_x = input_eq_obs(torch.tensor(x))\n    ret_w = weight_eq_obs(torch.tensor(w))\n    self.assertEqual((ret_x, ret_w), (x, w))\n    (ref_min_inputs, ref_max_inputs) = self.channel_minmax(x)\n    (min_inputs, max_inputs) = input_eq_obs.get_input_minmax()\n    self.assertEqual(min_inputs, torch.tensor(ref_min_inputs, dtype=torch.float32))\n    self.assertEqual(max_inputs, torch.tensor(ref_max_inputs, dtype=torch.float32))\n    (ref_min_weights_col, ref_max_weights_col) = self.channel_minmax(w)\n    (min_weights_col, max_weights_col) = weight_eq_obs.get_weight_col_minmax()\n    self.assertEqual(min_weights_col, torch.tensor(ref_min_weights_col, dtype=torch.float32))\n    self.assertEqual(max_weights_col, torch.tensor(ref_max_weights_col, dtype=torch.float32))\n    equalization_scale = calculate_equalization_scale(input_eq_obs, weight_eq_obs)\n    ref_equalization_scale = np.sqrt((ref_max_weights_col - ref_min_weights_col) / (ref_max_inputs - ref_min_inputs))\n    self.assertEqual(equalization_scale, torch.tensor(ref_equalization_scale, dtype=torch.float32))\n    input_eq_obs.set_equalization_scale(equalization_scale)\n    weight_eq_obs.set_equalization_scale(equalization_scale)\n    (min_input_scaled, max_input_scaled) = input_eq_obs.calculate_scaled_minmax()\n    input_quant_obs = MinMaxObserver(dtype=input_qdtype, qscheme=input_qscheme)\n    input_quant_obs.min_val = min_input_scaled\n    input_quant_obs.max_val = max_input_scaled\n    input_qparams = input_quant_obs.calculate_qparams()\n    ref_min_input_scaled = np.min(ref_min_inputs * ref_equalization_scale)\n    ref_min_input_scaled = min(0, ref_min_input_scaled)\n    ref_max_input_scaled = np.max(ref_max_inputs * ref_equalization_scale)\n    ref_max_input_scaled = max(0, ref_max_input_scaled)\n    if input_qscheme == torch.per_tensor_symmetric:\n        ref_scale = 2 * max(abs(ref_min_input_scaled), ref_max_input_scaled) / 255\n        ref_zero_point = 0 if input_qdtype is torch.qint8 else 128\n    else:\n        ref_scale = (ref_max_input_scaled - ref_min_input_scaled) / 255\n        quant_min = -128 if input_qdtype is torch.qint8 else 0\n        quant_max = 127 if input_qdtype is torch.qint8 else 255\n        ref_zero_point = quant_min - np.round(ref_min_input_scaled / ref_scale)\n        np.clip(ref_zero_point, quant_min, quant_max)\n    self.assertEqual(input_qparams[0].item(), ref_scale, atol=1e-05, rtol=0)\n    self.assertEqual(input_qparams[1].item(), ref_zero_point)\n    weight_quant_obs = PerChannelMinMaxObserver(ch_axis=1, dtype=weight_qdtype, qscheme=weight_qscheme)\n    new_shape = [1] * w.ndim\n    new_shape[1] = w.shape[1]\n    ref_w_scaled = w * np.reciprocal(ref_equalization_scale.reshape(tuple(new_shape)))\n    w = torch.tensor(w)\n    new_shape[1] = w.size(1)\n    w_scaled = torch.mul(w, torch.reciprocal(equalization_scale.view(new_shape)))\n    self.assertEqual(w_scaled, ref_w_scaled)\n    weight_quant_obs(w_scaled)\n    (ref_min_weights_scaled, ref_max_weights_scaled) = self.channel_minmax(ref_w_scaled)\n    self.assertEqual(weight_quant_obs.min_val, torch.tensor(ref_min_weights_scaled, dtype=torch.float32))\n    self.assertEqual(weight_quant_obs.max_val, torch.tensor(ref_max_weights_scaled, dtype=torch.float32))\n    weight_qparams = weight_quant_obs.calculate_qparams()\n    if weight_qscheme == torch.per_channel_symmetric:\n        ref_min_weights_scaled = np.minimum(np.zeros(ref_min_weights_scaled.shape), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros(ref_max_weights_scaled.shape), ref_max_weights_scaled)\n        ref_scales = 2 * np.maximum(np.abs(ref_min_weights_scaled), ref_max_weights_scaled) / 255\n        ref_zero_points = np.zeros_like(ref_scales) if weight_qdtype is torch.qint8 else np.ones_like(ref_scales) * 128\n    elif weight_qscheme == torch.per_channel_affine_float_qparams:\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_scales = np.where(ref_scales > 1e-07, ref_scales, np.ones_like(ref_scales))\n        ref_zero_points = -1 * ref_min_weights_scaled / ref_scales\n    else:\n        ref_min_weights_scaled = np.minimum(np.zeros_like(ref_min_weights_scaled), ref_min_weights_scaled)\n        ref_max_weights_scaled = np.maximum(np.zeros_like(ref_max_weights_scaled), ref_max_weights_scaled)\n        ref_scales = (ref_max_weights_scaled - ref_min_weights_scaled) / 255\n        ref_zero_points = -128 if weight_qdtype is torch.qint8 else 0\n        ref_zero_points = ref_zero_points - np.round(ref_min_weights_scaled / ref_scales)\n    self.assertEqual(weight_qparams[0], torch.tensor(ref_scales, dtype=weight_qparams[0].dtype), rtol=1e-05, atol=0.0001)\n    self.assertEqual(weight_qparams[1], torch.tensor(ref_zero_points, dtype=weight_qparams[1].dtype), rtol=1e-05, atol=1)"
        ]
    },
    {
        "func_name": "test_input_weight_equalization_prepare",
        "original": "def test_input_weight_equalization_prepare(self):\n    \"\"\" Tests that graphs created after prepare_fx is as expected\n        \"\"\"\n    single_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    two_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(MinMaxObserver): 3}\n    single_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(_WeightEqualizationObserver): 1, ns.call_module(MinMaxObserver): 3}\n    two_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 5}\n    fp_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 6}\n    tests = [(SingleLayerLinearModel, single_nn_layer_node_occurrence), (TwoLayerLinearModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalLinearModel, two_F_layer_node_occurrence), (FunctionalLinearAddModel, fp_F_layer_node_occurrence), (LinearReluModel, single_nn_layer_node_occurrence), (LinearReluLinearModel, two_nn_layer_node_occurrence), (FunctionalLinearReluModel, single_F_layer_node_occurrence), (FunctionalLinearReluLinearModel, two_F_layer_node_occurrence), (ConvModel, single_nn_layer_node_occurrence), (TwoLayerConvModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalConvModel, two_F_layer_node_occurrence), (ConvReluModel, single_nn_layer_node_occurrence), (ConvReluConvModel, two_nn_layer_node_occurrence), (FunctionalConvReluModel, single_F_layer_node_occurrence), (FunctionalConvReluConvModel, two_F_layer_node_occurrence)]\n    for (M, node_occurrence) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        self.checkGraphModuleNodes(prepared, expected_node_occurrence=node_occurrence)",
        "mutated": [
            "def test_input_weight_equalization_prepare(self):\n    if False:\n        i = 10\n    ' Tests that graphs created after prepare_fx is as expected\\n        '\n    single_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    two_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(MinMaxObserver): 3}\n    single_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(_WeightEqualizationObserver): 1, ns.call_module(MinMaxObserver): 3}\n    two_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 5}\n    fp_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 6}\n    tests = [(SingleLayerLinearModel, single_nn_layer_node_occurrence), (TwoLayerLinearModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalLinearModel, two_F_layer_node_occurrence), (FunctionalLinearAddModel, fp_F_layer_node_occurrence), (LinearReluModel, single_nn_layer_node_occurrence), (LinearReluLinearModel, two_nn_layer_node_occurrence), (FunctionalLinearReluModel, single_F_layer_node_occurrence), (FunctionalLinearReluLinearModel, two_F_layer_node_occurrence), (ConvModel, single_nn_layer_node_occurrence), (TwoLayerConvModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalConvModel, two_F_layer_node_occurrence), (ConvReluModel, single_nn_layer_node_occurrence), (ConvReluConvModel, two_nn_layer_node_occurrence), (FunctionalConvReluModel, single_F_layer_node_occurrence), (FunctionalConvReluConvModel, two_F_layer_node_occurrence)]\n    for (M, node_occurrence) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        self.checkGraphModuleNodes(prepared, expected_node_occurrence=node_occurrence)",
            "def test_input_weight_equalization_prepare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Tests that graphs created after prepare_fx is as expected\\n        '\n    single_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    two_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(MinMaxObserver): 3}\n    single_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(_WeightEqualizationObserver): 1, ns.call_module(MinMaxObserver): 3}\n    two_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 5}\n    fp_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 6}\n    tests = [(SingleLayerLinearModel, single_nn_layer_node_occurrence), (TwoLayerLinearModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalLinearModel, two_F_layer_node_occurrence), (FunctionalLinearAddModel, fp_F_layer_node_occurrence), (LinearReluModel, single_nn_layer_node_occurrence), (LinearReluLinearModel, two_nn_layer_node_occurrence), (FunctionalLinearReluModel, single_F_layer_node_occurrence), (FunctionalLinearReluLinearModel, two_F_layer_node_occurrence), (ConvModel, single_nn_layer_node_occurrence), (TwoLayerConvModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalConvModel, two_F_layer_node_occurrence), (ConvReluModel, single_nn_layer_node_occurrence), (ConvReluConvModel, two_nn_layer_node_occurrence), (FunctionalConvReluModel, single_F_layer_node_occurrence), (FunctionalConvReluConvModel, two_F_layer_node_occurrence)]\n    for (M, node_occurrence) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        self.checkGraphModuleNodes(prepared, expected_node_occurrence=node_occurrence)",
            "def test_input_weight_equalization_prepare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Tests that graphs created after prepare_fx is as expected\\n        '\n    single_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    two_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(MinMaxObserver): 3}\n    single_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(_WeightEqualizationObserver): 1, ns.call_module(MinMaxObserver): 3}\n    two_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 5}\n    fp_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 6}\n    tests = [(SingleLayerLinearModel, single_nn_layer_node_occurrence), (TwoLayerLinearModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalLinearModel, two_F_layer_node_occurrence), (FunctionalLinearAddModel, fp_F_layer_node_occurrence), (LinearReluModel, single_nn_layer_node_occurrence), (LinearReluLinearModel, two_nn_layer_node_occurrence), (FunctionalLinearReluModel, single_F_layer_node_occurrence), (FunctionalLinearReluLinearModel, two_F_layer_node_occurrence), (ConvModel, single_nn_layer_node_occurrence), (TwoLayerConvModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalConvModel, two_F_layer_node_occurrence), (ConvReluModel, single_nn_layer_node_occurrence), (ConvReluConvModel, two_nn_layer_node_occurrence), (FunctionalConvReluModel, single_F_layer_node_occurrence), (FunctionalConvReluConvModel, two_F_layer_node_occurrence)]\n    for (M, node_occurrence) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        self.checkGraphModuleNodes(prepared, expected_node_occurrence=node_occurrence)",
            "def test_input_weight_equalization_prepare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Tests that graphs created after prepare_fx is as expected\\n        '\n    single_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    two_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(MinMaxObserver): 3}\n    single_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(_WeightEqualizationObserver): 1, ns.call_module(MinMaxObserver): 3}\n    two_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 5}\n    fp_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 6}\n    tests = [(SingleLayerLinearModel, single_nn_layer_node_occurrence), (TwoLayerLinearModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalLinearModel, two_F_layer_node_occurrence), (FunctionalLinearAddModel, fp_F_layer_node_occurrence), (LinearReluModel, single_nn_layer_node_occurrence), (LinearReluLinearModel, two_nn_layer_node_occurrence), (FunctionalLinearReluModel, single_F_layer_node_occurrence), (FunctionalLinearReluLinearModel, two_F_layer_node_occurrence), (ConvModel, single_nn_layer_node_occurrence), (TwoLayerConvModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalConvModel, two_F_layer_node_occurrence), (ConvReluModel, single_nn_layer_node_occurrence), (ConvReluConvModel, two_nn_layer_node_occurrence), (FunctionalConvReluModel, single_F_layer_node_occurrence), (FunctionalConvReluConvModel, two_F_layer_node_occurrence)]\n    for (M, node_occurrence) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        self.checkGraphModuleNodes(prepared, expected_node_occurrence=node_occurrence)",
            "def test_input_weight_equalization_prepare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Tests that graphs created after prepare_fx is as expected\\n        '\n    single_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    two_nn_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(MinMaxObserver): 3}\n    single_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(_WeightEqualizationObserver): 1, ns.call_module(MinMaxObserver): 3}\n    two_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 5}\n    fp_F_layer_node_occurrence = {ns.call_module(_InputEqualizationObserver): 2, ns.call_module(_WeightEqualizationObserver): 2, ns.call_module(MinMaxObserver): 6}\n    tests = [(SingleLayerLinearModel, single_nn_layer_node_occurrence), (TwoLayerLinearModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalLinearModel, two_F_layer_node_occurrence), (FunctionalLinearAddModel, fp_F_layer_node_occurrence), (LinearReluModel, single_nn_layer_node_occurrence), (LinearReluLinearModel, two_nn_layer_node_occurrence), (FunctionalLinearReluModel, single_F_layer_node_occurrence), (FunctionalLinearReluLinearModel, two_F_layer_node_occurrence), (ConvModel, single_nn_layer_node_occurrence), (TwoLayerConvModel, two_nn_layer_node_occurrence), (TwoLayerFunctionalConvModel, two_F_layer_node_occurrence), (ConvReluModel, single_nn_layer_node_occurrence), (ConvReluConvModel, two_nn_layer_node_occurrence), (FunctionalConvReluModel, single_F_layer_node_occurrence), (FunctionalConvReluConvModel, two_F_layer_node_occurrence)]\n    for (M, node_occurrence) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        self.checkGraphModuleNodes(prepared, expected_node_occurrence=node_occurrence)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self) -> None:\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)\n    self.linear2 = nn.Linear(5, 5)",
        "mutated": [
            "def __init__(self) -> None:\n    if False:\n        i = 10\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)\n    self.linear2 = nn.Linear(5, 5)",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)\n    self.linear2 = nn.Linear(5, 5)",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)\n    self.linear2 = nn.Linear(5, 5)",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)\n    self.linear2 = nn.Linear(5, 5)",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)\n    self.linear2 = nn.Linear(5, 5)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    y = self.linear1(x)\n    z = self.linear2(x)\n    return torch.add(y, z)",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    y = self.linear1(x)\n    z = self.linear2(x)\n    return torch.add(y, z)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    y = self.linear1(x)\n    z = self.linear2(x)\n    return torch.add(y, z)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    y = self.linear1(x)\n    z = self.linear2(x)\n    return torch.add(y, z)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    y = self.linear1(x)\n    z = self.linear2(x)\n    return torch.add(y, z)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    y = self.linear1(x)\n    z = self.linear2(x)\n    return torch.add(y, z)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self) -> None:\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)",
        "mutated": [
            "def __init__(self) -> None:\n    if False:\n        i = 10\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.linear1 = nn.Linear(5, 5)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    y = self.linear1(x)\n    z = torch.add(x, 5)\n    return torch.add(y, z)",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    y = self.linear1(x)\n    z = torch.add(x, 5)\n    return torch.add(y, z)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    y = self.linear1(x)\n    z = torch.add(x, 5)\n    return torch.add(y, z)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    y = self.linear1(x)\n    z = torch.add(x, 5)\n    return torch.add(y, z)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    y = self.linear1(x)\n    z = torch.add(x, 5)\n    return torch.add(y, z)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    y = self.linear1(x)\n    z = torch.add(x, 5)\n    return torch.add(y, z)"
        ]
    },
    {
        "func_name": "test_input_weight_equalization_branching",
        "original": "def test_input_weight_equalization_branching(self):\n    \"\"\" Tests that graphs containing branches are prepared correctly.\n        Specifically, equalization observers should not be inserted in front of\n        branches in which both initial layers in the branches plan to be\n        quantized.\n        \"\"\"\n\n    class TestBranchingWithoutEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n            self.linear2 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = self.linear2(x)\n            return torch.add(y, z)\n    no_eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 0, ns.call_module(MinMaxObserver): 3}\n    m = TestBranchingWithoutEqualizationModel().eval()\n    example_inputs = (torch.rand(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=no_eq_branching_node_occurrence)\n\n    class TestBranchingWithEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = torch.add(x, 5)\n            return torch.add(y, z)\n    eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    m = TestBranchingWithEqualizationModel().eval()\n    example_inputs = (torch.randn(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=eq_branching_node_occurrence)",
        "mutated": [
            "def test_input_weight_equalization_branching(self):\n    if False:\n        i = 10\n    ' Tests that graphs containing branches are prepared correctly.\\n        Specifically, equalization observers should not be inserted in front of\\n        branches in which both initial layers in the branches plan to be\\n        quantized.\\n        '\n\n    class TestBranchingWithoutEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n            self.linear2 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = self.linear2(x)\n            return torch.add(y, z)\n    no_eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 0, ns.call_module(MinMaxObserver): 3}\n    m = TestBranchingWithoutEqualizationModel().eval()\n    example_inputs = (torch.rand(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=no_eq_branching_node_occurrence)\n\n    class TestBranchingWithEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = torch.add(x, 5)\n            return torch.add(y, z)\n    eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    m = TestBranchingWithEqualizationModel().eval()\n    example_inputs = (torch.randn(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=eq_branching_node_occurrence)",
            "def test_input_weight_equalization_branching(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Tests that graphs containing branches are prepared correctly.\\n        Specifically, equalization observers should not be inserted in front of\\n        branches in which both initial layers in the branches plan to be\\n        quantized.\\n        '\n\n    class TestBranchingWithoutEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n            self.linear2 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = self.linear2(x)\n            return torch.add(y, z)\n    no_eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 0, ns.call_module(MinMaxObserver): 3}\n    m = TestBranchingWithoutEqualizationModel().eval()\n    example_inputs = (torch.rand(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=no_eq_branching_node_occurrence)\n\n    class TestBranchingWithEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = torch.add(x, 5)\n            return torch.add(y, z)\n    eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    m = TestBranchingWithEqualizationModel().eval()\n    example_inputs = (torch.randn(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=eq_branching_node_occurrence)",
            "def test_input_weight_equalization_branching(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Tests that graphs containing branches are prepared correctly.\\n        Specifically, equalization observers should not be inserted in front of\\n        branches in which both initial layers in the branches plan to be\\n        quantized.\\n        '\n\n    class TestBranchingWithoutEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n            self.linear2 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = self.linear2(x)\n            return torch.add(y, z)\n    no_eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 0, ns.call_module(MinMaxObserver): 3}\n    m = TestBranchingWithoutEqualizationModel().eval()\n    example_inputs = (torch.rand(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=no_eq_branching_node_occurrence)\n\n    class TestBranchingWithEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = torch.add(x, 5)\n            return torch.add(y, z)\n    eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    m = TestBranchingWithEqualizationModel().eval()\n    example_inputs = (torch.randn(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=eq_branching_node_occurrence)",
            "def test_input_weight_equalization_branching(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Tests that graphs containing branches are prepared correctly.\\n        Specifically, equalization observers should not be inserted in front of\\n        branches in which both initial layers in the branches plan to be\\n        quantized.\\n        '\n\n    class TestBranchingWithoutEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n            self.linear2 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = self.linear2(x)\n            return torch.add(y, z)\n    no_eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 0, ns.call_module(MinMaxObserver): 3}\n    m = TestBranchingWithoutEqualizationModel().eval()\n    example_inputs = (torch.rand(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=no_eq_branching_node_occurrence)\n\n    class TestBranchingWithEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = torch.add(x, 5)\n            return torch.add(y, z)\n    eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    m = TestBranchingWithEqualizationModel().eval()\n    example_inputs = (torch.randn(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=eq_branching_node_occurrence)",
            "def test_input_weight_equalization_branching(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Tests that graphs containing branches are prepared correctly.\\n        Specifically, equalization observers should not be inserted in front of\\n        branches in which both initial layers in the branches plan to be\\n        quantized.\\n        '\n\n    class TestBranchingWithoutEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n            self.linear2 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = self.linear2(x)\n            return torch.add(y, z)\n    no_eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 0, ns.call_module(MinMaxObserver): 3}\n    m = TestBranchingWithoutEqualizationModel().eval()\n    example_inputs = (torch.rand(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=no_eq_branching_node_occurrence)\n\n    class TestBranchingWithEqualizationModel(nn.Module):\n\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear1 = nn.Linear(5, 5)\n\n        def forward(self, x):\n            y = self.linear1(x)\n            z = torch.add(x, 5)\n            return torch.add(y, z)\n    eq_branching_node_occurrence = {ns.call_module(_InputEqualizationObserver): 1, ns.call_module(MinMaxObserver): 2}\n    m = TestBranchingWithEqualizationModel().eval()\n    example_inputs = (torch.randn(1, 5),)\n    prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n    self.checkGraphModuleNodes(prepared, expected_node_occurrence=eq_branching_node_occurrence)"
        ]
    },
    {
        "func_name": "test_input_weight_equalization_convert",
        "original": "@skipIfNoFBGEMM\ndef test_input_weight_equalization_convert(self):\n    \"\"\" Tests that the modified model for equalization (before quantization)\n        returns the same output as the original model\n        \"\"\"\n    tests = [(SingleLayerLinearModel, 2), (LinearAddModel, 2), (TwoLayerLinearModel, 2), (SingleLayerFunctionalLinearModel, 2), (FunctionalLinearAddModel, 2), (TwoLayerFunctionalLinearModel, 2), (LinearReluModel, 2), (LinearReluLinearModel, 2), (LinearReluAddModel, 2), (FunctionalLinearReluModel, 2), (FunctionalLinearReluLinearModel, 2), (ConvModel, 4), (TwoLayerConvModel, 4), (SingleLayerFunctionalConvModel, 4), (TwoLayerFunctionalConvModel, 4), (ConvReluModel, 4), (ConvReluConvModel, 4), (ConvReluAddModel, 4), (FunctionalConvReluModel, 4), (FunctionalConvReluConvModel, 4)]\n    for (M, ndim) in tests:\n        m = M().eval()\n        if ndim == 2:\n            x = torch.rand((5, 5))\n        elif ndim == 4:\n            x = torch.rand((16, 3, 224, 224))\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        output = prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref_output = convert_ref(x)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_fx(prepared)\n        self.assertEqual(output, convert_ref_output)",
        "mutated": [
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_convert(self):\n    if False:\n        i = 10\n    ' Tests that the modified model for equalization (before quantization)\\n        returns the same output as the original model\\n        '\n    tests = [(SingleLayerLinearModel, 2), (LinearAddModel, 2), (TwoLayerLinearModel, 2), (SingleLayerFunctionalLinearModel, 2), (FunctionalLinearAddModel, 2), (TwoLayerFunctionalLinearModel, 2), (LinearReluModel, 2), (LinearReluLinearModel, 2), (LinearReluAddModel, 2), (FunctionalLinearReluModel, 2), (FunctionalLinearReluLinearModel, 2), (ConvModel, 4), (TwoLayerConvModel, 4), (SingleLayerFunctionalConvModel, 4), (TwoLayerFunctionalConvModel, 4), (ConvReluModel, 4), (ConvReluConvModel, 4), (ConvReluAddModel, 4), (FunctionalConvReluModel, 4), (FunctionalConvReluConvModel, 4)]\n    for (M, ndim) in tests:\n        m = M().eval()\n        if ndim == 2:\n            x = torch.rand((5, 5))\n        elif ndim == 4:\n            x = torch.rand((16, 3, 224, 224))\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        output = prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref_output = convert_ref(x)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_fx(prepared)\n        self.assertEqual(output, convert_ref_output)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_convert(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Tests that the modified model for equalization (before quantization)\\n        returns the same output as the original model\\n        '\n    tests = [(SingleLayerLinearModel, 2), (LinearAddModel, 2), (TwoLayerLinearModel, 2), (SingleLayerFunctionalLinearModel, 2), (FunctionalLinearAddModel, 2), (TwoLayerFunctionalLinearModel, 2), (LinearReluModel, 2), (LinearReluLinearModel, 2), (LinearReluAddModel, 2), (FunctionalLinearReluModel, 2), (FunctionalLinearReluLinearModel, 2), (ConvModel, 4), (TwoLayerConvModel, 4), (SingleLayerFunctionalConvModel, 4), (TwoLayerFunctionalConvModel, 4), (ConvReluModel, 4), (ConvReluConvModel, 4), (ConvReluAddModel, 4), (FunctionalConvReluModel, 4), (FunctionalConvReluConvModel, 4)]\n    for (M, ndim) in tests:\n        m = M().eval()\n        if ndim == 2:\n            x = torch.rand((5, 5))\n        elif ndim == 4:\n            x = torch.rand((16, 3, 224, 224))\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        output = prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref_output = convert_ref(x)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_fx(prepared)\n        self.assertEqual(output, convert_ref_output)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_convert(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Tests that the modified model for equalization (before quantization)\\n        returns the same output as the original model\\n        '\n    tests = [(SingleLayerLinearModel, 2), (LinearAddModel, 2), (TwoLayerLinearModel, 2), (SingleLayerFunctionalLinearModel, 2), (FunctionalLinearAddModel, 2), (TwoLayerFunctionalLinearModel, 2), (LinearReluModel, 2), (LinearReluLinearModel, 2), (LinearReluAddModel, 2), (FunctionalLinearReluModel, 2), (FunctionalLinearReluLinearModel, 2), (ConvModel, 4), (TwoLayerConvModel, 4), (SingleLayerFunctionalConvModel, 4), (TwoLayerFunctionalConvModel, 4), (ConvReluModel, 4), (ConvReluConvModel, 4), (ConvReluAddModel, 4), (FunctionalConvReluModel, 4), (FunctionalConvReluConvModel, 4)]\n    for (M, ndim) in tests:\n        m = M().eval()\n        if ndim == 2:\n            x = torch.rand((5, 5))\n        elif ndim == 4:\n            x = torch.rand((16, 3, 224, 224))\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        output = prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref_output = convert_ref(x)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_fx(prepared)\n        self.assertEqual(output, convert_ref_output)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_convert(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Tests that the modified model for equalization (before quantization)\\n        returns the same output as the original model\\n        '\n    tests = [(SingleLayerLinearModel, 2), (LinearAddModel, 2), (TwoLayerLinearModel, 2), (SingleLayerFunctionalLinearModel, 2), (FunctionalLinearAddModel, 2), (TwoLayerFunctionalLinearModel, 2), (LinearReluModel, 2), (LinearReluLinearModel, 2), (LinearReluAddModel, 2), (FunctionalLinearReluModel, 2), (FunctionalLinearReluLinearModel, 2), (ConvModel, 4), (TwoLayerConvModel, 4), (SingleLayerFunctionalConvModel, 4), (TwoLayerFunctionalConvModel, 4), (ConvReluModel, 4), (ConvReluConvModel, 4), (ConvReluAddModel, 4), (FunctionalConvReluModel, 4), (FunctionalConvReluConvModel, 4)]\n    for (M, ndim) in tests:\n        m = M().eval()\n        if ndim == 2:\n            x = torch.rand((5, 5))\n        elif ndim == 4:\n            x = torch.rand((16, 3, 224, 224))\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        output = prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref_output = convert_ref(x)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_fx(prepared)\n        self.assertEqual(output, convert_ref_output)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_convert(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Tests that the modified model for equalization (before quantization)\\n        returns the same output as the original model\\n        '\n    tests = [(SingleLayerLinearModel, 2), (LinearAddModel, 2), (TwoLayerLinearModel, 2), (SingleLayerFunctionalLinearModel, 2), (FunctionalLinearAddModel, 2), (TwoLayerFunctionalLinearModel, 2), (LinearReluModel, 2), (LinearReluLinearModel, 2), (LinearReluAddModel, 2), (FunctionalLinearReluModel, 2), (FunctionalLinearReluLinearModel, 2), (ConvModel, 4), (TwoLayerConvModel, 4), (SingleLayerFunctionalConvModel, 4), (TwoLayerFunctionalConvModel, 4), (ConvReluModel, 4), (ConvReluConvModel, 4), (ConvReluAddModel, 4), (FunctionalConvReluModel, 4), (FunctionalConvReluConvModel, 4)]\n    for (M, ndim) in tests:\n        m = M().eval()\n        if ndim == 2:\n            x = torch.rand((5, 5))\n        elif ndim == 4:\n            x = torch.rand((16, 3, 224, 224))\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        output = prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref_output = convert_ref(x)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_fx(prepared)\n        self.assertEqual(output, convert_ref_output)"
        ]
    },
    {
        "func_name": "calculate_equalization_scale_ref",
        "original": "def calculate_equalization_scale_ref(self, x, w):\n    \"\"\" Calculates the equalization scale based on the input and weight\n        \"\"\"\n    min_inputs = x.min(axis=0)\n    max_inputs = x.max(axis=0)\n    min_weights_col = w.min(axis=0)\n    max_weights_col = w.max(axis=0)\n    equalization_scale = np.sqrt((max_weights_col - min_weights_col) / (max_inputs - min_inputs))\n    return equalization_scale",
        "mutated": [
            "def calculate_equalization_scale_ref(self, x, w):\n    if False:\n        i = 10\n    ' Calculates the equalization scale based on the input and weight\\n        '\n    min_inputs = x.min(axis=0)\n    max_inputs = x.max(axis=0)\n    min_weights_col = w.min(axis=0)\n    max_weights_col = w.max(axis=0)\n    equalization_scale = np.sqrt((max_weights_col - min_weights_col) / (max_inputs - min_inputs))\n    return equalization_scale",
            "def calculate_equalization_scale_ref(self, x, w):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Calculates the equalization scale based on the input and weight\\n        '\n    min_inputs = x.min(axis=0)\n    max_inputs = x.max(axis=0)\n    min_weights_col = w.min(axis=0)\n    max_weights_col = w.max(axis=0)\n    equalization_scale = np.sqrt((max_weights_col - min_weights_col) / (max_inputs - min_inputs))\n    return equalization_scale",
            "def calculate_equalization_scale_ref(self, x, w):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Calculates the equalization scale based on the input and weight\\n        '\n    min_inputs = x.min(axis=0)\n    max_inputs = x.max(axis=0)\n    min_weights_col = w.min(axis=0)\n    max_weights_col = w.max(axis=0)\n    equalization_scale = np.sqrt((max_weights_col - min_weights_col) / (max_inputs - min_inputs))\n    return equalization_scale",
            "def calculate_equalization_scale_ref(self, x, w):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Calculates the equalization scale based on the input and weight\\n        '\n    min_inputs = x.min(axis=0)\n    max_inputs = x.max(axis=0)\n    min_weights_col = w.min(axis=0)\n    max_weights_col = w.max(axis=0)\n    equalization_scale = np.sqrt((max_weights_col - min_weights_col) / (max_inputs - min_inputs))\n    return equalization_scale",
            "def calculate_equalization_scale_ref(self, x, w):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Calculates the equalization scale based on the input and weight\\n        '\n    min_inputs = x.min(axis=0)\n    max_inputs = x.max(axis=0)\n    min_weights_col = w.min(axis=0)\n    max_weights_col = w.max(axis=0)\n    equalization_scale = np.sqrt((max_weights_col - min_weights_col) / (max_inputs - min_inputs))\n    return equalization_scale"
        ]
    },
    {
        "func_name": "get_expected_eq_scales",
        "original": "def get_expected_eq_scales(self, model, x):\n    \"\"\" For each module in the graph, we want to calculate the equalization\n        scale at that point. This only works for models containing single or\n        connected linear layers.\n        \"\"\"\n    exp_eq_scales = []\n    for (_, module) in model.named_children():\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        eq_scale = self.calculate_equalization_scale_ref(x, weight)\n        exp_eq_scales.append(eq_scale)\n        x = x @ weight.T + bias\n    return exp_eq_scales",
        "mutated": [
            "def get_expected_eq_scales(self, model, x):\n    if False:\n        i = 10\n    ' For each module in the graph, we want to calculate the equalization\\n        scale at that point. This only works for models containing single or\\n        connected linear layers.\\n        '\n    exp_eq_scales = []\n    for (_, module) in model.named_children():\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        eq_scale = self.calculate_equalization_scale_ref(x, weight)\n        exp_eq_scales.append(eq_scale)\n        x = x @ weight.T + bias\n    return exp_eq_scales",
            "def get_expected_eq_scales(self, model, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' For each module in the graph, we want to calculate the equalization\\n        scale at that point. This only works for models containing single or\\n        connected linear layers.\\n        '\n    exp_eq_scales = []\n    for (_, module) in model.named_children():\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        eq_scale = self.calculate_equalization_scale_ref(x, weight)\n        exp_eq_scales.append(eq_scale)\n        x = x @ weight.T + bias\n    return exp_eq_scales",
            "def get_expected_eq_scales(self, model, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' For each module in the graph, we want to calculate the equalization\\n        scale at that point. This only works for models containing single or\\n        connected linear layers.\\n        '\n    exp_eq_scales = []\n    for (_, module) in model.named_children():\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        eq_scale = self.calculate_equalization_scale_ref(x, weight)\n        exp_eq_scales.append(eq_scale)\n        x = x @ weight.T + bias\n    return exp_eq_scales",
            "def get_expected_eq_scales(self, model, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' For each module in the graph, we want to calculate the equalization\\n        scale at that point. This only works for models containing single or\\n        connected linear layers.\\n        '\n    exp_eq_scales = []\n    for (_, module) in model.named_children():\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        eq_scale = self.calculate_equalization_scale_ref(x, weight)\n        exp_eq_scales.append(eq_scale)\n        x = x @ weight.T + bias\n    return exp_eq_scales",
            "def get_expected_eq_scales(self, model, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' For each module in the graph, we want to calculate the equalization\\n        scale at that point. This only works for models containing single or\\n        connected linear layers.\\n        '\n    exp_eq_scales = []\n    for (_, module) in model.named_children():\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        eq_scale = self.calculate_equalization_scale_ref(x, weight)\n        exp_eq_scales.append(eq_scale)\n        x = x @ weight.T + bias\n    return exp_eq_scales"
        ]
    },
    {
        "func_name": "test_input_weight_equalization_equalization_scales",
        "original": "def test_input_weight_equalization_equalization_scales(self):\n    \"\"\" After applying the equalization functions, check if the equalization\n        scales are the expected values\n        \"\"\"\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(*example_inputs)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if 'equalization_scale' in node.name and node.op == 'get_attr':\n                self.assertEqual(convert_ref.get_buffer(str(node.target)).reshape(-1), exp_eq_scales[counter])\n                counter += 1",
        "mutated": [
            "def test_input_weight_equalization_equalization_scales(self):\n    if False:\n        i = 10\n    ' After applying the equalization functions, check if the equalization\\n        scales are the expected values\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(*example_inputs)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if 'equalization_scale' in node.name and node.op == 'get_attr':\n                self.assertEqual(convert_ref.get_buffer(str(node.target)).reshape(-1), exp_eq_scales[counter])\n                counter += 1",
            "def test_input_weight_equalization_equalization_scales(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' After applying the equalization functions, check if the equalization\\n        scales are the expected values\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(*example_inputs)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if 'equalization_scale' in node.name and node.op == 'get_attr':\n                self.assertEqual(convert_ref.get_buffer(str(node.target)).reshape(-1), exp_eq_scales[counter])\n                counter += 1",
            "def test_input_weight_equalization_equalization_scales(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' After applying the equalization functions, check if the equalization\\n        scales are the expected values\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(*example_inputs)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if 'equalization_scale' in node.name and node.op == 'get_attr':\n                self.assertEqual(convert_ref.get_buffer(str(node.target)).reshape(-1), exp_eq_scales[counter])\n                counter += 1",
            "def test_input_weight_equalization_equalization_scales(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' After applying the equalization functions, check if the equalization\\n        scales are the expected values\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(*example_inputs)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if 'equalization_scale' in node.name and node.op == 'get_attr':\n                self.assertEqual(convert_ref.get_buffer(str(node.target)).reshape(-1), exp_eq_scales[counter])\n                counter += 1",
            "def test_input_weight_equalization_equalization_scales(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' After applying the equalization functions, check if the equalization\\n        scales are the expected values\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(*example_inputs)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if 'equalization_scale' in node.name and node.op == 'get_attr':\n                self.assertEqual(convert_ref.get_buffer(str(node.target)).reshape(-1), exp_eq_scales[counter])\n                counter += 1"
        ]
    },
    {
        "func_name": "get_expected_weights_bias",
        "original": "def get_expected_weights_bias(self, model, x, exp_eq_scales):\n    \"\"\" For each module in the graph, we want to calculate the expected\n        scaled weight and bias values. This only works for models containing\n        single or connected linear layers.\n        \"\"\"\n    exp_weights = []\n    exp_bias = []\n    for (i, (_, module)) in enumerate(model.named_children()):\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        scaled_weight = weight * np.reciprocal(exp_eq_scales[i])\n        scaled_bias = bias\n        if i + 1 < len(exp_eq_scales):\n            scaled_weight = (scaled_weight.T * exp_eq_scales[i + 1]).T\n            scaled_bias = (scaled_bias.T * exp_eq_scales[i + 1]).T\n        exp_weights.append(scaled_weight)\n        exp_bias.append(scaled_bias)\n        x = x @ weight.T + bias\n    return (exp_weights, exp_bias)",
        "mutated": [
            "def get_expected_weights_bias(self, model, x, exp_eq_scales):\n    if False:\n        i = 10\n    ' For each module in the graph, we want to calculate the expected\\n        scaled weight and bias values. This only works for models containing\\n        single or connected linear layers.\\n        '\n    exp_weights = []\n    exp_bias = []\n    for (i, (_, module)) in enumerate(model.named_children()):\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        scaled_weight = weight * np.reciprocal(exp_eq_scales[i])\n        scaled_bias = bias\n        if i + 1 < len(exp_eq_scales):\n            scaled_weight = (scaled_weight.T * exp_eq_scales[i + 1]).T\n            scaled_bias = (scaled_bias.T * exp_eq_scales[i + 1]).T\n        exp_weights.append(scaled_weight)\n        exp_bias.append(scaled_bias)\n        x = x @ weight.T + bias\n    return (exp_weights, exp_bias)",
            "def get_expected_weights_bias(self, model, x, exp_eq_scales):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' For each module in the graph, we want to calculate the expected\\n        scaled weight and bias values. This only works for models containing\\n        single or connected linear layers.\\n        '\n    exp_weights = []\n    exp_bias = []\n    for (i, (_, module)) in enumerate(model.named_children()):\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        scaled_weight = weight * np.reciprocal(exp_eq_scales[i])\n        scaled_bias = bias\n        if i + 1 < len(exp_eq_scales):\n            scaled_weight = (scaled_weight.T * exp_eq_scales[i + 1]).T\n            scaled_bias = (scaled_bias.T * exp_eq_scales[i + 1]).T\n        exp_weights.append(scaled_weight)\n        exp_bias.append(scaled_bias)\n        x = x @ weight.T + bias\n    return (exp_weights, exp_bias)",
            "def get_expected_weights_bias(self, model, x, exp_eq_scales):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' For each module in the graph, we want to calculate the expected\\n        scaled weight and bias values. This only works for models containing\\n        single or connected linear layers.\\n        '\n    exp_weights = []\n    exp_bias = []\n    for (i, (_, module)) in enumerate(model.named_children()):\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        scaled_weight = weight * np.reciprocal(exp_eq_scales[i])\n        scaled_bias = bias\n        if i + 1 < len(exp_eq_scales):\n            scaled_weight = (scaled_weight.T * exp_eq_scales[i + 1]).T\n            scaled_bias = (scaled_bias.T * exp_eq_scales[i + 1]).T\n        exp_weights.append(scaled_weight)\n        exp_bias.append(scaled_bias)\n        x = x @ weight.T + bias\n    return (exp_weights, exp_bias)",
            "def get_expected_weights_bias(self, model, x, exp_eq_scales):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' For each module in the graph, we want to calculate the expected\\n        scaled weight and bias values. This only works for models containing\\n        single or connected linear layers.\\n        '\n    exp_weights = []\n    exp_bias = []\n    for (i, (_, module)) in enumerate(model.named_children()):\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        scaled_weight = weight * np.reciprocal(exp_eq_scales[i])\n        scaled_bias = bias\n        if i + 1 < len(exp_eq_scales):\n            scaled_weight = (scaled_weight.T * exp_eq_scales[i + 1]).T\n            scaled_bias = (scaled_bias.T * exp_eq_scales[i + 1]).T\n        exp_weights.append(scaled_weight)\n        exp_bias.append(scaled_bias)\n        x = x @ weight.T + bias\n    return (exp_weights, exp_bias)",
            "def get_expected_weights_bias(self, model, x, exp_eq_scales):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' For each module in the graph, we want to calculate the expected\\n        scaled weight and bias values. This only works for models containing\\n        single or connected linear layers.\\n        '\n    exp_weights = []\n    exp_bias = []\n    for (i, (_, module)) in enumerate(model.named_children()):\n        weight = module.weight.detach().numpy()\n        bias = module.bias.detach().numpy()\n        scaled_weight = weight * np.reciprocal(exp_eq_scales[i])\n        scaled_bias = bias\n        if i + 1 < len(exp_eq_scales):\n            scaled_weight = (scaled_weight.T * exp_eq_scales[i + 1]).T\n            scaled_bias = (scaled_bias.T * exp_eq_scales[i + 1]).T\n        exp_weights.append(scaled_weight)\n        exp_bias.append(scaled_bias)\n        x = x @ weight.T + bias\n    return (exp_weights, exp_bias)"
        ]
    },
    {
        "func_name": "test_input_weight_equalization_weights_bias",
        "original": "def test_input_weight_equalization_weights_bias(self):\n    \"\"\" After applying the equalization functions check if the weights and\n        biases are as expected\n        \"\"\"\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], nn.Linear):\n                self.assertEqual(modules[str(node.target)].weight, exp_weights[counter])\n                self.assertEqual(modules[str(node.target)].bias, exp_bias[counter])\n                counter += 1",
        "mutated": [
            "def test_input_weight_equalization_weights_bias(self):\n    if False:\n        i = 10\n    ' After applying the equalization functions check if the weights and\\n        biases are as expected\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], nn.Linear):\n                self.assertEqual(modules[str(node.target)].weight, exp_weights[counter])\n                self.assertEqual(modules[str(node.target)].bias, exp_bias[counter])\n                counter += 1",
            "def test_input_weight_equalization_weights_bias(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' After applying the equalization functions check if the weights and\\n        biases are as expected\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], nn.Linear):\n                self.assertEqual(modules[str(node.target)].weight, exp_weights[counter])\n                self.assertEqual(modules[str(node.target)].bias, exp_bias[counter])\n                counter += 1",
            "def test_input_weight_equalization_weights_bias(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' After applying the equalization functions check if the weights and\\n        biases are as expected\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], nn.Linear):\n                self.assertEqual(modules[str(node.target)].weight, exp_weights[counter])\n                self.assertEqual(modules[str(node.target)].bias, exp_bias[counter])\n                counter += 1",
            "def test_input_weight_equalization_weights_bias(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' After applying the equalization functions check if the weights and\\n        biases are as expected\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], nn.Linear):\n                self.assertEqual(modules[str(node.target)].weight, exp_weights[counter])\n                self.assertEqual(modules[str(node.target)].bias, exp_bias[counter])\n                counter += 1",
            "def test_input_weight_equalization_weights_bias(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' After applying the equalization functions check if the weights and\\n        biases are as expected\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        counter = 0\n        for node in convert_ref.graph.nodes:\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], nn.Linear):\n                self.assertEqual(modules[str(node.target)].weight, exp_weights[counter])\n                self.assertEqual(modules[str(node.target)].bias, exp_bias[counter])\n                counter += 1"
        ]
    },
    {
        "func_name": "get_expected_inp_act_vals",
        "original": "def get_expected_inp_act_vals(self, model, x, exp_eq_scales, exp_weights, exp_bias):\n    \"\"\" For each module in the graph, we want to calculate the expected\n        min/max values for every input activation node. This only works for\n        models containing only single or connected linear layers.\n        \"\"\"\n    x = x * exp_eq_scales[0]\n    exp_inp_activation_vals = []\n    for (i, _) in enumerate(model.named_children()):\n        exp_inp_activation_vals.append((x.min(), x.max()))\n        x = x @ exp_weights[i].T + exp_bias[i]\n    exp_inp_activation_vals.append((x.min(), x.max()))\n    return exp_inp_activation_vals",
        "mutated": [
            "def get_expected_inp_act_vals(self, model, x, exp_eq_scales, exp_weights, exp_bias):\n    if False:\n        i = 10\n    ' For each module in the graph, we want to calculate the expected\\n        min/max values for every input activation node. This only works for\\n        models containing only single or connected linear layers.\\n        '\n    x = x * exp_eq_scales[0]\n    exp_inp_activation_vals = []\n    for (i, _) in enumerate(model.named_children()):\n        exp_inp_activation_vals.append((x.min(), x.max()))\n        x = x @ exp_weights[i].T + exp_bias[i]\n    exp_inp_activation_vals.append((x.min(), x.max()))\n    return exp_inp_activation_vals",
            "def get_expected_inp_act_vals(self, model, x, exp_eq_scales, exp_weights, exp_bias):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' For each module in the graph, we want to calculate the expected\\n        min/max values for every input activation node. This only works for\\n        models containing only single or connected linear layers.\\n        '\n    x = x * exp_eq_scales[0]\n    exp_inp_activation_vals = []\n    for (i, _) in enumerate(model.named_children()):\n        exp_inp_activation_vals.append((x.min(), x.max()))\n        x = x @ exp_weights[i].T + exp_bias[i]\n    exp_inp_activation_vals.append((x.min(), x.max()))\n    return exp_inp_activation_vals",
            "def get_expected_inp_act_vals(self, model, x, exp_eq_scales, exp_weights, exp_bias):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' For each module in the graph, we want to calculate the expected\\n        min/max values for every input activation node. This only works for\\n        models containing only single or connected linear layers.\\n        '\n    x = x * exp_eq_scales[0]\n    exp_inp_activation_vals = []\n    for (i, _) in enumerate(model.named_children()):\n        exp_inp_activation_vals.append((x.min(), x.max()))\n        x = x @ exp_weights[i].T + exp_bias[i]\n    exp_inp_activation_vals.append((x.min(), x.max()))\n    return exp_inp_activation_vals",
            "def get_expected_inp_act_vals(self, model, x, exp_eq_scales, exp_weights, exp_bias):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' For each module in the graph, we want to calculate the expected\\n        min/max values for every input activation node. This only works for\\n        models containing only single or connected linear layers.\\n        '\n    x = x * exp_eq_scales[0]\n    exp_inp_activation_vals = []\n    for (i, _) in enumerate(model.named_children()):\n        exp_inp_activation_vals.append((x.min(), x.max()))\n        x = x @ exp_weights[i].T + exp_bias[i]\n    exp_inp_activation_vals.append((x.min(), x.max()))\n    return exp_inp_activation_vals",
            "def get_expected_inp_act_vals(self, model, x, exp_eq_scales, exp_weights, exp_bias):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' For each module in the graph, we want to calculate the expected\\n        min/max values for every input activation node. This only works for\\n        models containing only single or connected linear layers.\\n        '\n    x = x * exp_eq_scales[0]\n    exp_inp_activation_vals = []\n    for (i, _) in enumerate(model.named_children()):\n        exp_inp_activation_vals.append((x.min(), x.max()))\n        x = x @ exp_weights[i].T + exp_bias[i]\n    exp_inp_activation_vals.append((x.min(), x.max()))\n    return exp_inp_activation_vals"
        ]
    },
    {
        "func_name": "get_expected_weight_act_vals",
        "original": "def get_expected_weight_act_vals(self, exp_weights):\n    \"\"\" For each module in the graph, we want to calculate the expected\n        min/max values for every weight activation node. This is assuming that\n        the weight observers are all MinMaxObservers.\n        \"\"\"\n    exp_weight_activation_vals = []\n    for w in exp_weights:\n        exp_weight_activation_vals.append((w.min(), w.max()))\n    return exp_weight_activation_vals",
        "mutated": [
            "def get_expected_weight_act_vals(self, exp_weights):\n    if False:\n        i = 10\n    ' For each module in the graph, we want to calculate the expected\\n        min/max values for every weight activation node. This is assuming that\\n        the weight observers are all MinMaxObservers.\\n        '\n    exp_weight_activation_vals = []\n    for w in exp_weights:\n        exp_weight_activation_vals.append((w.min(), w.max()))\n    return exp_weight_activation_vals",
            "def get_expected_weight_act_vals(self, exp_weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' For each module in the graph, we want to calculate the expected\\n        min/max values for every weight activation node. This is assuming that\\n        the weight observers are all MinMaxObservers.\\n        '\n    exp_weight_activation_vals = []\n    for w in exp_weights:\n        exp_weight_activation_vals.append((w.min(), w.max()))\n    return exp_weight_activation_vals",
            "def get_expected_weight_act_vals(self, exp_weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' For each module in the graph, we want to calculate the expected\\n        min/max values for every weight activation node. This is assuming that\\n        the weight observers are all MinMaxObservers.\\n        '\n    exp_weight_activation_vals = []\n    for w in exp_weights:\n        exp_weight_activation_vals.append((w.min(), w.max()))\n    return exp_weight_activation_vals",
            "def get_expected_weight_act_vals(self, exp_weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' For each module in the graph, we want to calculate the expected\\n        min/max values for every weight activation node. This is assuming that\\n        the weight observers are all MinMaxObservers.\\n        '\n    exp_weight_activation_vals = []\n    for w in exp_weights:\n        exp_weight_activation_vals.append((w.min(), w.max()))\n    return exp_weight_activation_vals",
            "def get_expected_weight_act_vals(self, exp_weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' For each module in the graph, we want to calculate the expected\\n        min/max values for every weight activation node. This is assuming that\\n        the weight observers are all MinMaxObservers.\\n        '\n    exp_weight_activation_vals = []\n    for w in exp_weights:\n        exp_weight_activation_vals.append((w.min(), w.max()))\n    return exp_weight_activation_vals"
        ]
    },
    {
        "func_name": "test_input_weight_equalization_activation_values",
        "original": "def test_input_weight_equalization_activation_values(self):\n    \"\"\" After applying the equalization functions check if the input\n        observer's min/max values are as expected\n        \"\"\"\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    torch.manual_seed(0)\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        exp_inp_act_vals = self.get_expected_inp_act_vals(m, x, exp_eq_scales, exp_weights, exp_bias)\n        exp_weight_act_vals = self.get_expected_weight_act_vals(exp_weights)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        inp_counter = 0\n        weight_counter = 0\n        for node in convert_ref.graph.nodes:\n            users = list(node.users)\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], MinMaxObserver):\n                if len(users) == 1 and users[0].target == torch.nn.functional.linear and (users[0].args[1] == node):\n                    (exp_min_val, exp_max_val) = exp_weight_act_vals[weight_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    weight_counter += 1\n                else:\n                    (exp_min_val, exp_max_val) = exp_inp_act_vals[inp_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    inp_counter += 1",
        "mutated": [
            "def test_input_weight_equalization_activation_values(self):\n    if False:\n        i = 10\n    \" After applying the equalization functions check if the input\\n        observer's min/max values are as expected\\n        \"\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    torch.manual_seed(0)\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        exp_inp_act_vals = self.get_expected_inp_act_vals(m, x, exp_eq_scales, exp_weights, exp_bias)\n        exp_weight_act_vals = self.get_expected_weight_act_vals(exp_weights)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        inp_counter = 0\n        weight_counter = 0\n        for node in convert_ref.graph.nodes:\n            users = list(node.users)\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], MinMaxObserver):\n                if len(users) == 1 and users[0].target == torch.nn.functional.linear and (users[0].args[1] == node):\n                    (exp_min_val, exp_max_val) = exp_weight_act_vals[weight_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    weight_counter += 1\n                else:\n                    (exp_min_val, exp_max_val) = exp_inp_act_vals[inp_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    inp_counter += 1",
            "def test_input_weight_equalization_activation_values(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \" After applying the equalization functions check if the input\\n        observer's min/max values are as expected\\n        \"\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    torch.manual_seed(0)\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        exp_inp_act_vals = self.get_expected_inp_act_vals(m, x, exp_eq_scales, exp_weights, exp_bias)\n        exp_weight_act_vals = self.get_expected_weight_act_vals(exp_weights)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        inp_counter = 0\n        weight_counter = 0\n        for node in convert_ref.graph.nodes:\n            users = list(node.users)\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], MinMaxObserver):\n                if len(users) == 1 and users[0].target == torch.nn.functional.linear and (users[0].args[1] == node):\n                    (exp_min_val, exp_max_val) = exp_weight_act_vals[weight_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    weight_counter += 1\n                else:\n                    (exp_min_val, exp_max_val) = exp_inp_act_vals[inp_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    inp_counter += 1",
            "def test_input_weight_equalization_activation_values(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \" After applying the equalization functions check if the input\\n        observer's min/max values are as expected\\n        \"\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    torch.manual_seed(0)\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        exp_inp_act_vals = self.get_expected_inp_act_vals(m, x, exp_eq_scales, exp_weights, exp_bias)\n        exp_weight_act_vals = self.get_expected_weight_act_vals(exp_weights)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        inp_counter = 0\n        weight_counter = 0\n        for node in convert_ref.graph.nodes:\n            users = list(node.users)\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], MinMaxObserver):\n                if len(users) == 1 and users[0].target == torch.nn.functional.linear and (users[0].args[1] == node):\n                    (exp_min_val, exp_max_val) = exp_weight_act_vals[weight_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    weight_counter += 1\n                else:\n                    (exp_min_val, exp_max_val) = exp_inp_act_vals[inp_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    inp_counter += 1",
            "def test_input_weight_equalization_activation_values(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \" After applying the equalization functions check if the input\\n        observer's min/max values are as expected\\n        \"\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    torch.manual_seed(0)\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        exp_inp_act_vals = self.get_expected_inp_act_vals(m, x, exp_eq_scales, exp_weights, exp_bias)\n        exp_weight_act_vals = self.get_expected_weight_act_vals(exp_weights)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        inp_counter = 0\n        weight_counter = 0\n        for node in convert_ref.graph.nodes:\n            users = list(node.users)\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], MinMaxObserver):\n                if len(users) == 1 and users[0].target == torch.nn.functional.linear and (users[0].args[1] == node):\n                    (exp_min_val, exp_max_val) = exp_weight_act_vals[weight_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    weight_counter += 1\n                else:\n                    (exp_min_val, exp_max_val) = exp_inp_act_vals[inp_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    inp_counter += 1",
            "def test_input_weight_equalization_activation_values(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \" After applying the equalization functions check if the input\\n        observer's min/max values are as expected\\n        \"\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, SingleLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    torch.manual_seed(0)\n    for M in tests:\n        m = M().eval()\n        exp_eq_scales = self.get_expected_eq_scales(m, x.detach().numpy())\n        (exp_weights, exp_bias) = self.get_expected_weights_bias(m, x.detach().numpy(), exp_eq_scales)\n        exp_inp_act_vals = self.get_expected_inp_act_vals(m, x, exp_eq_scales, exp_weights, exp_bias)\n        exp_weight_act_vals = self.get_expected_weight_act_vals(exp_weights)\n        example_inputs = (x,)\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        convert_ref = _convert_equalization_ref(prepared)\n        convert_ref(x)\n        modules = dict(convert_ref.named_modules(remove_duplicate=False))\n        inp_counter = 0\n        weight_counter = 0\n        for node in convert_ref.graph.nodes:\n            users = list(node.users)\n            if node.op == 'call_module' and isinstance(modules[str(node.target)], MinMaxObserver):\n                if len(users) == 1 and users[0].target == torch.nn.functional.linear and (users[0].args[1] == node):\n                    (exp_min_val, exp_max_val) = exp_weight_act_vals[weight_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    weight_counter += 1\n                else:\n                    (exp_min_val, exp_max_val) = exp_inp_act_vals[inp_counter]\n                    self.assertEqual(modules[str(node.target)].min_val, exp_min_val)\n                    self.assertEqual(modules[str(node.target)].max_val, exp_max_val)\n                    inp_counter += 1"
        ]
    },
    {
        "func_name": "check_orig_and_eq_graphs",
        "original": "def check_orig_and_eq_graphs(self, orig_model, eq_model):\n    \"\"\" Given a non-equalized model and an equalized model, check that the\n        graphs are structured in the same way, except the equalized model has\n        additional 'equalization_scale' and 'mul' nodes.\n        \"\"\"\n    orig_idx = 0\n    orig_nodes = list(orig_model.graph.nodes)\n    orig_modules = dict(orig_model.named_modules(remove_duplicate=False))\n    eq_idx = 0\n    eq_nodes = list(eq_model.graph.nodes)\n    eq_modules = dict(eq_model.named_modules(remove_duplicate=False))\n    while orig_idx < len(orig_nodes) and eq_idx < len(eq_nodes):\n        if 'equalization_scale' in eq_nodes[eq_idx].name and 'mul' in eq_nodes[eq_idx + 1].name:\n            eq_idx += 2\n            continue\n        elif orig_nodes[orig_idx].op != eq_nodes[eq_idx].op:\n            return False\n        elif orig_nodes[orig_idx].op == 'call_module':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if type(orig_modules[orig_node.target]) is not type(eq_modules[eq_node.target]):\n                return False\n        elif orig_nodes[orig_idx].op == 'call_function':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if orig_node.target != eq_node.target:\n                return False\n        eq_idx += 1\n        orig_idx += 1\n    return True",
        "mutated": [
            "def check_orig_and_eq_graphs(self, orig_model, eq_model):\n    if False:\n        i = 10\n    \" Given a non-equalized model and an equalized model, check that the\\n        graphs are structured in the same way, except the equalized model has\\n        additional 'equalization_scale' and 'mul' nodes.\\n        \"\n    orig_idx = 0\n    orig_nodes = list(orig_model.graph.nodes)\n    orig_modules = dict(orig_model.named_modules(remove_duplicate=False))\n    eq_idx = 0\n    eq_nodes = list(eq_model.graph.nodes)\n    eq_modules = dict(eq_model.named_modules(remove_duplicate=False))\n    while orig_idx < len(orig_nodes) and eq_idx < len(eq_nodes):\n        if 'equalization_scale' in eq_nodes[eq_idx].name and 'mul' in eq_nodes[eq_idx + 1].name:\n            eq_idx += 2\n            continue\n        elif orig_nodes[orig_idx].op != eq_nodes[eq_idx].op:\n            return False\n        elif orig_nodes[orig_idx].op == 'call_module':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if type(orig_modules[orig_node.target]) is not type(eq_modules[eq_node.target]):\n                return False\n        elif orig_nodes[orig_idx].op == 'call_function':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if orig_node.target != eq_node.target:\n                return False\n        eq_idx += 1\n        orig_idx += 1\n    return True",
            "def check_orig_and_eq_graphs(self, orig_model, eq_model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \" Given a non-equalized model and an equalized model, check that the\\n        graphs are structured in the same way, except the equalized model has\\n        additional 'equalization_scale' and 'mul' nodes.\\n        \"\n    orig_idx = 0\n    orig_nodes = list(orig_model.graph.nodes)\n    orig_modules = dict(orig_model.named_modules(remove_duplicate=False))\n    eq_idx = 0\n    eq_nodes = list(eq_model.graph.nodes)\n    eq_modules = dict(eq_model.named_modules(remove_duplicate=False))\n    while orig_idx < len(orig_nodes) and eq_idx < len(eq_nodes):\n        if 'equalization_scale' in eq_nodes[eq_idx].name and 'mul' in eq_nodes[eq_idx + 1].name:\n            eq_idx += 2\n            continue\n        elif orig_nodes[orig_idx].op != eq_nodes[eq_idx].op:\n            return False\n        elif orig_nodes[orig_idx].op == 'call_module':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if type(orig_modules[orig_node.target]) is not type(eq_modules[eq_node.target]):\n                return False\n        elif orig_nodes[orig_idx].op == 'call_function':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if orig_node.target != eq_node.target:\n                return False\n        eq_idx += 1\n        orig_idx += 1\n    return True",
            "def check_orig_and_eq_graphs(self, orig_model, eq_model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \" Given a non-equalized model and an equalized model, check that the\\n        graphs are structured in the same way, except the equalized model has\\n        additional 'equalization_scale' and 'mul' nodes.\\n        \"\n    orig_idx = 0\n    orig_nodes = list(orig_model.graph.nodes)\n    orig_modules = dict(orig_model.named_modules(remove_duplicate=False))\n    eq_idx = 0\n    eq_nodes = list(eq_model.graph.nodes)\n    eq_modules = dict(eq_model.named_modules(remove_duplicate=False))\n    while orig_idx < len(orig_nodes) and eq_idx < len(eq_nodes):\n        if 'equalization_scale' in eq_nodes[eq_idx].name and 'mul' in eq_nodes[eq_idx + 1].name:\n            eq_idx += 2\n            continue\n        elif orig_nodes[orig_idx].op != eq_nodes[eq_idx].op:\n            return False\n        elif orig_nodes[orig_idx].op == 'call_module':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if type(orig_modules[orig_node.target]) is not type(eq_modules[eq_node.target]):\n                return False\n        elif orig_nodes[orig_idx].op == 'call_function':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if orig_node.target != eq_node.target:\n                return False\n        eq_idx += 1\n        orig_idx += 1\n    return True",
            "def check_orig_and_eq_graphs(self, orig_model, eq_model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \" Given a non-equalized model and an equalized model, check that the\\n        graphs are structured in the same way, except the equalized model has\\n        additional 'equalization_scale' and 'mul' nodes.\\n        \"\n    orig_idx = 0\n    orig_nodes = list(orig_model.graph.nodes)\n    orig_modules = dict(orig_model.named_modules(remove_duplicate=False))\n    eq_idx = 0\n    eq_nodes = list(eq_model.graph.nodes)\n    eq_modules = dict(eq_model.named_modules(remove_duplicate=False))\n    while orig_idx < len(orig_nodes) and eq_idx < len(eq_nodes):\n        if 'equalization_scale' in eq_nodes[eq_idx].name and 'mul' in eq_nodes[eq_idx + 1].name:\n            eq_idx += 2\n            continue\n        elif orig_nodes[orig_idx].op != eq_nodes[eq_idx].op:\n            return False\n        elif orig_nodes[orig_idx].op == 'call_module':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if type(orig_modules[orig_node.target]) is not type(eq_modules[eq_node.target]):\n                return False\n        elif orig_nodes[orig_idx].op == 'call_function':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if orig_node.target != eq_node.target:\n                return False\n        eq_idx += 1\n        orig_idx += 1\n    return True",
            "def check_orig_and_eq_graphs(self, orig_model, eq_model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \" Given a non-equalized model and an equalized model, check that the\\n        graphs are structured in the same way, except the equalized model has\\n        additional 'equalization_scale' and 'mul' nodes.\\n        \"\n    orig_idx = 0\n    orig_nodes = list(orig_model.graph.nodes)\n    orig_modules = dict(orig_model.named_modules(remove_duplicate=False))\n    eq_idx = 0\n    eq_nodes = list(eq_model.graph.nodes)\n    eq_modules = dict(eq_model.named_modules(remove_duplicate=False))\n    while orig_idx < len(orig_nodes) and eq_idx < len(eq_nodes):\n        if 'equalization_scale' in eq_nodes[eq_idx].name and 'mul' in eq_nodes[eq_idx + 1].name:\n            eq_idx += 2\n            continue\n        elif orig_nodes[orig_idx].op != eq_nodes[eq_idx].op:\n            return False\n        elif orig_nodes[orig_idx].op == 'call_module':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if type(orig_modules[orig_node.target]) is not type(eq_modules[eq_node.target]):\n                return False\n        elif orig_nodes[orig_idx].op == 'call_function':\n            orig_node = orig_nodes[orig_idx]\n            eq_node = eq_nodes[eq_idx]\n            if orig_node.target != eq_node.target:\n                return False\n        eq_idx += 1\n        orig_idx += 1\n    return True"
        ]
    },
    {
        "func_name": "test_input_weight_equalization_graphs",
        "original": "@skipIfNoFBGEMM\ndef test_input_weight_equalization_graphs(self):\n    \"\"\" Tests that the modified model for equalization has the same graph\n        structure as the model without equalization (before and after\n        quantization).\n        \"\"\"\n    linear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    linearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_method('dequantize')]\n    linearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_method('dequantize')]\n    functionalLinearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    conv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    conv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    functionalConv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    convRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_method('dequantize')]\n    convReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConvRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_method('dequantize')]\n    functionalConvReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    tests = [(SingleLayerLinearModel, linear_node_list), (LinearAddModel, linearAdd_node_list), (TwoLayerLinearModel, linear2_node_list), (SingleLayerFunctionalLinearModel, functionalLinear_node_list), (FunctionalLinearAddModel, functionalLinearAdd_node_list), (TwoLayerFunctionalLinearModel, functionalLinear2_node_list), (LinearReluModel, linearRelu_node_list), (LinearReluLinearModel, linearReluLinear_node_list), (FunctionalLinearReluModel, functionalLinearRelu_node_list), (FunctionalLinearReluLinearModel, functionalLinearReluLinear_node_list), (ConvModel, conv_node_list), (TwoLayerConvModel, conv2_node_list), (SingleLayerFunctionalConvModel, functionalConv_node_list), (TwoLayerFunctionalConvModel, functionalConv2_node_list), (ConvReluModel, convRelu_node_list), (ConvReluConvModel, convReluConv_node_list), (FunctionalConvReluModel, functionalConvRelu_node_list), (FunctionalConvReluConvModel, functionalConvReluConv_node_list)]\n    for (M, node_list) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        equalized_quantized_model = convert_fx(prepared)\n        self.checkGraphModuleNodes(equalized_quantized_model, expected_node_list=node_list)",
        "mutated": [
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_graphs(self):\n    if False:\n        i = 10\n    ' Tests that the modified model for equalization has the same graph\\n        structure as the model without equalization (before and after\\n        quantization).\\n        '\n    linear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    linearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_method('dequantize')]\n    linearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_method('dequantize')]\n    functionalLinearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    conv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    conv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    functionalConv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    convRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_method('dequantize')]\n    convReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConvRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_method('dequantize')]\n    functionalConvReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    tests = [(SingleLayerLinearModel, linear_node_list), (LinearAddModel, linearAdd_node_list), (TwoLayerLinearModel, linear2_node_list), (SingleLayerFunctionalLinearModel, functionalLinear_node_list), (FunctionalLinearAddModel, functionalLinearAdd_node_list), (TwoLayerFunctionalLinearModel, functionalLinear2_node_list), (LinearReluModel, linearRelu_node_list), (LinearReluLinearModel, linearReluLinear_node_list), (FunctionalLinearReluModel, functionalLinearRelu_node_list), (FunctionalLinearReluLinearModel, functionalLinearReluLinear_node_list), (ConvModel, conv_node_list), (TwoLayerConvModel, conv2_node_list), (SingleLayerFunctionalConvModel, functionalConv_node_list), (TwoLayerFunctionalConvModel, functionalConv2_node_list), (ConvReluModel, convRelu_node_list), (ConvReluConvModel, convReluConv_node_list), (FunctionalConvReluModel, functionalConvRelu_node_list), (FunctionalConvReluConvModel, functionalConvReluConv_node_list)]\n    for (M, node_list) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        equalized_quantized_model = convert_fx(prepared)\n        self.checkGraphModuleNodes(equalized_quantized_model, expected_node_list=node_list)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_graphs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Tests that the modified model for equalization has the same graph\\n        structure as the model without equalization (before and after\\n        quantization).\\n        '\n    linear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    linearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_method('dequantize')]\n    linearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_method('dequantize')]\n    functionalLinearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    conv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    conv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    functionalConv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    convRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_method('dequantize')]\n    convReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConvRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_method('dequantize')]\n    functionalConvReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    tests = [(SingleLayerLinearModel, linear_node_list), (LinearAddModel, linearAdd_node_list), (TwoLayerLinearModel, linear2_node_list), (SingleLayerFunctionalLinearModel, functionalLinear_node_list), (FunctionalLinearAddModel, functionalLinearAdd_node_list), (TwoLayerFunctionalLinearModel, functionalLinear2_node_list), (LinearReluModel, linearRelu_node_list), (LinearReluLinearModel, linearReluLinear_node_list), (FunctionalLinearReluModel, functionalLinearRelu_node_list), (FunctionalLinearReluLinearModel, functionalLinearReluLinear_node_list), (ConvModel, conv_node_list), (TwoLayerConvModel, conv2_node_list), (SingleLayerFunctionalConvModel, functionalConv_node_list), (TwoLayerFunctionalConvModel, functionalConv2_node_list), (ConvReluModel, convRelu_node_list), (ConvReluConvModel, convReluConv_node_list), (FunctionalConvReluModel, functionalConvRelu_node_list), (FunctionalConvReluConvModel, functionalConvReluConv_node_list)]\n    for (M, node_list) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        equalized_quantized_model = convert_fx(prepared)\n        self.checkGraphModuleNodes(equalized_quantized_model, expected_node_list=node_list)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_graphs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Tests that the modified model for equalization has the same graph\\n        structure as the model without equalization (before and after\\n        quantization).\\n        '\n    linear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    linearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_method('dequantize')]\n    linearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_method('dequantize')]\n    functionalLinearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    conv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    conv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    functionalConv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    convRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_method('dequantize')]\n    convReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConvRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_method('dequantize')]\n    functionalConvReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    tests = [(SingleLayerLinearModel, linear_node_list), (LinearAddModel, linearAdd_node_list), (TwoLayerLinearModel, linear2_node_list), (SingleLayerFunctionalLinearModel, functionalLinear_node_list), (FunctionalLinearAddModel, functionalLinearAdd_node_list), (TwoLayerFunctionalLinearModel, functionalLinear2_node_list), (LinearReluModel, linearRelu_node_list), (LinearReluLinearModel, linearReluLinear_node_list), (FunctionalLinearReluModel, functionalLinearRelu_node_list), (FunctionalLinearReluLinearModel, functionalLinearReluLinear_node_list), (ConvModel, conv_node_list), (TwoLayerConvModel, conv2_node_list), (SingleLayerFunctionalConvModel, functionalConv_node_list), (TwoLayerFunctionalConvModel, functionalConv2_node_list), (ConvReluModel, convRelu_node_list), (ConvReluConvModel, convReluConv_node_list), (FunctionalConvReluModel, functionalConvRelu_node_list), (FunctionalConvReluConvModel, functionalConvReluConv_node_list)]\n    for (M, node_list) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        equalized_quantized_model = convert_fx(prepared)\n        self.checkGraphModuleNodes(equalized_quantized_model, expected_node_list=node_list)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_graphs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Tests that the modified model for equalization has the same graph\\n        structure as the model without equalization (before and after\\n        quantization).\\n        '\n    linear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    linearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_method('dequantize')]\n    linearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_method('dequantize')]\n    functionalLinearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    conv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    conv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    functionalConv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    convRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_method('dequantize')]\n    convReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConvRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_method('dequantize')]\n    functionalConvReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    tests = [(SingleLayerLinearModel, linear_node_list), (LinearAddModel, linearAdd_node_list), (TwoLayerLinearModel, linear2_node_list), (SingleLayerFunctionalLinearModel, functionalLinear_node_list), (FunctionalLinearAddModel, functionalLinearAdd_node_list), (TwoLayerFunctionalLinearModel, functionalLinear2_node_list), (LinearReluModel, linearRelu_node_list), (LinearReluLinearModel, linearReluLinear_node_list), (FunctionalLinearReluModel, functionalLinearRelu_node_list), (FunctionalLinearReluLinearModel, functionalLinearReluLinear_node_list), (ConvModel, conv_node_list), (TwoLayerConvModel, conv2_node_list), (SingleLayerFunctionalConvModel, functionalConv_node_list), (TwoLayerFunctionalConvModel, functionalConv2_node_list), (ConvReluModel, convRelu_node_list), (ConvReluConvModel, convReluConv_node_list), (FunctionalConvReluModel, functionalConvRelu_node_list), (FunctionalConvReluConvModel, functionalConvReluConv_node_list)]\n    for (M, node_list) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        equalized_quantized_model = convert_fx(prepared)\n        self.checkGraphModuleNodes(equalized_quantized_model, expected_node_list=node_list)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_graphs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Tests that the modified model for equalization has the same graph\\n        structure as the model without equalization (before and after\\n        quantization).\\n        '\n    linear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    linear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinearAdd_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    functionalLinear2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    linearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_method('dequantize')]\n    linearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.LinearReLU), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    functionalLinearRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_method('dequantize')]\n    functionalLinearReluLinear_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.linear_relu), ns.call_function(torch.ops.quantized.linear), ns.call_method('dequantize')]\n    conv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    conv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Conv2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    functionalConv2_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    convRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_method('dequantize')]\n    convReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nniq.ConvReLU2d), ns.call_module(nnq.Conv2d), ns.call_method('dequantize')]\n    functionalConvRelu_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_method('dequantize')]\n    functionalConvReluConv_node_list = [ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_function(torch.ops.quantized.conv2d_relu), ns.call_function(torch.ops.quantized.conv2d), ns.call_method('dequantize')]\n    tests = [(SingleLayerLinearModel, linear_node_list), (LinearAddModel, linearAdd_node_list), (TwoLayerLinearModel, linear2_node_list), (SingleLayerFunctionalLinearModel, functionalLinear_node_list), (FunctionalLinearAddModel, functionalLinearAdd_node_list), (TwoLayerFunctionalLinearModel, functionalLinear2_node_list), (LinearReluModel, linearRelu_node_list), (LinearReluLinearModel, linearReluLinear_node_list), (FunctionalLinearReluModel, functionalLinearRelu_node_list), (FunctionalLinearReluLinearModel, functionalLinearReluLinear_node_list), (ConvModel, conv_node_list), (TwoLayerConvModel, conv2_node_list), (SingleLayerFunctionalConvModel, functionalConv_node_list), (TwoLayerFunctionalConvModel, functionalConv2_node_list), (ConvReluModel, convRelu_node_list), (ConvReluConvModel, convReluConv_node_list), (FunctionalConvReluModel, functionalConvRelu_node_list), (FunctionalConvReluConvModel, functionalConvReluConv_node_list)]\n    for (M, node_list) in tests:\n        m = M().eval()\n        example_inputs = m.get_example_inputs()\n        prepared = prepare_fx(m, specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        equalized_quantized_model = convert_fx(prepared)\n        self.checkGraphModuleNodes(equalized_quantized_model, expected_node_list=node_list)"
        ]
    },
    {
        "func_name": "test_input_weight_equalization_results",
        "original": "@skipIfNoFBGEMM\ndef test_input_weight_equalization_results(self):\n    \"\"\" Tests that for small models, the results of quantized models that\n        have been equalized are very close to models that have not been equalized.\n        \"\"\"\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, LinearAddModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config={})\n        prepared(x)\n        quantized = convert_fx(prepared)\n        quantized_output = quantized(x)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        equalized_and_quantized = convert_fx(prepared)\n        equalized_and_quantized_output = equalized_and_quantized(x)\n        self.assertEqual(quantized_output, equalized_and_quantized_output, rtol=1e-05, atol=0.1)",
        "mutated": [
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_results(self):\n    if False:\n        i = 10\n    ' Tests that for small models, the results of quantized models that\\n        have been equalized are very close to models that have not been equalized.\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, LinearAddModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config={})\n        prepared(x)\n        quantized = convert_fx(prepared)\n        quantized_output = quantized(x)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        equalized_and_quantized = convert_fx(prepared)\n        equalized_and_quantized_output = equalized_and_quantized(x)\n        self.assertEqual(quantized_output, equalized_and_quantized_output, rtol=1e-05, atol=0.1)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_results(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Tests that for small models, the results of quantized models that\\n        have been equalized are very close to models that have not been equalized.\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, LinearAddModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config={})\n        prepared(x)\n        quantized = convert_fx(prepared)\n        quantized_output = quantized(x)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        equalized_and_quantized = convert_fx(prepared)\n        equalized_and_quantized_output = equalized_and_quantized(x)\n        self.assertEqual(quantized_output, equalized_and_quantized_output, rtol=1e-05, atol=0.1)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_results(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Tests that for small models, the results of quantized models that\\n        have been equalized are very close to models that have not been equalized.\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, LinearAddModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config={})\n        prepared(x)\n        quantized = convert_fx(prepared)\n        quantized_output = quantized(x)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        equalized_and_quantized = convert_fx(prepared)\n        equalized_and_quantized_output = equalized_and_quantized(x)\n        self.assertEqual(quantized_output, equalized_and_quantized_output, rtol=1e-05, atol=0.1)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_results(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Tests that for small models, the results of quantized models that\\n        have been equalized are very close to models that have not been equalized.\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, LinearAddModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config={})\n        prepared(x)\n        quantized = convert_fx(prepared)\n        quantized_output = quantized(x)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        equalized_and_quantized = convert_fx(prepared)\n        equalized_and_quantized_output = equalized_and_quantized(x)\n        self.assertEqual(quantized_output, equalized_and_quantized_output, rtol=1e-05, atol=0.1)",
            "@skipIfNoFBGEMM\ndef test_input_weight_equalization_results(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Tests that for small models, the results of quantized models that\\n        have been equalized are very close to models that have not been equalized.\\n        '\n    tests = [SingleLayerLinearModel, TwoLayerLinearModel, LinearAddModel, SingleLayerFunctionalLinearModel, TwoLayerFunctionalLinearModel]\n    x = torch.rand((5, 5))\n    for M in tests:\n        m = M().eval()\n        example_inputs = (x,)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config={})\n        prepared(x)\n        quantized = convert_fx(prepared)\n        quantized_output = quantized(x)\n        prepared = prepare_fx(copy.deepcopy(m), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=default_equalization_qconfig_dict)\n        prepared(x)\n        equalized_and_quantized = convert_fx(prepared)\n        equalized_and_quantized_output = equalized_and_quantized(x)\n        self.assertEqual(quantized_output, equalized_and_quantized_output, rtol=1e-05, atol=0.1)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    super().__init__()\n    self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n    self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    super().__init__()\n    self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n    self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n    self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n    self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n    self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n    self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    x = self.bot(x)\n    x = torch.add(x, 5)\n    x = self.top(x)\n    return x",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    x = self.bot(x)\n    x = torch.add(x, 5)\n    x = self.top(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x = self.bot(x)\n    x = torch.add(x, 5)\n    x = self.top(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x = self.bot(x)\n    x = torch.add(x, 5)\n    x = self.top(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x = self.bot(x)\n    x = torch.add(x, 5)\n    x = self.top(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x = self.bot(x)\n    x = torch.add(x, 5)\n    x = self.top(x)\n    return x"
        ]
    },
    {
        "func_name": "test_selective_equalization",
        "original": "@skipIfNoFBGEMM\ndef test_selective_equalization(self):\n    \"\"\" Tests that we are able to run numeric suite on the equalized model\n        and construct a valid equalization_config equalizing only the top\n        4 layers with the highest quantization errors.\n        \"\"\"\n    torch.manual_seed(1)\n\n    class M(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n            self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))\n\n        def forward(self, x):\n            x = self.bot(x)\n            x = torch.add(x, 5)\n            x = self.top(x)\n            return x\n    float_model = M().eval()\n    x = torch.tensor([[0.0642, 0.7824, 0.4255, 0.7106, 0.5957], [0.8373, 0.8851, 0.8229, 0.0212, 0.8987], [0.9077, 0.7538, 0.453, 0.5772, 0.1376], [0.069, 0.9002, 0.7998, 0.2768, 0.8985], [0.0282, 0.5068, 0.6725, 0.1829, 0.548]])\n    example_inputs = (x,)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs)\n    prepared_model(x)\n    quantized_model = convert_fx(copy.deepcopy(prepared_model))\n    layer_to_sqnr_dict = get_layer_sqnr_dict(copy.deepcopy(prepared_model), quantized_model, x)\n    selective_equalization_qconfig_dict = get_equalization_qconfig_dict(layer_to_sqnr_dict, 1)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=selective_equalization_qconfig_dict)\n    prepared_model(x)\n    equalized_model = convert_fx(prepared_model)\n    node_list = [ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    self.checkGraphModuleNodes(equalized_model, expected_node_list=node_list)",
        "mutated": [
            "@skipIfNoFBGEMM\ndef test_selective_equalization(self):\n    if False:\n        i = 10\n    ' Tests that we are able to run numeric suite on the equalized model\\n        and construct a valid equalization_config equalizing only the top\\n        4 layers with the highest quantization errors.\\n        '\n    torch.manual_seed(1)\n\n    class M(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n            self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))\n\n        def forward(self, x):\n            x = self.bot(x)\n            x = torch.add(x, 5)\n            x = self.top(x)\n            return x\n    float_model = M().eval()\n    x = torch.tensor([[0.0642, 0.7824, 0.4255, 0.7106, 0.5957], [0.8373, 0.8851, 0.8229, 0.0212, 0.8987], [0.9077, 0.7538, 0.453, 0.5772, 0.1376], [0.069, 0.9002, 0.7998, 0.2768, 0.8985], [0.0282, 0.5068, 0.6725, 0.1829, 0.548]])\n    example_inputs = (x,)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs)\n    prepared_model(x)\n    quantized_model = convert_fx(copy.deepcopy(prepared_model))\n    layer_to_sqnr_dict = get_layer_sqnr_dict(copy.deepcopy(prepared_model), quantized_model, x)\n    selective_equalization_qconfig_dict = get_equalization_qconfig_dict(layer_to_sqnr_dict, 1)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=selective_equalization_qconfig_dict)\n    prepared_model(x)\n    equalized_model = convert_fx(prepared_model)\n    node_list = [ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    self.checkGraphModuleNodes(equalized_model, expected_node_list=node_list)",
            "@skipIfNoFBGEMM\ndef test_selective_equalization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Tests that we are able to run numeric suite on the equalized model\\n        and construct a valid equalization_config equalizing only the top\\n        4 layers with the highest quantization errors.\\n        '\n    torch.manual_seed(1)\n\n    class M(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n            self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))\n\n        def forward(self, x):\n            x = self.bot(x)\n            x = torch.add(x, 5)\n            x = self.top(x)\n            return x\n    float_model = M().eval()\n    x = torch.tensor([[0.0642, 0.7824, 0.4255, 0.7106, 0.5957], [0.8373, 0.8851, 0.8229, 0.0212, 0.8987], [0.9077, 0.7538, 0.453, 0.5772, 0.1376], [0.069, 0.9002, 0.7998, 0.2768, 0.8985], [0.0282, 0.5068, 0.6725, 0.1829, 0.548]])\n    example_inputs = (x,)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs)\n    prepared_model(x)\n    quantized_model = convert_fx(copy.deepcopy(prepared_model))\n    layer_to_sqnr_dict = get_layer_sqnr_dict(copy.deepcopy(prepared_model), quantized_model, x)\n    selective_equalization_qconfig_dict = get_equalization_qconfig_dict(layer_to_sqnr_dict, 1)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=selective_equalization_qconfig_dict)\n    prepared_model(x)\n    equalized_model = convert_fx(prepared_model)\n    node_list = [ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    self.checkGraphModuleNodes(equalized_model, expected_node_list=node_list)",
            "@skipIfNoFBGEMM\ndef test_selective_equalization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Tests that we are able to run numeric suite on the equalized model\\n        and construct a valid equalization_config equalizing only the top\\n        4 layers with the highest quantization errors.\\n        '\n    torch.manual_seed(1)\n\n    class M(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n            self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))\n\n        def forward(self, x):\n            x = self.bot(x)\n            x = torch.add(x, 5)\n            x = self.top(x)\n            return x\n    float_model = M().eval()\n    x = torch.tensor([[0.0642, 0.7824, 0.4255, 0.7106, 0.5957], [0.8373, 0.8851, 0.8229, 0.0212, 0.8987], [0.9077, 0.7538, 0.453, 0.5772, 0.1376], [0.069, 0.9002, 0.7998, 0.2768, 0.8985], [0.0282, 0.5068, 0.6725, 0.1829, 0.548]])\n    example_inputs = (x,)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs)\n    prepared_model(x)\n    quantized_model = convert_fx(copy.deepcopy(prepared_model))\n    layer_to_sqnr_dict = get_layer_sqnr_dict(copy.deepcopy(prepared_model), quantized_model, x)\n    selective_equalization_qconfig_dict = get_equalization_qconfig_dict(layer_to_sqnr_dict, 1)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=selective_equalization_qconfig_dict)\n    prepared_model(x)\n    equalized_model = convert_fx(prepared_model)\n    node_list = [ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    self.checkGraphModuleNodes(equalized_model, expected_node_list=node_list)",
            "@skipIfNoFBGEMM\ndef test_selective_equalization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Tests that we are able to run numeric suite on the equalized model\\n        and construct a valid equalization_config equalizing only the top\\n        4 layers with the highest quantization errors.\\n        '\n    torch.manual_seed(1)\n\n    class M(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n            self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))\n\n        def forward(self, x):\n            x = self.bot(x)\n            x = torch.add(x, 5)\n            x = self.top(x)\n            return x\n    float_model = M().eval()\n    x = torch.tensor([[0.0642, 0.7824, 0.4255, 0.7106, 0.5957], [0.8373, 0.8851, 0.8229, 0.0212, 0.8987], [0.9077, 0.7538, 0.453, 0.5772, 0.1376], [0.069, 0.9002, 0.7998, 0.2768, 0.8985], [0.0282, 0.5068, 0.6725, 0.1829, 0.548]])\n    example_inputs = (x,)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs)\n    prepared_model(x)\n    quantized_model = convert_fx(copy.deepcopy(prepared_model))\n    layer_to_sqnr_dict = get_layer_sqnr_dict(copy.deepcopy(prepared_model), quantized_model, x)\n    selective_equalization_qconfig_dict = get_equalization_qconfig_dict(layer_to_sqnr_dict, 1)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=selective_equalization_qconfig_dict)\n    prepared_model(x)\n    equalized_model = convert_fx(prepared_model)\n    node_list = [ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    self.checkGraphModuleNodes(equalized_model, expected_node_list=node_list)",
            "@skipIfNoFBGEMM\ndef test_selective_equalization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Tests that we are able to run numeric suite on the equalized model\\n        and construct a valid equalization_config equalizing only the top\\n        4 layers with the highest quantization errors.\\n        '\n    torch.manual_seed(1)\n\n    class M(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            self.bot = torch.nn.Sequential(torch.nn.Linear(5, 5))\n            self.top = torch.nn.Sequential(torch.nn.Linear(5, 5))\n\n        def forward(self, x):\n            x = self.bot(x)\n            x = torch.add(x, 5)\n            x = self.top(x)\n            return x\n    float_model = M().eval()\n    x = torch.tensor([[0.0642, 0.7824, 0.4255, 0.7106, 0.5957], [0.8373, 0.8851, 0.8229, 0.0212, 0.8987], [0.9077, 0.7538, 0.453, 0.5772, 0.1376], [0.069, 0.9002, 0.7998, 0.2768, 0.8985], [0.0282, 0.5068, 0.6725, 0.1829, 0.548]])\n    example_inputs = (x,)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs)\n    prepared_model(x)\n    quantized_model = convert_fx(copy.deepcopy(prepared_model))\n    layer_to_sqnr_dict = get_layer_sqnr_dict(copy.deepcopy(prepared_model), quantized_model, x)\n    selective_equalization_qconfig_dict = get_equalization_qconfig_dict(layer_to_sqnr_dict, 1)\n    prepared_model = prepare_fx(copy.deepcopy(float_model), specific_qconfig_dict, example_inputs=example_inputs, _equalization_config=selective_equalization_qconfig_dict)\n    prepared_model(x)\n    equalized_model = convert_fx(prepared_model)\n    node_list = [ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize'), ns.call_function(torch.add), ns.call_function(torch.mul), ns.call_function(torch.quantize_per_tensor), ns.call_module(nnq.Linear), ns.call_method('dequantize')]\n    self.checkGraphModuleNodes(equalized_model, expected_node_list=node_list)"
        ]
    }
]