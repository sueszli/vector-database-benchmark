[
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    self.global_ring_id = 0\n    self.batch_size = 10\n    self.num_class = 1000\n    self.nranks = 2\n    self.ring_id = 0\n    self.local_elements = int(self.num_class / self.nranks)",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    self.global_ring_id = 0\n    self.batch_size = 10\n    self.num_class = 1000\n    self.nranks = 2\n    self.ring_id = 0\n    self.local_elements = int(self.num_class / self.nranks)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.global_ring_id = 0\n    self.batch_size = 10\n    self.num_class = 1000\n    self.nranks = 2\n    self.ring_id = 0\n    self.local_elements = int(self.num_class / self.nranks)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.global_ring_id = 0\n    self.batch_size = 10\n    self.num_class = 1000\n    self.nranks = 2\n    self.ring_id = 0\n    self.local_elements = int(self.num_class / self.nranks)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.global_ring_id = 0\n    self.batch_size = 10\n    self.num_class = 1000\n    self.nranks = 2\n    self.ring_id = 0\n    self.local_elements = int(self.num_class / self.nranks)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.global_ring_id = 0\n    self.batch_size = 10\n    self.num_class = 1000\n    self.nranks = 2\n    self.ring_id = 0\n    self.local_elements = int(self.num_class / self.nranks)"
        ]
    },
    {
        "func_name": "get_model",
        "original": "def get_model(self, main_prog, startup_program, rank):\n    with program_guard(main_prog, startup_program):\n        logits = data(name='Logits', shape=[self.batch_size, self.local_elements], dtype='float32')\n        label = data(name='Label', shape=[self.batch_size, 1], dtype='int32')\n        softmax = main_prog.current_block().create_var(name='Softmax', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss = main_prog.current_block().create_var(name='Loss', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss_grad = main_prog.current_block().create_var(name='Loss@GRAD', shape=[self.batch_size, 1], dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        block = main_prog.global_block()\n        with paddle.static.device_guard('xpu'):\n            c_softmax_with_ce_op = block.append_op(type='c_softmax_with_cross_entropy', inputs={'Logits': logits, 'Label': label}, outputs={'Softmax': softmax, 'Loss': loss}, attrs={'ring_id': self.ring_id, 'rank': rank, 'nranks': self.nranks})\n            (grad_op_desc_list, op_grad_to_var) = core.get_grad_op_desc(c_softmax_with_ce_op.desc, set(), [])\n            for grad_op_desc in grad_op_desc_list:\n                new_op_desc = block.desc.append_op()\n                new_op_desc.copy_from(grad_op_desc)\n                for var_name in grad_op_desc.output_arg_names():\n                    block.desc.var(var_name.encode('ascii'))\n                grad_op_desc.infer_var_type(block.desc)\n                grad_op_desc.infer_shape(block.desc)\n                for arg in grad_op_desc.output_arg_names():\n                    grad_var = block.desc.find_var(arg.encode('ascii'))\n                    grad_var.set_dtype(core.VarDesc.VarType.FP32)\n                main_prog._sync_with_cpp()\n        return (loss, softmax)",
        "mutated": [
            "def get_model(self, main_prog, startup_program, rank):\n    if False:\n        i = 10\n    with program_guard(main_prog, startup_program):\n        logits = data(name='Logits', shape=[self.batch_size, self.local_elements], dtype='float32')\n        label = data(name='Label', shape=[self.batch_size, 1], dtype='int32')\n        softmax = main_prog.current_block().create_var(name='Softmax', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss = main_prog.current_block().create_var(name='Loss', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss_grad = main_prog.current_block().create_var(name='Loss@GRAD', shape=[self.batch_size, 1], dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        block = main_prog.global_block()\n        with paddle.static.device_guard('xpu'):\n            c_softmax_with_ce_op = block.append_op(type='c_softmax_with_cross_entropy', inputs={'Logits': logits, 'Label': label}, outputs={'Softmax': softmax, 'Loss': loss}, attrs={'ring_id': self.ring_id, 'rank': rank, 'nranks': self.nranks})\n            (grad_op_desc_list, op_grad_to_var) = core.get_grad_op_desc(c_softmax_with_ce_op.desc, set(), [])\n            for grad_op_desc in grad_op_desc_list:\n                new_op_desc = block.desc.append_op()\n                new_op_desc.copy_from(grad_op_desc)\n                for var_name in grad_op_desc.output_arg_names():\n                    block.desc.var(var_name.encode('ascii'))\n                grad_op_desc.infer_var_type(block.desc)\n                grad_op_desc.infer_shape(block.desc)\n                for arg in grad_op_desc.output_arg_names():\n                    grad_var = block.desc.find_var(arg.encode('ascii'))\n                    grad_var.set_dtype(core.VarDesc.VarType.FP32)\n                main_prog._sync_with_cpp()\n        return (loss, softmax)",
            "def get_model(self, main_prog, startup_program, rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with program_guard(main_prog, startup_program):\n        logits = data(name='Logits', shape=[self.batch_size, self.local_elements], dtype='float32')\n        label = data(name='Label', shape=[self.batch_size, 1], dtype='int32')\n        softmax = main_prog.current_block().create_var(name='Softmax', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss = main_prog.current_block().create_var(name='Loss', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss_grad = main_prog.current_block().create_var(name='Loss@GRAD', shape=[self.batch_size, 1], dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        block = main_prog.global_block()\n        with paddle.static.device_guard('xpu'):\n            c_softmax_with_ce_op = block.append_op(type='c_softmax_with_cross_entropy', inputs={'Logits': logits, 'Label': label}, outputs={'Softmax': softmax, 'Loss': loss}, attrs={'ring_id': self.ring_id, 'rank': rank, 'nranks': self.nranks})\n            (grad_op_desc_list, op_grad_to_var) = core.get_grad_op_desc(c_softmax_with_ce_op.desc, set(), [])\n            for grad_op_desc in grad_op_desc_list:\n                new_op_desc = block.desc.append_op()\n                new_op_desc.copy_from(grad_op_desc)\n                for var_name in grad_op_desc.output_arg_names():\n                    block.desc.var(var_name.encode('ascii'))\n                grad_op_desc.infer_var_type(block.desc)\n                grad_op_desc.infer_shape(block.desc)\n                for arg in grad_op_desc.output_arg_names():\n                    grad_var = block.desc.find_var(arg.encode('ascii'))\n                    grad_var.set_dtype(core.VarDesc.VarType.FP32)\n                main_prog._sync_with_cpp()\n        return (loss, softmax)",
            "def get_model(self, main_prog, startup_program, rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with program_guard(main_prog, startup_program):\n        logits = data(name='Logits', shape=[self.batch_size, self.local_elements], dtype='float32')\n        label = data(name='Label', shape=[self.batch_size, 1], dtype='int32')\n        softmax = main_prog.current_block().create_var(name='Softmax', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss = main_prog.current_block().create_var(name='Loss', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss_grad = main_prog.current_block().create_var(name='Loss@GRAD', shape=[self.batch_size, 1], dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        block = main_prog.global_block()\n        with paddle.static.device_guard('xpu'):\n            c_softmax_with_ce_op = block.append_op(type='c_softmax_with_cross_entropy', inputs={'Logits': logits, 'Label': label}, outputs={'Softmax': softmax, 'Loss': loss}, attrs={'ring_id': self.ring_id, 'rank': rank, 'nranks': self.nranks})\n            (grad_op_desc_list, op_grad_to_var) = core.get_grad_op_desc(c_softmax_with_ce_op.desc, set(), [])\n            for grad_op_desc in grad_op_desc_list:\n                new_op_desc = block.desc.append_op()\n                new_op_desc.copy_from(grad_op_desc)\n                for var_name in grad_op_desc.output_arg_names():\n                    block.desc.var(var_name.encode('ascii'))\n                grad_op_desc.infer_var_type(block.desc)\n                grad_op_desc.infer_shape(block.desc)\n                for arg in grad_op_desc.output_arg_names():\n                    grad_var = block.desc.find_var(arg.encode('ascii'))\n                    grad_var.set_dtype(core.VarDesc.VarType.FP32)\n                main_prog._sync_with_cpp()\n        return (loss, softmax)",
            "def get_model(self, main_prog, startup_program, rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with program_guard(main_prog, startup_program):\n        logits = data(name='Logits', shape=[self.batch_size, self.local_elements], dtype='float32')\n        label = data(name='Label', shape=[self.batch_size, 1], dtype='int32')\n        softmax = main_prog.current_block().create_var(name='Softmax', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss = main_prog.current_block().create_var(name='Loss', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss_grad = main_prog.current_block().create_var(name='Loss@GRAD', shape=[self.batch_size, 1], dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        block = main_prog.global_block()\n        with paddle.static.device_guard('xpu'):\n            c_softmax_with_ce_op = block.append_op(type='c_softmax_with_cross_entropy', inputs={'Logits': logits, 'Label': label}, outputs={'Softmax': softmax, 'Loss': loss}, attrs={'ring_id': self.ring_id, 'rank': rank, 'nranks': self.nranks})\n            (grad_op_desc_list, op_grad_to_var) = core.get_grad_op_desc(c_softmax_with_ce_op.desc, set(), [])\n            for grad_op_desc in grad_op_desc_list:\n                new_op_desc = block.desc.append_op()\n                new_op_desc.copy_from(grad_op_desc)\n                for var_name in grad_op_desc.output_arg_names():\n                    block.desc.var(var_name.encode('ascii'))\n                grad_op_desc.infer_var_type(block.desc)\n                grad_op_desc.infer_shape(block.desc)\n                for arg in grad_op_desc.output_arg_names():\n                    grad_var = block.desc.find_var(arg.encode('ascii'))\n                    grad_var.set_dtype(core.VarDesc.VarType.FP32)\n                main_prog._sync_with_cpp()\n        return (loss, softmax)",
            "def get_model(self, main_prog, startup_program, rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with program_guard(main_prog, startup_program):\n        logits = data(name='Logits', shape=[self.batch_size, self.local_elements], dtype='float32')\n        label = data(name='Label', shape=[self.batch_size, 1], dtype='int32')\n        softmax = main_prog.current_block().create_var(name='Softmax', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss = main_prog.current_block().create_var(name='Loss', dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        loss_grad = main_prog.current_block().create_var(name='Loss@GRAD', shape=[self.batch_size, 1], dtype=logits.dtype, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=False)\n        block = main_prog.global_block()\n        with paddle.static.device_guard('xpu'):\n            c_softmax_with_ce_op = block.append_op(type='c_softmax_with_cross_entropy', inputs={'Logits': logits, 'Label': label}, outputs={'Softmax': softmax, 'Loss': loss}, attrs={'ring_id': self.ring_id, 'rank': rank, 'nranks': self.nranks})\n            (grad_op_desc_list, op_grad_to_var) = core.get_grad_op_desc(c_softmax_with_ce_op.desc, set(), [])\n            for grad_op_desc in grad_op_desc_list:\n                new_op_desc = block.desc.append_op()\n                new_op_desc.copy_from(grad_op_desc)\n                for var_name in grad_op_desc.output_arg_names():\n                    block.desc.var(var_name.encode('ascii'))\n                grad_op_desc.infer_var_type(block.desc)\n                grad_op_desc.infer_shape(block.desc)\n                for arg in grad_op_desc.output_arg_names():\n                    grad_var = block.desc.find_var(arg.encode('ascii'))\n                    grad_var.set_dtype(core.VarDesc.VarType.FP32)\n                main_prog._sync_with_cpp()\n        return (loss, softmax)"
        ]
    },
    {
        "func_name": "run_trainer",
        "original": "def run_trainer(self, args):\n    train_prog = Program()\n    startup_prog = Program()\n    endpoints = args['endpoints'].split(',')\n    rank = args['trainerid']\n    current_endpoint = args['currentendpoint']\n    self.initCommunicator(startup_prog, rank, self.nranks, True, current_endpoint, endpoints)\n    np_data_type = DataTypeCast(args['data_type'])\n    (loss, softmax) = self.get_model(train_prog, startup_prog, rank)\n    device_id = int(os.getenv('FLAGS_selected_xpus', '0'))\n    place = paddle.XPUPlace(device_id)\n    exe = Executor(place)\n    exe.run(startup_prog)\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    loss_grad = np.random.uniform(low=-10.0, high=10.0, size=(self.batch_size, 1)).astype(np_data_type)\n    np.random.seed(os.getpid())\n    logits = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, self.local_elements)).astype(np_data_type)\n    out = exe.run(train_prog, feed={'Logits': logits, 'Label': label, 'Loss@GRAD': loss_grad}, fetch_list=[loss.name, softmax.name, 'Logits@GRAD'])\n    sys.stdout.buffer.write(pickle.dumps(out))",
        "mutated": [
            "def run_trainer(self, args):\n    if False:\n        i = 10\n    train_prog = Program()\n    startup_prog = Program()\n    endpoints = args['endpoints'].split(',')\n    rank = args['trainerid']\n    current_endpoint = args['currentendpoint']\n    self.initCommunicator(startup_prog, rank, self.nranks, True, current_endpoint, endpoints)\n    np_data_type = DataTypeCast(args['data_type'])\n    (loss, softmax) = self.get_model(train_prog, startup_prog, rank)\n    device_id = int(os.getenv('FLAGS_selected_xpus', '0'))\n    place = paddle.XPUPlace(device_id)\n    exe = Executor(place)\n    exe.run(startup_prog)\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    loss_grad = np.random.uniform(low=-10.0, high=10.0, size=(self.batch_size, 1)).astype(np_data_type)\n    np.random.seed(os.getpid())\n    logits = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, self.local_elements)).astype(np_data_type)\n    out = exe.run(train_prog, feed={'Logits': logits, 'Label': label, 'Loss@GRAD': loss_grad}, fetch_list=[loss.name, softmax.name, 'Logits@GRAD'])\n    sys.stdout.buffer.write(pickle.dumps(out))",
            "def run_trainer(self, args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    train_prog = Program()\n    startup_prog = Program()\n    endpoints = args['endpoints'].split(',')\n    rank = args['trainerid']\n    current_endpoint = args['currentendpoint']\n    self.initCommunicator(startup_prog, rank, self.nranks, True, current_endpoint, endpoints)\n    np_data_type = DataTypeCast(args['data_type'])\n    (loss, softmax) = self.get_model(train_prog, startup_prog, rank)\n    device_id = int(os.getenv('FLAGS_selected_xpus', '0'))\n    place = paddle.XPUPlace(device_id)\n    exe = Executor(place)\n    exe.run(startup_prog)\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    loss_grad = np.random.uniform(low=-10.0, high=10.0, size=(self.batch_size, 1)).astype(np_data_type)\n    np.random.seed(os.getpid())\n    logits = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, self.local_elements)).astype(np_data_type)\n    out = exe.run(train_prog, feed={'Logits': logits, 'Label': label, 'Loss@GRAD': loss_grad}, fetch_list=[loss.name, softmax.name, 'Logits@GRAD'])\n    sys.stdout.buffer.write(pickle.dumps(out))",
            "def run_trainer(self, args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    train_prog = Program()\n    startup_prog = Program()\n    endpoints = args['endpoints'].split(',')\n    rank = args['trainerid']\n    current_endpoint = args['currentendpoint']\n    self.initCommunicator(startup_prog, rank, self.nranks, True, current_endpoint, endpoints)\n    np_data_type = DataTypeCast(args['data_type'])\n    (loss, softmax) = self.get_model(train_prog, startup_prog, rank)\n    device_id = int(os.getenv('FLAGS_selected_xpus', '0'))\n    place = paddle.XPUPlace(device_id)\n    exe = Executor(place)\n    exe.run(startup_prog)\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    loss_grad = np.random.uniform(low=-10.0, high=10.0, size=(self.batch_size, 1)).astype(np_data_type)\n    np.random.seed(os.getpid())\n    logits = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, self.local_elements)).astype(np_data_type)\n    out = exe.run(train_prog, feed={'Logits': logits, 'Label': label, 'Loss@GRAD': loss_grad}, fetch_list=[loss.name, softmax.name, 'Logits@GRAD'])\n    sys.stdout.buffer.write(pickle.dumps(out))",
            "def run_trainer(self, args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    train_prog = Program()\n    startup_prog = Program()\n    endpoints = args['endpoints'].split(',')\n    rank = args['trainerid']\n    current_endpoint = args['currentendpoint']\n    self.initCommunicator(startup_prog, rank, self.nranks, True, current_endpoint, endpoints)\n    np_data_type = DataTypeCast(args['data_type'])\n    (loss, softmax) = self.get_model(train_prog, startup_prog, rank)\n    device_id = int(os.getenv('FLAGS_selected_xpus', '0'))\n    place = paddle.XPUPlace(device_id)\n    exe = Executor(place)\n    exe.run(startup_prog)\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    loss_grad = np.random.uniform(low=-10.0, high=10.0, size=(self.batch_size, 1)).astype(np_data_type)\n    np.random.seed(os.getpid())\n    logits = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, self.local_elements)).astype(np_data_type)\n    out = exe.run(train_prog, feed={'Logits': logits, 'Label': label, 'Loss@GRAD': loss_grad}, fetch_list=[loss.name, softmax.name, 'Logits@GRAD'])\n    sys.stdout.buffer.write(pickle.dumps(out))",
            "def run_trainer(self, args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    train_prog = Program()\n    startup_prog = Program()\n    endpoints = args['endpoints'].split(',')\n    rank = args['trainerid']\n    current_endpoint = args['currentendpoint']\n    self.initCommunicator(startup_prog, rank, self.nranks, True, current_endpoint, endpoints)\n    np_data_type = DataTypeCast(args['data_type'])\n    (loss, softmax) = self.get_model(train_prog, startup_prog, rank)\n    device_id = int(os.getenv('FLAGS_selected_xpus', '0'))\n    place = paddle.XPUPlace(device_id)\n    exe = Executor(place)\n    exe.run(startup_prog)\n    np.random.seed(os.getuid())\n    label = np.random.randint(0, self.num_class, size=(self.batch_size, 1), dtype='int32')\n    loss_grad = np.random.uniform(low=-10.0, high=10.0, size=(self.batch_size, 1)).astype(np_data_type)\n    np.random.seed(os.getpid())\n    logits = np.random.uniform(low=-40.0, high=40.0, size=(self.batch_size, self.local_elements)).astype(np_data_type)\n    out = exe.run(train_prog, feed={'Logits': logits, 'Label': label, 'Loss@GRAD': loss_grad}, fetch_list=[loss.name, softmax.name, 'Logits@GRAD'])\n    sys.stdout.buffer.write(pickle.dumps(out))"
        ]
    }
]