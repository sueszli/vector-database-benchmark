[
    {
        "func_name": "_get_job_info",
        "original": "def _get_job_info(self, player: ActivePlayer, eval_flag: bool=False) -> dict:\n    assert isinstance(player, ActivePlayer), player.__class__\n    player_job_info = EasyDict(player.get_job(eval_flag))\n    return {'agent_num': 2, 'launch_player': player.player_id, 'player_id': [player.player_id, player_job_info.opponent.player_id], 'checkpoint_path': [player.checkpoint_path, player_job_info.opponent.checkpoint_path], 'player_active_flag': [isinstance(p, ActivePlayer) for p in [player, player_job_info.opponent]]}",
        "mutated": [
            "def _get_job_info(self, player: ActivePlayer, eval_flag: bool=False) -> dict:\n    if False:\n        i = 10\n    assert isinstance(player, ActivePlayer), player.__class__\n    player_job_info = EasyDict(player.get_job(eval_flag))\n    return {'agent_num': 2, 'launch_player': player.player_id, 'player_id': [player.player_id, player_job_info.opponent.player_id], 'checkpoint_path': [player.checkpoint_path, player_job_info.opponent.checkpoint_path], 'player_active_flag': [isinstance(p, ActivePlayer) for p in [player, player_job_info.opponent]]}",
            "def _get_job_info(self, player: ActivePlayer, eval_flag: bool=False) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert isinstance(player, ActivePlayer), player.__class__\n    player_job_info = EasyDict(player.get_job(eval_flag))\n    return {'agent_num': 2, 'launch_player': player.player_id, 'player_id': [player.player_id, player_job_info.opponent.player_id], 'checkpoint_path': [player.checkpoint_path, player_job_info.opponent.checkpoint_path], 'player_active_flag': [isinstance(p, ActivePlayer) for p in [player, player_job_info.opponent]]}",
            "def _get_job_info(self, player: ActivePlayer, eval_flag: bool=False) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert isinstance(player, ActivePlayer), player.__class__\n    player_job_info = EasyDict(player.get_job(eval_flag))\n    return {'agent_num': 2, 'launch_player': player.player_id, 'player_id': [player.player_id, player_job_info.opponent.player_id], 'checkpoint_path': [player.checkpoint_path, player_job_info.opponent.checkpoint_path], 'player_active_flag': [isinstance(p, ActivePlayer) for p in [player, player_job_info.opponent]]}",
            "def _get_job_info(self, player: ActivePlayer, eval_flag: bool=False) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert isinstance(player, ActivePlayer), player.__class__\n    player_job_info = EasyDict(player.get_job(eval_flag))\n    return {'agent_num': 2, 'launch_player': player.player_id, 'player_id': [player.player_id, player_job_info.opponent.player_id], 'checkpoint_path': [player.checkpoint_path, player_job_info.opponent.checkpoint_path], 'player_active_flag': [isinstance(p, ActivePlayer) for p in [player, player_job_info.opponent]]}",
            "def _get_job_info(self, player: ActivePlayer, eval_flag: bool=False) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert isinstance(player, ActivePlayer), player.__class__\n    player_job_info = EasyDict(player.get_job(eval_flag))\n    return {'agent_num': 2, 'launch_player': player.player_id, 'player_id': [player.player_id, player_job_info.opponent.player_id], 'checkpoint_path': [player.checkpoint_path, player_job_info.opponent.checkpoint_path], 'player_active_flag': [isinstance(p, ActivePlayer) for p in [player, player_job_info.opponent]]}"
        ]
    },
    {
        "func_name": "_mutate_player",
        "original": "def _mutate_player(self, player: ActivePlayer):\n    pass",
        "mutated": [
            "def _mutate_player(self, player: ActivePlayer):\n    if False:\n        i = 10\n    pass",
            "def _mutate_player(self, player: ActivePlayer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "def _mutate_player(self, player: ActivePlayer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "def _mutate_player(self, player: ActivePlayer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "def _mutate_player(self, player: ActivePlayer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "_update_player",
        "original": "def _update_player(self, player: ActivePlayer, player_info: dict) -> None:\n    assert isinstance(player, ActivePlayer)\n    if 'learner_step' in player_info:\n        player.total_agent_step = player_info['learner_step']",
        "mutated": [
            "def _update_player(self, player: ActivePlayer, player_info: dict) -> None:\n    if False:\n        i = 10\n    assert isinstance(player, ActivePlayer)\n    if 'learner_step' in player_info:\n        player.total_agent_step = player_info['learner_step']",
            "def _update_player(self, player: ActivePlayer, player_info: dict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert isinstance(player, ActivePlayer)\n    if 'learner_step' in player_info:\n        player.total_agent_step = player_info['learner_step']",
            "def _update_player(self, player: ActivePlayer, player_info: dict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert isinstance(player, ActivePlayer)\n    if 'learner_step' in player_info:\n        player.total_agent_step = player_info['learner_step']",
            "def _update_player(self, player: ActivePlayer, player_info: dict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert isinstance(player, ActivePlayer)\n    if 'learner_step' in player_info:\n        player.total_agent_step = player_info['learner_step']",
            "def _update_player(self, player: ActivePlayer, player_info: dict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert isinstance(player, ActivePlayer)\n    if 'learner_step' in player_info:\n        player.total_agent_step = player_info['learner_step']"
        ]
    },
    {
        "func_name": "save_checkpoint",
        "original": "@staticmethod\ndef save_checkpoint(src_checkpoint_path: str, dst_checkpoint_path: str) -> None:\n    shutil.copy(src_checkpoint_path, dst_checkpoint_path)",
        "mutated": [
            "@staticmethod\ndef save_checkpoint(src_checkpoint_path: str, dst_checkpoint_path: str) -> None:\n    if False:\n        i = 10\n    shutil.copy(src_checkpoint_path, dst_checkpoint_path)",
            "@staticmethod\ndef save_checkpoint(src_checkpoint_path: str, dst_checkpoint_path: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    shutil.copy(src_checkpoint_path, dst_checkpoint_path)",
            "@staticmethod\ndef save_checkpoint(src_checkpoint_path: str, dst_checkpoint_path: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    shutil.copy(src_checkpoint_path, dst_checkpoint_path)",
            "@staticmethod\ndef save_checkpoint(src_checkpoint_path: str, dst_checkpoint_path: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    shutil.copy(src_checkpoint_path, dst_checkpoint_path)",
            "@staticmethod\ndef save_checkpoint(src_checkpoint_path: str, dst_checkpoint_path: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    shutil.copy(src_checkpoint_path, dst_checkpoint_path)"
        ]
    },
    {
        "func_name": "load_checkpoint_fn",
        "original": "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)",
        "mutated": [
            "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    if False:\n        i = 10\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)",
            "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)",
            "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)",
            "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)",
            "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(cfg, seed=0):\n    cfg = compile_config(cfg, SyncSubprocessEnvManager, PPOPolicy, BaseLearner, BattleEpisodeSerialCollector, InteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    collector_env_cfg = copy.deepcopy(cfg.env)\n    collector_env_cfg.agent_vs_agent = True\n    evaluator_env_cfg = copy.deepcopy(cfg.env)\n    evaluator_env_cfg.agent_vs_agent = False\n    evaluator_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, evaluator_env_cfg) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env.seed(seed, dynamic_seed=False)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = MyLeague(cfg.policy.other.league)\n    (policies, learners, collectors) = ({}, {}, {})\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, collector_env_cfg) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = BattleEpisodeSerialCollector(cfg.policy.collect.collector, collector_env, [policy.collect_mode, policy.collect_mode], tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator_cfg.stop_value = cfg.env.stop_value\n    evaluator = InteractionSerialEvaluator(evaluator_cfg, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name, instance_name='builtin_ai_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    count = 0\n    while True:\n        if evaluator.should_eval(main_learner.train_iter):\n            (stop_flag, eval_episode_info) = evaluator.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in eval_episode_info]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=100, sigma=1e-08), win_loss_result)\n            if stop_flag:\n                break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (new_data, episode_info) = collector.collect(train_iter=learner.train_iter, n_episode=cfg.policy.collect.n_episode)\n            train_data = sum(new_data[0], [])\n            learner.train(train_data, collector.envstep)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info[0]]}\n            league.finish_job(job_finish_info)\n        if count % 50 == 0:\n            payoff_string = repr(league.payoff)\n            rank_string = league.player_rank(string=True)\n            tb_logger.add_text('payoff_step', payoff_string, main_collector.envstep)\n            tb_logger.add_text('rank_step', rank_string, main_collector.envstep)\n        count += 1",
        "mutated": [
            "def main(cfg, seed=0):\n    if False:\n        i = 10\n    cfg = compile_config(cfg, SyncSubprocessEnvManager, PPOPolicy, BaseLearner, BattleEpisodeSerialCollector, InteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    collector_env_cfg = copy.deepcopy(cfg.env)\n    collector_env_cfg.agent_vs_agent = True\n    evaluator_env_cfg = copy.deepcopy(cfg.env)\n    evaluator_env_cfg.agent_vs_agent = False\n    evaluator_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, evaluator_env_cfg) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env.seed(seed, dynamic_seed=False)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = MyLeague(cfg.policy.other.league)\n    (policies, learners, collectors) = ({}, {}, {})\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, collector_env_cfg) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = BattleEpisodeSerialCollector(cfg.policy.collect.collector, collector_env, [policy.collect_mode, policy.collect_mode], tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator_cfg.stop_value = cfg.env.stop_value\n    evaluator = InteractionSerialEvaluator(evaluator_cfg, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name, instance_name='builtin_ai_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    count = 0\n    while True:\n        if evaluator.should_eval(main_learner.train_iter):\n            (stop_flag, eval_episode_info) = evaluator.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in eval_episode_info]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=100, sigma=1e-08), win_loss_result)\n            if stop_flag:\n                break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (new_data, episode_info) = collector.collect(train_iter=learner.train_iter, n_episode=cfg.policy.collect.n_episode)\n            train_data = sum(new_data[0], [])\n            learner.train(train_data, collector.envstep)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info[0]]}\n            league.finish_job(job_finish_info)\n        if count % 50 == 0:\n            payoff_string = repr(league.payoff)\n            rank_string = league.player_rank(string=True)\n            tb_logger.add_text('payoff_step', payoff_string, main_collector.envstep)\n            tb_logger.add_text('rank_step', rank_string, main_collector.envstep)\n        count += 1",
            "def main(cfg, seed=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cfg = compile_config(cfg, SyncSubprocessEnvManager, PPOPolicy, BaseLearner, BattleEpisodeSerialCollector, InteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    collector_env_cfg = copy.deepcopy(cfg.env)\n    collector_env_cfg.agent_vs_agent = True\n    evaluator_env_cfg = copy.deepcopy(cfg.env)\n    evaluator_env_cfg.agent_vs_agent = False\n    evaluator_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, evaluator_env_cfg) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env.seed(seed, dynamic_seed=False)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = MyLeague(cfg.policy.other.league)\n    (policies, learners, collectors) = ({}, {}, {})\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, collector_env_cfg) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = BattleEpisodeSerialCollector(cfg.policy.collect.collector, collector_env, [policy.collect_mode, policy.collect_mode], tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator_cfg.stop_value = cfg.env.stop_value\n    evaluator = InteractionSerialEvaluator(evaluator_cfg, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name, instance_name='builtin_ai_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    count = 0\n    while True:\n        if evaluator.should_eval(main_learner.train_iter):\n            (stop_flag, eval_episode_info) = evaluator.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in eval_episode_info]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=100, sigma=1e-08), win_loss_result)\n            if stop_flag:\n                break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (new_data, episode_info) = collector.collect(train_iter=learner.train_iter, n_episode=cfg.policy.collect.n_episode)\n            train_data = sum(new_data[0], [])\n            learner.train(train_data, collector.envstep)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info[0]]}\n            league.finish_job(job_finish_info)\n        if count % 50 == 0:\n            payoff_string = repr(league.payoff)\n            rank_string = league.player_rank(string=True)\n            tb_logger.add_text('payoff_step', payoff_string, main_collector.envstep)\n            tb_logger.add_text('rank_step', rank_string, main_collector.envstep)\n        count += 1",
            "def main(cfg, seed=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cfg = compile_config(cfg, SyncSubprocessEnvManager, PPOPolicy, BaseLearner, BattleEpisodeSerialCollector, InteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    collector_env_cfg = copy.deepcopy(cfg.env)\n    collector_env_cfg.agent_vs_agent = True\n    evaluator_env_cfg = copy.deepcopy(cfg.env)\n    evaluator_env_cfg.agent_vs_agent = False\n    evaluator_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, evaluator_env_cfg) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env.seed(seed, dynamic_seed=False)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = MyLeague(cfg.policy.other.league)\n    (policies, learners, collectors) = ({}, {}, {})\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, collector_env_cfg) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = BattleEpisodeSerialCollector(cfg.policy.collect.collector, collector_env, [policy.collect_mode, policy.collect_mode], tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator_cfg.stop_value = cfg.env.stop_value\n    evaluator = InteractionSerialEvaluator(evaluator_cfg, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name, instance_name='builtin_ai_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    count = 0\n    while True:\n        if evaluator.should_eval(main_learner.train_iter):\n            (stop_flag, eval_episode_info) = evaluator.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in eval_episode_info]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=100, sigma=1e-08), win_loss_result)\n            if stop_flag:\n                break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (new_data, episode_info) = collector.collect(train_iter=learner.train_iter, n_episode=cfg.policy.collect.n_episode)\n            train_data = sum(new_data[0], [])\n            learner.train(train_data, collector.envstep)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info[0]]}\n            league.finish_job(job_finish_info)\n        if count % 50 == 0:\n            payoff_string = repr(league.payoff)\n            rank_string = league.player_rank(string=True)\n            tb_logger.add_text('payoff_step', payoff_string, main_collector.envstep)\n            tb_logger.add_text('rank_step', rank_string, main_collector.envstep)\n        count += 1",
            "def main(cfg, seed=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cfg = compile_config(cfg, SyncSubprocessEnvManager, PPOPolicy, BaseLearner, BattleEpisodeSerialCollector, InteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    collector_env_cfg = copy.deepcopy(cfg.env)\n    collector_env_cfg.agent_vs_agent = True\n    evaluator_env_cfg = copy.deepcopy(cfg.env)\n    evaluator_env_cfg.agent_vs_agent = False\n    evaluator_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, evaluator_env_cfg) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env.seed(seed, dynamic_seed=False)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = MyLeague(cfg.policy.other.league)\n    (policies, learners, collectors) = ({}, {}, {})\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, collector_env_cfg) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = BattleEpisodeSerialCollector(cfg.policy.collect.collector, collector_env, [policy.collect_mode, policy.collect_mode], tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator_cfg.stop_value = cfg.env.stop_value\n    evaluator = InteractionSerialEvaluator(evaluator_cfg, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name, instance_name='builtin_ai_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    count = 0\n    while True:\n        if evaluator.should_eval(main_learner.train_iter):\n            (stop_flag, eval_episode_info) = evaluator.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in eval_episode_info]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=100, sigma=1e-08), win_loss_result)\n            if stop_flag:\n                break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (new_data, episode_info) = collector.collect(train_iter=learner.train_iter, n_episode=cfg.policy.collect.n_episode)\n            train_data = sum(new_data[0], [])\n            learner.train(train_data, collector.envstep)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info[0]]}\n            league.finish_job(job_finish_info)\n        if count % 50 == 0:\n            payoff_string = repr(league.payoff)\n            rank_string = league.player_rank(string=True)\n            tb_logger.add_text('payoff_step', payoff_string, main_collector.envstep)\n            tb_logger.add_text('rank_step', rank_string, main_collector.envstep)\n        count += 1",
            "def main(cfg, seed=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cfg = compile_config(cfg, SyncSubprocessEnvManager, PPOPolicy, BaseLearner, BattleEpisodeSerialCollector, InteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    collector_env_cfg = copy.deepcopy(cfg.env)\n    collector_env_cfg.agent_vs_agent = True\n    evaluator_env_cfg = copy.deepcopy(cfg.env)\n    evaluator_env_cfg.agent_vs_agent = False\n    evaluator_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, evaluator_env_cfg) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env.seed(seed, dynamic_seed=False)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = MyLeague(cfg.policy.other.league)\n    (policies, learners, collectors) = ({}, {}, {})\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = SyncSubprocessEnvManager(env_fn=[partial(SlimeVolleyEnv, collector_env_cfg) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = BattleEpisodeSerialCollector(cfg.policy.collect.collector, collector_env, [policy.collect_mode, policy.collect_mode], tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator_cfg.stop_value = cfg.env.stop_value\n    evaluator = InteractionSerialEvaluator(evaluator_cfg, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name, instance_name='builtin_ai_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    count = 0\n    while True:\n        if evaluator.should_eval(main_learner.train_iter):\n            (stop_flag, eval_episode_info) = evaluator.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in eval_episode_info]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=100, sigma=1e-08), win_loss_result)\n            if stop_flag:\n                break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (new_data, episode_info) = collector.collect(train_iter=learner.train_iter, n_episode=cfg.policy.collect.n_episode)\n            train_data = sum(new_data[0], [])\n            learner.train(train_data, collector.envstep)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info[0]]}\n            league.finish_job(job_finish_info)\n        if count % 50 == 0:\n            payoff_string = repr(league.payoff)\n            rank_string = league.player_rank(string=True)\n            tb_logger.add_text('payoff_step', payoff_string, main_collector.envstep)\n            tb_logger.add_text('rank_step', rank_string, main_collector.envstep)\n        count += 1"
        ]
    }
]