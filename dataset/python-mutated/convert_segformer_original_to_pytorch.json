[
    {
        "func_name": "rename_keys",
        "original": "def rename_keys(state_dict, encoder_only=False):\n    new_state_dict = OrderedDict()\n    for (key, value) in state_dict.items():\n        if encoder_only and (not key.startswith('head')):\n            key = 'segformer.encoder.' + key\n        if key.startswith('backbone'):\n            key = key.replace('backbone', 'segformer.encoder')\n        if 'patch_embed' in key:\n            idx = key[key.find('patch_embed') + len('patch_embed')]\n            key = key.replace(f'patch_embed{idx}', f'patch_embeddings.{int(idx) - 1}')\n        if 'norm' in key:\n            key = key.replace('norm', 'layer_norm')\n        if 'segformer.encoder.layer_norm' in key:\n            idx = key[key.find('segformer.encoder.layer_norm') + len('segformer.encoder.layer_norm')]\n            key = key.replace(f'layer_norm{idx}', f'layer_norm.{int(idx) - 1}')\n        if 'layer_norm1' in key:\n            key = key.replace('layer_norm1', 'layer_norm_1')\n        if 'layer_norm2' in key:\n            key = key.replace('layer_norm2', 'layer_norm_2')\n        if 'block' in key:\n            idx = key[key.find('block') + len('block')]\n            key = key.replace(f'block{idx}', f'block.{int(idx) - 1}')\n        if 'attn.q' in key:\n            key = key.replace('attn.q', 'attention.self.query')\n        if 'attn.proj' in key:\n            key = key.replace('attn.proj', 'attention.output.dense')\n        if 'attn' in key:\n            key = key.replace('attn', 'attention.self')\n        if 'fc1' in key:\n            key = key.replace('fc1', 'dense1')\n        if 'fc2' in key:\n            key = key.replace('fc2', 'dense2')\n        if 'linear_pred' in key:\n            key = key.replace('linear_pred', 'classifier')\n        if 'linear_fuse' in key:\n            key = key.replace('linear_fuse.conv', 'linear_fuse')\n            key = key.replace('linear_fuse.bn', 'batch_norm')\n        if 'linear_c' in key:\n            idx = key[key.find('linear_c') + len('linear_c')]\n            key = key.replace(f'linear_c{idx}', f'linear_c.{int(idx) - 1}')\n        if key.startswith('head'):\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict",
        "mutated": [
            "def rename_keys(state_dict, encoder_only=False):\n    if False:\n        i = 10\n    new_state_dict = OrderedDict()\n    for (key, value) in state_dict.items():\n        if encoder_only and (not key.startswith('head')):\n            key = 'segformer.encoder.' + key\n        if key.startswith('backbone'):\n            key = key.replace('backbone', 'segformer.encoder')\n        if 'patch_embed' in key:\n            idx = key[key.find('patch_embed') + len('patch_embed')]\n            key = key.replace(f'patch_embed{idx}', f'patch_embeddings.{int(idx) - 1}')\n        if 'norm' in key:\n            key = key.replace('norm', 'layer_norm')\n        if 'segformer.encoder.layer_norm' in key:\n            idx = key[key.find('segformer.encoder.layer_norm') + len('segformer.encoder.layer_norm')]\n            key = key.replace(f'layer_norm{idx}', f'layer_norm.{int(idx) - 1}')\n        if 'layer_norm1' in key:\n            key = key.replace('layer_norm1', 'layer_norm_1')\n        if 'layer_norm2' in key:\n            key = key.replace('layer_norm2', 'layer_norm_2')\n        if 'block' in key:\n            idx = key[key.find('block') + len('block')]\n            key = key.replace(f'block{idx}', f'block.{int(idx) - 1}')\n        if 'attn.q' in key:\n            key = key.replace('attn.q', 'attention.self.query')\n        if 'attn.proj' in key:\n            key = key.replace('attn.proj', 'attention.output.dense')\n        if 'attn' in key:\n            key = key.replace('attn', 'attention.self')\n        if 'fc1' in key:\n            key = key.replace('fc1', 'dense1')\n        if 'fc2' in key:\n            key = key.replace('fc2', 'dense2')\n        if 'linear_pred' in key:\n            key = key.replace('linear_pred', 'classifier')\n        if 'linear_fuse' in key:\n            key = key.replace('linear_fuse.conv', 'linear_fuse')\n            key = key.replace('linear_fuse.bn', 'batch_norm')\n        if 'linear_c' in key:\n            idx = key[key.find('linear_c') + len('linear_c')]\n            key = key.replace(f'linear_c{idx}', f'linear_c.{int(idx) - 1}')\n        if key.startswith('head'):\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict",
            "def rename_keys(state_dict, encoder_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    new_state_dict = OrderedDict()\n    for (key, value) in state_dict.items():\n        if encoder_only and (not key.startswith('head')):\n            key = 'segformer.encoder.' + key\n        if key.startswith('backbone'):\n            key = key.replace('backbone', 'segformer.encoder')\n        if 'patch_embed' in key:\n            idx = key[key.find('patch_embed') + len('patch_embed')]\n            key = key.replace(f'patch_embed{idx}', f'patch_embeddings.{int(idx) - 1}')\n        if 'norm' in key:\n            key = key.replace('norm', 'layer_norm')\n        if 'segformer.encoder.layer_norm' in key:\n            idx = key[key.find('segformer.encoder.layer_norm') + len('segformer.encoder.layer_norm')]\n            key = key.replace(f'layer_norm{idx}', f'layer_norm.{int(idx) - 1}')\n        if 'layer_norm1' in key:\n            key = key.replace('layer_norm1', 'layer_norm_1')\n        if 'layer_norm2' in key:\n            key = key.replace('layer_norm2', 'layer_norm_2')\n        if 'block' in key:\n            idx = key[key.find('block') + len('block')]\n            key = key.replace(f'block{idx}', f'block.{int(idx) - 1}')\n        if 'attn.q' in key:\n            key = key.replace('attn.q', 'attention.self.query')\n        if 'attn.proj' in key:\n            key = key.replace('attn.proj', 'attention.output.dense')\n        if 'attn' in key:\n            key = key.replace('attn', 'attention.self')\n        if 'fc1' in key:\n            key = key.replace('fc1', 'dense1')\n        if 'fc2' in key:\n            key = key.replace('fc2', 'dense2')\n        if 'linear_pred' in key:\n            key = key.replace('linear_pred', 'classifier')\n        if 'linear_fuse' in key:\n            key = key.replace('linear_fuse.conv', 'linear_fuse')\n            key = key.replace('linear_fuse.bn', 'batch_norm')\n        if 'linear_c' in key:\n            idx = key[key.find('linear_c') + len('linear_c')]\n            key = key.replace(f'linear_c{idx}', f'linear_c.{int(idx) - 1}')\n        if key.startswith('head'):\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict",
            "def rename_keys(state_dict, encoder_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    new_state_dict = OrderedDict()\n    for (key, value) in state_dict.items():\n        if encoder_only and (not key.startswith('head')):\n            key = 'segformer.encoder.' + key\n        if key.startswith('backbone'):\n            key = key.replace('backbone', 'segformer.encoder')\n        if 'patch_embed' in key:\n            idx = key[key.find('patch_embed') + len('patch_embed')]\n            key = key.replace(f'patch_embed{idx}', f'patch_embeddings.{int(idx) - 1}')\n        if 'norm' in key:\n            key = key.replace('norm', 'layer_norm')\n        if 'segformer.encoder.layer_norm' in key:\n            idx = key[key.find('segformer.encoder.layer_norm') + len('segformer.encoder.layer_norm')]\n            key = key.replace(f'layer_norm{idx}', f'layer_norm.{int(idx) - 1}')\n        if 'layer_norm1' in key:\n            key = key.replace('layer_norm1', 'layer_norm_1')\n        if 'layer_norm2' in key:\n            key = key.replace('layer_norm2', 'layer_norm_2')\n        if 'block' in key:\n            idx = key[key.find('block') + len('block')]\n            key = key.replace(f'block{idx}', f'block.{int(idx) - 1}')\n        if 'attn.q' in key:\n            key = key.replace('attn.q', 'attention.self.query')\n        if 'attn.proj' in key:\n            key = key.replace('attn.proj', 'attention.output.dense')\n        if 'attn' in key:\n            key = key.replace('attn', 'attention.self')\n        if 'fc1' in key:\n            key = key.replace('fc1', 'dense1')\n        if 'fc2' in key:\n            key = key.replace('fc2', 'dense2')\n        if 'linear_pred' in key:\n            key = key.replace('linear_pred', 'classifier')\n        if 'linear_fuse' in key:\n            key = key.replace('linear_fuse.conv', 'linear_fuse')\n            key = key.replace('linear_fuse.bn', 'batch_norm')\n        if 'linear_c' in key:\n            idx = key[key.find('linear_c') + len('linear_c')]\n            key = key.replace(f'linear_c{idx}', f'linear_c.{int(idx) - 1}')\n        if key.startswith('head'):\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict",
            "def rename_keys(state_dict, encoder_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    new_state_dict = OrderedDict()\n    for (key, value) in state_dict.items():\n        if encoder_only and (not key.startswith('head')):\n            key = 'segformer.encoder.' + key\n        if key.startswith('backbone'):\n            key = key.replace('backbone', 'segformer.encoder')\n        if 'patch_embed' in key:\n            idx = key[key.find('patch_embed') + len('patch_embed')]\n            key = key.replace(f'patch_embed{idx}', f'patch_embeddings.{int(idx) - 1}')\n        if 'norm' in key:\n            key = key.replace('norm', 'layer_norm')\n        if 'segformer.encoder.layer_norm' in key:\n            idx = key[key.find('segformer.encoder.layer_norm') + len('segformer.encoder.layer_norm')]\n            key = key.replace(f'layer_norm{idx}', f'layer_norm.{int(idx) - 1}')\n        if 'layer_norm1' in key:\n            key = key.replace('layer_norm1', 'layer_norm_1')\n        if 'layer_norm2' in key:\n            key = key.replace('layer_norm2', 'layer_norm_2')\n        if 'block' in key:\n            idx = key[key.find('block') + len('block')]\n            key = key.replace(f'block{idx}', f'block.{int(idx) - 1}')\n        if 'attn.q' in key:\n            key = key.replace('attn.q', 'attention.self.query')\n        if 'attn.proj' in key:\n            key = key.replace('attn.proj', 'attention.output.dense')\n        if 'attn' in key:\n            key = key.replace('attn', 'attention.self')\n        if 'fc1' in key:\n            key = key.replace('fc1', 'dense1')\n        if 'fc2' in key:\n            key = key.replace('fc2', 'dense2')\n        if 'linear_pred' in key:\n            key = key.replace('linear_pred', 'classifier')\n        if 'linear_fuse' in key:\n            key = key.replace('linear_fuse.conv', 'linear_fuse')\n            key = key.replace('linear_fuse.bn', 'batch_norm')\n        if 'linear_c' in key:\n            idx = key[key.find('linear_c') + len('linear_c')]\n            key = key.replace(f'linear_c{idx}', f'linear_c.{int(idx) - 1}')\n        if key.startswith('head'):\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict",
            "def rename_keys(state_dict, encoder_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    new_state_dict = OrderedDict()\n    for (key, value) in state_dict.items():\n        if encoder_only and (not key.startswith('head')):\n            key = 'segformer.encoder.' + key\n        if key.startswith('backbone'):\n            key = key.replace('backbone', 'segformer.encoder')\n        if 'patch_embed' in key:\n            idx = key[key.find('patch_embed') + len('patch_embed')]\n            key = key.replace(f'patch_embed{idx}', f'patch_embeddings.{int(idx) - 1}')\n        if 'norm' in key:\n            key = key.replace('norm', 'layer_norm')\n        if 'segformer.encoder.layer_norm' in key:\n            idx = key[key.find('segformer.encoder.layer_norm') + len('segformer.encoder.layer_norm')]\n            key = key.replace(f'layer_norm{idx}', f'layer_norm.{int(idx) - 1}')\n        if 'layer_norm1' in key:\n            key = key.replace('layer_norm1', 'layer_norm_1')\n        if 'layer_norm2' in key:\n            key = key.replace('layer_norm2', 'layer_norm_2')\n        if 'block' in key:\n            idx = key[key.find('block') + len('block')]\n            key = key.replace(f'block{idx}', f'block.{int(idx) - 1}')\n        if 'attn.q' in key:\n            key = key.replace('attn.q', 'attention.self.query')\n        if 'attn.proj' in key:\n            key = key.replace('attn.proj', 'attention.output.dense')\n        if 'attn' in key:\n            key = key.replace('attn', 'attention.self')\n        if 'fc1' in key:\n            key = key.replace('fc1', 'dense1')\n        if 'fc2' in key:\n            key = key.replace('fc2', 'dense2')\n        if 'linear_pred' in key:\n            key = key.replace('linear_pred', 'classifier')\n        if 'linear_fuse' in key:\n            key = key.replace('linear_fuse.conv', 'linear_fuse')\n            key = key.replace('linear_fuse.bn', 'batch_norm')\n        if 'linear_c' in key:\n            idx = key[key.find('linear_c') + len('linear_c')]\n            key = key.replace(f'linear_c{idx}', f'linear_c.{int(idx) - 1}')\n        if key.startswith('head'):\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict"
        ]
    },
    {
        "func_name": "read_in_k_v",
        "original": "def read_in_k_v(state_dict, config):\n    for i in range(config.num_encoder_blocks):\n        for j in range(config.depths[i]):\n            kv_weight = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.weight')\n            kv_bias = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.bias')\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.weight'] = kv_weight[:config.hidden_sizes[i], :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.bias'] = kv_bias[:config.hidden_sizes[i]]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.weight'] = kv_weight[config.hidden_sizes[i]:, :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.bias'] = kv_bias[config.hidden_sizes[i]:]",
        "mutated": [
            "def read_in_k_v(state_dict, config):\n    if False:\n        i = 10\n    for i in range(config.num_encoder_blocks):\n        for j in range(config.depths[i]):\n            kv_weight = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.weight')\n            kv_bias = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.bias')\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.weight'] = kv_weight[:config.hidden_sizes[i], :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.bias'] = kv_bias[:config.hidden_sizes[i]]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.weight'] = kv_weight[config.hidden_sizes[i]:, :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.bias'] = kv_bias[config.hidden_sizes[i]:]",
            "def read_in_k_v(state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for i in range(config.num_encoder_blocks):\n        for j in range(config.depths[i]):\n            kv_weight = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.weight')\n            kv_bias = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.bias')\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.weight'] = kv_weight[:config.hidden_sizes[i], :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.bias'] = kv_bias[:config.hidden_sizes[i]]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.weight'] = kv_weight[config.hidden_sizes[i]:, :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.bias'] = kv_bias[config.hidden_sizes[i]:]",
            "def read_in_k_v(state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for i in range(config.num_encoder_blocks):\n        for j in range(config.depths[i]):\n            kv_weight = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.weight')\n            kv_bias = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.bias')\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.weight'] = kv_weight[:config.hidden_sizes[i], :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.bias'] = kv_bias[:config.hidden_sizes[i]]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.weight'] = kv_weight[config.hidden_sizes[i]:, :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.bias'] = kv_bias[config.hidden_sizes[i]:]",
            "def read_in_k_v(state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for i in range(config.num_encoder_blocks):\n        for j in range(config.depths[i]):\n            kv_weight = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.weight')\n            kv_bias = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.bias')\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.weight'] = kv_weight[:config.hidden_sizes[i], :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.bias'] = kv_bias[:config.hidden_sizes[i]]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.weight'] = kv_weight[config.hidden_sizes[i]:, :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.bias'] = kv_bias[config.hidden_sizes[i]:]",
            "def read_in_k_v(state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for i in range(config.num_encoder_blocks):\n        for j in range(config.depths[i]):\n            kv_weight = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.weight')\n            kv_bias = state_dict.pop(f'segformer.encoder.block.{i}.{j}.attention.self.kv.bias')\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.weight'] = kv_weight[:config.hidden_sizes[i], :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.key.bias'] = kv_bias[:config.hidden_sizes[i]]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.weight'] = kv_weight[config.hidden_sizes[i]:, :]\n            state_dict[f'segformer.encoder.block.{i}.{j}.attention.self.value.bias'] = kv_bias[config.hidden_sizes[i]:]"
        ]
    },
    {
        "func_name": "prepare_img",
        "original": "def prepare_img():\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image",
        "mutated": [
            "def prepare_img():\n    if False:\n        i = 10\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image"
        ]
    },
    {
        "func_name": "convert_segformer_checkpoint",
        "original": "@torch.no_grad()\ndef convert_segformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    \"\"\"\n    Copy/paste/tweak model's weights to our SegFormer structure.\n    \"\"\"\n    config = SegformerConfig()\n    encoder_only = False\n    repo_id = 'huggingface/label-files'\n    if 'segformer' in model_name:\n        size = model_name[len('segformer.'):len('segformer.') + 2]\n        if 'ade' in model_name:\n            config.num_labels = 150\n            filename = 'ade20k-id2label.json'\n            expected_shape = (1, 150, 128, 128)\n        elif 'city' in model_name:\n            config.num_labels = 19\n            filename = 'cityscapes-id2label.json'\n            expected_shape = (1, 19, 128, 128)\n        else:\n            raise ValueError(f'Model {model_name} not supported')\n    elif 'mit' in model_name:\n        encoder_only = True\n        size = model_name[4:6]\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        expected_shape = (1, 1000)\n    else:\n        raise ValueError(f'Model {model_name} not supported')\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 'b0':\n        pass\n    elif size == 'b1':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 256\n    elif size == 'b2':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 6, 3]\n    elif size == 'b3':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 18, 3]\n    elif size == 'b4':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 8, 27, 3]\n    elif size == 'b5':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 6, 40, 3]\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = SegformerImageProcessor(image_scale=(512, 512), keep_ratio=False, align=False, do_random_crop=False)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    if encoder_only:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    else:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))['state_dict']\n    state_dict = rename_keys(state_dict, encoder_only=encoder_only)\n    if not encoder_only:\n        del state_dict['decode_head.conv_seg.weight']\n        del state_dict['decode_head.conv_seg.bias']\n    read_in_k_v(state_dict, config)\n    if encoder_only:\n        config.reshape_last_stage = False\n        model = SegformerForImageClassification(config)\n    else:\n        model = SegformerForSemanticSegmentation(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if model_name == 'segformer.b0.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-4.631, -5.5232, -6.2356], [-5.1921, -6.1444, -6.5996], [-5.4424, -6.279, -6.7574]], [[-12.1391, -13.3122, -13.9554], [-12.8732, -13.9352, -14.3563], [-12.9438, -13.8226, -14.2513]], [[-12.5134, -13.4686, -14.4915], [-12.8669, -14.4343, -14.7758], [-13.2523, -14.5819, -15.0694]]])\n    elif model_name == 'segformer.b1.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-7.582, -8.7231, -8.3215], [-8.06, -10.3529, -10.0304], [-7.5208, -9.4103, -9.6239]], [[-12.6918, -13.8994, -13.7137], [-13.3196, -15.7523, -15.4789], [-12.9343, -14.8757, -14.9689]], [[-11.1911, -11.9421, -11.3243], [-11.3342, -13.6839, -13.3581], [-10.3909, -12.1832, -12.4858]]])\n    elif model_name == 'segformer.b2.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-11.8173, -14.385, -16.3128], [-14.5648, -16.5804, -18.6568], [-14.7223, -15.7387, -18.4218]], [[-15.729, -17.9171, -19.4423], [-18.3105, -19.9448, -21.4661], [-17.9296, -18.6497, -20.791]], [[-15.0783, -17.0336, -18.2789], [-16.8771, -18.687, -20.1612], [-16.2454, -17.1426, -19.5055]]])\n    elif model_name == 'segformer.b3.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-9.0878, -10.2081, -10.1891], [-9.3144, -10.7941, -10.9843], [-9.2294, -10.3855, -10.5704]], [[-12.2316, -13.9068, -13.6102], [-12.9161, -14.3702, -14.3235], [-12.5233, -13.7174, -13.7932]], [[-14.6275, -15.249, -14.9727], [-14.34, -15.9687, -16.2827], [-14.1484, -15.4033, -15.8937]]])\n    elif model_name == 'segformer.b4.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-12.3144, -13.2447, -14.0802], [-13.3614, -14.5816, -15.6117], [-13.334, -14.4433, -16.2219]], [[-19.2781, -20.4128, -20.7506], [-20.6153, -21.6566, -22.0998], [-19.98, -21.043, -22.1494]], [[-18.8739, -19.7804, -21.1834], [-20.1233, -21.6765, -23.2944], [-20.0315, -21.2641, -23.6944]]])\n    elif model_name == 'segformer.b5.640x640.ade.160k':\n        expected_slice = torch.tensor([[[-9.5524, -12.0835, -11.7348], [-10.5229, -13.6446, -14.5662], [-9.5842, -12.8851, -13.9414]], [[-15.3432, -17.5323, -17.0818], [-16.333, -18.9255, -19.2101], [-15.134, -17.7848, -18.3971]], [[-12.6072, -14.9486, -14.6631], [-13.7629, -17.0907, -17.7745], [-12.7899, -16.1695, -17.1671]]])\n    elif model_name == 'segformer.b0.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.9295, -13.4057, -14.8106], [-13.3431, -14.8179, -15.3781], [-14.2836, -15.5942, -16.1588]], [[-11.4906, -12.8067, -13.6564], [-13.1189, -14.05, -14.1543], [-13.8748, -14.5136, -14.8789]], [[0.5374, 0.1067, -0.4742], [0.1141, -0.2255, -0.7099], [-0.3, -0.5924, -1.3105]]])\n    elif model_name == 'segformer.b0.512x1024.city.160k':\n        expected_slice = torch.tensor([[[-7.8217, -9.8767, -10.1717], [-9.4438, -10.9058, -11.4047], [-9.7939, -12.3495, -12.1079]], [[-7.1514, -9.5336, -10.086], [-9.7776, -11.6822, -11.8439], [-10.1411, -12.7655, -12.8972]], [[0.3021, 0.0805, -0.231], [-0.0328, -0.1605, -0.2714], [-0.1408, -0.5477, -0.6976]]])\n    elif model_name == 'segformer.b0.640x1280.city.160k':\n        expected_slice = torch.tensor([[[-11.372, -12.787, -13.477], [-12.536, -14.194, -14.409], [-13.217, -14.888, -15.327]], [[-14.791, -17.122, -18.277], [-17.163, -19.192, -19.533], [-17.897, -19.991, -20.315]], [[0.76723, 0.41921, -0.077878], [0.47772, 0.0095557, -0.28082], [0.36032, -0.24826, -0.51168]]])\n    elif model_name == 'segformer.b0.768x768.city.160k':\n        expected_slice = torch.tensor([[[-9.4959, -11.3087, -11.7479], [-11.0025, -12.654, -12.3319], [-11.4064, -13.0487, -12.9905]], [[-9.8905, -11.3084, -12.0854], [-11.1726, -12.7698, -12.9583], [-11.5985, -13.3278, -14.1774]], [[0.2213, 0.0192, -0.2466], [-0.1731, -0.4213, -0.4874], [-0.3126, -0.6541, -1.1389]]])\n    elif model_name == 'segformer.b1.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-13.5748, -13.9111, -12.65], [-14.35, -15.3683, -14.2328], [-14.7532, -16.0424, -15.6087]], [[-17.1651, -15.8725, -12.9653], [-17.258, -17.3718, -14.8223], [-16.6058, -16.8783, -16.7452]], [[-3.6456, -3.0209, -1.4203], [-3.0797, -3.1959, -2.0], [-1.8757, -1.9217, -1.6997]]])\n    elif model_name == 'segformer.b2.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-16.0976, -16.4856, -17.3962], [-16.6234, -19.0342, -19.7685], [-16.09, -18.0661, -19.118]], [[-18.475, -18.8488, -19.5074], [-19.403, -22.157, -22.5977], [-19.1191, -20.8486, -22.3783]], [[-4.5178, -5.5037, -6.5109], [-5.0884, -7.2174, -8.0334], [-4.4156, -5.8117, -7.297]]])\n    elif model_name == 'segformer.b3.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-14.2081, -14.4732, -14.1977], [-14.5867, -16.4423, -16.6356], [-13.4441, -14.9685, -16.8696]], [[-14.4576, -14.7073, -15.0451], [-15.0816, -17.6237, -17.9873], [-14.4213, -16.0199, -18.5992]], [[-4.7349, -4.9588, -5.0966], [-4.321, -6.9325, -7.2591], [-3.4312, -4.7484, -7.1917]]])\n    elif model_name == 'segformer.b4.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.7737, -11.9526, -11.3273], [-13.6692, -14.4574, -13.8878], [-13.8937, -14.6924, -15.9345]], [[-14.6706, -14.533, -14.1306], [-16.1502, -16.818, -16.4269], [-16.8338, -17.8939, -20.1746]], [[1.0491, 0.8289, 1.031], [1.1044, 0.5219, 0.8055], [1.0899, 0.6926, 0.559]]])\n    elif model_name == 'segformer.b5.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-12.5641, -13.4777, -13.0684], [-13.9587, -15.8983, -16.6557], [-13.3109, -15.735, -16.3141]], [[-14.7074, -15.4352, -14.5944], [-16.6353, -18.1663, -18.612], [-15.1702, -18.0329, -18.1547]], [[-1.799, -2.0951, -1.7784], [-2.6397, -3.8245, -3.9686], [-1.5264, -2.8126, -2.9316]]])\n    else:\n        predicted_class_idx = logits.argmax(-1).item()\n        print('Predicted class:', model.config.id2label[predicted_class_idx])\n    if not encoder_only:\n        assert logits.shape == expected_shape\n        assert torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    image_processor.save_pretrained(pytorch_dump_folder_path)",
        "mutated": [
            "@torch.no_grad()\ndef convert_segformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n    \"\\n    Copy/paste/tweak model's weights to our SegFormer structure.\\n    \"\n    config = SegformerConfig()\n    encoder_only = False\n    repo_id = 'huggingface/label-files'\n    if 'segformer' in model_name:\n        size = model_name[len('segformer.'):len('segformer.') + 2]\n        if 'ade' in model_name:\n            config.num_labels = 150\n            filename = 'ade20k-id2label.json'\n            expected_shape = (1, 150, 128, 128)\n        elif 'city' in model_name:\n            config.num_labels = 19\n            filename = 'cityscapes-id2label.json'\n            expected_shape = (1, 19, 128, 128)\n        else:\n            raise ValueError(f'Model {model_name} not supported')\n    elif 'mit' in model_name:\n        encoder_only = True\n        size = model_name[4:6]\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        expected_shape = (1, 1000)\n    else:\n        raise ValueError(f'Model {model_name} not supported')\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 'b0':\n        pass\n    elif size == 'b1':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 256\n    elif size == 'b2':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 6, 3]\n    elif size == 'b3':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 18, 3]\n    elif size == 'b4':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 8, 27, 3]\n    elif size == 'b5':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 6, 40, 3]\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = SegformerImageProcessor(image_scale=(512, 512), keep_ratio=False, align=False, do_random_crop=False)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    if encoder_only:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    else:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))['state_dict']\n    state_dict = rename_keys(state_dict, encoder_only=encoder_only)\n    if not encoder_only:\n        del state_dict['decode_head.conv_seg.weight']\n        del state_dict['decode_head.conv_seg.bias']\n    read_in_k_v(state_dict, config)\n    if encoder_only:\n        config.reshape_last_stage = False\n        model = SegformerForImageClassification(config)\n    else:\n        model = SegformerForSemanticSegmentation(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if model_name == 'segformer.b0.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-4.631, -5.5232, -6.2356], [-5.1921, -6.1444, -6.5996], [-5.4424, -6.279, -6.7574]], [[-12.1391, -13.3122, -13.9554], [-12.8732, -13.9352, -14.3563], [-12.9438, -13.8226, -14.2513]], [[-12.5134, -13.4686, -14.4915], [-12.8669, -14.4343, -14.7758], [-13.2523, -14.5819, -15.0694]]])\n    elif model_name == 'segformer.b1.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-7.582, -8.7231, -8.3215], [-8.06, -10.3529, -10.0304], [-7.5208, -9.4103, -9.6239]], [[-12.6918, -13.8994, -13.7137], [-13.3196, -15.7523, -15.4789], [-12.9343, -14.8757, -14.9689]], [[-11.1911, -11.9421, -11.3243], [-11.3342, -13.6839, -13.3581], [-10.3909, -12.1832, -12.4858]]])\n    elif model_name == 'segformer.b2.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-11.8173, -14.385, -16.3128], [-14.5648, -16.5804, -18.6568], [-14.7223, -15.7387, -18.4218]], [[-15.729, -17.9171, -19.4423], [-18.3105, -19.9448, -21.4661], [-17.9296, -18.6497, -20.791]], [[-15.0783, -17.0336, -18.2789], [-16.8771, -18.687, -20.1612], [-16.2454, -17.1426, -19.5055]]])\n    elif model_name == 'segformer.b3.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-9.0878, -10.2081, -10.1891], [-9.3144, -10.7941, -10.9843], [-9.2294, -10.3855, -10.5704]], [[-12.2316, -13.9068, -13.6102], [-12.9161, -14.3702, -14.3235], [-12.5233, -13.7174, -13.7932]], [[-14.6275, -15.249, -14.9727], [-14.34, -15.9687, -16.2827], [-14.1484, -15.4033, -15.8937]]])\n    elif model_name == 'segformer.b4.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-12.3144, -13.2447, -14.0802], [-13.3614, -14.5816, -15.6117], [-13.334, -14.4433, -16.2219]], [[-19.2781, -20.4128, -20.7506], [-20.6153, -21.6566, -22.0998], [-19.98, -21.043, -22.1494]], [[-18.8739, -19.7804, -21.1834], [-20.1233, -21.6765, -23.2944], [-20.0315, -21.2641, -23.6944]]])\n    elif model_name == 'segformer.b5.640x640.ade.160k':\n        expected_slice = torch.tensor([[[-9.5524, -12.0835, -11.7348], [-10.5229, -13.6446, -14.5662], [-9.5842, -12.8851, -13.9414]], [[-15.3432, -17.5323, -17.0818], [-16.333, -18.9255, -19.2101], [-15.134, -17.7848, -18.3971]], [[-12.6072, -14.9486, -14.6631], [-13.7629, -17.0907, -17.7745], [-12.7899, -16.1695, -17.1671]]])\n    elif model_name == 'segformer.b0.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.9295, -13.4057, -14.8106], [-13.3431, -14.8179, -15.3781], [-14.2836, -15.5942, -16.1588]], [[-11.4906, -12.8067, -13.6564], [-13.1189, -14.05, -14.1543], [-13.8748, -14.5136, -14.8789]], [[0.5374, 0.1067, -0.4742], [0.1141, -0.2255, -0.7099], [-0.3, -0.5924, -1.3105]]])\n    elif model_name == 'segformer.b0.512x1024.city.160k':\n        expected_slice = torch.tensor([[[-7.8217, -9.8767, -10.1717], [-9.4438, -10.9058, -11.4047], [-9.7939, -12.3495, -12.1079]], [[-7.1514, -9.5336, -10.086], [-9.7776, -11.6822, -11.8439], [-10.1411, -12.7655, -12.8972]], [[0.3021, 0.0805, -0.231], [-0.0328, -0.1605, -0.2714], [-0.1408, -0.5477, -0.6976]]])\n    elif model_name == 'segformer.b0.640x1280.city.160k':\n        expected_slice = torch.tensor([[[-11.372, -12.787, -13.477], [-12.536, -14.194, -14.409], [-13.217, -14.888, -15.327]], [[-14.791, -17.122, -18.277], [-17.163, -19.192, -19.533], [-17.897, -19.991, -20.315]], [[0.76723, 0.41921, -0.077878], [0.47772, 0.0095557, -0.28082], [0.36032, -0.24826, -0.51168]]])\n    elif model_name == 'segformer.b0.768x768.city.160k':\n        expected_slice = torch.tensor([[[-9.4959, -11.3087, -11.7479], [-11.0025, -12.654, -12.3319], [-11.4064, -13.0487, -12.9905]], [[-9.8905, -11.3084, -12.0854], [-11.1726, -12.7698, -12.9583], [-11.5985, -13.3278, -14.1774]], [[0.2213, 0.0192, -0.2466], [-0.1731, -0.4213, -0.4874], [-0.3126, -0.6541, -1.1389]]])\n    elif model_name == 'segformer.b1.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-13.5748, -13.9111, -12.65], [-14.35, -15.3683, -14.2328], [-14.7532, -16.0424, -15.6087]], [[-17.1651, -15.8725, -12.9653], [-17.258, -17.3718, -14.8223], [-16.6058, -16.8783, -16.7452]], [[-3.6456, -3.0209, -1.4203], [-3.0797, -3.1959, -2.0], [-1.8757, -1.9217, -1.6997]]])\n    elif model_name == 'segformer.b2.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-16.0976, -16.4856, -17.3962], [-16.6234, -19.0342, -19.7685], [-16.09, -18.0661, -19.118]], [[-18.475, -18.8488, -19.5074], [-19.403, -22.157, -22.5977], [-19.1191, -20.8486, -22.3783]], [[-4.5178, -5.5037, -6.5109], [-5.0884, -7.2174, -8.0334], [-4.4156, -5.8117, -7.297]]])\n    elif model_name == 'segformer.b3.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-14.2081, -14.4732, -14.1977], [-14.5867, -16.4423, -16.6356], [-13.4441, -14.9685, -16.8696]], [[-14.4576, -14.7073, -15.0451], [-15.0816, -17.6237, -17.9873], [-14.4213, -16.0199, -18.5992]], [[-4.7349, -4.9588, -5.0966], [-4.321, -6.9325, -7.2591], [-3.4312, -4.7484, -7.1917]]])\n    elif model_name == 'segformer.b4.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.7737, -11.9526, -11.3273], [-13.6692, -14.4574, -13.8878], [-13.8937, -14.6924, -15.9345]], [[-14.6706, -14.533, -14.1306], [-16.1502, -16.818, -16.4269], [-16.8338, -17.8939, -20.1746]], [[1.0491, 0.8289, 1.031], [1.1044, 0.5219, 0.8055], [1.0899, 0.6926, 0.559]]])\n    elif model_name == 'segformer.b5.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-12.5641, -13.4777, -13.0684], [-13.9587, -15.8983, -16.6557], [-13.3109, -15.735, -16.3141]], [[-14.7074, -15.4352, -14.5944], [-16.6353, -18.1663, -18.612], [-15.1702, -18.0329, -18.1547]], [[-1.799, -2.0951, -1.7784], [-2.6397, -3.8245, -3.9686], [-1.5264, -2.8126, -2.9316]]])\n    else:\n        predicted_class_idx = logits.argmax(-1).item()\n        print('Predicted class:', model.config.id2label[predicted_class_idx])\n    if not encoder_only:\n        assert logits.shape == expected_shape\n        assert torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    image_processor.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_segformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Copy/paste/tweak model's weights to our SegFormer structure.\\n    \"\n    config = SegformerConfig()\n    encoder_only = False\n    repo_id = 'huggingface/label-files'\n    if 'segformer' in model_name:\n        size = model_name[len('segformer.'):len('segformer.') + 2]\n        if 'ade' in model_name:\n            config.num_labels = 150\n            filename = 'ade20k-id2label.json'\n            expected_shape = (1, 150, 128, 128)\n        elif 'city' in model_name:\n            config.num_labels = 19\n            filename = 'cityscapes-id2label.json'\n            expected_shape = (1, 19, 128, 128)\n        else:\n            raise ValueError(f'Model {model_name} not supported')\n    elif 'mit' in model_name:\n        encoder_only = True\n        size = model_name[4:6]\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        expected_shape = (1, 1000)\n    else:\n        raise ValueError(f'Model {model_name} not supported')\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 'b0':\n        pass\n    elif size == 'b1':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 256\n    elif size == 'b2':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 6, 3]\n    elif size == 'b3':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 18, 3]\n    elif size == 'b4':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 8, 27, 3]\n    elif size == 'b5':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 6, 40, 3]\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = SegformerImageProcessor(image_scale=(512, 512), keep_ratio=False, align=False, do_random_crop=False)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    if encoder_only:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    else:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))['state_dict']\n    state_dict = rename_keys(state_dict, encoder_only=encoder_only)\n    if not encoder_only:\n        del state_dict['decode_head.conv_seg.weight']\n        del state_dict['decode_head.conv_seg.bias']\n    read_in_k_v(state_dict, config)\n    if encoder_only:\n        config.reshape_last_stage = False\n        model = SegformerForImageClassification(config)\n    else:\n        model = SegformerForSemanticSegmentation(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if model_name == 'segformer.b0.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-4.631, -5.5232, -6.2356], [-5.1921, -6.1444, -6.5996], [-5.4424, -6.279, -6.7574]], [[-12.1391, -13.3122, -13.9554], [-12.8732, -13.9352, -14.3563], [-12.9438, -13.8226, -14.2513]], [[-12.5134, -13.4686, -14.4915], [-12.8669, -14.4343, -14.7758], [-13.2523, -14.5819, -15.0694]]])\n    elif model_name == 'segformer.b1.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-7.582, -8.7231, -8.3215], [-8.06, -10.3529, -10.0304], [-7.5208, -9.4103, -9.6239]], [[-12.6918, -13.8994, -13.7137], [-13.3196, -15.7523, -15.4789], [-12.9343, -14.8757, -14.9689]], [[-11.1911, -11.9421, -11.3243], [-11.3342, -13.6839, -13.3581], [-10.3909, -12.1832, -12.4858]]])\n    elif model_name == 'segformer.b2.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-11.8173, -14.385, -16.3128], [-14.5648, -16.5804, -18.6568], [-14.7223, -15.7387, -18.4218]], [[-15.729, -17.9171, -19.4423], [-18.3105, -19.9448, -21.4661], [-17.9296, -18.6497, -20.791]], [[-15.0783, -17.0336, -18.2789], [-16.8771, -18.687, -20.1612], [-16.2454, -17.1426, -19.5055]]])\n    elif model_name == 'segformer.b3.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-9.0878, -10.2081, -10.1891], [-9.3144, -10.7941, -10.9843], [-9.2294, -10.3855, -10.5704]], [[-12.2316, -13.9068, -13.6102], [-12.9161, -14.3702, -14.3235], [-12.5233, -13.7174, -13.7932]], [[-14.6275, -15.249, -14.9727], [-14.34, -15.9687, -16.2827], [-14.1484, -15.4033, -15.8937]]])\n    elif model_name == 'segformer.b4.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-12.3144, -13.2447, -14.0802], [-13.3614, -14.5816, -15.6117], [-13.334, -14.4433, -16.2219]], [[-19.2781, -20.4128, -20.7506], [-20.6153, -21.6566, -22.0998], [-19.98, -21.043, -22.1494]], [[-18.8739, -19.7804, -21.1834], [-20.1233, -21.6765, -23.2944], [-20.0315, -21.2641, -23.6944]]])\n    elif model_name == 'segformer.b5.640x640.ade.160k':\n        expected_slice = torch.tensor([[[-9.5524, -12.0835, -11.7348], [-10.5229, -13.6446, -14.5662], [-9.5842, -12.8851, -13.9414]], [[-15.3432, -17.5323, -17.0818], [-16.333, -18.9255, -19.2101], [-15.134, -17.7848, -18.3971]], [[-12.6072, -14.9486, -14.6631], [-13.7629, -17.0907, -17.7745], [-12.7899, -16.1695, -17.1671]]])\n    elif model_name == 'segformer.b0.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.9295, -13.4057, -14.8106], [-13.3431, -14.8179, -15.3781], [-14.2836, -15.5942, -16.1588]], [[-11.4906, -12.8067, -13.6564], [-13.1189, -14.05, -14.1543], [-13.8748, -14.5136, -14.8789]], [[0.5374, 0.1067, -0.4742], [0.1141, -0.2255, -0.7099], [-0.3, -0.5924, -1.3105]]])\n    elif model_name == 'segformer.b0.512x1024.city.160k':\n        expected_slice = torch.tensor([[[-7.8217, -9.8767, -10.1717], [-9.4438, -10.9058, -11.4047], [-9.7939, -12.3495, -12.1079]], [[-7.1514, -9.5336, -10.086], [-9.7776, -11.6822, -11.8439], [-10.1411, -12.7655, -12.8972]], [[0.3021, 0.0805, -0.231], [-0.0328, -0.1605, -0.2714], [-0.1408, -0.5477, -0.6976]]])\n    elif model_name == 'segformer.b0.640x1280.city.160k':\n        expected_slice = torch.tensor([[[-11.372, -12.787, -13.477], [-12.536, -14.194, -14.409], [-13.217, -14.888, -15.327]], [[-14.791, -17.122, -18.277], [-17.163, -19.192, -19.533], [-17.897, -19.991, -20.315]], [[0.76723, 0.41921, -0.077878], [0.47772, 0.0095557, -0.28082], [0.36032, -0.24826, -0.51168]]])\n    elif model_name == 'segformer.b0.768x768.city.160k':\n        expected_slice = torch.tensor([[[-9.4959, -11.3087, -11.7479], [-11.0025, -12.654, -12.3319], [-11.4064, -13.0487, -12.9905]], [[-9.8905, -11.3084, -12.0854], [-11.1726, -12.7698, -12.9583], [-11.5985, -13.3278, -14.1774]], [[0.2213, 0.0192, -0.2466], [-0.1731, -0.4213, -0.4874], [-0.3126, -0.6541, -1.1389]]])\n    elif model_name == 'segformer.b1.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-13.5748, -13.9111, -12.65], [-14.35, -15.3683, -14.2328], [-14.7532, -16.0424, -15.6087]], [[-17.1651, -15.8725, -12.9653], [-17.258, -17.3718, -14.8223], [-16.6058, -16.8783, -16.7452]], [[-3.6456, -3.0209, -1.4203], [-3.0797, -3.1959, -2.0], [-1.8757, -1.9217, -1.6997]]])\n    elif model_name == 'segformer.b2.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-16.0976, -16.4856, -17.3962], [-16.6234, -19.0342, -19.7685], [-16.09, -18.0661, -19.118]], [[-18.475, -18.8488, -19.5074], [-19.403, -22.157, -22.5977], [-19.1191, -20.8486, -22.3783]], [[-4.5178, -5.5037, -6.5109], [-5.0884, -7.2174, -8.0334], [-4.4156, -5.8117, -7.297]]])\n    elif model_name == 'segformer.b3.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-14.2081, -14.4732, -14.1977], [-14.5867, -16.4423, -16.6356], [-13.4441, -14.9685, -16.8696]], [[-14.4576, -14.7073, -15.0451], [-15.0816, -17.6237, -17.9873], [-14.4213, -16.0199, -18.5992]], [[-4.7349, -4.9588, -5.0966], [-4.321, -6.9325, -7.2591], [-3.4312, -4.7484, -7.1917]]])\n    elif model_name == 'segformer.b4.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.7737, -11.9526, -11.3273], [-13.6692, -14.4574, -13.8878], [-13.8937, -14.6924, -15.9345]], [[-14.6706, -14.533, -14.1306], [-16.1502, -16.818, -16.4269], [-16.8338, -17.8939, -20.1746]], [[1.0491, 0.8289, 1.031], [1.1044, 0.5219, 0.8055], [1.0899, 0.6926, 0.559]]])\n    elif model_name == 'segformer.b5.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-12.5641, -13.4777, -13.0684], [-13.9587, -15.8983, -16.6557], [-13.3109, -15.735, -16.3141]], [[-14.7074, -15.4352, -14.5944], [-16.6353, -18.1663, -18.612], [-15.1702, -18.0329, -18.1547]], [[-1.799, -2.0951, -1.7784], [-2.6397, -3.8245, -3.9686], [-1.5264, -2.8126, -2.9316]]])\n    else:\n        predicted_class_idx = logits.argmax(-1).item()\n        print('Predicted class:', model.config.id2label[predicted_class_idx])\n    if not encoder_only:\n        assert logits.shape == expected_shape\n        assert torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    image_processor.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_segformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Copy/paste/tweak model's weights to our SegFormer structure.\\n    \"\n    config = SegformerConfig()\n    encoder_only = False\n    repo_id = 'huggingface/label-files'\n    if 'segformer' in model_name:\n        size = model_name[len('segformer.'):len('segformer.') + 2]\n        if 'ade' in model_name:\n            config.num_labels = 150\n            filename = 'ade20k-id2label.json'\n            expected_shape = (1, 150, 128, 128)\n        elif 'city' in model_name:\n            config.num_labels = 19\n            filename = 'cityscapes-id2label.json'\n            expected_shape = (1, 19, 128, 128)\n        else:\n            raise ValueError(f'Model {model_name} not supported')\n    elif 'mit' in model_name:\n        encoder_only = True\n        size = model_name[4:6]\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        expected_shape = (1, 1000)\n    else:\n        raise ValueError(f'Model {model_name} not supported')\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 'b0':\n        pass\n    elif size == 'b1':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 256\n    elif size == 'b2':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 6, 3]\n    elif size == 'b3':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 18, 3]\n    elif size == 'b4':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 8, 27, 3]\n    elif size == 'b5':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 6, 40, 3]\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = SegformerImageProcessor(image_scale=(512, 512), keep_ratio=False, align=False, do_random_crop=False)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    if encoder_only:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    else:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))['state_dict']\n    state_dict = rename_keys(state_dict, encoder_only=encoder_only)\n    if not encoder_only:\n        del state_dict['decode_head.conv_seg.weight']\n        del state_dict['decode_head.conv_seg.bias']\n    read_in_k_v(state_dict, config)\n    if encoder_only:\n        config.reshape_last_stage = False\n        model = SegformerForImageClassification(config)\n    else:\n        model = SegformerForSemanticSegmentation(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if model_name == 'segformer.b0.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-4.631, -5.5232, -6.2356], [-5.1921, -6.1444, -6.5996], [-5.4424, -6.279, -6.7574]], [[-12.1391, -13.3122, -13.9554], [-12.8732, -13.9352, -14.3563], [-12.9438, -13.8226, -14.2513]], [[-12.5134, -13.4686, -14.4915], [-12.8669, -14.4343, -14.7758], [-13.2523, -14.5819, -15.0694]]])\n    elif model_name == 'segformer.b1.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-7.582, -8.7231, -8.3215], [-8.06, -10.3529, -10.0304], [-7.5208, -9.4103, -9.6239]], [[-12.6918, -13.8994, -13.7137], [-13.3196, -15.7523, -15.4789], [-12.9343, -14.8757, -14.9689]], [[-11.1911, -11.9421, -11.3243], [-11.3342, -13.6839, -13.3581], [-10.3909, -12.1832, -12.4858]]])\n    elif model_name == 'segformer.b2.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-11.8173, -14.385, -16.3128], [-14.5648, -16.5804, -18.6568], [-14.7223, -15.7387, -18.4218]], [[-15.729, -17.9171, -19.4423], [-18.3105, -19.9448, -21.4661], [-17.9296, -18.6497, -20.791]], [[-15.0783, -17.0336, -18.2789], [-16.8771, -18.687, -20.1612], [-16.2454, -17.1426, -19.5055]]])\n    elif model_name == 'segformer.b3.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-9.0878, -10.2081, -10.1891], [-9.3144, -10.7941, -10.9843], [-9.2294, -10.3855, -10.5704]], [[-12.2316, -13.9068, -13.6102], [-12.9161, -14.3702, -14.3235], [-12.5233, -13.7174, -13.7932]], [[-14.6275, -15.249, -14.9727], [-14.34, -15.9687, -16.2827], [-14.1484, -15.4033, -15.8937]]])\n    elif model_name == 'segformer.b4.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-12.3144, -13.2447, -14.0802], [-13.3614, -14.5816, -15.6117], [-13.334, -14.4433, -16.2219]], [[-19.2781, -20.4128, -20.7506], [-20.6153, -21.6566, -22.0998], [-19.98, -21.043, -22.1494]], [[-18.8739, -19.7804, -21.1834], [-20.1233, -21.6765, -23.2944], [-20.0315, -21.2641, -23.6944]]])\n    elif model_name == 'segformer.b5.640x640.ade.160k':\n        expected_slice = torch.tensor([[[-9.5524, -12.0835, -11.7348], [-10.5229, -13.6446, -14.5662], [-9.5842, -12.8851, -13.9414]], [[-15.3432, -17.5323, -17.0818], [-16.333, -18.9255, -19.2101], [-15.134, -17.7848, -18.3971]], [[-12.6072, -14.9486, -14.6631], [-13.7629, -17.0907, -17.7745], [-12.7899, -16.1695, -17.1671]]])\n    elif model_name == 'segformer.b0.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.9295, -13.4057, -14.8106], [-13.3431, -14.8179, -15.3781], [-14.2836, -15.5942, -16.1588]], [[-11.4906, -12.8067, -13.6564], [-13.1189, -14.05, -14.1543], [-13.8748, -14.5136, -14.8789]], [[0.5374, 0.1067, -0.4742], [0.1141, -0.2255, -0.7099], [-0.3, -0.5924, -1.3105]]])\n    elif model_name == 'segformer.b0.512x1024.city.160k':\n        expected_slice = torch.tensor([[[-7.8217, -9.8767, -10.1717], [-9.4438, -10.9058, -11.4047], [-9.7939, -12.3495, -12.1079]], [[-7.1514, -9.5336, -10.086], [-9.7776, -11.6822, -11.8439], [-10.1411, -12.7655, -12.8972]], [[0.3021, 0.0805, -0.231], [-0.0328, -0.1605, -0.2714], [-0.1408, -0.5477, -0.6976]]])\n    elif model_name == 'segformer.b0.640x1280.city.160k':\n        expected_slice = torch.tensor([[[-11.372, -12.787, -13.477], [-12.536, -14.194, -14.409], [-13.217, -14.888, -15.327]], [[-14.791, -17.122, -18.277], [-17.163, -19.192, -19.533], [-17.897, -19.991, -20.315]], [[0.76723, 0.41921, -0.077878], [0.47772, 0.0095557, -0.28082], [0.36032, -0.24826, -0.51168]]])\n    elif model_name == 'segformer.b0.768x768.city.160k':\n        expected_slice = torch.tensor([[[-9.4959, -11.3087, -11.7479], [-11.0025, -12.654, -12.3319], [-11.4064, -13.0487, -12.9905]], [[-9.8905, -11.3084, -12.0854], [-11.1726, -12.7698, -12.9583], [-11.5985, -13.3278, -14.1774]], [[0.2213, 0.0192, -0.2466], [-0.1731, -0.4213, -0.4874], [-0.3126, -0.6541, -1.1389]]])\n    elif model_name == 'segformer.b1.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-13.5748, -13.9111, -12.65], [-14.35, -15.3683, -14.2328], [-14.7532, -16.0424, -15.6087]], [[-17.1651, -15.8725, -12.9653], [-17.258, -17.3718, -14.8223], [-16.6058, -16.8783, -16.7452]], [[-3.6456, -3.0209, -1.4203], [-3.0797, -3.1959, -2.0], [-1.8757, -1.9217, -1.6997]]])\n    elif model_name == 'segformer.b2.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-16.0976, -16.4856, -17.3962], [-16.6234, -19.0342, -19.7685], [-16.09, -18.0661, -19.118]], [[-18.475, -18.8488, -19.5074], [-19.403, -22.157, -22.5977], [-19.1191, -20.8486, -22.3783]], [[-4.5178, -5.5037, -6.5109], [-5.0884, -7.2174, -8.0334], [-4.4156, -5.8117, -7.297]]])\n    elif model_name == 'segformer.b3.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-14.2081, -14.4732, -14.1977], [-14.5867, -16.4423, -16.6356], [-13.4441, -14.9685, -16.8696]], [[-14.4576, -14.7073, -15.0451], [-15.0816, -17.6237, -17.9873], [-14.4213, -16.0199, -18.5992]], [[-4.7349, -4.9588, -5.0966], [-4.321, -6.9325, -7.2591], [-3.4312, -4.7484, -7.1917]]])\n    elif model_name == 'segformer.b4.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.7737, -11.9526, -11.3273], [-13.6692, -14.4574, -13.8878], [-13.8937, -14.6924, -15.9345]], [[-14.6706, -14.533, -14.1306], [-16.1502, -16.818, -16.4269], [-16.8338, -17.8939, -20.1746]], [[1.0491, 0.8289, 1.031], [1.1044, 0.5219, 0.8055], [1.0899, 0.6926, 0.559]]])\n    elif model_name == 'segformer.b5.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-12.5641, -13.4777, -13.0684], [-13.9587, -15.8983, -16.6557], [-13.3109, -15.735, -16.3141]], [[-14.7074, -15.4352, -14.5944], [-16.6353, -18.1663, -18.612], [-15.1702, -18.0329, -18.1547]], [[-1.799, -2.0951, -1.7784], [-2.6397, -3.8245, -3.9686], [-1.5264, -2.8126, -2.9316]]])\n    else:\n        predicted_class_idx = logits.argmax(-1).item()\n        print('Predicted class:', model.config.id2label[predicted_class_idx])\n    if not encoder_only:\n        assert logits.shape == expected_shape\n        assert torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    image_processor.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_segformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Copy/paste/tweak model's weights to our SegFormer structure.\\n    \"\n    config = SegformerConfig()\n    encoder_only = False\n    repo_id = 'huggingface/label-files'\n    if 'segformer' in model_name:\n        size = model_name[len('segformer.'):len('segformer.') + 2]\n        if 'ade' in model_name:\n            config.num_labels = 150\n            filename = 'ade20k-id2label.json'\n            expected_shape = (1, 150, 128, 128)\n        elif 'city' in model_name:\n            config.num_labels = 19\n            filename = 'cityscapes-id2label.json'\n            expected_shape = (1, 19, 128, 128)\n        else:\n            raise ValueError(f'Model {model_name} not supported')\n    elif 'mit' in model_name:\n        encoder_only = True\n        size = model_name[4:6]\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        expected_shape = (1, 1000)\n    else:\n        raise ValueError(f'Model {model_name} not supported')\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 'b0':\n        pass\n    elif size == 'b1':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 256\n    elif size == 'b2':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 6, 3]\n    elif size == 'b3':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 18, 3]\n    elif size == 'b4':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 8, 27, 3]\n    elif size == 'b5':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 6, 40, 3]\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = SegformerImageProcessor(image_scale=(512, 512), keep_ratio=False, align=False, do_random_crop=False)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    if encoder_only:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    else:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))['state_dict']\n    state_dict = rename_keys(state_dict, encoder_only=encoder_only)\n    if not encoder_only:\n        del state_dict['decode_head.conv_seg.weight']\n        del state_dict['decode_head.conv_seg.bias']\n    read_in_k_v(state_dict, config)\n    if encoder_only:\n        config.reshape_last_stage = False\n        model = SegformerForImageClassification(config)\n    else:\n        model = SegformerForSemanticSegmentation(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if model_name == 'segformer.b0.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-4.631, -5.5232, -6.2356], [-5.1921, -6.1444, -6.5996], [-5.4424, -6.279, -6.7574]], [[-12.1391, -13.3122, -13.9554], [-12.8732, -13.9352, -14.3563], [-12.9438, -13.8226, -14.2513]], [[-12.5134, -13.4686, -14.4915], [-12.8669, -14.4343, -14.7758], [-13.2523, -14.5819, -15.0694]]])\n    elif model_name == 'segformer.b1.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-7.582, -8.7231, -8.3215], [-8.06, -10.3529, -10.0304], [-7.5208, -9.4103, -9.6239]], [[-12.6918, -13.8994, -13.7137], [-13.3196, -15.7523, -15.4789], [-12.9343, -14.8757, -14.9689]], [[-11.1911, -11.9421, -11.3243], [-11.3342, -13.6839, -13.3581], [-10.3909, -12.1832, -12.4858]]])\n    elif model_name == 'segformer.b2.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-11.8173, -14.385, -16.3128], [-14.5648, -16.5804, -18.6568], [-14.7223, -15.7387, -18.4218]], [[-15.729, -17.9171, -19.4423], [-18.3105, -19.9448, -21.4661], [-17.9296, -18.6497, -20.791]], [[-15.0783, -17.0336, -18.2789], [-16.8771, -18.687, -20.1612], [-16.2454, -17.1426, -19.5055]]])\n    elif model_name == 'segformer.b3.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-9.0878, -10.2081, -10.1891], [-9.3144, -10.7941, -10.9843], [-9.2294, -10.3855, -10.5704]], [[-12.2316, -13.9068, -13.6102], [-12.9161, -14.3702, -14.3235], [-12.5233, -13.7174, -13.7932]], [[-14.6275, -15.249, -14.9727], [-14.34, -15.9687, -16.2827], [-14.1484, -15.4033, -15.8937]]])\n    elif model_name == 'segformer.b4.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-12.3144, -13.2447, -14.0802], [-13.3614, -14.5816, -15.6117], [-13.334, -14.4433, -16.2219]], [[-19.2781, -20.4128, -20.7506], [-20.6153, -21.6566, -22.0998], [-19.98, -21.043, -22.1494]], [[-18.8739, -19.7804, -21.1834], [-20.1233, -21.6765, -23.2944], [-20.0315, -21.2641, -23.6944]]])\n    elif model_name == 'segformer.b5.640x640.ade.160k':\n        expected_slice = torch.tensor([[[-9.5524, -12.0835, -11.7348], [-10.5229, -13.6446, -14.5662], [-9.5842, -12.8851, -13.9414]], [[-15.3432, -17.5323, -17.0818], [-16.333, -18.9255, -19.2101], [-15.134, -17.7848, -18.3971]], [[-12.6072, -14.9486, -14.6631], [-13.7629, -17.0907, -17.7745], [-12.7899, -16.1695, -17.1671]]])\n    elif model_name == 'segformer.b0.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.9295, -13.4057, -14.8106], [-13.3431, -14.8179, -15.3781], [-14.2836, -15.5942, -16.1588]], [[-11.4906, -12.8067, -13.6564], [-13.1189, -14.05, -14.1543], [-13.8748, -14.5136, -14.8789]], [[0.5374, 0.1067, -0.4742], [0.1141, -0.2255, -0.7099], [-0.3, -0.5924, -1.3105]]])\n    elif model_name == 'segformer.b0.512x1024.city.160k':\n        expected_slice = torch.tensor([[[-7.8217, -9.8767, -10.1717], [-9.4438, -10.9058, -11.4047], [-9.7939, -12.3495, -12.1079]], [[-7.1514, -9.5336, -10.086], [-9.7776, -11.6822, -11.8439], [-10.1411, -12.7655, -12.8972]], [[0.3021, 0.0805, -0.231], [-0.0328, -0.1605, -0.2714], [-0.1408, -0.5477, -0.6976]]])\n    elif model_name == 'segformer.b0.640x1280.city.160k':\n        expected_slice = torch.tensor([[[-11.372, -12.787, -13.477], [-12.536, -14.194, -14.409], [-13.217, -14.888, -15.327]], [[-14.791, -17.122, -18.277], [-17.163, -19.192, -19.533], [-17.897, -19.991, -20.315]], [[0.76723, 0.41921, -0.077878], [0.47772, 0.0095557, -0.28082], [0.36032, -0.24826, -0.51168]]])\n    elif model_name == 'segformer.b0.768x768.city.160k':\n        expected_slice = torch.tensor([[[-9.4959, -11.3087, -11.7479], [-11.0025, -12.654, -12.3319], [-11.4064, -13.0487, -12.9905]], [[-9.8905, -11.3084, -12.0854], [-11.1726, -12.7698, -12.9583], [-11.5985, -13.3278, -14.1774]], [[0.2213, 0.0192, -0.2466], [-0.1731, -0.4213, -0.4874], [-0.3126, -0.6541, -1.1389]]])\n    elif model_name == 'segformer.b1.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-13.5748, -13.9111, -12.65], [-14.35, -15.3683, -14.2328], [-14.7532, -16.0424, -15.6087]], [[-17.1651, -15.8725, -12.9653], [-17.258, -17.3718, -14.8223], [-16.6058, -16.8783, -16.7452]], [[-3.6456, -3.0209, -1.4203], [-3.0797, -3.1959, -2.0], [-1.8757, -1.9217, -1.6997]]])\n    elif model_name == 'segformer.b2.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-16.0976, -16.4856, -17.3962], [-16.6234, -19.0342, -19.7685], [-16.09, -18.0661, -19.118]], [[-18.475, -18.8488, -19.5074], [-19.403, -22.157, -22.5977], [-19.1191, -20.8486, -22.3783]], [[-4.5178, -5.5037, -6.5109], [-5.0884, -7.2174, -8.0334], [-4.4156, -5.8117, -7.297]]])\n    elif model_name == 'segformer.b3.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-14.2081, -14.4732, -14.1977], [-14.5867, -16.4423, -16.6356], [-13.4441, -14.9685, -16.8696]], [[-14.4576, -14.7073, -15.0451], [-15.0816, -17.6237, -17.9873], [-14.4213, -16.0199, -18.5992]], [[-4.7349, -4.9588, -5.0966], [-4.321, -6.9325, -7.2591], [-3.4312, -4.7484, -7.1917]]])\n    elif model_name == 'segformer.b4.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.7737, -11.9526, -11.3273], [-13.6692, -14.4574, -13.8878], [-13.8937, -14.6924, -15.9345]], [[-14.6706, -14.533, -14.1306], [-16.1502, -16.818, -16.4269], [-16.8338, -17.8939, -20.1746]], [[1.0491, 0.8289, 1.031], [1.1044, 0.5219, 0.8055], [1.0899, 0.6926, 0.559]]])\n    elif model_name == 'segformer.b5.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-12.5641, -13.4777, -13.0684], [-13.9587, -15.8983, -16.6557], [-13.3109, -15.735, -16.3141]], [[-14.7074, -15.4352, -14.5944], [-16.6353, -18.1663, -18.612], [-15.1702, -18.0329, -18.1547]], [[-1.799, -2.0951, -1.7784], [-2.6397, -3.8245, -3.9686], [-1.5264, -2.8126, -2.9316]]])\n    else:\n        predicted_class_idx = logits.argmax(-1).item()\n        print('Predicted class:', model.config.id2label[predicted_class_idx])\n    if not encoder_only:\n        assert logits.shape == expected_shape\n        assert torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    image_processor.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_segformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Copy/paste/tweak model's weights to our SegFormer structure.\\n    \"\n    config = SegformerConfig()\n    encoder_only = False\n    repo_id = 'huggingface/label-files'\n    if 'segformer' in model_name:\n        size = model_name[len('segformer.'):len('segformer.') + 2]\n        if 'ade' in model_name:\n            config.num_labels = 150\n            filename = 'ade20k-id2label.json'\n            expected_shape = (1, 150, 128, 128)\n        elif 'city' in model_name:\n            config.num_labels = 19\n            filename = 'cityscapes-id2label.json'\n            expected_shape = (1, 19, 128, 128)\n        else:\n            raise ValueError(f'Model {model_name} not supported')\n    elif 'mit' in model_name:\n        encoder_only = True\n        size = model_name[4:6]\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        expected_shape = (1, 1000)\n    else:\n        raise ValueError(f'Model {model_name} not supported')\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 'b0':\n        pass\n    elif size == 'b1':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 256\n    elif size == 'b2':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 6, 3]\n    elif size == 'b3':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 4, 18, 3]\n    elif size == 'b4':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 8, 27, 3]\n    elif size == 'b5':\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.decoder_hidden_size = 768\n        config.depths = [3, 6, 40, 3]\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = SegformerImageProcessor(image_scale=(512, 512), keep_ratio=False, align=False, do_random_crop=False)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    if encoder_only:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    else:\n        state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))['state_dict']\n    state_dict = rename_keys(state_dict, encoder_only=encoder_only)\n    if not encoder_only:\n        del state_dict['decode_head.conv_seg.weight']\n        del state_dict['decode_head.conv_seg.bias']\n    read_in_k_v(state_dict, config)\n    if encoder_only:\n        config.reshape_last_stage = False\n        model = SegformerForImageClassification(config)\n    else:\n        model = SegformerForSemanticSegmentation(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if model_name == 'segformer.b0.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-4.631, -5.5232, -6.2356], [-5.1921, -6.1444, -6.5996], [-5.4424, -6.279, -6.7574]], [[-12.1391, -13.3122, -13.9554], [-12.8732, -13.9352, -14.3563], [-12.9438, -13.8226, -14.2513]], [[-12.5134, -13.4686, -14.4915], [-12.8669, -14.4343, -14.7758], [-13.2523, -14.5819, -15.0694]]])\n    elif model_name == 'segformer.b1.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-7.582, -8.7231, -8.3215], [-8.06, -10.3529, -10.0304], [-7.5208, -9.4103, -9.6239]], [[-12.6918, -13.8994, -13.7137], [-13.3196, -15.7523, -15.4789], [-12.9343, -14.8757, -14.9689]], [[-11.1911, -11.9421, -11.3243], [-11.3342, -13.6839, -13.3581], [-10.3909, -12.1832, -12.4858]]])\n    elif model_name == 'segformer.b2.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-11.8173, -14.385, -16.3128], [-14.5648, -16.5804, -18.6568], [-14.7223, -15.7387, -18.4218]], [[-15.729, -17.9171, -19.4423], [-18.3105, -19.9448, -21.4661], [-17.9296, -18.6497, -20.791]], [[-15.0783, -17.0336, -18.2789], [-16.8771, -18.687, -20.1612], [-16.2454, -17.1426, -19.5055]]])\n    elif model_name == 'segformer.b3.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-9.0878, -10.2081, -10.1891], [-9.3144, -10.7941, -10.9843], [-9.2294, -10.3855, -10.5704]], [[-12.2316, -13.9068, -13.6102], [-12.9161, -14.3702, -14.3235], [-12.5233, -13.7174, -13.7932]], [[-14.6275, -15.249, -14.9727], [-14.34, -15.9687, -16.2827], [-14.1484, -15.4033, -15.8937]]])\n    elif model_name == 'segformer.b4.512x512.ade.160k':\n        expected_slice = torch.tensor([[[-12.3144, -13.2447, -14.0802], [-13.3614, -14.5816, -15.6117], [-13.334, -14.4433, -16.2219]], [[-19.2781, -20.4128, -20.7506], [-20.6153, -21.6566, -22.0998], [-19.98, -21.043, -22.1494]], [[-18.8739, -19.7804, -21.1834], [-20.1233, -21.6765, -23.2944], [-20.0315, -21.2641, -23.6944]]])\n    elif model_name == 'segformer.b5.640x640.ade.160k':\n        expected_slice = torch.tensor([[[-9.5524, -12.0835, -11.7348], [-10.5229, -13.6446, -14.5662], [-9.5842, -12.8851, -13.9414]], [[-15.3432, -17.5323, -17.0818], [-16.333, -18.9255, -19.2101], [-15.134, -17.7848, -18.3971]], [[-12.6072, -14.9486, -14.6631], [-13.7629, -17.0907, -17.7745], [-12.7899, -16.1695, -17.1671]]])\n    elif model_name == 'segformer.b0.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.9295, -13.4057, -14.8106], [-13.3431, -14.8179, -15.3781], [-14.2836, -15.5942, -16.1588]], [[-11.4906, -12.8067, -13.6564], [-13.1189, -14.05, -14.1543], [-13.8748, -14.5136, -14.8789]], [[0.5374, 0.1067, -0.4742], [0.1141, -0.2255, -0.7099], [-0.3, -0.5924, -1.3105]]])\n    elif model_name == 'segformer.b0.512x1024.city.160k':\n        expected_slice = torch.tensor([[[-7.8217, -9.8767, -10.1717], [-9.4438, -10.9058, -11.4047], [-9.7939, -12.3495, -12.1079]], [[-7.1514, -9.5336, -10.086], [-9.7776, -11.6822, -11.8439], [-10.1411, -12.7655, -12.8972]], [[0.3021, 0.0805, -0.231], [-0.0328, -0.1605, -0.2714], [-0.1408, -0.5477, -0.6976]]])\n    elif model_name == 'segformer.b0.640x1280.city.160k':\n        expected_slice = torch.tensor([[[-11.372, -12.787, -13.477], [-12.536, -14.194, -14.409], [-13.217, -14.888, -15.327]], [[-14.791, -17.122, -18.277], [-17.163, -19.192, -19.533], [-17.897, -19.991, -20.315]], [[0.76723, 0.41921, -0.077878], [0.47772, 0.0095557, -0.28082], [0.36032, -0.24826, -0.51168]]])\n    elif model_name == 'segformer.b0.768x768.city.160k':\n        expected_slice = torch.tensor([[[-9.4959, -11.3087, -11.7479], [-11.0025, -12.654, -12.3319], [-11.4064, -13.0487, -12.9905]], [[-9.8905, -11.3084, -12.0854], [-11.1726, -12.7698, -12.9583], [-11.5985, -13.3278, -14.1774]], [[0.2213, 0.0192, -0.2466], [-0.1731, -0.4213, -0.4874], [-0.3126, -0.6541, -1.1389]]])\n    elif model_name == 'segformer.b1.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-13.5748, -13.9111, -12.65], [-14.35, -15.3683, -14.2328], [-14.7532, -16.0424, -15.6087]], [[-17.1651, -15.8725, -12.9653], [-17.258, -17.3718, -14.8223], [-16.6058, -16.8783, -16.7452]], [[-3.6456, -3.0209, -1.4203], [-3.0797, -3.1959, -2.0], [-1.8757, -1.9217, -1.6997]]])\n    elif model_name == 'segformer.b2.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-16.0976, -16.4856, -17.3962], [-16.6234, -19.0342, -19.7685], [-16.09, -18.0661, -19.118]], [[-18.475, -18.8488, -19.5074], [-19.403, -22.157, -22.5977], [-19.1191, -20.8486, -22.3783]], [[-4.5178, -5.5037, -6.5109], [-5.0884, -7.2174, -8.0334], [-4.4156, -5.8117, -7.297]]])\n    elif model_name == 'segformer.b3.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-14.2081, -14.4732, -14.1977], [-14.5867, -16.4423, -16.6356], [-13.4441, -14.9685, -16.8696]], [[-14.4576, -14.7073, -15.0451], [-15.0816, -17.6237, -17.9873], [-14.4213, -16.0199, -18.5992]], [[-4.7349, -4.9588, -5.0966], [-4.321, -6.9325, -7.2591], [-3.4312, -4.7484, -7.1917]]])\n    elif model_name == 'segformer.b4.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-11.7737, -11.9526, -11.3273], [-13.6692, -14.4574, -13.8878], [-13.8937, -14.6924, -15.9345]], [[-14.6706, -14.533, -14.1306], [-16.1502, -16.818, -16.4269], [-16.8338, -17.8939, -20.1746]], [[1.0491, 0.8289, 1.031], [1.1044, 0.5219, 0.8055], [1.0899, 0.6926, 0.559]]])\n    elif model_name == 'segformer.b5.1024x1024.city.160k':\n        expected_slice = torch.tensor([[[-12.5641, -13.4777, -13.0684], [-13.9587, -15.8983, -16.6557], [-13.3109, -15.735, -16.3141]], [[-14.7074, -15.4352, -14.5944], [-16.6353, -18.1663, -18.612], [-15.1702, -18.0329, -18.1547]], [[-1.799, -2.0951, -1.7784], [-2.6397, -3.8245, -3.9686], [-1.5264, -2.8126, -2.9316]]])\n    else:\n        predicted_class_idx = logits.argmax(-1).item()\n        print('Predicted class:', model.config.id2label[predicted_class_idx])\n    if not encoder_only:\n        assert logits.shape == expected_shape\n        assert torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    image_processor.save_pretrained(pytorch_dump_folder_path)"
        ]
    }
]