[
    {
        "func_name": "_build_graph",
        "original": "def _build_graph(self):\n    \"\"\"The main function to create xdeepfm's logic.\n\n        Returns:\n            object: The prediction score made by the model.\n        \"\"\"\n    hparams = self.hparams\n    self.keep_prob_train = 1 - np.array(hparams.dropout)\n    self.keep_prob_test = np.ones_like(hparams.dropout)\n    with tf.compat.v1.variable_scope('XDeepFM') as scope:\n        with tf.compat.v1.variable_scope('embedding', initializer=self.initializer) as escope:\n            self.embedding = tf.compat.v1.get_variable(name='embedding_layer', shape=[hparams.FEATURE_COUNT, hparams.dim], dtype=tf.float32)\n            self.embed_params.append(self.embedding)\n            (embed_out, embed_layer_size) = self._build_embedding()\n        logit = 0\n        if hparams.use_Linear_part:\n            print('Add linear part.')\n            logit = logit + self._build_linear()\n        if hparams.use_FM_part:\n            print('Add FM part.')\n            logit = logit + self._build_fm()\n        if hparams.use_CIN_part:\n            print('Add CIN part.')\n            if hparams.fast_CIN_d <= 0:\n                logit = logit + self._build_CIN(embed_out, res=True, direct=False, bias=False, is_masked=True)\n            else:\n                logit = logit + self._build_fast_CIN(embed_out, res=True, direct=False, bias=False)\n        if hparams.use_DNN_part:\n            print('Add DNN part.')\n            logit = logit + self._build_dnn(embed_out, embed_layer_size)\n        return logit",
        "mutated": [
            "def _build_graph(self):\n    if False:\n        i = 10\n    \"The main function to create xdeepfm's logic.\\n\\n        Returns:\\n            object: The prediction score made by the model.\\n        \"\n    hparams = self.hparams\n    self.keep_prob_train = 1 - np.array(hparams.dropout)\n    self.keep_prob_test = np.ones_like(hparams.dropout)\n    with tf.compat.v1.variable_scope('XDeepFM') as scope:\n        with tf.compat.v1.variable_scope('embedding', initializer=self.initializer) as escope:\n            self.embedding = tf.compat.v1.get_variable(name='embedding_layer', shape=[hparams.FEATURE_COUNT, hparams.dim], dtype=tf.float32)\n            self.embed_params.append(self.embedding)\n            (embed_out, embed_layer_size) = self._build_embedding()\n        logit = 0\n        if hparams.use_Linear_part:\n            print('Add linear part.')\n            logit = logit + self._build_linear()\n        if hparams.use_FM_part:\n            print('Add FM part.')\n            logit = logit + self._build_fm()\n        if hparams.use_CIN_part:\n            print('Add CIN part.')\n            if hparams.fast_CIN_d <= 0:\n                logit = logit + self._build_CIN(embed_out, res=True, direct=False, bias=False, is_masked=True)\n            else:\n                logit = logit + self._build_fast_CIN(embed_out, res=True, direct=False, bias=False)\n        if hparams.use_DNN_part:\n            print('Add DNN part.')\n            logit = logit + self._build_dnn(embed_out, embed_layer_size)\n        return logit",
            "def _build_graph(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"The main function to create xdeepfm's logic.\\n\\n        Returns:\\n            object: The prediction score made by the model.\\n        \"\n    hparams = self.hparams\n    self.keep_prob_train = 1 - np.array(hparams.dropout)\n    self.keep_prob_test = np.ones_like(hparams.dropout)\n    with tf.compat.v1.variable_scope('XDeepFM') as scope:\n        with tf.compat.v1.variable_scope('embedding', initializer=self.initializer) as escope:\n            self.embedding = tf.compat.v1.get_variable(name='embedding_layer', shape=[hparams.FEATURE_COUNT, hparams.dim], dtype=tf.float32)\n            self.embed_params.append(self.embedding)\n            (embed_out, embed_layer_size) = self._build_embedding()\n        logit = 0\n        if hparams.use_Linear_part:\n            print('Add linear part.')\n            logit = logit + self._build_linear()\n        if hparams.use_FM_part:\n            print('Add FM part.')\n            logit = logit + self._build_fm()\n        if hparams.use_CIN_part:\n            print('Add CIN part.')\n            if hparams.fast_CIN_d <= 0:\n                logit = logit + self._build_CIN(embed_out, res=True, direct=False, bias=False, is_masked=True)\n            else:\n                logit = logit + self._build_fast_CIN(embed_out, res=True, direct=False, bias=False)\n        if hparams.use_DNN_part:\n            print('Add DNN part.')\n            logit = logit + self._build_dnn(embed_out, embed_layer_size)\n        return logit",
            "def _build_graph(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"The main function to create xdeepfm's logic.\\n\\n        Returns:\\n            object: The prediction score made by the model.\\n        \"\n    hparams = self.hparams\n    self.keep_prob_train = 1 - np.array(hparams.dropout)\n    self.keep_prob_test = np.ones_like(hparams.dropout)\n    with tf.compat.v1.variable_scope('XDeepFM') as scope:\n        with tf.compat.v1.variable_scope('embedding', initializer=self.initializer) as escope:\n            self.embedding = tf.compat.v1.get_variable(name='embedding_layer', shape=[hparams.FEATURE_COUNT, hparams.dim], dtype=tf.float32)\n            self.embed_params.append(self.embedding)\n            (embed_out, embed_layer_size) = self._build_embedding()\n        logit = 0\n        if hparams.use_Linear_part:\n            print('Add linear part.')\n            logit = logit + self._build_linear()\n        if hparams.use_FM_part:\n            print('Add FM part.')\n            logit = logit + self._build_fm()\n        if hparams.use_CIN_part:\n            print('Add CIN part.')\n            if hparams.fast_CIN_d <= 0:\n                logit = logit + self._build_CIN(embed_out, res=True, direct=False, bias=False, is_masked=True)\n            else:\n                logit = logit + self._build_fast_CIN(embed_out, res=True, direct=False, bias=False)\n        if hparams.use_DNN_part:\n            print('Add DNN part.')\n            logit = logit + self._build_dnn(embed_out, embed_layer_size)\n        return logit",
            "def _build_graph(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"The main function to create xdeepfm's logic.\\n\\n        Returns:\\n            object: The prediction score made by the model.\\n        \"\n    hparams = self.hparams\n    self.keep_prob_train = 1 - np.array(hparams.dropout)\n    self.keep_prob_test = np.ones_like(hparams.dropout)\n    with tf.compat.v1.variable_scope('XDeepFM') as scope:\n        with tf.compat.v1.variable_scope('embedding', initializer=self.initializer) as escope:\n            self.embedding = tf.compat.v1.get_variable(name='embedding_layer', shape=[hparams.FEATURE_COUNT, hparams.dim], dtype=tf.float32)\n            self.embed_params.append(self.embedding)\n            (embed_out, embed_layer_size) = self._build_embedding()\n        logit = 0\n        if hparams.use_Linear_part:\n            print('Add linear part.')\n            logit = logit + self._build_linear()\n        if hparams.use_FM_part:\n            print('Add FM part.')\n            logit = logit + self._build_fm()\n        if hparams.use_CIN_part:\n            print('Add CIN part.')\n            if hparams.fast_CIN_d <= 0:\n                logit = logit + self._build_CIN(embed_out, res=True, direct=False, bias=False, is_masked=True)\n            else:\n                logit = logit + self._build_fast_CIN(embed_out, res=True, direct=False, bias=False)\n        if hparams.use_DNN_part:\n            print('Add DNN part.')\n            logit = logit + self._build_dnn(embed_out, embed_layer_size)\n        return logit",
            "def _build_graph(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"The main function to create xdeepfm's logic.\\n\\n        Returns:\\n            object: The prediction score made by the model.\\n        \"\n    hparams = self.hparams\n    self.keep_prob_train = 1 - np.array(hparams.dropout)\n    self.keep_prob_test = np.ones_like(hparams.dropout)\n    with tf.compat.v1.variable_scope('XDeepFM') as scope:\n        with tf.compat.v1.variable_scope('embedding', initializer=self.initializer) as escope:\n            self.embedding = tf.compat.v1.get_variable(name='embedding_layer', shape=[hparams.FEATURE_COUNT, hparams.dim], dtype=tf.float32)\n            self.embed_params.append(self.embedding)\n            (embed_out, embed_layer_size) = self._build_embedding()\n        logit = 0\n        if hparams.use_Linear_part:\n            print('Add linear part.')\n            logit = logit + self._build_linear()\n        if hparams.use_FM_part:\n            print('Add FM part.')\n            logit = logit + self._build_fm()\n        if hparams.use_CIN_part:\n            print('Add CIN part.')\n            if hparams.fast_CIN_d <= 0:\n                logit = logit + self._build_CIN(embed_out, res=True, direct=False, bias=False, is_masked=True)\n            else:\n                logit = logit + self._build_fast_CIN(embed_out, res=True, direct=False, bias=False)\n        if hparams.use_DNN_part:\n            print('Add DNN part.')\n            logit = logit + self._build_dnn(embed_out, embed_layer_size)\n        return logit"
        ]
    },
    {
        "func_name": "_build_embedding",
        "original": "def _build_embedding(self):\n    \"\"\"The field embedding layer. MLP requires fixed-length vectors as input.\n        This function makes sum pooling of feature embeddings for each field.\n\n        Returns:\n            embedding:  The result of field embedding layer, with size of #_fields * #_dim.\n            embedding_size: #_fields * #_dim\n        \"\"\"\n    hparams = self.hparams\n    fm_sparse_index = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_values, self.iterator.dnn_feat_shape)\n    fm_sparse_weight = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_weights, self.iterator.dnn_feat_shape)\n    w_fm_nn_input_orgin = tf.nn.embedding_lookup_sparse(params=self.embedding, sp_ids=fm_sparse_index, sp_weights=fm_sparse_weight, combiner='sum')\n    embedding = tf.reshape(w_fm_nn_input_orgin, [-1, hparams.dim * hparams.FIELD_COUNT])\n    embedding_size = hparams.FIELD_COUNT * hparams.dim\n    return (embedding, embedding_size)",
        "mutated": [
            "def _build_embedding(self):\n    if False:\n        i = 10\n    'The field embedding layer. MLP requires fixed-length vectors as input.\\n        This function makes sum pooling of feature embeddings for each field.\\n\\n        Returns:\\n            embedding:  The result of field embedding layer, with size of #_fields * #_dim.\\n            embedding_size: #_fields * #_dim\\n        '\n    hparams = self.hparams\n    fm_sparse_index = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_values, self.iterator.dnn_feat_shape)\n    fm_sparse_weight = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_weights, self.iterator.dnn_feat_shape)\n    w_fm_nn_input_orgin = tf.nn.embedding_lookup_sparse(params=self.embedding, sp_ids=fm_sparse_index, sp_weights=fm_sparse_weight, combiner='sum')\n    embedding = tf.reshape(w_fm_nn_input_orgin, [-1, hparams.dim * hparams.FIELD_COUNT])\n    embedding_size = hparams.FIELD_COUNT * hparams.dim\n    return (embedding, embedding_size)",
            "def _build_embedding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'The field embedding layer. MLP requires fixed-length vectors as input.\\n        This function makes sum pooling of feature embeddings for each field.\\n\\n        Returns:\\n            embedding:  The result of field embedding layer, with size of #_fields * #_dim.\\n            embedding_size: #_fields * #_dim\\n        '\n    hparams = self.hparams\n    fm_sparse_index = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_values, self.iterator.dnn_feat_shape)\n    fm_sparse_weight = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_weights, self.iterator.dnn_feat_shape)\n    w_fm_nn_input_orgin = tf.nn.embedding_lookup_sparse(params=self.embedding, sp_ids=fm_sparse_index, sp_weights=fm_sparse_weight, combiner='sum')\n    embedding = tf.reshape(w_fm_nn_input_orgin, [-1, hparams.dim * hparams.FIELD_COUNT])\n    embedding_size = hparams.FIELD_COUNT * hparams.dim\n    return (embedding, embedding_size)",
            "def _build_embedding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'The field embedding layer. MLP requires fixed-length vectors as input.\\n        This function makes sum pooling of feature embeddings for each field.\\n\\n        Returns:\\n            embedding:  The result of field embedding layer, with size of #_fields * #_dim.\\n            embedding_size: #_fields * #_dim\\n        '\n    hparams = self.hparams\n    fm_sparse_index = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_values, self.iterator.dnn_feat_shape)\n    fm_sparse_weight = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_weights, self.iterator.dnn_feat_shape)\n    w_fm_nn_input_orgin = tf.nn.embedding_lookup_sparse(params=self.embedding, sp_ids=fm_sparse_index, sp_weights=fm_sparse_weight, combiner='sum')\n    embedding = tf.reshape(w_fm_nn_input_orgin, [-1, hparams.dim * hparams.FIELD_COUNT])\n    embedding_size = hparams.FIELD_COUNT * hparams.dim\n    return (embedding, embedding_size)",
            "def _build_embedding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'The field embedding layer. MLP requires fixed-length vectors as input.\\n        This function makes sum pooling of feature embeddings for each field.\\n\\n        Returns:\\n            embedding:  The result of field embedding layer, with size of #_fields * #_dim.\\n            embedding_size: #_fields * #_dim\\n        '\n    hparams = self.hparams\n    fm_sparse_index = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_values, self.iterator.dnn_feat_shape)\n    fm_sparse_weight = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_weights, self.iterator.dnn_feat_shape)\n    w_fm_nn_input_orgin = tf.nn.embedding_lookup_sparse(params=self.embedding, sp_ids=fm_sparse_index, sp_weights=fm_sparse_weight, combiner='sum')\n    embedding = tf.reshape(w_fm_nn_input_orgin, [-1, hparams.dim * hparams.FIELD_COUNT])\n    embedding_size = hparams.FIELD_COUNT * hparams.dim\n    return (embedding, embedding_size)",
            "def _build_embedding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'The field embedding layer. MLP requires fixed-length vectors as input.\\n        This function makes sum pooling of feature embeddings for each field.\\n\\n        Returns:\\n            embedding:  The result of field embedding layer, with size of #_fields * #_dim.\\n            embedding_size: #_fields * #_dim\\n        '\n    hparams = self.hparams\n    fm_sparse_index = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_values, self.iterator.dnn_feat_shape)\n    fm_sparse_weight = tf.SparseTensor(self.iterator.dnn_feat_indices, self.iterator.dnn_feat_weights, self.iterator.dnn_feat_shape)\n    w_fm_nn_input_orgin = tf.nn.embedding_lookup_sparse(params=self.embedding, sp_ids=fm_sparse_index, sp_weights=fm_sparse_weight, combiner='sum')\n    embedding = tf.reshape(w_fm_nn_input_orgin, [-1, hparams.dim * hparams.FIELD_COUNT])\n    embedding_size = hparams.FIELD_COUNT * hparams.dim\n    return (embedding, embedding_size)"
        ]
    },
    {
        "func_name": "_build_linear",
        "original": "def _build_linear(self):\n    \"\"\"Construct the linear part for the model.\n        This is a linear regression.\n\n        Returns:\n            object: Prediction score made by linear regression.\n        \"\"\"\n    with tf.compat.v1.variable_scope('linear_part', initializer=self.initializer) as scope:\n        w = tf.compat.v1.get_variable(name='w', shape=[self.hparams.FEATURE_COUNT, 1], dtype=tf.float32)\n        b = tf.compat.v1.get_variable(name='b', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        linear_output = tf.add(tf.sparse.sparse_dense_matmul(x, w), b)\n        self.layer_params.append(w)\n        self.layer_params.append(b)\n        tf.compat.v1.summary.histogram('linear_part/w', w)\n        tf.compat.v1.summary.histogram('linear_part/b', b)\n        return linear_output",
        "mutated": [
            "def _build_linear(self):\n    if False:\n        i = 10\n    'Construct the linear part for the model.\\n        This is a linear regression.\\n\\n        Returns:\\n            object: Prediction score made by linear regression.\\n        '\n    with tf.compat.v1.variable_scope('linear_part', initializer=self.initializer) as scope:\n        w = tf.compat.v1.get_variable(name='w', shape=[self.hparams.FEATURE_COUNT, 1], dtype=tf.float32)\n        b = tf.compat.v1.get_variable(name='b', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        linear_output = tf.add(tf.sparse.sparse_dense_matmul(x, w), b)\n        self.layer_params.append(w)\n        self.layer_params.append(b)\n        tf.compat.v1.summary.histogram('linear_part/w', w)\n        tf.compat.v1.summary.histogram('linear_part/b', b)\n        return linear_output",
            "def _build_linear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Construct the linear part for the model.\\n        This is a linear regression.\\n\\n        Returns:\\n            object: Prediction score made by linear regression.\\n        '\n    with tf.compat.v1.variable_scope('linear_part', initializer=self.initializer) as scope:\n        w = tf.compat.v1.get_variable(name='w', shape=[self.hparams.FEATURE_COUNT, 1], dtype=tf.float32)\n        b = tf.compat.v1.get_variable(name='b', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        linear_output = tf.add(tf.sparse.sparse_dense_matmul(x, w), b)\n        self.layer_params.append(w)\n        self.layer_params.append(b)\n        tf.compat.v1.summary.histogram('linear_part/w', w)\n        tf.compat.v1.summary.histogram('linear_part/b', b)\n        return linear_output",
            "def _build_linear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Construct the linear part for the model.\\n        This is a linear regression.\\n\\n        Returns:\\n            object: Prediction score made by linear regression.\\n        '\n    with tf.compat.v1.variable_scope('linear_part', initializer=self.initializer) as scope:\n        w = tf.compat.v1.get_variable(name='w', shape=[self.hparams.FEATURE_COUNT, 1], dtype=tf.float32)\n        b = tf.compat.v1.get_variable(name='b', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        linear_output = tf.add(tf.sparse.sparse_dense_matmul(x, w), b)\n        self.layer_params.append(w)\n        self.layer_params.append(b)\n        tf.compat.v1.summary.histogram('linear_part/w', w)\n        tf.compat.v1.summary.histogram('linear_part/b', b)\n        return linear_output",
            "def _build_linear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Construct the linear part for the model.\\n        This is a linear regression.\\n\\n        Returns:\\n            object: Prediction score made by linear regression.\\n        '\n    with tf.compat.v1.variable_scope('linear_part', initializer=self.initializer) as scope:\n        w = tf.compat.v1.get_variable(name='w', shape=[self.hparams.FEATURE_COUNT, 1], dtype=tf.float32)\n        b = tf.compat.v1.get_variable(name='b', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        linear_output = tf.add(tf.sparse.sparse_dense_matmul(x, w), b)\n        self.layer_params.append(w)\n        self.layer_params.append(b)\n        tf.compat.v1.summary.histogram('linear_part/w', w)\n        tf.compat.v1.summary.histogram('linear_part/b', b)\n        return linear_output",
            "def _build_linear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Construct the linear part for the model.\\n        This is a linear regression.\\n\\n        Returns:\\n            object: Prediction score made by linear regression.\\n        '\n    with tf.compat.v1.variable_scope('linear_part', initializer=self.initializer) as scope:\n        w = tf.compat.v1.get_variable(name='w', shape=[self.hparams.FEATURE_COUNT, 1], dtype=tf.float32)\n        b = tf.compat.v1.get_variable(name='b', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        linear_output = tf.add(tf.sparse.sparse_dense_matmul(x, w), b)\n        self.layer_params.append(w)\n        self.layer_params.append(b)\n        tf.compat.v1.summary.histogram('linear_part/w', w)\n        tf.compat.v1.summary.histogram('linear_part/b', b)\n        return linear_output"
        ]
    },
    {
        "func_name": "_build_fm",
        "original": "def _build_fm(self):\n    \"\"\"Construct the factorization machine part for the model.\n        This is a traditional 2-order FM module.\n\n        Returns:\n            object: Prediction score made by factorization machine.\n        \"\"\"\n    with tf.compat.v1.variable_scope('fm_part') as scope:\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        xx = tf.SparseTensor(self.iterator.fm_feat_indices, tf.pow(self.iterator.fm_feat_values, 2), self.iterator.fm_feat_shape)\n        fm_output = 0.5 * tf.reduce_sum(input_tensor=tf.pow(tf.sparse.sparse_dense_matmul(x, self.embedding), 2) - tf.sparse.sparse_dense_matmul(xx, tf.pow(self.embedding, 2)), axis=1, keepdims=True)\n        return fm_output",
        "mutated": [
            "def _build_fm(self):\n    if False:\n        i = 10\n    'Construct the factorization machine part for the model.\\n        This is a traditional 2-order FM module.\\n\\n        Returns:\\n            object: Prediction score made by factorization machine.\\n        '\n    with tf.compat.v1.variable_scope('fm_part') as scope:\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        xx = tf.SparseTensor(self.iterator.fm_feat_indices, tf.pow(self.iterator.fm_feat_values, 2), self.iterator.fm_feat_shape)\n        fm_output = 0.5 * tf.reduce_sum(input_tensor=tf.pow(tf.sparse.sparse_dense_matmul(x, self.embedding), 2) - tf.sparse.sparse_dense_matmul(xx, tf.pow(self.embedding, 2)), axis=1, keepdims=True)\n        return fm_output",
            "def _build_fm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Construct the factorization machine part for the model.\\n        This is a traditional 2-order FM module.\\n\\n        Returns:\\n            object: Prediction score made by factorization machine.\\n        '\n    with tf.compat.v1.variable_scope('fm_part') as scope:\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        xx = tf.SparseTensor(self.iterator.fm_feat_indices, tf.pow(self.iterator.fm_feat_values, 2), self.iterator.fm_feat_shape)\n        fm_output = 0.5 * tf.reduce_sum(input_tensor=tf.pow(tf.sparse.sparse_dense_matmul(x, self.embedding), 2) - tf.sparse.sparse_dense_matmul(xx, tf.pow(self.embedding, 2)), axis=1, keepdims=True)\n        return fm_output",
            "def _build_fm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Construct the factorization machine part for the model.\\n        This is a traditional 2-order FM module.\\n\\n        Returns:\\n            object: Prediction score made by factorization machine.\\n        '\n    with tf.compat.v1.variable_scope('fm_part') as scope:\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        xx = tf.SparseTensor(self.iterator.fm_feat_indices, tf.pow(self.iterator.fm_feat_values, 2), self.iterator.fm_feat_shape)\n        fm_output = 0.5 * tf.reduce_sum(input_tensor=tf.pow(tf.sparse.sparse_dense_matmul(x, self.embedding), 2) - tf.sparse.sparse_dense_matmul(xx, tf.pow(self.embedding, 2)), axis=1, keepdims=True)\n        return fm_output",
            "def _build_fm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Construct the factorization machine part for the model.\\n        This is a traditional 2-order FM module.\\n\\n        Returns:\\n            object: Prediction score made by factorization machine.\\n        '\n    with tf.compat.v1.variable_scope('fm_part') as scope:\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        xx = tf.SparseTensor(self.iterator.fm_feat_indices, tf.pow(self.iterator.fm_feat_values, 2), self.iterator.fm_feat_shape)\n        fm_output = 0.5 * tf.reduce_sum(input_tensor=tf.pow(tf.sparse.sparse_dense_matmul(x, self.embedding), 2) - tf.sparse.sparse_dense_matmul(xx, tf.pow(self.embedding, 2)), axis=1, keepdims=True)\n        return fm_output",
            "def _build_fm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Construct the factorization machine part for the model.\\n        This is a traditional 2-order FM module.\\n\\n        Returns:\\n            object: Prediction score made by factorization machine.\\n        '\n    with tf.compat.v1.variable_scope('fm_part') as scope:\n        x = tf.SparseTensor(self.iterator.fm_feat_indices, self.iterator.fm_feat_values, self.iterator.fm_feat_shape)\n        xx = tf.SparseTensor(self.iterator.fm_feat_indices, tf.pow(self.iterator.fm_feat_values, 2), self.iterator.fm_feat_shape)\n        fm_output = 0.5 * tf.reduce_sum(input_tensor=tf.pow(tf.sparse.sparse_dense_matmul(x, self.embedding), 2) - tf.sparse.sparse_dense_matmul(xx, tf.pow(self.embedding, 2)), axis=1, keepdims=True)\n        return fm_output"
        ]
    },
    {
        "func_name": "_build_CIN",
        "original": "def _build_CIN(self, nn_input, res=False, direct=False, bias=False, is_masked=False):\n    \"\"\"Construct the compressed interaction network.\n        This component provides explicit and vector-wise higher-order feature interactions.\n\n        Args:\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\n            bias (bool): Whether to add bias term when calculating the feature maps.\n            is_masked (bool): Controls whether to remove self-interaction in the first layer of CIN.\n\n        Returns:\n            object: Prediction score made by CIN.\n        \"\"\"\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    split_tensor0 = tf.split(hidden_nn_layers[0], hparams.dim * [1], 2)\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            split_tensor = tf.split(hidden_nn_layers[-1], hparams.dim * [1], 2)\n            dot_result_m = tf.matmul(split_tensor0, split_tensor, transpose_b=True)\n            dot_result_o = tf.reshape(dot_result_m, shape=[hparams.dim, -1, field_nums[0] * field_nums[-1]])\n            dot_result = tf.transpose(a=dot_result_o, perm=[1, 0, 2])\n            filters = tf.compat.v1.get_variable(name='f_' + str(idx), shape=[1, field_nums[-1] * field_nums[0], layer_size], dtype=tf.float32)\n            if is_masked and idx == 0:\n                ones = tf.ones([field_nums[0], field_nums[0]], dtype=tf.float32)\n                mask_matrix = tf.linalg.band_part(ones, 0, -1) - tf.linalg.tensor_diag(tf.ones(field_nums[0]))\n                mask_matrix = tf.reshape(mask_matrix, shape=[1, field_nums[0] * field_nums[0]])\n                dot_result = tf.multiply(dot_result, mask_matrix) * 2\n                self.dot_result = dot_result\n            curr_out = tf.nn.conv1d(input=dot_result, filters=filters, stride=1, padding='VALID')\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            curr_out = tf.transpose(a=curr_out, perm=[0, 2, 1])\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            else:\n                if idx != len(hparams.cross_layer_sizes) - 1:\n                    (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 1)\n                    final_len += int(layer_size / 2)\n                else:\n                    direct_connect = curr_out\n                    next_hidden = 0\n                    final_len += layer_size\n                field_nums.append(int(layer_size / 2))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n            self.cross_params.append(filters)\n        result = tf.concat(final_result, axis=1)\n        result = tf.reduce_sum(input_tensor=result, axis=-1)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = base_score + tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output)\n        return exFM_out",
        "mutated": [
            "def _build_CIN(self, nn_input, res=False, direct=False, bias=False, is_masked=False):\n    if False:\n        i = 10\n    'Construct the compressed interaction network.\\n        This component provides explicit and vector-wise higher-order feature interactions.\\n\\n        Args:\\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\\n            bias (bool): Whether to add bias term when calculating the feature maps.\\n            is_masked (bool): Controls whether to remove self-interaction in the first layer of CIN.\\n\\n        Returns:\\n            object: Prediction score made by CIN.\\n        '\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    split_tensor0 = tf.split(hidden_nn_layers[0], hparams.dim * [1], 2)\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            split_tensor = tf.split(hidden_nn_layers[-1], hparams.dim * [1], 2)\n            dot_result_m = tf.matmul(split_tensor0, split_tensor, transpose_b=True)\n            dot_result_o = tf.reshape(dot_result_m, shape=[hparams.dim, -1, field_nums[0] * field_nums[-1]])\n            dot_result = tf.transpose(a=dot_result_o, perm=[1, 0, 2])\n            filters = tf.compat.v1.get_variable(name='f_' + str(idx), shape=[1, field_nums[-1] * field_nums[0], layer_size], dtype=tf.float32)\n            if is_masked and idx == 0:\n                ones = tf.ones([field_nums[0], field_nums[0]], dtype=tf.float32)\n                mask_matrix = tf.linalg.band_part(ones, 0, -1) - tf.linalg.tensor_diag(tf.ones(field_nums[0]))\n                mask_matrix = tf.reshape(mask_matrix, shape=[1, field_nums[0] * field_nums[0]])\n                dot_result = tf.multiply(dot_result, mask_matrix) * 2\n                self.dot_result = dot_result\n            curr_out = tf.nn.conv1d(input=dot_result, filters=filters, stride=1, padding='VALID')\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            curr_out = tf.transpose(a=curr_out, perm=[0, 2, 1])\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            else:\n                if idx != len(hparams.cross_layer_sizes) - 1:\n                    (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 1)\n                    final_len += int(layer_size / 2)\n                else:\n                    direct_connect = curr_out\n                    next_hidden = 0\n                    final_len += layer_size\n                field_nums.append(int(layer_size / 2))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n            self.cross_params.append(filters)\n        result = tf.concat(final_result, axis=1)\n        result = tf.reduce_sum(input_tensor=result, axis=-1)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = base_score + tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output)\n        return exFM_out",
            "def _build_CIN(self, nn_input, res=False, direct=False, bias=False, is_masked=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Construct the compressed interaction network.\\n        This component provides explicit and vector-wise higher-order feature interactions.\\n\\n        Args:\\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\\n            bias (bool): Whether to add bias term when calculating the feature maps.\\n            is_masked (bool): Controls whether to remove self-interaction in the first layer of CIN.\\n\\n        Returns:\\n            object: Prediction score made by CIN.\\n        '\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    split_tensor0 = tf.split(hidden_nn_layers[0], hparams.dim * [1], 2)\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            split_tensor = tf.split(hidden_nn_layers[-1], hparams.dim * [1], 2)\n            dot_result_m = tf.matmul(split_tensor0, split_tensor, transpose_b=True)\n            dot_result_o = tf.reshape(dot_result_m, shape=[hparams.dim, -1, field_nums[0] * field_nums[-1]])\n            dot_result = tf.transpose(a=dot_result_o, perm=[1, 0, 2])\n            filters = tf.compat.v1.get_variable(name='f_' + str(idx), shape=[1, field_nums[-1] * field_nums[0], layer_size], dtype=tf.float32)\n            if is_masked and idx == 0:\n                ones = tf.ones([field_nums[0], field_nums[0]], dtype=tf.float32)\n                mask_matrix = tf.linalg.band_part(ones, 0, -1) - tf.linalg.tensor_diag(tf.ones(field_nums[0]))\n                mask_matrix = tf.reshape(mask_matrix, shape=[1, field_nums[0] * field_nums[0]])\n                dot_result = tf.multiply(dot_result, mask_matrix) * 2\n                self.dot_result = dot_result\n            curr_out = tf.nn.conv1d(input=dot_result, filters=filters, stride=1, padding='VALID')\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            curr_out = tf.transpose(a=curr_out, perm=[0, 2, 1])\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            else:\n                if idx != len(hparams.cross_layer_sizes) - 1:\n                    (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 1)\n                    final_len += int(layer_size / 2)\n                else:\n                    direct_connect = curr_out\n                    next_hidden = 0\n                    final_len += layer_size\n                field_nums.append(int(layer_size / 2))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n            self.cross_params.append(filters)\n        result = tf.concat(final_result, axis=1)\n        result = tf.reduce_sum(input_tensor=result, axis=-1)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = base_score + tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output)\n        return exFM_out",
            "def _build_CIN(self, nn_input, res=False, direct=False, bias=False, is_masked=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Construct the compressed interaction network.\\n        This component provides explicit and vector-wise higher-order feature interactions.\\n\\n        Args:\\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\\n            bias (bool): Whether to add bias term when calculating the feature maps.\\n            is_masked (bool): Controls whether to remove self-interaction in the first layer of CIN.\\n\\n        Returns:\\n            object: Prediction score made by CIN.\\n        '\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    split_tensor0 = tf.split(hidden_nn_layers[0], hparams.dim * [1], 2)\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            split_tensor = tf.split(hidden_nn_layers[-1], hparams.dim * [1], 2)\n            dot_result_m = tf.matmul(split_tensor0, split_tensor, transpose_b=True)\n            dot_result_o = tf.reshape(dot_result_m, shape=[hparams.dim, -1, field_nums[0] * field_nums[-1]])\n            dot_result = tf.transpose(a=dot_result_o, perm=[1, 0, 2])\n            filters = tf.compat.v1.get_variable(name='f_' + str(idx), shape=[1, field_nums[-1] * field_nums[0], layer_size], dtype=tf.float32)\n            if is_masked and idx == 0:\n                ones = tf.ones([field_nums[0], field_nums[0]], dtype=tf.float32)\n                mask_matrix = tf.linalg.band_part(ones, 0, -1) - tf.linalg.tensor_diag(tf.ones(field_nums[0]))\n                mask_matrix = tf.reshape(mask_matrix, shape=[1, field_nums[0] * field_nums[0]])\n                dot_result = tf.multiply(dot_result, mask_matrix) * 2\n                self.dot_result = dot_result\n            curr_out = tf.nn.conv1d(input=dot_result, filters=filters, stride=1, padding='VALID')\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            curr_out = tf.transpose(a=curr_out, perm=[0, 2, 1])\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            else:\n                if idx != len(hparams.cross_layer_sizes) - 1:\n                    (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 1)\n                    final_len += int(layer_size / 2)\n                else:\n                    direct_connect = curr_out\n                    next_hidden = 0\n                    final_len += layer_size\n                field_nums.append(int(layer_size / 2))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n            self.cross_params.append(filters)\n        result = tf.concat(final_result, axis=1)\n        result = tf.reduce_sum(input_tensor=result, axis=-1)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = base_score + tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output)\n        return exFM_out",
            "def _build_CIN(self, nn_input, res=False, direct=False, bias=False, is_masked=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Construct the compressed interaction network.\\n        This component provides explicit and vector-wise higher-order feature interactions.\\n\\n        Args:\\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\\n            bias (bool): Whether to add bias term when calculating the feature maps.\\n            is_masked (bool): Controls whether to remove self-interaction in the first layer of CIN.\\n\\n        Returns:\\n            object: Prediction score made by CIN.\\n        '\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    split_tensor0 = tf.split(hidden_nn_layers[0], hparams.dim * [1], 2)\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            split_tensor = tf.split(hidden_nn_layers[-1], hparams.dim * [1], 2)\n            dot_result_m = tf.matmul(split_tensor0, split_tensor, transpose_b=True)\n            dot_result_o = tf.reshape(dot_result_m, shape=[hparams.dim, -1, field_nums[0] * field_nums[-1]])\n            dot_result = tf.transpose(a=dot_result_o, perm=[1, 0, 2])\n            filters = tf.compat.v1.get_variable(name='f_' + str(idx), shape=[1, field_nums[-1] * field_nums[0], layer_size], dtype=tf.float32)\n            if is_masked and idx == 0:\n                ones = tf.ones([field_nums[0], field_nums[0]], dtype=tf.float32)\n                mask_matrix = tf.linalg.band_part(ones, 0, -1) - tf.linalg.tensor_diag(tf.ones(field_nums[0]))\n                mask_matrix = tf.reshape(mask_matrix, shape=[1, field_nums[0] * field_nums[0]])\n                dot_result = tf.multiply(dot_result, mask_matrix) * 2\n                self.dot_result = dot_result\n            curr_out = tf.nn.conv1d(input=dot_result, filters=filters, stride=1, padding='VALID')\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            curr_out = tf.transpose(a=curr_out, perm=[0, 2, 1])\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            else:\n                if idx != len(hparams.cross_layer_sizes) - 1:\n                    (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 1)\n                    final_len += int(layer_size / 2)\n                else:\n                    direct_connect = curr_out\n                    next_hidden = 0\n                    final_len += layer_size\n                field_nums.append(int(layer_size / 2))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n            self.cross_params.append(filters)\n        result = tf.concat(final_result, axis=1)\n        result = tf.reduce_sum(input_tensor=result, axis=-1)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = base_score + tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output)\n        return exFM_out",
            "def _build_CIN(self, nn_input, res=False, direct=False, bias=False, is_masked=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Construct the compressed interaction network.\\n        This component provides explicit and vector-wise higher-order feature interactions.\\n\\n        Args:\\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\\n            bias (bool): Whether to add bias term when calculating the feature maps.\\n            is_masked (bool): Controls whether to remove self-interaction in the first layer of CIN.\\n\\n        Returns:\\n            object: Prediction score made by CIN.\\n        '\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    split_tensor0 = tf.split(hidden_nn_layers[0], hparams.dim * [1], 2)\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            split_tensor = tf.split(hidden_nn_layers[-1], hparams.dim * [1], 2)\n            dot_result_m = tf.matmul(split_tensor0, split_tensor, transpose_b=True)\n            dot_result_o = tf.reshape(dot_result_m, shape=[hparams.dim, -1, field_nums[0] * field_nums[-1]])\n            dot_result = tf.transpose(a=dot_result_o, perm=[1, 0, 2])\n            filters = tf.compat.v1.get_variable(name='f_' + str(idx), shape=[1, field_nums[-1] * field_nums[0], layer_size], dtype=tf.float32)\n            if is_masked and idx == 0:\n                ones = tf.ones([field_nums[0], field_nums[0]], dtype=tf.float32)\n                mask_matrix = tf.linalg.band_part(ones, 0, -1) - tf.linalg.tensor_diag(tf.ones(field_nums[0]))\n                mask_matrix = tf.reshape(mask_matrix, shape=[1, field_nums[0] * field_nums[0]])\n                dot_result = tf.multiply(dot_result, mask_matrix) * 2\n                self.dot_result = dot_result\n            curr_out = tf.nn.conv1d(input=dot_result, filters=filters, stride=1, padding='VALID')\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            curr_out = tf.transpose(a=curr_out, perm=[0, 2, 1])\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            else:\n                if idx != len(hparams.cross_layer_sizes) - 1:\n                    (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 1)\n                    final_len += int(layer_size / 2)\n                else:\n                    direct_connect = curr_out\n                    next_hidden = 0\n                    final_len += layer_size\n                field_nums.append(int(layer_size / 2))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n            self.cross_params.append(filters)\n        result = tf.concat(final_result, axis=1)\n        result = tf.reduce_sum(input_tensor=result, axis=-1)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = base_score + tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output)\n        return exFM_out"
        ]
    },
    {
        "func_name": "_build_fast_CIN",
        "original": "def _build_fast_CIN(self, nn_input, res=False, direct=False, bias=False):\n    \"\"\"Construct the compressed interaction network with reduced parameters.\n        This component provides explicit and vector-wise higher-order feature interactions.\n        Parameters from the filters are reduced via a matrix decomposition method.\n        Fast CIN is more space and time efficient than CIN.\n\n        Args:\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\n            bias (bool): Whether to add bias term when calculating the feature maps.\n\n        Returns:\n            object: Prediction score made by fast CIN.\n        \"\"\"\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    fast_CIN_d = hparams.fast_CIN_d\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    nn_input = tf.transpose(a=nn_input, perm=[0, 2, 1])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            if idx == 0:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=tf.pow(nn_input, 2), filters=tf.pow(fast_w, 2), stride=1, padding='VALID')\n                dot_result = tf.reshape(0.5 * (dot_result_1 - dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            else:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                fast_v = tf.compat.v1.get_variable('fast_CIN_v_' + str(idx), shape=[1, field_nums[-1], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                self.cross_params.append(fast_v)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=hidden_nn_layers[-1], filters=fast_v, stride=1, padding='VALID')\n                dot_result = tf.reshape(tf.multiply(dot_result_1, dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[1, 1, layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            elif idx != len(hparams.cross_layer_sizes) - 1:\n                (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 2)\n                final_len += int(layer_size / 2)\n                field_nums.append(int(layer_size / 2))\n            else:\n                direct_connect = curr_out\n                next_hidden = 0\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n        result = tf.concat(final_result, axis=2)\n        result = tf.reduce_sum(input_tensor=result, axis=1, keepdims=False)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output) + base_score\n    return exFM_out",
        "mutated": [
            "def _build_fast_CIN(self, nn_input, res=False, direct=False, bias=False):\n    if False:\n        i = 10\n    'Construct the compressed interaction network with reduced parameters.\\n        This component provides explicit and vector-wise higher-order feature interactions.\\n        Parameters from the filters are reduced via a matrix decomposition method.\\n        Fast CIN is more space and time efficient than CIN.\\n\\n        Args:\\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\\n            bias (bool): Whether to add bias term when calculating the feature maps.\\n\\n        Returns:\\n            object: Prediction score made by fast CIN.\\n        '\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    fast_CIN_d = hparams.fast_CIN_d\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    nn_input = tf.transpose(a=nn_input, perm=[0, 2, 1])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            if idx == 0:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=tf.pow(nn_input, 2), filters=tf.pow(fast_w, 2), stride=1, padding='VALID')\n                dot_result = tf.reshape(0.5 * (dot_result_1 - dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            else:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                fast_v = tf.compat.v1.get_variable('fast_CIN_v_' + str(idx), shape=[1, field_nums[-1], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                self.cross_params.append(fast_v)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=hidden_nn_layers[-1], filters=fast_v, stride=1, padding='VALID')\n                dot_result = tf.reshape(tf.multiply(dot_result_1, dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[1, 1, layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            elif idx != len(hparams.cross_layer_sizes) - 1:\n                (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 2)\n                final_len += int(layer_size / 2)\n                field_nums.append(int(layer_size / 2))\n            else:\n                direct_connect = curr_out\n                next_hidden = 0\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n        result = tf.concat(final_result, axis=2)\n        result = tf.reduce_sum(input_tensor=result, axis=1, keepdims=False)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output) + base_score\n    return exFM_out",
            "def _build_fast_CIN(self, nn_input, res=False, direct=False, bias=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Construct the compressed interaction network with reduced parameters.\\n        This component provides explicit and vector-wise higher-order feature interactions.\\n        Parameters from the filters are reduced via a matrix decomposition method.\\n        Fast CIN is more space and time efficient than CIN.\\n\\n        Args:\\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\\n            bias (bool): Whether to add bias term when calculating the feature maps.\\n\\n        Returns:\\n            object: Prediction score made by fast CIN.\\n        '\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    fast_CIN_d = hparams.fast_CIN_d\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    nn_input = tf.transpose(a=nn_input, perm=[0, 2, 1])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            if idx == 0:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=tf.pow(nn_input, 2), filters=tf.pow(fast_w, 2), stride=1, padding='VALID')\n                dot_result = tf.reshape(0.5 * (dot_result_1 - dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            else:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                fast_v = tf.compat.v1.get_variable('fast_CIN_v_' + str(idx), shape=[1, field_nums[-1], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                self.cross_params.append(fast_v)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=hidden_nn_layers[-1], filters=fast_v, stride=1, padding='VALID')\n                dot_result = tf.reshape(tf.multiply(dot_result_1, dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[1, 1, layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            elif idx != len(hparams.cross_layer_sizes) - 1:\n                (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 2)\n                final_len += int(layer_size / 2)\n                field_nums.append(int(layer_size / 2))\n            else:\n                direct_connect = curr_out\n                next_hidden = 0\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n        result = tf.concat(final_result, axis=2)\n        result = tf.reduce_sum(input_tensor=result, axis=1, keepdims=False)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output) + base_score\n    return exFM_out",
            "def _build_fast_CIN(self, nn_input, res=False, direct=False, bias=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Construct the compressed interaction network with reduced parameters.\\n        This component provides explicit and vector-wise higher-order feature interactions.\\n        Parameters from the filters are reduced via a matrix decomposition method.\\n        Fast CIN is more space and time efficient than CIN.\\n\\n        Args:\\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\\n            bias (bool): Whether to add bias term when calculating the feature maps.\\n\\n        Returns:\\n            object: Prediction score made by fast CIN.\\n        '\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    fast_CIN_d = hparams.fast_CIN_d\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    nn_input = tf.transpose(a=nn_input, perm=[0, 2, 1])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            if idx == 0:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=tf.pow(nn_input, 2), filters=tf.pow(fast_w, 2), stride=1, padding='VALID')\n                dot_result = tf.reshape(0.5 * (dot_result_1 - dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            else:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                fast_v = tf.compat.v1.get_variable('fast_CIN_v_' + str(idx), shape=[1, field_nums[-1], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                self.cross_params.append(fast_v)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=hidden_nn_layers[-1], filters=fast_v, stride=1, padding='VALID')\n                dot_result = tf.reshape(tf.multiply(dot_result_1, dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[1, 1, layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            elif idx != len(hparams.cross_layer_sizes) - 1:\n                (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 2)\n                final_len += int(layer_size / 2)\n                field_nums.append(int(layer_size / 2))\n            else:\n                direct_connect = curr_out\n                next_hidden = 0\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n        result = tf.concat(final_result, axis=2)\n        result = tf.reduce_sum(input_tensor=result, axis=1, keepdims=False)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output) + base_score\n    return exFM_out",
            "def _build_fast_CIN(self, nn_input, res=False, direct=False, bias=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Construct the compressed interaction network with reduced parameters.\\n        This component provides explicit and vector-wise higher-order feature interactions.\\n        Parameters from the filters are reduced via a matrix decomposition method.\\n        Fast CIN is more space and time efficient than CIN.\\n\\n        Args:\\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\\n            bias (bool): Whether to add bias term when calculating the feature maps.\\n\\n        Returns:\\n            object: Prediction score made by fast CIN.\\n        '\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    fast_CIN_d = hparams.fast_CIN_d\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    nn_input = tf.transpose(a=nn_input, perm=[0, 2, 1])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            if idx == 0:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=tf.pow(nn_input, 2), filters=tf.pow(fast_w, 2), stride=1, padding='VALID')\n                dot_result = tf.reshape(0.5 * (dot_result_1 - dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            else:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                fast_v = tf.compat.v1.get_variable('fast_CIN_v_' + str(idx), shape=[1, field_nums[-1], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                self.cross_params.append(fast_v)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=hidden_nn_layers[-1], filters=fast_v, stride=1, padding='VALID')\n                dot_result = tf.reshape(tf.multiply(dot_result_1, dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[1, 1, layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            elif idx != len(hparams.cross_layer_sizes) - 1:\n                (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 2)\n                final_len += int(layer_size / 2)\n                field_nums.append(int(layer_size / 2))\n            else:\n                direct_connect = curr_out\n                next_hidden = 0\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n        result = tf.concat(final_result, axis=2)\n        result = tf.reduce_sum(input_tensor=result, axis=1, keepdims=False)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output) + base_score\n    return exFM_out",
            "def _build_fast_CIN(self, nn_input, res=False, direct=False, bias=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Construct the compressed interaction network with reduced parameters.\\n        This component provides explicit and vector-wise higher-order feature interactions.\\n        Parameters from the filters are reduced via a matrix decomposition method.\\n        Fast CIN is more space and time efficient than CIN.\\n\\n        Args:\\n            nn_input (object): The output of field-embedding layer. This is the input for CIN.\\n            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\\n            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\\n                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\\n            bias (bool): Whether to add bias term when calculating the feature maps.\\n\\n        Returns:\\n            object: Prediction score made by fast CIN.\\n        '\n    hparams = self.hparams\n    hidden_nn_layers = []\n    field_nums = []\n    final_len = 0\n    field_num = hparams.FIELD_COUNT\n    fast_CIN_d = hparams.fast_CIN_d\n    nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n    nn_input = tf.transpose(a=nn_input, perm=[0, 2, 1])\n    field_nums.append(int(field_num))\n    hidden_nn_layers.append(nn_input)\n    final_result = []\n    with tf.compat.v1.variable_scope('exfm_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.cross_layer_sizes):\n            if idx == 0:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=tf.pow(nn_input, 2), filters=tf.pow(fast_w, 2), stride=1, padding='VALID')\n                dot_result = tf.reshape(0.5 * (dot_result_1 - dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            else:\n                fast_w = tf.compat.v1.get_variable('fast_CIN_w_' + str(idx), shape=[1, field_nums[0], fast_CIN_d * layer_size], dtype=tf.float32)\n                fast_v = tf.compat.v1.get_variable('fast_CIN_v_' + str(idx), shape=[1, field_nums[-1], fast_CIN_d * layer_size], dtype=tf.float32)\n                self.cross_params.append(fast_w)\n                self.cross_params.append(fast_v)\n                dot_result_1 = tf.nn.conv1d(input=nn_input, filters=fast_w, stride=1, padding='VALID')\n                dot_result_2 = tf.nn.conv1d(input=hidden_nn_layers[-1], filters=fast_v, stride=1, padding='VALID')\n                dot_result = tf.reshape(tf.multiply(dot_result_1, dot_result_2), shape=[-1, hparams.dim, layer_size, fast_CIN_d])\n                curr_out = tf.reduce_sum(input_tensor=dot_result, axis=3, keepdims=False)\n            if bias:\n                b = tf.compat.v1.get_variable(name='f_b' + str(idx), shape=[1, 1, layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n                curr_out = tf.nn.bias_add(curr_out, b)\n                self.cross_params.append(b)\n            if hparams.enable_BN is True:\n                curr_out = tf.compat.v1.layers.batch_normalization(curr_out, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_out = self._activate(curr_out, hparams.cross_activation)\n            if direct:\n                direct_connect = curr_out\n                next_hidden = curr_out\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            elif idx != len(hparams.cross_layer_sizes) - 1:\n                (next_hidden, direct_connect) = tf.split(curr_out, 2 * [int(layer_size / 2)], 2)\n                final_len += int(layer_size / 2)\n                field_nums.append(int(layer_size / 2))\n            else:\n                direct_connect = curr_out\n                next_hidden = 0\n                final_len += layer_size\n                field_nums.append(int(layer_size))\n            final_result.append(direct_connect)\n            hidden_nn_layers.append(next_hidden)\n        result = tf.concat(final_result, axis=2)\n        result = tf.reduce_sum(input_tensor=result, axis=1, keepdims=False)\n        if res:\n            base_score = tf.reduce_sum(input_tensor=result, axis=1, keepdims=True)\n        else:\n            base_score = 0\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[final_len, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        exFM_out = tf.compat.v1.nn.xw_plus_b(result, w_nn_output, b_nn_output) + base_score\n    return exFM_out"
        ]
    },
    {
        "func_name": "_build_dnn",
        "original": "def _build_dnn(self, embed_out, embed_layer_size):\n    \"\"\"Construct the MLP part for the model.\n        This components provides implicit higher-order feature interactions.\n\n        Args:\n            embed_out (object): The output of field-embedding layer. This is the input for DNN.\n            embed_layer_size (object): Shape of the embed_out\n\n        Returns:\n            object: Prediction score made by fast CIN.\n        \"\"\"\n    hparams = self.hparams\n    w_fm_nn_input = embed_out\n    last_layer_size = embed_layer_size\n    layer_idx = 0\n    hidden_nn_layers = []\n    hidden_nn_layers.append(w_fm_nn_input)\n    with tf.compat.v1.variable_scope('nn_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.layer_sizes):\n            curr_w_nn_layer = tf.compat.v1.get_variable(name='w_nn_layer' + str(layer_idx), shape=[last_layer_size, layer_size], dtype=tf.float32)\n            curr_b_nn_layer = tf.compat.v1.get_variable(name='b_nn_layer' + str(layer_idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n            tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_layer' + str(layer_idx), curr_w_nn_layer)\n            tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_layer' + str(layer_idx), curr_b_nn_layer)\n            curr_hidden_nn_layer = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[layer_idx], curr_w_nn_layer, curr_b_nn_layer)\n            scope = 'nn_part' + str(idx)\n            activation = hparams.activation[idx]\n            if hparams.enable_BN is True:\n                curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(curr_hidden_nn_layer, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_hidden_nn_layer = self._active_layer(logit=curr_hidden_nn_layer, activation=activation, layer_idx=idx)\n            hidden_nn_layers.append(curr_hidden_nn_layer)\n            layer_idx += 1\n            last_layer_size = layer_size\n            self.layer_params.append(curr_w_nn_layer)\n            self.layer_params.append(curr_b_nn_layer)\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[last_layer_size, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_output' + str(layer_idx), w_nn_output)\n        tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_output' + str(layer_idx), b_nn_output)\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        nn_output = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[-1], w_nn_output, b_nn_output)\n        return nn_output",
        "mutated": [
            "def _build_dnn(self, embed_out, embed_layer_size):\n    if False:\n        i = 10\n    'Construct the MLP part for the model.\\n        This components provides implicit higher-order feature interactions.\\n\\n        Args:\\n            embed_out (object): The output of field-embedding layer. This is the input for DNN.\\n            embed_layer_size (object): Shape of the embed_out\\n\\n        Returns:\\n            object: Prediction score made by fast CIN.\\n        '\n    hparams = self.hparams\n    w_fm_nn_input = embed_out\n    last_layer_size = embed_layer_size\n    layer_idx = 0\n    hidden_nn_layers = []\n    hidden_nn_layers.append(w_fm_nn_input)\n    with tf.compat.v1.variable_scope('nn_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.layer_sizes):\n            curr_w_nn_layer = tf.compat.v1.get_variable(name='w_nn_layer' + str(layer_idx), shape=[last_layer_size, layer_size], dtype=tf.float32)\n            curr_b_nn_layer = tf.compat.v1.get_variable(name='b_nn_layer' + str(layer_idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n            tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_layer' + str(layer_idx), curr_w_nn_layer)\n            tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_layer' + str(layer_idx), curr_b_nn_layer)\n            curr_hidden_nn_layer = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[layer_idx], curr_w_nn_layer, curr_b_nn_layer)\n            scope = 'nn_part' + str(idx)\n            activation = hparams.activation[idx]\n            if hparams.enable_BN is True:\n                curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(curr_hidden_nn_layer, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_hidden_nn_layer = self._active_layer(logit=curr_hidden_nn_layer, activation=activation, layer_idx=idx)\n            hidden_nn_layers.append(curr_hidden_nn_layer)\n            layer_idx += 1\n            last_layer_size = layer_size\n            self.layer_params.append(curr_w_nn_layer)\n            self.layer_params.append(curr_b_nn_layer)\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[last_layer_size, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_output' + str(layer_idx), w_nn_output)\n        tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_output' + str(layer_idx), b_nn_output)\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        nn_output = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[-1], w_nn_output, b_nn_output)\n        return nn_output",
            "def _build_dnn(self, embed_out, embed_layer_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Construct the MLP part for the model.\\n        This components provides implicit higher-order feature interactions.\\n\\n        Args:\\n            embed_out (object): The output of field-embedding layer. This is the input for DNN.\\n            embed_layer_size (object): Shape of the embed_out\\n\\n        Returns:\\n            object: Prediction score made by fast CIN.\\n        '\n    hparams = self.hparams\n    w_fm_nn_input = embed_out\n    last_layer_size = embed_layer_size\n    layer_idx = 0\n    hidden_nn_layers = []\n    hidden_nn_layers.append(w_fm_nn_input)\n    with tf.compat.v1.variable_scope('nn_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.layer_sizes):\n            curr_w_nn_layer = tf.compat.v1.get_variable(name='w_nn_layer' + str(layer_idx), shape=[last_layer_size, layer_size], dtype=tf.float32)\n            curr_b_nn_layer = tf.compat.v1.get_variable(name='b_nn_layer' + str(layer_idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n            tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_layer' + str(layer_idx), curr_w_nn_layer)\n            tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_layer' + str(layer_idx), curr_b_nn_layer)\n            curr_hidden_nn_layer = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[layer_idx], curr_w_nn_layer, curr_b_nn_layer)\n            scope = 'nn_part' + str(idx)\n            activation = hparams.activation[idx]\n            if hparams.enable_BN is True:\n                curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(curr_hidden_nn_layer, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_hidden_nn_layer = self._active_layer(logit=curr_hidden_nn_layer, activation=activation, layer_idx=idx)\n            hidden_nn_layers.append(curr_hidden_nn_layer)\n            layer_idx += 1\n            last_layer_size = layer_size\n            self.layer_params.append(curr_w_nn_layer)\n            self.layer_params.append(curr_b_nn_layer)\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[last_layer_size, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_output' + str(layer_idx), w_nn_output)\n        tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_output' + str(layer_idx), b_nn_output)\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        nn_output = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[-1], w_nn_output, b_nn_output)\n        return nn_output",
            "def _build_dnn(self, embed_out, embed_layer_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Construct the MLP part for the model.\\n        This components provides implicit higher-order feature interactions.\\n\\n        Args:\\n            embed_out (object): The output of field-embedding layer. This is the input for DNN.\\n            embed_layer_size (object): Shape of the embed_out\\n\\n        Returns:\\n            object: Prediction score made by fast CIN.\\n        '\n    hparams = self.hparams\n    w_fm_nn_input = embed_out\n    last_layer_size = embed_layer_size\n    layer_idx = 0\n    hidden_nn_layers = []\n    hidden_nn_layers.append(w_fm_nn_input)\n    with tf.compat.v1.variable_scope('nn_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.layer_sizes):\n            curr_w_nn_layer = tf.compat.v1.get_variable(name='w_nn_layer' + str(layer_idx), shape=[last_layer_size, layer_size], dtype=tf.float32)\n            curr_b_nn_layer = tf.compat.v1.get_variable(name='b_nn_layer' + str(layer_idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n            tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_layer' + str(layer_idx), curr_w_nn_layer)\n            tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_layer' + str(layer_idx), curr_b_nn_layer)\n            curr_hidden_nn_layer = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[layer_idx], curr_w_nn_layer, curr_b_nn_layer)\n            scope = 'nn_part' + str(idx)\n            activation = hparams.activation[idx]\n            if hparams.enable_BN is True:\n                curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(curr_hidden_nn_layer, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_hidden_nn_layer = self._active_layer(logit=curr_hidden_nn_layer, activation=activation, layer_idx=idx)\n            hidden_nn_layers.append(curr_hidden_nn_layer)\n            layer_idx += 1\n            last_layer_size = layer_size\n            self.layer_params.append(curr_w_nn_layer)\n            self.layer_params.append(curr_b_nn_layer)\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[last_layer_size, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_output' + str(layer_idx), w_nn_output)\n        tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_output' + str(layer_idx), b_nn_output)\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        nn_output = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[-1], w_nn_output, b_nn_output)\n        return nn_output",
            "def _build_dnn(self, embed_out, embed_layer_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Construct the MLP part for the model.\\n        This components provides implicit higher-order feature interactions.\\n\\n        Args:\\n            embed_out (object): The output of field-embedding layer. This is the input for DNN.\\n            embed_layer_size (object): Shape of the embed_out\\n\\n        Returns:\\n            object: Prediction score made by fast CIN.\\n        '\n    hparams = self.hparams\n    w_fm_nn_input = embed_out\n    last_layer_size = embed_layer_size\n    layer_idx = 0\n    hidden_nn_layers = []\n    hidden_nn_layers.append(w_fm_nn_input)\n    with tf.compat.v1.variable_scope('nn_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.layer_sizes):\n            curr_w_nn_layer = tf.compat.v1.get_variable(name='w_nn_layer' + str(layer_idx), shape=[last_layer_size, layer_size], dtype=tf.float32)\n            curr_b_nn_layer = tf.compat.v1.get_variable(name='b_nn_layer' + str(layer_idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n            tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_layer' + str(layer_idx), curr_w_nn_layer)\n            tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_layer' + str(layer_idx), curr_b_nn_layer)\n            curr_hidden_nn_layer = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[layer_idx], curr_w_nn_layer, curr_b_nn_layer)\n            scope = 'nn_part' + str(idx)\n            activation = hparams.activation[idx]\n            if hparams.enable_BN is True:\n                curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(curr_hidden_nn_layer, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_hidden_nn_layer = self._active_layer(logit=curr_hidden_nn_layer, activation=activation, layer_idx=idx)\n            hidden_nn_layers.append(curr_hidden_nn_layer)\n            layer_idx += 1\n            last_layer_size = layer_size\n            self.layer_params.append(curr_w_nn_layer)\n            self.layer_params.append(curr_b_nn_layer)\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[last_layer_size, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_output' + str(layer_idx), w_nn_output)\n        tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_output' + str(layer_idx), b_nn_output)\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        nn_output = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[-1], w_nn_output, b_nn_output)\n        return nn_output",
            "def _build_dnn(self, embed_out, embed_layer_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Construct the MLP part for the model.\\n        This components provides implicit higher-order feature interactions.\\n\\n        Args:\\n            embed_out (object): The output of field-embedding layer. This is the input for DNN.\\n            embed_layer_size (object): Shape of the embed_out\\n\\n        Returns:\\n            object: Prediction score made by fast CIN.\\n        '\n    hparams = self.hparams\n    w_fm_nn_input = embed_out\n    last_layer_size = embed_layer_size\n    layer_idx = 0\n    hidden_nn_layers = []\n    hidden_nn_layers.append(w_fm_nn_input)\n    with tf.compat.v1.variable_scope('nn_part', initializer=self.initializer) as scope:\n        for (idx, layer_size) in enumerate(hparams.layer_sizes):\n            curr_w_nn_layer = tf.compat.v1.get_variable(name='w_nn_layer' + str(layer_idx), shape=[last_layer_size, layer_size], dtype=tf.float32)\n            curr_b_nn_layer = tf.compat.v1.get_variable(name='b_nn_layer' + str(layer_idx), shape=[layer_size], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n            tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_layer' + str(layer_idx), curr_w_nn_layer)\n            tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_layer' + str(layer_idx), curr_b_nn_layer)\n            curr_hidden_nn_layer = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[layer_idx], curr_w_nn_layer, curr_b_nn_layer)\n            scope = 'nn_part' + str(idx)\n            activation = hparams.activation[idx]\n            if hparams.enable_BN is True:\n                curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(curr_hidden_nn_layer, momentum=0.95, epsilon=0.0001, training=self.is_train_stage)\n            curr_hidden_nn_layer = self._active_layer(logit=curr_hidden_nn_layer, activation=activation, layer_idx=idx)\n            hidden_nn_layers.append(curr_hidden_nn_layer)\n            layer_idx += 1\n            last_layer_size = layer_size\n            self.layer_params.append(curr_w_nn_layer)\n            self.layer_params.append(curr_b_nn_layer)\n        w_nn_output = tf.compat.v1.get_variable(name='w_nn_output', shape=[last_layer_size, 1], dtype=tf.float32)\n        b_nn_output = tf.compat.v1.get_variable(name='b_nn_output', shape=[1], dtype=tf.float32, initializer=tf.compat.v1.zeros_initializer())\n        tf.compat.v1.summary.histogram('nn_part/' + 'w_nn_output' + str(layer_idx), w_nn_output)\n        tf.compat.v1.summary.histogram('nn_part/' + 'b_nn_output' + str(layer_idx), b_nn_output)\n        self.layer_params.append(w_nn_output)\n        self.layer_params.append(b_nn_output)\n        nn_output = tf.compat.v1.nn.xw_plus_b(hidden_nn_layers[-1], w_nn_output, b_nn_output)\n        return nn_output"
        ]
    }
]