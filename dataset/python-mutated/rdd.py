import copy
import sys
import os
import operator
import shlex
import warnings
import heapq
import bisect
import random
from subprocess import Popen, PIPE
from threading import Thread
from collections import defaultdict
from itertools import chain
from functools import reduce
from math import sqrt, log, isinf, isnan, pow, ceil
from typing import Any, Callable, Dict, Generic, Hashable, Iterable, Iterator, IO, List, NoReturn, Optional, Sequence, Tuple, Union, TypeVar, cast, overload, TYPE_CHECKING
from pyspark.java_gateway import local_connect_and_auth
from pyspark.serializers import AutoBatchedSerializer, BatchedSerializer, NoOpSerializer, CartesianDeserializer, CloudPickleSerializer, PairDeserializer, CPickleSerializer, Serializer, pack_long, read_int, write_int
from pyspark.join import python_join, python_left_outer_join, python_right_outer_join, python_full_outer_join, python_cogroup
from pyspark.statcounter import StatCounter
from pyspark.rddsampler import RDDSampler, RDDRangeSampler, RDDStratifiedSampler
from pyspark.storagelevel import StorageLevel
from pyspark.resource.requests import ExecutorResourceRequests, TaskResourceRequests
from pyspark.resource.profile import ResourceProfile
from pyspark.resultiterable import ResultIterable
from pyspark.shuffle import Aggregator, ExternalMerger, get_used_memory, ExternalSorter, ExternalGroupBy
from pyspark.traceback_utils import SCCallSiteSync
from pyspark.util import fail_on_stopiteration, _parse_memory
from pyspark.errors import PySparkRuntimeError
if TYPE_CHECKING:
    import socket
    import io
    from py4j.java_gateway import JavaObject
    from py4j.java_collections import JavaArray
    from pyspark._typing import NonUDFType
    from pyspark._typing import S, NumberOrArray
    from pyspark.context import SparkContext
    from pyspark.sql.pandas._typing import PandasScalarUDFType, PandasGroupedMapUDFType, PandasGroupedAggUDFType, PandasWindowAggUDFType, PandasScalarIterUDFType, PandasMapIterUDFType, PandasCogroupedMapUDFType, ArrowMapIterUDFType, PandasGroupedMapUDFWithStateType
    from pyspark.sql.dataframe import DataFrame
    from pyspark.sql.types import AtomicType, StructType
    from pyspark.sql._typing import AtomicValue, RowLike, SQLArrowBatchedUDFType, SQLArrowTableUDFType, SQLBatchedUDFType, SQLTableUDFType
T = TypeVar('T')
T_co = TypeVar('T_co', covariant=True)
U = TypeVar('U')
K = TypeVar('K', bound=Hashable)
V = TypeVar('V')
V1 = TypeVar('V1')
V2 = TypeVar('V2')
V3 = TypeVar('V3')
__all__ = ['RDD']

class PythonEvalType:
    """
    Evaluation type of python rdd.

    These values are internal to PySpark.

    These values should match values in org.apache.spark.api.python.PythonEvalType.
    """
    NON_UDF: 'NonUDFType' = 0
    SQL_BATCHED_UDF: 'SQLBatchedUDFType' = 100
    SQL_ARROW_BATCHED_UDF: 'SQLArrowBatchedUDFType' = 101
    SQL_SCALAR_PANDAS_UDF: 'PandasScalarUDFType' = 200
    SQL_GROUPED_MAP_PANDAS_UDF: 'PandasGroupedMapUDFType' = 201
    SQL_GROUPED_AGG_PANDAS_UDF: 'PandasGroupedAggUDFType' = 202
    SQL_WINDOW_AGG_PANDAS_UDF: 'PandasWindowAggUDFType' = 203
    SQL_SCALAR_PANDAS_ITER_UDF: 'PandasScalarIterUDFType' = 204
    SQL_MAP_PANDAS_ITER_UDF: 'PandasMapIterUDFType' = 205
    SQL_COGROUPED_MAP_PANDAS_UDF: 'PandasCogroupedMapUDFType' = 206
    SQL_MAP_ARROW_ITER_UDF: 'ArrowMapIterUDFType' = 207
    SQL_GROUPED_MAP_PANDAS_UDF_WITH_STATE: 'PandasGroupedMapUDFWithStateType' = 208
    SQL_TABLE_UDF: 'SQLTableUDFType' = 300
    SQL_ARROW_TABLE_UDF: 'SQLArrowTableUDFType' = 301

def portable_hash(x: Hashable) -> int:
    if False:
        return 10
    '\n    This function returns consistent hash code for builtin types, especially\n    for None and tuple with None.\n\n    The algorithm is similar to that one used by CPython 2.7\n\n    Examples\n    --------\n    >>> portable_hash(None)\n    0\n    >>> portable_hash((None, 1)) & 0xffffffff\n    219750521\n    '
    if 'PYTHONHASHSEED' not in os.environ:
        raise PySparkRuntimeError(error_class='PYTHON_HASH_SEED_NOT_SET', message_parameters={})
    if x is None:
        return 0
    if isinstance(x, tuple):
        h = 3430008
        for i in x:
            h ^= portable_hash(i)
            h *= 1000003
            h &= sys.maxsize
        h ^= len(x)
        if h == -1:
            h = -2
        return int(h)
    return hash(x)

class BoundedFloat(float):
    """
    Bounded value is generated by approximate job, with confidence and low
    bound and high bound.

    Examples
    --------
    >>> BoundedFloat(100.0, 0.95, 95.0, 105.0)
    100.0
    """
    confidence: float
    low: float
    high: float

    def __new__(cls, mean: float, confidence: float, low: float, high: float) -> 'BoundedFloat':
        if False:
            print('Hello World!')
        obj = float.__new__(cls, mean)
        obj.confidence = confidence
        obj.low = low
        obj.high = high
        return obj

def _create_local_socket(sock_info: 'JavaArray') -> 'io.BufferedRWPair':
    if False:
        for i in range(10):
            print('nop')
    '\n    Create a local socket that can be used to load deserialized data from the JVM\n\n    Parameters\n    ----------\n    sock_info : tuple\n        Tuple containing port number and authentication secret for a local socket.\n\n    Returns\n    -------\n    sockfile file descriptor of the local socket\n    '
    sockfile: 'io.BufferedRWPair'
    sock: 'socket.socket'
    port: int = sock_info[0]
    auth_secret: str = sock_info[1]
    (sockfile, sock) = local_connect_and_auth(port, auth_secret)
    sock.settimeout(None)
    return sockfile

def _load_from_socket(sock_info: 'JavaArray', serializer: Serializer) -> Iterator[Any]:
    if False:
        for i in range(10):
            print('nop')
    '\n    Connect to a local socket described by sock_info and use the given serializer to yield data\n\n    Parameters\n    ----------\n    sock_info : tuple\n        Tuple containing port number and authentication secret for a local socket.\n    serializer : class:`Serializer`\n        The PySpark serializer to use\n\n    Returns\n    -------\n    result of meth:`Serializer.load_stream`,\n    usually a generator that yields deserialized data\n    '
    sockfile = _create_local_socket(sock_info)
    return serializer.load_stream(sockfile)

def _local_iterator_from_socket(sock_info: 'JavaArray', serializer: Serializer) -> Iterator[Any]:
    if False:
        print('Hello World!')

    class PyLocalIterable:
        """Create a synchronous local iterable over a socket"""

        def __init__(self, _sock_info: 'JavaArray', _serializer: Serializer):
            if False:
                for i in range(10):
                    print('nop')
            port: int
            auth_secret: str
            jsocket_auth_server: 'JavaObject'
            (port, auth_secret, self.jsocket_auth_server) = _sock_info
            self._sockfile = _create_local_socket((port, auth_secret))
            self._serializer = _serializer
            self._read_iter: Iterator[Any] = iter([])
            self._read_status = 1

        def __iter__(self) -> Iterator[Any]:
            if False:
                for i in range(10):
                    print('nop')
            while self._read_status == 1:
                write_int(1, self._sockfile)
                self._sockfile.flush()
                self._read_status = read_int(self._sockfile)
                if self._read_status == 1:
                    self._read_iter = self._serializer.load_stream(self._sockfile)
                    for item in self._read_iter:
                        yield item
                elif self._read_status == -1:
                    self.jsocket_auth_server.getResult()

        def __del__(self) -> None:
            if False:
                while True:
                    i = 10
            if self._read_status == 1:
                try:
                    for _ in self._read_iter:
                        pass
                    write_int(0, self._sockfile)
                    self._sockfile.flush()
                except Exception:
                    pass
    return iter(PyLocalIterable(sock_info, serializer))

class Partitioner:

    def __init__(self, numPartitions: int, partitionFunc: Callable[[Any], int]):
        if False:
            print('Hello World!')
        self.numPartitions = numPartitions
        self.partitionFunc = partitionFunc

    def __eq__(self, other: Any) -> bool:
        if False:
            i = 10
            return i + 15
        return isinstance(other, Partitioner) and self.numPartitions == other.numPartitions and (self.partitionFunc == other.partitionFunc)

    def __call__(self, k: Any) -> int:
        if False:
            for i in range(10):
                print('nop')
        return self.partitionFunc(k) % self.numPartitions

class RDD(Generic[T_co]):
    """
    A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.
    Represents an immutable, partitioned collection of elements that can be
    operated on in parallel.
    """

    def __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: Serializer=AutoBatchedSerializer(CPickleSerializer())):
        if False:
            i = 10
            return i + 15
        self._jrdd = jrdd
        self.is_cached = False
        self.is_checkpointed = False
        self.has_resource_profile = False
        self.ctx = ctx
        self._jrdd_deserializer = jrdd_deserializer
        self._id = jrdd.id()
        self.partitioner: Optional[Partitioner] = None

    def _pickled(self: 'RDD[T]') -> 'RDD[T]':
        if False:
            i = 10
            return i + 15
        return self._reserialize(AutoBatchedSerializer(CPickleSerializer()))

    def id(self) -> int:
        if False:
            print('Hello World!')
        '\n        A unique ID for this RDD (within its SparkContext).\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        int\n            The unique ID for this :class:`RDD`\n\n        Examples\n        --------\n        >>> rdd = sc.range(5)\n        >>> rdd.id()  # doctest: +SKIP\n        3\n        '
        return self._id

    def __repr__(self) -> str:
        if False:
            i = 10
            return i + 15
        return self._jrdd.toString()

    def __getnewargs__(self) -> NoReturn:
        if False:
            print('Hello World!')
        raise PySparkRuntimeError(error_class='RDD_TRANSFORM_ONLY_VALID_ON_DRIVER', message_parameters={})

    @property
    def context(self) -> 'SparkContext':
        if False:
            return 10
        '\n        The :class:`SparkContext` that this RDD was created on.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        :class:`SparkContext`\n            The :class:`SparkContext` that this RDD was created on\n\n        Examples\n        --------\n        >>> rdd = sc.range(5)\n        >>> rdd.context\n        <SparkContext ...>\n        >>> rdd.context is sc\n        True\n        '
        return self.ctx

    def cache(self: 'RDD[T]') -> 'RDD[T]':
        if False:
            while True:
                i = 10
        "\n        Persist this RDD with the default storage level (`MEMORY_ONLY`).\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        :class:`RDD`\n            The same :class:`RDD` with storage level set to `MEMORY_ONLY`\n\n        See Also\n        --------\n        :meth:`RDD.persist`\n        :meth:`RDD.unpersist`\n        :meth:`RDD.getStorageLevel`\n\n        Examples\n        --------\n        >>> rdd = sc.range(5)\n        >>> rdd2 = rdd.cache()\n        >>> rdd2 is rdd\n        True\n        >>> str(rdd.getStorageLevel())\n        'Memory Serialized 1x Replicated'\n        >>> _ = rdd.unpersist()\n        "
        self.is_cached = True
        self.persist(StorageLevel.MEMORY_ONLY)
        return self

    def persist(self: 'RDD[T]', storageLevel: StorageLevel=StorageLevel.MEMORY_ONLY) -> 'RDD[T]':
        if False:
            i = 10
            return i + 15
        '\n        Set this RDD\'s storage level to persist its values across operations\n        after the first time it is computed. This can only be used to assign\n        a new storage level if the RDD does not have a storage level set yet.\n        If no storage level is specified defaults to (`MEMORY_ONLY`).\n\n        .. versionadded:: 0.9.1\n\n        Parameters\n        ----------\n        storageLevel : :class:`StorageLevel`, default `MEMORY_ONLY`\n            the target storage level\n\n        Returns\n        -------\n        :class:`RDD`\n            The same :class:`RDD` with storage level set to `storageLevel`.\n\n        See Also\n        --------\n        :meth:`RDD.cache`\n        :meth:`RDD.unpersist`\n        :meth:`RDD.getStorageLevel`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize(["b", "a", "c"])\n        >>> rdd.persist().is_cached\n        True\n        >>> str(rdd.getStorageLevel())\n        \'Memory Serialized 1x Replicated\'\n        >>> _ = rdd.unpersist()\n        >>> rdd.is_cached\n        False\n\n        >>> from pyspark import StorageLevel\n        >>> rdd2 = sc.range(5)\n        >>> _ = rdd2.persist(StorageLevel.MEMORY_AND_DISK)\n        >>> rdd2.is_cached\n        True\n        >>> str(rdd2.getStorageLevel())\n        \'Disk Memory Serialized 1x Replicated\'\n\n        Can not override existing storage level\n\n        >>> _ = rdd2.persist(StorageLevel.MEMORY_ONLY_2)\n        Traceback (most recent call last):\n            ...\n        py4j.protocol.Py4JJavaError: ...\n\n        Assign another storage level after `unpersist`\n\n        >>> _ = rdd2.unpersist()\n        >>> rdd2.is_cached\n        False\n        >>> _ = rdd2.persist(StorageLevel.MEMORY_ONLY_2)\n        >>> str(rdd2.getStorageLevel())\n        \'Memory Serialized 2x Replicated\'\n        >>> rdd2.is_cached\n        True\n        >>> _ = rdd2.unpersist()\n        '
        self.is_cached = True
        javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
        self._jrdd.persist(javaStorageLevel)
        return self

    def unpersist(self: 'RDD[T]', blocking: bool=False) -> 'RDD[T]':
        if False:
            return 10
        '\n        Mark the RDD as non-persistent, and remove all blocks for it from\n        memory and disk.\n\n        .. versionadded:: 0.9.1\n\n        Parameters\n        ----------\n        blocking : bool, optional, default False\n            whether to block until all blocks are deleted\n\n            .. versionadded:: 3.0.0\n\n        Returns\n        -------\n        :class:`RDD`\n            The same :class:`RDD`\n\n        See Also\n        --------\n        :meth:`RDD.cache`\n        :meth:`RDD.persist`\n        :meth:`RDD.getStorageLevel`\n\n        Examples\n        --------\n        >>> rdd = sc.range(5)\n        >>> rdd.is_cached\n        False\n        >>> _ = rdd.unpersist()\n        >>> rdd.is_cached\n        False\n        >>> _ = rdd.cache()\n        >>> rdd.is_cached\n        True\n        >>> _ = rdd.unpersist()\n        >>> rdd.is_cached\n        False\n        >>> _ = rdd.unpersist()\n        '
        self.is_cached = False
        self._jrdd.unpersist(blocking)
        return self

    def checkpoint(self) -> None:
        if False:
            for i in range(10):
                print('nop')
        '\n        Mark this RDD for checkpointing. It will be saved to a file inside the\n        checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n        all references to its parent RDDs will be removed. This function must\n        be called before any job has been executed on this RDD. It is strongly\n        recommended that this RDD is persisted in memory, otherwise saving it\n        on a file will require recomputation.\n\n        .. versionadded:: 0.7.0\n\n        See Also\n        --------\n        :meth:`RDD.isCheckpointed`\n        :meth:`RDD.getCheckpointFile`\n        :meth:`RDD.localCheckpoint`\n        :meth:`SparkContext.setCheckpointDir`\n        :meth:`SparkContext.getCheckpointDir`\n\n        Examples\n        --------\n        >>> rdd = sc.range(5)\n        >>> rdd.is_checkpointed\n        False\n        >>> rdd.getCheckpointFile() == None\n        True\n\n        >>> rdd.checkpoint()\n        >>> rdd.is_checkpointed\n        True\n        >>> rdd.getCheckpointFile() == None\n        True\n\n        >>> rdd.count()\n        5\n        >>> rdd.is_checkpointed\n        True\n        >>> rdd.getCheckpointFile() == None\n        False\n        '
        self.is_checkpointed = True
        self._jrdd.rdd().checkpoint()

    def isCheckpointed(self) -> bool:
        if False:
            for i in range(10):
                print('nop')
        '\n        Return whether this RDD is checkpointed and materialized, either reliably or locally.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        bool\n            whether this :class:`RDD` is checkpointed and materialized, either reliably or locally\n\n        See Also\n        --------\n        :meth:`RDD.checkpoint`\n        :meth:`RDD.getCheckpointFile`\n        :meth:`SparkContext.setCheckpointDir`\n        :meth:`SparkContext.getCheckpointDir`\n        '
        return self._jrdd.rdd().isCheckpointed()

    def localCheckpoint(self) -> None:
        if False:
            i = 10
            return i + 15
        "\n        Mark this RDD for local checkpointing using Spark's existing caching layer.\n\n        This method is for users who wish to truncate RDD lineages while skipping the expensive\n        step of replicating the materialized data in a reliable distributed file system. This is\n        useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n\n        Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n        data is written to ephemeral local storage in the executors instead of to a reliable,\n        fault-tolerant storage. The effect is that if an executor fails during the computation,\n        the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n\n        This is NOT safe to use with dynamic allocation, which removes executors along\n        with their cached blocks. If you must use both features, you are advised to set\n        `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n\n        The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n\n        .. versionadded:: 2.2.0\n\n        See Also\n        --------\n        :meth:`RDD.checkpoint`\n        :meth:`RDD.isLocallyCheckpointed`\n\n        Examples\n        --------\n        >>> rdd = sc.range(5)\n        >>> rdd.isLocallyCheckpointed()\n        False\n\n        >>> rdd.localCheckpoint()\n        >>> rdd.isLocallyCheckpointed()\n        True\n        "
        self._jrdd.rdd().localCheckpoint()

    def isLocallyCheckpointed(self) -> bool:
        if False:
            return 10
        '\n        Return whether this RDD is marked for local checkpointing.\n\n        Exposed for testing.\n\n        .. versionadded:: 2.2.0\n\n        Returns\n        -------\n        bool\n            whether this :class:`RDD` is marked for local checkpointing\n\n        See Also\n        --------\n        :meth:`RDD.localCheckpoint`\n        '
        return self._jrdd.rdd().isLocallyCheckpointed()

    def getCheckpointFile(self) -> Optional[str]:
        if False:
            i = 10
            return i + 15
        '\n        Gets the name of the file to which this RDD was checkpointed\n\n        Not defined if RDD is checkpointed locally.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        str\n            the name of the file to which this :class:`RDD` was checkpointed\n\n        See Also\n        --------\n        :meth:`RDD.checkpoint`\n        :meth:`SparkContext.setCheckpointDir`\n        :meth:`SparkContext.getCheckpointDir`\n        '
        checkpointFile = self._jrdd.rdd().getCheckpointFile()
        return checkpointFile.get() if checkpointFile.isDefined() else None

    def cleanShuffleDependencies(self, blocking: bool=False) -> None:
        if False:
            return 10
        "\n        Removes an RDD's shuffles and it's non-persisted ancestors.\n\n        When running without a shuffle service, cleaning up shuffle files enables downscaling.\n        If you use the RDD after this call, you should checkpoint and materialize it first.\n\n        .. versionadded:: 3.3.0\n\n        Parameters\n        ----------\n        blocking : bool, optional, default False\n           whether to block on shuffle cleanup tasks\n\n        Notes\n        -----\n        This API is a developer API.\n        "
        self._jrdd.rdd().cleanShuffleDependencies(blocking)

    def map(self: 'RDD[T]', f: Callable[[T], U], preservesPartitioning: bool=False) -> 'RDD[U]':
        if False:
            return 10
        '\n        Return a new RDD by applying a function to each element of this RDD.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        f : function\n            a function to run on each element of the RDD\n        preservesPartitioning : bool, optional, default False\n            indicates whether the input function preserves the partitioner,\n            which should be False unless this is a pair RDD and the input\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` by applying a function to all elements\n\n        See Also\n        --------\n        :meth:`RDD.flatMap`\n        :meth:`RDD.mapPartitions`\n        :meth:`RDD.mapPartitionsWithIndex`\n        :meth:`RDD.mapPartitionsWithSplit`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize(["b", "a", "c"])\n        >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n        [(\'a\', 1), (\'b\', 1), (\'c\', 1)]\n        '

        def func(_: int, iterator: Iterable[T]) -> Iterable[U]:
            if False:
                return 10
            return map(fail_on_stopiteration(f), iterator)
        return self.mapPartitionsWithIndex(func, preservesPartitioning)

    def flatMap(self: 'RDD[T]', f: Callable[[T], Iterable[U]], preservesPartitioning: bool=False) -> 'RDD[U]':
        if False:
            while True:
                i = 10
        '\n        Return a new RDD by first applying a function to all elements of this\n        RDD, and then flattening the results.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        f : function\n            a function to turn a T into a sequence of U\n        preservesPartitioning : bool, optional, default False\n            indicates whether the input function preserves the partitioner,\n            which should be False unless this is a pair RDD and the input\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` by applying a function to all elements\n\n        See Also\n        --------\n        :meth:`RDD.map`\n        :meth:`RDD.mapPartitions`\n        :meth:`RDD.mapPartitionsWithIndex`\n        :meth:`RDD.mapPartitionsWithSplit`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([2, 3, 4])\n        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n        [1, 1, 1, 2, 2, 3]\n        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n        '

        def func(_: int, iterator: Iterable[T]) -> Iterable[U]:
            if False:
                while True:
                    i = 10
            return chain.from_iterable(map(fail_on_stopiteration(f), iterator))
        return self.mapPartitionsWithIndex(func, preservesPartitioning)

    def mapPartitions(self: 'RDD[T]', f: Callable[[Iterable[T]], Iterable[U]], preservesPartitioning: bool=False) -> 'RDD[U]':
        if False:
            i = 10
            return i + 15
        '\n        Return a new RDD by applying a function to each partition of this RDD.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        f : function\n            a function to run on each partition of the RDD\n        preservesPartitioning : bool, optional, default False\n            indicates whether the input function preserves the partitioner,\n            which should be False unless this is a pair RDD and the input\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` by applying a function to each partition\n\n        See Also\n        --------\n        :meth:`RDD.map`\n        :meth:`RDD.flatMap`\n        :meth:`RDD.mapPartitionsWithIndex`\n        :meth:`RDD.mapPartitionsWithSplit`\n        :meth:`RDDBarrier.mapPartitions`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> def f(iterator): yield sum(iterator)\n        ...\n        >>> rdd.mapPartitions(f).collect()\n        [3, 7]\n        '

        def func(_: int, iterator: Iterable[T]) -> Iterable[U]:
            if False:
                i = 10
                return i + 15
            return f(iterator)
        return self.mapPartitionsWithIndex(func, preservesPartitioning)

    def mapPartitionsWithIndex(self: 'RDD[T]', f: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool=False) -> 'RDD[U]':
        if False:
            print('Hello World!')
        '\n        Return a new RDD by applying a function to each partition of this RDD,\n        while tracking the index of the original partition.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        f : function\n            a function to run on each partition of the RDD\n        preservesPartitioning : bool, optional, default False\n            indicates whether the input function preserves the partitioner,\n            which should be False unless this is a pair RDD and the input\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` by applying a function to each partition\n\n        See Also\n        --------\n        :meth:`RDD.map`\n        :meth:`RDD.flatMap`\n        :meth:`RDD.mapPartitions`\n        :meth:`RDD.mapPartitionsWithSplit`\n        :meth:`RDDBarrier.mapPartitionsWithIndex`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n        >>> def f(splitIndex, iterator): yield splitIndex\n        ...\n        >>> rdd.mapPartitionsWithIndex(f).sum()\n        6\n        '
        return PipelinedRDD(self, f, preservesPartitioning)

    def mapPartitionsWithSplit(self: 'RDD[T]', f: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool=False) -> 'RDD[U]':
        if False:
            while True:
                i = 10
        '\n        Return a new RDD by applying a function to each partition of this RDD,\n        while tracking the index of the original partition.\n\n        .. versionadded:: 0.7.0\n\n        .. deprecated:: 0.9.0\n            use meth:`RDD.mapPartitionsWithIndex` instead.\n\n        Parameters\n        ----------\n        f : function\n            a function to run on each partition of the RDD\n        preservesPartitioning : bool, optional, default False\n            indicates whether the input function preserves the partitioner,\n            which should be False unless this is a pair RDD and the input\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` by applying a function to each partition\n\n        See Also\n        --------\n        :meth:`RDD.map`\n        :meth:`RDD.flatMap`\n        :meth:`RDD.mapPartitions`\n        :meth:`RDD.mapPartitionsWithIndex`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n        >>> def f(splitIndex, iterator): yield splitIndex\n        ...\n        >>> rdd.mapPartitionsWithSplit(f).sum()\n        6\n        '
        warnings.warn('mapPartitionsWithSplit is deprecated; use mapPartitionsWithIndex instead', FutureWarning, stacklevel=2)
        return self.mapPartitionsWithIndex(f, preservesPartitioning)

    def getNumPartitions(self) -> int:
        if False:
            print('Hello World!')
        '\n        Returns the number of partitions in RDD\n\n        .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        int\n            number of partitions\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> rdd.getNumPartitions()\n        2\n        '
        return self._jrdd.partitions().size()

    def filter(self: 'RDD[T]', f: Callable[[T], bool]) -> 'RDD[T]':
        if False:
            print('Hello World!')
        '\n        Return a new RDD containing only the elements that satisfy a predicate.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        f : function\n            a function to run on each element of the RDD\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` by applying a function to each element\n\n        See Also\n        --------\n        :meth:`RDD.map`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n        >>> rdd.filter(lambda x: x % 2 == 0).collect()\n        [2, 4]\n        '

        def func(iterator: Iterable[T]) -> Iterable[T]:
            if False:
                print('Hello World!')
            return filter(fail_on_stopiteration(f), iterator)
        return self.mapPartitions(func, True)

    def distinct(self: 'RDD[T]', numPartitions: Optional[int]=None) -> 'RDD[T]':
        if False:
            while True:
                i = 10
        '\n        Return a new RDD containing the distinct elements in this RDD.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` containing the distinct elements\n\n        See Also\n        --------\n        :meth:`RDD.countApproxDistinct`\n\n        Examples\n        --------\n        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n        [1, 2, 3]\n        '
        return self.map(lambda x: (x, None)).reduceByKey(lambda x, _: x, numPartitions).map(lambda x: x[0])

    def sample(self: 'RDD[T]', withReplacement: bool, fraction: float, seed: Optional[int]=None) -> 'RDD[T]':
        if False:
            i = 10
            return i + 15
        "\n        Return a sampled subset of this RDD.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        withReplacement : bool\n            can elements be sampled multiple times (replaced when sampled out)\n        fraction : float\n            expected size of the sample as a fraction of this RDD's size\n            without replacement: probability that each element is chosen; fraction must be [0, 1]\n            with replacement: expected number of times each element is chosen; fraction must be >= 0\n        seed : int, optional\n            seed for the random number generator\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` containing a sampled subset of elements\n\n        See Also\n        --------\n        :meth:`RDD.takeSample`\n        :meth:`RDD.sampleByKey`\n        :meth:`pyspark.sql.DataFrame.sample`\n\n        Notes\n        -----\n        This is not guaranteed to provide exactly the fraction specified of the total\n        count of the given :class:`DataFrame`.\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize(range(100), 4)\n        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n        True\n        "
        if not fraction >= 0:
            raise ValueError('Fraction must be nonnegative.')
        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)

    def randomSplit(self: 'RDD[T]', weights: Sequence[Union[int, float]], seed: Optional[int]=None) -> 'List[RDD[T]]':
        if False:
            return 10
        "\n        Randomly splits this RDD with the provided weights.\n\n        .. versionadded:: 1.3.0\n\n        Parameters\n        ----------\n        weights : list\n            weights for splits, will be normalized if they don't sum to 1\n        seed : int, optional\n            random seed\n\n        Returns\n        -------\n        list\n            split :class:`RDD`\\s in a list\n\n        See Also\n        --------\n        :meth:`pyspark.sql.DataFrame.randomSplit`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize(range(500), 1)\n        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n        >>> len(rdd1.collect() + rdd2.collect())\n        500\n        >>> 150 < rdd1.count() < 250\n        True\n        >>> 250 < rdd2.count() < 350\n        True\n        "
        if not all((w >= 0 for w in weights)):
            raise ValueError('Weights must be nonnegative')
        s = float(sum(weights))
        if not s > 0:
            raise ValueError('Sum of weights must be positive')
        cweights = [0.0]
        for w in weights:
            cweights.append(cweights[-1] + w / s)
        if seed is None:
            seed = random.randint(0, 2 ** 32 - 1)
        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True) for (lb, ub) in zip(cweights, cweights[1:])]

    def takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Optional[int]=None) -> List[T]:
        if False:
            return 10
        "\n        Return a fixed-size sampled subset of this RDD.\n\n        .. versionadded:: 1.3.0\n\n        Parameters\n        ----------\n        withReplacement : list\n            whether sampling is done with replacement\n        num : int\n            size of the returned sample\n        seed : int, optional\n            random seed\n\n        Returns\n        -------\n        list\n            a fixed-size sampled subset of this :class:`RDD` in an array\n\n        See Also\n        --------\n        :meth:`RDD.sample`\n\n        Notes\n        -----\n        This method should only be used if the resulting array is expected\n        to be small, as all the data is loaded into the driver's memory.\n\n        Examples\n        --------\n        >>> import sys\n        >>> rdd = sc.parallelize(range(0, 10))\n        >>> len(rdd.takeSample(True, 20, 1))\n        20\n        >>> len(rdd.takeSample(False, 5, 2))\n        5\n        >>> len(rdd.takeSample(False, 15, 3))\n        10\n        >>> sc.range(0, 10).takeSample(False, sys.maxsize)\n        Traceback (most recent call last):\n            ...\n        ValueError: Sample size cannot be greater than ...\n        "
        numStDev = 10.0
        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))
        if num < 0:
            raise ValueError('Sample size cannot be negative.')
        elif num > maxSampleSize:
            raise ValueError('Sample size cannot be greater than %d.' % maxSampleSize)
        if num == 0 or self.getNumPartitions() == 0:
            return []
        initialCount = self.count()
        if initialCount == 0:
            return []
        rand = random.Random(seed)
        if not withReplacement and num >= initialCount:
            samples = self.collect()
            rand.shuffle(samples)
            return samples
        fraction = RDD._computeFractionForSampleSize(num, initialCount, withReplacement)
        samples = self.sample(withReplacement, fraction, seed).collect()
        while len(samples) < num:
            seed = rand.randint(0, sys.maxsize)
            samples = self.sample(withReplacement, fraction, seed).collect()
        rand.shuffle(samples)
        return samples[0:num]

    @staticmethod
    def _computeFractionForSampleSize(sampleSizeLowerBound: int, total: int, withReplacement: bool) -> float:
        if False:
            return 10
        "\n        Returns a sampling rate that guarantees a sample of\n        size >= sampleSizeLowerBound 99.99% of the time.\n\n        How the sampling rate is determined:\n        Let p = num / total, where num is the sample size and total is the\n        total number of data points in the RDD. We're trying to compute\n        q > p such that\n          - when sampling with replacement, we're drawing each data point\n            with prob_i ~ Pois(q), where we want to guarantee\n            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to\n            total), i.e. the failure rate of not having a sufficiently large\n            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient\n            to guarantee 0.9999 success rate for num > 12, but we need a\n            slightly larger q (9 empirically determined).\n          - when sampling without replacement, we're drawing each data point\n            with prob_i ~ Binomial(total, fraction) and our choice of q\n            guarantees 1-delta, or 0.9999 success rate, where success rate is\n            defined the same as in sampling with replacement.\n        "
        fraction = float(sampleSizeLowerBound) / total
        if withReplacement:
            numStDev = 5
            if sampleSizeLowerBound < 12:
                numStDev = 9
            return fraction + numStDev * sqrt(fraction / total)
        else:
            delta = 5e-05
            gamma = -log(delta) / total
            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))

    def union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]':
        if False:
            return 10
        '\n        Return the union of this RDD and another one.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            the union of this :class:`RDD` and another one\n\n        See Also\n        --------\n        :meth:`SparkContext.union`\n        :meth:`pyspark.sql.DataFrame.union`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 1, 2, 3])\n        >>> rdd.union(rdd).collect()\n        [1, 1, 2, 3, 1, 1, 2, 3]\n        '
        if self._jrdd_deserializer == other._jrdd_deserializer:
            rdd: 'RDD[Union[T, U]]' = RDD(self._jrdd.union(other._jrdd), self.ctx, self._jrdd_deserializer)
        else:
            self_copy = self._reserialize()
            other_copy = other._reserialize()
            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx, self.ctx.serializer)
        if self.partitioner == other.partitioner and self.getNumPartitions() == rdd.getNumPartitions():
            rdd.partitioner = self.partitioner
        return rdd

    def intersection(self: 'RDD[T]', other: 'RDD[T]') -> 'RDD[T]':
        if False:
            i = 10
            return i + 15
        '\n        Return the intersection of this RDD and another one. The output will\n        not contain any duplicate elements, even if the input RDDs did.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            the intersection of this :class:`RDD` and another one\n\n        See Also\n        --------\n        :meth:`pyspark.sql.DataFrame.intersect`\n\n        Notes\n        -----\n        This method performs a shuffle internally.\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n        >>> rdd1.intersection(rdd2).collect()\n        [1, 2, 3]\n        '
        return self.map(lambda v: (v, None)).cogroup(other.map(lambda v: (v, None))).filter(lambda k_vs: all(k_vs[1])).keys()

    def _reserialize(self: 'RDD[T]', serializer: Optional[Serializer]=None) -> 'RDD[T]':
        if False:
            for i in range(10):
                print('nop')
        serializer = serializer or self.ctx.serializer
        if self._jrdd_deserializer != serializer:
            self = self.map(lambda x: x, preservesPartitioning=True)
            self._jrdd_deserializer = serializer
        return self

    def __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]':
        if False:
            i = 10
            return i + 15
        '\n        Return the union of this RDD and another one.\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 1, 2, 3])\n        >>> (rdd + rdd).collect()\n        [1, 1, 2, 3, 1, 1, 2, 3]\n        '
        if not isinstance(other, RDD):
            raise TypeError
        return self.union(other)

    @overload
    def repartitionAndSortWithinPartitions(self: 'RDD[Tuple[S, V]]', numPartitions: Optional[int]=..., partitionFunc: Callable[['S'], int]=..., ascending: bool=...) -> 'RDD[Tuple[S, V]]':
        if False:
            while True:
                i = 10
        ...

    @overload
    def repartitionAndSortWithinPartitions(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int], partitionFunc: Callable[[K], int], ascending: bool, keyfunc: Callable[[K], 'S']) -> 'RDD[Tuple[K, V]]':
        if False:
            return 10
        ...

    @overload
    def repartitionAndSortWithinPartitions(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int]=..., partitionFunc: Callable[[K], int]=..., ascending: bool=..., *, keyfunc: Callable[[K], 'S']) -> 'RDD[Tuple[K, V]]':
        if False:
            for i in range(10):
                print('nop')
        ...

    def repartitionAndSortWithinPartitions(self: 'RDD[Tuple[Any, Any]]', numPartitions: Optional[int]=None, partitionFunc: Callable[[Any], int]=portable_hash, ascending: bool=True, keyfunc: Callable[[Any], Any]=lambda x: x) -> 'RDD[Tuple[Any, Any]]':
        if False:
            while True:
                i = 10
        '\n        Repartition the RDD according to the given partitioner and, within each resulting partition,\n        sort records by their keys.\n\n        .. versionadded:: 1.2.0\n\n        Parameters\n        ----------\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n        partitionFunc : function, optional, default `portable_hash`\n            a function to compute the partition index\n        ascending : bool, optional, default True\n            sort the keys in ascending or descending order\n        keyfunc : function, optional, default identity mapping\n            a function to compute the key\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD`\n\n        See Also\n        --------\n        :meth:`RDD.repartition`\n        :meth:`RDD.partitionBy`\n        :meth:`RDD.sortBy`\n        :meth:`RDD.sortByKey`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n        >>> rdd2.glom().collect()\n        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n        '
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        memory = self._memory_limit()
        serializer = self._jrdd_deserializer

        def sortPartition(iterator: Iterable[Tuple[K, V]]) -> Iterable[Tuple[K, V]]:
            if False:
                for i in range(10):
                    print('nop')
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=not ascending))
        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)

    @overload
    def sortByKey(self: 'RDD[Tuple[S, V]]', ascending: bool=..., numPartitions: Optional[int]=...) -> 'RDD[Tuple[K, V]]':
        if False:
            print('Hello World!')
        ...

    @overload
    def sortByKey(self: 'RDD[Tuple[K, V]]', ascending: bool, numPartitions: int, keyfunc: Callable[[K], 'S']) -> 'RDD[Tuple[K, V]]':
        if False:
            for i in range(10):
                print('nop')
        ...

    @overload
    def sortByKey(self: 'RDD[Tuple[K, V]]', ascending: bool=..., numPartitions: Optional[int]=..., *, keyfunc: Callable[[K], 'S']) -> 'RDD[Tuple[K, V]]':
        if False:
            return 10
        ...

    def sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Optional[bool]=True, numPartitions: Optional[int]=None, keyfunc: Callable[[Any], Any]=lambda x: x) -> 'RDD[Tuple[K, V]]':
        if False:
            for i in range(10):
                print('nop')
        "\n        Sorts this RDD, which is assumed to consist of (key, value) pairs.\n\n        .. versionadded:: 0.9.1\n\n        Parameters\n        ----------\n        ascending : bool, optional, default True\n            sort the keys in ascending or descending order\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n        keyfunc : function, optional, default identity mapping\n            a function to compute the key\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD`\n\n        See Also\n        --------\n        :meth:`RDD.sortBy`\n        :meth:`pyspark.sql.DataFrame.sort`\n\n        Examples\n        --------\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortByKey().first()\n        ('1', 3)\n        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n        "
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        memory = self._memory_limit()
        serializer = self._jrdd_deserializer

        def sortPartition(iterator: Iterable[Tuple[K, V]]) -> Iterable[Tuple[K, V]]:
            if False:
                i = 10
                return i + 15
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=not ascending))
        if numPartitions == 1:
            if self.getNumPartitions() > 1:
                self = self.coalesce(1)
            return self.mapPartitions(sortPartition, True)
        rddSize = self.count()
        if not rddSize:
            return self
        maxSampleSize = numPartitions * 20.0
        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)
        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()
        samples = sorted(samples, key=keyfunc)
        bounds = [samples[int(len(samples) * (i + 1) / numPartitions)] for i in range(0, numPartitions - 1)]

        def rangePartitioner(k: K) -> int:
            if False:
                return 10
            p = bisect.bisect_left(bounds, keyfunc(k))
            if ascending:
                return p
            else:
                return numPartitions - 1 - p
        return self.partitionBy(numPartitions, rangePartitioner).mapPartitions(sortPartition, True)

    def sortBy(self: 'RDD[T]', keyfunc: Callable[[T], 'S'], ascending: bool=True, numPartitions: Optional[int]=None) -> 'RDD[T]':
        if False:
            print('Hello World!')
        "\n        Sorts this RDD by the given keyfunc\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        keyfunc : function\n            a function to compute the key\n        ascending : bool, optional, default True\n            sort the keys in ascending or descending order\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD`\n\n        See Also\n        --------\n        :meth:`RDD.sortByKey`\n        :meth:`pyspark.sql.DataFrame.sort`\n\n        Examples\n        --------\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        "
        return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()

    def glom(self: 'RDD[T]') -> 'RDD[List[T]]':
        if False:
            for i in range(10):
                print('nop')
        '\n        Return an RDD created by coalescing all elements within each partition\n        into a list.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` coalescing all elements within each partition into a list\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> sorted(rdd.glom().collect())\n        [[1, 2], [3, 4]]\n        '

        def func(iterator: Iterable[T]) -> Iterable[List[T]]:
            if False:
                return 10
            yield list(iterator)
        return self.mapPartitions(func)

    def cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]':
        if False:
            return 10
        '\n        Return the Cartesian product of this RDD and another one, that is, the\n        RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n        ``b`` is in `other`.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            the Cartesian product of this :class:`RDD` and another one\n\n        See Also\n        --------\n        :meth:`pyspark.sql.DataFrame.crossJoin`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 2])\n        >>> sorted(rdd.cartesian(rdd).collect())\n        [(1, 1), (1, 2), (2, 1), (2, 2)]\n        '
        deserializer = CartesianDeserializer(self._jrdd_deserializer, other._jrdd_deserializer)
        return RDD(self._jrdd.cartesian(other._jrdd), self.ctx, deserializer)

    def groupBy(self: 'RDD[T]', f: Callable[[T], K], numPartitions: Optional[int]=None, partitionFunc: Callable[[K], int]=portable_hash) -> 'RDD[Tuple[K, Iterable[T]]]':
        if False:
            i = 10
            return i + 15
        '\n        Return an RDD of grouped items.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        f : function\n            a function to compute the key\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n        partitionFunc : function, optional, default `portable_hash`\n            a function to compute the partition index\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` of grouped items\n\n        See Also\n        --------\n        :meth:`RDD.groupByKey`\n        :meth:`pyspark.sql.DataFrame.groupBy`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n        >>> result = rdd.groupBy(lambda x: x % 2).collect()\n        >>> sorted([(x, sorted(y)) for (x, y) in result])\n        [(0, [2, 8]), (1, [1, 1, 3, 5])]\n        '
        return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)

    def pipe(self, command: str, env: Optional[Dict[str, str]]=None, checkCode: bool=False) -> 'RDD[str]':
        if False:
            while True:
                i = 10
        "\n        Return an RDD created by piping elements to a forked external process.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        command : str\n            command to run.\n        env : dict, optional\n            environment variables to set.\n        checkCode : bool, optional\n            whether to check the return value of the shell command.\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` of strings\n\n        Examples\n        --------\n        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n        ['1', '2', '', '3']\n        "
        if env is None:
            env = dict()

        def func(iterator: Iterable[T]) -> Iterable[str]:
            if False:
                for i in range(10):
                    print('nop')
            pipe = Popen(shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)

            def pipe_objs(out: IO[bytes]) -> None:
                if False:
                    print('Hello World!')
                for obj in iterator:
                    s = str(obj).rstrip('\n') + '\n'
                    out.write(s.encode('utf-8'))
                out.close()
            Thread(target=pipe_objs, args=[pipe.stdin]).start()

            def check_return_code() -> Iterable[int]:
                if False:
                    for i in range(10):
                        print('nop')
                pipe.wait()
                if checkCode and pipe.returncode:
                    raise PySparkRuntimeError(error_class='PIPE_FUNCTION_EXITED', message_parameters={'func_name': command, 'error_code': str(pipe.returncode)})
                else:
                    for i in range(0):
                        yield i
            return (cast(bytes, x).rstrip(b'\n').decode('utf-8') for x in chain(iter(cast(IO[bytes], pipe.stdout).readline, b''), check_return_code()))
        return self.mapPartitions(func)

    def foreach(self: 'RDD[T]', f: Callable[[T], None]) -> None:
        if False:
            for i in range(10):
                print('nop')
        '\n        Applies a function to all elements of this RDD.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        f : function\n            a function applied to each element\n\n        See Also\n        --------\n        :meth:`RDD.foreachPartition`\n        :meth:`pyspark.sql.DataFrame.foreach`\n        :meth:`pyspark.sql.DataFrame.foreachPartition`\n\n        Examples\n        --------\n        >>> def f(x): print(x)\n        ...\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n        '
        f = fail_on_stopiteration(f)

        def processPartition(iterator: Iterable[T]) -> Iterable[Any]:
            if False:
                return 10
            for x in iterator:
                f(x)
            return iter([])
        self.mapPartitions(processPartition).count()

    def foreachPartition(self: 'RDD[T]', f: Callable[[Iterable[T]], None]) -> None:
        if False:
            for i in range(10):
                print('nop')
        '\n        Applies a function to each partition of this RDD.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        f : function\n            a function applied to each partition\n\n        See Also\n        --------\n        :meth:`RDD.foreach`\n        :meth:`pyspark.sql.DataFrame.foreach`\n        :meth:`pyspark.sql.DataFrame.foreachPartition`\n\n        Examples\n        --------\n        >>> def f(iterator):\n        ...     for x in iterator:\n        ...          print(x)\n        ...\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n        '

        def func(it: Iterable[T]) -> Iterable[Any]:
            if False:
                while True:
                    i = 10
            r = f(it)
            try:
                return iter(r)
            except TypeError:
                return iter([])
        self.mapPartitions(func).count()

    def collect(self: 'RDD[T]') -> List[T]:
        if False:
            print('Hello World!')
        '\n        Return a list that contains all the elements in this RDD.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        list\n            a list containing all the elements\n\n        Notes\n        -----\n        This method should only be used if the resulting array is expected\n        to be small, as all the data is loaded into the driver\'s memory.\n\n        See Also\n        --------\n        :meth:`RDD.toLocalIterator`\n        :meth:`pyspark.sql.DataFrame.collect`\n\n        Examples\n        --------\n        >>> sc.range(5).collect()\n        [0, 1, 2, 3, 4]\n        >>> sc.parallelize(["x", "y", "z"]).collect()\n        [\'x\', \'y\', \'z\']\n        '
        with SCCallSiteSync(self.context):
            assert self.ctx._jvm is not None
            sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
        return list(_load_from_socket(sock_info, self._jrdd_deserializer))

    def collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool=False) -> 'List[T]':
        if False:
            print('Hello World!')
        '\n        When collect rdd, use this method to specify job group.\n\n        .. versionadded:: 3.0.0\n\n        .. deprecated:: 3.1.0\n            Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n\n        Parameters\n        ----------\n        groupId : str\n            The group ID to assign.\n        description : str\n            The description to set for the job group.\n        interruptOnCancel : bool, optional, default False\n            whether to interrupt jobs on job cancellation.\n\n        Returns\n        -------\n        list\n            a list containing all the elements\n\n        See Also\n        --------\n        :meth:`RDD.collect`\n        :meth:`SparkContext.setJobGroup`\n        '
        warnings.warn('Deprecated in 3.1, Use pyspark.InheritableThread with the pinned thread mode enabled.', FutureWarning)
        with SCCallSiteSync(self.context):
            assert self.ctx._jvm is not None
            sock_info = self.ctx._jvm.PythonRDD.collectAndServeWithJobGroup(self._jrdd.rdd(), groupId, description, interruptOnCancel)
        return list(_load_from_socket(sock_info, self._jrdd_deserializer))

    def reduce(self: 'RDD[T]', f: Callable[[T, T], T]) -> T:
        if False:
            while True:
                i = 10
        '\n        Reduces the elements of this RDD using the specified commutative and\n        associative binary operator. Currently reduces partitions locally.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        f : function\n            the reduce function\n\n        Returns\n        -------\n        T\n            the aggregated result\n\n        See Also\n        --------\n        :meth:`RDD.treeReduce`\n        :meth:`RDD.aggregate`\n        :meth:`RDD.treeAggregate`\n\n        Examples\n        --------\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n        15\n        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n        10\n        >>> sc.parallelize([]).reduce(add)\n        Traceback (most recent call last):\n            ...\n        ValueError: Can not reduce() empty RDD\n        '
        f = fail_on_stopiteration(f)

        def func(iterator: Iterable[T]) -> Iterable[T]:
            if False:
                while True:
                    i = 10
            iterator = iter(iterator)
            try:
                initial = next(iterator)
            except StopIteration:
                return
            yield reduce(f, iterator, initial)
        vals = self.mapPartitions(func).collect()
        if vals:
            return reduce(f, vals)
        raise ValueError('Can not reduce() empty RDD')

    def treeReduce(self: 'RDD[T]', f: Callable[[T, T], T], depth: int=2) -> T:
        if False:
            while True:
                i = 10
        '\n        Reduces the elements of this RDD in a multi-level tree pattern.\n\n        .. versionadded:: 1.3.0\n\n        Parameters\n        ----------\n        f : function\n            the reduce function\n        depth : int, optional, default 2\n            suggested depth of the tree (default: 2)\n\n        Returns\n        -------\n        T\n            the aggregated result\n\n        See Also\n        --------\n        :meth:`RDD.reduce`\n        :meth:`RDD.aggregate`\n        :meth:`RDD.treeAggregate`\n\n        Examples\n        --------\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeReduce(add)\n        -5\n        >>> rdd.treeReduce(add, 1)\n        -5\n        >>> rdd.treeReduce(add, 2)\n        -5\n        >>> rdd.treeReduce(add, 5)\n        -5\n        >>> rdd.treeReduce(add, 10)\n        -5\n        '
        if depth < 1:
            raise ValueError('Depth cannot be smaller than 1 but got %d.' % depth)
        zeroValue: Tuple[T, bool] = (None, True)

        def op(x: Tuple[T, bool], y: Tuple[T, bool]) -> Tuple[T, bool]:
            if False:
                return 10
            if x[1]:
                return y
            elif y[1]:
                return x
            else:
                return (f(x[0], y[0]), False)
        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)
        if reduced[1]:
            raise ValueError('Cannot reduce empty RDD.')
        return reduced[0]

    def fold(self: 'RDD[T]', zeroValue: T, op: Callable[[T, T], T]) -> T:
        if False:
            return 10
        '\n        Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given associative function and a neutral "zero value."\n\n        The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n        as its result value to avoid object allocation; however, it should not\n        modify ``t2``.\n\n        This behaves somewhat differently from fold operations implemented\n        for non-distributed collections in functional languages like Scala.\n        This fold operation may be applied to partitions individually, and then\n        fold those results into the final result, rather than apply the fold\n        to each element sequentially in some defined ordering. For functions\n        that are not commutative, the result may differ from that of a fold\n        applied to a non-distributed collection.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        zeroValue : T\n            the initial value for the accumulated result of each partition\n        op : function\n            a function used to both accumulate results within a partition and combine\n            results from different partitions\n\n        Returns\n        -------\n        T\n            the aggregated result\n\n        See Also\n        --------\n        :meth:`RDD.reduce`\n        :meth:`RDD.aggregate`\n\n        Examples\n        --------\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n        15\n        '
        op = fail_on_stopiteration(op)

        def func(iterator: Iterable[T]) -> Iterable[T]:
            if False:
                print('Hello World!')
            acc = zeroValue
            for obj in iterator:
                acc = op(acc, obj)
            yield acc
        vals = self.mapPartitions(func).collect()
        return reduce(op, vals, zeroValue)

    def aggregate(self: 'RDD[T]', zeroValue: U, seqOp: Callable[[U, T], U], combOp: Callable[[U, U], U]) -> U:
        if False:
            return 10
        '\n        Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given combine functions and a neutral "zero\n        value."\n\n        The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n        as its result value to avoid object allocation; however, it should not\n        modify ``t2``.\n\n        The first function (seqOp) can return a different result type, U, than\n        the type of this RDD. Thus, we need one operation for merging a T into\n        an U and one operation for merging two U\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        zeroValue : U\n            the initial value for the accumulated result of each partition\n        seqOp : function\n            a function used to accumulate results within a partition\n        combOp : function\n            an associative function used to combine results from different partitions\n\n        Returns\n        -------\n        U\n            the aggregated result\n\n        See Also\n        --------\n        :meth:`RDD.reduce`\n        :meth:`RDD.fold`\n\n        Examples\n        --------\n        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n        (10, 4)\n        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n        (0, 0)\n        '
        seqOp = fail_on_stopiteration(seqOp)
        combOp = fail_on_stopiteration(combOp)

        def func(iterator: Iterable[T]) -> Iterable[U]:
            if False:
                while True:
                    i = 10
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc
        vals = self.mapPartitions(func).collect()
        return reduce(combOp, vals, zeroValue)

    def treeAggregate(self: 'RDD[T]', zeroValue: U, seqOp: Callable[[U, T], U], combOp: Callable[[U, U], U], depth: int=2) -> U:
        if False:
            while True:
                i = 10
        '\n        Aggregates the elements of this RDD in a multi-level tree\n        pattern.\n\n        .. versionadded:: 1.3.0\n\n        Parameters\n        ----------\n        zeroValue : U\n            the initial value for the accumulated result of each partition\n        seqOp : function\n            a function used to accumulate results within a partition\n        combOp : function\n            an associative function used to combine results from different partitions\n        depth : int, optional, default 2\n            suggested depth of the tree\n\n        Returns\n        -------\n        U\n            the aggregated result\n\n        See Also\n        --------\n        :meth:`RDD.aggregate`\n        :meth:`RDD.treeReduce`\n\n        Examples\n        --------\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeAggregate(0, add, add)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 1)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 2)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 5)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 10)\n        -5\n        '
        if depth < 1:
            raise ValueError('Depth cannot be smaller than 1 but got %d.' % depth)
        if self.getNumPartitions() == 0:
            return zeroValue

        def aggregatePartition(iterator: Iterable[T]) -> Iterable[U]:
            if False:
                i = 10
                return i + 15
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc
        partiallyAggregated = self.mapPartitions(aggregatePartition)
        numPartitions = partiallyAggregated.getNumPartitions()
        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)
        while numPartitions > scale + numPartitions / scale:
            numPartitions /= scale
            curNumPartitions = int(numPartitions)

            def mapPartition(i: int, iterator: Iterable[U]) -> Iterable[Tuple[int, U]]:
                if False:
                    for i in range(10):
                        print('nop')
                for obj in iterator:
                    yield (i % curNumPartitions, obj)
            partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex(mapPartition).reduceByKey(combOp, curNumPartitions).values()
        return partiallyAggregated.reduce(combOp)

    @overload
    def max(self: 'RDD[S]') -> 'S':
        if False:
            while True:
                i = 10
        ...

    @overload
    def max(self: 'RDD[T]', key: Callable[[T], 'S']) -> T:
        if False:
            i = 10
            return i + 15
        ...

    def max(self: 'RDD[T]', key: Optional[Callable[[T], 'S']]=None) -> T:
        if False:
            return 10
        '\n        Find the maximum item in this RDD.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        key : function, optional\n            A function used to generate key for comparing\n\n        Returns\n        -------\n        T\n            the maximum item\n\n        See Also\n        --------\n        :meth:`RDD.min`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n        >>> rdd.max()\n        43.0\n        >>> rdd.max(key=str)\n        5.0\n        '
        if key is None:
            return self.reduce(max)
        return self.reduce(lambda a, b: max(a, b, key=key))

    @overload
    def min(self: 'RDD[S]') -> 'S':
        if False:
            return 10
        ...

    @overload
    def min(self: 'RDD[T]', key: Callable[[T], 'S']) -> T:
        if False:
            while True:
                i = 10
        ...

    def min(self: 'RDD[T]', key: Optional[Callable[[T], 'S']]=None) -> T:
        if False:
            for i in range(10):
                print('nop')
        '\n        Find the minimum item in this RDD.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        key : function, optional\n            A function used to generate key for comparing\n\n        Returns\n        -------\n        T\n            the minimum item\n\n        See Also\n        --------\n        :meth:`RDD.max`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n        >>> rdd.min()\n        2.0\n        >>> rdd.min(key=str)\n        10.0\n        '
        if key is None:
            return self.reduce(min)
        return self.reduce(lambda a, b: min(a, b, key=key))

    def sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray':
        if False:
            print('Hello World!')
        '\n        Add up the elements in this RDD.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        float, int, or complex\n            the sum of all elements\n\n        See Also\n        --------\n        :meth:`RDD.mean`\n        :meth:`RDD.sumApprox`\n\n        Examples\n        --------\n        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n        6.0\n        '
        return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)

    def count(self) -> int:
        if False:
            while True:
                i = 10
        '\n        Return the number of elements in this RDD.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        int\n            the number of elements\n\n        See Also\n        --------\n        :meth:`RDD.countApprox`\n        :meth:`pyspark.sql.DataFrame.count`\n\n        Examples\n        --------\n        >>> sc.parallelize([2, 3, 4]).count()\n        3\n        '
        return self.mapPartitions(lambda i: [sum((1 for _ in i))]).sum()

    def stats(self: 'RDD[NumberOrArray]') -> StatCounter:
        if False:
            while True:
                i = 10
        "\n        Return a :class:`StatCounter` object that captures the mean, variance\n        and count of the RDD's elements in one operation.\n\n        .. versionadded:: 0.9.1\n\n        Returns\n        -------\n        :class:`StatCounter`\n            a :class:`StatCounter` capturing the mean, variance and count of all elements\n\n        See Also\n        --------\n        :meth:`RDD.stdev`\n        :meth:`RDD.sampleStdev`\n        :meth:`RDD.variance`\n        :meth:`RDD.sampleVariance`\n        :meth:`RDD.histogram`\n        :meth:`pyspark.sql.DataFrame.stat`\n        "

        def redFunc(left_counter: StatCounter, right_counter: StatCounter) -> StatCounter:
            if False:
                print('Hello World!')
            return left_counter.mergeStats(right_counter)
        return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)

    def histogram(self: 'RDD[S]', buckets: Union[int, List['S'], Tuple['S', ...]]) -> Tuple[Sequence['S'], List[int]]:
        if False:
            return 10
        '\n        Compute a histogram using the provided buckets. The buckets\n        are all open to the right except for the last which is closed.\n        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n        and 50 we would have a histogram of 1,0,1.\n\n        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n        this can be switched from an O(log n) insertion to O(1) per\n        element (where n is the number of buckets).\n\n        Buckets must be sorted, not contain any duplicates, and have\n        at least two elements.\n\n        If `buckets` is a number, it will generate buckets which are\n        evenly spaced between the minimum and maximum of the RDD. For\n        example, if the min value is 0 and the max is 100, given `buckets`\n        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n        be at least 1. An exception is raised if the RDD contains infinity.\n        If the elements in the RDD do not vary (max == min), a single bucket\n        will be used.\n\n        .. versionadded:: 1.2.0\n\n        Parameters\n        ----------\n        buckets : int, or list, or tuple\n            if `buckets` is a number, it computes a histogram of the data using\n            `buckets` number of buckets evenly, otherwise, `buckets` is the provided\n            buckets to bin the data.\n\n        Returns\n        -------\n        tuple\n            a tuple of buckets and histogram\n\n        See Also\n        --------\n        :meth:`RDD.stats`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize(range(51))\n        >>> rdd.histogram(2)\n        ([0, 25, 50], [25, 26])\n        >>> rdd.histogram([0, 5, 25, 50])\n        ([0, 5, 25, 50], [5, 20, 26])\n        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n        ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n        >>> rdd = sc.parallelize(["ab", "ac", "b", "bd", "ef"])\n        >>> rdd.histogram(("a", "b", "c"))\n        ((\'a\', \'b\', \'c\'), [2, 2])\n        '
        if isinstance(buckets, int):
            if buckets < 1:
                raise ValueError('number of buckets must be >= 1')

            def comparable(x: Any) -> bool:
                if False:
                    for i in range(10):
                        print('nop')
                if x is None:
                    return False
                if type(x) is float and isnan(x):
                    return False
                return True
            filtered = self.filter(comparable)

            def minmax(a: Tuple['S', 'S'], b: Tuple['S', 'S']) -> Tuple['S', 'S']:
                if False:
                    for i in range(10):
                        print('nop')
                return (min(a[0], b[0]), max(a[1], b[1]))
            try:
                (minv, maxv) = filtered.map(lambda x: (x, x)).reduce(minmax)
            except TypeError as e:
                if ' empty ' in str(e):
                    raise ValueError('can not generate buckets from empty RDD')
                raise
            if minv == maxv or buckets == 1:
                return ([minv, maxv], [filtered.count()])
            try:
                inc = (maxv - minv) / buckets
            except TypeError:
                raise TypeError('Can not generate buckets with non-number in RDD')
            if isinf(inc):
                raise ValueError('Can not generate buckets with infinite value')
            inc = int(inc)
            if inc * buckets != maxv - minv:
                inc = (maxv - minv) * 1.0 / buckets
            buckets = [i * inc + minv for i in range(buckets)]
            buckets.append(maxv)
            even = True
        elif isinstance(buckets, (list, tuple)):
            if len(buckets) < 2:
                raise ValueError('buckets should have more than one value')
            if any((i is None or (isinstance(i, float) and isnan(i)) for i in buckets)):
                raise ValueError('can not have None or NaN in buckets')
            if sorted(buckets) != list(buckets):
                raise ValueError('buckets should be sorted')
            if len(set(buckets)) != len(buckets):
                raise ValueError('buckets should not contain duplicated values')
            minv = buckets[0]
            maxv = buckets[-1]
            even = False
            inc = None
            try:
                steps = [buckets[i + 1] - buckets[i] for i in range(len(buckets) - 1)]
            except TypeError:
                pass
            else:
                if max(steps) - min(steps) < 1e-10:
                    even = True
                    inc = (maxv - minv) / (len(buckets) - 1)
        else:
            raise TypeError('buckets should be a list or tuple or number(int or long)')

        def histogram(iterator: Iterable['S']) -> Iterable[List[int]]:
            if False:
                while True:
                    i = 10
            counters = [0] * len(buckets)
            for i in iterator:
                if i is None or (isinstance(i, float) and isnan(i)) or i > maxv or (i < minv):
                    continue
                t = int((i - minv) / inc) if even else bisect.bisect_right(buckets, i) - 1
                counters[t] += 1
            last = counters.pop()
            counters[-1] += last
            return [counters]

        def mergeCounters(a: List[int], b: List[int]) -> List[int]:
            if False:
                i = 10
                return i + 15
            return [i + j for (i, j) in zip(a, b)]
        return (buckets, self.mapPartitions(histogram).reduce(mergeCounters))

    def mean(self: 'RDD[NumberOrArray]') -> float:
        if False:
            return 10
        "\n        Compute the mean of this RDD's elements.\n\n        .. versionadded:: 0.9.1\n\n        Returns\n        -------\n        float\n            the mean of all elements\n\n        See Also\n        --------\n        :meth:`RDD.stats`\n        :meth:`RDD.sum`\n        :meth:`RDD.meanApprox`\n\n        Examples\n        --------\n        >>> sc.parallelize([1, 2, 3]).mean()\n        2.0\n        "
        return self.stats().mean()

    def variance(self: 'RDD[NumberOrArray]') -> float:
        if False:
            i = 10
            return i + 15
        "\n        Compute the variance of this RDD's elements.\n\n        .. versionadded:: 0.9.1\n\n        Returns\n        -------\n        float\n            the variance of all elements\n\n        See Also\n        --------\n        :meth:`RDD.stats`\n        :meth:`RDD.sampleVariance`\n        :meth:`RDD.stdev`\n        :meth:`RDD.sampleStdev`\n\n        Examples\n        --------\n        >>> sc.parallelize([1, 2, 3]).variance()\n        0.666...\n        "
        return self.stats().variance()

    def stdev(self: 'RDD[NumberOrArray]') -> float:
        if False:
            for i in range(10):
                print('nop')
        "\n        Compute the standard deviation of this RDD's elements.\n\n        .. versionadded:: 0.9.1\n\n        Returns\n        -------\n        float\n            the standard deviation of all elements\n\n        See Also\n        --------\n        :meth:`RDD.stats`\n        :meth:`RDD.sampleStdev`\n        :meth:`RDD.variance`\n        :meth:`RDD.sampleVariance`\n\n        Examples\n        --------\n        >>> sc.parallelize([1, 2, 3]).stdev()\n        0.816...\n        "
        return self.stats().stdev()

    def sampleStdev(self: 'RDD[NumberOrArray]') -> float:
        if False:
            while True:
                i = 10
        "\n        Compute the sample standard deviation of this RDD's elements (which\n        corrects for bias in estimating the standard deviation by dividing by\n        N-1 instead of N).\n\n        .. versionadded:: 0.9.1\n\n        Returns\n        -------\n        float\n            the sample standard deviation of all elements\n\n        See Also\n        --------\n        :meth:`RDD.stats`\n        :meth:`RDD.stdev`\n        :meth:`RDD.variance`\n        :meth:`RDD.sampleVariance`\n\n        Examples\n        --------\n        >>> sc.parallelize([1, 2, 3]).sampleStdev()\n        1.0\n        "
        return self.stats().sampleStdev()

    def sampleVariance(self: 'RDD[NumberOrArray]') -> float:
        if False:
            while True:
                i = 10
        "\n        Compute the sample variance of this RDD's elements (which corrects\n        for bias in estimating the variance by dividing by N-1 instead of N).\n\n        .. versionadded:: 0.9.1\n\n        Returns\n        -------\n        float\n            the sample variance of all elements\n\n        See Also\n        --------\n        :meth:`RDD.stats`\n        :meth:`RDD.variance`\n        :meth:`RDD.stdev`\n        :meth:`RDD.sampleStdev`\n\n        Examples\n        --------\n        >>> sc.parallelize([1, 2, 3]).sampleVariance()\n        1.0\n        "
        return self.stats().sampleVariance()

    def countByValue(self: 'RDD[K]') -> Dict[K, int]:
        if False:
            while True:
                i = 10
        '\n        Return the count of each unique value in this RDD as a dictionary of\n        (value, count) pairs.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        dict\n            a dictionary of (value, count) pairs\n\n        See Also\n        --------\n        :meth:`RDD.collectAsMap`\n        :meth:`RDD.countByKey`\n\n        Examples\n        --------\n        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n        [(1, 2), (2, 3)]\n        '

        def countPartition(iterator: Iterable[K]) -> Iterable[Dict[K, int]]:
            if False:
                return 10
            counts: Dict[K, int] = defaultdict(int)
            for obj in iterator:
                counts[obj] += 1
            yield counts

        def mergeMaps(m1: Dict[K, int], m2: Dict[K, int]) -> Dict[K, int]:
            if False:
                print('Hello World!')
            for (k, v) in m2.items():
                m1[k] += v
            return m1
        return self.mapPartitions(countPartition).reduce(mergeMaps)

    @overload
    def top(self: 'RDD[S]', num: int) -> List['S']:
        if False:
            for i in range(10):
                print('nop')
        ...

    @overload
    def top(self: 'RDD[T]', num: int, key: Callable[[T], 'S']) -> List[T]:
        if False:
            for i in range(10):
                print('nop')
        ...

    def top(self: 'RDD[T]', num: int, key: Optional[Callable[[T], 'S']]=None) -> List[T]:
        if False:
            i = 10
            return i + 15
        "\n        Get the top N elements from an RDD.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        num : int\n            top N\n        key : function, optional\n            a function used to generate key for comparing\n\n        Returns\n        -------\n        list\n            the top N elements\n\n        See Also\n        --------\n        :meth:`RDD.takeOrdered`\n        :meth:`RDD.max`\n        :meth:`RDD.min`\n\n        Notes\n        -----\n        This method should only be used if the resulting array is expected\n        to be small, as all the data is loaded into the driver's memory.\n\n        It returns the list sorted in descending order.\n\n        Examples\n        --------\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n        [12]\n        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n        [6, 5]\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n        [4, 3, 2]\n        "

        def topIterator(iterator: Iterable[T]) -> Iterable[List[T]]:
            if False:
                for i in range(10):
                    print('nop')
            yield heapq.nlargest(num, iterator, key=key)

        def merge(a: List[T], b: List[T]) -> List[T]:
            if False:
                return 10
            return heapq.nlargest(num, a + b, key=key)
        return self.mapPartitions(topIterator).reduce(merge)

    @overload
    def takeOrdered(self: 'RDD[S]', num: int) -> List['S']:
        if False:
            print('Hello World!')
        ...

    @overload
    def takeOrdered(self: 'RDD[T]', num: int, key: Callable[[T], 'S']) -> List[T]:
        if False:
            print('Hello World!')
        ...

    def takeOrdered(self: 'RDD[T]', num: int, key: Optional[Callable[[T], 'S']]=None) -> List[T]:
        if False:
            return 10
        "\n        Get the N elements from an RDD ordered in ascending order or as\n        specified by the optional key function.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        num : int\n            top N\n        key : function, optional\n            a function used to generate key for comparing\n\n        Returns\n        -------\n        list\n            the top N elements\n\n        See Also\n        --------\n        :meth:`RDD.top`\n        :meth:`RDD.max`\n        :meth:`RDD.min`\n\n        Notes\n        -----\n        This method should only be used if the resulting array is expected\n        to be small, as all the data is loaded into the driver's memory.\n\n        Examples\n        --------\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n        [1, 2, 3, 4, 5, 6]\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n        [10, 9, 7, 6, 5, 4]\n        >>> sc.emptyRDD().takeOrdered(3)\n        []\n        "
        if num < 0:
            raise ValueError('top N cannot be negative.')
        if num == 0 or self.getNumPartitions() == 0:
            return []
        else:

            def merge(a: List[T], b: List[T]) -> List[T]:
                if False:
                    i = 10
                    return i + 15
                return heapq.nsmallest(num, a + b, key)
            return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)

    def take(self: 'RDD[T]', num: int) -> List[T]:
        if False:
            for i in range(10):
                print('nop')
        "\n        Take the first num elements of the RDD.\n\n        It works by first scanning one partition, and use the results from\n        that partition to estimate the number of additional partitions needed\n        to satisfy the limit.\n\n        Translated from the Scala implementation in RDD#take().\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        num : int\n            first number of elements\n\n        Returns\n        -------\n        list\n            the first `num` elements\n\n        See Also\n        --------\n        :meth:`RDD.first`\n        :meth:`pyspark.sql.DataFrame.take`\n\n        Notes\n        -----\n        This method should only be used if the resulting array is expected\n        to be small, as all the data is loaded into the driver's memory.\n\n        Examples\n        --------\n        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n        [2, 3]\n        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n        [2, 3, 4, 5, 6]\n        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n        [91, 92, 93]\n        "
        items: List[T] = []
        totalParts = self.getNumPartitions()
        partsScanned = 0
        while len(items) < num and partsScanned < totalParts:
            numPartsToTry = 1
            if partsScanned > 0:
                if len(items) == 0:
                    numPartsToTry = partsScanned * 4
                else:
                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned
                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)
            left = num - len(items)

            def takeUpToNumLeft(iterator: Iterable[T]) -> Iterable[T]:
                if False:
                    print('Hello World!')
                iterator = iter(iterator)
                taken = 0
                while taken < left:
                    try:
                        yield next(iterator)
                    except StopIteration:
                        return
                    taken += 1
            p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
            res = self.context.runJob(self, takeUpToNumLeft, p)
            items += res
            partsScanned += numPartsToTry
        return items[:num]

    def first(self: 'RDD[T]') -> T:
        if False:
            for i in range(10):
                print('nop')
        '\n        Return the first element in this RDD.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        T\n            the first element\n\n        See Also\n        --------\n        :meth:`RDD.take`\n        :meth:`pyspark.sql.DataFrame.first`\n        :meth:`pyspark.sql.DataFrame.head`\n\n        Examples\n        --------\n        >>> sc.parallelize([2, 3, 4]).first()\n        2\n        >>> sc.parallelize([]).first()\n        Traceback (most recent call last):\n            ...\n        ValueError: RDD is empty\n        '
        rs = self.take(1)
        if rs:
            return rs[0]
        raise ValueError('RDD is empty')

    def isEmpty(self) -> bool:
        if False:
            return 10
        '\n        Returns true if and only if the RDD contains no elements at all.\n\n        .. versionadded:: 1.3.0\n\n        Returns\n        -------\n        bool\n            whether the :class:`RDD` is empty\n\n        See Also\n        --------\n        :meth:`RDD.first`\n        :meth:`pyspark.sql.DataFrame.isEmpty`\n\n        Notes\n        -----\n        An RDD may be empty even when it has at least 1 partition.\n\n        Examples\n        --------\n        >>> sc.parallelize([]).isEmpty()\n        True\n        >>> sc.parallelize([1]).isEmpty()\n        False\n        '
        return self.getNumPartitions() == 0 or len(self.take(1)) == 0

    def saveAsNewAPIHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str]=None, valueConverter: Optional[str]=None) -> None:
        if False:
            return 10
        '\n        Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n        converted for output using either user specified converters or, by default,\n        "org.apache.spark.api.python.JavaToWritableConverter".\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        conf : dict\n            Hadoop job configuration\n        keyConverter : str, optional\n            fully qualified classname of key converter (None by default)\n        valueConverter : str, optional\n            fully qualified classname of value converter (None by default)\n\n        See Also\n        --------\n        :meth:`SparkContext.newAPIHadoopRDD`\n        :meth:`RDD.saveAsHadoopDataset`\n        :meth:`RDD.saveAsHadoopFile`\n        :meth:`RDD.saveAsNewAPIHadoopFile`\n        :meth:`RDD.saveAsSequenceFile`\n\n        Examples\n        --------\n        >>> import os\n        >>> import tempfile\n\n        Set the related classes\n\n        >>> output_format_class = "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat"\n        >>> input_format_class = "org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat"\n        >>> key_class = "org.apache.hadoop.io.IntWritable"\n        >>> value_class = "org.apache.hadoop.io.Text"\n\n        >>> with tempfile.TemporaryDirectory() as d:\n        ...     path = os.path.join(d, "new_hadoop_file")\n        ...\n        ...     # Create the conf for writing\n        ...     write_conf = {\n        ...         "mapreduce.job.outputformat.class": (output_format_class),\n        ...         "mapreduce.job.output.key.class": key_class,\n        ...         "mapreduce.job.output.value.class": value_class,\n        ...         "mapreduce.output.fileoutputformat.outputdir": path,\n        ...     }\n        ...\n        ...     # Write a temporary Hadoop file\n        ...     rdd = sc.parallelize([(1, ""), (1, "a"), (3, "x")])\n        ...     rdd.saveAsNewAPIHadoopDataset(conf=write_conf)\n        ...\n        ...     # Create the conf for reading\n        ...     read_conf = {"mapreduce.input.fileinputformat.inputdir": path}\n        ...\n        ...     # Load this Hadoop file as an RDD\n        ...     loaded = sc.newAPIHadoopRDD(input_format_class,\n        ...         key_class, value_class, conf=read_conf)\n        ...     sorted(loaded.collect())\n        [(1, \'\'), (1, \'a\'), (3, \'x\')]\n        '
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        assert self.ctx._jvm is not None
        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf, keyConverter, valueConverter, True)

    def saveAsNewAPIHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str]=None, valueClass: Optional[str]=None, keyConverter: Optional[str]=None, valueConverter: Optional[str]=None, conf: Optional[Dict[str, str]]=None) -> None:
        if False:
            while True:
                i = 10
        '\n        Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n        will be inferred if not specified. Keys and values are converted for output using either\n        user specified converters or "org.apache.spark.api.python.JavaToWritableConverter". The\n        `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        path : str\n            path to Hadoop file\n        outputFormatClass : str\n            fully qualified classname of Hadoop OutputFormat\n            (e.g. "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat")\n        keyClass : str, optional\n            fully qualified classname of key Writable class\n             (e.g. "org.apache.hadoop.io.IntWritable", None by default)\n        valueClass : str, optional\n            fully qualified classname of value Writable class\n            (e.g. "org.apache.hadoop.io.Text", None by default)\n        keyConverter : str, optional\n            fully qualified classname of key converter (None by default)\n        valueConverter : str, optional\n            fully qualified classname of value converter (None by default)\n        conf : dict, optional\n            Hadoop job configuration (None by default)\n\n        See Also\n        --------\n        :meth:`SparkContext.newAPIHadoopFile`\n        :meth:`RDD.saveAsHadoopDataset`\n        :meth:`RDD.saveAsNewAPIHadoopDataset`\n        :meth:`RDD.saveAsHadoopFile`\n        :meth:`RDD.saveAsSequenceFile`\n\n        Examples\n        --------\n        >>> import os\n        >>> import tempfile\n\n        Set the class of output format\n\n        >>> output_format_class = "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat"\n\n        >>> with tempfile.TemporaryDirectory() as d:\n        ...     path = os.path.join(d, "hadoop_file")\n        ...\n        ...     # Write a temporary Hadoop file\n        ...     rdd = sc.parallelize([(1, {3.0: "bb"}), (2, {1.0: "aa"}), (3, {2.0: "dd"})])\n        ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class)\n        ...\n        ...     # Load this Hadoop file as an RDD\n        ...     sorted(sc.sequenceFile(path).collect())\n        [(1, {3.0: \'bb\'}), (2, {1.0: \'aa\'}), (3, {2.0: \'dd\'})]\n        '
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        assert self.ctx._jvm is not None
        self.ctx._jvm.PythonRDD.saveAsNewAPIHadoopFile(pickledRDD._jrdd, True, path, outputFormatClass, keyClass, valueClass, keyConverter, valueConverter, jconf)

    def saveAsHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str]=None, valueConverter: Optional[str]=None) -> None:
        if False:
            print('Hello World!')
        '\n        Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n        system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n        converted for output using either user specified converters or, by default,\n        "org.apache.spark.api.python.JavaToWritableConverter".\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        conf : dict\n            Hadoop job configuration\n        keyConverter : str, optional\n            fully qualified classname of key converter (None by default)\n        valueConverter : str, optional\n            fully qualified classname of value converter (None by default)\n\n        See Also\n        --------\n        :meth:`SparkContext.hadoopRDD`\n        :meth:`RDD.saveAsNewAPIHadoopDataset`\n        :meth:`RDD.saveAsHadoopFile`\n        :meth:`RDD.saveAsNewAPIHadoopFile`\n        :meth:`RDD.saveAsSequenceFile`\n\n        Examples\n        --------\n        >>> import os\n        >>> import tempfile\n\n        Set the related classes\n\n        >>> output_format_class = "org.apache.hadoop.mapred.TextOutputFormat"\n        >>> input_format_class = "org.apache.hadoop.mapred.TextInputFormat"\n        >>> key_class = "org.apache.hadoop.io.IntWritable"\n        >>> value_class = "org.apache.hadoop.io.Text"\n\n        >>> with tempfile.TemporaryDirectory() as d:\n        ...     path = os.path.join(d, "old_hadoop_file")\n        ...\n        ...     # Create the conf for writing\n        ...     write_conf = {\n        ...         "mapred.output.format.class": output_format_class,\n        ...         "mapreduce.job.output.key.class": key_class,\n        ...         "mapreduce.job.output.value.class": value_class,\n        ...         "mapreduce.output.fileoutputformat.outputdir": path,\n        ...     }\n        ...\n        ...     # Write a temporary Hadoop file\n        ...     rdd = sc.parallelize([(1, ""), (1, "a"), (3, "x")])\n        ...     rdd.saveAsHadoopDataset(conf=write_conf)\n        ...\n        ...     # Create the conf for reading\n        ...     read_conf = {"mapreduce.input.fileinputformat.inputdir": path}\n        ...\n        ...     # Load this Hadoop file as an RDD\n        ...     loaded = sc.hadoopRDD(input_format_class, key_class, value_class, conf=read_conf)\n        ...     sorted(loaded.collect())\n        [(0, \'1\\t\'), (0, \'1\\ta\'), (0, \'3\\tx\')]\n        '
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        assert self.ctx._jvm is not None
        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf, keyConverter, valueConverter, False)

    def saveAsHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str]=None, valueClass: Optional[str]=None, keyConverter: Optional[str]=None, valueConverter: Optional[str]=None, conf: Optional[Dict[str, str]]=None, compressionCodecClass: Optional[str]=None) -> None:
        if False:
            i = 10
            return i + 15
        '\n        Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n        system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n        will be inferred if not specified. Keys and values are converted for output using either\n        user specified converters or "org.apache.spark.api.python.JavaToWritableConverter". The\n        `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        path : str\n            path to Hadoop file\n        outputFormatClass : str\n            fully qualified classname of Hadoop OutputFormat\n            (e.g. "org.apache.hadoop.mapred.SequenceFileOutputFormat")\n        keyClass : str, optional\n            fully qualified classname of key Writable class\n            (e.g. "org.apache.hadoop.io.IntWritable", None by default)\n        valueClass : str, optional\n            fully qualified classname of value Writable class\n            (e.g. "org.apache.hadoop.io.Text", None by default)\n        keyConverter : str, optional\n            fully qualified classname of key converter (None by default)\n        valueConverter : str, optional\n            fully qualified classname of value converter (None by default)\n        conf : dict, optional\n            (None by default)\n        compressionCodecClass : str\n            fully qualified classname of the compression codec class\n            i.e. "org.apache.hadoop.io.compress.GzipCodec" (None by default)\n\n        See Also\n        --------\n        :meth:`SparkContext.hadoopFile`\n        :meth:`RDD.saveAsNewAPIHadoopFile`\n        :meth:`RDD.saveAsHadoopDataset`\n        :meth:`RDD.saveAsNewAPIHadoopDataset`\n        :meth:`RDD.saveAsSequenceFile`\n\n        Examples\n        --------\n        >>> import os\n        >>> import tempfile\n\n        Set the related classes\n\n        >>> output_format_class = "org.apache.hadoop.mapred.TextOutputFormat"\n        >>> input_format_class = "org.apache.hadoop.mapred.TextInputFormat"\n        >>> key_class = "org.apache.hadoop.io.IntWritable"\n        >>> value_class = "org.apache.hadoop.io.Text"\n\n        >>> with tempfile.TemporaryDirectory() as d:\n        ...     path = os.path.join(d, "old_hadoop_file")\n        ...\n        ...     # Write a temporary Hadoop file\n        ...     rdd = sc.parallelize([(1, ""), (1, "a"), (3, "x")])\n        ...     rdd.saveAsHadoopFile(path, output_format_class, key_class, value_class)\n        ...\n        ...     # Load this Hadoop file as an RDD\n        ...     loaded = sc.hadoopFile(path, input_format_class, key_class, value_class)\n        ...     sorted(loaded.collect())\n        [(0, \'1\\t\'), (0, \'1\\ta\'), (0, \'3\\tx\')]\n        '
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        assert self.ctx._jvm is not None
        self.ctx._jvm.PythonRDD.saveAsHadoopFile(pickledRDD._jrdd, True, path, outputFormatClass, keyClass, valueClass, keyConverter, valueConverter, jconf, compressionCodecClass)

    def saveAsSequenceFile(self: 'RDD[Tuple[K, V]]', path: str, compressionCodecClass: Optional[str]=None) -> None:
        if False:
            for i in range(10):
                print('nop')
        '\n        Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n        system, using the "org.apache.hadoop.io.Writable" types that we convert from the\n        RDD\'s key and value types. The mechanism is as follows:\n\n            1. Pickle is used to convert pickled Python RDD into RDD of Java objects.\n            2. Keys and values of this Java RDD are converted to Writables and written out.\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        path : str\n            path to sequence file\n        compressionCodecClass : str, optional\n            fully qualified classname of the compression codec class\n            i.e. "org.apache.hadoop.io.compress.GzipCodec" (None by default)\n\n        See Also\n        --------\n        :meth:`SparkContext.sequenceFile`\n        :meth:`RDD.saveAsHadoopFile`\n        :meth:`RDD.saveAsNewAPIHadoopFile`\n        :meth:`RDD.saveAsHadoopDataset`\n        :meth:`RDD.saveAsNewAPIHadoopDataset`\n        :meth:`RDD.saveAsSequenceFile`\n\n        Examples\n        --------\n        >>> import os\n        >>> import tempfile\n\n        Set the related classes\n\n        >>> with tempfile.TemporaryDirectory() as d:\n        ...     path = os.path.join(d, "sequence_file")\n        ...\n        ...     # Write a temporary sequence file\n        ...     rdd = sc.parallelize([(1, ""), (1, "a"), (3, "x")])\n        ...     rdd.saveAsSequenceFile(path)\n        ...\n        ...     # Load this sequence file as an RDD\n        ...     loaded = sc.sequenceFile(path)\n        ...     sorted(loaded.collect())\n        [(1, \'\'), (1, \'a\'), (3, \'x\')]\n        '
        pickledRDD = self._pickled()
        assert self.ctx._jvm is not None
        self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True, path, compressionCodecClass)

    def saveAsPickleFile(self, path: str, batchSize: int=10) -> None:
        if False:
            return 10
        '\n        Save this RDD as a SequenceFile of serialized objects. The serializer\n        used is :class:`pyspark.serializers.CPickleSerializer`, default batch size\n        is 10.\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        path : str\n            path to pickled file\n        batchSize : int, optional, default 10\n            the number of Python objects represented as a single Java object.\n\n        See Also\n        --------\n        :meth:`SparkContext.pickleFile`\n\n        Examples\n        --------\n        >>> import os\n        >>> import tempfile\n        >>> with tempfile.TemporaryDirectory() as d:\n        ...     path = os.path.join(d, "pickle_file")\n        ...\n        ...     # Write a temporary pickled file\n        ...     sc.parallelize(range(10)).saveAsPickleFile(path, 3)\n        ...\n        ...     # Load picked file as an RDD\n        ...     sorted(sc.pickleFile(path, 3).collect())\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        '
        ser: Serializer
        if batchSize == 0:
            ser = AutoBatchedSerializer(CPickleSerializer())
        else:
            ser = BatchedSerializer(CPickleSerializer(), batchSize)
        self._reserialize(ser)._jrdd.saveAsObjectFile(path)

    def saveAsTextFile(self, path: str, compressionCodecClass: Optional[str]=None) -> None:
        if False:
            return 10
        '\n        Save this RDD as a text file, using string representations of elements.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        path : str\n            path to text file\n        compressionCodecClass : str, optional\n            fully qualified classname of the compression codec class\n            i.e. "org.apache.hadoop.io.compress.GzipCodec" (None by default)\n\n        See Also\n        --------\n        :meth:`SparkContext.textFile`\n        :meth:`SparkContext.wholeTextFiles`\n\n        Examples\n        --------\n        >>> import os\n        >>> import tempfile\n        >>> from fileinput import input\n        >>> from glob import glob\n        >>> with tempfile.TemporaryDirectory() as d1:\n        ...     path1 = os.path.join(d1, "text_file1")\n        ...\n        ...     # Write a temporary text file\n        ...     sc.parallelize(range(10)).saveAsTextFile(path1)\n        ...\n        ...     # Load text file as an RDD\n        ...     \'\'.join(sorted(input(glob(path1 + "/part-0000*"))))\n        \'0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n\'\n\n        Empty lines are tolerated when saving to text files.\n\n        >>> with tempfile.TemporaryDirectory() as d2:\n        ...     path2 = os.path.join(d2, "text2_file2")\n        ...\n        ...     # Write another temporary text file\n        ...     sc.parallelize([\'\', \'foo\', \'\', \'bar\', \'\']).saveAsTextFile(path2)\n        ...\n        ...     # Load text file as an RDD\n        ...     \'\'.join(sorted(input(glob(path2 + "/part-0000*"))))\n        \'\\n\\n\\nbar\\nfoo\\n\'\n\n        Using compressionCodecClass\n\n        >>> from fileinput import input, hook_compressed\n        >>> with tempfile.TemporaryDirectory() as d3:\n        ...     path3 = os.path.join(d3, "text3")\n        ...     codec = "org.apache.hadoop.io.compress.GzipCodec"\n        ...\n        ...     # Write another temporary text file with specified codec\n        ...     sc.parallelize([\'foo\', \'bar\']).saveAsTextFile(path3, codec)\n        ...\n        ...     # Load text file as an RDD\n        ...     result = sorted(input(glob(path3 + "/part*.gz"), openhook=hook_compressed))\n        ...     \'\'.join([r.decode(\'utf-8\') if isinstance(r, bytes) else r for r in result])\n        \'bar\\nfoo\\n\'\n        '

        def func(split: int, iterator: Iterable[Any]) -> Iterable[bytes]:
            if False:
                i = 10
                return i + 15
            for x in iterator:
                if isinstance(x, bytes):
                    yield x
                elif isinstance(x, str):
                    yield x.encode('utf-8')
                else:
                    yield str(x).encode('utf-8')
        keyed = self.mapPartitionsWithIndex(func)
        keyed._bypass_serializer = True
        assert self.ctx._jvm is not None
        if compressionCodecClass:
            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)
        else:
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)

    def collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[K, V]:
        if False:
            for i in range(10):
                print('nop')
        "\n        Return the key-value pairs in this RDD to the master as a dictionary.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        :class:`dict`\n            a dictionary of (key, value) pairs\n\n        See Also\n        --------\n        :meth:`RDD.countByValue`\n\n        Notes\n        -----\n        This method should only be used if the resulting data is expected\n        to be small, as all the data is loaded into the driver's memory.\n\n        Examples\n        --------\n        >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n        >>> m[1]\n        2\n        >>> m[3]\n        4\n        "
        return dict(self.collect())

    def keys(self: 'RDD[Tuple[K, V]]') -> 'RDD[K]':
        if False:
            i = 10
            return i + 15
        '\n        Return an RDD with the keys of each tuple.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` only containing the keys\n\n        See Also\n        --------\n        :meth:`RDD.values`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([(1, 2), (3, 4)]).keys()\n        >>> rdd.collect()\n        [1, 3]\n        '
        return self.map(lambda x: x[0])

    def values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]':
        if False:
            i = 10
            return i + 15
        '\n        Return an RDD with the values of each tuple.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` only containing the values\n\n        See Also\n        --------\n        :meth:`RDD.keys`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([(1, 2), (3, 4)]).values()\n        >>> rdd.collect()\n        [2, 4]\n        '
        return self.map(lambda x: x[1])

    def reduceByKey(self: 'RDD[Tuple[K, V]]', func: Callable[[V, V], V], numPartitions: Optional[int]=None, partitionFunc: Callable[[K], int]=portable_hash) -> 'RDD[Tuple[K, V]]':
        if False:
            print('Hello World!')
        '\n        Merge the values for each key using an associative and commutative reduce function.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a "combiner" in MapReduce.\n\n        Output will be partitioned with `numPartitions` partitions, or\n        the default parallelism level if `numPartitions` is not specified.\n        Default partitioner is hash-partition.\n\n        .. versionadded:: 1.6.0\n\n        Parameters\n        ----------\n        func : function\n            the reduce function\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n        partitionFunc : function, optional, default `portable_hash`\n            function to compute the partition index\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the keys and the aggregated result for each key\n\n        See Also\n        --------\n        :meth:`RDD.reduceByKeyLocally`\n        :meth:`RDD.combineByKey`\n        :meth:`RDD.aggregateByKey`\n        :meth:`RDD.foldByKey`\n        :meth:`RDD.groupByKey`\n\n        Examples\n        --------\n        >>> from operator import add\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])\n        >>> sorted(rdd.reduceByKey(add).collect())\n        [(\'a\', 2), (\'b\', 1)]\n        '
        return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)

    def reduceByKeyLocally(self: 'RDD[Tuple[K, V]]', func: Callable[[V, V], V]) -> Dict[K, V]:
        if False:
            while True:
                i = 10
        '\n        Merge the values for each key using an associative and commutative reduce function, but\n        return the results immediately to the master as a dictionary.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a "combiner" in MapReduce.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        func : function\n            the reduce function\n\n        Returns\n        -------\n        dict\n            a dict containing the keys and the aggregated result for each key\n\n        See Also\n        --------\n        :meth:`RDD.reduceByKey`\n        :meth:`RDD.aggregateByKey`\n\n        Examples\n        --------\n        >>> from operator import add\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])\n        >>> sorted(rdd.reduceByKeyLocally(add).items())\n        [(\'a\', 2), (\'b\', 1)]\n        '
        func = fail_on_stopiteration(func)

        def reducePartition(iterator: Iterable[Tuple[K, V]]) -> Iterable[Dict[K, V]]:
            if False:
                print('Hello World!')
            m: Dict[K, V] = {}
            for (k, v) in iterator:
                m[k] = func(m[k], v) if k in m else v
            yield m

        def mergeMaps(m1: Dict[K, V], m2: Dict[K, V]) -> Dict[K, V]:
            if False:
                while True:
                    i = 10
            for (k, v) in m2.items():
                m1[k] = func(m1[k], v) if k in m1 else v
            return m1
        return self.mapPartitions(reducePartition).reduce(mergeMaps)

    def countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[K, int]:
        if False:
            while True:
                i = 10
        '\n        Count the number of elements for each key, and return the result to the\n        master as a dictionary.\n\n        .. versionadded:: 0.7.0\n\n        Returns\n        -------\n        dict\n            a dictionary of (key, count) pairs\n\n        See Also\n        --------\n        :meth:`RDD.collectAsMap`\n        :meth:`RDD.countByValue`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])\n        >>> sorted(rdd.countByKey().items())\n        [(\'a\', 2), (\'b\', 1)]\n        '
        return self.map(lambda x: x[0]).countByValue()

    def join(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int]=None) -> 'RDD[Tuple[K, Tuple[V, U]]]':
        if False:
            return 10
        '\n        Return an RDD containing all pairs of elements with matching keys in\n        `self` and `other`.\n\n        Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n        (k, v1) is in `self` and (k, v2) is in `other`.\n\n        Performs a hash join across the cluster.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing all pairs of elements with matching keys\n\n        See Also\n        --------\n        :meth:`RDD.leftOuterJoin`\n        :meth:`RDD.rightOuterJoin`\n        :meth:`RDD.fullOuterJoin`\n        :meth:`RDD.cogroup`\n        :meth:`RDD.groupWith`\n        :meth:`pyspark.sql.DataFrame.join`\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize([("a", 1), ("b", 4)])\n        >>> rdd2 = sc.parallelize([("a", 2), ("a", 3)])\n        >>> sorted(rdd1.join(rdd2).collect())\n        [(\'a\', (1, 2)), (\'a\', (1, 3))]\n        '
        return python_join(self, other, numPartitions)

    def leftOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int]=None) -> 'RDD[Tuple[K, Tuple[V, Optional[U]]]]':
        if False:
            print('Hello World!')
        '\n        Perform a left outer join of `self` and `other`.\n\n        For each element (k, v) in `self`, the resulting RDD will either\n        contain all pairs (k, (v, w)) for w in `other`, or the pair\n        (k, (v, None)) if no elements in `other` have key k.\n\n        Hash-partitions the resulting RDD into the given number of partitions.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing all pairs of elements with matching keys\n\n        See Also\n        --------\n        :meth:`RDD.join`\n        :meth:`RDD.rightOuterJoin`\n        :meth:`RDD.fullOuterJoin`\n        :meth:`pyspark.sql.DataFrame.join`\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize([("a", 1), ("b", 4)])\n        >>> rdd2 = sc.parallelize([("a", 2)])\n        >>> sorted(rdd1.leftOuterJoin(rdd2).collect())\n        [(\'a\', (1, 2)), (\'b\', (4, None))]\n        '
        return python_left_outer_join(self, other, numPartitions)

    def rightOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int]=None) -> 'RDD[Tuple[K, Tuple[Optional[V], U]]]':
        if False:
            i = 10
            return i + 15
        '\n        Perform a right outer join of `self` and `other`.\n\n        For each element (k, w) in `other`, the resulting RDD will either\n        contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n        if no elements in `self` have key k.\n\n        Hash-partitions the resulting RDD into the given number of partitions.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing all pairs of elements with matching keys\n\n        See Also\n        --------\n        :meth:`RDD.join`\n        :meth:`RDD.leftOuterJoin`\n        :meth:`RDD.fullOuterJoin`\n        :meth:`pyspark.sql.DataFrame.join`\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize([("a", 1), ("b", 4)])\n        >>> rdd2 = sc.parallelize([("a", 2)])\n        >>> sorted(rdd2.rightOuterJoin(rdd1).collect())\n        [(\'a\', (2, 1)), (\'b\', (None, 4))]\n        '
        return python_right_outer_join(self, other, numPartitions)

    def fullOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int]=None) -> 'RDD[Tuple[K, Tuple[Optional[V], Optional[U]]]]':
        if False:
            i = 10
            return i + 15
        '\n        Perform a right outer join of `self` and `other`.\n\n        For each element (k, v) in `self`, the resulting RDD will either\n        contain all pairs (k, (v, w)) for w in `other`, or the pair\n        (k, (v, None)) if no elements in `other` have key k.\n\n        Similarly, for each element (k, w) in `other`, the resulting RDD will\n        either contain all pairs (k, (v, w)) for v in `self`, or the pair\n        (k, (None, w)) if no elements in `self` have key k.\n\n        Hash-partitions the resulting RDD into the given number of partitions.\n\n        .. versionadded:: 1.2.0\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing all pairs of elements with matching keys\n\n        See Also\n        --------\n        :meth:`RDD.join`\n        :meth:`RDD.leftOuterJoin`\n        :meth:`RDD.fullOuterJoin`\n        :meth:`pyspark.sql.DataFrame.join`\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize([("a", 1), ("b", 4)])\n        >>> rdd2 = sc.parallelize([("a", 2), ("c", 8)])\n        >>> sorted(rdd1.fullOuterJoin(rdd2).collect())\n        [(\'a\', (1, 2)), (\'b\', (4, None)), (\'c\', (None, 8))]\n        '
        return python_full_outer_join(self, other, numPartitions)

    def partitionBy(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int], partitionFunc: Callable[[K], int]=portable_hash) -> 'RDD[Tuple[K, V]]':
        if False:
            while True:
                i = 10
        '\n        Return a copy of the RDD partitioned using the specified partitioner.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n        partitionFunc : function, optional, default `portable_hash`\n            function to compute the partition index\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` partitioned using the specified partitioner\n\n        See Also\n        --------\n        :meth:`RDD.repartition`\n        :meth:`RDD.repartitionAndSortWithinPartitions`\n\n        Examples\n        --------\n        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n        >>> sets = pairs.partitionBy(2).glom().collect()\n        >>> len(set(sets[0]).intersection(set(sets[1])))\n        0\n        '
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        partitioner = Partitioner(numPartitions, partitionFunc)
        if self.partitioner == partitioner:
            return self
        outputSerializer = self.ctx._unbatched_serializer
        limit = self._memory_limit() / 2

        def add_shuffle_key(split: int, iterator: Iterable[Tuple[K, V]]) -> Iterable[bytes]:
            if False:
                while True:
                    i = 10
            buckets = defaultdict(list)
            (c, batch) = (0, min(10 * numPartitions, 1000))
            for (k, v) in iterator:
                buckets[partitionFunc(k) % numPartitions].append((k, v))
                c += 1
                if c % 1000 == 0 and get_used_memory() > limit or c > batch:
                    (n, size) = (len(buckets), 0)
                    for split in list(buckets.keys()):
                        yield pack_long(split)
                        d = outputSerializer.dumps(buckets[split])
                        del buckets[split]
                        yield d
                        size += len(d)
                    avg = int(size / n) >> 20
                    if avg < 1:
                        batch = min(sys.maxsize, batch * 1.5)
                    elif avg > 10:
                        batch = max(int(batch / 1.5), 1)
                    c = 0
            for (split, items) in buckets.items():
                yield pack_long(split)
                yield outputSerializer.dumps(items)
        keyed = self.mapPartitionsWithIndex(add_shuffle_key, preservesPartitioning=True)
        keyed._bypass_serializer = True
        assert self.ctx._jvm is not None
        with SCCallSiteSync(self.context):
            pairRDD = self.ctx._jvm.PairwiseRDD(keyed._jrdd.rdd()).asJavaPairRDD()
            jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions, id(partitionFunc))
        jrdd = self.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))
        rdd: 'RDD[Tuple[K, V]]' = RDD(jrdd, self.ctx, BatchedSerializer(outputSerializer))
        rdd.partitioner = partitioner
        return rdd

    def combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[V], U], mergeValue: Callable[[U, V], U], mergeCombiners: Callable[[U, U], U], numPartitions: Optional[int]=None, partitionFunc: Callable[[K], int]=portable_hash) -> 'RDD[Tuple[K, U]]':
        if False:
            while True:
                i = 10
        '\n        Generic function to combine the elements for each key using a custom\n        set of aggregation functions.\n\n        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined\n        type" C.\n\n        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n        modify and return their first argument instead of creating a new C.\n\n        In addition, users can control the partitioning of the output RDD.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        createCombiner : function\n            a function to turns a V into a C\n        mergeValue : function\n            a function to merge a V into a C\n        mergeCombiners : function\n            a function to combine two C\'s into a single one\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n        partitionFunc : function, optional, default `portable_hash`\n            function to compute the partition index\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the keys and the aggregated result for each key\n\n        See Also\n        --------\n        :meth:`RDD.reduceByKey`\n        :meth:`RDD.aggregateByKey`\n        :meth:`RDD.foldByKey`\n        :meth:`RDD.groupByKey`\n\n        Notes\n        -----\n        V and C can be different -- for example, one might group an RDD of type\n            (Int, Int) into an RDD of type (Int, List[Int]).\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 2)])\n        >>> def to_list(a):\n        ...     return [a]\n        ...\n        >>> def append(a, b):\n        ...     a.append(b)\n        ...     return a\n        ...\n        >>> def extend(a, b):\n        ...     a.extend(b)\n        ...     return a\n        ...\n        >>> sorted(rdd.combineByKey(to_list, append, extend).collect())\n        [(\'a\', [1, 2]), (\'b\', [1])]\n        '
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        serializer = self.ctx.serializer
        memory = self._memory_limit()
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)

        def combineLocally(iterator: Iterable[Tuple[K, V]]) -> Iterable[Tuple[K, U]]:
            if False:
                i = 10
                return i + 15
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()
        locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)

        def _mergeCombiners(iterator: Iterable[Tuple[K, U]]) -> Iterable[Tuple[K, U]]:
            if False:
                i = 10
                return i + 15
            merger = ExternalMerger(agg, memory, serializer)
            merger.mergeCombiners(iterator)
            return merger.items()
        return shuffled.mapPartitions(_mergeCombiners, preservesPartitioning=True)

    def aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: U, seqFunc: Callable[[U, V], U], combFunc: Callable[[U, U], U], numPartitions: Optional[int]=None, partitionFunc: Callable[[K], int]=portable_hash) -> 'RDD[Tuple[K, U]]':
        if False:
            return 10
        '\n        Aggregate the values of each key, using given combine functions and a neutral\n        "zero value". This function can return a different result type, U, than the type\n        of the values in this RDD, V. Thus, we need one operation for merging a V into\n        a U and one operation for merging two U\'s, The former operation is used for merging\n        values within a partition, and the latter is used for merging values between\n        partitions. To avoid memory allocation, both of these functions are\n        allowed to modify and return their first argument instead of creating a new U.\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        zeroValue : U\n            the initial value for the accumulated result of each partition\n        seqFunc : function\n            a function to merge a V into a U\n        combFunc : function\n            a function to combine two U\'s into a single one\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n        partitionFunc : function, optional, default `portable_hash`\n            function to compute the partition index\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the keys and the aggregated result for each key\n\n        See Also\n        --------\n        :meth:`RDD.reduceByKey`\n        :meth:`RDD.combineByKey`\n        :meth:`RDD.foldByKey`\n        :meth:`RDD.groupByKey`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 2)])\n        >>> seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n        >>> combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n        >>> sorted(rdd.aggregateByKey((0, 0), seqFunc, combFunc).collect())\n        [(\'a\', (3, 2)), (\'b\', (1, 1))]\n        '

        def createZero() -> U:
            if False:
                print('Hello World!')
            return copy.deepcopy(zeroValue)
        return self.combineByKey(lambda v: seqFunc(createZero(), v), seqFunc, combFunc, numPartitions, partitionFunc)

    def foldByKey(self: 'RDD[Tuple[K, V]]', zeroValue: V, func: Callable[[V, V], V], numPartitions: Optional[int]=None, partitionFunc: Callable[[K], int]=portable_hash) -> 'RDD[Tuple[K, V]]':
        if False:
            return 10
        '\n        Merge the values for each key using an associative function "func"\n        and a neutral "zeroValue" which may be added to the result an\n        arbitrary number of times, and must not change the result\n        (e.g., 0 for addition, or 1 for multiplication.).\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        zeroValue : V\n            the initial value for the accumulated result of each partition\n        func : function\n            a function to combine two V\'s into a single one\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n        partitionFunc : function, optional, default `portable_hash`\n            function to compute the partition index\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the keys and the aggregated result for each key\n\n        See Also\n        --------\n        :meth:`RDD.reduceByKey`\n        :meth:`RDD.combineByKey`\n        :meth:`RDD.aggregateByKey`\n        :meth:`RDD.groupByKey`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])\n        >>> from operator import add\n        >>> sorted(rdd.foldByKey(0, add).collect())\n        [(\'a\', 2), (\'b\', 1)]\n        '

        def createZero() -> V:
            if False:
                while True:
                    i = 10
            return copy.deepcopy(zeroValue)
        return self.combineByKey(lambda v: func(createZero(), v), func, func, numPartitions, partitionFunc)

    def _memory_limit(self) -> int:
        if False:
            print('Hello World!')
        return _parse_memory(self.ctx._conf.get('spark.python.worker.memory', '512m'))

    def groupByKey(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int]=None, partitionFunc: Callable[[K], int]=portable_hash) -> 'RDD[Tuple[K, Iterable[V]]]':
        if False:
            for i in range(10):
                print('nop')
        '\n        Group the values for each key in the RDD into a single sequence.\n        Hash-partitions the resulting RDD with numPartitions partitions.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n        partitionFunc : function, optional, default `portable_hash`\n            function to compute the partition index\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the keys and the grouped result for each key\n\n        See Also\n        --------\n        :meth:`RDD.reduceByKey`\n        :meth:`RDD.combineByKey`\n        :meth:`RDD.aggregateByKey`\n        :meth:`RDD.foldByKey`\n\n        Notes\n        -----\n        If you are grouping in order to perform an aggregation (such as a\n        sum or average) over each key, using reduceByKey or aggregateByKey will\n        provide much better performance.\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])\n        >>> sorted(rdd.groupByKey().mapValues(len).collect())\n        [(\'a\', 2), (\'b\', 1)]\n        >>> sorted(rdd.groupByKey().mapValues(list).collect())\n        [(\'a\', [1, 1]), (\'b\', [1])]\n        '

        def createCombiner(x: V) -> List[V]:
            if False:
                print('Hello World!')
            return [x]

        def mergeValue(xs: List[V], x: V) -> List[V]:
            if False:
                print('Hello World!')
            xs.append(x)
            return xs

        def mergeCombiners(a: List[V], b: List[V]) -> List[V]:
            if False:
                return 10
            a.extend(b)
            return a
        memory = self._memory_limit()
        serializer = self._jrdd_deserializer
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)

        def combine(iterator: Iterable[Tuple[K, V]]) -> Iterable[Tuple[K, List[V]]]:
            if False:
                return 10
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()
        locally_combined = self.mapPartitions(combine, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)

        def groupByKey(it: Iterable[Tuple[K, List[V]]]) -> Iterable[Tuple[K, List[V]]]:
            if False:
                print('Hello World!')
            merger = ExternalGroupBy(agg, memory, serializer)
            merger.mergeCombiners(it)
            return merger.items()
        return shuffled.mapPartitions(groupByKey, True).mapValues(ResultIterable)

    def flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[V], Iterable[U]]) -> 'RDD[Tuple[K, U]]':
        if False:
            while True:
                i = 10
        '\n        Pass each value in the key-value pair RDD through a flatMap function\n        without changing the keys; this also retains the original RDD\'s\n        partitioning.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        f : function\n           a function to turn a V into a sequence of U\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the keys and the flat-mapped value\n\n        See Also\n        --------\n        :meth:`RDD.flatMap`\n        :meth:`RDD.mapValues`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([("a", ["x", "y", "z"]), ("b", ["p", "r"])])\n        >>> def f(x): return x\n        ...\n        >>> rdd.flatMapValues(f).collect()\n        [(\'a\', \'x\'), (\'a\', \'y\'), (\'a\', \'z\'), (\'b\', \'p\'), (\'b\', \'r\')]\n        '

        def flat_map_fn(kv: Tuple[K, V]) -> Iterable[Tuple[K, U]]:
            if False:
                i = 10
                return i + 15
            return ((kv[0], x) for x in f(kv[1]))
        return self.flatMap(flat_map_fn, preservesPartitioning=True)

    def mapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[V], U]) -> 'RDD[Tuple[K, U]]':
        if False:
            i = 10
            return i + 15
        '\n        Pass each value in the key-value pair RDD through a map function\n        without changing the keys; this also retains the original RDD\'s\n        partitioning.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        f : function\n           a function to turn a V into a U\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the keys and the mapped value\n\n        See Also\n        --------\n        :meth:`RDD.map`\n        :meth:`RDD.flatMapValues`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([("a", ["apple", "banana", "lemon"]), ("b", ["grapes"])])\n        >>> def f(x): return len(x)\n        ...\n        >>> rdd.mapValues(f).collect()\n        [(\'a\', 3), (\'b\', 1)]\n        '

        def map_values_fn(kv: Tuple[K, V]) -> Tuple[K, U]:
            if False:
                return 10
            return (kv[0], f(kv[1]))
        return self.map(map_values_fn, preservesPartitioning=True)

    @overload
    def groupWith(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, V1]]') -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[V1]]]]':
        if False:
            print('Hello World!')
        ...

    @overload
    def groupWith(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, V1]]', __o1: 'RDD[Tuple[K, V2]]') -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[V1], ResultIterable[V2]]]]':
        if False:
            i = 10
            return i + 15
        ...

    @overload
    def groupWith(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, V1]]', _o1: 'RDD[Tuple[K, V2]]', _o2: 'RDD[Tuple[K, V3]]') -> 'RDD[\n        Tuple[\n            K,\n            Tuple[\n                ResultIterable[V],\n                ResultIterable[V1],\n                ResultIterable[V2],\n                ResultIterable[V3],\n            ],\n        ]\n    ]':
        if False:
            return 10
        ...

    def groupWith(self: 'RDD[Tuple[Any, Any]]', other: 'RDD[Tuple[Any, Any]]', *others: 'RDD[Tuple[Any, Any]]') -> 'RDD[Tuple[Any, Tuple[ResultIterable[Any], ...]]]':
        if False:
            i = 10
            return i + 15
        '\n        Alias for cogroup but with support for multiple RDDs.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n        others : :class:`RDD`\n            other :class:`RDD`\\s\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the keys and cogrouped values\n\n        See Also\n        --------\n        :meth:`RDD.cogroup`\n        :meth:`RDD.join`\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize([("a", 5), ("b", 6)])\n        >>> rdd2 = sc.parallelize([("a", 1), ("b", 4)])\n        >>> rdd3 = sc.parallelize([("a", 2)])\n        >>> rdd4 = sc.parallelize([("b", 42)])\n        >>> [(x, tuple(map(list, y))) for x, y in\n        ...     sorted(list(rdd1.groupWith(rdd2, rdd3, rdd4).collect()))]\n        [(\'a\', ([5], [1], [2], [])), (\'b\', ([6], [4], [], [42]))]\n\n        '
        return python_cogroup((self, other) + others, numPartitions=None)

    def cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int]=None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]':
        if False:
            for i in range(10):
                print('nop')
        '\n        For each key k in `self` or `other`, return a resulting RDD that\n        contains a tuple with the list of values for that key in `self` as\n        well as `other`.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the keys and cogrouped values\n\n        See Also\n        --------\n        :meth:`RDD.groupWith`\n        :meth:`RDD.join`\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize([("a", 1), ("b", 4)])\n        >>> rdd2 = sc.parallelize([("a", 2)])\n        >>> [(x, tuple(map(list, y))) for x, y in sorted(list(rdd1.cogroup(rdd2).collect()))]\n        [(\'a\', ([1], [2])), (\'b\', ([4], []))]\n        '
        return python_cogroup((self, other), numPartitions)

    def sampleByKey(self: 'RDD[Tuple[K, V]]', withReplacement: bool, fractions: Dict[K, Union[float, int]], seed: Optional[int]=None) -> 'RDD[Tuple[K, V]]':
        if False:
            return 10
        '\n        Return a subset of this RDD sampled by key (via stratified sampling).\n        Create a sample of this RDD using variable sampling rates for\n        different keys as specified by fractions, a key to sampling rate map.\n\n        .. versionadded:: 0.7.0\n\n        Parameters\n        ----------\n        withReplacement : bool\n            whether to sample with or without replacement\n        fractions : dict\n            map of specific keys to sampling rates\n        seed : int, optional\n            seed for the random number generator\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the stratified sampling result\n\n        See Also\n        --------\n        :meth:`RDD.sample`\n\n        Examples\n        --------\n        >>> fractions = {"a": 0.2, "b": 0.1}\n        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n        >>> 100 < len(sample["a"]) < 300 and 50 < len(sample["b"]) < 150\n        True\n        >>> max(sample["a"]) <= 999 and min(sample["a"]) >= 0\n        True\n        >>> max(sample["b"]) <= 999 and min(sample["b"]) >= 0\n        True\n        '
        for fraction in fractions.values():
            assert fraction >= 0.0, 'Negative fraction value: %s' % fraction
        return self.mapPartitionsWithIndex(RDDStratifiedSampler(withReplacement, fractions, seed).func, True)

    def subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Optional[int]=None) -> 'RDD[Tuple[K, V]]':
        if False:
            while True:
                i = 10
        '\n        Return each (key, value) pair in `self` that has no pair with matching\n        key in `other`.\n\n        .. versionadded:: 0.9.1\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` with the pairs from this whose keys are not in `other`\n\n        See Also\n        --------\n        :meth:`RDD.subtract`\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 2)])\n        >>> rdd2 = sc.parallelize([("a", 3), ("c", None)])\n        >>> sorted(rdd1.subtractByKey(rdd2).collect())\n        [(\'b\', 4), (\'b\', 5)]\n        '

        def filter_func(pair: Tuple[K, Tuple[V, Any]]) -> bool:
            if False:
                return 10
            (key, (val1, val2)) = pair
            return val1 and (not val2)
        return self.cogroup(other, numPartitions).filter(filter_func).flatMapValues(lambda x: x[0])

    def subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Optional[int]=None) -> 'RDD[T]':
        if False:
            for i in range(10):
                print('nop')
        '\n        Return each value in `self` that is not contained in `other`.\n\n        .. versionadded:: 0.9.1\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` with the elements from this that are not in `other`\n\n        See Also\n        --------\n        :meth:`RDD.subtractByKey`\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 3)])\n        >>> rdd2 = sc.parallelize([("a", 3), ("c", None)])\n        >>> sorted(rdd1.subtract(rdd2).collect())\n        [(\'a\', 1), (\'b\', 4), (\'b\', 5)]\n        '
        rdd = other.map(lambda x: (x, True))
        return self.map(lambda x: (x, True)).subtractByKey(rdd, numPartitions).keys()

    def keyBy(self: 'RDD[T]', f: Callable[[T], K]) -> 'RDD[Tuple[K, T]]':
        if False:
            return 10
        '\n        Creates tuples of the elements in this RDD by applying `f`.\n\n        .. versionadded:: 0.9.1\n\n        Parameters\n        ----------\n        f : function\n            a function to compute the key\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` with the elements from this that are not in `other`\n\n        See Also\n        --------\n        :meth:`RDD.map`\n        :meth:`RDD.keys`\n        :meth:`RDD.values`\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n        >>> rdd2 = sc.parallelize(zip(range(0,5), range(0,5)))\n        >>> [(x, list(map(list, y))) for x, y in sorted(rdd1.cogroup(rdd2).collect())]\n        [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n        '
        return self.map(lambda x: (f(x), x))

    def repartition(self: 'RDD[T]', numPartitions: int) -> 'RDD[T]':
        if False:
            while True:
                i = 10
        '\n         Return a new RDD that has exactly numPartitions partitions.\n\n         Can increase or decrease the level of parallelism in this RDD.\n         Internally, this uses a shuffle to redistribute data.\n         If you are decreasing the number of partitions in this RDD, consider\n         using `coalesce`, which can avoid performing a shuffle.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` with exactly numPartitions partitions\n\n        See Also\n        --------\n        :meth:`RDD.coalesce`\n        :meth:`RDD.partitionBy`\n        :meth:`RDD.repartitionAndSortWithinPartitions`\n\n        Examples\n        --------\n         >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n         >>> sorted(rdd.glom().collect())\n         [[1], [2, 3], [4, 5], [6, 7]]\n         >>> len(rdd.repartition(2).glom().collect())\n         2\n         >>> len(rdd.repartition(10).glom().collect())\n         10\n        '
        return self.coalesce(numPartitions, shuffle=True)

    def coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool=False) -> 'RDD[T]':
        if False:
            for i in range(10):
                print('nop')
        '\n        Return a new RDD that is reduced into `numPartitions` partitions.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        numPartitions : int, optional\n            the number of partitions in new :class:`RDD`\n        shuffle : bool, optional, default False\n            whether to add a shuffle step\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` that is reduced into `numPartitions` partitions\n\n        See Also\n        --------\n        :meth:`RDD.repartition`\n\n        Examples\n        --------\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n        [[1], [2, 3], [4, 5]]\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n        [[1, 2, 3, 4, 5]]\n        '
        if not numPartitions > 0:
            raise ValueError('Number of partitions must be positive.')
        if shuffle:
            batchSize = min(10, self.ctx._batchSize or 1024)
            ser = BatchedSerializer(CPickleSerializer(), batchSize)
            selfCopy = self._reserialize(ser)
            jrdd_deserializer = selfCopy._jrdd_deserializer
            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)
        else:
            jrdd_deserializer = self._jrdd_deserializer
            jrdd = self._jrdd.coalesce(numPartitions, shuffle)
        return RDD(jrdd, self.ctx, jrdd_deserializer)

    def zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]':
        if False:
            print('Hello World!')
        '\n        Zips this RDD with another one, returning key-value pairs with the\n        first element in each RDD second element in each RDD, etc. Assumes\n        that the two RDDs have the same number of partitions and the same\n        number of elements in each partition (e.g. one was made through\n        a map on the other).\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        other : :class:`RDD`\n            another :class:`RDD`\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the zipped key-value pairs\n\n        See Also\n        --------\n        :meth:`RDD.zipWithIndex`\n        :meth:`RDD.zipWithUniqueId`\n\n        Examples\n        --------\n        >>> rdd1 = sc.parallelize(range(0,5))\n        >>> rdd2 = sc.parallelize(range(1000, 1005))\n        >>> rdd1.zip(rdd2).collect()\n        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n        '

        def get_batch_size(ser: Serializer) -> int:
            if False:
                i = 10
                return i + 15
            if isinstance(ser, BatchedSerializer):
                return ser.batchSize
            return 1

        def batch_as(rdd: 'RDD[V]', batchSize: int) -> 'RDD[V]':
            if False:
                return 10
            return rdd._reserialize(BatchedSerializer(CPickleSerializer(), batchSize))
        my_batch = get_batch_size(self._jrdd_deserializer)
        other_batch = get_batch_size(other._jrdd_deserializer)
        if my_batch != other_batch or not my_batch:
            batchSize = min(my_batch, other_batch)
            if batchSize <= 0:
                batchSize = 100
            other = batch_as(other, batchSize)
            self = batch_as(self, batchSize)
        if self.getNumPartitions() != other.getNumPartitions():
            raise ValueError('Can only zip with RDD which has the same number of partitions')
        pairRDD = self._jrdd.zip(other._jrdd)
        deserializer = PairDeserializer(self._jrdd_deserializer, other._jrdd_deserializer)
        return RDD(pairRDD, self.ctx, deserializer)

    def zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]':
        if False:
            return 10
        '\n        Zips this RDD with its element indices.\n\n        The ordering is first based on the partition index and then the\n        ordering of items within each partition. So the first item in\n        the first partition gets index 0, and the last item in the last\n        partition receives the largest index.\n\n        This method needs to trigger a spark job when this RDD contains\n        more than one partitions.\n\n        .. versionadded:: 1.2.0\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the zipped key-index pairs\n\n        See Also\n        --------\n        :meth:`RDD.zip`\n        :meth:`RDD.zipWithUniqueId`\n\n        Examples\n        --------\n        >>> sc.parallelize(["a", "b", "c", "d"], 3).zipWithIndex().collect()\n        [(\'a\', 0), (\'b\', 1), (\'c\', 2), (\'d\', 3)]\n        '
        starts = [0]
        if self.getNumPartitions() > 1:
            nums = self.mapPartitions(lambda it: [sum((1 for i in it))]).collect()
            for i in range(len(nums) - 1):
                starts.append(starts[-1] + nums[i])

        def func(k: int, it: Iterable[T]) -> Iterable[Tuple[T, int]]:
            if False:
                return 10
            for (i, v) in enumerate(it, starts[k]):
                yield (v, i)
        return self.mapPartitionsWithIndex(func)

    def zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]':
        if False:
            i = 10
            return i + 15
        '\n        Zips this RDD with generated unique Long ids.\n\n        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n        n is the number of partitions. So there may exist gaps, but this\n        method won\'t trigger a spark job, which is different from\n        :meth:`zipWithIndex`.\n\n        .. versionadded:: 1.2.0\n\n        Returns\n        -------\n        :class:`RDD`\n            a :class:`RDD` containing the zipped key-UniqueId pairs\n\n        See Also\n        --------\n        :meth:`RDD.zip`\n        :meth:`RDD.zipWithIndex`\n\n        Examples\n        --------\n        >>> sc.parallelize(["a", "b", "c", "d", "e"], 3).zipWithUniqueId().collect()\n        [(\'a\', 0), (\'b\', 1), (\'c\', 4), (\'d\', 2), (\'e\', 5)]\n        '
        n = self.getNumPartitions()

        def func(k: int, it: Iterable[T]) -> Iterable[Tuple[T, int]]:
            if False:
                for i in range(10):
                    print('nop')
            for (i, v) in enumerate(it):
                yield (v, i * n + k)
        return self.mapPartitionsWithIndex(func)

    def name(self) -> Optional[str]:
        if False:
            return 10
        '\n        Return the name of this RDD.\n\n        .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        str\n            :class:`RDD` name\n\n        See Also\n        --------\n        :meth:`RDD.setName`\n\n        Examples\n        --------\n        >>> rdd = sc.range(5)\n        >>> rdd.name() == None\n        True\n        '
        n = self._jrdd.name()
        return n if n else None

    def setName(self: 'RDD[T]', name: str) -> 'RDD[T]':
        if False:
            return 10
        "\n        Assign a name to this RDD.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        name : str\n            new name\n\n        Returns\n        -------\n        :class:`RDD`\n            the same :class:`RDD` with name updated\n\n        See Also\n        --------\n        :meth:`RDD.name`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 2])\n        >>> rdd.setName('I am an RDD').name()\n        'I am an RDD'\n        "
        self._jrdd.setName(name)
        return self

    def toDebugString(self) -> Optional[bytes]:
        if False:
            for i in range(10):
                print('nop')
        "\n        A description of this RDD and its recursive dependencies for debugging.\n\n        .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        bytes\n            debugging information of this :class:`RDD`\n\n        Examples\n        --------\n        >>> rdd = sc.range(5)\n        >>> rdd.toDebugString()\n        b'...PythonRDD...ParallelCollectionRDD...'\n        "
        debug_string = self._jrdd.toDebugString()
        return debug_string.encode('utf-8') if debug_string else None

    def getStorageLevel(self) -> StorageLevel:
        if False:
            for i in range(10):
                print('nop')
        "\n        Get the RDD's current storage level.\n\n        .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        :class:`StorageLevel`\n            current :class:`StorageLevel`\n\n        See Also\n        --------\n        :meth:`RDD.name`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1,2])\n        >>> rdd.getStorageLevel()\n        StorageLevel(False, False, False, False, 1)\n        >>> print(rdd.getStorageLevel())\n        Serialized 1x Replicated\n        "
        java_storage_level = self._jrdd.getStorageLevel()
        storage_level = StorageLevel(java_storage_level.useDisk(), java_storage_level.useMemory(), java_storage_level.useOffHeap(), java_storage_level.deserialized(), java_storage_level.replication())
        return storage_level

    def _defaultReducePartitions(self) -> int:
        if False:
            for i in range(10):
                print('nop')
        "\n        Returns the default number of partitions to use during reduce tasks (e.g., groupBy).\n        If spark.default.parallelism is set, then we'll use the value from SparkContext\n        defaultParallelism, otherwise we'll use the number of partitions in this RDD.\n\n        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce\n        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will\n        be inherent.\n        "
        if self.ctx._conf.contains('spark.default.parallelism'):
            return self.ctx.defaultParallelism
        else:
            return self.getNumPartitions()

    def lookup(self: 'RDD[Tuple[K, V]]', key: K) -> List[V]:
        if False:
            for i in range(10):
                print('nop')
        "\n        Return the list of values in the RDD for key `key`. This operation\n        is done efficiently if the RDD has a known partitioner by only\n        searching the partition that the key maps to.\n\n        .. versionadded:: 1.2.0\n\n        Parameters\n        ----------\n        key : K\n            the key to look up\n\n        Returns\n        -------\n        list\n            the list of values in the :class:`RDD` for key `key`\n\n        Examples\n        --------\n        >>> l = range(1000)\n        >>> rdd = sc.parallelize(zip(l, l), 10)\n        >>> rdd.lookup(42)  # slow\n        [42]\n        >>> sorted = rdd.sortByKey()\n        >>> sorted.lookup(42)  # fast\n        [42]\n        >>> sorted.lookup(1024)\n        []\n        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n        >>> list(rdd2.lookup(('a', 'b'))[0])\n        ['c']\n        "
        values = self.filter(lambda kv: kv[0] == key).values()
        if self.partitioner is not None:
            return self.ctx.runJob(values, lambda x: x, [self.partitioner(key)])
        return values.collect()

    def _to_java_object_rdd(self) -> 'JavaObject':
        if False:
            while True:
                i = 10
        'Return a JavaRDD of Object by unpickling\n\n        It will convert each Python object into Java object by Pickle, whenever the\n        RDD is serialized in batch or not.\n        '
        rdd = self._pickled()
        assert self.ctx._jvm is not None
        return self.ctx._jvm.SerDeUtil.pythonToJava(rdd._jrdd, True)

    def countApprox(self, timeout: int, confidence: float=0.95) -> int:
        if False:
            print('Hello World!')
        '\n        Approximate version of count() that returns a potentially incomplete\n        result within a timeout, even if not all tasks have finished.\n\n        .. versionadded:: 1.2.0\n\n        Parameters\n        ----------\n        timeout : int\n            maximum time to wait for the job, in milliseconds\n        confidence : float\n            the desired statistical confidence in the result\n\n        Returns\n        -------\n        int\n            a potentially incomplete result, with error bounds\n\n        See Also\n        --------\n        :meth:`RDD.count`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> rdd.countApprox(1000, 1.0)\n        1000\n        '
        drdd = self.mapPartitions(lambda it: [float(sum((1 for i in it)))])
        return int(drdd.sumApprox(timeout, confidence))

    def sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float=0.95) -> BoundedFloat:
        if False:
            for i in range(10):
                print('nop')
        '\n        Approximate operation to return the sum within a timeout\n        or meet the confidence.\n\n        .. versionadded:: 1.2.0\n\n        Parameters\n        ----------\n        timeout : int\n            maximum time to wait for the job, in milliseconds\n        confidence : float\n            the desired statistical confidence in the result\n\n        Returns\n        -------\n        :class:`BoundedFloat`\n            a potentially incomplete result, with error bounds\n\n        See Also\n        --------\n        :meth:`RDD.sum`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> r = sum(range(1000))\n        >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n        True\n        '
        jrdd = self.mapPartitions(lambda it: [float(sum(it))])._to_java_object_rdd()
        assert self.ctx._jvm is not None
        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())
        r = jdrdd.sumApprox(timeout, confidence).getFinalValue()
        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())

    def meanApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float=0.95) -> BoundedFloat:
        if False:
            i = 10
            return i + 15
        '\n        Approximate operation to return the mean within a timeout\n        or meet the confidence.\n\n        .. versionadded:: 1.2.0\n\n        Parameters\n        ----------\n        timeout : int\n            maximum time to wait for the job, in milliseconds\n        confidence : float\n            the desired statistical confidence in the result\n\n        Returns\n        -------\n        :class:`BoundedFloat`\n            a potentially incomplete result, with error bounds\n\n        See Also\n        --------\n        :meth:`RDD.mean`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> r = sum(range(1000)) / 1000.0\n        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n        True\n        '
        jrdd = self.map(float)._to_java_object_rdd()
        assert self.ctx._jvm is not None
        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())
        r = jdrdd.meanApprox(timeout, confidence).getFinalValue()
        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())

    def countApproxDistinct(self: 'RDD[T]', relativeSD: float=0.05) -> int:
        if False:
            print('Hello World!')
        '\n        Return approximate number of distinct elements in the RDD.\n\n        .. versionadded:: 1.2.0\n\n        Parameters\n        ----------\n        relativeSD : float, optional\n            Relative accuracy. Smaller values create\n            counters that require more space.\n            It must be greater than 0.000017.\n\n        Returns\n        -------\n        int\n            approximate number of distinct elements\n\n        See Also\n        --------\n        :meth:`RDD.distinct`\n\n        Notes\n        -----\n        The algorithm used is based on streamlib\'s implementation of\n        `"HyperLogLog in Practice: Algorithmic Engineering of a State\n        of The Art Cardinality Estimation Algorithm", available here\n        <https://doi.org/10.1145/2452376.2452456>`_.\n\n        Examples\n        --------\n        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n        >>> 900 < n < 1100\n        True\n        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n        >>> 16 < n < 24\n        True\n        '
        if relativeSD < 1.7e-05:
            raise ValueError('relativeSD should be greater than 0.000017')
        hashRDD = self.map(lambda x: portable_hash(x) & 4294967295)
        return hashRDD._to_java_object_rdd().countApproxDistinct(relativeSD)

    def toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool=False) -> Iterator[T]:
        if False:
            while True:
                i = 10
        '\n        Return an iterator that contains all of the elements in this RDD.\n        The iterator will consume as much memory as the largest partition in this RDD.\n        With prefetch it may consume up to the memory of the 2 largest partitions.\n\n        .. versionadded:: 1.3.0\n\n        Parameters\n        ----------\n        prefetchPartitions : bool, optional\n            If Spark should pre-fetch the next partition\n            before it is needed.\n\n        Returns\n        -------\n        :class:`collections.abc.Iterator`\n            an iterator that contains all of the elements in this :class:`RDD`\n\n        See Also\n        --------\n        :meth:`RDD.collect`\n        :meth:`pyspark.sql.DataFrame.toLocalIterator`\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize(range(10))\n        >>> [x for x in rdd.toLocalIterator()]\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        '
        assert self.ctx._jvm is not None
        with SCCallSiteSync(self.context):
            sock_info = self.ctx._jvm.PythonRDD.toLocalIteratorAndServe(self._jrdd.rdd(), prefetchPartitions)
        return _local_iterator_from_socket(sock_info, self._jrdd_deserializer)

    def barrier(self: 'RDD[T]') -> 'RDDBarrier[T]':
        if False:
            print('Hello World!')
        '\n        Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n        In case of a task failure, instead of only restarting the failed task, Spark will abort the\n        entire stage and relaunch all tasks for this stage.\n        The barrier execution mode feature is experimental and it only handles limited scenarios.\n        Please read the linked SPIP and design docs to understand the limitations and future plans.\n\n        .. versionadded:: 2.4.0\n\n        Returns\n        -------\n        :class:`RDDBarrier`\n            instance that provides actions within a barrier stage.\n\n        See Also\n        --------\n        :class:`pyspark.BarrierTaskContext`\n\n        Notes\n        -----\n        For additional information see\n\n        - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n        - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n\n        This API is experimental\n        '
        return RDDBarrier(self)

    def _is_barrier(self) -> bool:
        if False:
            for i in range(10):
                print('nop')
        '\n        Whether this RDD is in a barrier stage.\n        '
        return self._jrdd.rdd().isBarrier()

    def withResources(self: 'RDD[T]', profile: ResourceProfile) -> 'RDD[T]':
        if False:
            for i in range(10):
                print('nop')
        '\n        Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n        This is only supported on certain cluster managers and currently requires dynamic\n        allocation to be enabled. It will result in new executors with the resources specified\n        being acquired to calculate the RDD.\n\n        .. versionadded:: 3.1.0\n\n        Parameters\n        ----------\n        profile : :class:`pyspark.resource.ResourceProfile`\n            a resource profile\n\n        Returns\n        -------\n        :class:`RDD`\n            the same :class:`RDD` with user specified profile\n\n        See Also\n        --------\n        :meth:`RDD.getResourceProfile`\n\n        Notes\n        -----\n        This API is experimental\n        '
        self.has_resource_profile = True
        if profile._java_resource_profile is not None:
            jrp = profile._java_resource_profile
        else:
            assert self.ctx._jvm is not None
            builder = self.ctx._jvm.org.apache.spark.resource.ResourceProfileBuilder()
            ereqs = ExecutorResourceRequests(self.ctx._jvm, profile._executor_resource_requests)
            treqs = TaskResourceRequests(self.ctx._jvm, profile._task_resource_requests)
            builder.require(ereqs._java_executor_resource_requests)
            builder.require(treqs._java_task_resource_requests)
            jrp = builder.build()
        self._jrdd.withResources(jrp)
        return self

    def getResourceProfile(self) -> Optional[ResourceProfile]:
        if False:
            print('Hello World!')
        "\n        Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n        if it wasn't specified.\n\n        .. versionadded:: 3.1.0\n\n        Returns\n        -------\n        class:`pyspark.resource.ResourceProfile`\n            The user specified profile or None if none were specified\n\n        See Also\n        --------\n        :meth:`RDD.withResources`\n\n        Notes\n        -----\n        This API is experimental\n        "
        rp = self._jrdd.getResourceProfile()
        if rp is not None:
            return ResourceProfile(_java_resource_profile=rp)
        else:
            return None

    @overload
    def toDF(self: 'RDD[RowLike]', schema: Optional[Union[List[str], Tuple[str, ...]]]=None, sampleRatio: Optional[float]=None) -> 'DataFrame':
        if False:
            return 10
        ...

    @overload
    def toDF(self: 'RDD[RowLike]', schema: Optional[Union['StructType', str]]=None) -> 'DataFrame':
        if False:
            i = 10
            return i + 15
        ...

    @overload
    def toDF(self: 'RDD[AtomicValue]', schema: Union['AtomicType', str]) -> 'DataFrame':
        if False:
            while True:
                i = 10
        ...

    def toDF(self: 'RDD[Any]', schema: Optional[Any]=None, sampleRatio: Optional[float]=None) -> 'DataFrame':
        if False:
            while True:
                i = 10
        raise PySparkRuntimeError(error_class='CALL_BEFORE_INITIALIZE', message_parameters={'func_name': 'RDD.toDF', 'object': 'SparkSession'})

def _prepare_for_python_RDD(sc: 'SparkContext', command: Any) -> Tuple[bytes, Any, Any, Any]:
    if False:
        print('Hello World!')
    ser = CloudPickleSerializer()
    pickled_command = ser.dumps(command)
    assert sc._jvm is not None
    if len(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):
        broadcast = sc.broadcast(pickled_command)
        pickled_command = ser.dumps(broadcast)
    broadcast_vars = [x._jbroadcast for x in sc._pickled_broadcast_vars]
    sc._pickled_broadcast_vars.clear()
    return (pickled_command, broadcast_vars, sc.environment, sc._python_includes)

def _wrap_function(sc: 'SparkContext', func: Callable, deserializer: Any, serializer: Any, profiler: Any=None) -> 'JavaObject':
    if False:
        i = 10
        return i + 15
    assert deserializer, 'deserializer should not be empty'
    assert serializer, 'serializer should not be empty'
    command = (func, profiler, deserializer, serializer)
    (pickled_command, broadcast_vars, env, includes) = _prepare_for_python_RDD(sc, command)
    assert sc._jvm is not None
    return sc._jvm.SimplePythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec, sc.pythonVer, broadcast_vars, sc._javaAccumulator)

class RDDBarrier(Generic[T]):
    """
    Wraps an RDD in a barrier stage, which forces Spark to launch tasks of this stage together.
    :class:`RDDBarrier` instances are created by :meth:`RDD.barrier`.

    .. versionadded:: 2.4.0

    Notes
    -----
    This API is experimental
    """

    def __init__(self, rdd: RDD[T]):
        if False:
            while True:
                i = 10
        self.rdd = rdd

    def mapPartitions(self, f: Callable[[Iterable[T]], Iterable[U]], preservesPartitioning: bool=False) -> RDD[U]:
        if False:
            i = 10
            return i + 15
        '\n        Returns a new RDD by applying a function to each partition of the wrapped RDD,\n        where tasks are launched together in a barrier stage.\n        The interface is the same as :meth:`RDD.mapPartitions`.\n        Please see the API doc there.\n\n        .. versionadded:: 2.4.0\n\n        Parameters\n        ----------\n        f : function\n           a function to run on each partition of the RDD\n        preservesPartitioning : bool, optional, default False\n            indicates whether the input function preserves the partitioner,\n            which should be False unless this is a pair RDD and the input\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` by applying a function to each partition\n\n        See Also\n        --------\n        :meth:`RDD.mapPartitions`\n\n        Notes\n        -----\n        This API is experimental\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> def f(iterator): yield sum(iterator)\n        ...\n        >>> barrier = rdd.barrier()\n        >>> barrier\n        <pyspark.rdd.RDDBarrier ...>\n        >>> barrier.mapPartitions(f).collect()\n        [3, 7]\n        '

        def func(s: int, iterator: Iterable[T]) -> Iterable[U]:
            if False:
                while True:
                    i = 10
            return f(iterator)
        return PipelinedRDD(self.rdd, func, preservesPartitioning, isFromBarrier=True)

    def mapPartitionsWithIndex(self, f: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool=False) -> RDD[U]:
        if False:
            print('Hello World!')
        '\n        Returns a new RDD by applying a function to each partition of the wrapped RDD, while\n        tracking the index of the original partition. And all tasks are launched together\n        in a barrier stage.\n        The interface is the same as :meth:`RDD.mapPartitionsWithIndex`.\n        Please see the API doc there.\n\n        .. versionadded:: 3.0.0\n\n        Parameters\n        ----------\n        f : function\n           a function to run on each partition of the RDD\n        preservesPartitioning : bool, optional, default False\n            indicates whether the input function preserves the partitioner,\n            which should be False unless this is a pair RDD and the input\n\n        Returns\n        -------\n        :class:`RDD`\n            a new :class:`RDD` by applying a function to each partition\n\n        See Also\n        --------\n        :meth:`RDD.mapPartitionsWithIndex`\n\n        Notes\n        -----\n        This API is experimental\n\n        Examples\n        --------\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n        >>> def f(splitIndex, iterator): yield splitIndex\n        ...\n        >>> barrier = rdd.barrier()\n        >>> barrier\n        <pyspark.rdd.RDDBarrier ...>\n        >>> barrier.mapPartitionsWithIndex(f).sum()\n        6\n        '
        return PipelinedRDD(self.rdd, f, preservesPartitioning, isFromBarrier=True)

class PipelinedRDD(RDD[U], Generic[T, U]):
    """
    Examples
    --------
    Pipelined maps:

    >>> rdd = sc.parallelize([1, 2, 3, 4])
    >>> rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()
    [4, 8, 12, 16]
    >>> rdd.map(lambda x: 2 * x).map(lambda x: 2 * x).collect()
    [4, 8, 12, 16]

    Pipelined reduces:

    >>> from operator import add
    >>> rdd.map(lambda x: 2 * x).reduce(add)
    20
    >>> rdd.flatMap(lambda x: [x, x]).reduce(add)
    20
    """

    def __init__(self, prev: RDD[T], func: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool=False, isFromBarrier: bool=False):
        if False:
            return 10
        if not isinstance(prev, PipelinedRDD) or not prev._is_pipelinable():
            self.func = func
            self.preservesPartitioning = preservesPartitioning
            self._prev_jrdd = prev._jrdd
            self._prev_jrdd_deserializer = prev._jrdd_deserializer
        else:
            prev_func: Callable[[int, Iterable[V]], Iterable[T]] = prev.func

            def pipeline_func(split: int, iterator: Iterable[V]) -> Iterable[U]:
                if False:
                    print('Hello World!')
                return func(split, prev_func(split, iterator))
            self.func = pipeline_func
            self.preservesPartitioning = prev.preservesPartitioning and preservesPartitioning
            self._prev_jrdd = prev._prev_jrdd
            self._prev_jrdd_deserializer = prev._prev_jrdd_deserializer
        self.is_cached = False
        self.has_resource_profile = False
        self.is_checkpointed = False
        self.ctx = prev.ctx
        self.prev = prev
        self._jrdd_val: Optional['JavaObject'] = None
        self._id = None
        self._jrdd_deserializer = self.ctx.serializer
        self._bypass_serializer = False
        self.partitioner = prev.partitioner if self.preservesPartitioning else None
        self.is_barrier = isFromBarrier or prev._is_barrier()

    def getNumPartitions(self) -> int:
        if False:
            while True:
                i = 10
        return self._prev_jrdd.partitions().size()

    @property
    def _jrdd(self) -> 'JavaObject':
        if False:
            while True:
                i = 10
        if self._jrdd_val:
            return self._jrdd_val
        if self._bypass_serializer:
            self._jrdd_deserializer = NoOpSerializer()
        if self.ctx.profiler_collector and self.ctx._conf.get('spark.python.profile', 'false') == 'true':
            profiler = self.ctx.profiler_collector.new_profiler(self.ctx)
        else:
            profiler = None
        wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer, self._jrdd_deserializer, profiler)
        assert self.ctx._jvm is not None
        python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func, self.preservesPartitioning, self.is_barrier)
        self._jrdd_val = python_rdd.asJavaRDD()
        if profiler:
            assert self._jrdd_val is not None
            self._id = self._jrdd_val.id()
            self.ctx.profiler_collector.add_profiler(self._id, profiler)
        return self._jrdd_val

    def id(self) -> int:
        if False:
            while True:
                i = 10
        if self._id is None:
            self._id = self._jrdd.id()
        return self._id

    def _is_pipelinable(self) -> bool:
        if False:
            return 10
        return not (self.is_cached or self.is_checkpointed or self.has_resource_profile)

    def _is_barrier(self) -> bool:
        if False:
            return 10
        return self.is_barrier

def _test() -> None:
    if False:
        while True:
            i = 10
    import doctest
    import tempfile
    from pyspark.context import SparkContext
    tmp_dir = tempfile.TemporaryDirectory()
    globs = globals().copy()
    globs['sc'] = SparkContext('local[4]', 'PythonTest')
    globs['sc'].setCheckpointDir(tmp_dir.name)
    (failure_count, test_count) = doctest.testmod(globs=globs, optionflags=doctest.ELLIPSIS)
    globs['sc'].stop()
    tmp_dir.cleanup()
    if failure_count:
        tmp_dir.cleanup()
        sys.exit(-1)
if __name__ == '__main__':
    _test()