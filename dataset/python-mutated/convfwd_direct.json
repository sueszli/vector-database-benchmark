[
    {
        "func_name": "get_template_op",
        "original": "@autotvm.template\ndef get_template_op(**kargs):\n    N = op_attributes['N']\n    CI = op_attributes['C']\n    H = op_attributes['H']\n    W = op_attributes['W']\n    H = op_attributes['H']\n    CO = op_attributes['F']\n    KH = KW = op_attributes['K']\n    stride = op_attributes['ST']\n    padding = op_attributes['PD']\n    dilation = 1\n    data = tvm.placeholder((N, CI, H, W), name='data')\n    kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n    conv = topi.nn.conv2d_nchw(data, kernel, (stride, stride), (padding, padding), dilation=1, out_dtype='float32')\n    s = tvm.create_schedule([conv.op])\n    cfg = autotvm.get_config()\n    (n, f, y, x) = s[conv].op.axis\n    (rc, ry, rx) = s[conv].op.reduce_axis\n    cfg.define_split('tile_f', f, num_outputs=4)\n    cfg.define_split('tile_y', y, num_outputs=4)\n    cfg.define_split('tile_x', x, num_outputs=4)\n    cfg.define_split('tile_rc', rc, num_outputs=2)\n    cfg.define_split('tile_ry', ry, num_outputs=2)\n    cfg.define_split('tile_rx', rx, num_outputs=2)\n    cfg.define_knob('auto_unroll_max_step', [0, 125, 256])\n    target = tvm.target.current_target()\n    if target.target_name in ['nvptx', 'rocm']:\n        cfg.define_knob('unroll_explicit', [1])\n    else:\n        cfg.define_knob('unroll_explicit', [0, 1])\n    (pad_data, kernel) = s[conv].op.input_tensors\n    s[pad_data].compute_inline()\n    if isinstance(kernel.op, tvm.tensor.ComputeOp) and 'dilate' in kernel.op.tag:\n        s[kernel].compute_inline()\n    if conv.op in s.outputs:\n        output = conv\n        OL = s.cache_write(conv, 'local')\n    else:\n        output = s.outputs[0].output(0)\n        s[conv].set_scope('local')\n        OL = conv\n    AA = s.cache_read(pad_data, 'shared', [OL])\n    WW = s.cache_read(kernel, 'shared', [OL])\n    (n, f, y, x) = s[output].op.axis\n    (kernel_scope, n) = s[output].split(n, nparts=1)\n    (bf, vf, tf, fi) = cfg['tile_f'].apply(s, output, f)\n    (by, vy, ty, yi) = cfg['tile_y'].apply(s, output, y)\n    (bx, vx, tx, xi) = cfg['tile_x'].apply(s, output, x)\n    bf = s[output].fuse(n, bf)\n    s[output].bind(bf, tvm.thread_axis('blockIdx.z'))\n    s[output].bind(by, tvm.thread_axis('blockIdx.y'))\n    s[output].bind(bx, tvm.thread_axis('blockIdx.x'))\n    s[output].bind(vf, tvm.thread_axis('vthread'))\n    s[output].bind(vy, tvm.thread_axis('vthread'))\n    s[output].bind(vx, tvm.thread_axis('vthread'))\n    s[output].bind(tf, tvm.thread_axis('threadIdx.z'))\n    s[output].bind(ty, tvm.thread_axis('threadIdx.y'))\n    s[output].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].reorder(bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n    s[OL].compute_at(s[output], tx)\n    (n, f, y, x) = s[OL].op.axis\n    (rc, ry, rx) = s[OL].op.reduce_axis\n    (rco, rci) = cfg['tile_rc'].apply(s, OL, rc)\n    (ryo, ryi) = cfg['tile_rx'].apply(s, OL, ry)\n    (rxo, rxi) = cfg['tile_ry'].apply(s, OL, rx)\n    s[OL].reorder(rco, ryo, rxo, rci, ryi, rxi, n, f, y, x)\n    s[AA].compute_at(s[OL], rxo)\n    s[WW].compute_at(s[OL], rxo)\n    for load in [AA, WW]:\n        (n, f, y, x) = s[load].op.axis\n        fused = s[load].fuse(n, f, y, x)\n        (tz, fused) = s[load].split(fused, nparts=cfg['tile_f'].size[2])\n        (ty, fused) = s[load].split(fused, nparts=cfg['tile_y'].size[2])\n        (tx, fused) = s[load].split(fused, nparts=cfg['tile_x'].size[2])\n        s[load].bind(tz, tvm.thread_axis('threadIdx.z'))\n        s[load].bind(ty, tvm.thread_axis('threadIdx.y'))\n        s[load].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n    s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)\n    (N, CO, OH, OW) = get_const_tuple(output.shape)\n    (_, KH, KW, CI) = get_const_tuple(kernel.shape)\n    cfg.add_flop(2 * N * OH * OW * CO * CI * KH * KW)\n    return (s, [data, kernel, conv])",
        "mutated": [
            "@autotvm.template\ndef get_template_op(**kargs):\n    if False:\n        i = 10\n    N = op_attributes['N']\n    CI = op_attributes['C']\n    H = op_attributes['H']\n    W = op_attributes['W']\n    H = op_attributes['H']\n    CO = op_attributes['F']\n    KH = KW = op_attributes['K']\n    stride = op_attributes['ST']\n    padding = op_attributes['PD']\n    dilation = 1\n    data = tvm.placeholder((N, CI, H, W), name='data')\n    kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n    conv = topi.nn.conv2d_nchw(data, kernel, (stride, stride), (padding, padding), dilation=1, out_dtype='float32')\n    s = tvm.create_schedule([conv.op])\n    cfg = autotvm.get_config()\n    (n, f, y, x) = s[conv].op.axis\n    (rc, ry, rx) = s[conv].op.reduce_axis\n    cfg.define_split('tile_f', f, num_outputs=4)\n    cfg.define_split('tile_y', y, num_outputs=4)\n    cfg.define_split('tile_x', x, num_outputs=4)\n    cfg.define_split('tile_rc', rc, num_outputs=2)\n    cfg.define_split('tile_ry', ry, num_outputs=2)\n    cfg.define_split('tile_rx', rx, num_outputs=2)\n    cfg.define_knob('auto_unroll_max_step', [0, 125, 256])\n    target = tvm.target.current_target()\n    if target.target_name in ['nvptx', 'rocm']:\n        cfg.define_knob('unroll_explicit', [1])\n    else:\n        cfg.define_knob('unroll_explicit', [0, 1])\n    (pad_data, kernel) = s[conv].op.input_tensors\n    s[pad_data].compute_inline()\n    if isinstance(kernel.op, tvm.tensor.ComputeOp) and 'dilate' in kernel.op.tag:\n        s[kernel].compute_inline()\n    if conv.op in s.outputs:\n        output = conv\n        OL = s.cache_write(conv, 'local')\n    else:\n        output = s.outputs[0].output(0)\n        s[conv].set_scope('local')\n        OL = conv\n    AA = s.cache_read(pad_data, 'shared', [OL])\n    WW = s.cache_read(kernel, 'shared', [OL])\n    (n, f, y, x) = s[output].op.axis\n    (kernel_scope, n) = s[output].split(n, nparts=1)\n    (bf, vf, tf, fi) = cfg['tile_f'].apply(s, output, f)\n    (by, vy, ty, yi) = cfg['tile_y'].apply(s, output, y)\n    (bx, vx, tx, xi) = cfg['tile_x'].apply(s, output, x)\n    bf = s[output].fuse(n, bf)\n    s[output].bind(bf, tvm.thread_axis('blockIdx.z'))\n    s[output].bind(by, tvm.thread_axis('blockIdx.y'))\n    s[output].bind(bx, tvm.thread_axis('blockIdx.x'))\n    s[output].bind(vf, tvm.thread_axis('vthread'))\n    s[output].bind(vy, tvm.thread_axis('vthread'))\n    s[output].bind(vx, tvm.thread_axis('vthread'))\n    s[output].bind(tf, tvm.thread_axis('threadIdx.z'))\n    s[output].bind(ty, tvm.thread_axis('threadIdx.y'))\n    s[output].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].reorder(bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n    s[OL].compute_at(s[output], tx)\n    (n, f, y, x) = s[OL].op.axis\n    (rc, ry, rx) = s[OL].op.reduce_axis\n    (rco, rci) = cfg['tile_rc'].apply(s, OL, rc)\n    (ryo, ryi) = cfg['tile_rx'].apply(s, OL, ry)\n    (rxo, rxi) = cfg['tile_ry'].apply(s, OL, rx)\n    s[OL].reorder(rco, ryo, rxo, rci, ryi, rxi, n, f, y, x)\n    s[AA].compute_at(s[OL], rxo)\n    s[WW].compute_at(s[OL], rxo)\n    for load in [AA, WW]:\n        (n, f, y, x) = s[load].op.axis\n        fused = s[load].fuse(n, f, y, x)\n        (tz, fused) = s[load].split(fused, nparts=cfg['tile_f'].size[2])\n        (ty, fused) = s[load].split(fused, nparts=cfg['tile_y'].size[2])\n        (tx, fused) = s[load].split(fused, nparts=cfg['tile_x'].size[2])\n        s[load].bind(tz, tvm.thread_axis('threadIdx.z'))\n        s[load].bind(ty, tvm.thread_axis('threadIdx.y'))\n        s[load].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n    s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)\n    (N, CO, OH, OW) = get_const_tuple(output.shape)\n    (_, KH, KW, CI) = get_const_tuple(kernel.shape)\n    cfg.add_flop(2 * N * OH * OW * CO * CI * KH * KW)\n    return (s, [data, kernel, conv])",
            "@autotvm.template\ndef get_template_op(**kargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    N = op_attributes['N']\n    CI = op_attributes['C']\n    H = op_attributes['H']\n    W = op_attributes['W']\n    H = op_attributes['H']\n    CO = op_attributes['F']\n    KH = KW = op_attributes['K']\n    stride = op_attributes['ST']\n    padding = op_attributes['PD']\n    dilation = 1\n    data = tvm.placeholder((N, CI, H, W), name='data')\n    kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n    conv = topi.nn.conv2d_nchw(data, kernel, (stride, stride), (padding, padding), dilation=1, out_dtype='float32')\n    s = tvm.create_schedule([conv.op])\n    cfg = autotvm.get_config()\n    (n, f, y, x) = s[conv].op.axis\n    (rc, ry, rx) = s[conv].op.reduce_axis\n    cfg.define_split('tile_f', f, num_outputs=4)\n    cfg.define_split('tile_y', y, num_outputs=4)\n    cfg.define_split('tile_x', x, num_outputs=4)\n    cfg.define_split('tile_rc', rc, num_outputs=2)\n    cfg.define_split('tile_ry', ry, num_outputs=2)\n    cfg.define_split('tile_rx', rx, num_outputs=2)\n    cfg.define_knob('auto_unroll_max_step', [0, 125, 256])\n    target = tvm.target.current_target()\n    if target.target_name in ['nvptx', 'rocm']:\n        cfg.define_knob('unroll_explicit', [1])\n    else:\n        cfg.define_knob('unroll_explicit', [0, 1])\n    (pad_data, kernel) = s[conv].op.input_tensors\n    s[pad_data].compute_inline()\n    if isinstance(kernel.op, tvm.tensor.ComputeOp) and 'dilate' in kernel.op.tag:\n        s[kernel].compute_inline()\n    if conv.op in s.outputs:\n        output = conv\n        OL = s.cache_write(conv, 'local')\n    else:\n        output = s.outputs[0].output(0)\n        s[conv].set_scope('local')\n        OL = conv\n    AA = s.cache_read(pad_data, 'shared', [OL])\n    WW = s.cache_read(kernel, 'shared', [OL])\n    (n, f, y, x) = s[output].op.axis\n    (kernel_scope, n) = s[output].split(n, nparts=1)\n    (bf, vf, tf, fi) = cfg['tile_f'].apply(s, output, f)\n    (by, vy, ty, yi) = cfg['tile_y'].apply(s, output, y)\n    (bx, vx, tx, xi) = cfg['tile_x'].apply(s, output, x)\n    bf = s[output].fuse(n, bf)\n    s[output].bind(bf, tvm.thread_axis('blockIdx.z'))\n    s[output].bind(by, tvm.thread_axis('blockIdx.y'))\n    s[output].bind(bx, tvm.thread_axis('blockIdx.x'))\n    s[output].bind(vf, tvm.thread_axis('vthread'))\n    s[output].bind(vy, tvm.thread_axis('vthread'))\n    s[output].bind(vx, tvm.thread_axis('vthread'))\n    s[output].bind(tf, tvm.thread_axis('threadIdx.z'))\n    s[output].bind(ty, tvm.thread_axis('threadIdx.y'))\n    s[output].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].reorder(bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n    s[OL].compute_at(s[output], tx)\n    (n, f, y, x) = s[OL].op.axis\n    (rc, ry, rx) = s[OL].op.reduce_axis\n    (rco, rci) = cfg['tile_rc'].apply(s, OL, rc)\n    (ryo, ryi) = cfg['tile_rx'].apply(s, OL, ry)\n    (rxo, rxi) = cfg['tile_ry'].apply(s, OL, rx)\n    s[OL].reorder(rco, ryo, rxo, rci, ryi, rxi, n, f, y, x)\n    s[AA].compute_at(s[OL], rxo)\n    s[WW].compute_at(s[OL], rxo)\n    for load in [AA, WW]:\n        (n, f, y, x) = s[load].op.axis\n        fused = s[load].fuse(n, f, y, x)\n        (tz, fused) = s[load].split(fused, nparts=cfg['tile_f'].size[2])\n        (ty, fused) = s[load].split(fused, nparts=cfg['tile_y'].size[2])\n        (tx, fused) = s[load].split(fused, nparts=cfg['tile_x'].size[2])\n        s[load].bind(tz, tvm.thread_axis('threadIdx.z'))\n        s[load].bind(ty, tvm.thread_axis('threadIdx.y'))\n        s[load].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n    s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)\n    (N, CO, OH, OW) = get_const_tuple(output.shape)\n    (_, KH, KW, CI) = get_const_tuple(kernel.shape)\n    cfg.add_flop(2 * N * OH * OW * CO * CI * KH * KW)\n    return (s, [data, kernel, conv])",
            "@autotvm.template\ndef get_template_op(**kargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    N = op_attributes['N']\n    CI = op_attributes['C']\n    H = op_attributes['H']\n    W = op_attributes['W']\n    H = op_attributes['H']\n    CO = op_attributes['F']\n    KH = KW = op_attributes['K']\n    stride = op_attributes['ST']\n    padding = op_attributes['PD']\n    dilation = 1\n    data = tvm.placeholder((N, CI, H, W), name='data')\n    kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n    conv = topi.nn.conv2d_nchw(data, kernel, (stride, stride), (padding, padding), dilation=1, out_dtype='float32')\n    s = tvm.create_schedule([conv.op])\n    cfg = autotvm.get_config()\n    (n, f, y, x) = s[conv].op.axis\n    (rc, ry, rx) = s[conv].op.reduce_axis\n    cfg.define_split('tile_f', f, num_outputs=4)\n    cfg.define_split('tile_y', y, num_outputs=4)\n    cfg.define_split('tile_x', x, num_outputs=4)\n    cfg.define_split('tile_rc', rc, num_outputs=2)\n    cfg.define_split('tile_ry', ry, num_outputs=2)\n    cfg.define_split('tile_rx', rx, num_outputs=2)\n    cfg.define_knob('auto_unroll_max_step', [0, 125, 256])\n    target = tvm.target.current_target()\n    if target.target_name in ['nvptx', 'rocm']:\n        cfg.define_knob('unroll_explicit', [1])\n    else:\n        cfg.define_knob('unroll_explicit', [0, 1])\n    (pad_data, kernel) = s[conv].op.input_tensors\n    s[pad_data].compute_inline()\n    if isinstance(kernel.op, tvm.tensor.ComputeOp) and 'dilate' in kernel.op.tag:\n        s[kernel].compute_inline()\n    if conv.op in s.outputs:\n        output = conv\n        OL = s.cache_write(conv, 'local')\n    else:\n        output = s.outputs[0].output(0)\n        s[conv].set_scope('local')\n        OL = conv\n    AA = s.cache_read(pad_data, 'shared', [OL])\n    WW = s.cache_read(kernel, 'shared', [OL])\n    (n, f, y, x) = s[output].op.axis\n    (kernel_scope, n) = s[output].split(n, nparts=1)\n    (bf, vf, tf, fi) = cfg['tile_f'].apply(s, output, f)\n    (by, vy, ty, yi) = cfg['tile_y'].apply(s, output, y)\n    (bx, vx, tx, xi) = cfg['tile_x'].apply(s, output, x)\n    bf = s[output].fuse(n, bf)\n    s[output].bind(bf, tvm.thread_axis('blockIdx.z'))\n    s[output].bind(by, tvm.thread_axis('blockIdx.y'))\n    s[output].bind(bx, tvm.thread_axis('blockIdx.x'))\n    s[output].bind(vf, tvm.thread_axis('vthread'))\n    s[output].bind(vy, tvm.thread_axis('vthread'))\n    s[output].bind(vx, tvm.thread_axis('vthread'))\n    s[output].bind(tf, tvm.thread_axis('threadIdx.z'))\n    s[output].bind(ty, tvm.thread_axis('threadIdx.y'))\n    s[output].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].reorder(bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n    s[OL].compute_at(s[output], tx)\n    (n, f, y, x) = s[OL].op.axis\n    (rc, ry, rx) = s[OL].op.reduce_axis\n    (rco, rci) = cfg['tile_rc'].apply(s, OL, rc)\n    (ryo, ryi) = cfg['tile_rx'].apply(s, OL, ry)\n    (rxo, rxi) = cfg['tile_ry'].apply(s, OL, rx)\n    s[OL].reorder(rco, ryo, rxo, rci, ryi, rxi, n, f, y, x)\n    s[AA].compute_at(s[OL], rxo)\n    s[WW].compute_at(s[OL], rxo)\n    for load in [AA, WW]:\n        (n, f, y, x) = s[load].op.axis\n        fused = s[load].fuse(n, f, y, x)\n        (tz, fused) = s[load].split(fused, nparts=cfg['tile_f'].size[2])\n        (ty, fused) = s[load].split(fused, nparts=cfg['tile_y'].size[2])\n        (tx, fused) = s[load].split(fused, nparts=cfg['tile_x'].size[2])\n        s[load].bind(tz, tvm.thread_axis('threadIdx.z'))\n        s[load].bind(ty, tvm.thread_axis('threadIdx.y'))\n        s[load].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n    s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)\n    (N, CO, OH, OW) = get_const_tuple(output.shape)\n    (_, KH, KW, CI) = get_const_tuple(kernel.shape)\n    cfg.add_flop(2 * N * OH * OW * CO * CI * KH * KW)\n    return (s, [data, kernel, conv])",
            "@autotvm.template\ndef get_template_op(**kargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    N = op_attributes['N']\n    CI = op_attributes['C']\n    H = op_attributes['H']\n    W = op_attributes['W']\n    H = op_attributes['H']\n    CO = op_attributes['F']\n    KH = KW = op_attributes['K']\n    stride = op_attributes['ST']\n    padding = op_attributes['PD']\n    dilation = 1\n    data = tvm.placeholder((N, CI, H, W), name='data')\n    kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n    conv = topi.nn.conv2d_nchw(data, kernel, (stride, stride), (padding, padding), dilation=1, out_dtype='float32')\n    s = tvm.create_schedule([conv.op])\n    cfg = autotvm.get_config()\n    (n, f, y, x) = s[conv].op.axis\n    (rc, ry, rx) = s[conv].op.reduce_axis\n    cfg.define_split('tile_f', f, num_outputs=4)\n    cfg.define_split('tile_y', y, num_outputs=4)\n    cfg.define_split('tile_x', x, num_outputs=4)\n    cfg.define_split('tile_rc', rc, num_outputs=2)\n    cfg.define_split('tile_ry', ry, num_outputs=2)\n    cfg.define_split('tile_rx', rx, num_outputs=2)\n    cfg.define_knob('auto_unroll_max_step', [0, 125, 256])\n    target = tvm.target.current_target()\n    if target.target_name in ['nvptx', 'rocm']:\n        cfg.define_knob('unroll_explicit', [1])\n    else:\n        cfg.define_knob('unroll_explicit', [0, 1])\n    (pad_data, kernel) = s[conv].op.input_tensors\n    s[pad_data].compute_inline()\n    if isinstance(kernel.op, tvm.tensor.ComputeOp) and 'dilate' in kernel.op.tag:\n        s[kernel].compute_inline()\n    if conv.op in s.outputs:\n        output = conv\n        OL = s.cache_write(conv, 'local')\n    else:\n        output = s.outputs[0].output(0)\n        s[conv].set_scope('local')\n        OL = conv\n    AA = s.cache_read(pad_data, 'shared', [OL])\n    WW = s.cache_read(kernel, 'shared', [OL])\n    (n, f, y, x) = s[output].op.axis\n    (kernel_scope, n) = s[output].split(n, nparts=1)\n    (bf, vf, tf, fi) = cfg['tile_f'].apply(s, output, f)\n    (by, vy, ty, yi) = cfg['tile_y'].apply(s, output, y)\n    (bx, vx, tx, xi) = cfg['tile_x'].apply(s, output, x)\n    bf = s[output].fuse(n, bf)\n    s[output].bind(bf, tvm.thread_axis('blockIdx.z'))\n    s[output].bind(by, tvm.thread_axis('blockIdx.y'))\n    s[output].bind(bx, tvm.thread_axis('blockIdx.x'))\n    s[output].bind(vf, tvm.thread_axis('vthread'))\n    s[output].bind(vy, tvm.thread_axis('vthread'))\n    s[output].bind(vx, tvm.thread_axis('vthread'))\n    s[output].bind(tf, tvm.thread_axis('threadIdx.z'))\n    s[output].bind(ty, tvm.thread_axis('threadIdx.y'))\n    s[output].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].reorder(bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n    s[OL].compute_at(s[output], tx)\n    (n, f, y, x) = s[OL].op.axis\n    (rc, ry, rx) = s[OL].op.reduce_axis\n    (rco, rci) = cfg['tile_rc'].apply(s, OL, rc)\n    (ryo, ryi) = cfg['tile_rx'].apply(s, OL, ry)\n    (rxo, rxi) = cfg['tile_ry'].apply(s, OL, rx)\n    s[OL].reorder(rco, ryo, rxo, rci, ryi, rxi, n, f, y, x)\n    s[AA].compute_at(s[OL], rxo)\n    s[WW].compute_at(s[OL], rxo)\n    for load in [AA, WW]:\n        (n, f, y, x) = s[load].op.axis\n        fused = s[load].fuse(n, f, y, x)\n        (tz, fused) = s[load].split(fused, nparts=cfg['tile_f'].size[2])\n        (ty, fused) = s[load].split(fused, nparts=cfg['tile_y'].size[2])\n        (tx, fused) = s[load].split(fused, nparts=cfg['tile_x'].size[2])\n        s[load].bind(tz, tvm.thread_axis('threadIdx.z'))\n        s[load].bind(ty, tvm.thread_axis('threadIdx.y'))\n        s[load].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n    s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)\n    (N, CO, OH, OW) = get_const_tuple(output.shape)\n    (_, KH, KW, CI) = get_const_tuple(kernel.shape)\n    cfg.add_flop(2 * N * OH * OW * CO * CI * KH * KW)\n    return (s, [data, kernel, conv])",
            "@autotvm.template\ndef get_template_op(**kargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    N = op_attributes['N']\n    CI = op_attributes['C']\n    H = op_attributes['H']\n    W = op_attributes['W']\n    H = op_attributes['H']\n    CO = op_attributes['F']\n    KH = KW = op_attributes['K']\n    stride = op_attributes['ST']\n    padding = op_attributes['PD']\n    dilation = 1\n    data = tvm.placeholder((N, CI, H, W), name='data')\n    kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n    conv = topi.nn.conv2d_nchw(data, kernel, (stride, stride), (padding, padding), dilation=1, out_dtype='float32')\n    s = tvm.create_schedule([conv.op])\n    cfg = autotvm.get_config()\n    (n, f, y, x) = s[conv].op.axis\n    (rc, ry, rx) = s[conv].op.reduce_axis\n    cfg.define_split('tile_f', f, num_outputs=4)\n    cfg.define_split('tile_y', y, num_outputs=4)\n    cfg.define_split('tile_x', x, num_outputs=4)\n    cfg.define_split('tile_rc', rc, num_outputs=2)\n    cfg.define_split('tile_ry', ry, num_outputs=2)\n    cfg.define_split('tile_rx', rx, num_outputs=2)\n    cfg.define_knob('auto_unroll_max_step', [0, 125, 256])\n    target = tvm.target.current_target()\n    if target.target_name in ['nvptx', 'rocm']:\n        cfg.define_knob('unroll_explicit', [1])\n    else:\n        cfg.define_knob('unroll_explicit', [0, 1])\n    (pad_data, kernel) = s[conv].op.input_tensors\n    s[pad_data].compute_inline()\n    if isinstance(kernel.op, tvm.tensor.ComputeOp) and 'dilate' in kernel.op.tag:\n        s[kernel].compute_inline()\n    if conv.op in s.outputs:\n        output = conv\n        OL = s.cache_write(conv, 'local')\n    else:\n        output = s.outputs[0].output(0)\n        s[conv].set_scope('local')\n        OL = conv\n    AA = s.cache_read(pad_data, 'shared', [OL])\n    WW = s.cache_read(kernel, 'shared', [OL])\n    (n, f, y, x) = s[output].op.axis\n    (kernel_scope, n) = s[output].split(n, nparts=1)\n    (bf, vf, tf, fi) = cfg['tile_f'].apply(s, output, f)\n    (by, vy, ty, yi) = cfg['tile_y'].apply(s, output, y)\n    (bx, vx, tx, xi) = cfg['tile_x'].apply(s, output, x)\n    bf = s[output].fuse(n, bf)\n    s[output].bind(bf, tvm.thread_axis('blockIdx.z'))\n    s[output].bind(by, tvm.thread_axis('blockIdx.y'))\n    s[output].bind(bx, tvm.thread_axis('blockIdx.x'))\n    s[output].bind(vf, tvm.thread_axis('vthread'))\n    s[output].bind(vy, tvm.thread_axis('vthread'))\n    s[output].bind(vx, tvm.thread_axis('vthread'))\n    s[output].bind(tf, tvm.thread_axis('threadIdx.z'))\n    s[output].bind(ty, tvm.thread_axis('threadIdx.y'))\n    s[output].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].reorder(bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n    s[OL].compute_at(s[output], tx)\n    (n, f, y, x) = s[OL].op.axis\n    (rc, ry, rx) = s[OL].op.reduce_axis\n    (rco, rci) = cfg['tile_rc'].apply(s, OL, rc)\n    (ryo, ryi) = cfg['tile_rx'].apply(s, OL, ry)\n    (rxo, rxi) = cfg['tile_ry'].apply(s, OL, rx)\n    s[OL].reorder(rco, ryo, rxo, rci, ryi, rxi, n, f, y, x)\n    s[AA].compute_at(s[OL], rxo)\n    s[WW].compute_at(s[OL], rxo)\n    for load in [AA, WW]:\n        (n, f, y, x) = s[load].op.axis\n        fused = s[load].fuse(n, f, y, x)\n        (tz, fused) = s[load].split(fused, nparts=cfg['tile_f'].size[2])\n        (ty, fused) = s[load].split(fused, nparts=cfg['tile_y'].size[2])\n        (tx, fused) = s[load].split(fused, nparts=cfg['tile_x'].size[2])\n        s[load].bind(tz, tvm.thread_axis('threadIdx.z'))\n        s[load].bind(ty, tvm.thread_axis('threadIdx.y'))\n        s[load].bind(tx, tvm.thread_axis('threadIdx.x'))\n    s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n    s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)\n    (N, CO, OH, OW) = get_const_tuple(output.shape)\n    (_, KH, KW, CI) = get_const_tuple(kernel.shape)\n    cfg.add_flop(2 * N * OH * OW * CO * CI * KH * KW)\n    return (s, [data, kernel, conv])"
        ]
    }
]