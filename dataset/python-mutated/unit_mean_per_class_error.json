[
    {
        "func_name": "test_mean_per_class_error_binomial",
        "original": "def test_mean_per_class_error_binomial():\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['economy_20mpg'] = cars['economy_20mpg'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'economy_20mpg'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'bernoulli'\n    gbm.train(y=response_col, x=predictors, validation_frame=valid, training_frame=train)\n    mpce = gbm.mean_per_class_error([0.5, 0.8])\n    assert abs(mpce[0][1] - 0.008264) < 1e-05\n    assert abs(mpce[1][1] - 0.018716) < 1e-05\n    print(gbm.model_performance(train).mean_per_class_error(thresholds=[0.3, 0.5]))",
        "mutated": [
            "def test_mean_per_class_error_binomial():\n    if False:\n        i = 10\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['economy_20mpg'] = cars['economy_20mpg'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'economy_20mpg'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'bernoulli'\n    gbm.train(y=response_col, x=predictors, validation_frame=valid, training_frame=train)\n    mpce = gbm.mean_per_class_error([0.5, 0.8])\n    assert abs(mpce[0][1] - 0.008264) < 1e-05\n    assert abs(mpce[1][1] - 0.018716) < 1e-05\n    print(gbm.model_performance(train).mean_per_class_error(thresholds=[0.3, 0.5]))",
            "def test_mean_per_class_error_binomial():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['economy_20mpg'] = cars['economy_20mpg'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'economy_20mpg'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'bernoulli'\n    gbm.train(y=response_col, x=predictors, validation_frame=valid, training_frame=train)\n    mpce = gbm.mean_per_class_error([0.5, 0.8])\n    assert abs(mpce[0][1] - 0.008264) < 1e-05\n    assert abs(mpce[1][1] - 0.018716) < 1e-05\n    print(gbm.model_performance(train).mean_per_class_error(thresholds=[0.3, 0.5]))",
            "def test_mean_per_class_error_binomial():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['economy_20mpg'] = cars['economy_20mpg'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'economy_20mpg'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'bernoulli'\n    gbm.train(y=response_col, x=predictors, validation_frame=valid, training_frame=train)\n    mpce = gbm.mean_per_class_error([0.5, 0.8])\n    assert abs(mpce[0][1] - 0.008264) < 1e-05\n    assert abs(mpce[1][1] - 0.018716) < 1e-05\n    print(gbm.model_performance(train).mean_per_class_error(thresholds=[0.3, 0.5]))",
            "def test_mean_per_class_error_binomial():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['economy_20mpg'] = cars['economy_20mpg'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'economy_20mpg'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'bernoulli'\n    gbm.train(y=response_col, x=predictors, validation_frame=valid, training_frame=train)\n    mpce = gbm.mean_per_class_error([0.5, 0.8])\n    assert abs(mpce[0][1] - 0.008264) < 1e-05\n    assert abs(mpce[1][1] - 0.018716) < 1e-05\n    print(gbm.model_performance(train).mean_per_class_error(thresholds=[0.3, 0.5]))",
            "def test_mean_per_class_error_binomial():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['economy_20mpg'] = cars['economy_20mpg'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'economy_20mpg'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'bernoulli'\n    gbm.train(y=response_col, x=predictors, validation_frame=valid, training_frame=train)\n    mpce = gbm.mean_per_class_error([0.5, 0.8])\n    assert abs(mpce[0][1] - 0.008264) < 1e-05\n    assert abs(mpce[1][1] - 0.018716) < 1e-05\n    print(gbm.model_performance(train).mean_per_class_error(thresholds=[0.3, 0.5]))"
        ]
    },
    {
        "func_name": "test_mean_per_class_error_multinomial",
        "original": "def test_mean_per_class_error_multinomial():\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm.__class__.__mro__)\n    mpce = gbm.mean_per_class_error(train=True)\n    assert mpce == 0\n    mpce = gbm.mean_per_class_error(valid=True)\n    assert abs(mpce - 0.407142857143) < 1e-05\n    mpce = gbm.mean_per_class_error(xval=True)\n    assert abs(mpce - 0.35127653471) < 1e-05",
        "mutated": [
            "def test_mean_per_class_error_multinomial():\n    if False:\n        i = 10\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm.__class__.__mro__)\n    mpce = gbm.mean_per_class_error(train=True)\n    assert mpce == 0\n    mpce = gbm.mean_per_class_error(valid=True)\n    assert abs(mpce - 0.407142857143) < 1e-05\n    mpce = gbm.mean_per_class_error(xval=True)\n    assert abs(mpce - 0.35127653471) < 1e-05",
            "def test_mean_per_class_error_multinomial():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm.__class__.__mro__)\n    mpce = gbm.mean_per_class_error(train=True)\n    assert mpce == 0\n    mpce = gbm.mean_per_class_error(valid=True)\n    assert abs(mpce - 0.407142857143) < 1e-05\n    mpce = gbm.mean_per_class_error(xval=True)\n    assert abs(mpce - 0.35127653471) < 1e-05",
            "def test_mean_per_class_error_multinomial():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm.__class__.__mro__)\n    mpce = gbm.mean_per_class_error(train=True)\n    assert mpce == 0\n    mpce = gbm.mean_per_class_error(valid=True)\n    assert abs(mpce - 0.407142857143) < 1e-05\n    mpce = gbm.mean_per_class_error(xval=True)\n    assert abs(mpce - 0.35127653471) < 1e-05",
            "def test_mean_per_class_error_multinomial():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm.__class__.__mro__)\n    mpce = gbm.mean_per_class_error(train=True)\n    assert mpce == 0\n    mpce = gbm.mean_per_class_error(valid=True)\n    assert abs(mpce - 0.407142857143) < 1e-05\n    mpce = gbm.mean_per_class_error(xval=True)\n    assert abs(mpce - 0.35127653471) < 1e-05",
            "def test_mean_per_class_error_multinomial():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm.__class__.__mro__)\n    mpce = gbm.mean_per_class_error(train=True)\n    assert mpce == 0\n    mpce = gbm.mean_per_class_error(valid=True)\n    assert abs(mpce - 0.407142857143) < 1e-05\n    mpce = gbm.mean_per_class_error(xval=True)\n    assert abs(mpce - 0.35127653471) < 1e-05"
        ]
    },
    {
        "func_name": "test_mean_per_class_error_grid",
        "original": "def test_mean_per_class_error_grid():\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.stopping_rounds = 2\n    gbm.stopping_metric = 'mean_per_class_error'\n    gbm.ntrees = 10000\n    gbm.max_depth = 3\n    gbm.min_rows = 1\n    gbm.learn_rate = 0.01\n    gbm.score_tree_interval = 1\n    gbm.nfolds = None\n    gbm.fold_assignment = None\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm)\n    print(gbm.scoring_history())\n    hyper_params_tune = {'max_depth': list(range(1, 10 + 1, 1)), 'sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_per_tree': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_change_per_level': [x / 100.0 for x in range(90, 111)], 'min_rows': [2 ** x for x in range(0, int(math.log(train.nrow, 2) - 2) + 1)], 'nbins': [2 ** x for x in range(4, 11)], 'nbins_cats': [2 ** x for x in range(4, 13)], 'min_split_improvement': [0, 1e-08, 1e-06, 0.0001], 'histogram_type': ['UniformAdaptive', 'QuantilesGlobal', 'RoundRobin']}\n    search_criteria_tune = {'strategy': 'RandomDiscrete', 'max_runtime_secs': 600, 'max_models': 10, 'seed': 1234, 'stopping_rounds': 5, 'stopping_metric': 'mean_per_class_error', 'stopping_tolerance': 0.001}\n    grid = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params=hyper_params_tune, search_criteria=search_criteria_tune)\n    grid.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid, distribution='multinomial', seed=1234, stopping_rounds=10, stopping_metric='mean_per_class_error', stopping_tolerance=0.001)\n    print(grid)\n    print(grid.get_grid('mean_per_class_error'))",
        "mutated": [
            "def test_mean_per_class_error_grid():\n    if False:\n        i = 10\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.stopping_rounds = 2\n    gbm.stopping_metric = 'mean_per_class_error'\n    gbm.ntrees = 10000\n    gbm.max_depth = 3\n    gbm.min_rows = 1\n    gbm.learn_rate = 0.01\n    gbm.score_tree_interval = 1\n    gbm.nfolds = None\n    gbm.fold_assignment = None\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm)\n    print(gbm.scoring_history())\n    hyper_params_tune = {'max_depth': list(range(1, 10 + 1, 1)), 'sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_per_tree': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_change_per_level': [x / 100.0 for x in range(90, 111)], 'min_rows': [2 ** x for x in range(0, int(math.log(train.nrow, 2) - 2) + 1)], 'nbins': [2 ** x for x in range(4, 11)], 'nbins_cats': [2 ** x for x in range(4, 13)], 'min_split_improvement': [0, 1e-08, 1e-06, 0.0001], 'histogram_type': ['UniformAdaptive', 'QuantilesGlobal', 'RoundRobin']}\n    search_criteria_tune = {'strategy': 'RandomDiscrete', 'max_runtime_secs': 600, 'max_models': 10, 'seed': 1234, 'stopping_rounds': 5, 'stopping_metric': 'mean_per_class_error', 'stopping_tolerance': 0.001}\n    grid = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params=hyper_params_tune, search_criteria=search_criteria_tune)\n    grid.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid, distribution='multinomial', seed=1234, stopping_rounds=10, stopping_metric='mean_per_class_error', stopping_tolerance=0.001)\n    print(grid)\n    print(grid.get_grid('mean_per_class_error'))",
            "def test_mean_per_class_error_grid():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.stopping_rounds = 2\n    gbm.stopping_metric = 'mean_per_class_error'\n    gbm.ntrees = 10000\n    gbm.max_depth = 3\n    gbm.min_rows = 1\n    gbm.learn_rate = 0.01\n    gbm.score_tree_interval = 1\n    gbm.nfolds = None\n    gbm.fold_assignment = None\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm)\n    print(gbm.scoring_history())\n    hyper_params_tune = {'max_depth': list(range(1, 10 + 1, 1)), 'sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_per_tree': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_change_per_level': [x / 100.0 for x in range(90, 111)], 'min_rows': [2 ** x for x in range(0, int(math.log(train.nrow, 2) - 2) + 1)], 'nbins': [2 ** x for x in range(4, 11)], 'nbins_cats': [2 ** x for x in range(4, 13)], 'min_split_improvement': [0, 1e-08, 1e-06, 0.0001], 'histogram_type': ['UniformAdaptive', 'QuantilesGlobal', 'RoundRobin']}\n    search_criteria_tune = {'strategy': 'RandomDiscrete', 'max_runtime_secs': 600, 'max_models': 10, 'seed': 1234, 'stopping_rounds': 5, 'stopping_metric': 'mean_per_class_error', 'stopping_tolerance': 0.001}\n    grid = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params=hyper_params_tune, search_criteria=search_criteria_tune)\n    grid.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid, distribution='multinomial', seed=1234, stopping_rounds=10, stopping_metric='mean_per_class_error', stopping_tolerance=0.001)\n    print(grid)\n    print(grid.get_grid('mean_per_class_error'))",
            "def test_mean_per_class_error_grid():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.stopping_rounds = 2\n    gbm.stopping_metric = 'mean_per_class_error'\n    gbm.ntrees = 10000\n    gbm.max_depth = 3\n    gbm.min_rows = 1\n    gbm.learn_rate = 0.01\n    gbm.score_tree_interval = 1\n    gbm.nfolds = None\n    gbm.fold_assignment = None\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm)\n    print(gbm.scoring_history())\n    hyper_params_tune = {'max_depth': list(range(1, 10 + 1, 1)), 'sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_per_tree': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_change_per_level': [x / 100.0 for x in range(90, 111)], 'min_rows': [2 ** x for x in range(0, int(math.log(train.nrow, 2) - 2) + 1)], 'nbins': [2 ** x for x in range(4, 11)], 'nbins_cats': [2 ** x for x in range(4, 13)], 'min_split_improvement': [0, 1e-08, 1e-06, 0.0001], 'histogram_type': ['UniformAdaptive', 'QuantilesGlobal', 'RoundRobin']}\n    search_criteria_tune = {'strategy': 'RandomDiscrete', 'max_runtime_secs': 600, 'max_models': 10, 'seed': 1234, 'stopping_rounds': 5, 'stopping_metric': 'mean_per_class_error', 'stopping_tolerance': 0.001}\n    grid = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params=hyper_params_tune, search_criteria=search_criteria_tune)\n    grid.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid, distribution='multinomial', seed=1234, stopping_rounds=10, stopping_metric='mean_per_class_error', stopping_tolerance=0.001)\n    print(grid)\n    print(grid.get_grid('mean_per_class_error'))",
            "def test_mean_per_class_error_grid():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.stopping_rounds = 2\n    gbm.stopping_metric = 'mean_per_class_error'\n    gbm.ntrees = 10000\n    gbm.max_depth = 3\n    gbm.min_rows = 1\n    gbm.learn_rate = 0.01\n    gbm.score_tree_interval = 1\n    gbm.nfolds = None\n    gbm.fold_assignment = None\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm)\n    print(gbm.scoring_history())\n    hyper_params_tune = {'max_depth': list(range(1, 10 + 1, 1)), 'sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_per_tree': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_change_per_level': [x / 100.0 for x in range(90, 111)], 'min_rows': [2 ** x for x in range(0, int(math.log(train.nrow, 2) - 2) + 1)], 'nbins': [2 ** x for x in range(4, 11)], 'nbins_cats': [2 ** x for x in range(4, 13)], 'min_split_improvement': [0, 1e-08, 1e-06, 0.0001], 'histogram_type': ['UniformAdaptive', 'QuantilesGlobal', 'RoundRobin']}\n    search_criteria_tune = {'strategy': 'RandomDiscrete', 'max_runtime_secs': 600, 'max_models': 10, 'seed': 1234, 'stopping_rounds': 5, 'stopping_metric': 'mean_per_class_error', 'stopping_tolerance': 0.001}\n    grid = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params=hyper_params_tune, search_criteria=search_criteria_tune)\n    grid.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid, distribution='multinomial', seed=1234, stopping_rounds=10, stopping_metric='mean_per_class_error', stopping_tolerance=0.001)\n    print(grid)\n    print(grid.get_grid('mean_per_class_error'))",
            "def test_mean_per_class_error_grid():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    gbm = H2OGradientBoostingEstimator(nfolds=3, fold_assignment='Random', seed=1234)\n    cars = h2o.import_file(pyunit_utils.locate('smalldata/junit/cars_20mpg.csv'))\n    cars['cylinders'] = cars['cylinders'].asfactor()\n    r = cars[0].runif(seed=1234)\n    train = cars[r > 0.2]\n    valid = cars[r <= 0.2]\n    response_col = 'cylinders'\n    predictors = ['displacement', 'power', 'weight', 'acceleration', 'year']\n    gbm.distribution = 'multinomial'\n    gbm.stopping_rounds = 2\n    gbm.stopping_metric = 'mean_per_class_error'\n    gbm.ntrees = 10000\n    gbm.max_depth = 3\n    gbm.min_rows = 1\n    gbm.learn_rate = 0.01\n    gbm.score_tree_interval = 1\n    gbm.nfolds = None\n    gbm.fold_assignment = None\n    gbm.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid)\n    print(gbm)\n    print(gbm.scoring_history())\n    hyper_params_tune = {'max_depth': list(range(1, 10 + 1, 1)), 'sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_per_tree': [x / 100.0 for x in range(20, 101)], 'col_sample_rate_change_per_level': [x / 100.0 for x in range(90, 111)], 'min_rows': [2 ** x for x in range(0, int(math.log(train.nrow, 2) - 2) + 1)], 'nbins': [2 ** x for x in range(4, 11)], 'nbins_cats': [2 ** x for x in range(4, 13)], 'min_split_improvement': [0, 1e-08, 1e-06, 0.0001], 'histogram_type': ['UniformAdaptive', 'QuantilesGlobal', 'RoundRobin']}\n    search_criteria_tune = {'strategy': 'RandomDiscrete', 'max_runtime_secs': 600, 'max_models': 10, 'seed': 1234, 'stopping_rounds': 5, 'stopping_metric': 'mean_per_class_error', 'stopping_tolerance': 0.001}\n    grid = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params=hyper_params_tune, search_criteria=search_criteria_tune)\n    grid.train(x=predictors, y=response_col, training_frame=train, validation_frame=valid, distribution='multinomial', seed=1234, stopping_rounds=10, stopping_metric='mean_per_class_error', stopping_tolerance=0.001)\n    print(grid)\n    print(grid.get_grid('mean_per_class_error'))"
        ]
    }
]