"""
This module contains ``CachedDataset``, a dataset wrapper which caches in memory the data saved,
so that the user avoids io operations with slow storage media
"""
from __future__ import annotations
import logging
import warnings
from typing import Any
from kedro import KedroDeprecationWarning
from kedro.io.core import VERSIONED_FLAG_KEY, AbstractDataset, Version
from kedro.io.memory_dataset import MemoryDataset
CachedDataSet: type[CachedDataset]

class CachedDataset(AbstractDataset):
    """``CachedDataset`` is a dataset wrapper which caches in memory the data saved,
    so that the user avoids io operations with slow storage media.

    You can also specify a ``CachedDataset`` in catalog.yml:
    ::

        >>> test_ds:
        >>>    type: CachedDataset
        >>>    versioned: true
        >>>    dataset:
        >>>       type: pandas.CSVDataset
        >>>       filepath: example.csv

    Please note that if your dataset is versioned, this should be indicated in the wrapper
    class as shown above.
    """
    _SINGLE_PROCESS = True

    def __init__(self, dataset: AbstractDataset | dict, version: Version=None, copy_mode: str=None, metadata: dict[str, Any]=None):
        if False:
            i = 10
            return i + 15
        'Creates a new instance of ``CachedDataset`` pointing to the\n        provided Python object.\n\n        Args:\n            dataset: A Kedro Dataset object or a dictionary to cache.\n            version: If specified, should be an instance of\n                ``kedro.io.core.Version``. If its ``load`` attribute is\n                None, the latest version will be loaded. If its ``save``\n                attribute is None, save version will be autogenerated.\n            copy_mode: The copy mode used to copy the data. Possible\n                values are: "deepcopy", "copy" and "assign". If not\n                provided, it is inferred based on the data type.\n            metadata: Any arbitrary metadata.\n                This is ignored by Kedro, but may be consumed by users or external plugins.\n\n        Raises:\n            ValueError: If the provided dataset is not a valid dict/YAML\n                representation of a dataset or an actual dataset.\n        '
        if isinstance(dataset, dict):
            self._dataset = self._from_config(dataset, version)
        elif isinstance(dataset, AbstractDataset):
            self._dataset = dataset
        else:
            raise ValueError("The argument type of 'dataset' should be either a dict/YAML representation of the dataset, or the actual dataset object.")
        self._cache = MemoryDataset(copy_mode=copy_mode)
        self.metadata = metadata

    def _release(self) -> None:
        if False:
            i = 10
            return i + 15
        self._cache.release()
        self._dataset.release()

    @staticmethod
    def _from_config(config, version):
        if False:
            while True:
                i = 10
        if VERSIONED_FLAG_KEY in config:
            raise ValueError("Cached datasets should specify that they are versioned in the 'CachedDataset', not in the wrapped dataset.")
        if version:
            config[VERSIONED_FLAG_KEY] = True
            return AbstractDataset.from_config('_cached', config, version.load, version.save)
        return AbstractDataset.from_config('_cached', config)

    def _describe(self) -> dict[str, Any]:
        if False:
            for i in range(10):
                print('nop')
        return {'dataset': self._dataset._describe(), 'cache': self._cache._describe()}

    def _load(self):
        if False:
            for i in range(10):
                print('nop')
        data = self._cache.load() if self._cache.exists() else self._dataset.load()
        if not self._cache.exists():
            self._cache.save(data)
        return data

    def _save(self, data: Any) -> None:
        if False:
            return 10
        self._dataset.save(data)
        self._cache.save(data)

    def _exists(self) -> bool:
        if False:
            for i in range(10):
                print('nop')
        return self._cache.exists() or self._dataset.exists()

    def __getstate__(self):
        if False:
            i = 10
            return i + 15
        logging.getLogger(__name__).warning('%s: clearing cache to pickle.', str(self))
        self._cache.release()
        return self.__dict__

def __getattr__(name):
    if False:
        print('Hello World!')
    if name == 'CachedDataSet':
        alias = CachedDataset
        warnings.warn(f'{repr(name)} has been renamed to {repr(alias.__name__)}, and the alias will be removed in Kedro 0.19.0', KedroDeprecationWarning, stacklevel=2)
        return alias
    raise AttributeError(f'module {repr(__name__)} has no attribute {repr(name)}')