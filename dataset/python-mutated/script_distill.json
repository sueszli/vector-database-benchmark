[
    {
        "func_name": "main",
        "original": "def main(_):\n    args = config_distill.get_args_for_config(FLAGS.config_name)\n    args.logdir = FLAGS.logdir\n    args.solver.num_workers = FLAGS.num_workers\n    args.solver.task = FLAGS.task\n    args.solver.ps_tasks = FLAGS.ps_tasks\n    args.solver.master = FLAGS.master\n    args.buildinger.env_class = nav_env.MeshMapper\n    fu.makedirs(args.logdir)\n    args.buildinger.logdir = args.logdir\n    R = nav_env.get_multiplexor_class(args.buildinger, args.solver.task)\n    if False:\n        pr = cProfile.Profile()\n        pr.enable()\n        rng = np.random.RandomState(0)\n        for i in range(1):\n            (b, instances_perturbs) = R.sample_building(rng)\n            inputs = b.worker(*instances_perturbs)\n            for j in range(inputs['imgs'].shape[0]):\n                p = os.path.join('tmp', '{:d}.png'.format(j))\n                img = inputs['imgs'][j, 0, :, :, :3] * 1\n                img = img.astype(np.uint8)\n                fu.write_image(p, img)\n            print(inputs['imgs'].shape)\n            inputs = R.pre(inputs)\n        pr.disable()\n        pr.print_stats(2)\n    if args.control.train:\n        if not gfile.Exists(args.logdir):\n            gfile.MakeDirs(args.logdir)\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        config = tf.ConfigProto()\n        config.device_count['GPU'] = 1\n        config.gpu_options.allow_growth = True\n        config.gpu_options.per_process_gpu_memory_fraction = 0.8\n        with m.tf_graph.as_default():\n            with tf.device(tf.train.replica_device_setter(args.solver.ps_tasks)):\n                m = distill.setup_to_run(m, args, is_training=True, batch_norm_is_training=True)\n                train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, 'train'), rng_seed=args.solver.task, is_chief=args.solver.task == 0, iters=1, train_display_interval=args.summary.display_interval)\n                final_loss = slim.learning.train(train_op=m.train_op, logdir=args.logdir, master=args.solver.master, is_chief=args.solver.task == 0, number_of_steps=args.solver.max_steps, train_step_fn=tf_utils.train_step_custom, train_step_kwargs=train_step_kwargs, global_step=m.global_step_op, init_op=m.init_op, init_fn=m.init_fn, sync_optimizer=m.sync_optimizer, saver=m.saver_op, summary_op=None, session_config=config)\n    if args.control.test:\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        checkpoint_dir = os.path.join(format(args.logdir))\n        with m.tf_graph.as_default():\n            m = distill.setup_to_run(m, args, is_training=False, batch_norm_is_training=args.control.force_batchnorm_is_training_at_test)\n            train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, args.control.test_name), rng_seed=args.solver.task + 1, is_chief=args.solver.task == 0, iters=args.summary.test_iters, train_display_interval=None)\n            sv = slim.learning.supervisor.Supervisor(graph=ops.get_default_graph(), logdir=None, init_op=m.init_op, summary_op=None, summary_writer=None, global_step=None, saver=m.saver_op)\n            last_checkpoint = None\n            while True:\n                last_checkpoint = slim.evaluation.wait_for_new_checkpoint(checkpoint_dir, last_checkpoint)\n                checkpoint_iter = int(os.path.basename(last_checkpoint).split('-')[1])\n                start = time.time()\n                logging.info('Starting evaluation at %s using checkpoint %s.', time.strftime('%Y-%m-%d-%H:%M:%S', time.localtime()), last_checkpoint)\n                config = tf.ConfigProto()\n                config.device_count['GPU'] = 1\n                config.gpu_options.allow_growth = True\n                config.gpu_options.per_process_gpu_memory_fraction = 0.8\n                with sv.managed_session(args.solver.master, config=config, start_standard_services=False) as sess:\n                    sess.run(m.init_op)\n                    sv.saver.restore(sess, last_checkpoint)\n                    sv.start_queue_runners(sess)\n                    (vals, _) = tf_utils.train_step_custom(sess, None, m.global_step_op, train_step_kwargs, mode='val')\n                    if checkpoint_iter >= args.solver.max_steps:\n                        break",
        "mutated": [
            "def main(_):\n    if False:\n        i = 10\n    args = config_distill.get_args_for_config(FLAGS.config_name)\n    args.logdir = FLAGS.logdir\n    args.solver.num_workers = FLAGS.num_workers\n    args.solver.task = FLAGS.task\n    args.solver.ps_tasks = FLAGS.ps_tasks\n    args.solver.master = FLAGS.master\n    args.buildinger.env_class = nav_env.MeshMapper\n    fu.makedirs(args.logdir)\n    args.buildinger.logdir = args.logdir\n    R = nav_env.get_multiplexor_class(args.buildinger, args.solver.task)\n    if False:\n        pr = cProfile.Profile()\n        pr.enable()\n        rng = np.random.RandomState(0)\n        for i in range(1):\n            (b, instances_perturbs) = R.sample_building(rng)\n            inputs = b.worker(*instances_perturbs)\n            for j in range(inputs['imgs'].shape[0]):\n                p = os.path.join('tmp', '{:d}.png'.format(j))\n                img = inputs['imgs'][j, 0, :, :, :3] * 1\n                img = img.astype(np.uint8)\n                fu.write_image(p, img)\n            print(inputs['imgs'].shape)\n            inputs = R.pre(inputs)\n        pr.disable()\n        pr.print_stats(2)\n    if args.control.train:\n        if not gfile.Exists(args.logdir):\n            gfile.MakeDirs(args.logdir)\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        config = tf.ConfigProto()\n        config.device_count['GPU'] = 1\n        config.gpu_options.allow_growth = True\n        config.gpu_options.per_process_gpu_memory_fraction = 0.8\n        with m.tf_graph.as_default():\n            with tf.device(tf.train.replica_device_setter(args.solver.ps_tasks)):\n                m = distill.setup_to_run(m, args, is_training=True, batch_norm_is_training=True)\n                train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, 'train'), rng_seed=args.solver.task, is_chief=args.solver.task == 0, iters=1, train_display_interval=args.summary.display_interval)\n                final_loss = slim.learning.train(train_op=m.train_op, logdir=args.logdir, master=args.solver.master, is_chief=args.solver.task == 0, number_of_steps=args.solver.max_steps, train_step_fn=tf_utils.train_step_custom, train_step_kwargs=train_step_kwargs, global_step=m.global_step_op, init_op=m.init_op, init_fn=m.init_fn, sync_optimizer=m.sync_optimizer, saver=m.saver_op, summary_op=None, session_config=config)\n    if args.control.test:\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        checkpoint_dir = os.path.join(format(args.logdir))\n        with m.tf_graph.as_default():\n            m = distill.setup_to_run(m, args, is_training=False, batch_norm_is_training=args.control.force_batchnorm_is_training_at_test)\n            train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, args.control.test_name), rng_seed=args.solver.task + 1, is_chief=args.solver.task == 0, iters=args.summary.test_iters, train_display_interval=None)\n            sv = slim.learning.supervisor.Supervisor(graph=ops.get_default_graph(), logdir=None, init_op=m.init_op, summary_op=None, summary_writer=None, global_step=None, saver=m.saver_op)\n            last_checkpoint = None\n            while True:\n                last_checkpoint = slim.evaluation.wait_for_new_checkpoint(checkpoint_dir, last_checkpoint)\n                checkpoint_iter = int(os.path.basename(last_checkpoint).split('-')[1])\n                start = time.time()\n                logging.info('Starting evaluation at %s using checkpoint %s.', time.strftime('%Y-%m-%d-%H:%M:%S', time.localtime()), last_checkpoint)\n                config = tf.ConfigProto()\n                config.device_count['GPU'] = 1\n                config.gpu_options.allow_growth = True\n                config.gpu_options.per_process_gpu_memory_fraction = 0.8\n                with sv.managed_session(args.solver.master, config=config, start_standard_services=False) as sess:\n                    sess.run(m.init_op)\n                    sv.saver.restore(sess, last_checkpoint)\n                    sv.start_queue_runners(sess)\n                    (vals, _) = tf_utils.train_step_custom(sess, None, m.global_step_op, train_step_kwargs, mode='val')\n                    if checkpoint_iter >= args.solver.max_steps:\n                        break",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    args = config_distill.get_args_for_config(FLAGS.config_name)\n    args.logdir = FLAGS.logdir\n    args.solver.num_workers = FLAGS.num_workers\n    args.solver.task = FLAGS.task\n    args.solver.ps_tasks = FLAGS.ps_tasks\n    args.solver.master = FLAGS.master\n    args.buildinger.env_class = nav_env.MeshMapper\n    fu.makedirs(args.logdir)\n    args.buildinger.logdir = args.logdir\n    R = nav_env.get_multiplexor_class(args.buildinger, args.solver.task)\n    if False:\n        pr = cProfile.Profile()\n        pr.enable()\n        rng = np.random.RandomState(0)\n        for i in range(1):\n            (b, instances_perturbs) = R.sample_building(rng)\n            inputs = b.worker(*instances_perturbs)\n            for j in range(inputs['imgs'].shape[0]):\n                p = os.path.join('tmp', '{:d}.png'.format(j))\n                img = inputs['imgs'][j, 0, :, :, :3] * 1\n                img = img.astype(np.uint8)\n                fu.write_image(p, img)\n            print(inputs['imgs'].shape)\n            inputs = R.pre(inputs)\n        pr.disable()\n        pr.print_stats(2)\n    if args.control.train:\n        if not gfile.Exists(args.logdir):\n            gfile.MakeDirs(args.logdir)\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        config = tf.ConfigProto()\n        config.device_count['GPU'] = 1\n        config.gpu_options.allow_growth = True\n        config.gpu_options.per_process_gpu_memory_fraction = 0.8\n        with m.tf_graph.as_default():\n            with tf.device(tf.train.replica_device_setter(args.solver.ps_tasks)):\n                m = distill.setup_to_run(m, args, is_training=True, batch_norm_is_training=True)\n                train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, 'train'), rng_seed=args.solver.task, is_chief=args.solver.task == 0, iters=1, train_display_interval=args.summary.display_interval)\n                final_loss = slim.learning.train(train_op=m.train_op, logdir=args.logdir, master=args.solver.master, is_chief=args.solver.task == 0, number_of_steps=args.solver.max_steps, train_step_fn=tf_utils.train_step_custom, train_step_kwargs=train_step_kwargs, global_step=m.global_step_op, init_op=m.init_op, init_fn=m.init_fn, sync_optimizer=m.sync_optimizer, saver=m.saver_op, summary_op=None, session_config=config)\n    if args.control.test:\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        checkpoint_dir = os.path.join(format(args.logdir))\n        with m.tf_graph.as_default():\n            m = distill.setup_to_run(m, args, is_training=False, batch_norm_is_training=args.control.force_batchnorm_is_training_at_test)\n            train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, args.control.test_name), rng_seed=args.solver.task + 1, is_chief=args.solver.task == 0, iters=args.summary.test_iters, train_display_interval=None)\n            sv = slim.learning.supervisor.Supervisor(graph=ops.get_default_graph(), logdir=None, init_op=m.init_op, summary_op=None, summary_writer=None, global_step=None, saver=m.saver_op)\n            last_checkpoint = None\n            while True:\n                last_checkpoint = slim.evaluation.wait_for_new_checkpoint(checkpoint_dir, last_checkpoint)\n                checkpoint_iter = int(os.path.basename(last_checkpoint).split('-')[1])\n                start = time.time()\n                logging.info('Starting evaluation at %s using checkpoint %s.', time.strftime('%Y-%m-%d-%H:%M:%S', time.localtime()), last_checkpoint)\n                config = tf.ConfigProto()\n                config.device_count['GPU'] = 1\n                config.gpu_options.allow_growth = True\n                config.gpu_options.per_process_gpu_memory_fraction = 0.8\n                with sv.managed_session(args.solver.master, config=config, start_standard_services=False) as sess:\n                    sess.run(m.init_op)\n                    sv.saver.restore(sess, last_checkpoint)\n                    sv.start_queue_runners(sess)\n                    (vals, _) = tf_utils.train_step_custom(sess, None, m.global_step_op, train_step_kwargs, mode='val')\n                    if checkpoint_iter >= args.solver.max_steps:\n                        break",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    args = config_distill.get_args_for_config(FLAGS.config_name)\n    args.logdir = FLAGS.logdir\n    args.solver.num_workers = FLAGS.num_workers\n    args.solver.task = FLAGS.task\n    args.solver.ps_tasks = FLAGS.ps_tasks\n    args.solver.master = FLAGS.master\n    args.buildinger.env_class = nav_env.MeshMapper\n    fu.makedirs(args.logdir)\n    args.buildinger.logdir = args.logdir\n    R = nav_env.get_multiplexor_class(args.buildinger, args.solver.task)\n    if False:\n        pr = cProfile.Profile()\n        pr.enable()\n        rng = np.random.RandomState(0)\n        for i in range(1):\n            (b, instances_perturbs) = R.sample_building(rng)\n            inputs = b.worker(*instances_perturbs)\n            for j in range(inputs['imgs'].shape[0]):\n                p = os.path.join('tmp', '{:d}.png'.format(j))\n                img = inputs['imgs'][j, 0, :, :, :3] * 1\n                img = img.astype(np.uint8)\n                fu.write_image(p, img)\n            print(inputs['imgs'].shape)\n            inputs = R.pre(inputs)\n        pr.disable()\n        pr.print_stats(2)\n    if args.control.train:\n        if not gfile.Exists(args.logdir):\n            gfile.MakeDirs(args.logdir)\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        config = tf.ConfigProto()\n        config.device_count['GPU'] = 1\n        config.gpu_options.allow_growth = True\n        config.gpu_options.per_process_gpu_memory_fraction = 0.8\n        with m.tf_graph.as_default():\n            with tf.device(tf.train.replica_device_setter(args.solver.ps_tasks)):\n                m = distill.setup_to_run(m, args, is_training=True, batch_norm_is_training=True)\n                train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, 'train'), rng_seed=args.solver.task, is_chief=args.solver.task == 0, iters=1, train_display_interval=args.summary.display_interval)\n                final_loss = slim.learning.train(train_op=m.train_op, logdir=args.logdir, master=args.solver.master, is_chief=args.solver.task == 0, number_of_steps=args.solver.max_steps, train_step_fn=tf_utils.train_step_custom, train_step_kwargs=train_step_kwargs, global_step=m.global_step_op, init_op=m.init_op, init_fn=m.init_fn, sync_optimizer=m.sync_optimizer, saver=m.saver_op, summary_op=None, session_config=config)\n    if args.control.test:\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        checkpoint_dir = os.path.join(format(args.logdir))\n        with m.tf_graph.as_default():\n            m = distill.setup_to_run(m, args, is_training=False, batch_norm_is_training=args.control.force_batchnorm_is_training_at_test)\n            train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, args.control.test_name), rng_seed=args.solver.task + 1, is_chief=args.solver.task == 0, iters=args.summary.test_iters, train_display_interval=None)\n            sv = slim.learning.supervisor.Supervisor(graph=ops.get_default_graph(), logdir=None, init_op=m.init_op, summary_op=None, summary_writer=None, global_step=None, saver=m.saver_op)\n            last_checkpoint = None\n            while True:\n                last_checkpoint = slim.evaluation.wait_for_new_checkpoint(checkpoint_dir, last_checkpoint)\n                checkpoint_iter = int(os.path.basename(last_checkpoint).split('-')[1])\n                start = time.time()\n                logging.info('Starting evaluation at %s using checkpoint %s.', time.strftime('%Y-%m-%d-%H:%M:%S', time.localtime()), last_checkpoint)\n                config = tf.ConfigProto()\n                config.device_count['GPU'] = 1\n                config.gpu_options.allow_growth = True\n                config.gpu_options.per_process_gpu_memory_fraction = 0.8\n                with sv.managed_session(args.solver.master, config=config, start_standard_services=False) as sess:\n                    sess.run(m.init_op)\n                    sv.saver.restore(sess, last_checkpoint)\n                    sv.start_queue_runners(sess)\n                    (vals, _) = tf_utils.train_step_custom(sess, None, m.global_step_op, train_step_kwargs, mode='val')\n                    if checkpoint_iter >= args.solver.max_steps:\n                        break",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    args = config_distill.get_args_for_config(FLAGS.config_name)\n    args.logdir = FLAGS.logdir\n    args.solver.num_workers = FLAGS.num_workers\n    args.solver.task = FLAGS.task\n    args.solver.ps_tasks = FLAGS.ps_tasks\n    args.solver.master = FLAGS.master\n    args.buildinger.env_class = nav_env.MeshMapper\n    fu.makedirs(args.logdir)\n    args.buildinger.logdir = args.logdir\n    R = nav_env.get_multiplexor_class(args.buildinger, args.solver.task)\n    if False:\n        pr = cProfile.Profile()\n        pr.enable()\n        rng = np.random.RandomState(0)\n        for i in range(1):\n            (b, instances_perturbs) = R.sample_building(rng)\n            inputs = b.worker(*instances_perturbs)\n            for j in range(inputs['imgs'].shape[0]):\n                p = os.path.join('tmp', '{:d}.png'.format(j))\n                img = inputs['imgs'][j, 0, :, :, :3] * 1\n                img = img.astype(np.uint8)\n                fu.write_image(p, img)\n            print(inputs['imgs'].shape)\n            inputs = R.pre(inputs)\n        pr.disable()\n        pr.print_stats(2)\n    if args.control.train:\n        if not gfile.Exists(args.logdir):\n            gfile.MakeDirs(args.logdir)\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        config = tf.ConfigProto()\n        config.device_count['GPU'] = 1\n        config.gpu_options.allow_growth = True\n        config.gpu_options.per_process_gpu_memory_fraction = 0.8\n        with m.tf_graph.as_default():\n            with tf.device(tf.train.replica_device_setter(args.solver.ps_tasks)):\n                m = distill.setup_to_run(m, args, is_training=True, batch_norm_is_training=True)\n                train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, 'train'), rng_seed=args.solver.task, is_chief=args.solver.task == 0, iters=1, train_display_interval=args.summary.display_interval)\n                final_loss = slim.learning.train(train_op=m.train_op, logdir=args.logdir, master=args.solver.master, is_chief=args.solver.task == 0, number_of_steps=args.solver.max_steps, train_step_fn=tf_utils.train_step_custom, train_step_kwargs=train_step_kwargs, global_step=m.global_step_op, init_op=m.init_op, init_fn=m.init_fn, sync_optimizer=m.sync_optimizer, saver=m.saver_op, summary_op=None, session_config=config)\n    if args.control.test:\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        checkpoint_dir = os.path.join(format(args.logdir))\n        with m.tf_graph.as_default():\n            m = distill.setup_to_run(m, args, is_training=False, batch_norm_is_training=args.control.force_batchnorm_is_training_at_test)\n            train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, args.control.test_name), rng_seed=args.solver.task + 1, is_chief=args.solver.task == 0, iters=args.summary.test_iters, train_display_interval=None)\n            sv = slim.learning.supervisor.Supervisor(graph=ops.get_default_graph(), logdir=None, init_op=m.init_op, summary_op=None, summary_writer=None, global_step=None, saver=m.saver_op)\n            last_checkpoint = None\n            while True:\n                last_checkpoint = slim.evaluation.wait_for_new_checkpoint(checkpoint_dir, last_checkpoint)\n                checkpoint_iter = int(os.path.basename(last_checkpoint).split('-')[1])\n                start = time.time()\n                logging.info('Starting evaluation at %s using checkpoint %s.', time.strftime('%Y-%m-%d-%H:%M:%S', time.localtime()), last_checkpoint)\n                config = tf.ConfigProto()\n                config.device_count['GPU'] = 1\n                config.gpu_options.allow_growth = True\n                config.gpu_options.per_process_gpu_memory_fraction = 0.8\n                with sv.managed_session(args.solver.master, config=config, start_standard_services=False) as sess:\n                    sess.run(m.init_op)\n                    sv.saver.restore(sess, last_checkpoint)\n                    sv.start_queue_runners(sess)\n                    (vals, _) = tf_utils.train_step_custom(sess, None, m.global_step_op, train_step_kwargs, mode='val')\n                    if checkpoint_iter >= args.solver.max_steps:\n                        break",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    args = config_distill.get_args_for_config(FLAGS.config_name)\n    args.logdir = FLAGS.logdir\n    args.solver.num_workers = FLAGS.num_workers\n    args.solver.task = FLAGS.task\n    args.solver.ps_tasks = FLAGS.ps_tasks\n    args.solver.master = FLAGS.master\n    args.buildinger.env_class = nav_env.MeshMapper\n    fu.makedirs(args.logdir)\n    args.buildinger.logdir = args.logdir\n    R = nav_env.get_multiplexor_class(args.buildinger, args.solver.task)\n    if False:\n        pr = cProfile.Profile()\n        pr.enable()\n        rng = np.random.RandomState(0)\n        for i in range(1):\n            (b, instances_perturbs) = R.sample_building(rng)\n            inputs = b.worker(*instances_perturbs)\n            for j in range(inputs['imgs'].shape[0]):\n                p = os.path.join('tmp', '{:d}.png'.format(j))\n                img = inputs['imgs'][j, 0, :, :, :3] * 1\n                img = img.astype(np.uint8)\n                fu.write_image(p, img)\n            print(inputs['imgs'].shape)\n            inputs = R.pre(inputs)\n        pr.disable()\n        pr.print_stats(2)\n    if args.control.train:\n        if not gfile.Exists(args.logdir):\n            gfile.MakeDirs(args.logdir)\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        config = tf.ConfigProto()\n        config.device_count['GPU'] = 1\n        config.gpu_options.allow_growth = True\n        config.gpu_options.per_process_gpu_memory_fraction = 0.8\n        with m.tf_graph.as_default():\n            with tf.device(tf.train.replica_device_setter(args.solver.ps_tasks)):\n                m = distill.setup_to_run(m, args, is_training=True, batch_norm_is_training=True)\n                train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, 'train'), rng_seed=args.solver.task, is_chief=args.solver.task == 0, iters=1, train_display_interval=args.summary.display_interval)\n                final_loss = slim.learning.train(train_op=m.train_op, logdir=args.logdir, master=args.solver.master, is_chief=args.solver.task == 0, number_of_steps=args.solver.max_steps, train_step_fn=tf_utils.train_step_custom, train_step_kwargs=train_step_kwargs, global_step=m.global_step_op, init_op=m.init_op, init_fn=m.init_fn, sync_optimizer=m.sync_optimizer, saver=m.saver_op, summary_op=None, session_config=config)\n    if args.control.test:\n        m = utils.Foo()\n        m.tf_graph = tf.Graph()\n        checkpoint_dir = os.path.join(format(args.logdir))\n        with m.tf_graph.as_default():\n            m = distill.setup_to_run(m, args, is_training=False, batch_norm_is_training=args.control.force_batchnorm_is_training_at_test)\n            train_step_kwargs = distill.setup_train_step_kwargs_mesh(m, R, os.path.join(args.logdir, args.control.test_name), rng_seed=args.solver.task + 1, is_chief=args.solver.task == 0, iters=args.summary.test_iters, train_display_interval=None)\n            sv = slim.learning.supervisor.Supervisor(graph=ops.get_default_graph(), logdir=None, init_op=m.init_op, summary_op=None, summary_writer=None, global_step=None, saver=m.saver_op)\n            last_checkpoint = None\n            while True:\n                last_checkpoint = slim.evaluation.wait_for_new_checkpoint(checkpoint_dir, last_checkpoint)\n                checkpoint_iter = int(os.path.basename(last_checkpoint).split('-')[1])\n                start = time.time()\n                logging.info('Starting evaluation at %s using checkpoint %s.', time.strftime('%Y-%m-%d-%H:%M:%S', time.localtime()), last_checkpoint)\n                config = tf.ConfigProto()\n                config.device_count['GPU'] = 1\n                config.gpu_options.allow_growth = True\n                config.gpu_options.per_process_gpu_memory_fraction = 0.8\n                with sv.managed_session(args.solver.master, config=config, start_standard_services=False) as sess:\n                    sess.run(m.init_op)\n                    sv.saver.restore(sess, last_checkpoint)\n                    sv.start_queue_runners(sess)\n                    (vals, _) = tf_utils.train_step_custom(sess, None, m.global_step_op, train_step_kwargs, mode='val')\n                    if checkpoint_iter >= args.solver.max_steps:\n                        break"
        ]
    }
]