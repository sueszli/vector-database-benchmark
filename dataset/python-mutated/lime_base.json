[
    {
        "func_name": "__init__",
        "original": "def __init__(self, kernel_fn, verbose=False, random_state=None):\n    \"\"\"Init function\n\n        Args:\n            kernel_fn: function that transforms an array of distances into an\n                        array of proximity values (floats).\n            verbose: if true, print local prediction values from linear model.\n            random_state: an integer or numpy.RandomState that will be used to\n                generate random numbers. If None, the random state will be\n                initialized using the internal numpy seed.\n        \"\"\"\n    self.kernel_fn = kernel_fn\n    self.verbose = verbose\n    self.random_state = check_random_state(random_state)",
        "mutated": [
            "def __init__(self, kernel_fn, verbose=False, random_state=None):\n    if False:\n        i = 10\n    'Init function\\n\\n        Args:\\n            kernel_fn: function that transforms an array of distances into an\\n                        array of proximity values (floats).\\n            verbose: if true, print local prediction values from linear model.\\n            random_state: an integer or numpy.RandomState that will be used to\\n                generate random numbers. If None, the random state will be\\n                initialized using the internal numpy seed.\\n        '\n    self.kernel_fn = kernel_fn\n    self.verbose = verbose\n    self.random_state = check_random_state(random_state)",
            "def __init__(self, kernel_fn, verbose=False, random_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Init function\\n\\n        Args:\\n            kernel_fn: function that transforms an array of distances into an\\n                        array of proximity values (floats).\\n            verbose: if true, print local prediction values from linear model.\\n            random_state: an integer or numpy.RandomState that will be used to\\n                generate random numbers. If None, the random state will be\\n                initialized using the internal numpy seed.\\n        '\n    self.kernel_fn = kernel_fn\n    self.verbose = verbose\n    self.random_state = check_random_state(random_state)",
            "def __init__(self, kernel_fn, verbose=False, random_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Init function\\n\\n        Args:\\n            kernel_fn: function that transforms an array of distances into an\\n                        array of proximity values (floats).\\n            verbose: if true, print local prediction values from linear model.\\n            random_state: an integer or numpy.RandomState that will be used to\\n                generate random numbers. If None, the random state will be\\n                initialized using the internal numpy seed.\\n        '\n    self.kernel_fn = kernel_fn\n    self.verbose = verbose\n    self.random_state = check_random_state(random_state)",
            "def __init__(self, kernel_fn, verbose=False, random_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Init function\\n\\n        Args:\\n            kernel_fn: function that transforms an array of distances into an\\n                        array of proximity values (floats).\\n            verbose: if true, print local prediction values from linear model.\\n            random_state: an integer or numpy.RandomState that will be used to\\n                generate random numbers. If None, the random state will be\\n                initialized using the internal numpy seed.\\n        '\n    self.kernel_fn = kernel_fn\n    self.verbose = verbose\n    self.random_state = check_random_state(random_state)",
            "def __init__(self, kernel_fn, verbose=False, random_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Init function\\n\\n        Args:\\n            kernel_fn: function that transforms an array of distances into an\\n                        array of proximity values (floats).\\n            verbose: if true, print local prediction values from linear model.\\n            random_state: an integer or numpy.RandomState that will be used to\\n                generate random numbers. If None, the random state will be\\n                initialized using the internal numpy seed.\\n        '\n    self.kernel_fn = kernel_fn\n    self.verbose = verbose\n    self.random_state = check_random_state(random_state)"
        ]
    },
    {
        "func_name": "generate_lars_path",
        "original": "@staticmethod\ndef generate_lars_path(weighted_data, weighted_labels):\n    \"\"\"Generates the lars path for weighted data.\n\n        Args:\n            weighted_data: data that has been weighted by kernel\n            weighted_label: labels, weighted by kernel\n\n        Returns:\n            (alphas, coefs), both are arrays corresponding to the\n            regularization parameter and coefficients, respectively\n        \"\"\"\n    x_vector = weighted_data\n    (alphas, _, coefs) = lars_path(x_vector, weighted_labels, method='lasso', verbose=False)\n    return (alphas, coefs)",
        "mutated": [
            "@staticmethod\ndef generate_lars_path(weighted_data, weighted_labels):\n    if False:\n        i = 10\n    'Generates the lars path for weighted data.\\n\\n        Args:\\n            weighted_data: data that has been weighted by kernel\\n            weighted_label: labels, weighted by kernel\\n\\n        Returns:\\n            (alphas, coefs), both are arrays corresponding to the\\n            regularization parameter and coefficients, respectively\\n        '\n    x_vector = weighted_data\n    (alphas, _, coefs) = lars_path(x_vector, weighted_labels, method='lasso', verbose=False)\n    return (alphas, coefs)",
            "@staticmethod\ndef generate_lars_path(weighted_data, weighted_labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generates the lars path for weighted data.\\n\\n        Args:\\n            weighted_data: data that has been weighted by kernel\\n            weighted_label: labels, weighted by kernel\\n\\n        Returns:\\n            (alphas, coefs), both are arrays corresponding to the\\n            regularization parameter and coefficients, respectively\\n        '\n    x_vector = weighted_data\n    (alphas, _, coefs) = lars_path(x_vector, weighted_labels, method='lasso', verbose=False)\n    return (alphas, coefs)",
            "@staticmethod\ndef generate_lars_path(weighted_data, weighted_labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generates the lars path for weighted data.\\n\\n        Args:\\n            weighted_data: data that has been weighted by kernel\\n            weighted_label: labels, weighted by kernel\\n\\n        Returns:\\n            (alphas, coefs), both are arrays corresponding to the\\n            regularization parameter and coefficients, respectively\\n        '\n    x_vector = weighted_data\n    (alphas, _, coefs) = lars_path(x_vector, weighted_labels, method='lasso', verbose=False)\n    return (alphas, coefs)",
            "@staticmethod\ndef generate_lars_path(weighted_data, weighted_labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generates the lars path for weighted data.\\n\\n        Args:\\n            weighted_data: data that has been weighted by kernel\\n            weighted_label: labels, weighted by kernel\\n\\n        Returns:\\n            (alphas, coefs), both are arrays corresponding to the\\n            regularization parameter and coefficients, respectively\\n        '\n    x_vector = weighted_data\n    (alphas, _, coefs) = lars_path(x_vector, weighted_labels, method='lasso', verbose=False)\n    return (alphas, coefs)",
            "@staticmethod\ndef generate_lars_path(weighted_data, weighted_labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generates the lars path for weighted data.\\n\\n        Args:\\n            weighted_data: data that has been weighted by kernel\\n            weighted_label: labels, weighted by kernel\\n\\n        Returns:\\n            (alphas, coefs), both are arrays corresponding to the\\n            regularization parameter and coefficients, respectively\\n        '\n    x_vector = weighted_data\n    (alphas, _, coefs) = lars_path(x_vector, weighted_labels, method='lasso', verbose=False)\n    return (alphas, coefs)"
        ]
    },
    {
        "func_name": "forward_selection",
        "original": "def forward_selection(self, data, labels, weights, num_features):\n    \"\"\"Iteratively adds features to the model\"\"\"\n    clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n    used_features = []\n    for _ in range(min(num_features, data.shape[1])):\n        max_ = -100000000\n        best = 0\n        for feature in range(data.shape[1]):\n            if feature in used_features:\n                continue\n            clf.fit(data[:, used_features + [feature]], labels, sample_weight=weights)\n            score = clf.score(data[:, used_features + [feature]], labels, sample_weight=weights)\n            if score > max_:\n                best = feature\n                max_ = score\n        used_features.append(best)\n    return np.array(used_features)",
        "mutated": [
            "def forward_selection(self, data, labels, weights, num_features):\n    if False:\n        i = 10\n    'Iteratively adds features to the model'\n    clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n    used_features = []\n    for _ in range(min(num_features, data.shape[1])):\n        max_ = -100000000\n        best = 0\n        for feature in range(data.shape[1]):\n            if feature in used_features:\n                continue\n            clf.fit(data[:, used_features + [feature]], labels, sample_weight=weights)\n            score = clf.score(data[:, used_features + [feature]], labels, sample_weight=weights)\n            if score > max_:\n                best = feature\n                max_ = score\n        used_features.append(best)\n    return np.array(used_features)",
            "def forward_selection(self, data, labels, weights, num_features):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Iteratively adds features to the model'\n    clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n    used_features = []\n    for _ in range(min(num_features, data.shape[1])):\n        max_ = -100000000\n        best = 0\n        for feature in range(data.shape[1]):\n            if feature in used_features:\n                continue\n            clf.fit(data[:, used_features + [feature]], labels, sample_weight=weights)\n            score = clf.score(data[:, used_features + [feature]], labels, sample_weight=weights)\n            if score > max_:\n                best = feature\n                max_ = score\n        used_features.append(best)\n    return np.array(used_features)",
            "def forward_selection(self, data, labels, weights, num_features):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Iteratively adds features to the model'\n    clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n    used_features = []\n    for _ in range(min(num_features, data.shape[1])):\n        max_ = -100000000\n        best = 0\n        for feature in range(data.shape[1]):\n            if feature in used_features:\n                continue\n            clf.fit(data[:, used_features + [feature]], labels, sample_weight=weights)\n            score = clf.score(data[:, used_features + [feature]], labels, sample_weight=weights)\n            if score > max_:\n                best = feature\n                max_ = score\n        used_features.append(best)\n    return np.array(used_features)",
            "def forward_selection(self, data, labels, weights, num_features):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Iteratively adds features to the model'\n    clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n    used_features = []\n    for _ in range(min(num_features, data.shape[1])):\n        max_ = -100000000\n        best = 0\n        for feature in range(data.shape[1]):\n            if feature in used_features:\n                continue\n            clf.fit(data[:, used_features + [feature]], labels, sample_weight=weights)\n            score = clf.score(data[:, used_features + [feature]], labels, sample_weight=weights)\n            if score > max_:\n                best = feature\n                max_ = score\n        used_features.append(best)\n    return np.array(used_features)",
            "def forward_selection(self, data, labels, weights, num_features):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Iteratively adds features to the model'\n    clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n    used_features = []\n    for _ in range(min(num_features, data.shape[1])):\n        max_ = -100000000\n        best = 0\n        for feature in range(data.shape[1]):\n            if feature in used_features:\n                continue\n            clf.fit(data[:, used_features + [feature]], labels, sample_weight=weights)\n            score = clf.score(data[:, used_features + [feature]], labels, sample_weight=weights)\n            if score > max_:\n                best = feature\n                max_ = score\n        used_features.append(best)\n    return np.array(used_features)"
        ]
    },
    {
        "func_name": "feature_selection",
        "original": "def feature_selection(self, data, labels, weights, num_features, method):\n    \"\"\"Selects features for the model. see explain_instance_with_data to\n           understand the parameters.\"\"\"\n    if method == 'none':\n        return np.array(range(data.shape[1]))\n    elif method == 'forward_selection':\n        return self.forward_selection(data, labels, weights, num_features)\n    elif method == 'highest_weights':\n        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n        clf.fit(data, labels, sample_weight=weights)\n        feature_weights = sorted(zip(range(data.shape[0]), clf.coef_ * data[0]), key=lambda x: np.abs(x[1]), reverse=True)\n        return np.array([x[0] for x in feature_weights[:num_features]])\n    elif method == 'lasso_path':\n        weighted_data = (data - np.average(data, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])\n        weighted_labels = (labels - np.average(labels, weights=weights)) * np.sqrt(weights)\n        nonzero = range(weighted_data.shape[1])\n        (_, coefs) = self.generate_lars_path(weighted_data, weighted_labels)\n        for i in range(len(coefs.T) - 1, 0, -1):\n            nonzero = coefs.T[i].nonzero()[0]\n            if len(nonzero) <= num_features:\n                break\n        used_features = nonzero\n        return used_features\n    elif method == 'auto':\n        if num_features <= 6:\n            n_method = 'forward_selection'\n        else:\n            n_method = 'highest_weights'\n        return self.feature_selection(data, labels, weights, num_features, n_method)",
        "mutated": [
            "def feature_selection(self, data, labels, weights, num_features, method):\n    if False:\n        i = 10\n    'Selects features for the model. see explain_instance_with_data to\\n           understand the parameters.'\n    if method == 'none':\n        return np.array(range(data.shape[1]))\n    elif method == 'forward_selection':\n        return self.forward_selection(data, labels, weights, num_features)\n    elif method == 'highest_weights':\n        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n        clf.fit(data, labels, sample_weight=weights)\n        feature_weights = sorted(zip(range(data.shape[0]), clf.coef_ * data[0]), key=lambda x: np.abs(x[1]), reverse=True)\n        return np.array([x[0] for x in feature_weights[:num_features]])\n    elif method == 'lasso_path':\n        weighted_data = (data - np.average(data, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])\n        weighted_labels = (labels - np.average(labels, weights=weights)) * np.sqrt(weights)\n        nonzero = range(weighted_data.shape[1])\n        (_, coefs) = self.generate_lars_path(weighted_data, weighted_labels)\n        for i in range(len(coefs.T) - 1, 0, -1):\n            nonzero = coefs.T[i].nonzero()[0]\n            if len(nonzero) <= num_features:\n                break\n        used_features = nonzero\n        return used_features\n    elif method == 'auto':\n        if num_features <= 6:\n            n_method = 'forward_selection'\n        else:\n            n_method = 'highest_weights'\n        return self.feature_selection(data, labels, weights, num_features, n_method)",
            "def feature_selection(self, data, labels, weights, num_features, method):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Selects features for the model. see explain_instance_with_data to\\n           understand the parameters.'\n    if method == 'none':\n        return np.array(range(data.shape[1]))\n    elif method == 'forward_selection':\n        return self.forward_selection(data, labels, weights, num_features)\n    elif method == 'highest_weights':\n        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n        clf.fit(data, labels, sample_weight=weights)\n        feature_weights = sorted(zip(range(data.shape[0]), clf.coef_ * data[0]), key=lambda x: np.abs(x[1]), reverse=True)\n        return np.array([x[0] for x in feature_weights[:num_features]])\n    elif method == 'lasso_path':\n        weighted_data = (data - np.average(data, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])\n        weighted_labels = (labels - np.average(labels, weights=weights)) * np.sqrt(weights)\n        nonzero = range(weighted_data.shape[1])\n        (_, coefs) = self.generate_lars_path(weighted_data, weighted_labels)\n        for i in range(len(coefs.T) - 1, 0, -1):\n            nonzero = coefs.T[i].nonzero()[0]\n            if len(nonzero) <= num_features:\n                break\n        used_features = nonzero\n        return used_features\n    elif method == 'auto':\n        if num_features <= 6:\n            n_method = 'forward_selection'\n        else:\n            n_method = 'highest_weights'\n        return self.feature_selection(data, labels, weights, num_features, n_method)",
            "def feature_selection(self, data, labels, weights, num_features, method):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Selects features for the model. see explain_instance_with_data to\\n           understand the parameters.'\n    if method == 'none':\n        return np.array(range(data.shape[1]))\n    elif method == 'forward_selection':\n        return self.forward_selection(data, labels, weights, num_features)\n    elif method == 'highest_weights':\n        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n        clf.fit(data, labels, sample_weight=weights)\n        feature_weights = sorted(zip(range(data.shape[0]), clf.coef_ * data[0]), key=lambda x: np.abs(x[1]), reverse=True)\n        return np.array([x[0] for x in feature_weights[:num_features]])\n    elif method == 'lasso_path':\n        weighted_data = (data - np.average(data, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])\n        weighted_labels = (labels - np.average(labels, weights=weights)) * np.sqrt(weights)\n        nonzero = range(weighted_data.shape[1])\n        (_, coefs) = self.generate_lars_path(weighted_data, weighted_labels)\n        for i in range(len(coefs.T) - 1, 0, -1):\n            nonzero = coefs.T[i].nonzero()[0]\n            if len(nonzero) <= num_features:\n                break\n        used_features = nonzero\n        return used_features\n    elif method == 'auto':\n        if num_features <= 6:\n            n_method = 'forward_selection'\n        else:\n            n_method = 'highest_weights'\n        return self.feature_selection(data, labels, weights, num_features, n_method)",
            "def feature_selection(self, data, labels, weights, num_features, method):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Selects features for the model. see explain_instance_with_data to\\n           understand the parameters.'\n    if method == 'none':\n        return np.array(range(data.shape[1]))\n    elif method == 'forward_selection':\n        return self.forward_selection(data, labels, weights, num_features)\n    elif method == 'highest_weights':\n        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n        clf.fit(data, labels, sample_weight=weights)\n        feature_weights = sorted(zip(range(data.shape[0]), clf.coef_ * data[0]), key=lambda x: np.abs(x[1]), reverse=True)\n        return np.array([x[0] for x in feature_weights[:num_features]])\n    elif method == 'lasso_path':\n        weighted_data = (data - np.average(data, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])\n        weighted_labels = (labels - np.average(labels, weights=weights)) * np.sqrt(weights)\n        nonzero = range(weighted_data.shape[1])\n        (_, coefs) = self.generate_lars_path(weighted_data, weighted_labels)\n        for i in range(len(coefs.T) - 1, 0, -1):\n            nonzero = coefs.T[i].nonzero()[0]\n            if len(nonzero) <= num_features:\n                break\n        used_features = nonzero\n        return used_features\n    elif method == 'auto':\n        if num_features <= 6:\n            n_method = 'forward_selection'\n        else:\n            n_method = 'highest_weights'\n        return self.feature_selection(data, labels, weights, num_features, n_method)",
            "def feature_selection(self, data, labels, weights, num_features, method):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Selects features for the model. see explain_instance_with_data to\\n           understand the parameters.'\n    if method == 'none':\n        return np.array(range(data.shape[1]))\n    elif method == 'forward_selection':\n        return self.forward_selection(data, labels, weights, num_features)\n    elif method == 'highest_weights':\n        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n        clf.fit(data, labels, sample_weight=weights)\n        feature_weights = sorted(zip(range(data.shape[0]), clf.coef_ * data[0]), key=lambda x: np.abs(x[1]), reverse=True)\n        return np.array([x[0] for x in feature_weights[:num_features]])\n    elif method == 'lasso_path':\n        weighted_data = (data - np.average(data, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])\n        weighted_labels = (labels - np.average(labels, weights=weights)) * np.sqrt(weights)\n        nonzero = range(weighted_data.shape[1])\n        (_, coefs) = self.generate_lars_path(weighted_data, weighted_labels)\n        for i in range(len(coefs.T) - 1, 0, -1):\n            nonzero = coefs.T[i].nonzero()[0]\n            if len(nonzero) <= num_features:\n                break\n        used_features = nonzero\n        return used_features\n    elif method == 'auto':\n        if num_features <= 6:\n            n_method = 'forward_selection'\n        else:\n            n_method = 'highest_weights'\n        return self.feature_selection(data, labels, weights, num_features, n_method)"
        ]
    },
    {
        "func_name": "explain_instance_with_data",
        "original": "def explain_instance_with_data(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection='auto', model_regressor=None):\n    \"\"\"Takes perturbed data, labels and distances, returns explanation.\n\n        Args:\n            neighborhood_data: perturbed data, 2d array. first element is\n                               assumed to be the original data point.\n            neighborhood_labels: corresponding perturbed labels. should have as\n                                 many columns as the number of possible labels.\n            distances: distances to original data point.\n            label: label for which we want an explanation\n            num_features: maximum number of features in explanation\n            feature_selection: how to select num_features. options are:\n                'forward_selection': iteratively add features to the model.\n                    This is costly when num_features is high\n                'highest_weights': selects the features that have the highest\n                    product of absolute weight * original data point when\n                    learning with all the features\n                'lasso_path': chooses features based on the lasso\n                    regularization path\n                'none': uses all features, ignores num_features\n                'auto': uses forward_selection if num_features <= 6, and\n                    'highest_weights' otherwise.\n            model_regressor: sklearn regressor to use in explanation.\n                Defaults to Ridge regression if None. Must have\n                model_regressor.coef_ and 'sample_weight' as a parameter\n                to model_regressor.fit()\n\n        Returns:\n            (intercept, exp, score, local_pred):\n            intercept is a float.\n            exp is a sorted list of tuples, where each tuple (x,y) corresponds\n            to the feature id (x) and the local weight (y). The list is sorted\n            by decreasing absolute value of y.\n            score is the R^2 value of the returned explanation\n            local_pred is the prediction of the explanation model on the original instance\n        \"\"\"\n    weights = self.kernel_fn(distances)\n    labels_column = neighborhood_labels[:, label]\n    used_features = self.feature_selection(neighborhood_data, labels_column, weights, num_features, feature_selection)\n    if model_regressor is None:\n        model_regressor = Ridge(alpha=1, fit_intercept=True, random_state=self.random_state)\n    easy_model = model_regressor\n    easy_model.fit(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    prediction_score = easy_model.score(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n    if self.verbose:\n        print('Intercept', easy_model.intercept_)\n        print('Prediction_local', local_pred)\n        print('Right:', neighborhood_labels[0, label])\n    return (easy_model.intercept_, sorted(zip(used_features, easy_model.coef_), key=lambda x: np.abs(x[1]), reverse=True), prediction_score, local_pred)",
        "mutated": [
            "def explain_instance_with_data(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection='auto', model_regressor=None):\n    if False:\n        i = 10\n    \"Takes perturbed data, labels and distances, returns explanation.\\n\\n        Args:\\n            neighborhood_data: perturbed data, 2d array. first element is\\n                               assumed to be the original data point.\\n            neighborhood_labels: corresponding perturbed labels. should have as\\n                                 many columns as the number of possible labels.\\n            distances: distances to original data point.\\n            label: label for which we want an explanation\\n            num_features: maximum number of features in explanation\\n            feature_selection: how to select num_features. options are:\\n                'forward_selection': iteratively add features to the model.\\n                    This is costly when num_features is high\\n                'highest_weights': selects the features that have the highest\\n                    product of absolute weight * original data point when\\n                    learning with all the features\\n                'lasso_path': chooses features based on the lasso\\n                    regularization path\\n                'none': uses all features, ignores num_features\\n                'auto': uses forward_selection if num_features <= 6, and\\n                    'highest_weights' otherwise.\\n            model_regressor: sklearn regressor to use in explanation.\\n                Defaults to Ridge regression if None. Must have\\n                model_regressor.coef_ and 'sample_weight' as a parameter\\n                to model_regressor.fit()\\n\\n        Returns:\\n            (intercept, exp, score, local_pred):\\n            intercept is a float.\\n            exp is a sorted list of tuples, where each tuple (x,y) corresponds\\n            to the feature id (x) and the local weight (y). The list is sorted\\n            by decreasing absolute value of y.\\n            score is the R^2 value of the returned explanation\\n            local_pred is the prediction of the explanation model on the original instance\\n        \"\n    weights = self.kernel_fn(distances)\n    labels_column = neighborhood_labels[:, label]\n    used_features = self.feature_selection(neighborhood_data, labels_column, weights, num_features, feature_selection)\n    if model_regressor is None:\n        model_regressor = Ridge(alpha=1, fit_intercept=True, random_state=self.random_state)\n    easy_model = model_regressor\n    easy_model.fit(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    prediction_score = easy_model.score(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n    if self.verbose:\n        print('Intercept', easy_model.intercept_)\n        print('Prediction_local', local_pred)\n        print('Right:', neighborhood_labels[0, label])\n    return (easy_model.intercept_, sorted(zip(used_features, easy_model.coef_), key=lambda x: np.abs(x[1]), reverse=True), prediction_score, local_pred)",
            "def explain_instance_with_data(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection='auto', model_regressor=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Takes perturbed data, labels and distances, returns explanation.\\n\\n        Args:\\n            neighborhood_data: perturbed data, 2d array. first element is\\n                               assumed to be the original data point.\\n            neighborhood_labels: corresponding perturbed labels. should have as\\n                                 many columns as the number of possible labels.\\n            distances: distances to original data point.\\n            label: label for which we want an explanation\\n            num_features: maximum number of features in explanation\\n            feature_selection: how to select num_features. options are:\\n                'forward_selection': iteratively add features to the model.\\n                    This is costly when num_features is high\\n                'highest_weights': selects the features that have the highest\\n                    product of absolute weight * original data point when\\n                    learning with all the features\\n                'lasso_path': chooses features based on the lasso\\n                    regularization path\\n                'none': uses all features, ignores num_features\\n                'auto': uses forward_selection if num_features <= 6, and\\n                    'highest_weights' otherwise.\\n            model_regressor: sklearn regressor to use in explanation.\\n                Defaults to Ridge regression if None. Must have\\n                model_regressor.coef_ and 'sample_weight' as a parameter\\n                to model_regressor.fit()\\n\\n        Returns:\\n            (intercept, exp, score, local_pred):\\n            intercept is a float.\\n            exp is a sorted list of tuples, where each tuple (x,y) corresponds\\n            to the feature id (x) and the local weight (y). The list is sorted\\n            by decreasing absolute value of y.\\n            score is the R^2 value of the returned explanation\\n            local_pred is the prediction of the explanation model on the original instance\\n        \"\n    weights = self.kernel_fn(distances)\n    labels_column = neighborhood_labels[:, label]\n    used_features = self.feature_selection(neighborhood_data, labels_column, weights, num_features, feature_selection)\n    if model_regressor is None:\n        model_regressor = Ridge(alpha=1, fit_intercept=True, random_state=self.random_state)\n    easy_model = model_regressor\n    easy_model.fit(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    prediction_score = easy_model.score(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n    if self.verbose:\n        print('Intercept', easy_model.intercept_)\n        print('Prediction_local', local_pred)\n        print('Right:', neighborhood_labels[0, label])\n    return (easy_model.intercept_, sorted(zip(used_features, easy_model.coef_), key=lambda x: np.abs(x[1]), reverse=True), prediction_score, local_pred)",
            "def explain_instance_with_data(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection='auto', model_regressor=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Takes perturbed data, labels and distances, returns explanation.\\n\\n        Args:\\n            neighborhood_data: perturbed data, 2d array. first element is\\n                               assumed to be the original data point.\\n            neighborhood_labels: corresponding perturbed labels. should have as\\n                                 many columns as the number of possible labels.\\n            distances: distances to original data point.\\n            label: label for which we want an explanation\\n            num_features: maximum number of features in explanation\\n            feature_selection: how to select num_features. options are:\\n                'forward_selection': iteratively add features to the model.\\n                    This is costly when num_features is high\\n                'highest_weights': selects the features that have the highest\\n                    product of absolute weight * original data point when\\n                    learning with all the features\\n                'lasso_path': chooses features based on the lasso\\n                    regularization path\\n                'none': uses all features, ignores num_features\\n                'auto': uses forward_selection if num_features <= 6, and\\n                    'highest_weights' otherwise.\\n            model_regressor: sklearn regressor to use in explanation.\\n                Defaults to Ridge regression if None. Must have\\n                model_regressor.coef_ and 'sample_weight' as a parameter\\n                to model_regressor.fit()\\n\\n        Returns:\\n            (intercept, exp, score, local_pred):\\n            intercept is a float.\\n            exp is a sorted list of tuples, where each tuple (x,y) corresponds\\n            to the feature id (x) and the local weight (y). The list is sorted\\n            by decreasing absolute value of y.\\n            score is the R^2 value of the returned explanation\\n            local_pred is the prediction of the explanation model on the original instance\\n        \"\n    weights = self.kernel_fn(distances)\n    labels_column = neighborhood_labels[:, label]\n    used_features = self.feature_selection(neighborhood_data, labels_column, weights, num_features, feature_selection)\n    if model_regressor is None:\n        model_regressor = Ridge(alpha=1, fit_intercept=True, random_state=self.random_state)\n    easy_model = model_regressor\n    easy_model.fit(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    prediction_score = easy_model.score(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n    if self.verbose:\n        print('Intercept', easy_model.intercept_)\n        print('Prediction_local', local_pred)\n        print('Right:', neighborhood_labels[0, label])\n    return (easy_model.intercept_, sorted(zip(used_features, easy_model.coef_), key=lambda x: np.abs(x[1]), reverse=True), prediction_score, local_pred)",
            "def explain_instance_with_data(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection='auto', model_regressor=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Takes perturbed data, labels and distances, returns explanation.\\n\\n        Args:\\n            neighborhood_data: perturbed data, 2d array. first element is\\n                               assumed to be the original data point.\\n            neighborhood_labels: corresponding perturbed labels. should have as\\n                                 many columns as the number of possible labels.\\n            distances: distances to original data point.\\n            label: label for which we want an explanation\\n            num_features: maximum number of features in explanation\\n            feature_selection: how to select num_features. options are:\\n                'forward_selection': iteratively add features to the model.\\n                    This is costly when num_features is high\\n                'highest_weights': selects the features that have the highest\\n                    product of absolute weight * original data point when\\n                    learning with all the features\\n                'lasso_path': chooses features based on the lasso\\n                    regularization path\\n                'none': uses all features, ignores num_features\\n                'auto': uses forward_selection if num_features <= 6, and\\n                    'highest_weights' otherwise.\\n            model_regressor: sklearn regressor to use in explanation.\\n                Defaults to Ridge regression if None. Must have\\n                model_regressor.coef_ and 'sample_weight' as a parameter\\n                to model_regressor.fit()\\n\\n        Returns:\\n            (intercept, exp, score, local_pred):\\n            intercept is a float.\\n            exp is a sorted list of tuples, where each tuple (x,y) corresponds\\n            to the feature id (x) and the local weight (y). The list is sorted\\n            by decreasing absolute value of y.\\n            score is the R^2 value of the returned explanation\\n            local_pred is the prediction of the explanation model on the original instance\\n        \"\n    weights = self.kernel_fn(distances)\n    labels_column = neighborhood_labels[:, label]\n    used_features = self.feature_selection(neighborhood_data, labels_column, weights, num_features, feature_selection)\n    if model_regressor is None:\n        model_regressor = Ridge(alpha=1, fit_intercept=True, random_state=self.random_state)\n    easy_model = model_regressor\n    easy_model.fit(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    prediction_score = easy_model.score(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n    if self.verbose:\n        print('Intercept', easy_model.intercept_)\n        print('Prediction_local', local_pred)\n        print('Right:', neighborhood_labels[0, label])\n    return (easy_model.intercept_, sorted(zip(used_features, easy_model.coef_), key=lambda x: np.abs(x[1]), reverse=True), prediction_score, local_pred)",
            "def explain_instance_with_data(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection='auto', model_regressor=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Takes perturbed data, labels and distances, returns explanation.\\n\\n        Args:\\n            neighborhood_data: perturbed data, 2d array. first element is\\n                               assumed to be the original data point.\\n            neighborhood_labels: corresponding perturbed labels. should have as\\n                                 many columns as the number of possible labels.\\n            distances: distances to original data point.\\n            label: label for which we want an explanation\\n            num_features: maximum number of features in explanation\\n            feature_selection: how to select num_features. options are:\\n                'forward_selection': iteratively add features to the model.\\n                    This is costly when num_features is high\\n                'highest_weights': selects the features that have the highest\\n                    product of absolute weight * original data point when\\n                    learning with all the features\\n                'lasso_path': chooses features based on the lasso\\n                    regularization path\\n                'none': uses all features, ignores num_features\\n                'auto': uses forward_selection if num_features <= 6, and\\n                    'highest_weights' otherwise.\\n            model_regressor: sklearn regressor to use in explanation.\\n                Defaults to Ridge regression if None. Must have\\n                model_regressor.coef_ and 'sample_weight' as a parameter\\n                to model_regressor.fit()\\n\\n        Returns:\\n            (intercept, exp, score, local_pred):\\n            intercept is a float.\\n            exp is a sorted list of tuples, where each tuple (x,y) corresponds\\n            to the feature id (x) and the local weight (y). The list is sorted\\n            by decreasing absolute value of y.\\n            score is the R^2 value of the returned explanation\\n            local_pred is the prediction of the explanation model on the original instance\\n        \"\n    weights = self.kernel_fn(distances)\n    labels_column = neighborhood_labels[:, label]\n    used_features = self.feature_selection(neighborhood_data, labels_column, weights, num_features, feature_selection)\n    if model_regressor is None:\n        model_regressor = Ridge(alpha=1, fit_intercept=True, random_state=self.random_state)\n    easy_model = model_regressor\n    easy_model.fit(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    prediction_score = easy_model.score(neighborhood_data[:, used_features], labels_column, sample_weight=weights)\n    local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n    if self.verbose:\n        print('Intercept', easy_model.intercept_)\n        print('Prediction_local', local_pred)\n        print('Right:', neighborhood_labels[0, label])\n    return (easy_model.intercept_, sorted(zip(used_features, easy_model.coef_), key=lambda x: np.abs(x[1]), reverse=True), prediction_score, local_pred)"
        ]
    }
]