[
    {
        "func_name": "convert_xmod_checkpoint_to_pytorch",
        "original": "def convert_xmod_checkpoint_to_pytorch(xmod_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    data_dir = Path('data_bin')\n    xmod = FairseqXmodModel.from_pretrained(model_name_or_path=str(Path(xmod_checkpoint_path).parent), checkpoint_file=Path(xmod_checkpoint_path).name, _name='xmod_base', arch='xmod_base', task='multilingual_masked_lm', data_name_or_path=str(data_dir), bpe='sentencepiece', sentencepiece_model=str(Path(xmod_checkpoint_path).parent / 'sentencepiece.bpe.model'), src_dict=str(data_dir / 'dict.txt'))\n    xmod.eval()\n    print(xmod)\n    xmod_sent_encoder = xmod.model.encoder.sentence_encoder\n    config = XmodConfig(vocab_size=xmod_sent_encoder.embed_tokens.num_embeddings, hidden_size=xmod.cfg.model.encoder_embed_dim, num_hidden_layers=xmod.cfg.model.encoder_layers, num_attention_heads=xmod.cfg.model.encoder_attention_heads, intermediate_size=xmod.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05, pre_norm=xmod.cfg.model.encoder_normalize_before, adapter_reduction_factor=getattr(xmod.cfg.model, 'bottleneck', 2), adapter_layer_norm=xmod.cfg.model.adapter_layer_norm, adapter_reuse_layer_norm=xmod.cfg.model.adapter_reuse_layer_norm, ln_before_adapter=xmod.cfg.model.ln_before_adapter, languages=xmod.cfg.model.languages)\n    if classification_head:\n        config.num_labels = xmod.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our X-MOD config:', config)\n    model = XmodForSequenceClassification(config) if classification_head else XmodForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = xmod_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = xmod_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.embeddings.LayerNorm.weight = xmod_sent_encoder.layernorm_embedding.weight\n    model.roberta.embeddings.LayerNorm.bias = xmod_sent_encoder.layernorm_embedding.bias\n    for i in range(config.num_hidden_layers):\n        layer = model.roberta.encoder.layer[i]\n        xmod_layer = xmod_sent_encoder.layers[i]\n        self_attn = layer.attention.self\n        if not xmod_layer.self_attn.k_proj.weight.data.shape == xmod_layer.self_attn.q_proj.weight.data.shape == xmod_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size)):\n            raise AssertionError('Dimensions of self-attention weights do not match.')\n        self_attn.query.weight.data = xmod_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = xmod_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = xmod_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = xmod_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = xmod_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = xmod_layer.self_attn.v_proj.bias\n        self_output = layer.attention.output\n        if self_output.dense.weight.shape != xmod_layer.self_attn.out_proj.weight.shape:\n            raise AssertionError('Dimensions of self-attention output weights do not match.')\n        self_output.dense.weight = xmod_layer.self_attn.out_proj.weight\n        self_output.dense.bias = xmod_layer.self_attn.out_proj.bias\n        self_output.LayerNorm.weight = xmod_layer.self_attn_layer_norm.weight\n        self_output.LayerNorm.bias = xmod_layer.self_attn_layer_norm.bias\n        intermediate = layer.intermediate\n        if intermediate.dense.weight.shape != xmod_layer.fc1.weight.shape:\n            raise AssertionError('Dimensions of intermediate weights do not match.')\n        intermediate.dense.weight = xmod_layer.fc1.weight\n        intermediate.dense.bias = xmod_layer.fc1.bias\n        bert_output = layer.output\n        if bert_output.dense.weight.shape != xmod_layer.fc2.weight.shape:\n            raise AssertionError('Dimensions of feed-forward weights do not match.')\n        bert_output.dense.weight = xmod_layer.fc2.weight\n        bert_output.dense.bias = xmod_layer.fc2.bias\n        bert_output.LayerNorm.weight = xmod_layer.final_layer_norm.weight\n        bert_output.LayerNorm.bias = xmod_layer.final_layer_norm.bias\n        if bert_output.adapter_layer_norm is not None:\n            bert_output.adapter_layer_norm.weight = xmod_layer.adapter_layer_norm.weight\n            bert_output.adapter_layer_norm.bias = xmod_layer.adapter_layer_norm.bias\n        if sorted(bert_output.adapter_modules.keys()) != sorted(xmod_layer.adapter_modules.keys()):\n            raise AssertionError('Lists of language adapters do not match.')\n        for (lang_code, adapter) in xmod_layer.adapter_modules.items():\n            to_adapter = bert_output.adapter_modules[lang_code]\n            from_adapter = xmod_layer.adapter_modules[lang_code]\n            to_adapter.dense1.weight = from_adapter.fc1.weight\n            to_adapter.dense1.bias = from_adapter.fc1.bias\n            to_adapter.dense2.weight = from_adapter.fc2.weight\n            to_adapter.dense2.bias = from_adapter.fc2.bias\n    if xmod_sent_encoder.layer_norm is not None:\n        model.roberta.encoder.LayerNorm.weight = xmod_sent_encoder.layer_norm.weight\n        model.roberta.encoder.LayerNorm.bias = xmod_sent_encoder.layer_norm.bias\n    if classification_head:\n        model.classifier.dense.weight = xmod.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = xmod.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = xmod.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = xmod.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = xmod.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = xmod.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = xmod.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = xmod.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = xmod.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = xmod.model.encoder.lm_head.bias\n    input_ids = xmod.encode(SAMPLE_TEXT).unsqueeze(0)\n    model.roberta.set_default_language(SAMPLE_LANGUAGE)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = xmod.model.classification_heads['mnli'](xmod.extract_features(input_ids))\n    else:\n        their_output = xmod.model(input_ids, lang_id=[SAMPLE_LANGUAGE])[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
        "mutated": [
            "def convert_xmod_checkpoint_to_pytorch(xmod_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    if False:\n        i = 10\n    data_dir = Path('data_bin')\n    xmod = FairseqXmodModel.from_pretrained(model_name_or_path=str(Path(xmod_checkpoint_path).parent), checkpoint_file=Path(xmod_checkpoint_path).name, _name='xmod_base', arch='xmod_base', task='multilingual_masked_lm', data_name_or_path=str(data_dir), bpe='sentencepiece', sentencepiece_model=str(Path(xmod_checkpoint_path).parent / 'sentencepiece.bpe.model'), src_dict=str(data_dir / 'dict.txt'))\n    xmod.eval()\n    print(xmod)\n    xmod_sent_encoder = xmod.model.encoder.sentence_encoder\n    config = XmodConfig(vocab_size=xmod_sent_encoder.embed_tokens.num_embeddings, hidden_size=xmod.cfg.model.encoder_embed_dim, num_hidden_layers=xmod.cfg.model.encoder_layers, num_attention_heads=xmod.cfg.model.encoder_attention_heads, intermediate_size=xmod.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05, pre_norm=xmod.cfg.model.encoder_normalize_before, adapter_reduction_factor=getattr(xmod.cfg.model, 'bottleneck', 2), adapter_layer_norm=xmod.cfg.model.adapter_layer_norm, adapter_reuse_layer_norm=xmod.cfg.model.adapter_reuse_layer_norm, ln_before_adapter=xmod.cfg.model.ln_before_adapter, languages=xmod.cfg.model.languages)\n    if classification_head:\n        config.num_labels = xmod.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our X-MOD config:', config)\n    model = XmodForSequenceClassification(config) if classification_head else XmodForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = xmod_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = xmod_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.embeddings.LayerNorm.weight = xmod_sent_encoder.layernorm_embedding.weight\n    model.roberta.embeddings.LayerNorm.bias = xmod_sent_encoder.layernorm_embedding.bias\n    for i in range(config.num_hidden_layers):\n        layer = model.roberta.encoder.layer[i]\n        xmod_layer = xmod_sent_encoder.layers[i]\n        self_attn = layer.attention.self\n        if not xmod_layer.self_attn.k_proj.weight.data.shape == xmod_layer.self_attn.q_proj.weight.data.shape == xmod_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size)):\n            raise AssertionError('Dimensions of self-attention weights do not match.')\n        self_attn.query.weight.data = xmod_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = xmod_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = xmod_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = xmod_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = xmod_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = xmod_layer.self_attn.v_proj.bias\n        self_output = layer.attention.output\n        if self_output.dense.weight.shape != xmod_layer.self_attn.out_proj.weight.shape:\n            raise AssertionError('Dimensions of self-attention output weights do not match.')\n        self_output.dense.weight = xmod_layer.self_attn.out_proj.weight\n        self_output.dense.bias = xmod_layer.self_attn.out_proj.bias\n        self_output.LayerNorm.weight = xmod_layer.self_attn_layer_norm.weight\n        self_output.LayerNorm.bias = xmod_layer.self_attn_layer_norm.bias\n        intermediate = layer.intermediate\n        if intermediate.dense.weight.shape != xmod_layer.fc1.weight.shape:\n            raise AssertionError('Dimensions of intermediate weights do not match.')\n        intermediate.dense.weight = xmod_layer.fc1.weight\n        intermediate.dense.bias = xmod_layer.fc1.bias\n        bert_output = layer.output\n        if bert_output.dense.weight.shape != xmod_layer.fc2.weight.shape:\n            raise AssertionError('Dimensions of feed-forward weights do not match.')\n        bert_output.dense.weight = xmod_layer.fc2.weight\n        bert_output.dense.bias = xmod_layer.fc2.bias\n        bert_output.LayerNorm.weight = xmod_layer.final_layer_norm.weight\n        bert_output.LayerNorm.bias = xmod_layer.final_layer_norm.bias\n        if bert_output.adapter_layer_norm is not None:\n            bert_output.adapter_layer_norm.weight = xmod_layer.adapter_layer_norm.weight\n            bert_output.adapter_layer_norm.bias = xmod_layer.adapter_layer_norm.bias\n        if sorted(bert_output.adapter_modules.keys()) != sorted(xmod_layer.adapter_modules.keys()):\n            raise AssertionError('Lists of language adapters do not match.')\n        for (lang_code, adapter) in xmod_layer.adapter_modules.items():\n            to_adapter = bert_output.adapter_modules[lang_code]\n            from_adapter = xmod_layer.adapter_modules[lang_code]\n            to_adapter.dense1.weight = from_adapter.fc1.weight\n            to_adapter.dense1.bias = from_adapter.fc1.bias\n            to_adapter.dense2.weight = from_adapter.fc2.weight\n            to_adapter.dense2.bias = from_adapter.fc2.bias\n    if xmod_sent_encoder.layer_norm is not None:\n        model.roberta.encoder.LayerNorm.weight = xmod_sent_encoder.layer_norm.weight\n        model.roberta.encoder.LayerNorm.bias = xmod_sent_encoder.layer_norm.bias\n    if classification_head:\n        model.classifier.dense.weight = xmod.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = xmod.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = xmod.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = xmod.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = xmod.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = xmod.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = xmod.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = xmod.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = xmod.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = xmod.model.encoder.lm_head.bias\n    input_ids = xmod.encode(SAMPLE_TEXT).unsqueeze(0)\n    model.roberta.set_default_language(SAMPLE_LANGUAGE)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = xmod.model.classification_heads['mnli'](xmod.extract_features(input_ids))\n    else:\n        their_output = xmod.model(input_ids, lang_id=[SAMPLE_LANGUAGE])[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "def convert_xmod_checkpoint_to_pytorch(xmod_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    data_dir = Path('data_bin')\n    xmod = FairseqXmodModel.from_pretrained(model_name_or_path=str(Path(xmod_checkpoint_path).parent), checkpoint_file=Path(xmod_checkpoint_path).name, _name='xmod_base', arch='xmod_base', task='multilingual_masked_lm', data_name_or_path=str(data_dir), bpe='sentencepiece', sentencepiece_model=str(Path(xmod_checkpoint_path).parent / 'sentencepiece.bpe.model'), src_dict=str(data_dir / 'dict.txt'))\n    xmod.eval()\n    print(xmod)\n    xmod_sent_encoder = xmod.model.encoder.sentence_encoder\n    config = XmodConfig(vocab_size=xmod_sent_encoder.embed_tokens.num_embeddings, hidden_size=xmod.cfg.model.encoder_embed_dim, num_hidden_layers=xmod.cfg.model.encoder_layers, num_attention_heads=xmod.cfg.model.encoder_attention_heads, intermediate_size=xmod.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05, pre_norm=xmod.cfg.model.encoder_normalize_before, adapter_reduction_factor=getattr(xmod.cfg.model, 'bottleneck', 2), adapter_layer_norm=xmod.cfg.model.adapter_layer_norm, adapter_reuse_layer_norm=xmod.cfg.model.adapter_reuse_layer_norm, ln_before_adapter=xmod.cfg.model.ln_before_adapter, languages=xmod.cfg.model.languages)\n    if classification_head:\n        config.num_labels = xmod.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our X-MOD config:', config)\n    model = XmodForSequenceClassification(config) if classification_head else XmodForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = xmod_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = xmod_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.embeddings.LayerNorm.weight = xmod_sent_encoder.layernorm_embedding.weight\n    model.roberta.embeddings.LayerNorm.bias = xmod_sent_encoder.layernorm_embedding.bias\n    for i in range(config.num_hidden_layers):\n        layer = model.roberta.encoder.layer[i]\n        xmod_layer = xmod_sent_encoder.layers[i]\n        self_attn = layer.attention.self\n        if not xmod_layer.self_attn.k_proj.weight.data.shape == xmod_layer.self_attn.q_proj.weight.data.shape == xmod_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size)):\n            raise AssertionError('Dimensions of self-attention weights do not match.')\n        self_attn.query.weight.data = xmod_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = xmod_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = xmod_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = xmod_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = xmod_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = xmod_layer.self_attn.v_proj.bias\n        self_output = layer.attention.output\n        if self_output.dense.weight.shape != xmod_layer.self_attn.out_proj.weight.shape:\n            raise AssertionError('Dimensions of self-attention output weights do not match.')\n        self_output.dense.weight = xmod_layer.self_attn.out_proj.weight\n        self_output.dense.bias = xmod_layer.self_attn.out_proj.bias\n        self_output.LayerNorm.weight = xmod_layer.self_attn_layer_norm.weight\n        self_output.LayerNorm.bias = xmod_layer.self_attn_layer_norm.bias\n        intermediate = layer.intermediate\n        if intermediate.dense.weight.shape != xmod_layer.fc1.weight.shape:\n            raise AssertionError('Dimensions of intermediate weights do not match.')\n        intermediate.dense.weight = xmod_layer.fc1.weight\n        intermediate.dense.bias = xmod_layer.fc1.bias\n        bert_output = layer.output\n        if bert_output.dense.weight.shape != xmod_layer.fc2.weight.shape:\n            raise AssertionError('Dimensions of feed-forward weights do not match.')\n        bert_output.dense.weight = xmod_layer.fc2.weight\n        bert_output.dense.bias = xmod_layer.fc2.bias\n        bert_output.LayerNorm.weight = xmod_layer.final_layer_norm.weight\n        bert_output.LayerNorm.bias = xmod_layer.final_layer_norm.bias\n        if bert_output.adapter_layer_norm is not None:\n            bert_output.adapter_layer_norm.weight = xmod_layer.adapter_layer_norm.weight\n            bert_output.adapter_layer_norm.bias = xmod_layer.adapter_layer_norm.bias\n        if sorted(bert_output.adapter_modules.keys()) != sorted(xmod_layer.adapter_modules.keys()):\n            raise AssertionError('Lists of language adapters do not match.')\n        for (lang_code, adapter) in xmod_layer.adapter_modules.items():\n            to_adapter = bert_output.adapter_modules[lang_code]\n            from_adapter = xmod_layer.adapter_modules[lang_code]\n            to_adapter.dense1.weight = from_adapter.fc1.weight\n            to_adapter.dense1.bias = from_adapter.fc1.bias\n            to_adapter.dense2.weight = from_adapter.fc2.weight\n            to_adapter.dense2.bias = from_adapter.fc2.bias\n    if xmod_sent_encoder.layer_norm is not None:\n        model.roberta.encoder.LayerNorm.weight = xmod_sent_encoder.layer_norm.weight\n        model.roberta.encoder.LayerNorm.bias = xmod_sent_encoder.layer_norm.bias\n    if classification_head:\n        model.classifier.dense.weight = xmod.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = xmod.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = xmod.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = xmod.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = xmod.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = xmod.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = xmod.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = xmod.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = xmod.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = xmod.model.encoder.lm_head.bias\n    input_ids = xmod.encode(SAMPLE_TEXT).unsqueeze(0)\n    model.roberta.set_default_language(SAMPLE_LANGUAGE)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = xmod.model.classification_heads['mnli'](xmod.extract_features(input_ids))\n    else:\n        their_output = xmod.model(input_ids, lang_id=[SAMPLE_LANGUAGE])[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "def convert_xmod_checkpoint_to_pytorch(xmod_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    data_dir = Path('data_bin')\n    xmod = FairseqXmodModel.from_pretrained(model_name_or_path=str(Path(xmod_checkpoint_path).parent), checkpoint_file=Path(xmod_checkpoint_path).name, _name='xmod_base', arch='xmod_base', task='multilingual_masked_lm', data_name_or_path=str(data_dir), bpe='sentencepiece', sentencepiece_model=str(Path(xmod_checkpoint_path).parent / 'sentencepiece.bpe.model'), src_dict=str(data_dir / 'dict.txt'))\n    xmod.eval()\n    print(xmod)\n    xmod_sent_encoder = xmod.model.encoder.sentence_encoder\n    config = XmodConfig(vocab_size=xmod_sent_encoder.embed_tokens.num_embeddings, hidden_size=xmod.cfg.model.encoder_embed_dim, num_hidden_layers=xmod.cfg.model.encoder_layers, num_attention_heads=xmod.cfg.model.encoder_attention_heads, intermediate_size=xmod.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05, pre_norm=xmod.cfg.model.encoder_normalize_before, adapter_reduction_factor=getattr(xmod.cfg.model, 'bottleneck', 2), adapter_layer_norm=xmod.cfg.model.adapter_layer_norm, adapter_reuse_layer_norm=xmod.cfg.model.adapter_reuse_layer_norm, ln_before_adapter=xmod.cfg.model.ln_before_adapter, languages=xmod.cfg.model.languages)\n    if classification_head:\n        config.num_labels = xmod.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our X-MOD config:', config)\n    model = XmodForSequenceClassification(config) if classification_head else XmodForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = xmod_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = xmod_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.embeddings.LayerNorm.weight = xmod_sent_encoder.layernorm_embedding.weight\n    model.roberta.embeddings.LayerNorm.bias = xmod_sent_encoder.layernorm_embedding.bias\n    for i in range(config.num_hidden_layers):\n        layer = model.roberta.encoder.layer[i]\n        xmod_layer = xmod_sent_encoder.layers[i]\n        self_attn = layer.attention.self\n        if not xmod_layer.self_attn.k_proj.weight.data.shape == xmod_layer.self_attn.q_proj.weight.data.shape == xmod_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size)):\n            raise AssertionError('Dimensions of self-attention weights do not match.')\n        self_attn.query.weight.data = xmod_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = xmod_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = xmod_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = xmod_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = xmod_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = xmod_layer.self_attn.v_proj.bias\n        self_output = layer.attention.output\n        if self_output.dense.weight.shape != xmod_layer.self_attn.out_proj.weight.shape:\n            raise AssertionError('Dimensions of self-attention output weights do not match.')\n        self_output.dense.weight = xmod_layer.self_attn.out_proj.weight\n        self_output.dense.bias = xmod_layer.self_attn.out_proj.bias\n        self_output.LayerNorm.weight = xmod_layer.self_attn_layer_norm.weight\n        self_output.LayerNorm.bias = xmod_layer.self_attn_layer_norm.bias\n        intermediate = layer.intermediate\n        if intermediate.dense.weight.shape != xmod_layer.fc1.weight.shape:\n            raise AssertionError('Dimensions of intermediate weights do not match.')\n        intermediate.dense.weight = xmod_layer.fc1.weight\n        intermediate.dense.bias = xmod_layer.fc1.bias\n        bert_output = layer.output\n        if bert_output.dense.weight.shape != xmod_layer.fc2.weight.shape:\n            raise AssertionError('Dimensions of feed-forward weights do not match.')\n        bert_output.dense.weight = xmod_layer.fc2.weight\n        bert_output.dense.bias = xmod_layer.fc2.bias\n        bert_output.LayerNorm.weight = xmod_layer.final_layer_norm.weight\n        bert_output.LayerNorm.bias = xmod_layer.final_layer_norm.bias\n        if bert_output.adapter_layer_norm is not None:\n            bert_output.adapter_layer_norm.weight = xmod_layer.adapter_layer_norm.weight\n            bert_output.adapter_layer_norm.bias = xmod_layer.adapter_layer_norm.bias\n        if sorted(bert_output.adapter_modules.keys()) != sorted(xmod_layer.adapter_modules.keys()):\n            raise AssertionError('Lists of language adapters do not match.')\n        for (lang_code, adapter) in xmod_layer.adapter_modules.items():\n            to_adapter = bert_output.adapter_modules[lang_code]\n            from_adapter = xmod_layer.adapter_modules[lang_code]\n            to_adapter.dense1.weight = from_adapter.fc1.weight\n            to_adapter.dense1.bias = from_adapter.fc1.bias\n            to_adapter.dense2.weight = from_adapter.fc2.weight\n            to_adapter.dense2.bias = from_adapter.fc2.bias\n    if xmod_sent_encoder.layer_norm is not None:\n        model.roberta.encoder.LayerNorm.weight = xmod_sent_encoder.layer_norm.weight\n        model.roberta.encoder.LayerNorm.bias = xmod_sent_encoder.layer_norm.bias\n    if classification_head:\n        model.classifier.dense.weight = xmod.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = xmod.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = xmod.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = xmod.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = xmod.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = xmod.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = xmod.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = xmod.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = xmod.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = xmod.model.encoder.lm_head.bias\n    input_ids = xmod.encode(SAMPLE_TEXT).unsqueeze(0)\n    model.roberta.set_default_language(SAMPLE_LANGUAGE)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = xmod.model.classification_heads['mnli'](xmod.extract_features(input_ids))\n    else:\n        their_output = xmod.model(input_ids, lang_id=[SAMPLE_LANGUAGE])[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "def convert_xmod_checkpoint_to_pytorch(xmod_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    data_dir = Path('data_bin')\n    xmod = FairseqXmodModel.from_pretrained(model_name_or_path=str(Path(xmod_checkpoint_path).parent), checkpoint_file=Path(xmod_checkpoint_path).name, _name='xmod_base', arch='xmod_base', task='multilingual_masked_lm', data_name_or_path=str(data_dir), bpe='sentencepiece', sentencepiece_model=str(Path(xmod_checkpoint_path).parent / 'sentencepiece.bpe.model'), src_dict=str(data_dir / 'dict.txt'))\n    xmod.eval()\n    print(xmod)\n    xmod_sent_encoder = xmod.model.encoder.sentence_encoder\n    config = XmodConfig(vocab_size=xmod_sent_encoder.embed_tokens.num_embeddings, hidden_size=xmod.cfg.model.encoder_embed_dim, num_hidden_layers=xmod.cfg.model.encoder_layers, num_attention_heads=xmod.cfg.model.encoder_attention_heads, intermediate_size=xmod.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05, pre_norm=xmod.cfg.model.encoder_normalize_before, adapter_reduction_factor=getattr(xmod.cfg.model, 'bottleneck', 2), adapter_layer_norm=xmod.cfg.model.adapter_layer_norm, adapter_reuse_layer_norm=xmod.cfg.model.adapter_reuse_layer_norm, ln_before_adapter=xmod.cfg.model.ln_before_adapter, languages=xmod.cfg.model.languages)\n    if classification_head:\n        config.num_labels = xmod.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our X-MOD config:', config)\n    model = XmodForSequenceClassification(config) if classification_head else XmodForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = xmod_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = xmod_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.embeddings.LayerNorm.weight = xmod_sent_encoder.layernorm_embedding.weight\n    model.roberta.embeddings.LayerNorm.bias = xmod_sent_encoder.layernorm_embedding.bias\n    for i in range(config.num_hidden_layers):\n        layer = model.roberta.encoder.layer[i]\n        xmod_layer = xmod_sent_encoder.layers[i]\n        self_attn = layer.attention.self\n        if not xmod_layer.self_attn.k_proj.weight.data.shape == xmod_layer.self_attn.q_proj.weight.data.shape == xmod_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size)):\n            raise AssertionError('Dimensions of self-attention weights do not match.')\n        self_attn.query.weight.data = xmod_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = xmod_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = xmod_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = xmod_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = xmod_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = xmod_layer.self_attn.v_proj.bias\n        self_output = layer.attention.output\n        if self_output.dense.weight.shape != xmod_layer.self_attn.out_proj.weight.shape:\n            raise AssertionError('Dimensions of self-attention output weights do not match.')\n        self_output.dense.weight = xmod_layer.self_attn.out_proj.weight\n        self_output.dense.bias = xmod_layer.self_attn.out_proj.bias\n        self_output.LayerNorm.weight = xmod_layer.self_attn_layer_norm.weight\n        self_output.LayerNorm.bias = xmod_layer.self_attn_layer_norm.bias\n        intermediate = layer.intermediate\n        if intermediate.dense.weight.shape != xmod_layer.fc1.weight.shape:\n            raise AssertionError('Dimensions of intermediate weights do not match.')\n        intermediate.dense.weight = xmod_layer.fc1.weight\n        intermediate.dense.bias = xmod_layer.fc1.bias\n        bert_output = layer.output\n        if bert_output.dense.weight.shape != xmod_layer.fc2.weight.shape:\n            raise AssertionError('Dimensions of feed-forward weights do not match.')\n        bert_output.dense.weight = xmod_layer.fc2.weight\n        bert_output.dense.bias = xmod_layer.fc2.bias\n        bert_output.LayerNorm.weight = xmod_layer.final_layer_norm.weight\n        bert_output.LayerNorm.bias = xmod_layer.final_layer_norm.bias\n        if bert_output.adapter_layer_norm is not None:\n            bert_output.adapter_layer_norm.weight = xmod_layer.adapter_layer_norm.weight\n            bert_output.adapter_layer_norm.bias = xmod_layer.adapter_layer_norm.bias\n        if sorted(bert_output.adapter_modules.keys()) != sorted(xmod_layer.adapter_modules.keys()):\n            raise AssertionError('Lists of language adapters do not match.')\n        for (lang_code, adapter) in xmod_layer.adapter_modules.items():\n            to_adapter = bert_output.adapter_modules[lang_code]\n            from_adapter = xmod_layer.adapter_modules[lang_code]\n            to_adapter.dense1.weight = from_adapter.fc1.weight\n            to_adapter.dense1.bias = from_adapter.fc1.bias\n            to_adapter.dense2.weight = from_adapter.fc2.weight\n            to_adapter.dense2.bias = from_adapter.fc2.bias\n    if xmod_sent_encoder.layer_norm is not None:\n        model.roberta.encoder.LayerNorm.weight = xmod_sent_encoder.layer_norm.weight\n        model.roberta.encoder.LayerNorm.bias = xmod_sent_encoder.layer_norm.bias\n    if classification_head:\n        model.classifier.dense.weight = xmod.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = xmod.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = xmod.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = xmod.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = xmod.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = xmod.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = xmod.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = xmod.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = xmod.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = xmod.model.encoder.lm_head.bias\n    input_ids = xmod.encode(SAMPLE_TEXT).unsqueeze(0)\n    model.roberta.set_default_language(SAMPLE_LANGUAGE)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = xmod.model.classification_heads['mnli'](xmod.extract_features(input_ids))\n    else:\n        their_output = xmod.model(input_ids, lang_id=[SAMPLE_LANGUAGE])[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "def convert_xmod_checkpoint_to_pytorch(xmod_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    data_dir = Path('data_bin')\n    xmod = FairseqXmodModel.from_pretrained(model_name_or_path=str(Path(xmod_checkpoint_path).parent), checkpoint_file=Path(xmod_checkpoint_path).name, _name='xmod_base', arch='xmod_base', task='multilingual_masked_lm', data_name_or_path=str(data_dir), bpe='sentencepiece', sentencepiece_model=str(Path(xmod_checkpoint_path).parent / 'sentencepiece.bpe.model'), src_dict=str(data_dir / 'dict.txt'))\n    xmod.eval()\n    print(xmod)\n    xmod_sent_encoder = xmod.model.encoder.sentence_encoder\n    config = XmodConfig(vocab_size=xmod_sent_encoder.embed_tokens.num_embeddings, hidden_size=xmod.cfg.model.encoder_embed_dim, num_hidden_layers=xmod.cfg.model.encoder_layers, num_attention_heads=xmod.cfg.model.encoder_attention_heads, intermediate_size=xmod.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05, pre_norm=xmod.cfg.model.encoder_normalize_before, adapter_reduction_factor=getattr(xmod.cfg.model, 'bottleneck', 2), adapter_layer_norm=xmod.cfg.model.adapter_layer_norm, adapter_reuse_layer_norm=xmod.cfg.model.adapter_reuse_layer_norm, ln_before_adapter=xmod.cfg.model.ln_before_adapter, languages=xmod.cfg.model.languages)\n    if classification_head:\n        config.num_labels = xmod.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our X-MOD config:', config)\n    model = XmodForSequenceClassification(config) if classification_head else XmodForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = xmod_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = xmod_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.embeddings.LayerNorm.weight = xmod_sent_encoder.layernorm_embedding.weight\n    model.roberta.embeddings.LayerNorm.bias = xmod_sent_encoder.layernorm_embedding.bias\n    for i in range(config.num_hidden_layers):\n        layer = model.roberta.encoder.layer[i]\n        xmod_layer = xmod_sent_encoder.layers[i]\n        self_attn = layer.attention.self\n        if not xmod_layer.self_attn.k_proj.weight.data.shape == xmod_layer.self_attn.q_proj.weight.data.shape == xmod_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size)):\n            raise AssertionError('Dimensions of self-attention weights do not match.')\n        self_attn.query.weight.data = xmod_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = xmod_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = xmod_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = xmod_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = xmod_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = xmod_layer.self_attn.v_proj.bias\n        self_output = layer.attention.output\n        if self_output.dense.weight.shape != xmod_layer.self_attn.out_proj.weight.shape:\n            raise AssertionError('Dimensions of self-attention output weights do not match.')\n        self_output.dense.weight = xmod_layer.self_attn.out_proj.weight\n        self_output.dense.bias = xmod_layer.self_attn.out_proj.bias\n        self_output.LayerNorm.weight = xmod_layer.self_attn_layer_norm.weight\n        self_output.LayerNorm.bias = xmod_layer.self_attn_layer_norm.bias\n        intermediate = layer.intermediate\n        if intermediate.dense.weight.shape != xmod_layer.fc1.weight.shape:\n            raise AssertionError('Dimensions of intermediate weights do not match.')\n        intermediate.dense.weight = xmod_layer.fc1.weight\n        intermediate.dense.bias = xmod_layer.fc1.bias\n        bert_output = layer.output\n        if bert_output.dense.weight.shape != xmod_layer.fc2.weight.shape:\n            raise AssertionError('Dimensions of feed-forward weights do not match.')\n        bert_output.dense.weight = xmod_layer.fc2.weight\n        bert_output.dense.bias = xmod_layer.fc2.bias\n        bert_output.LayerNorm.weight = xmod_layer.final_layer_norm.weight\n        bert_output.LayerNorm.bias = xmod_layer.final_layer_norm.bias\n        if bert_output.adapter_layer_norm is not None:\n            bert_output.adapter_layer_norm.weight = xmod_layer.adapter_layer_norm.weight\n            bert_output.adapter_layer_norm.bias = xmod_layer.adapter_layer_norm.bias\n        if sorted(bert_output.adapter_modules.keys()) != sorted(xmod_layer.adapter_modules.keys()):\n            raise AssertionError('Lists of language adapters do not match.')\n        for (lang_code, adapter) in xmod_layer.adapter_modules.items():\n            to_adapter = bert_output.adapter_modules[lang_code]\n            from_adapter = xmod_layer.adapter_modules[lang_code]\n            to_adapter.dense1.weight = from_adapter.fc1.weight\n            to_adapter.dense1.bias = from_adapter.fc1.bias\n            to_adapter.dense2.weight = from_adapter.fc2.weight\n            to_adapter.dense2.bias = from_adapter.fc2.bias\n    if xmod_sent_encoder.layer_norm is not None:\n        model.roberta.encoder.LayerNorm.weight = xmod_sent_encoder.layer_norm.weight\n        model.roberta.encoder.LayerNorm.bias = xmod_sent_encoder.layer_norm.bias\n    if classification_head:\n        model.classifier.dense.weight = xmod.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = xmod.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = xmod.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = xmod.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = xmod.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = xmod.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = xmod.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = xmod.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = xmod.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = xmod.model.encoder.lm_head.bias\n    input_ids = xmod.encode(SAMPLE_TEXT).unsqueeze(0)\n    model.roberta.set_default_language(SAMPLE_LANGUAGE)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = xmod.model.classification_heads['mnli'](xmod.extract_features(input_ids))\n    else:\n        their_output = xmod.model(input_ids, lang_id=[SAMPLE_LANGUAGE])[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)"
        ]
    }
]