[
    {
        "func_name": "inference_and_evaluation",
        "original": "def inference_and_evaluation(dlrm, test_dataloader, device):\n    \"\"\"Perform inference and evaluation on the test dataset.\n    The function returns the dictionary that contains evaluation metrics such as accuracy, f1, auc,\n    precision, recall.\n    Note: This function is a rewritten version of ```inference()``` present in dlrm_s_pytorch.py\n\n    Args:\n        dlrm (nn.Module)\n            dlrm model object\n        test_data_loader (torch dataloader):\n            dataloader for the test dataset\n        device (torch.device)\n            device on which the inference happens\n    \"\"\"\n    nbatches = len(test_dataloader)\n    scores = []\n    targets = []\n    for (i, testBatch) in enumerate(test_dataloader):\n        if nbatches > 0 and i >= nbatches:\n            break\n        (X_test, lS_o_test, lS_i_test, T_test, _, _) = unpack_batch(testBatch)\n        (X_test, lS_o_test, lS_i_test) = dlrm_wrap(X_test, lS_o_test, lS_i_test, device, ndevices=1)\n        Z_test = dlrm(X_test, lS_o_test, lS_i_test)\n        S_test = Z_test.detach().cpu().numpy()\n        T_test = T_test.detach().cpu().numpy()\n        scores.append(S_test)\n        targets.append(T_test)\n    scores = np.concatenate(scores, axis=0)\n    targets = np.concatenate(targets, axis=0)\n    metrics = {'recall': lambda y_true, y_score: sklearn.metrics.recall_score(y_true=y_true, y_pred=np.round(y_score)), 'precision': lambda y_true, y_score: sklearn.metrics.precision_score(y_true=y_true, y_pred=np.round(y_score)), 'f1': lambda y_true, y_score: sklearn.metrics.f1_score(y_true=y_true, y_pred=np.round(y_score)), 'ap': sklearn.metrics.average_precision_score, 'roc_auc': sklearn.metrics.roc_auc_score, 'accuracy': lambda y_true, y_score: sklearn.metrics.accuracy_score(y_true=y_true, y_pred=np.round(y_score)), 'log_loss': lambda y_true, y_score: sklearn.metrics.log_loss(y_true=y_true, y_pred=y_score)}\n    all_metrics = {}\n    for (metric_name, metric_function) in metrics.items():\n        all_metrics[metric_name] = round(metric_function(targets, scores), 3)\n    return all_metrics",
        "mutated": [
            "def inference_and_evaluation(dlrm, test_dataloader, device):\n    if False:\n        i = 10\n    'Perform inference and evaluation on the test dataset.\\n    The function returns the dictionary that contains evaluation metrics such as accuracy, f1, auc,\\n    precision, recall.\\n    Note: This function is a rewritten version of ```inference()``` present in dlrm_s_pytorch.py\\n\\n    Args:\\n        dlrm (nn.Module)\\n            dlrm model object\\n        test_data_loader (torch dataloader):\\n            dataloader for the test dataset\\n        device (torch.device)\\n            device on which the inference happens\\n    '\n    nbatches = len(test_dataloader)\n    scores = []\n    targets = []\n    for (i, testBatch) in enumerate(test_dataloader):\n        if nbatches > 0 and i >= nbatches:\n            break\n        (X_test, lS_o_test, lS_i_test, T_test, _, _) = unpack_batch(testBatch)\n        (X_test, lS_o_test, lS_i_test) = dlrm_wrap(X_test, lS_o_test, lS_i_test, device, ndevices=1)\n        Z_test = dlrm(X_test, lS_o_test, lS_i_test)\n        S_test = Z_test.detach().cpu().numpy()\n        T_test = T_test.detach().cpu().numpy()\n        scores.append(S_test)\n        targets.append(T_test)\n    scores = np.concatenate(scores, axis=0)\n    targets = np.concatenate(targets, axis=0)\n    metrics = {'recall': lambda y_true, y_score: sklearn.metrics.recall_score(y_true=y_true, y_pred=np.round(y_score)), 'precision': lambda y_true, y_score: sklearn.metrics.precision_score(y_true=y_true, y_pred=np.round(y_score)), 'f1': lambda y_true, y_score: sklearn.metrics.f1_score(y_true=y_true, y_pred=np.round(y_score)), 'ap': sklearn.metrics.average_precision_score, 'roc_auc': sklearn.metrics.roc_auc_score, 'accuracy': lambda y_true, y_score: sklearn.metrics.accuracy_score(y_true=y_true, y_pred=np.round(y_score)), 'log_loss': lambda y_true, y_score: sklearn.metrics.log_loss(y_true=y_true, y_pred=y_score)}\n    all_metrics = {}\n    for (metric_name, metric_function) in metrics.items():\n        all_metrics[metric_name] = round(metric_function(targets, scores), 3)\n    return all_metrics",
            "def inference_and_evaluation(dlrm, test_dataloader, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Perform inference and evaluation on the test dataset.\\n    The function returns the dictionary that contains evaluation metrics such as accuracy, f1, auc,\\n    precision, recall.\\n    Note: This function is a rewritten version of ```inference()``` present in dlrm_s_pytorch.py\\n\\n    Args:\\n        dlrm (nn.Module)\\n            dlrm model object\\n        test_data_loader (torch dataloader):\\n            dataloader for the test dataset\\n        device (torch.device)\\n            device on which the inference happens\\n    '\n    nbatches = len(test_dataloader)\n    scores = []\n    targets = []\n    for (i, testBatch) in enumerate(test_dataloader):\n        if nbatches > 0 and i >= nbatches:\n            break\n        (X_test, lS_o_test, lS_i_test, T_test, _, _) = unpack_batch(testBatch)\n        (X_test, lS_o_test, lS_i_test) = dlrm_wrap(X_test, lS_o_test, lS_i_test, device, ndevices=1)\n        Z_test = dlrm(X_test, lS_o_test, lS_i_test)\n        S_test = Z_test.detach().cpu().numpy()\n        T_test = T_test.detach().cpu().numpy()\n        scores.append(S_test)\n        targets.append(T_test)\n    scores = np.concatenate(scores, axis=0)\n    targets = np.concatenate(targets, axis=0)\n    metrics = {'recall': lambda y_true, y_score: sklearn.metrics.recall_score(y_true=y_true, y_pred=np.round(y_score)), 'precision': lambda y_true, y_score: sklearn.metrics.precision_score(y_true=y_true, y_pred=np.round(y_score)), 'f1': lambda y_true, y_score: sklearn.metrics.f1_score(y_true=y_true, y_pred=np.round(y_score)), 'ap': sklearn.metrics.average_precision_score, 'roc_auc': sklearn.metrics.roc_auc_score, 'accuracy': lambda y_true, y_score: sklearn.metrics.accuracy_score(y_true=y_true, y_pred=np.round(y_score)), 'log_loss': lambda y_true, y_score: sklearn.metrics.log_loss(y_true=y_true, y_pred=y_score)}\n    all_metrics = {}\n    for (metric_name, metric_function) in metrics.items():\n        all_metrics[metric_name] = round(metric_function(targets, scores), 3)\n    return all_metrics",
            "def inference_and_evaluation(dlrm, test_dataloader, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Perform inference and evaluation on the test dataset.\\n    The function returns the dictionary that contains evaluation metrics such as accuracy, f1, auc,\\n    precision, recall.\\n    Note: This function is a rewritten version of ```inference()``` present in dlrm_s_pytorch.py\\n\\n    Args:\\n        dlrm (nn.Module)\\n            dlrm model object\\n        test_data_loader (torch dataloader):\\n            dataloader for the test dataset\\n        device (torch.device)\\n            device on which the inference happens\\n    '\n    nbatches = len(test_dataloader)\n    scores = []\n    targets = []\n    for (i, testBatch) in enumerate(test_dataloader):\n        if nbatches > 0 and i >= nbatches:\n            break\n        (X_test, lS_o_test, lS_i_test, T_test, _, _) = unpack_batch(testBatch)\n        (X_test, lS_o_test, lS_i_test) = dlrm_wrap(X_test, lS_o_test, lS_i_test, device, ndevices=1)\n        Z_test = dlrm(X_test, lS_o_test, lS_i_test)\n        S_test = Z_test.detach().cpu().numpy()\n        T_test = T_test.detach().cpu().numpy()\n        scores.append(S_test)\n        targets.append(T_test)\n    scores = np.concatenate(scores, axis=0)\n    targets = np.concatenate(targets, axis=0)\n    metrics = {'recall': lambda y_true, y_score: sklearn.metrics.recall_score(y_true=y_true, y_pred=np.round(y_score)), 'precision': lambda y_true, y_score: sklearn.metrics.precision_score(y_true=y_true, y_pred=np.round(y_score)), 'f1': lambda y_true, y_score: sklearn.metrics.f1_score(y_true=y_true, y_pred=np.round(y_score)), 'ap': sklearn.metrics.average_precision_score, 'roc_auc': sklearn.metrics.roc_auc_score, 'accuracy': lambda y_true, y_score: sklearn.metrics.accuracy_score(y_true=y_true, y_pred=np.round(y_score)), 'log_loss': lambda y_true, y_score: sklearn.metrics.log_loss(y_true=y_true, y_pred=y_score)}\n    all_metrics = {}\n    for (metric_name, metric_function) in metrics.items():\n        all_metrics[metric_name] = round(metric_function(targets, scores), 3)\n    return all_metrics",
            "def inference_and_evaluation(dlrm, test_dataloader, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Perform inference and evaluation on the test dataset.\\n    The function returns the dictionary that contains evaluation metrics such as accuracy, f1, auc,\\n    precision, recall.\\n    Note: This function is a rewritten version of ```inference()``` present in dlrm_s_pytorch.py\\n\\n    Args:\\n        dlrm (nn.Module)\\n            dlrm model object\\n        test_data_loader (torch dataloader):\\n            dataloader for the test dataset\\n        device (torch.device)\\n            device on which the inference happens\\n    '\n    nbatches = len(test_dataloader)\n    scores = []\n    targets = []\n    for (i, testBatch) in enumerate(test_dataloader):\n        if nbatches > 0 and i >= nbatches:\n            break\n        (X_test, lS_o_test, lS_i_test, T_test, _, _) = unpack_batch(testBatch)\n        (X_test, lS_o_test, lS_i_test) = dlrm_wrap(X_test, lS_o_test, lS_i_test, device, ndevices=1)\n        Z_test = dlrm(X_test, lS_o_test, lS_i_test)\n        S_test = Z_test.detach().cpu().numpy()\n        T_test = T_test.detach().cpu().numpy()\n        scores.append(S_test)\n        targets.append(T_test)\n    scores = np.concatenate(scores, axis=0)\n    targets = np.concatenate(targets, axis=0)\n    metrics = {'recall': lambda y_true, y_score: sklearn.metrics.recall_score(y_true=y_true, y_pred=np.round(y_score)), 'precision': lambda y_true, y_score: sklearn.metrics.precision_score(y_true=y_true, y_pred=np.round(y_score)), 'f1': lambda y_true, y_score: sklearn.metrics.f1_score(y_true=y_true, y_pred=np.round(y_score)), 'ap': sklearn.metrics.average_precision_score, 'roc_auc': sklearn.metrics.roc_auc_score, 'accuracy': lambda y_true, y_score: sklearn.metrics.accuracy_score(y_true=y_true, y_pred=np.round(y_score)), 'log_loss': lambda y_true, y_score: sklearn.metrics.log_loss(y_true=y_true, y_pred=y_score)}\n    all_metrics = {}\n    for (metric_name, metric_function) in metrics.items():\n        all_metrics[metric_name] = round(metric_function(targets, scores), 3)\n    return all_metrics",
            "def inference_and_evaluation(dlrm, test_dataloader, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Perform inference and evaluation on the test dataset.\\n    The function returns the dictionary that contains evaluation metrics such as accuracy, f1, auc,\\n    precision, recall.\\n    Note: This function is a rewritten version of ```inference()``` present in dlrm_s_pytorch.py\\n\\n    Args:\\n        dlrm (nn.Module)\\n            dlrm model object\\n        test_data_loader (torch dataloader):\\n            dataloader for the test dataset\\n        device (torch.device)\\n            device on which the inference happens\\n    '\n    nbatches = len(test_dataloader)\n    scores = []\n    targets = []\n    for (i, testBatch) in enumerate(test_dataloader):\n        if nbatches > 0 and i >= nbatches:\n            break\n        (X_test, lS_o_test, lS_i_test, T_test, _, _) = unpack_batch(testBatch)\n        (X_test, lS_o_test, lS_i_test) = dlrm_wrap(X_test, lS_o_test, lS_i_test, device, ndevices=1)\n        Z_test = dlrm(X_test, lS_o_test, lS_i_test)\n        S_test = Z_test.detach().cpu().numpy()\n        T_test = T_test.detach().cpu().numpy()\n        scores.append(S_test)\n        targets.append(T_test)\n    scores = np.concatenate(scores, axis=0)\n    targets = np.concatenate(targets, axis=0)\n    metrics = {'recall': lambda y_true, y_score: sklearn.metrics.recall_score(y_true=y_true, y_pred=np.round(y_score)), 'precision': lambda y_true, y_score: sklearn.metrics.precision_score(y_true=y_true, y_pred=np.round(y_score)), 'f1': lambda y_true, y_score: sklearn.metrics.f1_score(y_true=y_true, y_pred=np.round(y_score)), 'ap': sklearn.metrics.average_precision_score, 'roc_auc': sklearn.metrics.roc_auc_score, 'accuracy': lambda y_true, y_score: sklearn.metrics.accuracy_score(y_true=y_true, y_pred=np.round(y_score)), 'log_loss': lambda y_true, y_score: sklearn.metrics.log_loss(y_true=y_true, y_pred=y_score)}\n    all_metrics = {}\n    for (metric_name, metric_function) in metrics.items():\n        all_metrics[metric_name] = round(metric_function(targets, scores), 3)\n    return all_metrics"
        ]
    },
    {
        "func_name": "evaluate_metrics",
        "original": "def evaluate_metrics(test_dataloader, sparse_model_metadata):\n    \"\"\"Evaluates the metrics the sparsified metrics for the dlrm model on various sparsity levels,\n    block shapes and norms. This function evaluates the model on the test dataset and dumps\n    evaluation metrics in a csv file [model_performance.csv]\n    \"\"\"\n    metadata = pd.read_csv(sparse_model_metadata)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    metrics_dict: Dict[str, List] = {'norm': [], 'sparse_block_shape': [], 'sparsity_level': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': [], 'accuracy': [], 'log_loss': []}\n    for (_, row) in metadata.iterrows():\n        (norm, sbs, sl) = (row['norm'], row['sparse_block_shape'], row['sparsity_level'])\n        model_path = row['path']\n        model = fetch_model(model_path, device)\n        model_metrics = inference_and_evaluation(model, test_dataloader, device)\n        key = f'{norm}_{sbs}_{sl}'\n        print(key, '=', model_metrics)\n        metrics_dict['norm'].append(norm)\n        metrics_dict['sparse_block_shape'].append(sbs)\n        metrics_dict['sparsity_level'].append(sl)\n        for (key, value) in model_metrics.items():\n            if key in metrics_dict:\n                metrics_dict[key].append(value)\n    sparse_model_metrics = pd.DataFrame(metrics_dict)\n    print(sparse_model_metrics)\n    filename = 'sparse_model_metrics.csv'\n    sparse_model_metrics.to_csv(filename, index=False)\n    print(f'Model metrics file saved to {filename}')",
        "mutated": [
            "def evaluate_metrics(test_dataloader, sparse_model_metadata):\n    if False:\n        i = 10\n    'Evaluates the metrics the sparsified metrics for the dlrm model on various sparsity levels,\\n    block shapes and norms. This function evaluates the model on the test dataset and dumps\\n    evaluation metrics in a csv file [model_performance.csv]\\n    '\n    metadata = pd.read_csv(sparse_model_metadata)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    metrics_dict: Dict[str, List] = {'norm': [], 'sparse_block_shape': [], 'sparsity_level': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': [], 'accuracy': [], 'log_loss': []}\n    for (_, row) in metadata.iterrows():\n        (norm, sbs, sl) = (row['norm'], row['sparse_block_shape'], row['sparsity_level'])\n        model_path = row['path']\n        model = fetch_model(model_path, device)\n        model_metrics = inference_and_evaluation(model, test_dataloader, device)\n        key = f'{norm}_{sbs}_{sl}'\n        print(key, '=', model_metrics)\n        metrics_dict['norm'].append(norm)\n        metrics_dict['sparse_block_shape'].append(sbs)\n        metrics_dict['sparsity_level'].append(sl)\n        for (key, value) in model_metrics.items():\n            if key in metrics_dict:\n                metrics_dict[key].append(value)\n    sparse_model_metrics = pd.DataFrame(metrics_dict)\n    print(sparse_model_metrics)\n    filename = 'sparse_model_metrics.csv'\n    sparse_model_metrics.to_csv(filename, index=False)\n    print(f'Model metrics file saved to {filename}')",
            "def evaluate_metrics(test_dataloader, sparse_model_metadata):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Evaluates the metrics the sparsified metrics for the dlrm model on various sparsity levels,\\n    block shapes and norms. This function evaluates the model on the test dataset and dumps\\n    evaluation metrics in a csv file [model_performance.csv]\\n    '\n    metadata = pd.read_csv(sparse_model_metadata)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    metrics_dict: Dict[str, List] = {'norm': [], 'sparse_block_shape': [], 'sparsity_level': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': [], 'accuracy': [], 'log_loss': []}\n    for (_, row) in metadata.iterrows():\n        (norm, sbs, sl) = (row['norm'], row['sparse_block_shape'], row['sparsity_level'])\n        model_path = row['path']\n        model = fetch_model(model_path, device)\n        model_metrics = inference_and_evaluation(model, test_dataloader, device)\n        key = f'{norm}_{sbs}_{sl}'\n        print(key, '=', model_metrics)\n        metrics_dict['norm'].append(norm)\n        metrics_dict['sparse_block_shape'].append(sbs)\n        metrics_dict['sparsity_level'].append(sl)\n        for (key, value) in model_metrics.items():\n            if key in metrics_dict:\n                metrics_dict[key].append(value)\n    sparse_model_metrics = pd.DataFrame(metrics_dict)\n    print(sparse_model_metrics)\n    filename = 'sparse_model_metrics.csv'\n    sparse_model_metrics.to_csv(filename, index=False)\n    print(f'Model metrics file saved to {filename}')",
            "def evaluate_metrics(test_dataloader, sparse_model_metadata):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Evaluates the metrics the sparsified metrics for the dlrm model on various sparsity levels,\\n    block shapes and norms. This function evaluates the model on the test dataset and dumps\\n    evaluation metrics in a csv file [model_performance.csv]\\n    '\n    metadata = pd.read_csv(sparse_model_metadata)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    metrics_dict: Dict[str, List] = {'norm': [], 'sparse_block_shape': [], 'sparsity_level': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': [], 'accuracy': [], 'log_loss': []}\n    for (_, row) in metadata.iterrows():\n        (norm, sbs, sl) = (row['norm'], row['sparse_block_shape'], row['sparsity_level'])\n        model_path = row['path']\n        model = fetch_model(model_path, device)\n        model_metrics = inference_and_evaluation(model, test_dataloader, device)\n        key = f'{norm}_{sbs}_{sl}'\n        print(key, '=', model_metrics)\n        metrics_dict['norm'].append(norm)\n        metrics_dict['sparse_block_shape'].append(sbs)\n        metrics_dict['sparsity_level'].append(sl)\n        for (key, value) in model_metrics.items():\n            if key in metrics_dict:\n                metrics_dict[key].append(value)\n    sparse_model_metrics = pd.DataFrame(metrics_dict)\n    print(sparse_model_metrics)\n    filename = 'sparse_model_metrics.csv'\n    sparse_model_metrics.to_csv(filename, index=False)\n    print(f'Model metrics file saved to {filename}')",
            "def evaluate_metrics(test_dataloader, sparse_model_metadata):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Evaluates the metrics the sparsified metrics for the dlrm model on various sparsity levels,\\n    block shapes and norms. This function evaluates the model on the test dataset and dumps\\n    evaluation metrics in a csv file [model_performance.csv]\\n    '\n    metadata = pd.read_csv(sparse_model_metadata)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    metrics_dict: Dict[str, List] = {'norm': [], 'sparse_block_shape': [], 'sparsity_level': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': [], 'accuracy': [], 'log_loss': []}\n    for (_, row) in metadata.iterrows():\n        (norm, sbs, sl) = (row['norm'], row['sparse_block_shape'], row['sparsity_level'])\n        model_path = row['path']\n        model = fetch_model(model_path, device)\n        model_metrics = inference_and_evaluation(model, test_dataloader, device)\n        key = f'{norm}_{sbs}_{sl}'\n        print(key, '=', model_metrics)\n        metrics_dict['norm'].append(norm)\n        metrics_dict['sparse_block_shape'].append(sbs)\n        metrics_dict['sparsity_level'].append(sl)\n        for (key, value) in model_metrics.items():\n            if key in metrics_dict:\n                metrics_dict[key].append(value)\n    sparse_model_metrics = pd.DataFrame(metrics_dict)\n    print(sparse_model_metrics)\n    filename = 'sparse_model_metrics.csv'\n    sparse_model_metrics.to_csv(filename, index=False)\n    print(f'Model metrics file saved to {filename}')",
            "def evaluate_metrics(test_dataloader, sparse_model_metadata):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Evaluates the metrics the sparsified metrics for the dlrm model on various sparsity levels,\\n    block shapes and norms. This function evaluates the model on the test dataset and dumps\\n    evaluation metrics in a csv file [model_performance.csv]\\n    '\n    metadata = pd.read_csv(sparse_model_metadata)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    metrics_dict: Dict[str, List] = {'norm': [], 'sparse_block_shape': [], 'sparsity_level': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': [], 'accuracy': [], 'log_loss': []}\n    for (_, row) in metadata.iterrows():\n        (norm, sbs, sl) = (row['norm'], row['sparse_block_shape'], row['sparsity_level'])\n        model_path = row['path']\n        model = fetch_model(model_path, device)\n        model_metrics = inference_and_evaluation(model, test_dataloader, device)\n        key = f'{norm}_{sbs}_{sl}'\n        print(key, '=', model_metrics)\n        metrics_dict['norm'].append(norm)\n        metrics_dict['sparse_block_shape'].append(sbs)\n        metrics_dict['sparsity_level'].append(sl)\n        for (key, value) in model_metrics.items():\n            if key in metrics_dict:\n                metrics_dict[key].append(value)\n    sparse_model_metrics = pd.DataFrame(metrics_dict)\n    print(sparse_model_metrics)\n    filename = 'sparse_model_metrics.csv'\n    sparse_model_metrics.to_csv(filename, index=False)\n    print(f'Model metrics file saved to {filename}')"
        ]
    }
]