[
    {
        "func_name": "expand_quantized_bins",
        "original": "def expand_quantized_bins(quantized_bins, reference_bins):\n    \"\"\"\n    Expand hist bins.\n    \"\"\"\n    expanded_quantized_bins = [0] * len(reference_bins)\n    num_merged_bins = int(len(reference_bins) / len(quantized_bins))\n    j_start = 0\n    j_end = num_merged_bins\n    for idx in range(len(quantized_bins)):\n        zero_count = reference_bins[j_start:j_end].count(0)\n        num_merged_bins = j_end - j_start\n        if zero_count == num_merged_bins:\n            avg_bin_ele = 0\n        else:\n            avg_bin_ele = quantized_bins[idx] / (num_merged_bins - zero_count + 0.0)\n        for idx1 in range(j_start, j_end):\n            expanded_quantized_bins[idx1] = 0 if reference_bins[idx1] == 0 else avg_bin_ele\n        j_start += num_merged_bins\n        j_end += num_merged_bins\n        if idx + 1 == len(quantized_bins) - 1:\n            j_end = len(reference_bins)\n    return expanded_quantized_bins",
        "mutated": [
            "def expand_quantized_bins(quantized_bins, reference_bins):\n    if False:\n        i = 10\n    '\\n    Expand hist bins.\\n    '\n    expanded_quantized_bins = [0] * len(reference_bins)\n    num_merged_bins = int(len(reference_bins) / len(quantized_bins))\n    j_start = 0\n    j_end = num_merged_bins\n    for idx in range(len(quantized_bins)):\n        zero_count = reference_bins[j_start:j_end].count(0)\n        num_merged_bins = j_end - j_start\n        if zero_count == num_merged_bins:\n            avg_bin_ele = 0\n        else:\n            avg_bin_ele = quantized_bins[idx] / (num_merged_bins - zero_count + 0.0)\n        for idx1 in range(j_start, j_end):\n            expanded_quantized_bins[idx1] = 0 if reference_bins[idx1] == 0 else avg_bin_ele\n        j_start += num_merged_bins\n        j_end += num_merged_bins\n        if idx + 1 == len(quantized_bins) - 1:\n            j_end = len(reference_bins)\n    return expanded_quantized_bins",
            "def expand_quantized_bins(quantized_bins, reference_bins):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Expand hist bins.\\n    '\n    expanded_quantized_bins = [0] * len(reference_bins)\n    num_merged_bins = int(len(reference_bins) / len(quantized_bins))\n    j_start = 0\n    j_end = num_merged_bins\n    for idx in range(len(quantized_bins)):\n        zero_count = reference_bins[j_start:j_end].count(0)\n        num_merged_bins = j_end - j_start\n        if zero_count == num_merged_bins:\n            avg_bin_ele = 0\n        else:\n            avg_bin_ele = quantized_bins[idx] / (num_merged_bins - zero_count + 0.0)\n        for idx1 in range(j_start, j_end):\n            expanded_quantized_bins[idx1] = 0 if reference_bins[idx1] == 0 else avg_bin_ele\n        j_start += num_merged_bins\n        j_end += num_merged_bins\n        if idx + 1 == len(quantized_bins) - 1:\n            j_end = len(reference_bins)\n    return expanded_quantized_bins",
            "def expand_quantized_bins(quantized_bins, reference_bins):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Expand hist bins.\\n    '\n    expanded_quantized_bins = [0] * len(reference_bins)\n    num_merged_bins = int(len(reference_bins) / len(quantized_bins))\n    j_start = 0\n    j_end = num_merged_bins\n    for idx in range(len(quantized_bins)):\n        zero_count = reference_bins[j_start:j_end].count(0)\n        num_merged_bins = j_end - j_start\n        if zero_count == num_merged_bins:\n            avg_bin_ele = 0\n        else:\n            avg_bin_ele = quantized_bins[idx] / (num_merged_bins - zero_count + 0.0)\n        for idx1 in range(j_start, j_end):\n            expanded_quantized_bins[idx1] = 0 if reference_bins[idx1] == 0 else avg_bin_ele\n        j_start += num_merged_bins\n        j_end += num_merged_bins\n        if idx + 1 == len(quantized_bins) - 1:\n            j_end = len(reference_bins)\n    return expanded_quantized_bins",
            "def expand_quantized_bins(quantized_bins, reference_bins):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Expand hist bins.\\n    '\n    expanded_quantized_bins = [0] * len(reference_bins)\n    num_merged_bins = int(len(reference_bins) / len(quantized_bins))\n    j_start = 0\n    j_end = num_merged_bins\n    for idx in range(len(quantized_bins)):\n        zero_count = reference_bins[j_start:j_end].count(0)\n        num_merged_bins = j_end - j_start\n        if zero_count == num_merged_bins:\n            avg_bin_ele = 0\n        else:\n            avg_bin_ele = quantized_bins[idx] / (num_merged_bins - zero_count + 0.0)\n        for idx1 in range(j_start, j_end):\n            expanded_quantized_bins[idx1] = 0 if reference_bins[idx1] == 0 else avg_bin_ele\n        j_start += num_merged_bins\n        j_end += num_merged_bins\n        if idx + 1 == len(quantized_bins) - 1:\n            j_end = len(reference_bins)\n    return expanded_quantized_bins",
            "def expand_quantized_bins(quantized_bins, reference_bins):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Expand hist bins.\\n    '\n    expanded_quantized_bins = [0] * len(reference_bins)\n    num_merged_bins = int(len(reference_bins) / len(quantized_bins))\n    j_start = 0\n    j_end = num_merged_bins\n    for idx in range(len(quantized_bins)):\n        zero_count = reference_bins[j_start:j_end].count(0)\n        num_merged_bins = j_end - j_start\n        if zero_count == num_merged_bins:\n            avg_bin_ele = 0\n        else:\n            avg_bin_ele = quantized_bins[idx] / (num_merged_bins - zero_count + 0.0)\n        for idx1 in range(j_start, j_end):\n            expanded_quantized_bins[idx1] = 0 if reference_bins[idx1] == 0 else avg_bin_ele\n        j_start += num_merged_bins\n        j_end += num_merged_bins\n        if idx + 1 == len(quantized_bins) - 1:\n            j_end = len(reference_bins)\n    return expanded_quantized_bins"
        ]
    },
    {
        "func_name": "safe_entropy",
        "original": "def safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum):\n    \"\"\"\n    Calculate the entropy.\n    \"\"\"\n    assert len(reference_distr_P) == len(candidate_distr_Q)\n    tmp_sum1 = 0\n    tmp_sum2 = 0\n    for idx in range(len(reference_distr_P)):\n        p_idx = reference_distr_P[idx]\n        q_idx = candidate_distr_Q[idx]\n        if p_idx == 0:\n            tmp_sum1 += 0\n            tmp_sum2 += 0\n        else:\n            if q_idx == 0:\n                _logger.error('Fatal error!, idx = ' + str(idx) + ' qindex = 0! p_idx = ' + str(p_idx))\n            tmp_sum1 += p_idx * math.log(Q_sum * p_idx)\n            tmp_sum2 += p_idx * math.log(P_sum * q_idx)\n    return (tmp_sum1 - tmp_sum2) / P_sum",
        "mutated": [
            "def safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum):\n    if False:\n        i = 10\n    '\\n    Calculate the entropy.\\n    '\n    assert len(reference_distr_P) == len(candidate_distr_Q)\n    tmp_sum1 = 0\n    tmp_sum2 = 0\n    for idx in range(len(reference_distr_P)):\n        p_idx = reference_distr_P[idx]\n        q_idx = candidate_distr_Q[idx]\n        if p_idx == 0:\n            tmp_sum1 += 0\n            tmp_sum2 += 0\n        else:\n            if q_idx == 0:\n                _logger.error('Fatal error!, idx = ' + str(idx) + ' qindex = 0! p_idx = ' + str(p_idx))\n            tmp_sum1 += p_idx * math.log(Q_sum * p_idx)\n            tmp_sum2 += p_idx * math.log(P_sum * q_idx)\n    return (tmp_sum1 - tmp_sum2) / P_sum",
            "def safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculate the entropy.\\n    '\n    assert len(reference_distr_P) == len(candidate_distr_Q)\n    tmp_sum1 = 0\n    tmp_sum2 = 0\n    for idx in range(len(reference_distr_P)):\n        p_idx = reference_distr_P[idx]\n        q_idx = candidate_distr_Q[idx]\n        if p_idx == 0:\n            tmp_sum1 += 0\n            tmp_sum2 += 0\n        else:\n            if q_idx == 0:\n                _logger.error('Fatal error!, idx = ' + str(idx) + ' qindex = 0! p_idx = ' + str(p_idx))\n            tmp_sum1 += p_idx * math.log(Q_sum * p_idx)\n            tmp_sum2 += p_idx * math.log(P_sum * q_idx)\n    return (tmp_sum1 - tmp_sum2) / P_sum",
            "def safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculate the entropy.\\n    '\n    assert len(reference_distr_P) == len(candidate_distr_Q)\n    tmp_sum1 = 0\n    tmp_sum2 = 0\n    for idx in range(len(reference_distr_P)):\n        p_idx = reference_distr_P[idx]\n        q_idx = candidate_distr_Q[idx]\n        if p_idx == 0:\n            tmp_sum1 += 0\n            tmp_sum2 += 0\n        else:\n            if q_idx == 0:\n                _logger.error('Fatal error!, idx = ' + str(idx) + ' qindex = 0! p_idx = ' + str(p_idx))\n            tmp_sum1 += p_idx * math.log(Q_sum * p_idx)\n            tmp_sum2 += p_idx * math.log(P_sum * q_idx)\n    return (tmp_sum1 - tmp_sum2) / P_sum",
            "def safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculate the entropy.\\n    '\n    assert len(reference_distr_P) == len(candidate_distr_Q)\n    tmp_sum1 = 0\n    tmp_sum2 = 0\n    for idx in range(len(reference_distr_P)):\n        p_idx = reference_distr_P[idx]\n        q_idx = candidate_distr_Q[idx]\n        if p_idx == 0:\n            tmp_sum1 += 0\n            tmp_sum2 += 0\n        else:\n            if q_idx == 0:\n                _logger.error('Fatal error!, idx = ' + str(idx) + ' qindex = 0! p_idx = ' + str(p_idx))\n            tmp_sum1 += p_idx * math.log(Q_sum * p_idx)\n            tmp_sum2 += p_idx * math.log(P_sum * q_idx)\n    return (tmp_sum1 - tmp_sum2) / P_sum",
            "def safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculate the entropy.\\n    '\n    assert len(reference_distr_P) == len(candidate_distr_Q)\n    tmp_sum1 = 0\n    tmp_sum2 = 0\n    for idx in range(len(reference_distr_P)):\n        p_idx = reference_distr_P[idx]\n        q_idx = candidate_distr_Q[idx]\n        if p_idx == 0:\n            tmp_sum1 += 0\n            tmp_sum2 += 0\n        else:\n            if q_idx == 0:\n                _logger.error('Fatal error!, idx = ' + str(idx) + ' qindex = 0! p_idx = ' + str(p_idx))\n            tmp_sum1 += p_idx * math.log(Q_sum * p_idx)\n            tmp_sum2 += p_idx * math.log(P_sum * q_idx)\n    return (tmp_sum1 - tmp_sum2) / P_sum"
        ]
    },
    {
        "func_name": "cal_kl_threshold",
        "original": "def cal_kl_threshold(hist, bin_width, bits):\n    \"\"\"\n    Using the KL-divergenc method to get the more precise threshold.\n\n    Args:\n        hist(List): The hist of the tensor.\n        bin_width(float): The bin width for the hist.\n        bits(int): The quantization bits.\n    \"\"\"\n    assert hist.ndim == 1\n    hist_bins = hist.shape[0]\n    starting_iter = int((hist_bins - 1) * 0.5)\n    quant_range = 2 ** (bits - 1) - 1\n    P_sum = np.sum(np.array(hist).ravel())\n    min_kl_divergence = 0\n    min_kl_index = 0\n    kl_inited = False\n    for i in range(starting_iter, hist_bins):\n        reference_distr_P = hist[0:i].tolist()\n        outliers_count = sum(hist[i:])\n        if reference_distr_P[i - 1] == 0:\n            continue\n        reference_distr_P[i - 1] += outliers_count\n        reference_distr_bins = reference_distr_P[:]\n        candidate_distr_Q = hist[0:i].tolist()\n        num_merged_bins = int(i / quant_range)\n        candidate_distr_Q_quantized = [0] * quant_range\n        j_start = 0\n        j_end = num_merged_bins\n        for idx in range(quant_range):\n            candidate_distr_Q_quantized[idx] = sum(candidate_distr_Q[j_start:j_end])\n            j_start += num_merged_bins\n            j_end += num_merged_bins\n            if idx + 1 == quant_range - 1:\n                j_end = i\n        candidate_distr_Q = expand_quantized_bins(candidate_distr_Q_quantized, reference_distr_bins)\n        Q_sum = sum(candidate_distr_Q)\n        kl_divergence = safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum)\n        if not kl_inited:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n            kl_inited = True\n        elif kl_divergence < min_kl_divergence:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n        else:\n            pass\n    if min_kl_index == 0:\n        while starting_iter > 0:\n            if hist[starting_iter] == 0:\n                starting_iter -= 1\n                continue\n            else:\n                break\n        min_kl_index = starting_iter\n    return (min_kl_index + 0.5) * bin_width",
        "mutated": [
            "def cal_kl_threshold(hist, bin_width, bits):\n    if False:\n        i = 10\n    '\\n    Using the KL-divergenc method to get the more precise threshold.\\n\\n    Args:\\n        hist(List): The hist of the tensor.\\n        bin_width(float): The bin width for the hist.\\n        bits(int): The quantization bits.\\n    '\n    assert hist.ndim == 1\n    hist_bins = hist.shape[0]\n    starting_iter = int((hist_bins - 1) * 0.5)\n    quant_range = 2 ** (bits - 1) - 1\n    P_sum = np.sum(np.array(hist).ravel())\n    min_kl_divergence = 0\n    min_kl_index = 0\n    kl_inited = False\n    for i in range(starting_iter, hist_bins):\n        reference_distr_P = hist[0:i].tolist()\n        outliers_count = sum(hist[i:])\n        if reference_distr_P[i - 1] == 0:\n            continue\n        reference_distr_P[i - 1] += outliers_count\n        reference_distr_bins = reference_distr_P[:]\n        candidate_distr_Q = hist[0:i].tolist()\n        num_merged_bins = int(i / quant_range)\n        candidate_distr_Q_quantized = [0] * quant_range\n        j_start = 0\n        j_end = num_merged_bins\n        for idx in range(quant_range):\n            candidate_distr_Q_quantized[idx] = sum(candidate_distr_Q[j_start:j_end])\n            j_start += num_merged_bins\n            j_end += num_merged_bins\n            if idx + 1 == quant_range - 1:\n                j_end = i\n        candidate_distr_Q = expand_quantized_bins(candidate_distr_Q_quantized, reference_distr_bins)\n        Q_sum = sum(candidate_distr_Q)\n        kl_divergence = safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum)\n        if not kl_inited:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n            kl_inited = True\n        elif kl_divergence < min_kl_divergence:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n        else:\n            pass\n    if min_kl_index == 0:\n        while starting_iter > 0:\n            if hist[starting_iter] == 0:\n                starting_iter -= 1\n                continue\n            else:\n                break\n        min_kl_index = starting_iter\n    return (min_kl_index + 0.5) * bin_width",
            "def cal_kl_threshold(hist, bin_width, bits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Using the KL-divergenc method to get the more precise threshold.\\n\\n    Args:\\n        hist(List): The hist of the tensor.\\n        bin_width(float): The bin width for the hist.\\n        bits(int): The quantization bits.\\n    '\n    assert hist.ndim == 1\n    hist_bins = hist.shape[0]\n    starting_iter = int((hist_bins - 1) * 0.5)\n    quant_range = 2 ** (bits - 1) - 1\n    P_sum = np.sum(np.array(hist).ravel())\n    min_kl_divergence = 0\n    min_kl_index = 0\n    kl_inited = False\n    for i in range(starting_iter, hist_bins):\n        reference_distr_P = hist[0:i].tolist()\n        outliers_count = sum(hist[i:])\n        if reference_distr_P[i - 1] == 0:\n            continue\n        reference_distr_P[i - 1] += outliers_count\n        reference_distr_bins = reference_distr_P[:]\n        candidate_distr_Q = hist[0:i].tolist()\n        num_merged_bins = int(i / quant_range)\n        candidate_distr_Q_quantized = [0] * quant_range\n        j_start = 0\n        j_end = num_merged_bins\n        for idx in range(quant_range):\n            candidate_distr_Q_quantized[idx] = sum(candidate_distr_Q[j_start:j_end])\n            j_start += num_merged_bins\n            j_end += num_merged_bins\n            if idx + 1 == quant_range - 1:\n                j_end = i\n        candidate_distr_Q = expand_quantized_bins(candidate_distr_Q_quantized, reference_distr_bins)\n        Q_sum = sum(candidate_distr_Q)\n        kl_divergence = safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum)\n        if not kl_inited:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n            kl_inited = True\n        elif kl_divergence < min_kl_divergence:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n        else:\n            pass\n    if min_kl_index == 0:\n        while starting_iter > 0:\n            if hist[starting_iter] == 0:\n                starting_iter -= 1\n                continue\n            else:\n                break\n        min_kl_index = starting_iter\n    return (min_kl_index + 0.5) * bin_width",
            "def cal_kl_threshold(hist, bin_width, bits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Using the KL-divergenc method to get the more precise threshold.\\n\\n    Args:\\n        hist(List): The hist of the tensor.\\n        bin_width(float): The bin width for the hist.\\n        bits(int): The quantization bits.\\n    '\n    assert hist.ndim == 1\n    hist_bins = hist.shape[0]\n    starting_iter = int((hist_bins - 1) * 0.5)\n    quant_range = 2 ** (bits - 1) - 1\n    P_sum = np.sum(np.array(hist).ravel())\n    min_kl_divergence = 0\n    min_kl_index = 0\n    kl_inited = False\n    for i in range(starting_iter, hist_bins):\n        reference_distr_P = hist[0:i].tolist()\n        outliers_count = sum(hist[i:])\n        if reference_distr_P[i - 1] == 0:\n            continue\n        reference_distr_P[i - 1] += outliers_count\n        reference_distr_bins = reference_distr_P[:]\n        candidate_distr_Q = hist[0:i].tolist()\n        num_merged_bins = int(i / quant_range)\n        candidate_distr_Q_quantized = [0] * quant_range\n        j_start = 0\n        j_end = num_merged_bins\n        for idx in range(quant_range):\n            candidate_distr_Q_quantized[idx] = sum(candidate_distr_Q[j_start:j_end])\n            j_start += num_merged_bins\n            j_end += num_merged_bins\n            if idx + 1 == quant_range - 1:\n                j_end = i\n        candidate_distr_Q = expand_quantized_bins(candidate_distr_Q_quantized, reference_distr_bins)\n        Q_sum = sum(candidate_distr_Q)\n        kl_divergence = safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum)\n        if not kl_inited:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n            kl_inited = True\n        elif kl_divergence < min_kl_divergence:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n        else:\n            pass\n    if min_kl_index == 0:\n        while starting_iter > 0:\n            if hist[starting_iter] == 0:\n                starting_iter -= 1\n                continue\n            else:\n                break\n        min_kl_index = starting_iter\n    return (min_kl_index + 0.5) * bin_width",
            "def cal_kl_threshold(hist, bin_width, bits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Using the KL-divergenc method to get the more precise threshold.\\n\\n    Args:\\n        hist(List): The hist of the tensor.\\n        bin_width(float): The bin width for the hist.\\n        bits(int): The quantization bits.\\n    '\n    assert hist.ndim == 1\n    hist_bins = hist.shape[0]\n    starting_iter = int((hist_bins - 1) * 0.5)\n    quant_range = 2 ** (bits - 1) - 1\n    P_sum = np.sum(np.array(hist).ravel())\n    min_kl_divergence = 0\n    min_kl_index = 0\n    kl_inited = False\n    for i in range(starting_iter, hist_bins):\n        reference_distr_P = hist[0:i].tolist()\n        outliers_count = sum(hist[i:])\n        if reference_distr_P[i - 1] == 0:\n            continue\n        reference_distr_P[i - 1] += outliers_count\n        reference_distr_bins = reference_distr_P[:]\n        candidate_distr_Q = hist[0:i].tolist()\n        num_merged_bins = int(i / quant_range)\n        candidate_distr_Q_quantized = [0] * quant_range\n        j_start = 0\n        j_end = num_merged_bins\n        for idx in range(quant_range):\n            candidate_distr_Q_quantized[idx] = sum(candidate_distr_Q[j_start:j_end])\n            j_start += num_merged_bins\n            j_end += num_merged_bins\n            if idx + 1 == quant_range - 1:\n                j_end = i\n        candidate_distr_Q = expand_quantized_bins(candidate_distr_Q_quantized, reference_distr_bins)\n        Q_sum = sum(candidate_distr_Q)\n        kl_divergence = safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum)\n        if not kl_inited:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n            kl_inited = True\n        elif kl_divergence < min_kl_divergence:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n        else:\n            pass\n    if min_kl_index == 0:\n        while starting_iter > 0:\n            if hist[starting_iter] == 0:\n                starting_iter -= 1\n                continue\n            else:\n                break\n        min_kl_index = starting_iter\n    return (min_kl_index + 0.5) * bin_width",
            "def cal_kl_threshold(hist, bin_width, bits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Using the KL-divergenc method to get the more precise threshold.\\n\\n    Args:\\n        hist(List): The hist of the tensor.\\n        bin_width(float): The bin width for the hist.\\n        bits(int): The quantization bits.\\n    '\n    assert hist.ndim == 1\n    hist_bins = hist.shape[0]\n    starting_iter = int((hist_bins - 1) * 0.5)\n    quant_range = 2 ** (bits - 1) - 1\n    P_sum = np.sum(np.array(hist).ravel())\n    min_kl_divergence = 0\n    min_kl_index = 0\n    kl_inited = False\n    for i in range(starting_iter, hist_bins):\n        reference_distr_P = hist[0:i].tolist()\n        outliers_count = sum(hist[i:])\n        if reference_distr_P[i - 1] == 0:\n            continue\n        reference_distr_P[i - 1] += outliers_count\n        reference_distr_bins = reference_distr_P[:]\n        candidate_distr_Q = hist[0:i].tolist()\n        num_merged_bins = int(i / quant_range)\n        candidate_distr_Q_quantized = [0] * quant_range\n        j_start = 0\n        j_end = num_merged_bins\n        for idx in range(quant_range):\n            candidate_distr_Q_quantized[idx] = sum(candidate_distr_Q[j_start:j_end])\n            j_start += num_merged_bins\n            j_end += num_merged_bins\n            if idx + 1 == quant_range - 1:\n                j_end = i\n        candidate_distr_Q = expand_quantized_bins(candidate_distr_Q_quantized, reference_distr_bins)\n        Q_sum = sum(candidate_distr_Q)\n        kl_divergence = safe_entropy(reference_distr_P, P_sum, candidate_distr_Q, Q_sum)\n        if not kl_inited:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n            kl_inited = True\n        elif kl_divergence < min_kl_divergence:\n            min_kl_divergence = kl_divergence\n            min_kl_index = i\n        else:\n            pass\n    if min_kl_index == 0:\n        while starting_iter > 0:\n            if hist[starting_iter] == 0:\n                starting_iter -= 1\n                continue\n            else:\n                break\n        min_kl_index = starting_iter\n    return (min_kl_index + 0.5) * bin_width"
        ]
    }
]