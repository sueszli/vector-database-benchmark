[
    {
        "func_name": "__init__",
        "original": "def __init__(self, hidden_size):\n    super().__init__()\n    self.embedding = paddle.nn.Embedding(hidden_size, hidden_size)\n    self.col_linear = fleet.meta_parallel.ColumnParallelLinear(in_features=hidden_size, out_features=hidden_size, weight_attr=None, has_bias=True, gather_output=False)\n    self.row_linear = fleet.meta_parallel.RowParallelLinear(in_features=hidden_size, out_features=hidden_size, has_bias=True, input_is_parallel=True)\n    self.layer_norm = paddle.nn.LayerNorm(hidden_size)",
        "mutated": [
            "def __init__(self, hidden_size):\n    if False:\n        i = 10\n    super().__init__()\n    self.embedding = paddle.nn.Embedding(hidden_size, hidden_size)\n    self.col_linear = fleet.meta_parallel.ColumnParallelLinear(in_features=hidden_size, out_features=hidden_size, weight_attr=None, has_bias=True, gather_output=False)\n    self.row_linear = fleet.meta_parallel.RowParallelLinear(in_features=hidden_size, out_features=hidden_size, has_bias=True, input_is_parallel=True)\n    self.layer_norm = paddle.nn.LayerNorm(hidden_size)",
            "def __init__(self, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.embedding = paddle.nn.Embedding(hidden_size, hidden_size)\n    self.col_linear = fleet.meta_parallel.ColumnParallelLinear(in_features=hidden_size, out_features=hidden_size, weight_attr=None, has_bias=True, gather_output=False)\n    self.row_linear = fleet.meta_parallel.RowParallelLinear(in_features=hidden_size, out_features=hidden_size, has_bias=True, input_is_parallel=True)\n    self.layer_norm = paddle.nn.LayerNorm(hidden_size)",
            "def __init__(self, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.embedding = paddle.nn.Embedding(hidden_size, hidden_size)\n    self.col_linear = fleet.meta_parallel.ColumnParallelLinear(in_features=hidden_size, out_features=hidden_size, weight_attr=None, has_bias=True, gather_output=False)\n    self.row_linear = fleet.meta_parallel.RowParallelLinear(in_features=hidden_size, out_features=hidden_size, has_bias=True, input_is_parallel=True)\n    self.layer_norm = paddle.nn.LayerNorm(hidden_size)",
            "def __init__(self, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.embedding = paddle.nn.Embedding(hidden_size, hidden_size)\n    self.col_linear = fleet.meta_parallel.ColumnParallelLinear(in_features=hidden_size, out_features=hidden_size, weight_attr=None, has_bias=True, gather_output=False)\n    self.row_linear = fleet.meta_parallel.RowParallelLinear(in_features=hidden_size, out_features=hidden_size, has_bias=True, input_is_parallel=True)\n    self.layer_norm = paddle.nn.LayerNorm(hidden_size)",
            "def __init__(self, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.embedding = paddle.nn.Embedding(hidden_size, hidden_size)\n    self.col_linear = fleet.meta_parallel.ColumnParallelLinear(in_features=hidden_size, out_features=hidden_size, weight_attr=None, has_bias=True, gather_output=False)\n    self.row_linear = fleet.meta_parallel.RowParallelLinear(in_features=hidden_size, out_features=hidden_size, has_bias=True, input_is_parallel=True)\n    self.layer_norm = paddle.nn.LayerNorm(hidden_size)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    out = self.embedding(x)\n    out = self.col_linear(out)\n    out = self.row_linear(out)\n    output = self.layer_norm(out)\n    return output",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    out = self.embedding(x)\n    out = self.col_linear(out)\n    out = self.row_linear(out)\n    output = self.layer_norm(out)\n    return output",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    out = self.embedding(x)\n    out = self.col_linear(out)\n    out = self.row_linear(out)\n    output = self.layer_norm(out)\n    return output",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    out = self.embedding(x)\n    out = self.col_linear(out)\n    out = self.row_linear(out)\n    output = self.layer_norm(out)\n    return output",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    out = self.embedding(x)\n    out = self.col_linear(out)\n    out = self.row_linear(out)\n    output = self.layer_norm(out)\n    return output",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    out = self.embedding(x)\n    out = self.col_linear(out)\n    out = self.row_linear(out)\n    output = self.layer_norm(out)\n    return output"
        ]
    },
    {
        "func_name": "filter_fn",
        "original": "def filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    \"\"\"\n    Layer fliter function for tensor parallelism transformer.\n\n    In tensor parallelism of transformer like model, there is 4 kind of param\n    that are supposed to be the same in all tensor parallel peers:\n        * position embedding\n        * scale of layer norm\n        * bias of layer norm\n        * bias of row parallel linear\n\n    set corresponding input args to select specific layers.\n    NOTE  adopting the param name pattern for different transformer blocks.\n    \"\"\"\n    p_name = param.name\n    if pos_emb and p_name.startswith('embedding'):\n        return True\n    elif layer_norm and p_name.startswith('layer_norm'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False",
        "mutated": [
            "def filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    if False:\n        i = 10\n    '\\n    Layer fliter function for tensor parallelism transformer.\\n\\n    In tensor parallelism of transformer like model, there is 4 kind of param\\n    that are supposed to be the same in all tensor parallel peers:\\n        * position embedding\\n        * scale of layer norm\\n        * bias of layer norm\\n        * bias of row parallel linear\\n\\n    set corresponding input args to select specific layers.\\n    NOTE  adopting the param name pattern for different transformer blocks.\\n    '\n    p_name = param.name\n    if pos_emb and p_name.startswith('embedding'):\n        return True\n    elif layer_norm and p_name.startswith('layer_norm'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False",
            "def filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Layer fliter function for tensor parallelism transformer.\\n\\n    In tensor parallelism of transformer like model, there is 4 kind of param\\n    that are supposed to be the same in all tensor parallel peers:\\n        * position embedding\\n        * scale of layer norm\\n        * bias of layer norm\\n        * bias of row parallel linear\\n\\n    set corresponding input args to select specific layers.\\n    NOTE  adopting the param name pattern for different transformer blocks.\\n    '\n    p_name = param.name\n    if pos_emb and p_name.startswith('embedding'):\n        return True\n    elif layer_norm and p_name.startswith('layer_norm'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False",
            "def filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Layer fliter function for tensor parallelism transformer.\\n\\n    In tensor parallelism of transformer like model, there is 4 kind of param\\n    that are supposed to be the same in all tensor parallel peers:\\n        * position embedding\\n        * scale of layer norm\\n        * bias of layer norm\\n        * bias of row parallel linear\\n\\n    set corresponding input args to select specific layers.\\n    NOTE  adopting the param name pattern for different transformer blocks.\\n    '\n    p_name = param.name\n    if pos_emb and p_name.startswith('embedding'):\n        return True\n    elif layer_norm and p_name.startswith('layer_norm'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False",
            "def filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Layer fliter function for tensor parallelism transformer.\\n\\n    In tensor parallelism of transformer like model, there is 4 kind of param\\n    that are supposed to be the same in all tensor parallel peers:\\n        * position embedding\\n        * scale of layer norm\\n        * bias of layer norm\\n        * bias of row parallel linear\\n\\n    set corresponding input args to select specific layers.\\n    NOTE  adopting the param name pattern for different transformer blocks.\\n    '\n    p_name = param.name\n    if pos_emb and p_name.startswith('embedding'):\n        return True\n    elif layer_norm and p_name.startswith('layer_norm'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False",
            "def filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Layer fliter function for tensor parallelism transformer.\\n\\n    In tensor parallelism of transformer like model, there is 4 kind of param\\n    that are supposed to be the same in all tensor parallel peers:\\n        * position embedding\\n        * scale of layer norm\\n        * bias of layer norm\\n        * bias of row parallel linear\\n\\n    set corresponding input args to select specific layers.\\n    NOTE  adopting the param name pattern for different transformer blocks.\\n    '\n    p_name = param.name\n    if pos_emb and p_name.startswith('embedding'):\n        return True\n    elif layer_norm and p_name.startswith('layer_norm'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False"
        ]
    },
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002'",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002'",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002'",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002'",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002'",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002'"
        ]
    },
    {
        "func_name": "test_tensor_parallel_extra_sync",
        "original": "def test_tensor_parallel_extra_sync(self):\n    from paddle.distributed import fleet\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    fleet.init(is_collective=True, strategy=strategy)\n    (main_program, startup_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, startup_program):\n        hidden_size = 512\n        input_x = paddle.static.data(name='x', shape=[-1, hidden_size], dtype='int64')\n        model_a = TensorParallelNet(hidden_size)\n        y = model_a(input_x)\n        loss = paddle.mean(y)\n    optimizer = paddle.optimizer.Adam(0.01)\n    optimizer = fleet.distributed_optimizer(optimizer, strategy=strategy)\n    optimizer.minimize(loss)\n    ref_ops = ['lookup_table_v2', 'c_identity', 'matmul_v2', 'elementwise_add', 'matmul_v2', 'c_allreduce_sum', 'elementwise_add', 'layer_norm', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'layer_norm_grad', 'elementwise_add_grad', 'c_identity', 'matmul_v2_grad', 'elementwise_add_grad', 'matmul_v2_grad', 'c_allreduce_sum', 'lookup_table_v2_grad', 'adam', 'adam', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam']\n    paddle.distributed.fleet.utils.tensor_parallel_utils.add_extra_synchronization(main_program, params_filter_fn=filter_fn)\n    ops = [op.type for op in main_program.global_block().ops]\n    self.assertTrue(ops == ref_ops)",
        "mutated": [
            "def test_tensor_parallel_extra_sync(self):\n    if False:\n        i = 10\n    from paddle.distributed import fleet\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    fleet.init(is_collective=True, strategy=strategy)\n    (main_program, startup_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, startup_program):\n        hidden_size = 512\n        input_x = paddle.static.data(name='x', shape=[-1, hidden_size], dtype='int64')\n        model_a = TensorParallelNet(hidden_size)\n        y = model_a(input_x)\n        loss = paddle.mean(y)\n    optimizer = paddle.optimizer.Adam(0.01)\n    optimizer = fleet.distributed_optimizer(optimizer, strategy=strategy)\n    optimizer.minimize(loss)\n    ref_ops = ['lookup_table_v2', 'c_identity', 'matmul_v2', 'elementwise_add', 'matmul_v2', 'c_allreduce_sum', 'elementwise_add', 'layer_norm', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'layer_norm_grad', 'elementwise_add_grad', 'c_identity', 'matmul_v2_grad', 'elementwise_add_grad', 'matmul_v2_grad', 'c_allreduce_sum', 'lookup_table_v2_grad', 'adam', 'adam', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam']\n    paddle.distributed.fleet.utils.tensor_parallel_utils.add_extra_synchronization(main_program, params_filter_fn=filter_fn)\n    ops = [op.type for op in main_program.global_block().ops]\n    self.assertTrue(ops == ref_ops)",
            "def test_tensor_parallel_extra_sync(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from paddle.distributed import fleet\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    fleet.init(is_collective=True, strategy=strategy)\n    (main_program, startup_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, startup_program):\n        hidden_size = 512\n        input_x = paddle.static.data(name='x', shape=[-1, hidden_size], dtype='int64')\n        model_a = TensorParallelNet(hidden_size)\n        y = model_a(input_x)\n        loss = paddle.mean(y)\n    optimizer = paddle.optimizer.Adam(0.01)\n    optimizer = fleet.distributed_optimizer(optimizer, strategy=strategy)\n    optimizer.minimize(loss)\n    ref_ops = ['lookup_table_v2', 'c_identity', 'matmul_v2', 'elementwise_add', 'matmul_v2', 'c_allreduce_sum', 'elementwise_add', 'layer_norm', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'layer_norm_grad', 'elementwise_add_grad', 'c_identity', 'matmul_v2_grad', 'elementwise_add_grad', 'matmul_v2_grad', 'c_allreduce_sum', 'lookup_table_v2_grad', 'adam', 'adam', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam']\n    paddle.distributed.fleet.utils.tensor_parallel_utils.add_extra_synchronization(main_program, params_filter_fn=filter_fn)\n    ops = [op.type for op in main_program.global_block().ops]\n    self.assertTrue(ops == ref_ops)",
            "def test_tensor_parallel_extra_sync(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from paddle.distributed import fleet\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    fleet.init(is_collective=True, strategy=strategy)\n    (main_program, startup_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, startup_program):\n        hidden_size = 512\n        input_x = paddle.static.data(name='x', shape=[-1, hidden_size], dtype='int64')\n        model_a = TensorParallelNet(hidden_size)\n        y = model_a(input_x)\n        loss = paddle.mean(y)\n    optimizer = paddle.optimizer.Adam(0.01)\n    optimizer = fleet.distributed_optimizer(optimizer, strategy=strategy)\n    optimizer.minimize(loss)\n    ref_ops = ['lookup_table_v2', 'c_identity', 'matmul_v2', 'elementwise_add', 'matmul_v2', 'c_allreduce_sum', 'elementwise_add', 'layer_norm', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'layer_norm_grad', 'elementwise_add_grad', 'c_identity', 'matmul_v2_grad', 'elementwise_add_grad', 'matmul_v2_grad', 'c_allreduce_sum', 'lookup_table_v2_grad', 'adam', 'adam', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam']\n    paddle.distributed.fleet.utils.tensor_parallel_utils.add_extra_synchronization(main_program, params_filter_fn=filter_fn)\n    ops = [op.type for op in main_program.global_block().ops]\n    self.assertTrue(ops == ref_ops)",
            "def test_tensor_parallel_extra_sync(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from paddle.distributed import fleet\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    fleet.init(is_collective=True, strategy=strategy)\n    (main_program, startup_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, startup_program):\n        hidden_size = 512\n        input_x = paddle.static.data(name='x', shape=[-1, hidden_size], dtype='int64')\n        model_a = TensorParallelNet(hidden_size)\n        y = model_a(input_x)\n        loss = paddle.mean(y)\n    optimizer = paddle.optimizer.Adam(0.01)\n    optimizer = fleet.distributed_optimizer(optimizer, strategy=strategy)\n    optimizer.minimize(loss)\n    ref_ops = ['lookup_table_v2', 'c_identity', 'matmul_v2', 'elementwise_add', 'matmul_v2', 'c_allreduce_sum', 'elementwise_add', 'layer_norm', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'layer_norm_grad', 'elementwise_add_grad', 'c_identity', 'matmul_v2_grad', 'elementwise_add_grad', 'matmul_v2_grad', 'c_allreduce_sum', 'lookup_table_v2_grad', 'adam', 'adam', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam']\n    paddle.distributed.fleet.utils.tensor_parallel_utils.add_extra_synchronization(main_program, params_filter_fn=filter_fn)\n    ops = [op.type for op in main_program.global_block().ops]\n    self.assertTrue(ops == ref_ops)",
            "def test_tensor_parallel_extra_sync(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from paddle.distributed import fleet\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.tensor_parallel = True\n    strategy.tensor_parallel_configs = {'tensor_parallel_degree': 2}\n    fleet.init(is_collective=True, strategy=strategy)\n    (main_program, startup_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, startup_program):\n        hidden_size = 512\n        input_x = paddle.static.data(name='x', shape=[-1, hidden_size], dtype='int64')\n        model_a = TensorParallelNet(hidden_size)\n        y = model_a(input_x)\n        loss = paddle.mean(y)\n    optimizer = paddle.optimizer.Adam(0.01)\n    optimizer = fleet.distributed_optimizer(optimizer, strategy=strategy)\n    optimizer.minimize(loss)\n    ref_ops = ['lookup_table_v2', 'c_identity', 'matmul_v2', 'elementwise_add', 'matmul_v2', 'c_allreduce_sum', 'elementwise_add', 'layer_norm', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'layer_norm_grad', 'elementwise_add_grad', 'c_identity', 'matmul_v2_grad', 'elementwise_add_grad', 'matmul_v2_grad', 'c_allreduce_sum', 'lookup_table_v2_grad', 'adam', 'adam', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam', 'c_broadcast', 'adam']\n    paddle.distributed.fleet.utils.tensor_parallel_utils.add_extra_synchronization(main_program, params_filter_fn=filter_fn)\n    ops = [op.type for op in main_program.global_block().ops]\n    self.assertTrue(ops == ref_ops)"
        ]
    }
]