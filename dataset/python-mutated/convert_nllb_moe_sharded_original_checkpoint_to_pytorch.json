[
    {
        "func_name": "remove_ignore_keys_",
        "original": "def remove_ignore_keys_(state_dict):\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', 'decoder.output_projection.weight', '_float_tensor', 'encoder.embed_positions._float_tensor', 'decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)",
        "mutated": [
            "def remove_ignore_keys_(state_dict):\n    if False:\n        i = 10\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', 'decoder.output_projection.weight', '_float_tensor', 'encoder.embed_positions._float_tensor', 'decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)",
            "def remove_ignore_keys_(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', 'decoder.output_projection.weight', '_float_tensor', 'encoder.embed_positions._float_tensor', 'decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)",
            "def remove_ignore_keys_(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', 'decoder.output_projection.weight', '_float_tensor', 'encoder.embed_positions._float_tensor', 'decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)",
            "def remove_ignore_keys_(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', 'decoder.output_projection.weight', '_float_tensor', 'encoder.embed_positions._float_tensor', 'decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)",
            "def remove_ignore_keys_(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', 'decoder.output_projection.weight', '_float_tensor', 'encoder.embed_positions._float_tensor', 'decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)"
        ]
    },
    {
        "func_name": "make_linear_from_emb",
        "original": "def make_linear_from_emb(emb):\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer",
        "mutated": [
            "def make_linear_from_emb(emb):\n    if False:\n        i = 10\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer",
            "def make_linear_from_emb(emb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer",
            "def make_linear_from_emb(emb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer",
            "def make_linear_from_emb(emb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer",
            "def make_linear_from_emb(emb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer"
        ]
    },
    {
        "func_name": "rename_fairseq_keys",
        "original": "def rename_fairseq_keys(state_dict, expert_idx=None):\n    new_dict = {}\n    for old_key in state_dict.keys():\n        key = old_key\n        if 'moe_layer.experts.' in key:\n            if expert_idx is not None:\n                key = key.replace('moe_layer.experts.0', f'ffn.experts.expert_{expert_idx}')\n            else:\n                key = key.replace('moe_layer.experts.', 'ffn.experts.expert_')\n        if 'gate' in key:\n            key = key.replace('.moe_layer.gate.wg', '.ffn.router.classifier')\n        if 'fc2' and 'experts' not in key:\n            key = key.replace('.fc2.', '.ffn.fc2.')\n        if 'fc1' and 'experts' not in key:\n            key = key.replace('.fc1.', '.ffn.fc1.')\n        if '.encoder_attn.' in key:\n            key = key.replace('.encoder_attn.', '.cross_attention.')\n        if 'encoder_attn_layer_norm' in key:\n            key = key.replace('encoder_attn_layer_norm', 'cross_attention_layer_norm')\n        if 'final_layer_norm' in key:\n            key = key.replace('final_layer_norm', 'ff_layer_norm')\n        new_dict[key] = state_dict[old_key]\n    return new_dict",
        "mutated": [
            "def rename_fairseq_keys(state_dict, expert_idx=None):\n    if False:\n        i = 10\n    new_dict = {}\n    for old_key in state_dict.keys():\n        key = old_key\n        if 'moe_layer.experts.' in key:\n            if expert_idx is not None:\n                key = key.replace('moe_layer.experts.0', f'ffn.experts.expert_{expert_idx}')\n            else:\n                key = key.replace('moe_layer.experts.', 'ffn.experts.expert_')\n        if 'gate' in key:\n            key = key.replace('.moe_layer.gate.wg', '.ffn.router.classifier')\n        if 'fc2' and 'experts' not in key:\n            key = key.replace('.fc2.', '.ffn.fc2.')\n        if 'fc1' and 'experts' not in key:\n            key = key.replace('.fc1.', '.ffn.fc1.')\n        if '.encoder_attn.' in key:\n            key = key.replace('.encoder_attn.', '.cross_attention.')\n        if 'encoder_attn_layer_norm' in key:\n            key = key.replace('encoder_attn_layer_norm', 'cross_attention_layer_norm')\n        if 'final_layer_norm' in key:\n            key = key.replace('final_layer_norm', 'ff_layer_norm')\n        new_dict[key] = state_dict[old_key]\n    return new_dict",
            "def rename_fairseq_keys(state_dict, expert_idx=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    new_dict = {}\n    for old_key in state_dict.keys():\n        key = old_key\n        if 'moe_layer.experts.' in key:\n            if expert_idx is not None:\n                key = key.replace('moe_layer.experts.0', f'ffn.experts.expert_{expert_idx}')\n            else:\n                key = key.replace('moe_layer.experts.', 'ffn.experts.expert_')\n        if 'gate' in key:\n            key = key.replace('.moe_layer.gate.wg', '.ffn.router.classifier')\n        if 'fc2' and 'experts' not in key:\n            key = key.replace('.fc2.', '.ffn.fc2.')\n        if 'fc1' and 'experts' not in key:\n            key = key.replace('.fc1.', '.ffn.fc1.')\n        if '.encoder_attn.' in key:\n            key = key.replace('.encoder_attn.', '.cross_attention.')\n        if 'encoder_attn_layer_norm' in key:\n            key = key.replace('encoder_attn_layer_norm', 'cross_attention_layer_norm')\n        if 'final_layer_norm' in key:\n            key = key.replace('final_layer_norm', 'ff_layer_norm')\n        new_dict[key] = state_dict[old_key]\n    return new_dict",
            "def rename_fairseq_keys(state_dict, expert_idx=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    new_dict = {}\n    for old_key in state_dict.keys():\n        key = old_key\n        if 'moe_layer.experts.' in key:\n            if expert_idx is not None:\n                key = key.replace('moe_layer.experts.0', f'ffn.experts.expert_{expert_idx}')\n            else:\n                key = key.replace('moe_layer.experts.', 'ffn.experts.expert_')\n        if 'gate' in key:\n            key = key.replace('.moe_layer.gate.wg', '.ffn.router.classifier')\n        if 'fc2' and 'experts' not in key:\n            key = key.replace('.fc2.', '.ffn.fc2.')\n        if 'fc1' and 'experts' not in key:\n            key = key.replace('.fc1.', '.ffn.fc1.')\n        if '.encoder_attn.' in key:\n            key = key.replace('.encoder_attn.', '.cross_attention.')\n        if 'encoder_attn_layer_norm' in key:\n            key = key.replace('encoder_attn_layer_norm', 'cross_attention_layer_norm')\n        if 'final_layer_norm' in key:\n            key = key.replace('final_layer_norm', 'ff_layer_norm')\n        new_dict[key] = state_dict[old_key]\n    return new_dict",
            "def rename_fairseq_keys(state_dict, expert_idx=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    new_dict = {}\n    for old_key in state_dict.keys():\n        key = old_key\n        if 'moe_layer.experts.' in key:\n            if expert_idx is not None:\n                key = key.replace('moe_layer.experts.0', f'ffn.experts.expert_{expert_idx}')\n            else:\n                key = key.replace('moe_layer.experts.', 'ffn.experts.expert_')\n        if 'gate' in key:\n            key = key.replace('.moe_layer.gate.wg', '.ffn.router.classifier')\n        if 'fc2' and 'experts' not in key:\n            key = key.replace('.fc2.', '.ffn.fc2.')\n        if 'fc1' and 'experts' not in key:\n            key = key.replace('.fc1.', '.ffn.fc1.')\n        if '.encoder_attn.' in key:\n            key = key.replace('.encoder_attn.', '.cross_attention.')\n        if 'encoder_attn_layer_norm' in key:\n            key = key.replace('encoder_attn_layer_norm', 'cross_attention_layer_norm')\n        if 'final_layer_norm' in key:\n            key = key.replace('final_layer_norm', 'ff_layer_norm')\n        new_dict[key] = state_dict[old_key]\n    return new_dict",
            "def rename_fairseq_keys(state_dict, expert_idx=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    new_dict = {}\n    for old_key in state_dict.keys():\n        key = old_key\n        if 'moe_layer.experts.' in key:\n            if expert_idx is not None:\n                key = key.replace('moe_layer.experts.0', f'ffn.experts.expert_{expert_idx}')\n            else:\n                key = key.replace('moe_layer.experts.', 'ffn.experts.expert_')\n        if 'gate' in key:\n            key = key.replace('.moe_layer.gate.wg', '.ffn.router.classifier')\n        if 'fc2' and 'experts' not in key:\n            key = key.replace('.fc2.', '.ffn.fc2.')\n        if 'fc1' and 'experts' not in key:\n            key = key.replace('.fc1.', '.ffn.fc1.')\n        if '.encoder_attn.' in key:\n            key = key.replace('.encoder_attn.', '.cross_attention.')\n        if 'encoder_attn_layer_norm' in key:\n            key = key.replace('encoder_attn_layer_norm', 'cross_attention_layer_norm')\n        if 'final_layer_norm' in key:\n            key = key.replace('final_layer_norm', 'ff_layer_norm')\n        new_dict[key] = state_dict[old_key]\n    return new_dict"
        ]
    },
    {
        "func_name": "shard_on_the_fly",
        "original": "def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weights_name: str=WEIGHTS_NAME):\n    sharded_state_dicts = []\n    total_size = 0\n    os.makedirs(dump_path, exist_ok=True)\n    for expert in range(num_experts):\n        expert_path = switch_checkpoint_path + f'-rank-{expert}.pt'\n        if os.path.isfile(expert_path):\n            expert_state = torch.load(expert_path)['model']\n            remove_ignore_keys_(expert_state)\n            expert_state = rename_fairseq_keys(expert_state, expert)\n            save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n            torch.save(expert_state, save_path)\n            sharded_state_dicts.append(expert_state.keys())\n            total_size += sum([value.numel() for (key, value) in expert_state.items()]) * dtype_byte_size(expert_state[list(expert_state)[0]].dtype)\n    save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n    shared_weights = torch.load(switch_checkpoint_path + '-shared.pt')['model']\n    remove_ignore_keys_(shared_weights)\n    shared_weights = rename_fairseq_keys(shared_weights, None)\n    shared_weights['shared.weight'] = shared_weights['decoder.embed_tokens.weight']\n    sharded_state_dicts.append(shared_weights.keys())\n    if len(sharded_state_dicts) == 1:\n        save_path = os.path.join(dump_path, weights_name)\n        torch.save(shared_weights, save_path)\n        return ({weights_name: sharded_state_dicts[0]}, None)\n    else:\n        torch.save(shared_weights, save_path)\n    weight_map = {}\n    for (idx, shard) in enumerate(sharded_state_dicts):\n        shard_file = weights_name.replace('.bin', f'-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.bin')\n        temp_filename = os.path.join(dump_path, weights_name.replace('.bin', f'-{idx + 1:05d}-of-???.bin'))\n        os.rename(temp_filename, os.path.join(dump_path, shard_file))\n        for key in shard:\n            weight_map[key] = shard_file\n    metadata = {'total_size': total_size}\n    index = {'metadata': metadata, 'weight_map': weight_map}\n    with open(os.path.join(dump_path, WEIGHTS_INDEX_NAME), 'w', encoding='utf-8') as f:\n        content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n        f.write(content)\n    return (metadata, index)",
        "mutated": [
            "def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weights_name: str=WEIGHTS_NAME):\n    if False:\n        i = 10\n    sharded_state_dicts = []\n    total_size = 0\n    os.makedirs(dump_path, exist_ok=True)\n    for expert in range(num_experts):\n        expert_path = switch_checkpoint_path + f'-rank-{expert}.pt'\n        if os.path.isfile(expert_path):\n            expert_state = torch.load(expert_path)['model']\n            remove_ignore_keys_(expert_state)\n            expert_state = rename_fairseq_keys(expert_state, expert)\n            save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n            torch.save(expert_state, save_path)\n            sharded_state_dicts.append(expert_state.keys())\n            total_size += sum([value.numel() for (key, value) in expert_state.items()]) * dtype_byte_size(expert_state[list(expert_state)[0]].dtype)\n    save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n    shared_weights = torch.load(switch_checkpoint_path + '-shared.pt')['model']\n    remove_ignore_keys_(shared_weights)\n    shared_weights = rename_fairseq_keys(shared_weights, None)\n    shared_weights['shared.weight'] = shared_weights['decoder.embed_tokens.weight']\n    sharded_state_dicts.append(shared_weights.keys())\n    if len(sharded_state_dicts) == 1:\n        save_path = os.path.join(dump_path, weights_name)\n        torch.save(shared_weights, save_path)\n        return ({weights_name: sharded_state_dicts[0]}, None)\n    else:\n        torch.save(shared_weights, save_path)\n    weight_map = {}\n    for (idx, shard) in enumerate(sharded_state_dicts):\n        shard_file = weights_name.replace('.bin', f'-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.bin')\n        temp_filename = os.path.join(dump_path, weights_name.replace('.bin', f'-{idx + 1:05d}-of-???.bin'))\n        os.rename(temp_filename, os.path.join(dump_path, shard_file))\n        for key in shard:\n            weight_map[key] = shard_file\n    metadata = {'total_size': total_size}\n    index = {'metadata': metadata, 'weight_map': weight_map}\n    with open(os.path.join(dump_path, WEIGHTS_INDEX_NAME), 'w', encoding='utf-8') as f:\n        content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n        f.write(content)\n    return (metadata, index)",
            "def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weights_name: str=WEIGHTS_NAME):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    sharded_state_dicts = []\n    total_size = 0\n    os.makedirs(dump_path, exist_ok=True)\n    for expert in range(num_experts):\n        expert_path = switch_checkpoint_path + f'-rank-{expert}.pt'\n        if os.path.isfile(expert_path):\n            expert_state = torch.load(expert_path)['model']\n            remove_ignore_keys_(expert_state)\n            expert_state = rename_fairseq_keys(expert_state, expert)\n            save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n            torch.save(expert_state, save_path)\n            sharded_state_dicts.append(expert_state.keys())\n            total_size += sum([value.numel() for (key, value) in expert_state.items()]) * dtype_byte_size(expert_state[list(expert_state)[0]].dtype)\n    save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n    shared_weights = torch.load(switch_checkpoint_path + '-shared.pt')['model']\n    remove_ignore_keys_(shared_weights)\n    shared_weights = rename_fairseq_keys(shared_weights, None)\n    shared_weights['shared.weight'] = shared_weights['decoder.embed_tokens.weight']\n    sharded_state_dicts.append(shared_weights.keys())\n    if len(sharded_state_dicts) == 1:\n        save_path = os.path.join(dump_path, weights_name)\n        torch.save(shared_weights, save_path)\n        return ({weights_name: sharded_state_dicts[0]}, None)\n    else:\n        torch.save(shared_weights, save_path)\n    weight_map = {}\n    for (idx, shard) in enumerate(sharded_state_dicts):\n        shard_file = weights_name.replace('.bin', f'-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.bin')\n        temp_filename = os.path.join(dump_path, weights_name.replace('.bin', f'-{idx + 1:05d}-of-???.bin'))\n        os.rename(temp_filename, os.path.join(dump_path, shard_file))\n        for key in shard:\n            weight_map[key] = shard_file\n    metadata = {'total_size': total_size}\n    index = {'metadata': metadata, 'weight_map': weight_map}\n    with open(os.path.join(dump_path, WEIGHTS_INDEX_NAME), 'w', encoding='utf-8') as f:\n        content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n        f.write(content)\n    return (metadata, index)",
            "def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weights_name: str=WEIGHTS_NAME):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    sharded_state_dicts = []\n    total_size = 0\n    os.makedirs(dump_path, exist_ok=True)\n    for expert in range(num_experts):\n        expert_path = switch_checkpoint_path + f'-rank-{expert}.pt'\n        if os.path.isfile(expert_path):\n            expert_state = torch.load(expert_path)['model']\n            remove_ignore_keys_(expert_state)\n            expert_state = rename_fairseq_keys(expert_state, expert)\n            save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n            torch.save(expert_state, save_path)\n            sharded_state_dicts.append(expert_state.keys())\n            total_size += sum([value.numel() for (key, value) in expert_state.items()]) * dtype_byte_size(expert_state[list(expert_state)[0]].dtype)\n    save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n    shared_weights = torch.load(switch_checkpoint_path + '-shared.pt')['model']\n    remove_ignore_keys_(shared_weights)\n    shared_weights = rename_fairseq_keys(shared_weights, None)\n    shared_weights['shared.weight'] = shared_weights['decoder.embed_tokens.weight']\n    sharded_state_dicts.append(shared_weights.keys())\n    if len(sharded_state_dicts) == 1:\n        save_path = os.path.join(dump_path, weights_name)\n        torch.save(shared_weights, save_path)\n        return ({weights_name: sharded_state_dicts[0]}, None)\n    else:\n        torch.save(shared_weights, save_path)\n    weight_map = {}\n    for (idx, shard) in enumerate(sharded_state_dicts):\n        shard_file = weights_name.replace('.bin', f'-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.bin')\n        temp_filename = os.path.join(dump_path, weights_name.replace('.bin', f'-{idx + 1:05d}-of-???.bin'))\n        os.rename(temp_filename, os.path.join(dump_path, shard_file))\n        for key in shard:\n            weight_map[key] = shard_file\n    metadata = {'total_size': total_size}\n    index = {'metadata': metadata, 'weight_map': weight_map}\n    with open(os.path.join(dump_path, WEIGHTS_INDEX_NAME), 'w', encoding='utf-8') as f:\n        content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n        f.write(content)\n    return (metadata, index)",
            "def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weights_name: str=WEIGHTS_NAME):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    sharded_state_dicts = []\n    total_size = 0\n    os.makedirs(dump_path, exist_ok=True)\n    for expert in range(num_experts):\n        expert_path = switch_checkpoint_path + f'-rank-{expert}.pt'\n        if os.path.isfile(expert_path):\n            expert_state = torch.load(expert_path)['model']\n            remove_ignore_keys_(expert_state)\n            expert_state = rename_fairseq_keys(expert_state, expert)\n            save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n            torch.save(expert_state, save_path)\n            sharded_state_dicts.append(expert_state.keys())\n            total_size += sum([value.numel() for (key, value) in expert_state.items()]) * dtype_byte_size(expert_state[list(expert_state)[0]].dtype)\n    save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n    shared_weights = torch.load(switch_checkpoint_path + '-shared.pt')['model']\n    remove_ignore_keys_(shared_weights)\n    shared_weights = rename_fairseq_keys(shared_weights, None)\n    shared_weights['shared.weight'] = shared_weights['decoder.embed_tokens.weight']\n    sharded_state_dicts.append(shared_weights.keys())\n    if len(sharded_state_dicts) == 1:\n        save_path = os.path.join(dump_path, weights_name)\n        torch.save(shared_weights, save_path)\n        return ({weights_name: sharded_state_dicts[0]}, None)\n    else:\n        torch.save(shared_weights, save_path)\n    weight_map = {}\n    for (idx, shard) in enumerate(sharded_state_dicts):\n        shard_file = weights_name.replace('.bin', f'-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.bin')\n        temp_filename = os.path.join(dump_path, weights_name.replace('.bin', f'-{idx + 1:05d}-of-???.bin'))\n        os.rename(temp_filename, os.path.join(dump_path, shard_file))\n        for key in shard:\n            weight_map[key] = shard_file\n    metadata = {'total_size': total_size}\n    index = {'metadata': metadata, 'weight_map': weight_map}\n    with open(os.path.join(dump_path, WEIGHTS_INDEX_NAME), 'w', encoding='utf-8') as f:\n        content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n        f.write(content)\n    return (metadata, index)",
            "def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weights_name: str=WEIGHTS_NAME):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    sharded_state_dicts = []\n    total_size = 0\n    os.makedirs(dump_path, exist_ok=True)\n    for expert in range(num_experts):\n        expert_path = switch_checkpoint_path + f'-rank-{expert}.pt'\n        if os.path.isfile(expert_path):\n            expert_state = torch.load(expert_path)['model']\n            remove_ignore_keys_(expert_state)\n            expert_state = rename_fairseq_keys(expert_state, expert)\n            save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n            torch.save(expert_state, save_path)\n            sharded_state_dicts.append(expert_state.keys())\n            total_size += sum([value.numel() for (key, value) in expert_state.items()]) * dtype_byte_size(expert_state[list(expert_state)[0]].dtype)\n    save_path = os.path.join(dump_path, weights_name.replace('.bin', f'-{len(sharded_state_dicts) + 1:05d}-of-???.bin'))\n    shared_weights = torch.load(switch_checkpoint_path + '-shared.pt')['model']\n    remove_ignore_keys_(shared_weights)\n    shared_weights = rename_fairseq_keys(shared_weights, None)\n    shared_weights['shared.weight'] = shared_weights['decoder.embed_tokens.weight']\n    sharded_state_dicts.append(shared_weights.keys())\n    if len(sharded_state_dicts) == 1:\n        save_path = os.path.join(dump_path, weights_name)\n        torch.save(shared_weights, save_path)\n        return ({weights_name: sharded_state_dicts[0]}, None)\n    else:\n        torch.save(shared_weights, save_path)\n    weight_map = {}\n    for (idx, shard) in enumerate(sharded_state_dicts):\n        shard_file = weights_name.replace('.bin', f'-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.bin')\n        temp_filename = os.path.join(dump_path, weights_name.replace('.bin', f'-{idx + 1:05d}-of-???.bin'))\n        os.rename(temp_filename, os.path.join(dump_path, shard_file))\n        for key in shard:\n            weight_map[key] = shard_file\n    metadata = {'total_size': total_size}\n    index = {'metadata': metadata, 'weight_map': weight_map}\n    with open(os.path.join(dump_path, WEIGHTS_INDEX_NAME), 'w', encoding='utf-8') as f:\n        content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n        f.write(content)\n    return (metadata, index)"
        ]
    }
]