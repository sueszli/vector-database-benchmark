[
    {
        "func_name": "check_finite_and_unscale",
        "original": "def check_finite_and_unscale(x, scale, name=None, float_status=None):\n    \"\"\"\n    Check if input X contains all finite data, if yes, scale it by input Scale.\n\n    $$Out = X / scale$$\n\n    If any tensor in X contains Inf or Nan, the Out will generate a indicator.\n    FoundInfinite will be 1 (True), and Out will not be scaled. In this case, the data of\n    Out should not be used, and its data may not be deterministic.\n    Otherwise, FoundInfinite will be 0 (False).\n\n    Args:\n        x(list|tuple): The input tensors of check_finite_and_unscale operator.\n        scale: The scale of check_finite_and_unscale operator.\n        float_status(Tensor): (Only used on NPU) The float status to check overflow.\n    \"\"\"\n    helper = LayerHelper('check_finite_and_unscale', **locals())\n    found_inf = helper.create_variable_for_type_inference(dtype='bool')\n    if in_dygraph_mode():\n        (x, found_inf) = _C_ops.check_finite_and_unscale_(x, scale)\n        return (x, found_inf)\n    check_type(x, 'x', (tuple, list), 'check_finite_and_unscale')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'check_finite_and_unscale')\n    inputs = {'X': x, 'Scale': scale}\n    outputs = {'Out': x, 'FoundInfinite': found_inf}\n    helper.append_op(type='check_finite_and_unscale', inputs=inputs, outputs=outputs)\n    return (x, found_inf)",
        "mutated": [
            "def check_finite_and_unscale(x, scale, name=None, float_status=None):\n    if False:\n        i = 10\n    '\\n    Check if input X contains all finite data, if yes, scale it by input Scale.\\n\\n    $$Out = X / scale$$\\n\\n    If any tensor in X contains Inf or Nan, the Out will generate a indicator.\\n    FoundInfinite will be 1 (True), and Out will not be scaled. In this case, the data of\\n    Out should not be used, and its data may not be deterministic.\\n    Otherwise, FoundInfinite will be 0 (False).\\n\\n    Args:\\n        x(list|tuple): The input tensors of check_finite_and_unscale operator.\\n        scale: The scale of check_finite_and_unscale operator.\\n        float_status(Tensor): (Only used on NPU) The float status to check overflow.\\n    '\n    helper = LayerHelper('check_finite_and_unscale', **locals())\n    found_inf = helper.create_variable_for_type_inference(dtype='bool')\n    if in_dygraph_mode():\n        (x, found_inf) = _C_ops.check_finite_and_unscale_(x, scale)\n        return (x, found_inf)\n    check_type(x, 'x', (tuple, list), 'check_finite_and_unscale')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'check_finite_and_unscale')\n    inputs = {'X': x, 'Scale': scale}\n    outputs = {'Out': x, 'FoundInfinite': found_inf}\n    helper.append_op(type='check_finite_and_unscale', inputs=inputs, outputs=outputs)\n    return (x, found_inf)",
            "def check_finite_and_unscale(x, scale, name=None, float_status=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Check if input X contains all finite data, if yes, scale it by input Scale.\\n\\n    $$Out = X / scale$$\\n\\n    If any tensor in X contains Inf or Nan, the Out will generate a indicator.\\n    FoundInfinite will be 1 (True), and Out will not be scaled. In this case, the data of\\n    Out should not be used, and its data may not be deterministic.\\n    Otherwise, FoundInfinite will be 0 (False).\\n\\n    Args:\\n        x(list|tuple): The input tensors of check_finite_and_unscale operator.\\n        scale: The scale of check_finite_and_unscale operator.\\n        float_status(Tensor): (Only used on NPU) The float status to check overflow.\\n    '\n    helper = LayerHelper('check_finite_and_unscale', **locals())\n    found_inf = helper.create_variable_for_type_inference(dtype='bool')\n    if in_dygraph_mode():\n        (x, found_inf) = _C_ops.check_finite_and_unscale_(x, scale)\n        return (x, found_inf)\n    check_type(x, 'x', (tuple, list), 'check_finite_and_unscale')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'check_finite_and_unscale')\n    inputs = {'X': x, 'Scale': scale}\n    outputs = {'Out': x, 'FoundInfinite': found_inf}\n    helper.append_op(type='check_finite_and_unscale', inputs=inputs, outputs=outputs)\n    return (x, found_inf)",
            "def check_finite_and_unscale(x, scale, name=None, float_status=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Check if input X contains all finite data, if yes, scale it by input Scale.\\n\\n    $$Out = X / scale$$\\n\\n    If any tensor in X contains Inf or Nan, the Out will generate a indicator.\\n    FoundInfinite will be 1 (True), and Out will not be scaled. In this case, the data of\\n    Out should not be used, and its data may not be deterministic.\\n    Otherwise, FoundInfinite will be 0 (False).\\n\\n    Args:\\n        x(list|tuple): The input tensors of check_finite_and_unscale operator.\\n        scale: The scale of check_finite_and_unscale operator.\\n        float_status(Tensor): (Only used on NPU) The float status to check overflow.\\n    '\n    helper = LayerHelper('check_finite_and_unscale', **locals())\n    found_inf = helper.create_variable_for_type_inference(dtype='bool')\n    if in_dygraph_mode():\n        (x, found_inf) = _C_ops.check_finite_and_unscale_(x, scale)\n        return (x, found_inf)\n    check_type(x, 'x', (tuple, list), 'check_finite_and_unscale')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'check_finite_and_unscale')\n    inputs = {'X': x, 'Scale': scale}\n    outputs = {'Out': x, 'FoundInfinite': found_inf}\n    helper.append_op(type='check_finite_and_unscale', inputs=inputs, outputs=outputs)\n    return (x, found_inf)",
            "def check_finite_and_unscale(x, scale, name=None, float_status=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Check if input X contains all finite data, if yes, scale it by input Scale.\\n\\n    $$Out = X / scale$$\\n\\n    If any tensor in X contains Inf or Nan, the Out will generate a indicator.\\n    FoundInfinite will be 1 (True), and Out will not be scaled. In this case, the data of\\n    Out should not be used, and its data may not be deterministic.\\n    Otherwise, FoundInfinite will be 0 (False).\\n\\n    Args:\\n        x(list|tuple): The input tensors of check_finite_and_unscale operator.\\n        scale: The scale of check_finite_and_unscale operator.\\n        float_status(Tensor): (Only used on NPU) The float status to check overflow.\\n    '\n    helper = LayerHelper('check_finite_and_unscale', **locals())\n    found_inf = helper.create_variable_for_type_inference(dtype='bool')\n    if in_dygraph_mode():\n        (x, found_inf) = _C_ops.check_finite_and_unscale_(x, scale)\n        return (x, found_inf)\n    check_type(x, 'x', (tuple, list), 'check_finite_and_unscale')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'check_finite_and_unscale')\n    inputs = {'X': x, 'Scale': scale}\n    outputs = {'Out': x, 'FoundInfinite': found_inf}\n    helper.append_op(type='check_finite_and_unscale', inputs=inputs, outputs=outputs)\n    return (x, found_inf)",
            "def check_finite_and_unscale(x, scale, name=None, float_status=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Check if input X contains all finite data, if yes, scale it by input Scale.\\n\\n    $$Out = X / scale$$\\n\\n    If any tensor in X contains Inf or Nan, the Out will generate a indicator.\\n    FoundInfinite will be 1 (True), and Out will not be scaled. In this case, the data of\\n    Out should not be used, and its data may not be deterministic.\\n    Otherwise, FoundInfinite will be 0 (False).\\n\\n    Args:\\n        x(list|tuple): The input tensors of check_finite_and_unscale operator.\\n        scale: The scale of check_finite_and_unscale operator.\\n        float_status(Tensor): (Only used on NPU) The float status to check overflow.\\n    '\n    helper = LayerHelper('check_finite_and_unscale', **locals())\n    found_inf = helper.create_variable_for_type_inference(dtype='bool')\n    if in_dygraph_mode():\n        (x, found_inf) = _C_ops.check_finite_and_unscale_(x, scale)\n        return (x, found_inf)\n    check_type(x, 'x', (tuple, list), 'check_finite_and_unscale')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'check_finite_and_unscale')\n    inputs = {'X': x, 'Scale': scale}\n    outputs = {'Out': x, 'FoundInfinite': found_inf}\n    helper.append_op(type='check_finite_and_unscale', inputs=inputs, outputs=outputs)\n    return (x, found_inf)"
        ]
    },
    {
        "func_name": "update_loss_scaling",
        "original": "def update_loss_scaling(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update=False, name=None):\n    \"\"\"\n    Update loss scaling according to overall gradients. If all gradients is\n    finite after incr_every_n_steps, loss scaling will increase by incr_ratio.\n    Otherwise, loss scaling will decrease by decr_ratio after\n    decr_every_n_nan_or_inf steps and each step some gradients are infinite.\n\n    Args:\n        x(list|tuple): The input tensors of update_loss_scaling operator.\n        found_inf (Variable): A boolean variable indicates whether\n                                     there is any infinite gradient.\n        prev_loss_scaling (Variable): Previous loss scaling.\n        num_good_steps (Variable): A variable accumulates good steps in which\n                                   all gradients are finite.\n        num_bad_steps (Variable): A variable accumulates bad steps in which\n                                  some gradients are infinite.\n        incr_every_n_steps (int): A variable represents increasing loss\n                                       scaling every n consecutive steps with\n                                       finite gradients.\n        decr_every_n_nan_or_inf (int): A variable represents decreasing\n                                            loss scaling every n accumulated\n                                            steps with nan or inf gradients.\n        incr_ratio(float): The multiplier to use when increasing the loss\n                           scaling.\n        decr_ratio(float): The less-than-one-multiplier to use when decreasing\n                           loss scaling.\n    \"\"\"\n    if in_dygraph_mode():\n        _C_ops.update_loss_scaling_(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update)\n        return x\n    check_variable_and_dtype(prev_loss_scaling, 'prev_loss_scaling', ['float32', 'float64'], 'update_loss_scaling')\n    check_type(x, 'x', (tuple, list), 'update_loss_scaling')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'update_loss_scaling')\n        if e.dtype in [core.VarDesc.VarType.FP16, core.VarDesc.VarType.BF16]:\n            assert prev_loss_scaling.dtype == core.VarDesc.VarType.FP32, 'The dtype of prev_loss_scaling should be float32 when the dtype of x is float16 or bfloat16.'\n        else:\n            assert prev_loss_scaling.dtype == e.dtype, 'The dtype of prev_loss_scaling should be equal to the dtype of x.'\n    helper = LayerHelper('update_loss_scaling', **locals())\n    inputs = {'X': x, 'FoundInfinite': found_inf, 'PrevLossScaling': prev_loss_scaling, 'InGoodSteps': num_good_steps, 'InBadSteps': num_bad_steps}\n    outputs = {'Out': x, 'LossScaling': prev_loss_scaling, 'OutGoodSteps': num_good_steps, 'OutBadSteps': num_bad_steps}\n    attrs = {'incr_every_n_steps': incr_every_n_steps, 'decr_every_n_nan_or_inf': decr_every_n_nan_or_inf, 'incr_ratio': incr_ratio, 'decr_ratio': decr_ratio}\n    if isinstance(stop_update, Variable):\n        inputs['StopUpdate'] = stop_update\n    else:\n        attrs['stop_update'] = stop_update\n    helper.append_op(type='update_loss_scaling', inputs=inputs, outputs=outputs, attrs=attrs)\n    return x",
        "mutated": [
            "def update_loss_scaling(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update=False, name=None):\n    if False:\n        i = 10\n    '\\n    Update loss scaling according to overall gradients. If all gradients is\\n    finite after incr_every_n_steps, loss scaling will increase by incr_ratio.\\n    Otherwise, loss scaling will decrease by decr_ratio after\\n    decr_every_n_nan_or_inf steps and each step some gradients are infinite.\\n\\n    Args:\\n        x(list|tuple): The input tensors of update_loss_scaling operator.\\n        found_inf (Variable): A boolean variable indicates whether\\n                                     there is any infinite gradient.\\n        prev_loss_scaling (Variable): Previous loss scaling.\\n        num_good_steps (Variable): A variable accumulates good steps in which\\n                                   all gradients are finite.\\n        num_bad_steps (Variable): A variable accumulates bad steps in which\\n                                  some gradients are infinite.\\n        incr_every_n_steps (int): A variable represents increasing loss\\n                                       scaling every n consecutive steps with\\n                                       finite gradients.\\n        decr_every_n_nan_or_inf (int): A variable represents decreasing\\n                                            loss scaling every n accumulated\\n                                            steps with nan or inf gradients.\\n        incr_ratio(float): The multiplier to use when increasing the loss\\n                           scaling.\\n        decr_ratio(float): The less-than-one-multiplier to use when decreasing\\n                           loss scaling.\\n    '\n    if in_dygraph_mode():\n        _C_ops.update_loss_scaling_(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update)\n        return x\n    check_variable_and_dtype(prev_loss_scaling, 'prev_loss_scaling', ['float32', 'float64'], 'update_loss_scaling')\n    check_type(x, 'x', (tuple, list), 'update_loss_scaling')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'update_loss_scaling')\n        if e.dtype in [core.VarDesc.VarType.FP16, core.VarDesc.VarType.BF16]:\n            assert prev_loss_scaling.dtype == core.VarDesc.VarType.FP32, 'The dtype of prev_loss_scaling should be float32 when the dtype of x is float16 or bfloat16.'\n        else:\n            assert prev_loss_scaling.dtype == e.dtype, 'The dtype of prev_loss_scaling should be equal to the dtype of x.'\n    helper = LayerHelper('update_loss_scaling', **locals())\n    inputs = {'X': x, 'FoundInfinite': found_inf, 'PrevLossScaling': prev_loss_scaling, 'InGoodSteps': num_good_steps, 'InBadSteps': num_bad_steps}\n    outputs = {'Out': x, 'LossScaling': prev_loss_scaling, 'OutGoodSteps': num_good_steps, 'OutBadSteps': num_bad_steps}\n    attrs = {'incr_every_n_steps': incr_every_n_steps, 'decr_every_n_nan_or_inf': decr_every_n_nan_or_inf, 'incr_ratio': incr_ratio, 'decr_ratio': decr_ratio}\n    if isinstance(stop_update, Variable):\n        inputs['StopUpdate'] = stop_update\n    else:\n        attrs['stop_update'] = stop_update\n    helper.append_op(type='update_loss_scaling', inputs=inputs, outputs=outputs, attrs=attrs)\n    return x",
            "def update_loss_scaling(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Update loss scaling according to overall gradients. If all gradients is\\n    finite after incr_every_n_steps, loss scaling will increase by incr_ratio.\\n    Otherwise, loss scaling will decrease by decr_ratio after\\n    decr_every_n_nan_or_inf steps and each step some gradients are infinite.\\n\\n    Args:\\n        x(list|tuple): The input tensors of update_loss_scaling operator.\\n        found_inf (Variable): A boolean variable indicates whether\\n                                     there is any infinite gradient.\\n        prev_loss_scaling (Variable): Previous loss scaling.\\n        num_good_steps (Variable): A variable accumulates good steps in which\\n                                   all gradients are finite.\\n        num_bad_steps (Variable): A variable accumulates bad steps in which\\n                                  some gradients are infinite.\\n        incr_every_n_steps (int): A variable represents increasing loss\\n                                       scaling every n consecutive steps with\\n                                       finite gradients.\\n        decr_every_n_nan_or_inf (int): A variable represents decreasing\\n                                            loss scaling every n accumulated\\n                                            steps with nan or inf gradients.\\n        incr_ratio(float): The multiplier to use when increasing the loss\\n                           scaling.\\n        decr_ratio(float): The less-than-one-multiplier to use when decreasing\\n                           loss scaling.\\n    '\n    if in_dygraph_mode():\n        _C_ops.update_loss_scaling_(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update)\n        return x\n    check_variable_and_dtype(prev_loss_scaling, 'prev_loss_scaling', ['float32', 'float64'], 'update_loss_scaling')\n    check_type(x, 'x', (tuple, list), 'update_loss_scaling')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'update_loss_scaling')\n        if e.dtype in [core.VarDesc.VarType.FP16, core.VarDesc.VarType.BF16]:\n            assert prev_loss_scaling.dtype == core.VarDesc.VarType.FP32, 'The dtype of prev_loss_scaling should be float32 when the dtype of x is float16 or bfloat16.'\n        else:\n            assert prev_loss_scaling.dtype == e.dtype, 'The dtype of prev_loss_scaling should be equal to the dtype of x.'\n    helper = LayerHelper('update_loss_scaling', **locals())\n    inputs = {'X': x, 'FoundInfinite': found_inf, 'PrevLossScaling': prev_loss_scaling, 'InGoodSteps': num_good_steps, 'InBadSteps': num_bad_steps}\n    outputs = {'Out': x, 'LossScaling': prev_loss_scaling, 'OutGoodSteps': num_good_steps, 'OutBadSteps': num_bad_steps}\n    attrs = {'incr_every_n_steps': incr_every_n_steps, 'decr_every_n_nan_or_inf': decr_every_n_nan_or_inf, 'incr_ratio': incr_ratio, 'decr_ratio': decr_ratio}\n    if isinstance(stop_update, Variable):\n        inputs['StopUpdate'] = stop_update\n    else:\n        attrs['stop_update'] = stop_update\n    helper.append_op(type='update_loss_scaling', inputs=inputs, outputs=outputs, attrs=attrs)\n    return x",
            "def update_loss_scaling(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Update loss scaling according to overall gradients. If all gradients is\\n    finite after incr_every_n_steps, loss scaling will increase by incr_ratio.\\n    Otherwise, loss scaling will decrease by decr_ratio after\\n    decr_every_n_nan_or_inf steps and each step some gradients are infinite.\\n\\n    Args:\\n        x(list|tuple): The input tensors of update_loss_scaling operator.\\n        found_inf (Variable): A boolean variable indicates whether\\n                                     there is any infinite gradient.\\n        prev_loss_scaling (Variable): Previous loss scaling.\\n        num_good_steps (Variable): A variable accumulates good steps in which\\n                                   all gradients are finite.\\n        num_bad_steps (Variable): A variable accumulates bad steps in which\\n                                  some gradients are infinite.\\n        incr_every_n_steps (int): A variable represents increasing loss\\n                                       scaling every n consecutive steps with\\n                                       finite gradients.\\n        decr_every_n_nan_or_inf (int): A variable represents decreasing\\n                                            loss scaling every n accumulated\\n                                            steps with nan or inf gradients.\\n        incr_ratio(float): The multiplier to use when increasing the loss\\n                           scaling.\\n        decr_ratio(float): The less-than-one-multiplier to use when decreasing\\n                           loss scaling.\\n    '\n    if in_dygraph_mode():\n        _C_ops.update_loss_scaling_(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update)\n        return x\n    check_variable_and_dtype(prev_loss_scaling, 'prev_loss_scaling', ['float32', 'float64'], 'update_loss_scaling')\n    check_type(x, 'x', (tuple, list), 'update_loss_scaling')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'update_loss_scaling')\n        if e.dtype in [core.VarDesc.VarType.FP16, core.VarDesc.VarType.BF16]:\n            assert prev_loss_scaling.dtype == core.VarDesc.VarType.FP32, 'The dtype of prev_loss_scaling should be float32 when the dtype of x is float16 or bfloat16.'\n        else:\n            assert prev_loss_scaling.dtype == e.dtype, 'The dtype of prev_loss_scaling should be equal to the dtype of x.'\n    helper = LayerHelper('update_loss_scaling', **locals())\n    inputs = {'X': x, 'FoundInfinite': found_inf, 'PrevLossScaling': prev_loss_scaling, 'InGoodSteps': num_good_steps, 'InBadSteps': num_bad_steps}\n    outputs = {'Out': x, 'LossScaling': prev_loss_scaling, 'OutGoodSteps': num_good_steps, 'OutBadSteps': num_bad_steps}\n    attrs = {'incr_every_n_steps': incr_every_n_steps, 'decr_every_n_nan_or_inf': decr_every_n_nan_or_inf, 'incr_ratio': incr_ratio, 'decr_ratio': decr_ratio}\n    if isinstance(stop_update, Variable):\n        inputs['StopUpdate'] = stop_update\n    else:\n        attrs['stop_update'] = stop_update\n    helper.append_op(type='update_loss_scaling', inputs=inputs, outputs=outputs, attrs=attrs)\n    return x",
            "def update_loss_scaling(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Update loss scaling according to overall gradients. If all gradients is\\n    finite after incr_every_n_steps, loss scaling will increase by incr_ratio.\\n    Otherwise, loss scaling will decrease by decr_ratio after\\n    decr_every_n_nan_or_inf steps and each step some gradients are infinite.\\n\\n    Args:\\n        x(list|tuple): The input tensors of update_loss_scaling operator.\\n        found_inf (Variable): A boolean variable indicates whether\\n                                     there is any infinite gradient.\\n        prev_loss_scaling (Variable): Previous loss scaling.\\n        num_good_steps (Variable): A variable accumulates good steps in which\\n                                   all gradients are finite.\\n        num_bad_steps (Variable): A variable accumulates bad steps in which\\n                                  some gradients are infinite.\\n        incr_every_n_steps (int): A variable represents increasing loss\\n                                       scaling every n consecutive steps with\\n                                       finite gradients.\\n        decr_every_n_nan_or_inf (int): A variable represents decreasing\\n                                            loss scaling every n accumulated\\n                                            steps with nan or inf gradients.\\n        incr_ratio(float): The multiplier to use when increasing the loss\\n                           scaling.\\n        decr_ratio(float): The less-than-one-multiplier to use when decreasing\\n                           loss scaling.\\n    '\n    if in_dygraph_mode():\n        _C_ops.update_loss_scaling_(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update)\n        return x\n    check_variable_and_dtype(prev_loss_scaling, 'prev_loss_scaling', ['float32', 'float64'], 'update_loss_scaling')\n    check_type(x, 'x', (tuple, list), 'update_loss_scaling')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'update_loss_scaling')\n        if e.dtype in [core.VarDesc.VarType.FP16, core.VarDesc.VarType.BF16]:\n            assert prev_loss_scaling.dtype == core.VarDesc.VarType.FP32, 'The dtype of prev_loss_scaling should be float32 when the dtype of x is float16 or bfloat16.'\n        else:\n            assert prev_loss_scaling.dtype == e.dtype, 'The dtype of prev_loss_scaling should be equal to the dtype of x.'\n    helper = LayerHelper('update_loss_scaling', **locals())\n    inputs = {'X': x, 'FoundInfinite': found_inf, 'PrevLossScaling': prev_loss_scaling, 'InGoodSteps': num_good_steps, 'InBadSteps': num_bad_steps}\n    outputs = {'Out': x, 'LossScaling': prev_loss_scaling, 'OutGoodSteps': num_good_steps, 'OutBadSteps': num_bad_steps}\n    attrs = {'incr_every_n_steps': incr_every_n_steps, 'decr_every_n_nan_or_inf': decr_every_n_nan_or_inf, 'incr_ratio': incr_ratio, 'decr_ratio': decr_ratio}\n    if isinstance(stop_update, Variable):\n        inputs['StopUpdate'] = stop_update\n    else:\n        attrs['stop_update'] = stop_update\n    helper.append_op(type='update_loss_scaling', inputs=inputs, outputs=outputs, attrs=attrs)\n    return x",
            "def update_loss_scaling(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Update loss scaling according to overall gradients. If all gradients is\\n    finite after incr_every_n_steps, loss scaling will increase by incr_ratio.\\n    Otherwise, loss scaling will decrease by decr_ratio after\\n    decr_every_n_nan_or_inf steps and each step some gradients are infinite.\\n\\n    Args:\\n        x(list|tuple): The input tensors of update_loss_scaling operator.\\n        found_inf (Variable): A boolean variable indicates whether\\n                                     there is any infinite gradient.\\n        prev_loss_scaling (Variable): Previous loss scaling.\\n        num_good_steps (Variable): A variable accumulates good steps in which\\n                                   all gradients are finite.\\n        num_bad_steps (Variable): A variable accumulates bad steps in which\\n                                  some gradients are infinite.\\n        incr_every_n_steps (int): A variable represents increasing loss\\n                                       scaling every n consecutive steps with\\n                                       finite gradients.\\n        decr_every_n_nan_or_inf (int): A variable represents decreasing\\n                                            loss scaling every n accumulated\\n                                            steps with nan or inf gradients.\\n        incr_ratio(float): The multiplier to use when increasing the loss\\n                           scaling.\\n        decr_ratio(float): The less-than-one-multiplier to use when decreasing\\n                           loss scaling.\\n    '\n    if in_dygraph_mode():\n        _C_ops.update_loss_scaling_(x, found_inf, prev_loss_scaling, num_good_steps, num_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, stop_update)\n        return x\n    check_variable_and_dtype(prev_loss_scaling, 'prev_loss_scaling', ['float32', 'float64'], 'update_loss_scaling')\n    check_type(x, 'x', (tuple, list), 'update_loss_scaling')\n    for e in x:\n        check_variable_and_dtype(e, 'x', ['float16', 'float32', 'float64', 'uint16'], 'update_loss_scaling')\n        if e.dtype in [core.VarDesc.VarType.FP16, core.VarDesc.VarType.BF16]:\n            assert prev_loss_scaling.dtype == core.VarDesc.VarType.FP32, 'The dtype of prev_loss_scaling should be float32 when the dtype of x is float16 or bfloat16.'\n        else:\n            assert prev_loss_scaling.dtype == e.dtype, 'The dtype of prev_loss_scaling should be equal to the dtype of x.'\n    helper = LayerHelper('update_loss_scaling', **locals())\n    inputs = {'X': x, 'FoundInfinite': found_inf, 'PrevLossScaling': prev_loss_scaling, 'InGoodSteps': num_good_steps, 'InBadSteps': num_bad_steps}\n    outputs = {'Out': x, 'LossScaling': prev_loss_scaling, 'OutGoodSteps': num_good_steps, 'OutBadSteps': num_bad_steps}\n    attrs = {'incr_every_n_steps': incr_every_n_steps, 'decr_every_n_nan_or_inf': decr_every_n_nan_or_inf, 'incr_ratio': incr_ratio, 'decr_ratio': decr_ratio}\n    if isinstance(stop_update, Variable):\n        inputs['StopUpdate'] = stop_update\n    else:\n        attrs['stop_update'] = stop_update\n    helper.append_op(type='update_loss_scaling', inputs=inputs, outputs=outputs, attrs=attrs)\n    return x"
        ]
    }
]