[
    {
        "func_name": "__init__",
        "original": "def __init__(self, input_dim, output_dim, lorder=None, rorder=None, hidden_size=None, layer_norm=False, dropout=0):\n    super(DeepFsmn, self).__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    if lorder is None:\n        return\n    self.lorder = lorder\n    self.rorder = rorder\n    self.hidden_size = hidden_size\n    self.layer_norm = layer_norm\n    self.linear = nn.Linear(input_dim, hidden_size)\n    self.norm = nn.LayerNorm(hidden_size)\n    self.drop1 = nn.Dropout(p=dropout)\n    self.drop2 = nn.Dropout(p=dropout)\n    self.project = nn.Linear(hidden_size, output_dim, bias=False)\n    self.conv1 = nn.Conv2d(output_dim, output_dim, [lorder, 1], [1, 1], groups=output_dim, bias=False)\n    self.conv2 = nn.Conv2d(output_dim, output_dim, [rorder, 1], [1, 1], groups=output_dim, bias=False)",
        "mutated": [
            "def __init__(self, input_dim, output_dim, lorder=None, rorder=None, hidden_size=None, layer_norm=False, dropout=0):\n    if False:\n        i = 10\n    super(DeepFsmn, self).__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    if lorder is None:\n        return\n    self.lorder = lorder\n    self.rorder = rorder\n    self.hidden_size = hidden_size\n    self.layer_norm = layer_norm\n    self.linear = nn.Linear(input_dim, hidden_size)\n    self.norm = nn.LayerNorm(hidden_size)\n    self.drop1 = nn.Dropout(p=dropout)\n    self.drop2 = nn.Dropout(p=dropout)\n    self.project = nn.Linear(hidden_size, output_dim, bias=False)\n    self.conv1 = nn.Conv2d(output_dim, output_dim, [lorder, 1], [1, 1], groups=output_dim, bias=False)\n    self.conv2 = nn.Conv2d(output_dim, output_dim, [rorder, 1], [1, 1], groups=output_dim, bias=False)",
            "def __init__(self, input_dim, output_dim, lorder=None, rorder=None, hidden_size=None, layer_norm=False, dropout=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(DeepFsmn, self).__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    if lorder is None:\n        return\n    self.lorder = lorder\n    self.rorder = rorder\n    self.hidden_size = hidden_size\n    self.layer_norm = layer_norm\n    self.linear = nn.Linear(input_dim, hidden_size)\n    self.norm = nn.LayerNorm(hidden_size)\n    self.drop1 = nn.Dropout(p=dropout)\n    self.drop2 = nn.Dropout(p=dropout)\n    self.project = nn.Linear(hidden_size, output_dim, bias=False)\n    self.conv1 = nn.Conv2d(output_dim, output_dim, [lorder, 1], [1, 1], groups=output_dim, bias=False)\n    self.conv2 = nn.Conv2d(output_dim, output_dim, [rorder, 1], [1, 1], groups=output_dim, bias=False)",
            "def __init__(self, input_dim, output_dim, lorder=None, rorder=None, hidden_size=None, layer_norm=False, dropout=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(DeepFsmn, self).__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    if lorder is None:\n        return\n    self.lorder = lorder\n    self.rorder = rorder\n    self.hidden_size = hidden_size\n    self.layer_norm = layer_norm\n    self.linear = nn.Linear(input_dim, hidden_size)\n    self.norm = nn.LayerNorm(hidden_size)\n    self.drop1 = nn.Dropout(p=dropout)\n    self.drop2 = nn.Dropout(p=dropout)\n    self.project = nn.Linear(hidden_size, output_dim, bias=False)\n    self.conv1 = nn.Conv2d(output_dim, output_dim, [lorder, 1], [1, 1], groups=output_dim, bias=False)\n    self.conv2 = nn.Conv2d(output_dim, output_dim, [rorder, 1], [1, 1], groups=output_dim, bias=False)",
            "def __init__(self, input_dim, output_dim, lorder=None, rorder=None, hidden_size=None, layer_norm=False, dropout=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(DeepFsmn, self).__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    if lorder is None:\n        return\n    self.lorder = lorder\n    self.rorder = rorder\n    self.hidden_size = hidden_size\n    self.layer_norm = layer_norm\n    self.linear = nn.Linear(input_dim, hidden_size)\n    self.norm = nn.LayerNorm(hidden_size)\n    self.drop1 = nn.Dropout(p=dropout)\n    self.drop2 = nn.Dropout(p=dropout)\n    self.project = nn.Linear(hidden_size, output_dim, bias=False)\n    self.conv1 = nn.Conv2d(output_dim, output_dim, [lorder, 1], [1, 1], groups=output_dim, bias=False)\n    self.conv2 = nn.Conv2d(output_dim, output_dim, [rorder, 1], [1, 1], groups=output_dim, bias=False)",
            "def __init__(self, input_dim, output_dim, lorder=None, rorder=None, hidden_size=None, layer_norm=False, dropout=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(DeepFsmn, self).__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    if lorder is None:\n        return\n    self.lorder = lorder\n    self.rorder = rorder\n    self.hidden_size = hidden_size\n    self.layer_norm = layer_norm\n    self.linear = nn.Linear(input_dim, hidden_size)\n    self.norm = nn.LayerNorm(hidden_size)\n    self.drop1 = nn.Dropout(p=dropout)\n    self.drop2 = nn.Dropout(p=dropout)\n    self.project = nn.Linear(hidden_size, output_dim, bias=False)\n    self.conv1 = nn.Conv2d(output_dim, output_dim, [lorder, 1], [1, 1], groups=output_dim, bias=False)\n    self.conv2 = nn.Conv2d(output_dim, output_dim, [rorder, 1], [1, 1], groups=output_dim, bias=False)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, input):\n    f1 = F.relu(self.linear(input))\n    f1 = self.drop1(f1)\n    if self.layer_norm:\n        f1 = self.norm(f1)\n    p1 = self.project(f1)\n    x = th.unsqueeze(p1, 1)\n    x_per = x.permute(0, 3, 2, 1)\n    y = F.pad(x_per, [0, 0, self.lorder - 1, 0])\n    yr = F.pad(x_per, [0, 0, 0, self.rorder])\n    yr = yr[:, :, 1:, :]\n    out = x_per + self.conv1(y) + self.conv2(yr)\n    out = self.drop2(out)\n    out1 = out.permute(0, 3, 2, 1)\n    return input + out1.squeeze()",
        "mutated": [
            "def forward(self, input):\n    if False:\n        i = 10\n    f1 = F.relu(self.linear(input))\n    f1 = self.drop1(f1)\n    if self.layer_norm:\n        f1 = self.norm(f1)\n    p1 = self.project(f1)\n    x = th.unsqueeze(p1, 1)\n    x_per = x.permute(0, 3, 2, 1)\n    y = F.pad(x_per, [0, 0, self.lorder - 1, 0])\n    yr = F.pad(x_per, [0, 0, 0, self.rorder])\n    yr = yr[:, :, 1:, :]\n    out = x_per + self.conv1(y) + self.conv2(yr)\n    out = self.drop2(out)\n    out1 = out.permute(0, 3, 2, 1)\n    return input + out1.squeeze()",
            "def forward(self, input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    f1 = F.relu(self.linear(input))\n    f1 = self.drop1(f1)\n    if self.layer_norm:\n        f1 = self.norm(f1)\n    p1 = self.project(f1)\n    x = th.unsqueeze(p1, 1)\n    x_per = x.permute(0, 3, 2, 1)\n    y = F.pad(x_per, [0, 0, self.lorder - 1, 0])\n    yr = F.pad(x_per, [0, 0, 0, self.rorder])\n    yr = yr[:, :, 1:, :]\n    out = x_per + self.conv1(y) + self.conv2(yr)\n    out = self.drop2(out)\n    out1 = out.permute(0, 3, 2, 1)\n    return input + out1.squeeze()",
            "def forward(self, input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    f1 = F.relu(self.linear(input))\n    f1 = self.drop1(f1)\n    if self.layer_norm:\n        f1 = self.norm(f1)\n    p1 = self.project(f1)\n    x = th.unsqueeze(p1, 1)\n    x_per = x.permute(0, 3, 2, 1)\n    y = F.pad(x_per, [0, 0, self.lorder - 1, 0])\n    yr = F.pad(x_per, [0, 0, 0, self.rorder])\n    yr = yr[:, :, 1:, :]\n    out = x_per + self.conv1(y) + self.conv2(yr)\n    out = self.drop2(out)\n    out1 = out.permute(0, 3, 2, 1)\n    return input + out1.squeeze()",
            "def forward(self, input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    f1 = F.relu(self.linear(input))\n    f1 = self.drop1(f1)\n    if self.layer_norm:\n        f1 = self.norm(f1)\n    p1 = self.project(f1)\n    x = th.unsqueeze(p1, 1)\n    x_per = x.permute(0, 3, 2, 1)\n    y = F.pad(x_per, [0, 0, self.lorder - 1, 0])\n    yr = F.pad(x_per, [0, 0, 0, self.rorder])\n    yr = yr[:, :, 1:, :]\n    out = x_per + self.conv1(y) + self.conv2(yr)\n    out = self.drop2(out)\n    out1 = out.permute(0, 3, 2, 1)\n    return input + out1.squeeze()",
            "def forward(self, input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    f1 = F.relu(self.linear(input))\n    f1 = self.drop1(f1)\n    if self.layer_norm:\n        f1 = self.norm(f1)\n    p1 = self.project(f1)\n    x = th.unsqueeze(p1, 1)\n    x_per = x.permute(0, 3, 2, 1)\n    y = F.pad(x_per, [0, 0, self.lorder - 1, 0])\n    yr = F.pad(x_per, [0, 0, 0, self.rorder])\n    yr = yr[:, :, 1:, :]\n    out = x_per + self.conv1(y) + self.conv2(yr)\n    out = self.drop2(out)\n    out1 = out.permute(0, 3, 2, 1)\n    return input + out1.squeeze()"
        ]
    },
    {
        "func_name": "to_kaldi_nnet",
        "original": "def to_kaldi_nnet(self):\n    re_str = ''\n    re_str += '<UniDeepFsmn> %d %d\\n' % (self.output_dim, self.input_dim)\n    re_str += '<LearnRateCoef> %d <HidSize> %d <LOrder> %d <LStride> %d <MaxNorm> 0\\n' % (1, self.hidden_size, self.lorder, 1)\n    lfiters = self.state_dict()['conv1.weight']\n    x = np.flipud(lfiters.squeeze().numpy().T)\n    re_str += to_kaldi_matrix(x)\n    proj_weights = self.state_dict()['project.weight']\n    x = proj_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_weights = self.state_dict()['linear.weight']\n    x = linear_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_bias = self.state_dict()['linear.bias']\n    x = linear_bias.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    return re_str",
        "mutated": [
            "def to_kaldi_nnet(self):\n    if False:\n        i = 10\n    re_str = ''\n    re_str += '<UniDeepFsmn> %d %d\\n' % (self.output_dim, self.input_dim)\n    re_str += '<LearnRateCoef> %d <HidSize> %d <LOrder> %d <LStride> %d <MaxNorm> 0\\n' % (1, self.hidden_size, self.lorder, 1)\n    lfiters = self.state_dict()['conv1.weight']\n    x = np.flipud(lfiters.squeeze().numpy().T)\n    re_str += to_kaldi_matrix(x)\n    proj_weights = self.state_dict()['project.weight']\n    x = proj_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_weights = self.state_dict()['linear.weight']\n    x = linear_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_bias = self.state_dict()['linear.bias']\n    x = linear_bias.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    return re_str",
            "def to_kaldi_nnet(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    re_str = ''\n    re_str += '<UniDeepFsmn> %d %d\\n' % (self.output_dim, self.input_dim)\n    re_str += '<LearnRateCoef> %d <HidSize> %d <LOrder> %d <LStride> %d <MaxNorm> 0\\n' % (1, self.hidden_size, self.lorder, 1)\n    lfiters = self.state_dict()['conv1.weight']\n    x = np.flipud(lfiters.squeeze().numpy().T)\n    re_str += to_kaldi_matrix(x)\n    proj_weights = self.state_dict()['project.weight']\n    x = proj_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_weights = self.state_dict()['linear.weight']\n    x = linear_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_bias = self.state_dict()['linear.bias']\n    x = linear_bias.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    return re_str",
            "def to_kaldi_nnet(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    re_str = ''\n    re_str += '<UniDeepFsmn> %d %d\\n' % (self.output_dim, self.input_dim)\n    re_str += '<LearnRateCoef> %d <HidSize> %d <LOrder> %d <LStride> %d <MaxNorm> 0\\n' % (1, self.hidden_size, self.lorder, 1)\n    lfiters = self.state_dict()['conv1.weight']\n    x = np.flipud(lfiters.squeeze().numpy().T)\n    re_str += to_kaldi_matrix(x)\n    proj_weights = self.state_dict()['project.weight']\n    x = proj_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_weights = self.state_dict()['linear.weight']\n    x = linear_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_bias = self.state_dict()['linear.bias']\n    x = linear_bias.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    return re_str",
            "def to_kaldi_nnet(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    re_str = ''\n    re_str += '<UniDeepFsmn> %d %d\\n' % (self.output_dim, self.input_dim)\n    re_str += '<LearnRateCoef> %d <HidSize> %d <LOrder> %d <LStride> %d <MaxNorm> 0\\n' % (1, self.hidden_size, self.lorder, 1)\n    lfiters = self.state_dict()['conv1.weight']\n    x = np.flipud(lfiters.squeeze().numpy().T)\n    re_str += to_kaldi_matrix(x)\n    proj_weights = self.state_dict()['project.weight']\n    x = proj_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_weights = self.state_dict()['linear.weight']\n    x = linear_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_bias = self.state_dict()['linear.bias']\n    x = linear_bias.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    return re_str",
            "def to_kaldi_nnet(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    re_str = ''\n    re_str += '<UniDeepFsmn> %d %d\\n' % (self.output_dim, self.input_dim)\n    re_str += '<LearnRateCoef> %d <HidSize> %d <LOrder> %d <LStride> %d <MaxNorm> 0\\n' % (1, self.hidden_size, self.lorder, 1)\n    lfiters = self.state_dict()['conv1.weight']\n    x = np.flipud(lfiters.squeeze().numpy().T)\n    re_str += to_kaldi_matrix(x)\n    proj_weights = self.state_dict()['project.weight']\n    x = proj_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_weights = self.state_dict()['linear.weight']\n    x = linear_weights.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    linear_bias = self.state_dict()['linear.bias']\n    x = linear_bias.squeeze().numpy()\n    re_str += to_kaldi_matrix(x)\n    return re_str"
        ]
    },
    {
        "func_name": "load_kaldi_nnet",
        "original": "def load_kaldi_nnet(self, instr):\n    output = expect_token_number(instr, '<LearnRateCoef>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LearnRateCoef>')\n    (instr, lr) = output\n    output = expect_token_number(instr, '<HidSize>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <HidSize>')\n    (instr, hiddensize) = output\n    self.hidden_size = int(hiddensize)\n    output = expect_token_number(instr, '<LOrder>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LOrder>')\n    (instr, lorder) = output\n    self.lorder = int(lorder)\n    output = expect_token_number(instr, '<LStride>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LStride>')\n    (instr, lstride) = output\n    self.lstride = lstride\n    output = expect_token_number(instr, '<MaxNorm>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <MaxNorm>')\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    mat1 = np.fliplr(mat.T).copy()\n    self.conv1 = nn.Conv2d(self.output_dim, self.output_dim, [self.lorder, 1], [1, 1], groups=self.output_dim, bias=False)\n    mat_th = th.from_numpy(mat1).type(th.FloatTensor)\n    mat_th = mat_th.unsqueeze(1)\n    mat_th = mat_th.unsqueeze(3)\n    self.conv1.weight = th.nn.Parameter(mat_th)\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.project = nn.Linear(self.hidden_size, self.output_dim, bias=False)\n    self.linear = nn.Linear(self.input_dim, self.hidden_size)\n    self.project.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.bias = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    return instr",
        "mutated": [
            "def load_kaldi_nnet(self, instr):\n    if False:\n        i = 10\n    output = expect_token_number(instr, '<LearnRateCoef>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LearnRateCoef>')\n    (instr, lr) = output\n    output = expect_token_number(instr, '<HidSize>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <HidSize>')\n    (instr, hiddensize) = output\n    self.hidden_size = int(hiddensize)\n    output = expect_token_number(instr, '<LOrder>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LOrder>')\n    (instr, lorder) = output\n    self.lorder = int(lorder)\n    output = expect_token_number(instr, '<LStride>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LStride>')\n    (instr, lstride) = output\n    self.lstride = lstride\n    output = expect_token_number(instr, '<MaxNorm>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <MaxNorm>')\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    mat1 = np.fliplr(mat.T).copy()\n    self.conv1 = nn.Conv2d(self.output_dim, self.output_dim, [self.lorder, 1], [1, 1], groups=self.output_dim, bias=False)\n    mat_th = th.from_numpy(mat1).type(th.FloatTensor)\n    mat_th = mat_th.unsqueeze(1)\n    mat_th = mat_th.unsqueeze(3)\n    self.conv1.weight = th.nn.Parameter(mat_th)\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.project = nn.Linear(self.hidden_size, self.output_dim, bias=False)\n    self.linear = nn.Linear(self.input_dim, self.hidden_size)\n    self.project.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.bias = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    return instr",
            "def load_kaldi_nnet(self, instr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    output = expect_token_number(instr, '<LearnRateCoef>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LearnRateCoef>')\n    (instr, lr) = output\n    output = expect_token_number(instr, '<HidSize>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <HidSize>')\n    (instr, hiddensize) = output\n    self.hidden_size = int(hiddensize)\n    output = expect_token_number(instr, '<LOrder>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LOrder>')\n    (instr, lorder) = output\n    self.lorder = int(lorder)\n    output = expect_token_number(instr, '<LStride>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LStride>')\n    (instr, lstride) = output\n    self.lstride = lstride\n    output = expect_token_number(instr, '<MaxNorm>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <MaxNorm>')\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    mat1 = np.fliplr(mat.T).copy()\n    self.conv1 = nn.Conv2d(self.output_dim, self.output_dim, [self.lorder, 1], [1, 1], groups=self.output_dim, bias=False)\n    mat_th = th.from_numpy(mat1).type(th.FloatTensor)\n    mat_th = mat_th.unsqueeze(1)\n    mat_th = mat_th.unsqueeze(3)\n    self.conv1.weight = th.nn.Parameter(mat_th)\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.project = nn.Linear(self.hidden_size, self.output_dim, bias=False)\n    self.linear = nn.Linear(self.input_dim, self.hidden_size)\n    self.project.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.bias = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    return instr",
            "def load_kaldi_nnet(self, instr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    output = expect_token_number(instr, '<LearnRateCoef>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LearnRateCoef>')\n    (instr, lr) = output\n    output = expect_token_number(instr, '<HidSize>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <HidSize>')\n    (instr, hiddensize) = output\n    self.hidden_size = int(hiddensize)\n    output = expect_token_number(instr, '<LOrder>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LOrder>')\n    (instr, lorder) = output\n    self.lorder = int(lorder)\n    output = expect_token_number(instr, '<LStride>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LStride>')\n    (instr, lstride) = output\n    self.lstride = lstride\n    output = expect_token_number(instr, '<MaxNorm>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <MaxNorm>')\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    mat1 = np.fliplr(mat.T).copy()\n    self.conv1 = nn.Conv2d(self.output_dim, self.output_dim, [self.lorder, 1], [1, 1], groups=self.output_dim, bias=False)\n    mat_th = th.from_numpy(mat1).type(th.FloatTensor)\n    mat_th = mat_th.unsqueeze(1)\n    mat_th = mat_th.unsqueeze(3)\n    self.conv1.weight = th.nn.Parameter(mat_th)\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.project = nn.Linear(self.hidden_size, self.output_dim, bias=False)\n    self.linear = nn.Linear(self.input_dim, self.hidden_size)\n    self.project.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.bias = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    return instr",
            "def load_kaldi_nnet(self, instr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    output = expect_token_number(instr, '<LearnRateCoef>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LearnRateCoef>')\n    (instr, lr) = output\n    output = expect_token_number(instr, '<HidSize>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <HidSize>')\n    (instr, hiddensize) = output\n    self.hidden_size = int(hiddensize)\n    output = expect_token_number(instr, '<LOrder>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LOrder>')\n    (instr, lorder) = output\n    self.lorder = int(lorder)\n    output = expect_token_number(instr, '<LStride>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LStride>')\n    (instr, lstride) = output\n    self.lstride = lstride\n    output = expect_token_number(instr, '<MaxNorm>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <MaxNorm>')\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    mat1 = np.fliplr(mat.T).copy()\n    self.conv1 = nn.Conv2d(self.output_dim, self.output_dim, [self.lorder, 1], [1, 1], groups=self.output_dim, bias=False)\n    mat_th = th.from_numpy(mat1).type(th.FloatTensor)\n    mat_th = mat_th.unsqueeze(1)\n    mat_th = mat_th.unsqueeze(3)\n    self.conv1.weight = th.nn.Parameter(mat_th)\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.project = nn.Linear(self.hidden_size, self.output_dim, bias=False)\n    self.linear = nn.Linear(self.input_dim, self.hidden_size)\n    self.project.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.bias = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    return instr",
            "def load_kaldi_nnet(self, instr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    output = expect_token_number(instr, '<LearnRateCoef>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LearnRateCoef>')\n    (instr, lr) = output\n    output = expect_token_number(instr, '<HidSize>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <HidSize>')\n    (instr, hiddensize) = output\n    self.hidden_size = int(hiddensize)\n    output = expect_token_number(instr, '<LOrder>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LOrder>')\n    (instr, lorder) = output\n    self.lorder = int(lorder)\n    output = expect_token_number(instr, '<LStride>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <LStride>')\n    (instr, lstride) = output\n    self.lstride = lstride\n    output = expect_token_number(instr, '<MaxNorm>')\n    if output is None:\n        raise Exception('UniDeepFsmn format error for <MaxNorm>')\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    mat1 = np.fliplr(mat.T).copy()\n    self.conv1 = nn.Conv2d(self.output_dim, self.output_dim, [self.lorder, 1], [1, 1], groups=self.output_dim, bias=False)\n    mat_th = th.from_numpy(mat1).type(th.FloatTensor)\n    mat_th = mat_th.unsqueeze(1)\n    mat_th = mat_th.unsqueeze(3)\n    self.conv1.weight = th.nn.Parameter(mat_th)\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.project = nn.Linear(self.hidden_size, self.output_dim, bias=False)\n    self.linear = nn.Linear(self.input_dim, self.hidden_size)\n    self.project.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.weight = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    output = expect_kaldi_matrix(instr)\n    if output is None:\n        raise Exception('UniDeepFsmn format error for parsing matrix')\n    (instr, mat) = output\n    self.linear.bias = th.nn.Parameter(th.from_numpy(mat).type(th.FloatTensor))\n    return instr"
        ]
    }
]