[
    {
        "func_name": "replace_key_with_offset",
        "original": "def replace_key_with_offset(key, offset, original_name, new_name):\n    \"\"\"\n    Replaces the key by subtracting the offset from the original layer number\n    \"\"\"\n    to_find = original_name.split('.')[0]\n    key_list = key.split('.')\n    orig_block_num = int(key_list[key_list.index(to_find) - 2])\n    layer_num = int(key_list[key_list.index(to_find) - 1])\n    new_block_num = orig_block_num - offset\n    key = key.replace(f'{orig_block_num}.{layer_num}.{original_name}', f'block.{new_block_num}.{layer_num}.{new_name}')\n    return key",
        "mutated": [
            "def replace_key_with_offset(key, offset, original_name, new_name):\n    if False:\n        i = 10\n    '\\n    Replaces the key by subtracting the offset from the original layer number\\n    '\n    to_find = original_name.split('.')[0]\n    key_list = key.split('.')\n    orig_block_num = int(key_list[key_list.index(to_find) - 2])\n    layer_num = int(key_list[key_list.index(to_find) - 1])\n    new_block_num = orig_block_num - offset\n    key = key.replace(f'{orig_block_num}.{layer_num}.{original_name}', f'block.{new_block_num}.{layer_num}.{new_name}')\n    return key",
            "def replace_key_with_offset(key, offset, original_name, new_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Replaces the key by subtracting the offset from the original layer number\\n    '\n    to_find = original_name.split('.')[0]\n    key_list = key.split('.')\n    orig_block_num = int(key_list[key_list.index(to_find) - 2])\n    layer_num = int(key_list[key_list.index(to_find) - 1])\n    new_block_num = orig_block_num - offset\n    key = key.replace(f'{orig_block_num}.{layer_num}.{original_name}', f'block.{new_block_num}.{layer_num}.{new_name}')\n    return key",
            "def replace_key_with_offset(key, offset, original_name, new_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Replaces the key by subtracting the offset from the original layer number\\n    '\n    to_find = original_name.split('.')[0]\n    key_list = key.split('.')\n    orig_block_num = int(key_list[key_list.index(to_find) - 2])\n    layer_num = int(key_list[key_list.index(to_find) - 1])\n    new_block_num = orig_block_num - offset\n    key = key.replace(f'{orig_block_num}.{layer_num}.{original_name}', f'block.{new_block_num}.{layer_num}.{new_name}')\n    return key",
            "def replace_key_with_offset(key, offset, original_name, new_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Replaces the key by subtracting the offset from the original layer number\\n    '\n    to_find = original_name.split('.')[0]\n    key_list = key.split('.')\n    orig_block_num = int(key_list[key_list.index(to_find) - 2])\n    layer_num = int(key_list[key_list.index(to_find) - 1])\n    new_block_num = orig_block_num - offset\n    key = key.replace(f'{orig_block_num}.{layer_num}.{original_name}', f'block.{new_block_num}.{layer_num}.{new_name}')\n    return key",
            "def replace_key_with_offset(key, offset, original_name, new_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Replaces the key by subtracting the offset from the original layer number\\n    '\n    to_find = original_name.split('.')[0]\n    key_list = key.split('.')\n    orig_block_num = int(key_list[key_list.index(to_find) - 2])\n    layer_num = int(key_list[key_list.index(to_find) - 1])\n    new_block_num = orig_block_num - offset\n    key = key.replace(f'{orig_block_num}.{layer_num}.{original_name}', f'block.{new_block_num}.{layer_num}.{new_name}')\n    return key"
        ]
    },
    {
        "func_name": "rename_keys",
        "original": "def rename_keys(state_dict):\n    new_state_dict = OrderedDict()\n    (total_embed_found, patch_emb_offset) = (0, 0)\n    for (key, value) in state_dict.items():\n        if key.startswith('network'):\n            key = key.replace('network', 'poolformer.encoder')\n        if 'proj' in key:\n            if key.endswith('bias') and 'patch_embed' not in key:\n                patch_emb_offset += 1\n            to_replace = key[:key.find('proj')]\n            key = key.replace(to_replace, f'patch_embeddings.{total_embed_found}.')\n            key = key.replace('proj', 'projection')\n            if key.endswith('bias'):\n                total_embed_found += 1\n        if 'patch_embeddings' in key:\n            key = 'poolformer.encoder.' + key\n        if 'mlp.fc1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc1', 'output.conv1')\n        if 'mlp.fc2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc2', 'output.conv2')\n        if 'norm1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm1', 'before_norm')\n        if 'norm2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm2', 'after_norm')\n        if 'layer_scale_1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_1', 'layer_scale_1')\n        if 'layer_scale_2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_2', 'layer_scale_2')\n        if 'head' in key:\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict",
        "mutated": [
            "def rename_keys(state_dict):\n    if False:\n        i = 10\n    new_state_dict = OrderedDict()\n    (total_embed_found, patch_emb_offset) = (0, 0)\n    for (key, value) in state_dict.items():\n        if key.startswith('network'):\n            key = key.replace('network', 'poolformer.encoder')\n        if 'proj' in key:\n            if key.endswith('bias') and 'patch_embed' not in key:\n                patch_emb_offset += 1\n            to_replace = key[:key.find('proj')]\n            key = key.replace(to_replace, f'patch_embeddings.{total_embed_found}.')\n            key = key.replace('proj', 'projection')\n            if key.endswith('bias'):\n                total_embed_found += 1\n        if 'patch_embeddings' in key:\n            key = 'poolformer.encoder.' + key\n        if 'mlp.fc1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc1', 'output.conv1')\n        if 'mlp.fc2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc2', 'output.conv2')\n        if 'norm1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm1', 'before_norm')\n        if 'norm2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm2', 'after_norm')\n        if 'layer_scale_1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_1', 'layer_scale_1')\n        if 'layer_scale_2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_2', 'layer_scale_2')\n        if 'head' in key:\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict",
            "def rename_keys(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    new_state_dict = OrderedDict()\n    (total_embed_found, patch_emb_offset) = (0, 0)\n    for (key, value) in state_dict.items():\n        if key.startswith('network'):\n            key = key.replace('network', 'poolformer.encoder')\n        if 'proj' in key:\n            if key.endswith('bias') and 'patch_embed' not in key:\n                patch_emb_offset += 1\n            to_replace = key[:key.find('proj')]\n            key = key.replace(to_replace, f'patch_embeddings.{total_embed_found}.')\n            key = key.replace('proj', 'projection')\n            if key.endswith('bias'):\n                total_embed_found += 1\n        if 'patch_embeddings' in key:\n            key = 'poolformer.encoder.' + key\n        if 'mlp.fc1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc1', 'output.conv1')\n        if 'mlp.fc2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc2', 'output.conv2')\n        if 'norm1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm1', 'before_norm')\n        if 'norm2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm2', 'after_norm')\n        if 'layer_scale_1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_1', 'layer_scale_1')\n        if 'layer_scale_2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_2', 'layer_scale_2')\n        if 'head' in key:\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict",
            "def rename_keys(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    new_state_dict = OrderedDict()\n    (total_embed_found, patch_emb_offset) = (0, 0)\n    for (key, value) in state_dict.items():\n        if key.startswith('network'):\n            key = key.replace('network', 'poolformer.encoder')\n        if 'proj' in key:\n            if key.endswith('bias') and 'patch_embed' not in key:\n                patch_emb_offset += 1\n            to_replace = key[:key.find('proj')]\n            key = key.replace(to_replace, f'patch_embeddings.{total_embed_found}.')\n            key = key.replace('proj', 'projection')\n            if key.endswith('bias'):\n                total_embed_found += 1\n        if 'patch_embeddings' in key:\n            key = 'poolformer.encoder.' + key\n        if 'mlp.fc1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc1', 'output.conv1')\n        if 'mlp.fc2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc2', 'output.conv2')\n        if 'norm1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm1', 'before_norm')\n        if 'norm2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm2', 'after_norm')\n        if 'layer_scale_1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_1', 'layer_scale_1')\n        if 'layer_scale_2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_2', 'layer_scale_2')\n        if 'head' in key:\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict",
            "def rename_keys(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    new_state_dict = OrderedDict()\n    (total_embed_found, patch_emb_offset) = (0, 0)\n    for (key, value) in state_dict.items():\n        if key.startswith('network'):\n            key = key.replace('network', 'poolformer.encoder')\n        if 'proj' in key:\n            if key.endswith('bias') and 'patch_embed' not in key:\n                patch_emb_offset += 1\n            to_replace = key[:key.find('proj')]\n            key = key.replace(to_replace, f'patch_embeddings.{total_embed_found}.')\n            key = key.replace('proj', 'projection')\n            if key.endswith('bias'):\n                total_embed_found += 1\n        if 'patch_embeddings' in key:\n            key = 'poolformer.encoder.' + key\n        if 'mlp.fc1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc1', 'output.conv1')\n        if 'mlp.fc2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc2', 'output.conv2')\n        if 'norm1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm1', 'before_norm')\n        if 'norm2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm2', 'after_norm')\n        if 'layer_scale_1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_1', 'layer_scale_1')\n        if 'layer_scale_2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_2', 'layer_scale_2')\n        if 'head' in key:\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict",
            "def rename_keys(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    new_state_dict = OrderedDict()\n    (total_embed_found, patch_emb_offset) = (0, 0)\n    for (key, value) in state_dict.items():\n        if key.startswith('network'):\n            key = key.replace('network', 'poolformer.encoder')\n        if 'proj' in key:\n            if key.endswith('bias') and 'patch_embed' not in key:\n                patch_emb_offset += 1\n            to_replace = key[:key.find('proj')]\n            key = key.replace(to_replace, f'patch_embeddings.{total_embed_found}.')\n            key = key.replace('proj', 'projection')\n            if key.endswith('bias'):\n                total_embed_found += 1\n        if 'patch_embeddings' in key:\n            key = 'poolformer.encoder.' + key\n        if 'mlp.fc1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc1', 'output.conv1')\n        if 'mlp.fc2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'mlp.fc2', 'output.conv2')\n        if 'norm1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm1', 'before_norm')\n        if 'norm2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'norm2', 'after_norm')\n        if 'layer_scale_1' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_1', 'layer_scale_1')\n        if 'layer_scale_2' in key:\n            key = replace_key_with_offset(key, patch_emb_offset, 'layer_scale_2', 'layer_scale_2')\n        if 'head' in key:\n            key = key.replace('head', 'classifier')\n        new_state_dict[key] = value\n    return new_state_dict"
        ]
    },
    {
        "func_name": "prepare_img",
        "original": "def prepare_img():\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image",
        "mutated": [
            "def prepare_img():\n    if False:\n        i = 10\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n    image = Image.open(requests.get(url, stream=True).raw)\n    return image"
        ]
    },
    {
        "func_name": "convert_poolformer_checkpoint",
        "original": "@torch.no_grad()\ndef convert_poolformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    \"\"\"\n    Copy/paste/tweak model's weights to our PoolFormer structure.\n    \"\"\"\n    config = PoolFormerConfig()\n    repo_id = 'huggingface/label-files'\n    size = model_name[-3:]\n    config.num_labels = 1000\n    filename = 'imagenet-1k-id2label.json'\n    expected_shape = (1, 1000)\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 's12':\n        config.depths = [2, 2, 6, 2]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's24':\n        config.depths = [4, 4, 12, 4]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.9\n    elif size == 'm36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    elif size == 'm48':\n        config.depths = [8, 8, 24, 8]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    state_dict = rename_keys(state_dict)\n    model = PoolFormerForImageClassification(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    pixel_values = image_processor(images=prepare_img(), return_tensors='pt').pixel_values\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if size == 's12':\n        expected_slice = torch.tensor([-0.3045, -0.6758, -0.4869])\n    elif size == 's24':\n        expected_slice = torch.tensor([0.4402, -0.1374, -0.8045])\n    elif size == 's36':\n        expected_slice = torch.tensor([-0.608, -0.5133, -0.5898])\n    elif size == 'm36':\n        expected_slice = torch.tensor([0.3952, 0.2263, -1.2668])\n    elif size == 'm48':\n        expected_slice = torch.tensor([0.1167, -0.0656, -0.3423])\n    else:\n        raise ValueError(f'Size {size} not supported')\n    assert logits.shape == expected_shape\n    assert torch.allclose(logits[0, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    print(f'Saving image processor to {pytorch_dump_folder_path}')\n    image_processor.save_pretrained(pytorch_dump_folder_path)",
        "mutated": [
            "@torch.no_grad()\ndef convert_poolformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n    \"\\n    Copy/paste/tweak model's weights to our PoolFormer structure.\\n    \"\n    config = PoolFormerConfig()\n    repo_id = 'huggingface/label-files'\n    size = model_name[-3:]\n    config.num_labels = 1000\n    filename = 'imagenet-1k-id2label.json'\n    expected_shape = (1, 1000)\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 's12':\n        config.depths = [2, 2, 6, 2]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's24':\n        config.depths = [4, 4, 12, 4]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.9\n    elif size == 'm36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    elif size == 'm48':\n        config.depths = [8, 8, 24, 8]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    state_dict = rename_keys(state_dict)\n    model = PoolFormerForImageClassification(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    pixel_values = image_processor(images=prepare_img(), return_tensors='pt').pixel_values\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if size == 's12':\n        expected_slice = torch.tensor([-0.3045, -0.6758, -0.4869])\n    elif size == 's24':\n        expected_slice = torch.tensor([0.4402, -0.1374, -0.8045])\n    elif size == 's36':\n        expected_slice = torch.tensor([-0.608, -0.5133, -0.5898])\n    elif size == 'm36':\n        expected_slice = torch.tensor([0.3952, 0.2263, -1.2668])\n    elif size == 'm48':\n        expected_slice = torch.tensor([0.1167, -0.0656, -0.3423])\n    else:\n        raise ValueError(f'Size {size} not supported')\n    assert logits.shape == expected_shape\n    assert torch.allclose(logits[0, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    print(f'Saving image processor to {pytorch_dump_folder_path}')\n    image_processor.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_poolformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Copy/paste/tweak model's weights to our PoolFormer structure.\\n    \"\n    config = PoolFormerConfig()\n    repo_id = 'huggingface/label-files'\n    size = model_name[-3:]\n    config.num_labels = 1000\n    filename = 'imagenet-1k-id2label.json'\n    expected_shape = (1, 1000)\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 's12':\n        config.depths = [2, 2, 6, 2]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's24':\n        config.depths = [4, 4, 12, 4]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.9\n    elif size == 'm36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    elif size == 'm48':\n        config.depths = [8, 8, 24, 8]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    state_dict = rename_keys(state_dict)\n    model = PoolFormerForImageClassification(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    pixel_values = image_processor(images=prepare_img(), return_tensors='pt').pixel_values\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if size == 's12':\n        expected_slice = torch.tensor([-0.3045, -0.6758, -0.4869])\n    elif size == 's24':\n        expected_slice = torch.tensor([0.4402, -0.1374, -0.8045])\n    elif size == 's36':\n        expected_slice = torch.tensor([-0.608, -0.5133, -0.5898])\n    elif size == 'm36':\n        expected_slice = torch.tensor([0.3952, 0.2263, -1.2668])\n    elif size == 'm48':\n        expected_slice = torch.tensor([0.1167, -0.0656, -0.3423])\n    else:\n        raise ValueError(f'Size {size} not supported')\n    assert logits.shape == expected_shape\n    assert torch.allclose(logits[0, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    print(f'Saving image processor to {pytorch_dump_folder_path}')\n    image_processor.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_poolformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Copy/paste/tweak model's weights to our PoolFormer structure.\\n    \"\n    config = PoolFormerConfig()\n    repo_id = 'huggingface/label-files'\n    size = model_name[-3:]\n    config.num_labels = 1000\n    filename = 'imagenet-1k-id2label.json'\n    expected_shape = (1, 1000)\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 's12':\n        config.depths = [2, 2, 6, 2]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's24':\n        config.depths = [4, 4, 12, 4]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.9\n    elif size == 'm36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    elif size == 'm48':\n        config.depths = [8, 8, 24, 8]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    state_dict = rename_keys(state_dict)\n    model = PoolFormerForImageClassification(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    pixel_values = image_processor(images=prepare_img(), return_tensors='pt').pixel_values\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if size == 's12':\n        expected_slice = torch.tensor([-0.3045, -0.6758, -0.4869])\n    elif size == 's24':\n        expected_slice = torch.tensor([0.4402, -0.1374, -0.8045])\n    elif size == 's36':\n        expected_slice = torch.tensor([-0.608, -0.5133, -0.5898])\n    elif size == 'm36':\n        expected_slice = torch.tensor([0.3952, 0.2263, -1.2668])\n    elif size == 'm48':\n        expected_slice = torch.tensor([0.1167, -0.0656, -0.3423])\n    else:\n        raise ValueError(f'Size {size} not supported')\n    assert logits.shape == expected_shape\n    assert torch.allclose(logits[0, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    print(f'Saving image processor to {pytorch_dump_folder_path}')\n    image_processor.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_poolformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Copy/paste/tweak model's weights to our PoolFormer structure.\\n    \"\n    config = PoolFormerConfig()\n    repo_id = 'huggingface/label-files'\n    size = model_name[-3:]\n    config.num_labels = 1000\n    filename = 'imagenet-1k-id2label.json'\n    expected_shape = (1, 1000)\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 's12':\n        config.depths = [2, 2, 6, 2]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's24':\n        config.depths = [4, 4, 12, 4]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.9\n    elif size == 'm36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    elif size == 'm48':\n        config.depths = [8, 8, 24, 8]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    state_dict = rename_keys(state_dict)\n    model = PoolFormerForImageClassification(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    pixel_values = image_processor(images=prepare_img(), return_tensors='pt').pixel_values\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if size == 's12':\n        expected_slice = torch.tensor([-0.3045, -0.6758, -0.4869])\n    elif size == 's24':\n        expected_slice = torch.tensor([0.4402, -0.1374, -0.8045])\n    elif size == 's36':\n        expected_slice = torch.tensor([-0.608, -0.5133, -0.5898])\n    elif size == 'm36':\n        expected_slice = torch.tensor([0.3952, 0.2263, -1.2668])\n    elif size == 'm48':\n        expected_slice = torch.tensor([0.1167, -0.0656, -0.3423])\n    else:\n        raise ValueError(f'Size {size} not supported')\n    assert logits.shape == expected_shape\n    assert torch.allclose(logits[0, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    print(f'Saving image processor to {pytorch_dump_folder_path}')\n    image_processor.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_poolformer_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Copy/paste/tweak model's weights to our PoolFormer structure.\\n    \"\n    config = PoolFormerConfig()\n    repo_id = 'huggingface/label-files'\n    size = model_name[-3:]\n    config.num_labels = 1000\n    filename = 'imagenet-1k-id2label.json'\n    expected_shape = (1, 1000)\n    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n    id2label = {int(k): v for (k, v) in id2label.items()}\n    config.id2label = id2label\n    config.label2id = {v: k for (k, v) in id2label.items()}\n    if size == 's12':\n        config.depths = [2, 2, 6, 2]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's24':\n        config.depths = [4, 4, 12, 4]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        crop_pct = 0.9\n    elif size == 's36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [64, 128, 320, 512]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.9\n    elif size == 'm36':\n        config.depths = [6, 6, 18, 6]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    elif size == 'm48':\n        config.depths = [8, 8, 24, 8]\n        config.hidden_sizes = [96, 192, 384, 768]\n        config.mlp_ratio = 4.0\n        config.layer_scale_init_value = 1e-06\n        crop_pct = 0.95\n    else:\n        raise ValueError(f'Size {size} not supported')\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    image = prepare_img()\n    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n    logger.info(f'Converting model {model_name}...')\n    state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    state_dict = rename_keys(state_dict)\n    model = PoolFormerForImageClassification(config)\n    model.load_state_dict(state_dict)\n    model.eval()\n    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)\n    pixel_values = image_processor(images=prepare_img(), return_tensors='pt').pixel_values\n    outputs = model(pixel_values)\n    logits = outputs.logits\n    if size == 's12':\n        expected_slice = torch.tensor([-0.3045, -0.6758, -0.4869])\n    elif size == 's24':\n        expected_slice = torch.tensor([0.4402, -0.1374, -0.8045])\n    elif size == 's36':\n        expected_slice = torch.tensor([-0.608, -0.5133, -0.5898])\n    elif size == 'm36':\n        expected_slice = torch.tensor([0.3952, 0.2263, -1.2668])\n    elif size == 'm48':\n        expected_slice = torch.tensor([0.1167, -0.0656, -0.3423])\n    else:\n        raise ValueError(f'Size {size} not supported')\n    assert logits.shape == expected_shape\n    assert torch.allclose(logits[0, :3], expected_slice, atol=0.01)\n    logger.info(f'Saving PyTorch model and image processor to {pytorch_dump_folder_path}...')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)\n    print(f'Saving image processor to {pytorch_dump_folder_path}')\n    image_processor.save_pretrained(pytorch_dump_folder_path)"
        ]
    }
]