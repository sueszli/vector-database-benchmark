"""Simple Python Parser

Parse Python code into a list of logical lines, represented by LogicalLine
objects. This uses Python's tokenizer to generate the tokens. As such, YAPF must
be run with the appropriate Python version---Python >=3.7 for Python 3.7 code,
Python >=3.8 for Python 3.8 code, etc.

This parser uses Python's native "tokenizer" module to generate a list of tokens
for the source code. It then uses Python's native "ast" module to assign
subtypes, calculate split penalties, etc.

A "logical line" produced by Python's "tokenizer" module ends with a
tokenize.NEWLINE, rather than a tokenize.NL, making it easy to separate them
out. Comments all end with a tokentizer.NL, so we need to make sure we don't
errantly pick up non-comment tokens when parsing comment blocks.

  ParseCode(): parse the code producing a list of logical lines.
"""
import ast
import codecs
import os
import token
import tokenize
from io import StringIO
from tokenize import TokenInfo
from yapf.pyparser import split_penalty_visitor
from yapf.yapflib import format_token
from yapf.yapflib import logical_line
CONTINUATION = token.N_TOKENS

def ParseCode(unformatted_source, filename='<unknown>'):
    if False:
        for i in range(10):
            print('nop')
    "Parse a string of Python code into logical lines.\n\n  This provides an alternative entry point to YAPF.\n\n  Arguments:\n    unformatted_source: (unicode) The code to format.\n    filename: (unicode) The name of the file being reformatted.\n\n  Returns:\n    A list of LogicalLines.\n\n  Raises:\n    An exception is raised if there's an error during AST parsing.\n  "
    if not unformatted_source.endswith(os.linesep):
        unformatted_source += os.linesep
    try:
        ast_tree = ast.parse(unformatted_source, filename)
        ast.fix_missing_locations(ast_tree)
        readline = StringIO(unformatted_source).readline
        tokens = tokenize.generate_tokens(readline)
    except Exception:
        raise
    logical_lines = _CreateLogicalLines(tokens)
    split_penalty_visitor.SplitPenalty(logical_lines).visit(ast_tree)
    return logical_lines

def _CreateLogicalLines(tokens):
    if False:
        i = 10
        return i + 15
    'Separate tokens into logical lines.\n\n  Arguments:\n    tokens: (list of tokenizer.TokenInfo) Tokens generated by tokenizer.\n\n  Returns:\n    A list of LogicalLines.\n  '
    formatted_tokens = []
    prev_tok = None
    for tok in tokens:
        tok = TokenInfo(*tok)
        if prev_tok and prev_tok.line.rstrip().endswith('\\') and (prev_tok.start[0] < tok.start[0]):
            ctok = TokenInfo(type=CONTINUATION, string='\\', start=(prev_tok.start[0], prev_tok.start[1] + 1), end=(prev_tok.end[0], prev_tok.end[0] + 2), line=prev_tok.line)
            ctok.lineno = ctok.start[0]
            ctok.column = ctok.start[1]
            ctok.value = '\\'
            formatted_tokens.append(format_token.FormatToken(ctok, 'CONTINUATION'))
        tok.lineno = tok.start[0]
        tok.column = tok.start[1]
        tok.value = tok.string
        formatted_tokens.append(format_token.FormatToken(tok, token.tok_name[tok.type]))
        prev_tok = tok
    (logical_lines, cur_logical_line) = ([], [])
    depth = 0
    for tok in formatted_tokens:
        if tok.type == tokenize.ENDMARKER:
            break
        if tok.type == tokenize.NEWLINE:
            logical_lines.append(logical_line.LogicalLine(depth, cur_logical_line))
            cur_logical_line = []
        elif tok.type == tokenize.INDENT:
            depth += 1
        elif tok.type == tokenize.DEDENT:
            depth -= 1
        elif tok.type == tokenize.NL:
            pass
        else:
            if cur_logical_line and (not tok.type == tokenize.COMMENT) and (cur_logical_line[0].type == tokenize.COMMENT):
                logical_lines.append(logical_line.LogicalLine(depth, cur_logical_line))
                cur_logical_line = []
            cur_logical_line.append(tok)
    for line in logical_lines:
        previous = line.first
        bracket_stack = [previous] if previous.OpensScope() else []
        for tok in line.tokens[1:]:
            tok.previous_token = previous
            previous.next_token = tok
            previous = tok
            if tok.OpensScope():
                bracket_stack.append(tok)
            elif tok.ClosesScope():
                bracket_stack[-1].matching_bracket = tok
                tok.matching_bracket = bracket_stack.pop()
    return logical_lines