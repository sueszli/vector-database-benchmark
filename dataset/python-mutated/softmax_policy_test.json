[
    {
        "func_name": "test_softmax",
        "original": "@parameterized.named_parameters(('python', 'python_mfg_crowd_modelling'), ('cpp', 'mfg_crowd_modelling'))\ndef test_softmax(self, name):\n    \"\"\"Check if the softmax policy works as expected.\n\n    The test checks that:\n    - uniform prior policy gives the same results than no prior.\n    - very high temperature gives almost a uniform policy.\n    - very low temperature gives almost a deterministic policy for the best\n    action.\n\n    Args:\n      name: Name of the game.\n    \"\"\"\n    game = pyspiel.load_game(name)\n    uniform_policy = policy.UniformRandomPolicy(game)\n    dist = distribution.DistributionPolicy(game, uniform_policy)\n    br_value = best_response_value.BestResponse(game, dist, value.TabularValueFunction(game))\n    br_init_val = br_value(game.new_initial_state())\n    softmax_pi_uniform_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, uniform_policy).to_tabular()\n    softmax_pi_uniform_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_uniform_prior, value.TabularValueFunction(game))\n    softmax_pi_uniform_prior_init_val = softmax_pi_uniform_prior_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(softmax_pi_uniform_prior_init_val, softmax_pi_no_prior_init_val)\n    uniform_policy = uniform_policy.to_tabular()\n    uniform_value = policy_value.PolicyValue(game, dist, uniform_policy, value.TabularValueFunction(game))\n    uniform_init_val = uniform_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 100000000, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(uniform_init_val, softmax_pi_no_prior_init_val)\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 0.0001, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(br_init_val, softmax_pi_no_prior_init_val)",
        "mutated": [
            "@parameterized.named_parameters(('python', 'python_mfg_crowd_modelling'), ('cpp', 'mfg_crowd_modelling'))\ndef test_softmax(self, name):\n    if False:\n        i = 10\n    'Check if the softmax policy works as expected.\\n\\n    The test checks that:\\n    - uniform prior policy gives the same results than no prior.\\n    - very high temperature gives almost a uniform policy.\\n    - very low temperature gives almost a deterministic policy for the best\\n    action.\\n\\n    Args:\\n      name: Name of the game.\\n    '\n    game = pyspiel.load_game(name)\n    uniform_policy = policy.UniformRandomPolicy(game)\n    dist = distribution.DistributionPolicy(game, uniform_policy)\n    br_value = best_response_value.BestResponse(game, dist, value.TabularValueFunction(game))\n    br_init_val = br_value(game.new_initial_state())\n    softmax_pi_uniform_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, uniform_policy).to_tabular()\n    softmax_pi_uniform_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_uniform_prior, value.TabularValueFunction(game))\n    softmax_pi_uniform_prior_init_val = softmax_pi_uniform_prior_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(softmax_pi_uniform_prior_init_val, softmax_pi_no_prior_init_val)\n    uniform_policy = uniform_policy.to_tabular()\n    uniform_value = policy_value.PolicyValue(game, dist, uniform_policy, value.TabularValueFunction(game))\n    uniform_init_val = uniform_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 100000000, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(uniform_init_val, softmax_pi_no_prior_init_val)\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 0.0001, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(br_init_val, softmax_pi_no_prior_init_val)",
            "@parameterized.named_parameters(('python', 'python_mfg_crowd_modelling'), ('cpp', 'mfg_crowd_modelling'))\ndef test_softmax(self, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check if the softmax policy works as expected.\\n\\n    The test checks that:\\n    - uniform prior policy gives the same results than no prior.\\n    - very high temperature gives almost a uniform policy.\\n    - very low temperature gives almost a deterministic policy for the best\\n    action.\\n\\n    Args:\\n      name: Name of the game.\\n    '\n    game = pyspiel.load_game(name)\n    uniform_policy = policy.UniformRandomPolicy(game)\n    dist = distribution.DistributionPolicy(game, uniform_policy)\n    br_value = best_response_value.BestResponse(game, dist, value.TabularValueFunction(game))\n    br_init_val = br_value(game.new_initial_state())\n    softmax_pi_uniform_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, uniform_policy).to_tabular()\n    softmax_pi_uniform_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_uniform_prior, value.TabularValueFunction(game))\n    softmax_pi_uniform_prior_init_val = softmax_pi_uniform_prior_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(softmax_pi_uniform_prior_init_val, softmax_pi_no_prior_init_val)\n    uniform_policy = uniform_policy.to_tabular()\n    uniform_value = policy_value.PolicyValue(game, dist, uniform_policy, value.TabularValueFunction(game))\n    uniform_init_val = uniform_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 100000000, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(uniform_init_val, softmax_pi_no_prior_init_val)\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 0.0001, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(br_init_val, softmax_pi_no_prior_init_val)",
            "@parameterized.named_parameters(('python', 'python_mfg_crowd_modelling'), ('cpp', 'mfg_crowd_modelling'))\ndef test_softmax(self, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check if the softmax policy works as expected.\\n\\n    The test checks that:\\n    - uniform prior policy gives the same results than no prior.\\n    - very high temperature gives almost a uniform policy.\\n    - very low temperature gives almost a deterministic policy for the best\\n    action.\\n\\n    Args:\\n      name: Name of the game.\\n    '\n    game = pyspiel.load_game(name)\n    uniform_policy = policy.UniformRandomPolicy(game)\n    dist = distribution.DistributionPolicy(game, uniform_policy)\n    br_value = best_response_value.BestResponse(game, dist, value.TabularValueFunction(game))\n    br_init_val = br_value(game.new_initial_state())\n    softmax_pi_uniform_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, uniform_policy).to_tabular()\n    softmax_pi_uniform_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_uniform_prior, value.TabularValueFunction(game))\n    softmax_pi_uniform_prior_init_val = softmax_pi_uniform_prior_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(softmax_pi_uniform_prior_init_val, softmax_pi_no_prior_init_val)\n    uniform_policy = uniform_policy.to_tabular()\n    uniform_value = policy_value.PolicyValue(game, dist, uniform_policy, value.TabularValueFunction(game))\n    uniform_init_val = uniform_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 100000000, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(uniform_init_val, softmax_pi_no_prior_init_val)\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 0.0001, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(br_init_val, softmax_pi_no_prior_init_val)",
            "@parameterized.named_parameters(('python', 'python_mfg_crowd_modelling'), ('cpp', 'mfg_crowd_modelling'))\ndef test_softmax(self, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check if the softmax policy works as expected.\\n\\n    The test checks that:\\n    - uniform prior policy gives the same results than no prior.\\n    - very high temperature gives almost a uniform policy.\\n    - very low temperature gives almost a deterministic policy for the best\\n    action.\\n\\n    Args:\\n      name: Name of the game.\\n    '\n    game = pyspiel.load_game(name)\n    uniform_policy = policy.UniformRandomPolicy(game)\n    dist = distribution.DistributionPolicy(game, uniform_policy)\n    br_value = best_response_value.BestResponse(game, dist, value.TabularValueFunction(game))\n    br_init_val = br_value(game.new_initial_state())\n    softmax_pi_uniform_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, uniform_policy).to_tabular()\n    softmax_pi_uniform_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_uniform_prior, value.TabularValueFunction(game))\n    softmax_pi_uniform_prior_init_val = softmax_pi_uniform_prior_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(softmax_pi_uniform_prior_init_val, softmax_pi_no_prior_init_val)\n    uniform_policy = uniform_policy.to_tabular()\n    uniform_value = policy_value.PolicyValue(game, dist, uniform_policy, value.TabularValueFunction(game))\n    uniform_init_val = uniform_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 100000000, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(uniform_init_val, softmax_pi_no_prior_init_val)\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 0.0001, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(br_init_val, softmax_pi_no_prior_init_val)",
            "@parameterized.named_parameters(('python', 'python_mfg_crowd_modelling'), ('cpp', 'mfg_crowd_modelling'))\ndef test_softmax(self, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check if the softmax policy works as expected.\\n\\n    The test checks that:\\n    - uniform prior policy gives the same results than no prior.\\n    - very high temperature gives almost a uniform policy.\\n    - very low temperature gives almost a deterministic policy for the best\\n    action.\\n\\n    Args:\\n      name: Name of the game.\\n    '\n    game = pyspiel.load_game(name)\n    uniform_policy = policy.UniformRandomPolicy(game)\n    dist = distribution.DistributionPolicy(game, uniform_policy)\n    br_value = best_response_value.BestResponse(game, dist, value.TabularValueFunction(game))\n    br_init_val = br_value(game.new_initial_state())\n    softmax_pi_uniform_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, uniform_policy).to_tabular()\n    softmax_pi_uniform_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_uniform_prior, value.TabularValueFunction(game))\n    softmax_pi_uniform_prior_init_val = softmax_pi_uniform_prior_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 1.0, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(softmax_pi_uniform_prior_init_val, softmax_pi_no_prior_init_val)\n    uniform_policy = uniform_policy.to_tabular()\n    uniform_value = policy_value.PolicyValue(game, dist, uniform_policy, value.TabularValueFunction(game))\n    uniform_init_val = uniform_value(game.new_initial_state())\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 100000000, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(uniform_init_val, softmax_pi_no_prior_init_val)\n    softmax_pi_no_prior = softmax_policy.SoftmaxPolicy(game, None, 0.0001, br_value, None)\n    softmax_pi_no_prior_value = policy_value.PolicyValue(game, dist, softmax_pi_no_prior, value.TabularValueFunction(game))\n    softmax_pi_no_prior_init_val = softmax_pi_no_prior_value(game.new_initial_state())\n    self.assertAlmostEqual(br_init_val, softmax_pi_no_prior_init_val)"
        ]
    }
]