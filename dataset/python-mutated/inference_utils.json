[
    {
        "func_name": "quantize",
        "original": "@deprecated(func_name='model.quantize', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.quantize` instead.')\ndef quantize(self, x: Union[tf.Tensor, np.ndarray, tf.data.Dataset], y: Union[tf.Tensor, np.ndarray]=None, precision: str='int8', accelerator: Optional[str]=None, input_spec=None, metric: Optional[Metric]=None, accuracy_criterion: Optional[dict]=None, approach: str='static', method: Optional[str]=None, conf: Optional[str]=None, tuning_strategy: Optional[str]=None, timeout: Optional[int]=None, max_trials: Optional[int]=None, batch: Optional[int]=None, thread_num: Optional[int]=None, inputs: List[str]=None, outputs: List[str]=None, sample_size: int=100, onnxruntime_session_options=None, openvino_config=None, logging: bool=True):\n    \"\"\"\n        Post-training quantization on a keras model.\n\n        :param x: Input data which is used for training. It could be:\n\n                  | 1. a Numpy array (or array-like), or a list of arrays (in case the model\n                  | has multiple inputs).\n                  |\n                  | 2. a TensorFlow tensor, or a list of tensors (in case the model has\n                  | multiple inputs).\n                  |\n                  | 3. an unbatched tf.data.Dataset. Should return a tuple of (inputs, targets).\n\n                  X will be used as calibration dataset for Post-Training Static Quantization (PTQ),\n                  as well as be used for generating input_sample to calculate latency.\n                  To avoid data leak during calibration, please use training dataset.\n        :param y: Target data. Like the input data x, it could be either Numpy array(s) or\n                  TensorFlow tensor(s). Its length should be consistent with x.\n                  If x is a dataset, y will be ignored (since targets will be obtained from x).\n        :param precision:       Global precision of quantized model,\n                                supported type: 'int8', defaults to 'int8'.\n        :param accelerator:     Use accelerator 'None', 'onnxruntime', 'openvino', defaults to None.\n                                None means staying in tensorflow.\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\n                           defining the shape/dtype of the input. If ``accelerator='onnxruntime'``,\n                           ``input_spec`` is required. If ``accelerator='openvino'``, or\n                           ``accelerator=None`` and ``precision='int8'``, ``input_spec``\n                           is required when you have a custom Keras model.\n        :param metric:          A tensorflow.keras.metrics.Metric object for evaluation.\n        :param accuracy_criterion:  Tolerable accuracy drop.\n                                    accuracy_criterion = {'relative': 0.1, 'higher_is_better': True}\n                                    allows relative accuracy loss: 1%. accuracy_criterion =\n                                    {'absolute': 0.99, 'higher_is_better':False} means accuracy\n                                    must be smaller than 0.99.\n        :param approach:        'static' or 'dynamic'.\n                                'static': post_training_static_quant,\n                                'dynamic': post_training_dynamic_quant.\n                                Default: 'static'. Only 'static' approach is supported now.\n        :param method:      Method to do quantization. When accelerator=None, supported methods:\n                None. When accelerator='onnxruntime', supported methods: 'qlinear', 'integer',\n                defaults to 'qlinear'. Suggest 'qlinear' for lower accuracy drop if using\n                static quantization.\n                More details in https://onnxruntime.ai/docs/performance/quantization.html.\n                This argument doesn't take effect for OpenVINO, don't change it for OpenVINO.\n        :param conf:        A path to conf yaml file for quantization.\n                                Default: None, using default config.\n        :param tuning_strategy:    'bayesian', 'basic', 'mse', 'sigopt'. Default: 'bayesian'.\n        :param timeout:     Tuning timeout (seconds). Default: None,  which means early stop.\n                            Combine with max_trials field to decide when to exit.\n        :param max_trials:  Max tune times. Default: None, which means no tuning.\n                            Combine with timeout field to decide when to exit.\n                            \"timeout=0, max_trials=1\" means it will try quantization only once and\n                            return satisfying best model.\n        :param batch:       Batch size of dataloader for calib_dataset. Defaults to None, if the\n                            dataset is not a BatchDataset, batchsize equals to 1. Otherwise,\n                            batchsize complies with the dataset._batch_size.\n        :param thread_num:  (optional) a int represents how many threads(cores) is needed for\n                            inference, only valid for accelerator='onnxruntime'\n                            or accelerator='openvino'.\n        :param inputs:      A list of input names.\n                            Default: None, automatically get names from graph.\n        :param outputs:     A list of output names.\n                            Default: None, automatically get names from graph.\n        :param sample_size: (optional) a int represents how many samples will be used for\n                            Post-training Optimization Tools (POT) from OpenVINO toolkit,\n                            only valid for accelerator='openvino'. Default to 100.\n                            The larger the value, the more accurate the conversion,\n                            the lower the performance degradation, but the longer the time.\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\n                                            accelerator='onnxruntime', otherwise will be ignored.\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\n                                accelerator='openvino', otherwise will be ignored.\n        :param logging: whether to log detailed information of model conversion, only valid when\n                        accelerator='openvino', otherwise will be ignored. Default: ``True``.\n        :return:            A TensorflowBaseModel for INC. If there is no model found, return None.\n\n        .. warning::\n           This function will be deprecated in future release.\n\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.quantize`` instead.\n        \"\"\"\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.quantize(model=self, x=x, y=y, precision=precision, accelerator=accelerator, input_spec=input_spec, metric=metric, accuracy_criterion=accuracy_criterion, approach=approach, method=method, conf=conf, tuning_strategy=tuning_strategy, timeout=timeout, max_trials=max_trials, batch=batch, thread_num=thread_num, inputs=inputs, outputs=outputs, sample_size=sample_size, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)",
        "mutated": [
            "@deprecated(func_name='model.quantize', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.quantize` instead.')\ndef quantize(self, x: Union[tf.Tensor, np.ndarray, tf.data.Dataset], y: Union[tf.Tensor, np.ndarray]=None, precision: str='int8', accelerator: Optional[str]=None, input_spec=None, metric: Optional[Metric]=None, accuracy_criterion: Optional[dict]=None, approach: str='static', method: Optional[str]=None, conf: Optional[str]=None, tuning_strategy: Optional[str]=None, timeout: Optional[int]=None, max_trials: Optional[int]=None, batch: Optional[int]=None, thread_num: Optional[int]=None, inputs: List[str]=None, outputs: List[str]=None, sample_size: int=100, onnxruntime_session_options=None, openvino_config=None, logging: bool=True):\n    if False:\n        i = 10\n    '\\n        Post-training quantization on a keras model.\\n\\n        :param x: Input data which is used for training. It could be:\\n\\n                  | 1. a Numpy array (or array-like), or a list of arrays (in case the model\\n                  | has multiple inputs).\\n                  |\\n                  | 2. a TensorFlow tensor, or a list of tensors (in case the model has\\n                  | multiple inputs).\\n                  |\\n                  | 3. an unbatched tf.data.Dataset. Should return a tuple of (inputs, targets).\\n\\n                  X will be used as calibration dataset for Post-Training Static Quantization (PTQ),\\n                  as well as be used for generating input_sample to calculate latency.\\n                  To avoid data leak during calibration, please use training dataset.\\n        :param y: Target data. Like the input data x, it could be either Numpy array(s) or\\n                  TensorFlow tensor(s). Its length should be consistent with x.\\n                  If x is a dataset, y will be ignored (since targets will be obtained from x).\\n        :param precision:       Global precision of quantized model,\\n                                supported type: \\'int8\\', defaults to \\'int8\\'.\\n        :param accelerator:     Use accelerator \\'None\\', \\'onnxruntime\\', \\'openvino\\', defaults to None.\\n                                None means staying in tensorflow.\\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\\n                           defining the shape/dtype of the input. If ``accelerator=\\'onnxruntime\\'``,\\n                           ``input_spec`` is required. If ``accelerator=\\'openvino\\'``, or\\n                           ``accelerator=None`` and ``precision=\\'int8\\'``, ``input_spec``\\n                           is required when you have a custom Keras model.\\n        :param metric:          A tensorflow.keras.metrics.Metric object for evaluation.\\n        :param accuracy_criterion:  Tolerable accuracy drop.\\n                                    accuracy_criterion = {\\'relative\\': 0.1, \\'higher_is_better\\': True}\\n                                    allows relative accuracy loss: 1%. accuracy_criterion =\\n                                    {\\'absolute\\': 0.99, \\'higher_is_better\\':False} means accuracy\\n                                    must be smaller than 0.99.\\n        :param approach:        \\'static\\' or \\'dynamic\\'.\\n                                \\'static\\': post_training_static_quant,\\n                                \\'dynamic\\': post_training_dynamic_quant.\\n                                Default: \\'static\\'. Only \\'static\\' approach is supported now.\\n        :param method:      Method to do quantization. When accelerator=None, supported methods:\\n                None. When accelerator=\\'onnxruntime\\', supported methods: \\'qlinear\\', \\'integer\\',\\n                defaults to \\'qlinear\\'. Suggest \\'qlinear\\' for lower accuracy drop if using\\n                static quantization.\\n                More details in https://onnxruntime.ai/docs/performance/quantization.html.\\n                This argument doesn\\'t take effect for OpenVINO, don\\'t change it for OpenVINO.\\n        :param conf:        A path to conf yaml file for quantization.\\n                                Default: None, using default config.\\n        :param tuning_strategy:    \\'bayesian\\', \\'basic\\', \\'mse\\', \\'sigopt\\'. Default: \\'bayesian\\'.\\n        :param timeout:     Tuning timeout (seconds). Default: None,  which means early stop.\\n                            Combine with max_trials field to decide when to exit.\\n        :param max_trials:  Max tune times. Default: None, which means no tuning.\\n                            Combine with timeout field to decide when to exit.\\n                            \"timeout=0, max_trials=1\" means it will try quantization only once and\\n                            return satisfying best model.\\n        :param batch:       Batch size of dataloader for calib_dataset. Defaults to None, if the\\n                            dataset is not a BatchDataset, batchsize equals to 1. Otherwise,\\n                            batchsize complies with the dataset._batch_size.\\n        :param thread_num:  (optional) a int represents how many threads(cores) is needed for\\n                            inference, only valid for accelerator=\\'onnxruntime\\'\\n                            or accelerator=\\'openvino\\'.\\n        :param inputs:      A list of input names.\\n                            Default: None, automatically get names from graph.\\n        :param outputs:     A list of output names.\\n                            Default: None, automatically get names from graph.\\n        :param sample_size: (optional) a int represents how many samples will be used for\\n                            Post-training Optimization Tools (POT) from OpenVINO toolkit,\\n                            only valid for accelerator=\\'openvino\\'. Default to 100.\\n                            The larger the value, the more accurate the conversion,\\n                            the lower the performance degradation, but the longer the time.\\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\\n                                            accelerator=\\'onnxruntime\\', otherwise will be ignored.\\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\\n                                accelerator=\\'openvino\\', otherwise will be ignored.\\n        :param logging: whether to log detailed information of model conversion, only valid when\\n                        accelerator=\\'openvino\\', otherwise will be ignored. Default: ``True``.\\n        :return:            A TensorflowBaseModel for INC. If there is no model found, return None.\\n\\n        .. warning::\\n           This function will be deprecated in future release.\\n\\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.quantize`` instead.\\n        '\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.quantize(model=self, x=x, y=y, precision=precision, accelerator=accelerator, input_spec=input_spec, metric=metric, accuracy_criterion=accuracy_criterion, approach=approach, method=method, conf=conf, tuning_strategy=tuning_strategy, timeout=timeout, max_trials=max_trials, batch=batch, thread_num=thread_num, inputs=inputs, outputs=outputs, sample_size=sample_size, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)",
            "@deprecated(func_name='model.quantize', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.quantize` instead.')\ndef quantize(self, x: Union[tf.Tensor, np.ndarray, tf.data.Dataset], y: Union[tf.Tensor, np.ndarray]=None, precision: str='int8', accelerator: Optional[str]=None, input_spec=None, metric: Optional[Metric]=None, accuracy_criterion: Optional[dict]=None, approach: str='static', method: Optional[str]=None, conf: Optional[str]=None, tuning_strategy: Optional[str]=None, timeout: Optional[int]=None, max_trials: Optional[int]=None, batch: Optional[int]=None, thread_num: Optional[int]=None, inputs: List[str]=None, outputs: List[str]=None, sample_size: int=100, onnxruntime_session_options=None, openvino_config=None, logging: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Post-training quantization on a keras model.\\n\\n        :param x: Input data which is used for training. It could be:\\n\\n                  | 1. a Numpy array (or array-like), or a list of arrays (in case the model\\n                  | has multiple inputs).\\n                  |\\n                  | 2. a TensorFlow tensor, or a list of tensors (in case the model has\\n                  | multiple inputs).\\n                  |\\n                  | 3. an unbatched tf.data.Dataset. Should return a tuple of (inputs, targets).\\n\\n                  X will be used as calibration dataset for Post-Training Static Quantization (PTQ),\\n                  as well as be used for generating input_sample to calculate latency.\\n                  To avoid data leak during calibration, please use training dataset.\\n        :param y: Target data. Like the input data x, it could be either Numpy array(s) or\\n                  TensorFlow tensor(s). Its length should be consistent with x.\\n                  If x is a dataset, y will be ignored (since targets will be obtained from x).\\n        :param precision:       Global precision of quantized model,\\n                                supported type: \\'int8\\', defaults to \\'int8\\'.\\n        :param accelerator:     Use accelerator \\'None\\', \\'onnxruntime\\', \\'openvino\\', defaults to None.\\n                                None means staying in tensorflow.\\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\\n                           defining the shape/dtype of the input. If ``accelerator=\\'onnxruntime\\'``,\\n                           ``input_spec`` is required. If ``accelerator=\\'openvino\\'``, or\\n                           ``accelerator=None`` and ``precision=\\'int8\\'``, ``input_spec``\\n                           is required when you have a custom Keras model.\\n        :param metric:          A tensorflow.keras.metrics.Metric object for evaluation.\\n        :param accuracy_criterion:  Tolerable accuracy drop.\\n                                    accuracy_criterion = {\\'relative\\': 0.1, \\'higher_is_better\\': True}\\n                                    allows relative accuracy loss: 1%. accuracy_criterion =\\n                                    {\\'absolute\\': 0.99, \\'higher_is_better\\':False} means accuracy\\n                                    must be smaller than 0.99.\\n        :param approach:        \\'static\\' or \\'dynamic\\'.\\n                                \\'static\\': post_training_static_quant,\\n                                \\'dynamic\\': post_training_dynamic_quant.\\n                                Default: \\'static\\'. Only \\'static\\' approach is supported now.\\n        :param method:      Method to do quantization. When accelerator=None, supported methods:\\n                None. When accelerator=\\'onnxruntime\\', supported methods: \\'qlinear\\', \\'integer\\',\\n                defaults to \\'qlinear\\'. Suggest \\'qlinear\\' for lower accuracy drop if using\\n                static quantization.\\n                More details in https://onnxruntime.ai/docs/performance/quantization.html.\\n                This argument doesn\\'t take effect for OpenVINO, don\\'t change it for OpenVINO.\\n        :param conf:        A path to conf yaml file for quantization.\\n                                Default: None, using default config.\\n        :param tuning_strategy:    \\'bayesian\\', \\'basic\\', \\'mse\\', \\'sigopt\\'. Default: \\'bayesian\\'.\\n        :param timeout:     Tuning timeout (seconds). Default: None,  which means early stop.\\n                            Combine with max_trials field to decide when to exit.\\n        :param max_trials:  Max tune times. Default: None, which means no tuning.\\n                            Combine with timeout field to decide when to exit.\\n                            \"timeout=0, max_trials=1\" means it will try quantization only once and\\n                            return satisfying best model.\\n        :param batch:       Batch size of dataloader for calib_dataset. Defaults to None, if the\\n                            dataset is not a BatchDataset, batchsize equals to 1. Otherwise,\\n                            batchsize complies with the dataset._batch_size.\\n        :param thread_num:  (optional) a int represents how many threads(cores) is needed for\\n                            inference, only valid for accelerator=\\'onnxruntime\\'\\n                            or accelerator=\\'openvino\\'.\\n        :param inputs:      A list of input names.\\n                            Default: None, automatically get names from graph.\\n        :param outputs:     A list of output names.\\n                            Default: None, automatically get names from graph.\\n        :param sample_size: (optional) a int represents how many samples will be used for\\n                            Post-training Optimization Tools (POT) from OpenVINO toolkit,\\n                            only valid for accelerator=\\'openvino\\'. Default to 100.\\n                            The larger the value, the more accurate the conversion,\\n                            the lower the performance degradation, but the longer the time.\\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\\n                                            accelerator=\\'onnxruntime\\', otherwise will be ignored.\\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\\n                                accelerator=\\'openvino\\', otherwise will be ignored.\\n        :param logging: whether to log detailed information of model conversion, only valid when\\n                        accelerator=\\'openvino\\', otherwise will be ignored. Default: ``True``.\\n        :return:            A TensorflowBaseModel for INC. If there is no model found, return None.\\n\\n        .. warning::\\n           This function will be deprecated in future release.\\n\\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.quantize`` instead.\\n        '\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.quantize(model=self, x=x, y=y, precision=precision, accelerator=accelerator, input_spec=input_spec, metric=metric, accuracy_criterion=accuracy_criterion, approach=approach, method=method, conf=conf, tuning_strategy=tuning_strategy, timeout=timeout, max_trials=max_trials, batch=batch, thread_num=thread_num, inputs=inputs, outputs=outputs, sample_size=sample_size, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)",
            "@deprecated(func_name='model.quantize', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.quantize` instead.')\ndef quantize(self, x: Union[tf.Tensor, np.ndarray, tf.data.Dataset], y: Union[tf.Tensor, np.ndarray]=None, precision: str='int8', accelerator: Optional[str]=None, input_spec=None, metric: Optional[Metric]=None, accuracy_criterion: Optional[dict]=None, approach: str='static', method: Optional[str]=None, conf: Optional[str]=None, tuning_strategy: Optional[str]=None, timeout: Optional[int]=None, max_trials: Optional[int]=None, batch: Optional[int]=None, thread_num: Optional[int]=None, inputs: List[str]=None, outputs: List[str]=None, sample_size: int=100, onnxruntime_session_options=None, openvino_config=None, logging: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Post-training quantization on a keras model.\\n\\n        :param x: Input data which is used for training. It could be:\\n\\n                  | 1. a Numpy array (or array-like), or a list of arrays (in case the model\\n                  | has multiple inputs).\\n                  |\\n                  | 2. a TensorFlow tensor, or a list of tensors (in case the model has\\n                  | multiple inputs).\\n                  |\\n                  | 3. an unbatched tf.data.Dataset. Should return a tuple of (inputs, targets).\\n\\n                  X will be used as calibration dataset for Post-Training Static Quantization (PTQ),\\n                  as well as be used for generating input_sample to calculate latency.\\n                  To avoid data leak during calibration, please use training dataset.\\n        :param y: Target data. Like the input data x, it could be either Numpy array(s) or\\n                  TensorFlow tensor(s). Its length should be consistent with x.\\n                  If x is a dataset, y will be ignored (since targets will be obtained from x).\\n        :param precision:       Global precision of quantized model,\\n                                supported type: \\'int8\\', defaults to \\'int8\\'.\\n        :param accelerator:     Use accelerator \\'None\\', \\'onnxruntime\\', \\'openvino\\', defaults to None.\\n                                None means staying in tensorflow.\\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\\n                           defining the shape/dtype of the input. If ``accelerator=\\'onnxruntime\\'``,\\n                           ``input_spec`` is required. If ``accelerator=\\'openvino\\'``, or\\n                           ``accelerator=None`` and ``precision=\\'int8\\'``, ``input_spec``\\n                           is required when you have a custom Keras model.\\n        :param metric:          A tensorflow.keras.metrics.Metric object for evaluation.\\n        :param accuracy_criterion:  Tolerable accuracy drop.\\n                                    accuracy_criterion = {\\'relative\\': 0.1, \\'higher_is_better\\': True}\\n                                    allows relative accuracy loss: 1%. accuracy_criterion =\\n                                    {\\'absolute\\': 0.99, \\'higher_is_better\\':False} means accuracy\\n                                    must be smaller than 0.99.\\n        :param approach:        \\'static\\' or \\'dynamic\\'.\\n                                \\'static\\': post_training_static_quant,\\n                                \\'dynamic\\': post_training_dynamic_quant.\\n                                Default: \\'static\\'. Only \\'static\\' approach is supported now.\\n        :param method:      Method to do quantization. When accelerator=None, supported methods:\\n                None. When accelerator=\\'onnxruntime\\', supported methods: \\'qlinear\\', \\'integer\\',\\n                defaults to \\'qlinear\\'. Suggest \\'qlinear\\' for lower accuracy drop if using\\n                static quantization.\\n                More details in https://onnxruntime.ai/docs/performance/quantization.html.\\n                This argument doesn\\'t take effect for OpenVINO, don\\'t change it for OpenVINO.\\n        :param conf:        A path to conf yaml file for quantization.\\n                                Default: None, using default config.\\n        :param tuning_strategy:    \\'bayesian\\', \\'basic\\', \\'mse\\', \\'sigopt\\'. Default: \\'bayesian\\'.\\n        :param timeout:     Tuning timeout (seconds). Default: None,  which means early stop.\\n                            Combine with max_trials field to decide when to exit.\\n        :param max_trials:  Max tune times. Default: None, which means no tuning.\\n                            Combine with timeout field to decide when to exit.\\n                            \"timeout=0, max_trials=1\" means it will try quantization only once and\\n                            return satisfying best model.\\n        :param batch:       Batch size of dataloader for calib_dataset. Defaults to None, if the\\n                            dataset is not a BatchDataset, batchsize equals to 1. Otherwise,\\n                            batchsize complies with the dataset._batch_size.\\n        :param thread_num:  (optional) a int represents how many threads(cores) is needed for\\n                            inference, only valid for accelerator=\\'onnxruntime\\'\\n                            or accelerator=\\'openvino\\'.\\n        :param inputs:      A list of input names.\\n                            Default: None, automatically get names from graph.\\n        :param outputs:     A list of output names.\\n                            Default: None, automatically get names from graph.\\n        :param sample_size: (optional) a int represents how many samples will be used for\\n                            Post-training Optimization Tools (POT) from OpenVINO toolkit,\\n                            only valid for accelerator=\\'openvino\\'. Default to 100.\\n                            The larger the value, the more accurate the conversion,\\n                            the lower the performance degradation, but the longer the time.\\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\\n                                            accelerator=\\'onnxruntime\\', otherwise will be ignored.\\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\\n                                accelerator=\\'openvino\\', otherwise will be ignored.\\n        :param logging: whether to log detailed information of model conversion, only valid when\\n                        accelerator=\\'openvino\\', otherwise will be ignored. Default: ``True``.\\n        :return:            A TensorflowBaseModel for INC. If there is no model found, return None.\\n\\n        .. warning::\\n           This function will be deprecated in future release.\\n\\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.quantize`` instead.\\n        '\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.quantize(model=self, x=x, y=y, precision=precision, accelerator=accelerator, input_spec=input_spec, metric=metric, accuracy_criterion=accuracy_criterion, approach=approach, method=method, conf=conf, tuning_strategy=tuning_strategy, timeout=timeout, max_trials=max_trials, batch=batch, thread_num=thread_num, inputs=inputs, outputs=outputs, sample_size=sample_size, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)",
            "@deprecated(func_name='model.quantize', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.quantize` instead.')\ndef quantize(self, x: Union[tf.Tensor, np.ndarray, tf.data.Dataset], y: Union[tf.Tensor, np.ndarray]=None, precision: str='int8', accelerator: Optional[str]=None, input_spec=None, metric: Optional[Metric]=None, accuracy_criterion: Optional[dict]=None, approach: str='static', method: Optional[str]=None, conf: Optional[str]=None, tuning_strategy: Optional[str]=None, timeout: Optional[int]=None, max_trials: Optional[int]=None, batch: Optional[int]=None, thread_num: Optional[int]=None, inputs: List[str]=None, outputs: List[str]=None, sample_size: int=100, onnxruntime_session_options=None, openvino_config=None, logging: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Post-training quantization on a keras model.\\n\\n        :param x: Input data which is used for training. It could be:\\n\\n                  | 1. a Numpy array (or array-like), or a list of arrays (in case the model\\n                  | has multiple inputs).\\n                  |\\n                  | 2. a TensorFlow tensor, or a list of tensors (in case the model has\\n                  | multiple inputs).\\n                  |\\n                  | 3. an unbatched tf.data.Dataset. Should return a tuple of (inputs, targets).\\n\\n                  X will be used as calibration dataset for Post-Training Static Quantization (PTQ),\\n                  as well as be used for generating input_sample to calculate latency.\\n                  To avoid data leak during calibration, please use training dataset.\\n        :param y: Target data. Like the input data x, it could be either Numpy array(s) or\\n                  TensorFlow tensor(s). Its length should be consistent with x.\\n                  If x is a dataset, y will be ignored (since targets will be obtained from x).\\n        :param precision:       Global precision of quantized model,\\n                                supported type: \\'int8\\', defaults to \\'int8\\'.\\n        :param accelerator:     Use accelerator \\'None\\', \\'onnxruntime\\', \\'openvino\\', defaults to None.\\n                                None means staying in tensorflow.\\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\\n                           defining the shape/dtype of the input. If ``accelerator=\\'onnxruntime\\'``,\\n                           ``input_spec`` is required. If ``accelerator=\\'openvino\\'``, or\\n                           ``accelerator=None`` and ``precision=\\'int8\\'``, ``input_spec``\\n                           is required when you have a custom Keras model.\\n        :param metric:          A tensorflow.keras.metrics.Metric object for evaluation.\\n        :param accuracy_criterion:  Tolerable accuracy drop.\\n                                    accuracy_criterion = {\\'relative\\': 0.1, \\'higher_is_better\\': True}\\n                                    allows relative accuracy loss: 1%. accuracy_criterion =\\n                                    {\\'absolute\\': 0.99, \\'higher_is_better\\':False} means accuracy\\n                                    must be smaller than 0.99.\\n        :param approach:        \\'static\\' or \\'dynamic\\'.\\n                                \\'static\\': post_training_static_quant,\\n                                \\'dynamic\\': post_training_dynamic_quant.\\n                                Default: \\'static\\'. Only \\'static\\' approach is supported now.\\n        :param method:      Method to do quantization. When accelerator=None, supported methods:\\n                None. When accelerator=\\'onnxruntime\\', supported methods: \\'qlinear\\', \\'integer\\',\\n                defaults to \\'qlinear\\'. Suggest \\'qlinear\\' for lower accuracy drop if using\\n                static quantization.\\n                More details in https://onnxruntime.ai/docs/performance/quantization.html.\\n                This argument doesn\\'t take effect for OpenVINO, don\\'t change it for OpenVINO.\\n        :param conf:        A path to conf yaml file for quantization.\\n                                Default: None, using default config.\\n        :param tuning_strategy:    \\'bayesian\\', \\'basic\\', \\'mse\\', \\'sigopt\\'. Default: \\'bayesian\\'.\\n        :param timeout:     Tuning timeout (seconds). Default: None,  which means early stop.\\n                            Combine with max_trials field to decide when to exit.\\n        :param max_trials:  Max tune times. Default: None, which means no tuning.\\n                            Combine with timeout field to decide when to exit.\\n                            \"timeout=0, max_trials=1\" means it will try quantization only once and\\n                            return satisfying best model.\\n        :param batch:       Batch size of dataloader for calib_dataset. Defaults to None, if the\\n                            dataset is not a BatchDataset, batchsize equals to 1. Otherwise,\\n                            batchsize complies with the dataset._batch_size.\\n        :param thread_num:  (optional) a int represents how many threads(cores) is needed for\\n                            inference, only valid for accelerator=\\'onnxruntime\\'\\n                            or accelerator=\\'openvino\\'.\\n        :param inputs:      A list of input names.\\n                            Default: None, automatically get names from graph.\\n        :param outputs:     A list of output names.\\n                            Default: None, automatically get names from graph.\\n        :param sample_size: (optional) a int represents how many samples will be used for\\n                            Post-training Optimization Tools (POT) from OpenVINO toolkit,\\n                            only valid for accelerator=\\'openvino\\'. Default to 100.\\n                            The larger the value, the more accurate the conversion,\\n                            the lower the performance degradation, but the longer the time.\\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\\n                                            accelerator=\\'onnxruntime\\', otherwise will be ignored.\\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\\n                                accelerator=\\'openvino\\', otherwise will be ignored.\\n        :param logging: whether to log detailed information of model conversion, only valid when\\n                        accelerator=\\'openvino\\', otherwise will be ignored. Default: ``True``.\\n        :return:            A TensorflowBaseModel for INC. If there is no model found, return None.\\n\\n        .. warning::\\n           This function will be deprecated in future release.\\n\\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.quantize`` instead.\\n        '\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.quantize(model=self, x=x, y=y, precision=precision, accelerator=accelerator, input_spec=input_spec, metric=metric, accuracy_criterion=accuracy_criterion, approach=approach, method=method, conf=conf, tuning_strategy=tuning_strategy, timeout=timeout, max_trials=max_trials, batch=batch, thread_num=thread_num, inputs=inputs, outputs=outputs, sample_size=sample_size, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)",
            "@deprecated(func_name='model.quantize', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.quantize` instead.')\ndef quantize(self, x: Union[tf.Tensor, np.ndarray, tf.data.Dataset], y: Union[tf.Tensor, np.ndarray]=None, precision: str='int8', accelerator: Optional[str]=None, input_spec=None, metric: Optional[Metric]=None, accuracy_criterion: Optional[dict]=None, approach: str='static', method: Optional[str]=None, conf: Optional[str]=None, tuning_strategy: Optional[str]=None, timeout: Optional[int]=None, max_trials: Optional[int]=None, batch: Optional[int]=None, thread_num: Optional[int]=None, inputs: List[str]=None, outputs: List[str]=None, sample_size: int=100, onnxruntime_session_options=None, openvino_config=None, logging: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Post-training quantization on a keras model.\\n\\n        :param x: Input data which is used for training. It could be:\\n\\n                  | 1. a Numpy array (or array-like), or a list of arrays (in case the model\\n                  | has multiple inputs).\\n                  |\\n                  | 2. a TensorFlow tensor, or a list of tensors (in case the model has\\n                  | multiple inputs).\\n                  |\\n                  | 3. an unbatched tf.data.Dataset. Should return a tuple of (inputs, targets).\\n\\n                  X will be used as calibration dataset for Post-Training Static Quantization (PTQ),\\n                  as well as be used for generating input_sample to calculate latency.\\n                  To avoid data leak during calibration, please use training dataset.\\n        :param y: Target data. Like the input data x, it could be either Numpy array(s) or\\n                  TensorFlow tensor(s). Its length should be consistent with x.\\n                  If x is a dataset, y will be ignored (since targets will be obtained from x).\\n        :param precision:       Global precision of quantized model,\\n                                supported type: \\'int8\\', defaults to \\'int8\\'.\\n        :param accelerator:     Use accelerator \\'None\\', \\'onnxruntime\\', \\'openvino\\', defaults to None.\\n                                None means staying in tensorflow.\\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\\n                           defining the shape/dtype of the input. If ``accelerator=\\'onnxruntime\\'``,\\n                           ``input_spec`` is required. If ``accelerator=\\'openvino\\'``, or\\n                           ``accelerator=None`` and ``precision=\\'int8\\'``, ``input_spec``\\n                           is required when you have a custom Keras model.\\n        :param metric:          A tensorflow.keras.metrics.Metric object for evaluation.\\n        :param accuracy_criterion:  Tolerable accuracy drop.\\n                                    accuracy_criterion = {\\'relative\\': 0.1, \\'higher_is_better\\': True}\\n                                    allows relative accuracy loss: 1%. accuracy_criterion =\\n                                    {\\'absolute\\': 0.99, \\'higher_is_better\\':False} means accuracy\\n                                    must be smaller than 0.99.\\n        :param approach:        \\'static\\' or \\'dynamic\\'.\\n                                \\'static\\': post_training_static_quant,\\n                                \\'dynamic\\': post_training_dynamic_quant.\\n                                Default: \\'static\\'. Only \\'static\\' approach is supported now.\\n        :param method:      Method to do quantization. When accelerator=None, supported methods:\\n                None. When accelerator=\\'onnxruntime\\', supported methods: \\'qlinear\\', \\'integer\\',\\n                defaults to \\'qlinear\\'. Suggest \\'qlinear\\' for lower accuracy drop if using\\n                static quantization.\\n                More details in https://onnxruntime.ai/docs/performance/quantization.html.\\n                This argument doesn\\'t take effect for OpenVINO, don\\'t change it for OpenVINO.\\n        :param conf:        A path to conf yaml file for quantization.\\n                                Default: None, using default config.\\n        :param tuning_strategy:    \\'bayesian\\', \\'basic\\', \\'mse\\', \\'sigopt\\'. Default: \\'bayesian\\'.\\n        :param timeout:     Tuning timeout (seconds). Default: None,  which means early stop.\\n                            Combine with max_trials field to decide when to exit.\\n        :param max_trials:  Max tune times. Default: None, which means no tuning.\\n                            Combine with timeout field to decide when to exit.\\n                            \"timeout=0, max_trials=1\" means it will try quantization only once and\\n                            return satisfying best model.\\n        :param batch:       Batch size of dataloader for calib_dataset. Defaults to None, if the\\n                            dataset is not a BatchDataset, batchsize equals to 1. Otherwise,\\n                            batchsize complies with the dataset._batch_size.\\n        :param thread_num:  (optional) a int represents how many threads(cores) is needed for\\n                            inference, only valid for accelerator=\\'onnxruntime\\'\\n                            or accelerator=\\'openvino\\'.\\n        :param inputs:      A list of input names.\\n                            Default: None, automatically get names from graph.\\n        :param outputs:     A list of output names.\\n                            Default: None, automatically get names from graph.\\n        :param sample_size: (optional) a int represents how many samples will be used for\\n                            Post-training Optimization Tools (POT) from OpenVINO toolkit,\\n                            only valid for accelerator=\\'openvino\\'. Default to 100.\\n                            The larger the value, the more accurate the conversion,\\n                            the lower the performance degradation, but the longer the time.\\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\\n                                            accelerator=\\'onnxruntime\\', otherwise will be ignored.\\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\\n                                accelerator=\\'openvino\\', otherwise will be ignored.\\n        :param logging: whether to log detailed information of model conversion, only valid when\\n                        accelerator=\\'openvino\\', otherwise will be ignored. Default: ``True``.\\n        :return:            A TensorflowBaseModel for INC. If there is no model found, return None.\\n\\n        .. warning::\\n           This function will be deprecated in future release.\\n\\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.quantize`` instead.\\n        '\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.quantize(model=self, x=x, y=y, precision=precision, accelerator=accelerator, input_spec=input_spec, metric=metric, accuracy_criterion=accuracy_criterion, approach=approach, method=method, conf=conf, tuning_strategy=tuning_strategy, timeout=timeout, max_trials=max_trials, batch=batch, thread_num=thread_num, inputs=inputs, outputs=outputs, sample_size=sample_size, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)"
        ]
    },
    {
        "func_name": "trace",
        "original": "@deprecated(func_name='model.trace', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.trace` instead.')\ndef trace(self, accelerator: Optional[str]=None, input_spec=None, thread_num: Optional[int]=None, onnxruntime_session_options=None, openvino_config=None, logging=True):\n    \"\"\"\n        Trace a Keras model and convert it into an accelerated module for inference.\n\n        :param accelerator: The accelerator to use, defaults to None meaning staying in Keras\n                            backend. 'openvino' and 'onnxruntime' are supported for now.\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\n                           defining the shape/dtype of the input. If ``accelerator='onnxruntime'``,\n                           ``input_spec`` is required. If ``accelerator='openvino'``,\n                           ``input_spec`` is only required when you have a custom Keras model.\n        :param thread_num: (optional) a int represents how many threads(cores) is needed for\n                           inference, only valid for accelerator='onnxruntime'\n                           or accelerator='openvino'.\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\n                                            accelerator='onnxruntime', otherwise will be ignored.\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\n                                accelerator='openvino', otherwise will be ignored.\n        :param logging: whether to log detailed information of model conversion, only valid when\n                        accelerator='openvino', otherwise will be ignored. Default: ``True``.\n        :return: Model with different acceleration(OpenVINO/ONNX Runtime).\n\n        .. warning::\n           This function will be deprecated in future release.\n\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.trace`` instead.\n        \"\"\"\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.trace(model=self, accelerator=accelerator, input_spec=input_spec, thread_num=thread_num, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)",
        "mutated": [
            "@deprecated(func_name='model.trace', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.trace` instead.')\ndef trace(self, accelerator: Optional[str]=None, input_spec=None, thread_num: Optional[int]=None, onnxruntime_session_options=None, openvino_config=None, logging=True):\n    if False:\n        i = 10\n    \"\\n        Trace a Keras model and convert it into an accelerated module for inference.\\n\\n        :param accelerator: The accelerator to use, defaults to None meaning staying in Keras\\n                            backend. 'openvino' and 'onnxruntime' are supported for now.\\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\\n                           defining the shape/dtype of the input. If ``accelerator='onnxruntime'``,\\n                           ``input_spec`` is required. If ``accelerator='openvino'``,\\n                           ``input_spec`` is only required when you have a custom Keras model.\\n        :param thread_num: (optional) a int represents how many threads(cores) is needed for\\n                           inference, only valid for accelerator='onnxruntime'\\n                           or accelerator='openvino'.\\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\\n                                            accelerator='onnxruntime', otherwise will be ignored.\\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\\n                                accelerator='openvino', otherwise will be ignored.\\n        :param logging: whether to log detailed information of model conversion, only valid when\\n                        accelerator='openvino', otherwise will be ignored. Default: ``True``.\\n        :return: Model with different acceleration(OpenVINO/ONNX Runtime).\\n\\n        .. warning::\\n           This function will be deprecated in future release.\\n\\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.trace`` instead.\\n        \"\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.trace(model=self, accelerator=accelerator, input_spec=input_spec, thread_num=thread_num, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)",
            "@deprecated(func_name='model.trace', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.trace` instead.')\ndef trace(self, accelerator: Optional[str]=None, input_spec=None, thread_num: Optional[int]=None, onnxruntime_session_options=None, openvino_config=None, logging=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Trace a Keras model and convert it into an accelerated module for inference.\\n\\n        :param accelerator: The accelerator to use, defaults to None meaning staying in Keras\\n                            backend. 'openvino' and 'onnxruntime' are supported for now.\\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\\n                           defining the shape/dtype of the input. If ``accelerator='onnxruntime'``,\\n                           ``input_spec`` is required. If ``accelerator='openvino'``,\\n                           ``input_spec`` is only required when you have a custom Keras model.\\n        :param thread_num: (optional) a int represents how many threads(cores) is needed for\\n                           inference, only valid for accelerator='onnxruntime'\\n                           or accelerator='openvino'.\\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\\n                                            accelerator='onnxruntime', otherwise will be ignored.\\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\\n                                accelerator='openvino', otherwise will be ignored.\\n        :param logging: whether to log detailed information of model conversion, only valid when\\n                        accelerator='openvino', otherwise will be ignored. Default: ``True``.\\n        :return: Model with different acceleration(OpenVINO/ONNX Runtime).\\n\\n        .. warning::\\n           This function will be deprecated in future release.\\n\\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.trace`` instead.\\n        \"\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.trace(model=self, accelerator=accelerator, input_spec=input_spec, thread_num=thread_num, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)",
            "@deprecated(func_name='model.trace', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.trace` instead.')\ndef trace(self, accelerator: Optional[str]=None, input_spec=None, thread_num: Optional[int]=None, onnxruntime_session_options=None, openvino_config=None, logging=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Trace a Keras model and convert it into an accelerated module for inference.\\n\\n        :param accelerator: The accelerator to use, defaults to None meaning staying in Keras\\n                            backend. 'openvino' and 'onnxruntime' are supported for now.\\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\\n                           defining the shape/dtype of the input. If ``accelerator='onnxruntime'``,\\n                           ``input_spec`` is required. If ``accelerator='openvino'``,\\n                           ``input_spec`` is only required when you have a custom Keras model.\\n        :param thread_num: (optional) a int represents how many threads(cores) is needed for\\n                           inference, only valid for accelerator='onnxruntime'\\n                           or accelerator='openvino'.\\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\\n                                            accelerator='onnxruntime', otherwise will be ignored.\\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\\n                                accelerator='openvino', otherwise will be ignored.\\n        :param logging: whether to log detailed information of model conversion, only valid when\\n                        accelerator='openvino', otherwise will be ignored. Default: ``True``.\\n        :return: Model with different acceleration(OpenVINO/ONNX Runtime).\\n\\n        .. warning::\\n           This function will be deprecated in future release.\\n\\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.trace`` instead.\\n        \"\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.trace(model=self, accelerator=accelerator, input_spec=input_spec, thread_num=thread_num, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)",
            "@deprecated(func_name='model.trace', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.trace` instead.')\ndef trace(self, accelerator: Optional[str]=None, input_spec=None, thread_num: Optional[int]=None, onnxruntime_session_options=None, openvino_config=None, logging=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Trace a Keras model and convert it into an accelerated module for inference.\\n\\n        :param accelerator: The accelerator to use, defaults to None meaning staying in Keras\\n                            backend. 'openvino' and 'onnxruntime' are supported for now.\\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\\n                           defining the shape/dtype of the input. If ``accelerator='onnxruntime'``,\\n                           ``input_spec`` is required. If ``accelerator='openvino'``,\\n                           ``input_spec`` is only required when you have a custom Keras model.\\n        :param thread_num: (optional) a int represents how many threads(cores) is needed for\\n                           inference, only valid for accelerator='onnxruntime'\\n                           or accelerator='openvino'.\\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\\n                                            accelerator='onnxruntime', otherwise will be ignored.\\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\\n                                accelerator='openvino', otherwise will be ignored.\\n        :param logging: whether to log detailed information of model conversion, only valid when\\n                        accelerator='openvino', otherwise will be ignored. Default: ``True``.\\n        :return: Model with different acceleration(OpenVINO/ONNX Runtime).\\n\\n        .. warning::\\n           This function will be deprecated in future release.\\n\\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.trace`` instead.\\n        \"\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.trace(model=self, accelerator=accelerator, input_spec=input_spec, thread_num=thread_num, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)",
            "@deprecated(func_name='model.trace', message='Please use `bigdl.nano.tf.keras.InferenceOptimizer.trace` instead.')\ndef trace(self, accelerator: Optional[str]=None, input_spec=None, thread_num: Optional[int]=None, onnxruntime_session_options=None, openvino_config=None, logging=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Trace a Keras model and convert it into an accelerated module for inference.\\n\\n        :param accelerator: The accelerator to use, defaults to None meaning staying in Keras\\n                            backend. 'openvino' and 'onnxruntime' are supported for now.\\n        :param input_spec: (optional) A (tuple or list of) ``tf.TensorSpec``\\n                           defining the shape/dtype of the input. If ``accelerator='onnxruntime'``,\\n                           ``input_spec`` is required. If ``accelerator='openvino'``,\\n                           ``input_spec`` is only required when you have a custom Keras model.\\n        :param thread_num: (optional) a int represents how many threads(cores) is needed for\\n                           inference, only valid for accelerator='onnxruntime'\\n                           or accelerator='openvino'.\\n        :param onnxruntime_session_options: The session option for onnxruntime, only valid when\\n                                            accelerator='onnxruntime', otherwise will be ignored.\\n        :param openvino_config: The config to be inputted in core.compile_model. Only valid when\\n                                accelerator='openvino', otherwise will be ignored.\\n        :param logging: whether to log detailed information of model conversion, only valid when\\n                        accelerator='openvino', otherwise will be ignored. Default: ``True``.\\n        :return: Model with different acceleration(OpenVINO/ONNX Runtime).\\n\\n        .. warning::\\n           This function will be deprecated in future release.\\n\\n           Please use ``bigdl.nano.tf.keras.InferenceOptimizer.trace`` instead.\\n        \"\n    from bigdl.nano.tf.keras import InferenceOptimizer\n    return InferenceOptimizer.trace(model=self, accelerator=accelerator, input_spec=input_spec, thread_num=thread_num, onnxruntime_session_options=onnxruntime_session_options, openvino_config=openvino_config, logging=logging)"
        ]
    }
]