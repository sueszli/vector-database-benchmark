[
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    pass",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "_opt_wtd_nuis_regress",
        "original": "def _opt_wtd_nuis_regress(self, test_vals):\n    \"\"\"\n        A function that is optimized over nuisance parameters to conduct a\n        hypothesis test for the parameters of interest\n\n        Parameters\n        ----------\n\n        params: 1d array\n            The regression coefficients of the model.  This includes the\n            nuisance and parameters of interests.\n\n        Returns\n        -------\n        llr : float\n            -2 times the log likelihood of the nuisance parameters and the\n            hypothesized value of the parameter(s) of interest.\n        \"\"\"\n    test_params = test_vals.reshape(self.model.nvar, 1)\n    est_vect = self.model.uncens_exog * (self.model.uncens_endog - np.dot(self.model.uncens_exog, test_params))\n    eta_star = self._modif_newton(np.zeros(self.model.nvar), est_vect, self.model._fit_weights)\n    denom = np.sum(self.model._fit_weights) + np.dot(eta_star, est_vect.T)\n    self.new_weights = self.model._fit_weights / denom\n    return -1 * np.sum(np.log(self.new_weights))",
        "mutated": [
            "def _opt_wtd_nuis_regress(self, test_vals):\n    if False:\n        i = 10\n    '\\n        A function that is optimized over nuisance parameters to conduct a\\n        hypothesis test for the parameters of interest\\n\\n        Parameters\\n        ----------\\n\\n        params: 1d array\\n            The regression coefficients of the model.  This includes the\\n            nuisance and parameters of interests.\\n\\n        Returns\\n        -------\\n        llr : float\\n            -2 times the log likelihood of the nuisance parameters and the\\n            hypothesized value of the parameter(s) of interest.\\n        '\n    test_params = test_vals.reshape(self.model.nvar, 1)\n    est_vect = self.model.uncens_exog * (self.model.uncens_endog - np.dot(self.model.uncens_exog, test_params))\n    eta_star = self._modif_newton(np.zeros(self.model.nvar), est_vect, self.model._fit_weights)\n    denom = np.sum(self.model._fit_weights) + np.dot(eta_star, est_vect.T)\n    self.new_weights = self.model._fit_weights / denom\n    return -1 * np.sum(np.log(self.new_weights))",
            "def _opt_wtd_nuis_regress(self, test_vals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        A function that is optimized over nuisance parameters to conduct a\\n        hypothesis test for the parameters of interest\\n\\n        Parameters\\n        ----------\\n\\n        params: 1d array\\n            The regression coefficients of the model.  This includes the\\n            nuisance and parameters of interests.\\n\\n        Returns\\n        -------\\n        llr : float\\n            -2 times the log likelihood of the nuisance parameters and the\\n            hypothesized value of the parameter(s) of interest.\\n        '\n    test_params = test_vals.reshape(self.model.nvar, 1)\n    est_vect = self.model.uncens_exog * (self.model.uncens_endog - np.dot(self.model.uncens_exog, test_params))\n    eta_star = self._modif_newton(np.zeros(self.model.nvar), est_vect, self.model._fit_weights)\n    denom = np.sum(self.model._fit_weights) + np.dot(eta_star, est_vect.T)\n    self.new_weights = self.model._fit_weights / denom\n    return -1 * np.sum(np.log(self.new_weights))",
            "def _opt_wtd_nuis_regress(self, test_vals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        A function that is optimized over nuisance parameters to conduct a\\n        hypothesis test for the parameters of interest\\n\\n        Parameters\\n        ----------\\n\\n        params: 1d array\\n            The regression coefficients of the model.  This includes the\\n            nuisance and parameters of interests.\\n\\n        Returns\\n        -------\\n        llr : float\\n            -2 times the log likelihood of the nuisance parameters and the\\n            hypothesized value of the parameter(s) of interest.\\n        '\n    test_params = test_vals.reshape(self.model.nvar, 1)\n    est_vect = self.model.uncens_exog * (self.model.uncens_endog - np.dot(self.model.uncens_exog, test_params))\n    eta_star = self._modif_newton(np.zeros(self.model.nvar), est_vect, self.model._fit_weights)\n    denom = np.sum(self.model._fit_weights) + np.dot(eta_star, est_vect.T)\n    self.new_weights = self.model._fit_weights / denom\n    return -1 * np.sum(np.log(self.new_weights))",
            "def _opt_wtd_nuis_regress(self, test_vals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        A function that is optimized over nuisance parameters to conduct a\\n        hypothesis test for the parameters of interest\\n\\n        Parameters\\n        ----------\\n\\n        params: 1d array\\n            The regression coefficients of the model.  This includes the\\n            nuisance and parameters of interests.\\n\\n        Returns\\n        -------\\n        llr : float\\n            -2 times the log likelihood of the nuisance parameters and the\\n            hypothesized value of the parameter(s) of interest.\\n        '\n    test_params = test_vals.reshape(self.model.nvar, 1)\n    est_vect = self.model.uncens_exog * (self.model.uncens_endog - np.dot(self.model.uncens_exog, test_params))\n    eta_star = self._modif_newton(np.zeros(self.model.nvar), est_vect, self.model._fit_weights)\n    denom = np.sum(self.model._fit_weights) + np.dot(eta_star, est_vect.T)\n    self.new_weights = self.model._fit_weights / denom\n    return -1 * np.sum(np.log(self.new_weights))",
            "def _opt_wtd_nuis_regress(self, test_vals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        A function that is optimized over nuisance parameters to conduct a\\n        hypothesis test for the parameters of interest\\n\\n        Parameters\\n        ----------\\n\\n        params: 1d array\\n            The regression coefficients of the model.  This includes the\\n            nuisance and parameters of interests.\\n\\n        Returns\\n        -------\\n        llr : float\\n            -2 times the log likelihood of the nuisance parameters and the\\n            hypothesized value of the parameter(s) of interest.\\n        '\n    test_params = test_vals.reshape(self.model.nvar, 1)\n    est_vect = self.model.uncens_exog * (self.model.uncens_endog - np.dot(self.model.uncens_exog, test_params))\n    eta_star = self._modif_newton(np.zeros(self.model.nvar), est_vect, self.model._fit_weights)\n    denom = np.sum(self.model._fit_weights) + np.dot(eta_star, est_vect.T)\n    self.new_weights = self.model._fit_weights / denom\n    return -1 * np.sum(np.log(self.new_weights))"
        ]
    },
    {
        "func_name": "_EM_test",
        "original": "def _EM_test(self, nuisance_params, params=None, param_nums=None, b0_vals=None, F=None, survidx=None, uncens_nobs=None, numcensbelow=None, km=None, uncensored=None, censored=None, maxiter=None, ftol=None):\n    \"\"\"\n        Uses EM algorithm to compute the maximum likelihood of a test\n\n        Parameters\n        ----------\n\n        nuisance_params : ndarray\n            Vector of values to be used as nuisance params.\n\n        maxiter : int\n            Number of iterations in the EM algorithm for a parameter vector\n\n        Returns\n        -------\n        -2 ''*'' log likelihood ratio at hypothesized values and\n        nuisance params\n\n        Notes\n        -----\n        Optional parameters are provided by the test_beta function.\n        \"\"\"\n    iters = 0\n    params[param_nums] = b0_vals\n    nuis_param_index = np.int_(np.delete(np.arange(self.model.nvar), param_nums))\n    params[nuis_param_index] = nuisance_params\n    to_test = params.reshape(self.model.nvar, 1)\n    opt_res = np.inf\n    diff = np.inf\n    while iters < maxiter and diff > ftol:\n        F = F.flatten()\n        death = np.cumsum(F[::-1])\n        survivalprob = death[::-1]\n        surv_point_mat = np.dot(F.reshape(-1, 1), 1.0 / survivalprob[survidx].reshape(1, -1))\n        surv_point_mat = add_constant(surv_point_mat)\n        summed_wts = np.cumsum(surv_point_mat, axis=1)\n        wts = summed_wts[np.int_(np.arange(uncens_nobs)), numcensbelow[uncensored]]\n        self.model._fit_weights = wts\n        new_opt_res = self._opt_wtd_nuis_regress(to_test)\n        F = self.new_weights\n        diff = np.abs(new_opt_res - opt_res)\n        opt_res = new_opt_res\n        iters = iters + 1\n    death = np.cumsum(F.flatten()[::-1])\n    survivalprob = death[::-1]\n    llike = -opt_res + np.sum(np.log(survivalprob[survidx]))\n    wtd_km = km.flatten() / np.sum(km)\n    survivalmax = np.cumsum(wtd_km[::-1])[::-1]\n    llikemax = np.sum(np.log(wtd_km[uncensored])) + np.sum(np.log(survivalmax[censored]))\n    if iters == maxiter:\n        warnings.warn('The EM reached the maximum number of iterations', IterationLimitWarning)\n    return -2 * (llike - llikemax)",
        "mutated": [
            "def _EM_test(self, nuisance_params, params=None, param_nums=None, b0_vals=None, F=None, survidx=None, uncens_nobs=None, numcensbelow=None, km=None, uncensored=None, censored=None, maxiter=None, ftol=None):\n    if False:\n        i = 10\n    \"\\n        Uses EM algorithm to compute the maximum likelihood of a test\\n\\n        Parameters\\n        ----------\\n\\n        nuisance_params : ndarray\\n            Vector of values to be used as nuisance params.\\n\\n        maxiter : int\\n            Number of iterations in the EM algorithm for a parameter vector\\n\\n        Returns\\n        -------\\n        -2 ''*'' log likelihood ratio at hypothesized values and\\n        nuisance params\\n\\n        Notes\\n        -----\\n        Optional parameters are provided by the test_beta function.\\n        \"\n    iters = 0\n    params[param_nums] = b0_vals\n    nuis_param_index = np.int_(np.delete(np.arange(self.model.nvar), param_nums))\n    params[nuis_param_index] = nuisance_params\n    to_test = params.reshape(self.model.nvar, 1)\n    opt_res = np.inf\n    diff = np.inf\n    while iters < maxiter and diff > ftol:\n        F = F.flatten()\n        death = np.cumsum(F[::-1])\n        survivalprob = death[::-1]\n        surv_point_mat = np.dot(F.reshape(-1, 1), 1.0 / survivalprob[survidx].reshape(1, -1))\n        surv_point_mat = add_constant(surv_point_mat)\n        summed_wts = np.cumsum(surv_point_mat, axis=1)\n        wts = summed_wts[np.int_(np.arange(uncens_nobs)), numcensbelow[uncensored]]\n        self.model._fit_weights = wts\n        new_opt_res = self._opt_wtd_nuis_regress(to_test)\n        F = self.new_weights\n        diff = np.abs(new_opt_res - opt_res)\n        opt_res = new_opt_res\n        iters = iters + 1\n    death = np.cumsum(F.flatten()[::-1])\n    survivalprob = death[::-1]\n    llike = -opt_res + np.sum(np.log(survivalprob[survidx]))\n    wtd_km = km.flatten() / np.sum(km)\n    survivalmax = np.cumsum(wtd_km[::-1])[::-1]\n    llikemax = np.sum(np.log(wtd_km[uncensored])) + np.sum(np.log(survivalmax[censored]))\n    if iters == maxiter:\n        warnings.warn('The EM reached the maximum number of iterations', IterationLimitWarning)\n    return -2 * (llike - llikemax)",
            "def _EM_test(self, nuisance_params, params=None, param_nums=None, b0_vals=None, F=None, survidx=None, uncens_nobs=None, numcensbelow=None, km=None, uncensored=None, censored=None, maxiter=None, ftol=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Uses EM algorithm to compute the maximum likelihood of a test\\n\\n        Parameters\\n        ----------\\n\\n        nuisance_params : ndarray\\n            Vector of values to be used as nuisance params.\\n\\n        maxiter : int\\n            Number of iterations in the EM algorithm for a parameter vector\\n\\n        Returns\\n        -------\\n        -2 ''*'' log likelihood ratio at hypothesized values and\\n        nuisance params\\n\\n        Notes\\n        -----\\n        Optional parameters are provided by the test_beta function.\\n        \"\n    iters = 0\n    params[param_nums] = b0_vals\n    nuis_param_index = np.int_(np.delete(np.arange(self.model.nvar), param_nums))\n    params[nuis_param_index] = nuisance_params\n    to_test = params.reshape(self.model.nvar, 1)\n    opt_res = np.inf\n    diff = np.inf\n    while iters < maxiter and diff > ftol:\n        F = F.flatten()\n        death = np.cumsum(F[::-1])\n        survivalprob = death[::-1]\n        surv_point_mat = np.dot(F.reshape(-1, 1), 1.0 / survivalprob[survidx].reshape(1, -1))\n        surv_point_mat = add_constant(surv_point_mat)\n        summed_wts = np.cumsum(surv_point_mat, axis=1)\n        wts = summed_wts[np.int_(np.arange(uncens_nobs)), numcensbelow[uncensored]]\n        self.model._fit_weights = wts\n        new_opt_res = self._opt_wtd_nuis_regress(to_test)\n        F = self.new_weights\n        diff = np.abs(new_opt_res - opt_res)\n        opt_res = new_opt_res\n        iters = iters + 1\n    death = np.cumsum(F.flatten()[::-1])\n    survivalprob = death[::-1]\n    llike = -opt_res + np.sum(np.log(survivalprob[survidx]))\n    wtd_km = km.flatten() / np.sum(km)\n    survivalmax = np.cumsum(wtd_km[::-1])[::-1]\n    llikemax = np.sum(np.log(wtd_km[uncensored])) + np.sum(np.log(survivalmax[censored]))\n    if iters == maxiter:\n        warnings.warn('The EM reached the maximum number of iterations', IterationLimitWarning)\n    return -2 * (llike - llikemax)",
            "def _EM_test(self, nuisance_params, params=None, param_nums=None, b0_vals=None, F=None, survidx=None, uncens_nobs=None, numcensbelow=None, km=None, uncensored=None, censored=None, maxiter=None, ftol=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Uses EM algorithm to compute the maximum likelihood of a test\\n\\n        Parameters\\n        ----------\\n\\n        nuisance_params : ndarray\\n            Vector of values to be used as nuisance params.\\n\\n        maxiter : int\\n            Number of iterations in the EM algorithm for a parameter vector\\n\\n        Returns\\n        -------\\n        -2 ''*'' log likelihood ratio at hypothesized values and\\n        nuisance params\\n\\n        Notes\\n        -----\\n        Optional parameters are provided by the test_beta function.\\n        \"\n    iters = 0\n    params[param_nums] = b0_vals\n    nuis_param_index = np.int_(np.delete(np.arange(self.model.nvar), param_nums))\n    params[nuis_param_index] = nuisance_params\n    to_test = params.reshape(self.model.nvar, 1)\n    opt_res = np.inf\n    diff = np.inf\n    while iters < maxiter and diff > ftol:\n        F = F.flatten()\n        death = np.cumsum(F[::-1])\n        survivalprob = death[::-1]\n        surv_point_mat = np.dot(F.reshape(-1, 1), 1.0 / survivalprob[survidx].reshape(1, -1))\n        surv_point_mat = add_constant(surv_point_mat)\n        summed_wts = np.cumsum(surv_point_mat, axis=1)\n        wts = summed_wts[np.int_(np.arange(uncens_nobs)), numcensbelow[uncensored]]\n        self.model._fit_weights = wts\n        new_opt_res = self._opt_wtd_nuis_regress(to_test)\n        F = self.new_weights\n        diff = np.abs(new_opt_res - opt_res)\n        opt_res = new_opt_res\n        iters = iters + 1\n    death = np.cumsum(F.flatten()[::-1])\n    survivalprob = death[::-1]\n    llike = -opt_res + np.sum(np.log(survivalprob[survidx]))\n    wtd_km = km.flatten() / np.sum(km)\n    survivalmax = np.cumsum(wtd_km[::-1])[::-1]\n    llikemax = np.sum(np.log(wtd_km[uncensored])) + np.sum(np.log(survivalmax[censored]))\n    if iters == maxiter:\n        warnings.warn('The EM reached the maximum number of iterations', IterationLimitWarning)\n    return -2 * (llike - llikemax)",
            "def _EM_test(self, nuisance_params, params=None, param_nums=None, b0_vals=None, F=None, survidx=None, uncens_nobs=None, numcensbelow=None, km=None, uncensored=None, censored=None, maxiter=None, ftol=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Uses EM algorithm to compute the maximum likelihood of a test\\n\\n        Parameters\\n        ----------\\n\\n        nuisance_params : ndarray\\n            Vector of values to be used as nuisance params.\\n\\n        maxiter : int\\n            Number of iterations in the EM algorithm for a parameter vector\\n\\n        Returns\\n        -------\\n        -2 ''*'' log likelihood ratio at hypothesized values and\\n        nuisance params\\n\\n        Notes\\n        -----\\n        Optional parameters are provided by the test_beta function.\\n        \"\n    iters = 0\n    params[param_nums] = b0_vals\n    nuis_param_index = np.int_(np.delete(np.arange(self.model.nvar), param_nums))\n    params[nuis_param_index] = nuisance_params\n    to_test = params.reshape(self.model.nvar, 1)\n    opt_res = np.inf\n    diff = np.inf\n    while iters < maxiter and diff > ftol:\n        F = F.flatten()\n        death = np.cumsum(F[::-1])\n        survivalprob = death[::-1]\n        surv_point_mat = np.dot(F.reshape(-1, 1), 1.0 / survivalprob[survidx].reshape(1, -1))\n        surv_point_mat = add_constant(surv_point_mat)\n        summed_wts = np.cumsum(surv_point_mat, axis=1)\n        wts = summed_wts[np.int_(np.arange(uncens_nobs)), numcensbelow[uncensored]]\n        self.model._fit_weights = wts\n        new_opt_res = self._opt_wtd_nuis_regress(to_test)\n        F = self.new_weights\n        diff = np.abs(new_opt_res - opt_res)\n        opt_res = new_opt_res\n        iters = iters + 1\n    death = np.cumsum(F.flatten()[::-1])\n    survivalprob = death[::-1]\n    llike = -opt_res + np.sum(np.log(survivalprob[survidx]))\n    wtd_km = km.flatten() / np.sum(km)\n    survivalmax = np.cumsum(wtd_km[::-1])[::-1]\n    llikemax = np.sum(np.log(wtd_km[uncensored])) + np.sum(np.log(survivalmax[censored]))\n    if iters == maxiter:\n        warnings.warn('The EM reached the maximum number of iterations', IterationLimitWarning)\n    return -2 * (llike - llikemax)",
            "def _EM_test(self, nuisance_params, params=None, param_nums=None, b0_vals=None, F=None, survidx=None, uncens_nobs=None, numcensbelow=None, km=None, uncensored=None, censored=None, maxiter=None, ftol=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Uses EM algorithm to compute the maximum likelihood of a test\\n\\n        Parameters\\n        ----------\\n\\n        nuisance_params : ndarray\\n            Vector of values to be used as nuisance params.\\n\\n        maxiter : int\\n            Number of iterations in the EM algorithm for a parameter vector\\n\\n        Returns\\n        -------\\n        -2 ''*'' log likelihood ratio at hypothesized values and\\n        nuisance params\\n\\n        Notes\\n        -----\\n        Optional parameters are provided by the test_beta function.\\n        \"\n    iters = 0\n    params[param_nums] = b0_vals\n    nuis_param_index = np.int_(np.delete(np.arange(self.model.nvar), param_nums))\n    params[nuis_param_index] = nuisance_params\n    to_test = params.reshape(self.model.nvar, 1)\n    opt_res = np.inf\n    diff = np.inf\n    while iters < maxiter and diff > ftol:\n        F = F.flatten()\n        death = np.cumsum(F[::-1])\n        survivalprob = death[::-1]\n        surv_point_mat = np.dot(F.reshape(-1, 1), 1.0 / survivalprob[survidx].reshape(1, -1))\n        surv_point_mat = add_constant(surv_point_mat)\n        summed_wts = np.cumsum(surv_point_mat, axis=1)\n        wts = summed_wts[np.int_(np.arange(uncens_nobs)), numcensbelow[uncensored]]\n        self.model._fit_weights = wts\n        new_opt_res = self._opt_wtd_nuis_regress(to_test)\n        F = self.new_weights\n        diff = np.abs(new_opt_res - opt_res)\n        opt_res = new_opt_res\n        iters = iters + 1\n    death = np.cumsum(F.flatten()[::-1])\n    survivalprob = death[::-1]\n    llike = -opt_res + np.sum(np.log(survivalprob[survidx]))\n    wtd_km = km.flatten() / np.sum(km)\n    survivalmax = np.cumsum(wtd_km[::-1])[::-1]\n    llikemax = np.sum(np.log(wtd_km[uncensored])) + np.sum(np.log(survivalmax[censored]))\n    if iters == maxiter:\n        warnings.warn('The EM reached the maximum number of iterations', IterationLimitWarning)\n    return -2 * (llike - llikemax)"
        ]
    },
    {
        "func_name": "_ci_limits_beta",
        "original": "def _ci_limits_beta(self, b0, param_num=None):\n    \"\"\"\n        Returns the difference between the log likelihood for a\n        parameter and some critical value.\n\n        Parameters\n        ----------\n        b0: float\n            Value of a regression parameter\n        param_num : int\n            Parameter index of b0\n        \"\"\"\n    return self.test_beta([b0], [param_num])[0] - self.r0",
        "mutated": [
            "def _ci_limits_beta(self, b0, param_num=None):\n    if False:\n        i = 10\n    '\\n        Returns the difference between the log likelihood for a\\n        parameter and some critical value.\\n\\n        Parameters\\n        ----------\\n        b0: float\\n            Value of a regression parameter\\n        param_num : int\\n            Parameter index of b0\\n        '\n    return self.test_beta([b0], [param_num])[0] - self.r0",
            "def _ci_limits_beta(self, b0, param_num=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the difference between the log likelihood for a\\n        parameter and some critical value.\\n\\n        Parameters\\n        ----------\\n        b0: float\\n            Value of a regression parameter\\n        param_num : int\\n            Parameter index of b0\\n        '\n    return self.test_beta([b0], [param_num])[0] - self.r0",
            "def _ci_limits_beta(self, b0, param_num=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the difference between the log likelihood for a\\n        parameter and some critical value.\\n\\n        Parameters\\n        ----------\\n        b0: float\\n            Value of a regression parameter\\n        param_num : int\\n            Parameter index of b0\\n        '\n    return self.test_beta([b0], [param_num])[0] - self.r0",
            "def _ci_limits_beta(self, b0, param_num=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the difference between the log likelihood for a\\n        parameter and some critical value.\\n\\n        Parameters\\n        ----------\\n        b0: float\\n            Value of a regression parameter\\n        param_num : int\\n            Parameter index of b0\\n        '\n    return self.test_beta([b0], [param_num])[0] - self.r0",
            "def _ci_limits_beta(self, b0, param_num=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the difference between the log likelihood for a\\n        parameter and some critical value.\\n\\n        Parameters\\n        ----------\\n        b0: float\\n            Value of a regression parameter\\n        param_num : int\\n            Parameter index of b0\\n        '\n    return self.test_beta([b0], [param_num])[0] - self.r0"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, endog, exog, censors):\n    self.nobs = np.shape(exog)[0]\n    self.endog = endog.reshape(self.nobs, 1)\n    self.exog = exog.reshape(self.nobs, -1)\n    self.censors = np.asarray(censors).reshape(self.nobs, 1)\n    self.nvar = self.exog.shape[1]\n    idx = np.lexsort((-self.censors[:, 0], self.endog[:, 0]))\n    self.endog = self.endog[idx]\n    self.exog = self.exog[idx]\n    self.censors = self.censors[idx]\n    self.censors[-1] = 1\n    self.uncens_nobs = int(np.sum(self.censors))\n    mask = self.censors.ravel().astype(bool)\n    self.uncens_endog = self.endog[mask, :].reshape(-1, 1)\n    self.uncens_exog = self.exog[mask, :]",
        "mutated": [
            "def __init__(self, endog, exog, censors):\n    if False:\n        i = 10\n    self.nobs = np.shape(exog)[0]\n    self.endog = endog.reshape(self.nobs, 1)\n    self.exog = exog.reshape(self.nobs, -1)\n    self.censors = np.asarray(censors).reshape(self.nobs, 1)\n    self.nvar = self.exog.shape[1]\n    idx = np.lexsort((-self.censors[:, 0], self.endog[:, 0]))\n    self.endog = self.endog[idx]\n    self.exog = self.exog[idx]\n    self.censors = self.censors[idx]\n    self.censors[-1] = 1\n    self.uncens_nobs = int(np.sum(self.censors))\n    mask = self.censors.ravel().astype(bool)\n    self.uncens_endog = self.endog[mask, :].reshape(-1, 1)\n    self.uncens_exog = self.exog[mask, :]",
            "def __init__(self, endog, exog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.nobs = np.shape(exog)[0]\n    self.endog = endog.reshape(self.nobs, 1)\n    self.exog = exog.reshape(self.nobs, -1)\n    self.censors = np.asarray(censors).reshape(self.nobs, 1)\n    self.nvar = self.exog.shape[1]\n    idx = np.lexsort((-self.censors[:, 0], self.endog[:, 0]))\n    self.endog = self.endog[idx]\n    self.exog = self.exog[idx]\n    self.censors = self.censors[idx]\n    self.censors[-1] = 1\n    self.uncens_nobs = int(np.sum(self.censors))\n    mask = self.censors.ravel().astype(bool)\n    self.uncens_endog = self.endog[mask, :].reshape(-1, 1)\n    self.uncens_exog = self.exog[mask, :]",
            "def __init__(self, endog, exog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.nobs = np.shape(exog)[0]\n    self.endog = endog.reshape(self.nobs, 1)\n    self.exog = exog.reshape(self.nobs, -1)\n    self.censors = np.asarray(censors).reshape(self.nobs, 1)\n    self.nvar = self.exog.shape[1]\n    idx = np.lexsort((-self.censors[:, 0], self.endog[:, 0]))\n    self.endog = self.endog[idx]\n    self.exog = self.exog[idx]\n    self.censors = self.censors[idx]\n    self.censors[-1] = 1\n    self.uncens_nobs = int(np.sum(self.censors))\n    mask = self.censors.ravel().astype(bool)\n    self.uncens_endog = self.endog[mask, :].reshape(-1, 1)\n    self.uncens_exog = self.exog[mask, :]",
            "def __init__(self, endog, exog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.nobs = np.shape(exog)[0]\n    self.endog = endog.reshape(self.nobs, 1)\n    self.exog = exog.reshape(self.nobs, -1)\n    self.censors = np.asarray(censors).reshape(self.nobs, 1)\n    self.nvar = self.exog.shape[1]\n    idx = np.lexsort((-self.censors[:, 0], self.endog[:, 0]))\n    self.endog = self.endog[idx]\n    self.exog = self.exog[idx]\n    self.censors = self.censors[idx]\n    self.censors[-1] = 1\n    self.uncens_nobs = int(np.sum(self.censors))\n    mask = self.censors.ravel().astype(bool)\n    self.uncens_endog = self.endog[mask, :].reshape(-1, 1)\n    self.uncens_exog = self.exog[mask, :]",
            "def __init__(self, endog, exog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.nobs = np.shape(exog)[0]\n    self.endog = endog.reshape(self.nobs, 1)\n    self.exog = exog.reshape(self.nobs, -1)\n    self.censors = np.asarray(censors).reshape(self.nobs, 1)\n    self.nvar = self.exog.shape[1]\n    idx = np.lexsort((-self.censors[:, 0], self.endog[:, 0]))\n    self.endog = self.endog[idx]\n    self.exog = self.exog[idx]\n    self.censors = self.censors[idx]\n    self.censors[-1] = 1\n    self.uncens_nobs = int(np.sum(self.censors))\n    mask = self.censors.ravel().astype(bool)\n    self.uncens_endog = self.endog[mask, :].reshape(-1, 1)\n    self.uncens_exog = self.exog[mask, :]"
        ]
    },
    {
        "func_name": "_is_tied",
        "original": "def _is_tied(self, endog, censors):\n    \"\"\"\n        Indicated if an observation takes the same value as the next\n        ordered observation.\n\n        Parameters\n        ----------\n        endog : ndarray\n            Models endogenous variable\n        censors : ndarray\n            arrat indicating a censored array\n\n        Returns\n        -------\n        indic_ties : ndarray\n            ties[i]=1 if endog[i]==endog[i+1] and\n            censors[i]=censors[i+1]\n        \"\"\"\n    nobs = int(self.nobs)\n    endog_idx = endog[np.arange(nobs - 1)] == endog[np.arange(nobs - 1) + 1]\n    censors_idx = censors[np.arange(nobs - 1)] == censors[np.arange(nobs - 1) + 1]\n    indic_ties = endog_idx * censors_idx\n    return np.int_(indic_ties)",
        "mutated": [
            "def _is_tied(self, endog, censors):\n    if False:\n        i = 10\n    '\\n        Indicated if an observation takes the same value as the next\\n        ordered observation.\\n\\n        Parameters\\n        ----------\\n        endog : ndarray\\n            Models endogenous variable\\n        censors : ndarray\\n            arrat indicating a censored array\\n\\n        Returns\\n        -------\\n        indic_ties : ndarray\\n            ties[i]=1 if endog[i]==endog[i+1] and\\n            censors[i]=censors[i+1]\\n        '\n    nobs = int(self.nobs)\n    endog_idx = endog[np.arange(nobs - 1)] == endog[np.arange(nobs - 1) + 1]\n    censors_idx = censors[np.arange(nobs - 1)] == censors[np.arange(nobs - 1) + 1]\n    indic_ties = endog_idx * censors_idx\n    return np.int_(indic_ties)",
            "def _is_tied(self, endog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Indicated if an observation takes the same value as the next\\n        ordered observation.\\n\\n        Parameters\\n        ----------\\n        endog : ndarray\\n            Models endogenous variable\\n        censors : ndarray\\n            arrat indicating a censored array\\n\\n        Returns\\n        -------\\n        indic_ties : ndarray\\n            ties[i]=1 if endog[i]==endog[i+1] and\\n            censors[i]=censors[i+1]\\n        '\n    nobs = int(self.nobs)\n    endog_idx = endog[np.arange(nobs - 1)] == endog[np.arange(nobs - 1) + 1]\n    censors_idx = censors[np.arange(nobs - 1)] == censors[np.arange(nobs - 1) + 1]\n    indic_ties = endog_idx * censors_idx\n    return np.int_(indic_ties)",
            "def _is_tied(self, endog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Indicated if an observation takes the same value as the next\\n        ordered observation.\\n\\n        Parameters\\n        ----------\\n        endog : ndarray\\n            Models endogenous variable\\n        censors : ndarray\\n            arrat indicating a censored array\\n\\n        Returns\\n        -------\\n        indic_ties : ndarray\\n            ties[i]=1 if endog[i]==endog[i+1] and\\n            censors[i]=censors[i+1]\\n        '\n    nobs = int(self.nobs)\n    endog_idx = endog[np.arange(nobs - 1)] == endog[np.arange(nobs - 1) + 1]\n    censors_idx = censors[np.arange(nobs - 1)] == censors[np.arange(nobs - 1) + 1]\n    indic_ties = endog_idx * censors_idx\n    return np.int_(indic_ties)",
            "def _is_tied(self, endog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Indicated if an observation takes the same value as the next\\n        ordered observation.\\n\\n        Parameters\\n        ----------\\n        endog : ndarray\\n            Models endogenous variable\\n        censors : ndarray\\n            arrat indicating a censored array\\n\\n        Returns\\n        -------\\n        indic_ties : ndarray\\n            ties[i]=1 if endog[i]==endog[i+1] and\\n            censors[i]=censors[i+1]\\n        '\n    nobs = int(self.nobs)\n    endog_idx = endog[np.arange(nobs - 1)] == endog[np.arange(nobs - 1) + 1]\n    censors_idx = censors[np.arange(nobs - 1)] == censors[np.arange(nobs - 1) + 1]\n    indic_ties = endog_idx * censors_idx\n    return np.int_(indic_ties)",
            "def _is_tied(self, endog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Indicated if an observation takes the same value as the next\\n        ordered observation.\\n\\n        Parameters\\n        ----------\\n        endog : ndarray\\n            Models endogenous variable\\n        censors : ndarray\\n            arrat indicating a censored array\\n\\n        Returns\\n        -------\\n        indic_ties : ndarray\\n            ties[i]=1 if endog[i]==endog[i+1] and\\n            censors[i]=censors[i+1]\\n        '\n    nobs = int(self.nobs)\n    endog_idx = endog[np.arange(nobs - 1)] == endog[np.arange(nobs - 1) + 1]\n    censors_idx = censors[np.arange(nobs - 1)] == censors[np.arange(nobs - 1) + 1]\n    indic_ties = endog_idx * censors_idx\n    return np.int_(indic_ties)"
        ]
    },
    {
        "func_name": "_km_w_ties",
        "original": "def _km_w_ties(self, tie_indic, untied_km):\n    \"\"\"\n        Computes KM estimator value at each observation, taking into acocunt\n        ties in the data.\n\n        Parameters\n        ----------\n        tie_indic: 1d array\n            Indicates if the i'th observation is the same as the ith +1\n        untied_km: 1d array\n            Km estimates at each observation assuming no ties.\n        \"\"\"\n    num_same = 1\n    idx_nums = []\n    for obs_num in np.arange(int(self.nobs - 1))[::-1]:\n        if tie_indic[obs_num] == 1:\n            idx_nums.append(obs_num)\n            num_same = num_same + 1\n            untied_km[obs_num] = untied_km[obs_num + 1]\n        elif tie_indic[obs_num] == 0 and num_same > 1:\n            idx_nums.append(max(idx_nums) + 1)\n            idx_nums = np.asarray(idx_nums)\n            untied_km[idx_nums] = untied_km[idx_nums]\n            num_same = 1\n            idx_nums = []\n    return untied_km.reshape(self.nobs, 1)",
        "mutated": [
            "def _km_w_ties(self, tie_indic, untied_km):\n    if False:\n        i = 10\n    \"\\n        Computes KM estimator value at each observation, taking into acocunt\\n        ties in the data.\\n\\n        Parameters\\n        ----------\\n        tie_indic: 1d array\\n            Indicates if the i'th observation is the same as the ith +1\\n        untied_km: 1d array\\n            Km estimates at each observation assuming no ties.\\n        \"\n    num_same = 1\n    idx_nums = []\n    for obs_num in np.arange(int(self.nobs - 1))[::-1]:\n        if tie_indic[obs_num] == 1:\n            idx_nums.append(obs_num)\n            num_same = num_same + 1\n            untied_km[obs_num] = untied_km[obs_num + 1]\n        elif tie_indic[obs_num] == 0 and num_same > 1:\n            idx_nums.append(max(idx_nums) + 1)\n            idx_nums = np.asarray(idx_nums)\n            untied_km[idx_nums] = untied_km[idx_nums]\n            num_same = 1\n            idx_nums = []\n    return untied_km.reshape(self.nobs, 1)",
            "def _km_w_ties(self, tie_indic, untied_km):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Computes KM estimator value at each observation, taking into acocunt\\n        ties in the data.\\n\\n        Parameters\\n        ----------\\n        tie_indic: 1d array\\n            Indicates if the i'th observation is the same as the ith +1\\n        untied_km: 1d array\\n            Km estimates at each observation assuming no ties.\\n        \"\n    num_same = 1\n    idx_nums = []\n    for obs_num in np.arange(int(self.nobs - 1))[::-1]:\n        if tie_indic[obs_num] == 1:\n            idx_nums.append(obs_num)\n            num_same = num_same + 1\n            untied_km[obs_num] = untied_km[obs_num + 1]\n        elif tie_indic[obs_num] == 0 and num_same > 1:\n            idx_nums.append(max(idx_nums) + 1)\n            idx_nums = np.asarray(idx_nums)\n            untied_km[idx_nums] = untied_km[idx_nums]\n            num_same = 1\n            idx_nums = []\n    return untied_km.reshape(self.nobs, 1)",
            "def _km_w_ties(self, tie_indic, untied_km):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Computes KM estimator value at each observation, taking into acocunt\\n        ties in the data.\\n\\n        Parameters\\n        ----------\\n        tie_indic: 1d array\\n            Indicates if the i'th observation is the same as the ith +1\\n        untied_km: 1d array\\n            Km estimates at each observation assuming no ties.\\n        \"\n    num_same = 1\n    idx_nums = []\n    for obs_num in np.arange(int(self.nobs - 1))[::-1]:\n        if tie_indic[obs_num] == 1:\n            idx_nums.append(obs_num)\n            num_same = num_same + 1\n            untied_km[obs_num] = untied_km[obs_num + 1]\n        elif tie_indic[obs_num] == 0 and num_same > 1:\n            idx_nums.append(max(idx_nums) + 1)\n            idx_nums = np.asarray(idx_nums)\n            untied_km[idx_nums] = untied_km[idx_nums]\n            num_same = 1\n            idx_nums = []\n    return untied_km.reshape(self.nobs, 1)",
            "def _km_w_ties(self, tie_indic, untied_km):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Computes KM estimator value at each observation, taking into acocunt\\n        ties in the data.\\n\\n        Parameters\\n        ----------\\n        tie_indic: 1d array\\n            Indicates if the i'th observation is the same as the ith +1\\n        untied_km: 1d array\\n            Km estimates at each observation assuming no ties.\\n        \"\n    num_same = 1\n    idx_nums = []\n    for obs_num in np.arange(int(self.nobs - 1))[::-1]:\n        if tie_indic[obs_num] == 1:\n            idx_nums.append(obs_num)\n            num_same = num_same + 1\n            untied_km[obs_num] = untied_km[obs_num + 1]\n        elif tie_indic[obs_num] == 0 and num_same > 1:\n            idx_nums.append(max(idx_nums) + 1)\n            idx_nums = np.asarray(idx_nums)\n            untied_km[idx_nums] = untied_km[idx_nums]\n            num_same = 1\n            idx_nums = []\n    return untied_km.reshape(self.nobs, 1)",
            "def _km_w_ties(self, tie_indic, untied_km):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Computes KM estimator value at each observation, taking into acocunt\\n        ties in the data.\\n\\n        Parameters\\n        ----------\\n        tie_indic: 1d array\\n            Indicates if the i'th observation is the same as the ith +1\\n        untied_km: 1d array\\n            Km estimates at each observation assuming no ties.\\n        \"\n    num_same = 1\n    idx_nums = []\n    for obs_num in np.arange(int(self.nobs - 1))[::-1]:\n        if tie_indic[obs_num] == 1:\n            idx_nums.append(obs_num)\n            num_same = num_same + 1\n            untied_km[obs_num] = untied_km[obs_num + 1]\n        elif tie_indic[obs_num] == 0 and num_same > 1:\n            idx_nums.append(max(idx_nums) + 1)\n            idx_nums = np.asarray(idx_nums)\n            untied_km[idx_nums] = untied_km[idx_nums]\n            num_same = 1\n            idx_nums = []\n    return untied_km.reshape(self.nobs, 1)"
        ]
    },
    {
        "func_name": "_make_km",
        "original": "def _make_km(self, endog, censors):\n    \"\"\"\n\n        Computes the Kaplan-Meier estimate for the weights in the AFT model\n\n        Parameters\n        ----------\n        endog: nx1 array\n            Array of response variables\n        censors: nx1 array\n            Censor-indicating variable\n\n        Returns\n        -------\n        Kaplan Meier estimate for each observation\n\n        Notes\n        -----\n\n        This function makes calls to _is_tied and km_w_ties to handle ties in\n        the data.If a censored observation and an uncensored observation has\n        the same value, it is assumed that the uncensored happened first.\n        \"\"\"\n    nobs = self.nobs\n    num = nobs - (np.arange(nobs) + 1.0)\n    denom = nobs - (np.arange(nobs) + 1.0) + 1.0\n    km = (num / denom).reshape(nobs, 1)\n    km = km ** np.abs(censors - 1.0)\n    km = np.cumprod(km)\n    tied = self._is_tied(endog, censors)\n    wtd_km = self._km_w_ties(tied, km)\n    return (censors / wtd_km).reshape(nobs, 1)",
        "mutated": [
            "def _make_km(self, endog, censors):\n    if False:\n        i = 10\n    '\\n\\n        Computes the Kaplan-Meier estimate for the weights in the AFT model\\n\\n        Parameters\\n        ----------\\n        endog: nx1 array\\n            Array of response variables\\n        censors: nx1 array\\n            Censor-indicating variable\\n\\n        Returns\\n        -------\\n        Kaplan Meier estimate for each observation\\n\\n        Notes\\n        -----\\n\\n        This function makes calls to _is_tied and km_w_ties to handle ties in\\n        the data.If a censored observation and an uncensored observation has\\n        the same value, it is assumed that the uncensored happened first.\\n        '\n    nobs = self.nobs\n    num = nobs - (np.arange(nobs) + 1.0)\n    denom = nobs - (np.arange(nobs) + 1.0) + 1.0\n    km = (num / denom).reshape(nobs, 1)\n    km = km ** np.abs(censors - 1.0)\n    km = np.cumprod(km)\n    tied = self._is_tied(endog, censors)\n    wtd_km = self._km_w_ties(tied, km)\n    return (censors / wtd_km).reshape(nobs, 1)",
            "def _make_km(self, endog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n        Computes the Kaplan-Meier estimate for the weights in the AFT model\\n\\n        Parameters\\n        ----------\\n        endog: nx1 array\\n            Array of response variables\\n        censors: nx1 array\\n            Censor-indicating variable\\n\\n        Returns\\n        -------\\n        Kaplan Meier estimate for each observation\\n\\n        Notes\\n        -----\\n\\n        This function makes calls to _is_tied and km_w_ties to handle ties in\\n        the data.If a censored observation and an uncensored observation has\\n        the same value, it is assumed that the uncensored happened first.\\n        '\n    nobs = self.nobs\n    num = nobs - (np.arange(nobs) + 1.0)\n    denom = nobs - (np.arange(nobs) + 1.0) + 1.0\n    km = (num / denom).reshape(nobs, 1)\n    km = km ** np.abs(censors - 1.0)\n    km = np.cumprod(km)\n    tied = self._is_tied(endog, censors)\n    wtd_km = self._km_w_ties(tied, km)\n    return (censors / wtd_km).reshape(nobs, 1)",
            "def _make_km(self, endog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n        Computes the Kaplan-Meier estimate for the weights in the AFT model\\n\\n        Parameters\\n        ----------\\n        endog: nx1 array\\n            Array of response variables\\n        censors: nx1 array\\n            Censor-indicating variable\\n\\n        Returns\\n        -------\\n        Kaplan Meier estimate for each observation\\n\\n        Notes\\n        -----\\n\\n        This function makes calls to _is_tied and km_w_ties to handle ties in\\n        the data.If a censored observation and an uncensored observation has\\n        the same value, it is assumed that the uncensored happened first.\\n        '\n    nobs = self.nobs\n    num = nobs - (np.arange(nobs) + 1.0)\n    denom = nobs - (np.arange(nobs) + 1.0) + 1.0\n    km = (num / denom).reshape(nobs, 1)\n    km = km ** np.abs(censors - 1.0)\n    km = np.cumprod(km)\n    tied = self._is_tied(endog, censors)\n    wtd_km = self._km_w_ties(tied, km)\n    return (censors / wtd_km).reshape(nobs, 1)",
            "def _make_km(self, endog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n        Computes the Kaplan-Meier estimate for the weights in the AFT model\\n\\n        Parameters\\n        ----------\\n        endog: nx1 array\\n            Array of response variables\\n        censors: nx1 array\\n            Censor-indicating variable\\n\\n        Returns\\n        -------\\n        Kaplan Meier estimate for each observation\\n\\n        Notes\\n        -----\\n\\n        This function makes calls to _is_tied and km_w_ties to handle ties in\\n        the data.If a censored observation and an uncensored observation has\\n        the same value, it is assumed that the uncensored happened first.\\n        '\n    nobs = self.nobs\n    num = nobs - (np.arange(nobs) + 1.0)\n    denom = nobs - (np.arange(nobs) + 1.0) + 1.0\n    km = (num / denom).reshape(nobs, 1)\n    km = km ** np.abs(censors - 1.0)\n    km = np.cumprod(km)\n    tied = self._is_tied(endog, censors)\n    wtd_km = self._km_w_ties(tied, km)\n    return (censors / wtd_km).reshape(nobs, 1)",
            "def _make_km(self, endog, censors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n        Computes the Kaplan-Meier estimate for the weights in the AFT model\\n\\n        Parameters\\n        ----------\\n        endog: nx1 array\\n            Array of response variables\\n        censors: nx1 array\\n            Censor-indicating variable\\n\\n        Returns\\n        -------\\n        Kaplan Meier estimate for each observation\\n\\n        Notes\\n        -----\\n\\n        This function makes calls to _is_tied and km_w_ties to handle ties in\\n        the data.If a censored observation and an uncensored observation has\\n        the same value, it is assumed that the uncensored happened first.\\n        '\n    nobs = self.nobs\n    num = nobs - (np.arange(nobs) + 1.0)\n    denom = nobs - (np.arange(nobs) + 1.0) + 1.0\n    km = (num / denom).reshape(nobs, 1)\n    km = km ** np.abs(censors - 1.0)\n    km = np.cumprod(km)\n    tied = self._is_tied(endog, censors)\n    wtd_km = self._km_w_ties(tied, km)\n    return (censors / wtd_km).reshape(nobs, 1)"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self):\n    \"\"\"\n\n        Fits an AFT model and returns results instance\n\n        Parameters\n        ----------\n        None\n\n\n        Returns\n        -------\n        Results instance.\n\n        Notes\n        -----\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\n        \"\"\"\n    return AFTResults(self)",
        "mutated": [
            "def fit(self):\n    if False:\n        i = 10\n    '\\n\\n        Fits an AFT model and returns results instance\\n\\n        Parameters\\n        ----------\\n        None\\n\\n\\n        Returns\\n        -------\\n        Results instance.\\n\\n        Notes\\n        -----\\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\\n        '\n    return AFTResults(self)",
            "def fit(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n        Fits an AFT model and returns results instance\\n\\n        Parameters\\n        ----------\\n        None\\n\\n\\n        Returns\\n        -------\\n        Results instance.\\n\\n        Notes\\n        -----\\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\\n        '\n    return AFTResults(self)",
            "def fit(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n        Fits an AFT model and returns results instance\\n\\n        Parameters\\n        ----------\\n        None\\n\\n\\n        Returns\\n        -------\\n        Results instance.\\n\\n        Notes\\n        -----\\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\\n        '\n    return AFTResults(self)",
            "def fit(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n        Fits an AFT model and returns results instance\\n\\n        Parameters\\n        ----------\\n        None\\n\\n\\n        Returns\\n        -------\\n        Results instance.\\n\\n        Notes\\n        -----\\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\\n        '\n    return AFTResults(self)",
            "def fit(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n        Fits an AFT model and returns results instance\\n\\n        Parameters\\n        ----------\\n        None\\n\\n\\n        Returns\\n        -------\\n        Results instance.\\n\\n        Notes\\n        -----\\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\\n        '\n    return AFTResults(self)"
        ]
    },
    {
        "func_name": "predict",
        "original": "def predict(self, params, endog=None):\n    if endog is None:\n        endog = self.endog\n    return np.dot(endog, params)",
        "mutated": [
            "def predict(self, params, endog=None):\n    if False:\n        i = 10\n    if endog is None:\n        endog = self.endog\n    return np.dot(endog, params)",
            "def predict(self, params, endog=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if endog is None:\n        endog = self.endog\n    return np.dot(endog, params)",
            "def predict(self, params, endog=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if endog is None:\n        endog = self.endog\n    return np.dot(endog, params)",
            "def predict(self, params, endog=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if endog is None:\n        endog = self.endog\n    return np.dot(endog, params)",
            "def predict(self, params, endog=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if endog is None:\n        endog = self.endog\n    return np.dot(endog, params)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, model):\n    self.model = model",
        "mutated": [
            "def __init__(self, model):\n    if False:\n        i = 10\n    self.model = model",
            "def __init__(self, model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.model = model",
            "def __init__(self, model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.model = model",
            "def __init__(self, model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.model = model",
            "def __init__(self, model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.model = model"
        ]
    },
    {
        "func_name": "params",
        "original": "def params(self):\n    \"\"\"\n\n        Fits an AFT model and returns parameters.\n\n        Parameters\n        ----------\n        None\n\n\n        Returns\n        -------\n        Fitted params\n\n        Notes\n        -----\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\n        \"\"\"\n    self.model.modif_censors = np.copy(self.model.censors)\n    self.model.modif_censors[-1] = 1\n    wts = self.model._make_km(self.model.endog, self.model.modif_censors)\n    res = WLS(self.model.endog, self.model.exog, wts).fit()\n    params = res.params\n    return params",
        "mutated": [
            "def params(self):\n    if False:\n        i = 10\n    '\\n\\n        Fits an AFT model and returns parameters.\\n\\n        Parameters\\n        ----------\\n        None\\n\\n\\n        Returns\\n        -------\\n        Fitted params\\n\\n        Notes\\n        -----\\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\\n        '\n    self.model.modif_censors = np.copy(self.model.censors)\n    self.model.modif_censors[-1] = 1\n    wts = self.model._make_km(self.model.endog, self.model.modif_censors)\n    res = WLS(self.model.endog, self.model.exog, wts).fit()\n    params = res.params\n    return params",
            "def params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n        Fits an AFT model and returns parameters.\\n\\n        Parameters\\n        ----------\\n        None\\n\\n\\n        Returns\\n        -------\\n        Fitted params\\n\\n        Notes\\n        -----\\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\\n        '\n    self.model.modif_censors = np.copy(self.model.censors)\n    self.model.modif_censors[-1] = 1\n    wts = self.model._make_km(self.model.endog, self.model.modif_censors)\n    res = WLS(self.model.endog, self.model.exog, wts).fit()\n    params = res.params\n    return params",
            "def params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n        Fits an AFT model and returns parameters.\\n\\n        Parameters\\n        ----------\\n        None\\n\\n\\n        Returns\\n        -------\\n        Fitted params\\n\\n        Notes\\n        -----\\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\\n        '\n    self.model.modif_censors = np.copy(self.model.censors)\n    self.model.modif_censors[-1] = 1\n    wts = self.model._make_km(self.model.endog, self.model.modif_censors)\n    res = WLS(self.model.endog, self.model.exog, wts).fit()\n    params = res.params\n    return params",
            "def params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n        Fits an AFT model and returns parameters.\\n\\n        Parameters\\n        ----------\\n        None\\n\\n\\n        Returns\\n        -------\\n        Fitted params\\n\\n        Notes\\n        -----\\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\\n        '\n    self.model.modif_censors = np.copy(self.model.censors)\n    self.model.modif_censors[-1] = 1\n    wts = self.model._make_km(self.model.endog, self.model.modif_censors)\n    res = WLS(self.model.endog, self.model.exog, wts).fit()\n    params = res.params\n    return params",
            "def params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n        Fits an AFT model and returns parameters.\\n\\n        Parameters\\n        ----------\\n        None\\n\\n\\n        Returns\\n        -------\\n        Fitted params\\n\\n        Notes\\n        -----\\n        To avoid dividing by zero, max(endog) is assumed to be uncensored.\\n        '\n    self.model.modif_censors = np.copy(self.model.censors)\n    self.model.modif_censors[-1] = 1\n    wts = self.model._make_km(self.model.endog, self.model.modif_censors)\n    res = WLS(self.model.endog, self.model.exog, wts).fit()\n    params = res.params\n    return params"
        ]
    },
    {
        "func_name": "test_beta",
        "original": "def test_beta(self, b0_vals, param_nums, ftol=10 ** (-5), maxiter=30, print_weights=1):\n    \"\"\"\n        Returns the profile log likelihood for regression parameters\n        'param_num' at 'b0_vals.'\n\n        Parameters\n        ----------\n        b0_vals : list\n            The value of parameters to be tested\n        param_num : list\n            Which parameters to be tested\n        maxiter : int, optional\n            How many iterations to use in the EM algorithm.  Default is 30\n        ftol : float, optional\n            The function tolerance for the EM optimization.\n            Default is 10''**''-5\n        print_weights : bool\n            If true, returns the weights tate maximize the profile\n            log likelihood. Default is False\n\n        Returns\n        -------\n\n        test_results : tuple\n            The log-likelihood and p-pvalue of the test.\n\n        Notes\n        -----\n\n        The function will warn if the EM reaches the maxiter.  However, when\n        optimizing over nuisance parameters, it is possible to reach a\n        maximum number of inner iterations for a specific value for the\n        nuisance parameters while the resultsof the function are still valid.\n        This usually occurs when the optimization over the nuisance parameters\n        selects parameter values that yield a log-likihood ratio close to\n        infinity.\n\n        Examples\n        --------\n\n        >>> import statsmodels.api as sm\n        >>> import numpy as np\n\n        # Test parameter is .05 in one regressor no intercept model\n        >>> data=sm.datasets.heart.load()\n        >>> y = np.log10(data.endog)\n        >>> x = data.exog\n        >>> cens = data.censors\n        >>> model = sm.emplike.emplikeAFT(y, x, cens)\n        >>> res=model.test_beta([0], [0])\n        >>> res\n        (1.4657739632606308, 0.22601365256959183)\n\n        #Test slope is 0 in  model with intercept\n\n        >>> data=sm.datasets.heart.load()\n        >>> y = np.log10(data.endog)\n        >>> x = data.exog\n        >>> cens = data.censors\n        >>> model = sm.emplike.emplikeAFT(y, sm.add_constant(x), cens)\n        >>> res = model.test_beta([0], [1])\n        >>> res\n        (4.623487775078047, 0.031537049752572731)\n        \"\"\"\n    censors = self.model.censors\n    endog = self.model.endog\n    exog = self.model.exog\n    uncensored = (censors == 1).flatten()\n    censored = (censors == 0).flatten()\n    uncens_endog = endog[uncensored]\n    uncens_exog = exog[uncensored, :]\n    reg_model = OLS(uncens_endog, uncens_exog).fit()\n    (llr, pval, new_weights) = reg_model.el_test(b0_vals, param_nums, return_weights=True)\n    km = self.model._make_km(endog, censors).flatten()\n    uncens_nobs = self.model.uncens_nobs\n    F = np.asarray(new_weights).reshape(uncens_nobs)\n    params = self.params()\n    survidx = np.where(censors == 0)\n    survidx = survidx[0] - np.arange(len(survidx[0]))\n    numcensbelow = np.int_(np.cumsum(1 - censors))\n    if len(param_nums) == len(params):\n        llr = self._EM_test([], F=F, params=params, param_nums=param_nums, b0_vals=b0_vals, survidx=survidx, uncens_nobs=uncens_nobs, numcensbelow=numcensbelow, km=km, uncensored=uncensored, censored=censored, ftol=ftol, maxiter=25)\n        return (llr, chi2.sf(llr, self.model.nvar))\n    else:\n        x0 = np.delete(params, param_nums)\n        try:\n            res = optimize.fmin(self._EM_test, x0, (params, param_nums, b0_vals, F, survidx, uncens_nobs, numcensbelow, km, uncensored, censored, maxiter, ftol), full_output=1, disp=0)\n            llr = res[1]\n            return (llr, chi2.sf(llr, len(param_nums)))\n        except np.linalg.linalg.LinAlgError:\n            return (np.inf, 0)",
        "mutated": [
            "def test_beta(self, b0_vals, param_nums, ftol=10 ** (-5), maxiter=30, print_weights=1):\n    if False:\n        i = 10\n    \"\\n        Returns the profile log likelihood for regression parameters\\n        'param_num' at 'b0_vals.'\\n\\n        Parameters\\n        ----------\\n        b0_vals : list\\n            The value of parameters to be tested\\n        param_num : list\\n            Which parameters to be tested\\n        maxiter : int, optional\\n            How many iterations to use in the EM algorithm.  Default is 30\\n        ftol : float, optional\\n            The function tolerance for the EM optimization.\\n            Default is 10''**''-5\\n        print_weights : bool\\n            If true, returns the weights tate maximize the profile\\n            log likelihood. Default is False\\n\\n        Returns\\n        -------\\n\\n        test_results : tuple\\n            The log-likelihood and p-pvalue of the test.\\n\\n        Notes\\n        -----\\n\\n        The function will warn if the EM reaches the maxiter.  However, when\\n        optimizing over nuisance parameters, it is possible to reach a\\n        maximum number of inner iterations for a specific value for the\\n        nuisance parameters while the resultsof the function are still valid.\\n        This usually occurs when the optimization over the nuisance parameters\\n        selects parameter values that yield a log-likihood ratio close to\\n        infinity.\\n\\n        Examples\\n        --------\\n\\n        >>> import statsmodels.api as sm\\n        >>> import numpy as np\\n\\n        # Test parameter is .05 in one regressor no intercept model\\n        >>> data=sm.datasets.heart.load()\\n        >>> y = np.log10(data.endog)\\n        >>> x = data.exog\\n        >>> cens = data.censors\\n        >>> model = sm.emplike.emplikeAFT(y, x, cens)\\n        >>> res=model.test_beta([0], [0])\\n        >>> res\\n        (1.4657739632606308, 0.22601365256959183)\\n\\n        #Test slope is 0 in  model with intercept\\n\\n        >>> data=sm.datasets.heart.load()\\n        >>> y = np.log10(data.endog)\\n        >>> x = data.exog\\n        >>> cens = data.censors\\n        >>> model = sm.emplike.emplikeAFT(y, sm.add_constant(x), cens)\\n        >>> res = model.test_beta([0], [1])\\n        >>> res\\n        (4.623487775078047, 0.031537049752572731)\\n        \"\n    censors = self.model.censors\n    endog = self.model.endog\n    exog = self.model.exog\n    uncensored = (censors == 1).flatten()\n    censored = (censors == 0).flatten()\n    uncens_endog = endog[uncensored]\n    uncens_exog = exog[uncensored, :]\n    reg_model = OLS(uncens_endog, uncens_exog).fit()\n    (llr, pval, new_weights) = reg_model.el_test(b0_vals, param_nums, return_weights=True)\n    km = self.model._make_km(endog, censors).flatten()\n    uncens_nobs = self.model.uncens_nobs\n    F = np.asarray(new_weights).reshape(uncens_nobs)\n    params = self.params()\n    survidx = np.where(censors == 0)\n    survidx = survidx[0] - np.arange(len(survidx[0]))\n    numcensbelow = np.int_(np.cumsum(1 - censors))\n    if len(param_nums) == len(params):\n        llr = self._EM_test([], F=F, params=params, param_nums=param_nums, b0_vals=b0_vals, survidx=survidx, uncens_nobs=uncens_nobs, numcensbelow=numcensbelow, km=km, uncensored=uncensored, censored=censored, ftol=ftol, maxiter=25)\n        return (llr, chi2.sf(llr, self.model.nvar))\n    else:\n        x0 = np.delete(params, param_nums)\n        try:\n            res = optimize.fmin(self._EM_test, x0, (params, param_nums, b0_vals, F, survidx, uncens_nobs, numcensbelow, km, uncensored, censored, maxiter, ftol), full_output=1, disp=0)\n            llr = res[1]\n            return (llr, chi2.sf(llr, len(param_nums)))\n        except np.linalg.linalg.LinAlgError:\n            return (np.inf, 0)",
            "def test_beta(self, b0_vals, param_nums, ftol=10 ** (-5), maxiter=30, print_weights=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Returns the profile log likelihood for regression parameters\\n        'param_num' at 'b0_vals.'\\n\\n        Parameters\\n        ----------\\n        b0_vals : list\\n            The value of parameters to be tested\\n        param_num : list\\n            Which parameters to be tested\\n        maxiter : int, optional\\n            How many iterations to use in the EM algorithm.  Default is 30\\n        ftol : float, optional\\n            The function tolerance for the EM optimization.\\n            Default is 10''**''-5\\n        print_weights : bool\\n            If true, returns the weights tate maximize the profile\\n            log likelihood. Default is False\\n\\n        Returns\\n        -------\\n\\n        test_results : tuple\\n            The log-likelihood and p-pvalue of the test.\\n\\n        Notes\\n        -----\\n\\n        The function will warn if the EM reaches the maxiter.  However, when\\n        optimizing over nuisance parameters, it is possible to reach a\\n        maximum number of inner iterations for a specific value for the\\n        nuisance parameters while the resultsof the function are still valid.\\n        This usually occurs when the optimization over the nuisance parameters\\n        selects parameter values that yield a log-likihood ratio close to\\n        infinity.\\n\\n        Examples\\n        --------\\n\\n        >>> import statsmodels.api as sm\\n        >>> import numpy as np\\n\\n        # Test parameter is .05 in one regressor no intercept model\\n        >>> data=sm.datasets.heart.load()\\n        >>> y = np.log10(data.endog)\\n        >>> x = data.exog\\n        >>> cens = data.censors\\n        >>> model = sm.emplike.emplikeAFT(y, x, cens)\\n        >>> res=model.test_beta([0], [0])\\n        >>> res\\n        (1.4657739632606308, 0.22601365256959183)\\n\\n        #Test slope is 0 in  model with intercept\\n\\n        >>> data=sm.datasets.heart.load()\\n        >>> y = np.log10(data.endog)\\n        >>> x = data.exog\\n        >>> cens = data.censors\\n        >>> model = sm.emplike.emplikeAFT(y, sm.add_constant(x), cens)\\n        >>> res = model.test_beta([0], [1])\\n        >>> res\\n        (4.623487775078047, 0.031537049752572731)\\n        \"\n    censors = self.model.censors\n    endog = self.model.endog\n    exog = self.model.exog\n    uncensored = (censors == 1).flatten()\n    censored = (censors == 0).flatten()\n    uncens_endog = endog[uncensored]\n    uncens_exog = exog[uncensored, :]\n    reg_model = OLS(uncens_endog, uncens_exog).fit()\n    (llr, pval, new_weights) = reg_model.el_test(b0_vals, param_nums, return_weights=True)\n    km = self.model._make_km(endog, censors).flatten()\n    uncens_nobs = self.model.uncens_nobs\n    F = np.asarray(new_weights).reshape(uncens_nobs)\n    params = self.params()\n    survidx = np.where(censors == 0)\n    survidx = survidx[0] - np.arange(len(survidx[0]))\n    numcensbelow = np.int_(np.cumsum(1 - censors))\n    if len(param_nums) == len(params):\n        llr = self._EM_test([], F=F, params=params, param_nums=param_nums, b0_vals=b0_vals, survidx=survidx, uncens_nobs=uncens_nobs, numcensbelow=numcensbelow, km=km, uncensored=uncensored, censored=censored, ftol=ftol, maxiter=25)\n        return (llr, chi2.sf(llr, self.model.nvar))\n    else:\n        x0 = np.delete(params, param_nums)\n        try:\n            res = optimize.fmin(self._EM_test, x0, (params, param_nums, b0_vals, F, survidx, uncens_nobs, numcensbelow, km, uncensored, censored, maxiter, ftol), full_output=1, disp=0)\n            llr = res[1]\n            return (llr, chi2.sf(llr, len(param_nums)))\n        except np.linalg.linalg.LinAlgError:\n            return (np.inf, 0)",
            "def test_beta(self, b0_vals, param_nums, ftol=10 ** (-5), maxiter=30, print_weights=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Returns the profile log likelihood for regression parameters\\n        'param_num' at 'b0_vals.'\\n\\n        Parameters\\n        ----------\\n        b0_vals : list\\n            The value of parameters to be tested\\n        param_num : list\\n            Which parameters to be tested\\n        maxiter : int, optional\\n            How many iterations to use in the EM algorithm.  Default is 30\\n        ftol : float, optional\\n            The function tolerance for the EM optimization.\\n            Default is 10''**''-5\\n        print_weights : bool\\n            If true, returns the weights tate maximize the profile\\n            log likelihood. Default is False\\n\\n        Returns\\n        -------\\n\\n        test_results : tuple\\n            The log-likelihood and p-pvalue of the test.\\n\\n        Notes\\n        -----\\n\\n        The function will warn if the EM reaches the maxiter.  However, when\\n        optimizing over nuisance parameters, it is possible to reach a\\n        maximum number of inner iterations for a specific value for the\\n        nuisance parameters while the resultsof the function are still valid.\\n        This usually occurs when the optimization over the nuisance parameters\\n        selects parameter values that yield a log-likihood ratio close to\\n        infinity.\\n\\n        Examples\\n        --------\\n\\n        >>> import statsmodels.api as sm\\n        >>> import numpy as np\\n\\n        # Test parameter is .05 in one regressor no intercept model\\n        >>> data=sm.datasets.heart.load()\\n        >>> y = np.log10(data.endog)\\n        >>> x = data.exog\\n        >>> cens = data.censors\\n        >>> model = sm.emplike.emplikeAFT(y, x, cens)\\n        >>> res=model.test_beta([0], [0])\\n        >>> res\\n        (1.4657739632606308, 0.22601365256959183)\\n\\n        #Test slope is 0 in  model with intercept\\n\\n        >>> data=sm.datasets.heart.load()\\n        >>> y = np.log10(data.endog)\\n        >>> x = data.exog\\n        >>> cens = data.censors\\n        >>> model = sm.emplike.emplikeAFT(y, sm.add_constant(x), cens)\\n        >>> res = model.test_beta([0], [1])\\n        >>> res\\n        (4.623487775078047, 0.031537049752572731)\\n        \"\n    censors = self.model.censors\n    endog = self.model.endog\n    exog = self.model.exog\n    uncensored = (censors == 1).flatten()\n    censored = (censors == 0).flatten()\n    uncens_endog = endog[uncensored]\n    uncens_exog = exog[uncensored, :]\n    reg_model = OLS(uncens_endog, uncens_exog).fit()\n    (llr, pval, new_weights) = reg_model.el_test(b0_vals, param_nums, return_weights=True)\n    km = self.model._make_km(endog, censors).flatten()\n    uncens_nobs = self.model.uncens_nobs\n    F = np.asarray(new_weights).reshape(uncens_nobs)\n    params = self.params()\n    survidx = np.where(censors == 0)\n    survidx = survidx[0] - np.arange(len(survidx[0]))\n    numcensbelow = np.int_(np.cumsum(1 - censors))\n    if len(param_nums) == len(params):\n        llr = self._EM_test([], F=F, params=params, param_nums=param_nums, b0_vals=b0_vals, survidx=survidx, uncens_nobs=uncens_nobs, numcensbelow=numcensbelow, km=km, uncensored=uncensored, censored=censored, ftol=ftol, maxiter=25)\n        return (llr, chi2.sf(llr, self.model.nvar))\n    else:\n        x0 = np.delete(params, param_nums)\n        try:\n            res = optimize.fmin(self._EM_test, x0, (params, param_nums, b0_vals, F, survidx, uncens_nobs, numcensbelow, km, uncensored, censored, maxiter, ftol), full_output=1, disp=0)\n            llr = res[1]\n            return (llr, chi2.sf(llr, len(param_nums)))\n        except np.linalg.linalg.LinAlgError:\n            return (np.inf, 0)",
            "def test_beta(self, b0_vals, param_nums, ftol=10 ** (-5), maxiter=30, print_weights=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Returns the profile log likelihood for regression parameters\\n        'param_num' at 'b0_vals.'\\n\\n        Parameters\\n        ----------\\n        b0_vals : list\\n            The value of parameters to be tested\\n        param_num : list\\n            Which parameters to be tested\\n        maxiter : int, optional\\n            How many iterations to use in the EM algorithm.  Default is 30\\n        ftol : float, optional\\n            The function tolerance for the EM optimization.\\n            Default is 10''**''-5\\n        print_weights : bool\\n            If true, returns the weights tate maximize the profile\\n            log likelihood. Default is False\\n\\n        Returns\\n        -------\\n\\n        test_results : tuple\\n            The log-likelihood and p-pvalue of the test.\\n\\n        Notes\\n        -----\\n\\n        The function will warn if the EM reaches the maxiter.  However, when\\n        optimizing over nuisance parameters, it is possible to reach a\\n        maximum number of inner iterations for a specific value for the\\n        nuisance parameters while the resultsof the function are still valid.\\n        This usually occurs when the optimization over the nuisance parameters\\n        selects parameter values that yield a log-likihood ratio close to\\n        infinity.\\n\\n        Examples\\n        --------\\n\\n        >>> import statsmodels.api as sm\\n        >>> import numpy as np\\n\\n        # Test parameter is .05 in one regressor no intercept model\\n        >>> data=sm.datasets.heart.load()\\n        >>> y = np.log10(data.endog)\\n        >>> x = data.exog\\n        >>> cens = data.censors\\n        >>> model = sm.emplike.emplikeAFT(y, x, cens)\\n        >>> res=model.test_beta([0], [0])\\n        >>> res\\n        (1.4657739632606308, 0.22601365256959183)\\n\\n        #Test slope is 0 in  model with intercept\\n\\n        >>> data=sm.datasets.heart.load()\\n        >>> y = np.log10(data.endog)\\n        >>> x = data.exog\\n        >>> cens = data.censors\\n        >>> model = sm.emplike.emplikeAFT(y, sm.add_constant(x), cens)\\n        >>> res = model.test_beta([0], [1])\\n        >>> res\\n        (4.623487775078047, 0.031537049752572731)\\n        \"\n    censors = self.model.censors\n    endog = self.model.endog\n    exog = self.model.exog\n    uncensored = (censors == 1).flatten()\n    censored = (censors == 0).flatten()\n    uncens_endog = endog[uncensored]\n    uncens_exog = exog[uncensored, :]\n    reg_model = OLS(uncens_endog, uncens_exog).fit()\n    (llr, pval, new_weights) = reg_model.el_test(b0_vals, param_nums, return_weights=True)\n    km = self.model._make_km(endog, censors).flatten()\n    uncens_nobs = self.model.uncens_nobs\n    F = np.asarray(new_weights).reshape(uncens_nobs)\n    params = self.params()\n    survidx = np.where(censors == 0)\n    survidx = survidx[0] - np.arange(len(survidx[0]))\n    numcensbelow = np.int_(np.cumsum(1 - censors))\n    if len(param_nums) == len(params):\n        llr = self._EM_test([], F=F, params=params, param_nums=param_nums, b0_vals=b0_vals, survidx=survidx, uncens_nobs=uncens_nobs, numcensbelow=numcensbelow, km=km, uncensored=uncensored, censored=censored, ftol=ftol, maxiter=25)\n        return (llr, chi2.sf(llr, self.model.nvar))\n    else:\n        x0 = np.delete(params, param_nums)\n        try:\n            res = optimize.fmin(self._EM_test, x0, (params, param_nums, b0_vals, F, survidx, uncens_nobs, numcensbelow, km, uncensored, censored, maxiter, ftol), full_output=1, disp=0)\n            llr = res[1]\n            return (llr, chi2.sf(llr, len(param_nums)))\n        except np.linalg.linalg.LinAlgError:\n            return (np.inf, 0)",
            "def test_beta(self, b0_vals, param_nums, ftol=10 ** (-5), maxiter=30, print_weights=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Returns the profile log likelihood for regression parameters\\n        'param_num' at 'b0_vals.'\\n\\n        Parameters\\n        ----------\\n        b0_vals : list\\n            The value of parameters to be tested\\n        param_num : list\\n            Which parameters to be tested\\n        maxiter : int, optional\\n            How many iterations to use in the EM algorithm.  Default is 30\\n        ftol : float, optional\\n            The function tolerance for the EM optimization.\\n            Default is 10''**''-5\\n        print_weights : bool\\n            If true, returns the weights tate maximize the profile\\n            log likelihood. Default is False\\n\\n        Returns\\n        -------\\n\\n        test_results : tuple\\n            The log-likelihood and p-pvalue of the test.\\n\\n        Notes\\n        -----\\n\\n        The function will warn if the EM reaches the maxiter.  However, when\\n        optimizing over nuisance parameters, it is possible to reach a\\n        maximum number of inner iterations for a specific value for the\\n        nuisance parameters while the resultsof the function are still valid.\\n        This usually occurs when the optimization over the nuisance parameters\\n        selects parameter values that yield a log-likihood ratio close to\\n        infinity.\\n\\n        Examples\\n        --------\\n\\n        >>> import statsmodels.api as sm\\n        >>> import numpy as np\\n\\n        # Test parameter is .05 in one regressor no intercept model\\n        >>> data=sm.datasets.heart.load()\\n        >>> y = np.log10(data.endog)\\n        >>> x = data.exog\\n        >>> cens = data.censors\\n        >>> model = sm.emplike.emplikeAFT(y, x, cens)\\n        >>> res=model.test_beta([0], [0])\\n        >>> res\\n        (1.4657739632606308, 0.22601365256959183)\\n\\n        #Test slope is 0 in  model with intercept\\n\\n        >>> data=sm.datasets.heart.load()\\n        >>> y = np.log10(data.endog)\\n        >>> x = data.exog\\n        >>> cens = data.censors\\n        >>> model = sm.emplike.emplikeAFT(y, sm.add_constant(x), cens)\\n        >>> res = model.test_beta([0], [1])\\n        >>> res\\n        (4.623487775078047, 0.031537049752572731)\\n        \"\n    censors = self.model.censors\n    endog = self.model.endog\n    exog = self.model.exog\n    uncensored = (censors == 1).flatten()\n    censored = (censors == 0).flatten()\n    uncens_endog = endog[uncensored]\n    uncens_exog = exog[uncensored, :]\n    reg_model = OLS(uncens_endog, uncens_exog).fit()\n    (llr, pval, new_weights) = reg_model.el_test(b0_vals, param_nums, return_weights=True)\n    km = self.model._make_km(endog, censors).flatten()\n    uncens_nobs = self.model.uncens_nobs\n    F = np.asarray(new_weights).reshape(uncens_nobs)\n    params = self.params()\n    survidx = np.where(censors == 0)\n    survidx = survidx[0] - np.arange(len(survidx[0]))\n    numcensbelow = np.int_(np.cumsum(1 - censors))\n    if len(param_nums) == len(params):\n        llr = self._EM_test([], F=F, params=params, param_nums=param_nums, b0_vals=b0_vals, survidx=survidx, uncens_nobs=uncens_nobs, numcensbelow=numcensbelow, km=km, uncensored=uncensored, censored=censored, ftol=ftol, maxiter=25)\n        return (llr, chi2.sf(llr, self.model.nvar))\n    else:\n        x0 = np.delete(params, param_nums)\n        try:\n            res = optimize.fmin(self._EM_test, x0, (params, param_nums, b0_vals, F, survidx, uncens_nobs, numcensbelow, km, uncensored, censored, maxiter, ftol), full_output=1, disp=0)\n            llr = res[1]\n            return (llr, chi2.sf(llr, len(param_nums)))\n        except np.linalg.linalg.LinAlgError:\n            return (np.inf, 0)"
        ]
    },
    {
        "func_name": "ci_beta",
        "original": "def ci_beta(self, param_num, beta_high, beta_low, sig=0.05):\n    \"\"\"\n        Returns the confidence interval for a regression\n        parameter in the AFT model.\n\n        Parameters\n        ----------\n        param_num : int\n            Parameter number of interest\n        beta_high : float\n            Upper bound for the confidence interval\n        beta_low : float\n            Lower bound for the confidence interval\n        sig : float, optional\n            Significance level.  Default is .05\n\n        Notes\n        -----\n        If the function returns f(a) and f(b) must have different signs,\n        consider widening the search area by adjusting beta_low and\n        beta_high.\n\n        Also note that this process is computational intensive.  There\n        are 4 levels of optimization/solving.  From outer to inner:\n\n        1) Solving so that llr-critical value = 0\n        2) maximizing over nuisance parameters\n        3) Using  EM at each value of nuisamce parameters\n        4) Using the _modified_Newton optimizer at each iteration\n           of the EM algorithm.\n\n        Also, for very unlikely nuisance parameters, it is possible for\n        the EM algorithm to not converge.  This is not an indicator\n        that the solver did not find the correct solution.  It just means\n        for a specific iteration of the nuisance parameters, the optimizer\n        was unable to converge.\n\n        If the user desires to verify the success of the optimization,\n        it is recommended to test the limits using test_beta.\n        \"\"\"\n    params = self.params()\n    self.r0 = chi2.ppf(1 - sig, 1)\n    ll = optimize.brentq(self._ci_limits_beta, beta_low, params[param_num], param_num)\n    ul = optimize.brentq(self._ci_limits_beta, params[param_num], beta_high, param_num)\n    return (ll, ul)",
        "mutated": [
            "def ci_beta(self, param_num, beta_high, beta_low, sig=0.05):\n    if False:\n        i = 10\n    '\\n        Returns the confidence interval for a regression\\n        parameter in the AFT model.\\n\\n        Parameters\\n        ----------\\n        param_num : int\\n            Parameter number of interest\\n        beta_high : float\\n            Upper bound for the confidence interval\\n        beta_low : float\\n            Lower bound for the confidence interval\\n        sig : float, optional\\n            Significance level.  Default is .05\\n\\n        Notes\\n        -----\\n        If the function returns f(a) and f(b) must have different signs,\\n        consider widening the search area by adjusting beta_low and\\n        beta_high.\\n\\n        Also note that this process is computational intensive.  There\\n        are 4 levels of optimization/solving.  From outer to inner:\\n\\n        1) Solving so that llr-critical value = 0\\n        2) maximizing over nuisance parameters\\n        3) Using  EM at each value of nuisamce parameters\\n        4) Using the _modified_Newton optimizer at each iteration\\n           of the EM algorithm.\\n\\n        Also, for very unlikely nuisance parameters, it is possible for\\n        the EM algorithm to not converge.  This is not an indicator\\n        that the solver did not find the correct solution.  It just means\\n        for a specific iteration of the nuisance parameters, the optimizer\\n        was unable to converge.\\n\\n        If the user desires to verify the success of the optimization,\\n        it is recommended to test the limits using test_beta.\\n        '\n    params = self.params()\n    self.r0 = chi2.ppf(1 - sig, 1)\n    ll = optimize.brentq(self._ci_limits_beta, beta_low, params[param_num], param_num)\n    ul = optimize.brentq(self._ci_limits_beta, params[param_num], beta_high, param_num)\n    return (ll, ul)",
            "def ci_beta(self, param_num, beta_high, beta_low, sig=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the confidence interval for a regression\\n        parameter in the AFT model.\\n\\n        Parameters\\n        ----------\\n        param_num : int\\n            Parameter number of interest\\n        beta_high : float\\n            Upper bound for the confidence interval\\n        beta_low : float\\n            Lower bound for the confidence interval\\n        sig : float, optional\\n            Significance level.  Default is .05\\n\\n        Notes\\n        -----\\n        If the function returns f(a) and f(b) must have different signs,\\n        consider widening the search area by adjusting beta_low and\\n        beta_high.\\n\\n        Also note that this process is computational intensive.  There\\n        are 4 levels of optimization/solving.  From outer to inner:\\n\\n        1) Solving so that llr-critical value = 0\\n        2) maximizing over nuisance parameters\\n        3) Using  EM at each value of nuisamce parameters\\n        4) Using the _modified_Newton optimizer at each iteration\\n           of the EM algorithm.\\n\\n        Also, for very unlikely nuisance parameters, it is possible for\\n        the EM algorithm to not converge.  This is not an indicator\\n        that the solver did not find the correct solution.  It just means\\n        for a specific iteration of the nuisance parameters, the optimizer\\n        was unable to converge.\\n\\n        If the user desires to verify the success of the optimization,\\n        it is recommended to test the limits using test_beta.\\n        '\n    params = self.params()\n    self.r0 = chi2.ppf(1 - sig, 1)\n    ll = optimize.brentq(self._ci_limits_beta, beta_low, params[param_num], param_num)\n    ul = optimize.brentq(self._ci_limits_beta, params[param_num], beta_high, param_num)\n    return (ll, ul)",
            "def ci_beta(self, param_num, beta_high, beta_low, sig=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the confidence interval for a regression\\n        parameter in the AFT model.\\n\\n        Parameters\\n        ----------\\n        param_num : int\\n            Parameter number of interest\\n        beta_high : float\\n            Upper bound for the confidence interval\\n        beta_low : float\\n            Lower bound for the confidence interval\\n        sig : float, optional\\n            Significance level.  Default is .05\\n\\n        Notes\\n        -----\\n        If the function returns f(a) and f(b) must have different signs,\\n        consider widening the search area by adjusting beta_low and\\n        beta_high.\\n\\n        Also note that this process is computational intensive.  There\\n        are 4 levels of optimization/solving.  From outer to inner:\\n\\n        1) Solving so that llr-critical value = 0\\n        2) maximizing over nuisance parameters\\n        3) Using  EM at each value of nuisamce parameters\\n        4) Using the _modified_Newton optimizer at each iteration\\n           of the EM algorithm.\\n\\n        Also, for very unlikely nuisance parameters, it is possible for\\n        the EM algorithm to not converge.  This is not an indicator\\n        that the solver did not find the correct solution.  It just means\\n        for a specific iteration of the nuisance parameters, the optimizer\\n        was unable to converge.\\n\\n        If the user desires to verify the success of the optimization,\\n        it is recommended to test the limits using test_beta.\\n        '\n    params = self.params()\n    self.r0 = chi2.ppf(1 - sig, 1)\n    ll = optimize.brentq(self._ci_limits_beta, beta_low, params[param_num], param_num)\n    ul = optimize.brentq(self._ci_limits_beta, params[param_num], beta_high, param_num)\n    return (ll, ul)",
            "def ci_beta(self, param_num, beta_high, beta_low, sig=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the confidence interval for a regression\\n        parameter in the AFT model.\\n\\n        Parameters\\n        ----------\\n        param_num : int\\n            Parameter number of interest\\n        beta_high : float\\n            Upper bound for the confidence interval\\n        beta_low : float\\n            Lower bound for the confidence interval\\n        sig : float, optional\\n            Significance level.  Default is .05\\n\\n        Notes\\n        -----\\n        If the function returns f(a) and f(b) must have different signs,\\n        consider widening the search area by adjusting beta_low and\\n        beta_high.\\n\\n        Also note that this process is computational intensive.  There\\n        are 4 levels of optimization/solving.  From outer to inner:\\n\\n        1) Solving so that llr-critical value = 0\\n        2) maximizing over nuisance parameters\\n        3) Using  EM at each value of nuisamce parameters\\n        4) Using the _modified_Newton optimizer at each iteration\\n           of the EM algorithm.\\n\\n        Also, for very unlikely nuisance parameters, it is possible for\\n        the EM algorithm to not converge.  This is not an indicator\\n        that the solver did not find the correct solution.  It just means\\n        for a specific iteration of the nuisance parameters, the optimizer\\n        was unable to converge.\\n\\n        If the user desires to verify the success of the optimization,\\n        it is recommended to test the limits using test_beta.\\n        '\n    params = self.params()\n    self.r0 = chi2.ppf(1 - sig, 1)\n    ll = optimize.brentq(self._ci_limits_beta, beta_low, params[param_num], param_num)\n    ul = optimize.brentq(self._ci_limits_beta, params[param_num], beta_high, param_num)\n    return (ll, ul)",
            "def ci_beta(self, param_num, beta_high, beta_low, sig=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the confidence interval for a regression\\n        parameter in the AFT model.\\n\\n        Parameters\\n        ----------\\n        param_num : int\\n            Parameter number of interest\\n        beta_high : float\\n            Upper bound for the confidence interval\\n        beta_low : float\\n            Lower bound for the confidence interval\\n        sig : float, optional\\n            Significance level.  Default is .05\\n\\n        Notes\\n        -----\\n        If the function returns f(a) and f(b) must have different signs,\\n        consider widening the search area by adjusting beta_low and\\n        beta_high.\\n\\n        Also note that this process is computational intensive.  There\\n        are 4 levels of optimization/solving.  From outer to inner:\\n\\n        1) Solving so that llr-critical value = 0\\n        2) maximizing over nuisance parameters\\n        3) Using  EM at each value of nuisamce parameters\\n        4) Using the _modified_Newton optimizer at each iteration\\n           of the EM algorithm.\\n\\n        Also, for very unlikely nuisance parameters, it is possible for\\n        the EM algorithm to not converge.  This is not an indicator\\n        that the solver did not find the correct solution.  It just means\\n        for a specific iteration of the nuisance parameters, the optimizer\\n        was unable to converge.\\n\\n        If the user desires to verify the success of the optimization,\\n        it is recommended to test the limits using test_beta.\\n        '\n    params = self.params()\n    self.r0 = chi2.ppf(1 - sig, 1)\n    ll = optimize.brentq(self._ci_limits_beta, beta_low, params[param_num], param_num)\n    ul = optimize.brentq(self._ci_limits_beta, params[param_num], beta_high, param_num)\n    return (ll, ul)"
        ]
    }
]