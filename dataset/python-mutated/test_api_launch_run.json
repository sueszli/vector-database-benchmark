[
    {
        "func_name": "_check_event_log_contains",
        "original": "def _check_event_log_contains(event_log, expected_type_and_message):\n    types_and_messages = [(e.dagster_event.event_type_value, e.message) for e in event_log if e.is_dagster_event]\n    for (expected_event_type, expected_message_fragment) in expected_type_and_message:\n        assert any((event_type == expected_event_type and expected_message_fragment in message for (event_type, message) in types_and_messages))",
        "mutated": [
            "def _check_event_log_contains(event_log, expected_type_and_message):\n    if False:\n        i = 10\n    types_and_messages = [(e.dagster_event.event_type_value, e.message) for e in event_log if e.is_dagster_event]\n    for (expected_event_type, expected_message_fragment) in expected_type_and_message:\n        assert any((event_type == expected_event_type and expected_message_fragment in message for (event_type, message) in types_and_messages))",
            "def _check_event_log_contains(event_log, expected_type_and_message):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    types_and_messages = [(e.dagster_event.event_type_value, e.message) for e in event_log if e.is_dagster_event]\n    for (expected_event_type, expected_message_fragment) in expected_type_and_message:\n        assert any((event_type == expected_event_type and expected_message_fragment in message for (event_type, message) in types_and_messages))",
            "def _check_event_log_contains(event_log, expected_type_and_message):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    types_and_messages = [(e.dagster_event.event_type_value, e.message) for e in event_log if e.is_dagster_event]\n    for (expected_event_type, expected_message_fragment) in expected_type_and_message:\n        assert any((event_type == expected_event_type and expected_message_fragment in message for (event_type, message) in types_and_messages))",
            "def _check_event_log_contains(event_log, expected_type_and_message):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    types_and_messages = [(e.dagster_event.event_type_value, e.message) for e in event_log if e.is_dagster_event]\n    for (expected_event_type, expected_message_fragment) in expected_type_and_message:\n        assert any((event_type == expected_event_type and expected_message_fragment in message for (event_type, message) in types_and_messages))",
            "def _check_event_log_contains(event_log, expected_type_and_message):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    types_and_messages = [(e.dagster_event.event_type_value, e.message) for e in event_log if e.is_dagster_event]\n    for (expected_event_type, expected_message_fragment) in expected_type_and_message:\n        assert any((event_type == expected_event_type and expected_message_fragment in message for (event_type, message) in types_and_messages))"
        ]
    },
    {
        "func_name": "test_launch_run_with_unloadable_job_grpc",
        "original": "def test_launch_run_with_unloadable_job_grpc():\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            original_origin = job_handle.get_external_origin()\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=original_origin._replace(job_name='i_am_fake_pipeline'), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.FAILURE\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', 'Started process for run'), ('ENGINE_EVENT', 'Could not load job definition'), ('PIPELINE_FAILURE', 'This run has been marked as failed from outside the execution context'), ('ENGINE_EVENT', 'Process for run exited')])",
        "mutated": [
            "def test_launch_run_with_unloadable_job_grpc():\n    if False:\n        i = 10\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            original_origin = job_handle.get_external_origin()\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=original_origin._replace(job_name='i_am_fake_pipeline'), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.FAILURE\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', 'Started process for run'), ('ENGINE_EVENT', 'Could not load job definition'), ('PIPELINE_FAILURE', 'This run has been marked as failed from outside the execution context'), ('ENGINE_EVENT', 'Process for run exited')])",
            "def test_launch_run_with_unloadable_job_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            original_origin = job_handle.get_external_origin()\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=original_origin._replace(job_name='i_am_fake_pipeline'), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.FAILURE\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', 'Started process for run'), ('ENGINE_EVENT', 'Could not load job definition'), ('PIPELINE_FAILURE', 'This run has been marked as failed from outside the execution context'), ('ENGINE_EVENT', 'Process for run exited')])",
            "def test_launch_run_with_unloadable_job_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            original_origin = job_handle.get_external_origin()\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=original_origin._replace(job_name='i_am_fake_pipeline'), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.FAILURE\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', 'Started process for run'), ('ENGINE_EVENT', 'Could not load job definition'), ('PIPELINE_FAILURE', 'This run has been marked as failed from outside the execution context'), ('ENGINE_EVENT', 'Process for run exited')])",
            "def test_launch_run_with_unloadable_job_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            original_origin = job_handle.get_external_origin()\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=original_origin._replace(job_name='i_am_fake_pipeline'), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.FAILURE\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', 'Started process for run'), ('ENGINE_EVENT', 'Could not load job definition'), ('PIPELINE_FAILURE', 'This run has been marked as failed from outside the execution context'), ('ENGINE_EVENT', 'Process for run exited')])",
            "def test_launch_run_with_unloadable_job_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            original_origin = job_handle.get_external_origin()\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=original_origin._replace(job_name='i_am_fake_pipeline'), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.FAILURE\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', 'Started process for run'), ('ENGINE_EVENT', 'Could not load job definition'), ('PIPELINE_FAILURE', 'This run has been marked as failed from outside the execution context'), ('ENGINE_EVENT', 'Process for run exited')])"
        ]
    },
    {
        "func_name": "test_launch_run_grpc",
        "original": "def test_launch_run_grpc():\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.SUCCESS\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', msg) for msg in ['Started process for run', 'Executing steps using multiprocess executor', 'Multiprocess executor: parent process exiting', 'Process for run exited']])",
        "mutated": [
            "def test_launch_run_grpc():\n    if False:\n        i = 10\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.SUCCESS\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', msg) for msg in ['Started process for run', 'Executing steps using multiprocess executor', 'Multiprocess executor: parent process exiting', 'Process for run exited']])",
            "def test_launch_run_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.SUCCESS\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', msg) for msg in ['Started process for run', 'Executing steps using multiprocess executor', 'Multiprocess executor: parent process exiting', 'Process for run exited']])",
            "def test_launch_run_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.SUCCESS\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', msg) for msg in ['Started process for run', 'Executing steps using multiprocess executor', 'Multiprocess executor: parent process exiting', 'Process for run exited']])",
            "def test_launch_run_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.SUCCESS\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', msg) for msg in ['Started process for run', 'Executing steps using multiprocess executor', 'Multiprocess executor: parent process exiting', 'Process for run exited']])",
            "def test_launch_run_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=instance.get_ref())), StartRunResult)\n            assert res.success\n            finished_run = poll_for_finished_run(instance, run_id)\n            assert finished_run\n            assert finished_run.run_id == run_id\n            assert finished_run.status == DagsterRunStatus.SUCCESS\n            poll_for_event(instance, run_id, event_type='ENGINE_EVENT', message='Process for run exited')\n            event_records = instance.all_logs(run_id)\n            _check_event_log_contains(event_records, [('ENGINE_EVENT', msg) for msg in ['Started process for run', 'Executing steps using multiprocess executor', 'Multiprocess executor: parent process exiting', 'Process for run exited']])"
        ]
    },
    {
        "func_name": "test_launch_unloadable_run_grpc",
        "original": "def test_launch_unloadable_run_grpc():\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            with instance_for_test() as other_instance:\n                res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=other_instance.get_ref())), StartRunResult)\n                assert not res.success\n                assert f'gRPC server could not load run {run_id} in order to execute it. Make sure that the gRPC server has access to your run storage.' in res.serializable_error_info.message",
        "mutated": [
            "def test_launch_unloadable_run_grpc():\n    if False:\n        i = 10\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            with instance_for_test() as other_instance:\n                res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=other_instance.get_ref())), StartRunResult)\n                assert not res.success\n                assert f'gRPC server could not load run {run_id} in order to execute it. Make sure that the gRPC server has access to your run storage.' in res.serializable_error_info.message",
            "def test_launch_unloadable_run_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            with instance_for_test() as other_instance:\n                res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=other_instance.get_ref())), StartRunResult)\n                assert not res.success\n                assert f'gRPC server could not load run {run_id} in order to execute it. Make sure that the gRPC server has access to your run storage.' in res.serializable_error_info.message",
            "def test_launch_unloadable_run_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            with instance_for_test() as other_instance:\n                res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=other_instance.get_ref())), StartRunResult)\n                assert not res.success\n                assert f'gRPC server could not load run {run_id} in order to execute it. Make sure that the gRPC server has access to your run storage.' in res.serializable_error_info.message",
            "def test_launch_unloadable_run_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            with instance_for_test() as other_instance:\n                res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=other_instance.get_ref())), StartRunResult)\n                assert not res.success\n                assert f'gRPC server could not load run {run_id} in order to execute it. Make sure that the gRPC server has access to your run storage.' in res.serializable_error_info.message",
            "def test_launch_unloadable_run_grpc():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with instance_for_test() as instance:\n        with get_bar_repo_code_location(instance) as code_location:\n            job_handle = JobHandle('foo', code_location.get_repository('bar_repo').handle)\n            api_client = code_location.client\n            run = create_run_for_test(instance, 'foo')\n            run_id = run.run_id\n            with instance_for_test() as other_instance:\n                res = deserialize_value(api_client.start_run(ExecuteExternalJobArgs(job_origin=job_handle.get_external_origin(), run_id=run_id, instance_ref=other_instance.get_ref())), StartRunResult)\n                assert not res.success\n                assert f'gRPC server could not load run {run_id} in order to execute it. Make sure that the gRPC server has access to your run storage.' in res.serializable_error_info.message"
        ]
    }
]