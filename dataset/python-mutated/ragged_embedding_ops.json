[
    {
        "func_name": "embedding_lookup",
        "original": "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup)\ndef embedding_lookup(params, ids: ragged_tensor.Ragged, partition_strategy='mod', name=None, validate_indices=True, max_norm=None):\n    \"\"\"Look up the ragged ids in a list of embedding tensors.\n\n  Args:\n    params: A tensor representing the complete embedding tensor having the shape\n      [e1, ...eM]\n    ragged_ids: A 'RaggedTensor' with type 'int32' or 'int64' containing the ids\n      to be looked up in 'params' of shape [r0, ..rN]. Values must be in the\n      range '[0, params.shape[0]]'.\n    partition_strategy: A string specifying the partitioning strategy.\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\n      than this value.\n    name: A name for the operation (optional)\n\n  Returns:\n    A ragged tensor of shape [r0, r1, ...rN, e1, ...eM].\n\n  Raises:\n    ValueError: When params is empty or the type of the ids is not int32 or\n      int64.\n  \"\"\"\n    if params is None:\n        raise ValueError('params must be specified.')\n    if isinstance(params, (list, tuple)) and (not params):\n        raise ValueError('params should not be empty.')\n    if ids.dtype != dtypes.int32 and ids.dtype != dtypes.int64:\n        raise ValueError(f'The values contained by the inputs have type {str(ids.dtype)} and cannot be processed. All values should be indices, either of type `int32` or `int64`.')\n    with ops.name_scope(name, 'embedding_lookup_ragged') as name:\n        looked_up_ragged = ragged_functional_ops.map_flat_values(embedding_ops.embedding_lookup, params=params, ids=ids, partition_strategy=partition_strategy, max_norm=max_norm)\n        return looked_up_ragged",
        "mutated": [
            "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup)\ndef embedding_lookup(params, ids: ragged_tensor.Ragged, partition_strategy='mod', name=None, validate_indices=True, max_norm=None):\n    if False:\n        i = 10\n    \"Look up the ragged ids in a list of embedding tensors.\\n\\n  Args:\\n    params: A tensor representing the complete embedding tensor having the shape\\n      [e1, ...eM]\\n    ragged_ids: A 'RaggedTensor' with type 'int32' or 'int64' containing the ids\\n      to be looked up in 'params' of shape [r0, ..rN]. Values must be in the\\n      range '[0, params.shape[0]]'.\\n    partition_strategy: A string specifying the partitioning strategy.\\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\\n      than this value.\\n    name: A name for the operation (optional)\\n\\n  Returns:\\n    A ragged tensor of shape [r0, r1, ...rN, e1, ...eM].\\n\\n  Raises:\\n    ValueError: When params is empty or the type of the ids is not int32 or\\n      int64.\\n  \"\n    if params is None:\n        raise ValueError('params must be specified.')\n    if isinstance(params, (list, tuple)) and (not params):\n        raise ValueError('params should not be empty.')\n    if ids.dtype != dtypes.int32 and ids.dtype != dtypes.int64:\n        raise ValueError(f'The values contained by the inputs have type {str(ids.dtype)} and cannot be processed. All values should be indices, either of type `int32` or `int64`.')\n    with ops.name_scope(name, 'embedding_lookup_ragged') as name:\n        looked_up_ragged = ragged_functional_ops.map_flat_values(embedding_ops.embedding_lookup, params=params, ids=ids, partition_strategy=partition_strategy, max_norm=max_norm)\n        return looked_up_ragged",
            "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup)\ndef embedding_lookup(params, ids: ragged_tensor.Ragged, partition_strategy='mod', name=None, validate_indices=True, max_norm=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Look up the ragged ids in a list of embedding tensors.\\n\\n  Args:\\n    params: A tensor representing the complete embedding tensor having the shape\\n      [e1, ...eM]\\n    ragged_ids: A 'RaggedTensor' with type 'int32' or 'int64' containing the ids\\n      to be looked up in 'params' of shape [r0, ..rN]. Values must be in the\\n      range '[0, params.shape[0]]'.\\n    partition_strategy: A string specifying the partitioning strategy.\\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\\n      than this value.\\n    name: A name for the operation (optional)\\n\\n  Returns:\\n    A ragged tensor of shape [r0, r1, ...rN, e1, ...eM].\\n\\n  Raises:\\n    ValueError: When params is empty or the type of the ids is not int32 or\\n      int64.\\n  \"\n    if params is None:\n        raise ValueError('params must be specified.')\n    if isinstance(params, (list, tuple)) and (not params):\n        raise ValueError('params should not be empty.')\n    if ids.dtype != dtypes.int32 and ids.dtype != dtypes.int64:\n        raise ValueError(f'The values contained by the inputs have type {str(ids.dtype)} and cannot be processed. All values should be indices, either of type `int32` or `int64`.')\n    with ops.name_scope(name, 'embedding_lookup_ragged') as name:\n        looked_up_ragged = ragged_functional_ops.map_flat_values(embedding_ops.embedding_lookup, params=params, ids=ids, partition_strategy=partition_strategy, max_norm=max_norm)\n        return looked_up_ragged",
            "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup)\ndef embedding_lookup(params, ids: ragged_tensor.Ragged, partition_strategy='mod', name=None, validate_indices=True, max_norm=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Look up the ragged ids in a list of embedding tensors.\\n\\n  Args:\\n    params: A tensor representing the complete embedding tensor having the shape\\n      [e1, ...eM]\\n    ragged_ids: A 'RaggedTensor' with type 'int32' or 'int64' containing the ids\\n      to be looked up in 'params' of shape [r0, ..rN]. Values must be in the\\n      range '[0, params.shape[0]]'.\\n    partition_strategy: A string specifying the partitioning strategy.\\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\\n      than this value.\\n    name: A name for the operation (optional)\\n\\n  Returns:\\n    A ragged tensor of shape [r0, r1, ...rN, e1, ...eM].\\n\\n  Raises:\\n    ValueError: When params is empty or the type of the ids is not int32 or\\n      int64.\\n  \"\n    if params is None:\n        raise ValueError('params must be specified.')\n    if isinstance(params, (list, tuple)) and (not params):\n        raise ValueError('params should not be empty.')\n    if ids.dtype != dtypes.int32 and ids.dtype != dtypes.int64:\n        raise ValueError(f'The values contained by the inputs have type {str(ids.dtype)} and cannot be processed. All values should be indices, either of type `int32` or `int64`.')\n    with ops.name_scope(name, 'embedding_lookup_ragged') as name:\n        looked_up_ragged = ragged_functional_ops.map_flat_values(embedding_ops.embedding_lookup, params=params, ids=ids, partition_strategy=partition_strategy, max_norm=max_norm)\n        return looked_up_ragged",
            "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup)\ndef embedding_lookup(params, ids: ragged_tensor.Ragged, partition_strategy='mod', name=None, validate_indices=True, max_norm=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Look up the ragged ids in a list of embedding tensors.\\n\\n  Args:\\n    params: A tensor representing the complete embedding tensor having the shape\\n      [e1, ...eM]\\n    ragged_ids: A 'RaggedTensor' with type 'int32' or 'int64' containing the ids\\n      to be looked up in 'params' of shape [r0, ..rN]. Values must be in the\\n      range '[0, params.shape[0]]'.\\n    partition_strategy: A string specifying the partitioning strategy.\\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\\n      than this value.\\n    name: A name for the operation (optional)\\n\\n  Returns:\\n    A ragged tensor of shape [r0, r1, ...rN, e1, ...eM].\\n\\n  Raises:\\n    ValueError: When params is empty or the type of the ids is not int32 or\\n      int64.\\n  \"\n    if params is None:\n        raise ValueError('params must be specified.')\n    if isinstance(params, (list, tuple)) and (not params):\n        raise ValueError('params should not be empty.')\n    if ids.dtype != dtypes.int32 and ids.dtype != dtypes.int64:\n        raise ValueError(f'The values contained by the inputs have type {str(ids.dtype)} and cannot be processed. All values should be indices, either of type `int32` or `int64`.')\n    with ops.name_scope(name, 'embedding_lookup_ragged') as name:\n        looked_up_ragged = ragged_functional_ops.map_flat_values(embedding_ops.embedding_lookup, params=params, ids=ids, partition_strategy=partition_strategy, max_norm=max_norm)\n        return looked_up_ragged",
            "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup)\ndef embedding_lookup(params, ids: ragged_tensor.Ragged, partition_strategy='mod', name=None, validate_indices=True, max_norm=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Look up the ragged ids in a list of embedding tensors.\\n\\n  Args:\\n    params: A tensor representing the complete embedding tensor having the shape\\n      [e1, ...eM]\\n    ragged_ids: A 'RaggedTensor' with type 'int32' or 'int64' containing the ids\\n      to be looked up in 'params' of shape [r0, ..rN]. Values must be in the\\n      range '[0, params.shape[0]]'.\\n    partition_strategy: A string specifying the partitioning strategy.\\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\\n      than this value.\\n    name: A name for the operation (optional)\\n\\n  Returns:\\n    A ragged tensor of shape [r0, r1, ...rN, e1, ...eM].\\n\\n  Raises:\\n    ValueError: When params is empty or the type of the ids is not int32 or\\n      int64.\\n  \"\n    if params is None:\n        raise ValueError('params must be specified.')\n    if isinstance(params, (list, tuple)) and (not params):\n        raise ValueError('params should not be empty.')\n    if ids.dtype != dtypes.int32 and ids.dtype != dtypes.int64:\n        raise ValueError(f'The values contained by the inputs have type {str(ids.dtype)} and cannot be processed. All values should be indices, either of type `int32` or `int64`.')\n    with ops.name_scope(name, 'embedding_lookup_ragged') as name:\n        looked_up_ragged = ragged_functional_ops.map_flat_values(embedding_ops.embedding_lookup, params=params, ids=ids, partition_strategy=partition_strategy, max_norm=max_norm)\n        return looked_up_ragged"
        ]
    },
    {
        "func_name": "embedding_lookup_sparse",
        "original": "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup_sparse)\ndef embedding_lookup_sparse(params, sp_ids: ragged_tensor.Ragged, sp_weights, partition_strategy='mod', name=None, combiner=None, max_norm=None, allow_fast_lookup=False):\n    \"\"\"Looks up embeddings for the given ids and weights from a list of tensors.\n\n  This op assumes that there is at least one id for each row in the dense tensor\n  represented by sp_ids (i.e. there are no rows with empty features), and that\n  all the indices of sp_ids are in canonical row-major order.\n\n  `sp_ids` and `sp_weights` (if not None) are `RaggedTensor`s with rank of 2.\n  Embeddings are always aggregated along the last dimension.\n\n  It also assumes that all id values lie in the range [0, p0), where p0\n  is the sum of the size of params along dimension 0.\n\n  Args:\n    params: A single tensor representing the complete embedding tensor, or a\n      list tensors all of same shape except for the first dimension,\n      representing sharded embedding tensors. Alternatively, a\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\n      element must be appropriately sized for the given `partition_strategy`.\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\n      reasons.\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\n      containing float / double weights corresponding to `sparse_ids`, or `None`\n      if all weights are assumed to be 1.0.\n    partition_strategy: A string specifying the partitioning strategy, relevant\n      if `len(params) > 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\n      is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\n    name: Optional name for the op.\n    combiner: A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\n      and \"sum\" are supported. \"sum\" computes the weighted sum of the embedding\n      results for each row. \"mean\" is the weighted sum divided by the total\n      weight. \"sqrtn\" is the weighted sum divided by the square root of the sum\n      of the squares of the weights. Defaults to `mean`.\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\n      than this value, before combining.\n    allow_fast_lookup: An optional boolean specifying whether to allow\n      simplified embedding lookups when `params` is a single tensor and\n      `max_norm` is `None`. Setting this flag to `True` during training can\n      cause the use of dense gradients with increased memory footprint.\n\n  Returns:\n    A dense tensor representing the combined embeddings for the\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\n    looks up the embeddings for all ids in that row, multiplies them by the\n    corresponding weight, and combines these embeddings as specified.\n\n    In other words, if\n\n      `shape(combined params) = [p0, p1, ..., pm]`\n\n    and\n\n      `shape(sp_ids) = shape(sp_weights) = [d0, d1]`\n\n    then\n\n      `shape(output) = [d0, p1, ..., pm]`.\n\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\n\n      ```python\n      [0, 0]: id 1, weight 2.0\n      [0, 1]: id 3, weight 0.5\n      [1, 0]: id 0, weight 1.0\n      [2, 3]: id 1, weight 3.0\n      ```\n\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\n\n      ```python\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\n      output[1, :] = (params[0, :] * 1.0) / 1.0\n      output[2, :] = (params[1, :] * 3.0) / 3.0\n      ```\n\n  Raises:\n    TypeError: If `sp_weights` is neither `None` nor of the same type as\n      `sp_ids`.\n    ValueError: If `combiner` is not one of {\"mean\", \"sqrtn\", \"sum\"}.\n  \"\"\"\n    rt_ids = sp_ids\n    rt_weights = sp_weights\n    if combiner is None:\n        combiner = 'mean'\n    if combiner not in ('mean', 'sqrtn', 'sum'):\n        raise ValueError(f\"combiner must be one of 'mean', 'sqrtn' or 'sum', got {combiner}\")\n    if isinstance(params, variables.PartitionedVariable):\n        params = list(params)\n    if not isinstance(params, list):\n        params = [params]\n    ignore_weights = rt_weights is None\n    if not ignore_weights:\n        if not isinstance(rt_weights, ragged_tensor.RaggedTensor):\n            raise TypeError(f'sp_ids must be of the same type as sp_weights, received {{type(sp_ids).__name__!r}} for sp_ids and {{type(sp_weights).__name__!r}} for sp_weights.')\n        rt_ids.values.get_shape().assert_is_compatible_with(rt_weights.values.get_shape())\n        rt_ids.get_shape().assert_is_compatible_with(rt_weights.get_shape())\n    with ops.name_scope(name, 'embedding_lookup_sparse', params + [rt_ids]) as name:\n        segment_ids = rt_ids.value_rowids()\n        ids = rt_ids.flat_values\n        return embedding_ops.embedding_lookup_sparse_impl(params, segment_ids, sp_weights, ids, combiner, ignore_weights, max_norm, allow_fast_lookup, partition_strategy, name)",
        "mutated": [
            "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup_sparse)\ndef embedding_lookup_sparse(params, sp_ids: ragged_tensor.Ragged, sp_weights, partition_strategy='mod', name=None, combiner=None, max_norm=None, allow_fast_lookup=False):\n    if False:\n        i = 10\n    'Looks up embeddings for the given ids and weights from a list of tensors.\\n\\n  This op assumes that there is at least one id for each row in the dense tensor\\n  represented by sp_ids (i.e. there are no rows with empty features), and that\\n  all the indices of sp_ids are in canonical row-major order.\\n\\n  `sp_ids` and `sp_weights` (if not None) are `RaggedTensor`s with rank of 2.\\n  Embeddings are always aggregated along the last dimension.\\n\\n  It also assumes that all id values lie in the range [0, p0), where p0\\n  is the sum of the size of params along dimension 0.\\n\\n  Args:\\n    params: A single tensor representing the complete embedding tensor, or a\\n      list tensors all of same shape except for the first dimension,\\n      representing sharded embedding tensors. Alternatively, a\\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\\n      element must be appropriately sized for the given `partition_strategy`.\\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\\n      reasons.\\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\\n      containing float / double weights corresponding to `sparse_ids`, or `None`\\n      if all weights are assumed to be 1.0.\\n    partition_strategy: A string specifying the partitioning strategy, relevant\\n      if `len(params) > 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\\n      is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\\n    name: Optional name for the op.\\n    combiner: A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\\n      and \"sum\" are supported. \"sum\" computes the weighted sum of the embedding\\n      results for each row. \"mean\" is the weighted sum divided by the total\\n      weight. \"sqrtn\" is the weighted sum divided by the square root of the sum\\n      of the squares of the weights. Defaults to `mean`.\\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\\n      than this value, before combining.\\n    allow_fast_lookup: An optional boolean specifying whether to allow\\n      simplified embedding lookups when `params` is a single tensor and\\n      `max_norm` is `None`. Setting this flag to `True` during training can\\n      cause the use of dense gradients with increased memory footprint.\\n\\n  Returns:\\n    A dense tensor representing the combined embeddings for the\\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\\n    looks up the embeddings for all ids in that row, multiplies them by the\\n    corresponding weight, and combines these embeddings as specified.\\n\\n    In other words, if\\n\\n      `shape(combined params) = [p0, p1, ..., pm]`\\n\\n    and\\n\\n      `shape(sp_ids) = shape(sp_weights) = [d0, d1]`\\n\\n    then\\n\\n      `shape(output) = [d0, p1, ..., pm]`.\\n\\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\\n\\n      ```python\\n      [0, 0]: id 1, weight 2.0\\n      [0, 1]: id 3, weight 0.5\\n      [1, 0]: id 0, weight 1.0\\n      [2, 3]: id 1, weight 3.0\\n      ```\\n\\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\\n\\n      ```python\\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\\n      output[1, :] = (params[0, :] * 1.0) / 1.0\\n      output[2, :] = (params[1, :] * 3.0) / 3.0\\n      ```\\n\\n  Raises:\\n    TypeError: If `sp_weights` is neither `None` nor of the same type as\\n      `sp_ids`.\\n    ValueError: If `combiner` is not one of {\"mean\", \"sqrtn\", \"sum\"}.\\n  '\n    rt_ids = sp_ids\n    rt_weights = sp_weights\n    if combiner is None:\n        combiner = 'mean'\n    if combiner not in ('mean', 'sqrtn', 'sum'):\n        raise ValueError(f\"combiner must be one of 'mean', 'sqrtn' or 'sum', got {combiner}\")\n    if isinstance(params, variables.PartitionedVariable):\n        params = list(params)\n    if not isinstance(params, list):\n        params = [params]\n    ignore_weights = rt_weights is None\n    if not ignore_weights:\n        if not isinstance(rt_weights, ragged_tensor.RaggedTensor):\n            raise TypeError(f'sp_ids must be of the same type as sp_weights, received {{type(sp_ids).__name__!r}} for sp_ids and {{type(sp_weights).__name__!r}} for sp_weights.')\n        rt_ids.values.get_shape().assert_is_compatible_with(rt_weights.values.get_shape())\n        rt_ids.get_shape().assert_is_compatible_with(rt_weights.get_shape())\n    with ops.name_scope(name, 'embedding_lookup_sparse', params + [rt_ids]) as name:\n        segment_ids = rt_ids.value_rowids()\n        ids = rt_ids.flat_values\n        return embedding_ops.embedding_lookup_sparse_impl(params, segment_ids, sp_weights, ids, combiner, ignore_weights, max_norm, allow_fast_lookup, partition_strategy, name)",
            "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup_sparse)\ndef embedding_lookup_sparse(params, sp_ids: ragged_tensor.Ragged, sp_weights, partition_strategy='mod', name=None, combiner=None, max_norm=None, allow_fast_lookup=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Looks up embeddings for the given ids and weights from a list of tensors.\\n\\n  This op assumes that there is at least one id for each row in the dense tensor\\n  represented by sp_ids (i.e. there are no rows with empty features), and that\\n  all the indices of sp_ids are in canonical row-major order.\\n\\n  `sp_ids` and `sp_weights` (if not None) are `RaggedTensor`s with rank of 2.\\n  Embeddings are always aggregated along the last dimension.\\n\\n  It also assumes that all id values lie in the range [0, p0), where p0\\n  is the sum of the size of params along dimension 0.\\n\\n  Args:\\n    params: A single tensor representing the complete embedding tensor, or a\\n      list tensors all of same shape except for the first dimension,\\n      representing sharded embedding tensors. Alternatively, a\\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\\n      element must be appropriately sized for the given `partition_strategy`.\\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\\n      reasons.\\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\\n      containing float / double weights corresponding to `sparse_ids`, or `None`\\n      if all weights are assumed to be 1.0.\\n    partition_strategy: A string specifying the partitioning strategy, relevant\\n      if `len(params) > 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\\n      is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\\n    name: Optional name for the op.\\n    combiner: A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\\n      and \"sum\" are supported. \"sum\" computes the weighted sum of the embedding\\n      results for each row. \"mean\" is the weighted sum divided by the total\\n      weight. \"sqrtn\" is the weighted sum divided by the square root of the sum\\n      of the squares of the weights. Defaults to `mean`.\\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\\n      than this value, before combining.\\n    allow_fast_lookup: An optional boolean specifying whether to allow\\n      simplified embedding lookups when `params` is a single tensor and\\n      `max_norm` is `None`. Setting this flag to `True` during training can\\n      cause the use of dense gradients with increased memory footprint.\\n\\n  Returns:\\n    A dense tensor representing the combined embeddings for the\\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\\n    looks up the embeddings for all ids in that row, multiplies them by the\\n    corresponding weight, and combines these embeddings as specified.\\n\\n    In other words, if\\n\\n      `shape(combined params) = [p0, p1, ..., pm]`\\n\\n    and\\n\\n      `shape(sp_ids) = shape(sp_weights) = [d0, d1]`\\n\\n    then\\n\\n      `shape(output) = [d0, p1, ..., pm]`.\\n\\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\\n\\n      ```python\\n      [0, 0]: id 1, weight 2.0\\n      [0, 1]: id 3, weight 0.5\\n      [1, 0]: id 0, weight 1.0\\n      [2, 3]: id 1, weight 3.0\\n      ```\\n\\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\\n\\n      ```python\\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\\n      output[1, :] = (params[0, :] * 1.0) / 1.0\\n      output[2, :] = (params[1, :] * 3.0) / 3.0\\n      ```\\n\\n  Raises:\\n    TypeError: If `sp_weights` is neither `None` nor of the same type as\\n      `sp_ids`.\\n    ValueError: If `combiner` is not one of {\"mean\", \"sqrtn\", \"sum\"}.\\n  '\n    rt_ids = sp_ids\n    rt_weights = sp_weights\n    if combiner is None:\n        combiner = 'mean'\n    if combiner not in ('mean', 'sqrtn', 'sum'):\n        raise ValueError(f\"combiner must be one of 'mean', 'sqrtn' or 'sum', got {combiner}\")\n    if isinstance(params, variables.PartitionedVariable):\n        params = list(params)\n    if not isinstance(params, list):\n        params = [params]\n    ignore_weights = rt_weights is None\n    if not ignore_weights:\n        if not isinstance(rt_weights, ragged_tensor.RaggedTensor):\n            raise TypeError(f'sp_ids must be of the same type as sp_weights, received {{type(sp_ids).__name__!r}} for sp_ids and {{type(sp_weights).__name__!r}} for sp_weights.')\n        rt_ids.values.get_shape().assert_is_compatible_with(rt_weights.values.get_shape())\n        rt_ids.get_shape().assert_is_compatible_with(rt_weights.get_shape())\n    with ops.name_scope(name, 'embedding_lookup_sparse', params + [rt_ids]) as name:\n        segment_ids = rt_ids.value_rowids()\n        ids = rt_ids.flat_values\n        return embedding_ops.embedding_lookup_sparse_impl(params, segment_ids, sp_weights, ids, combiner, ignore_weights, max_norm, allow_fast_lookup, partition_strategy, name)",
            "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup_sparse)\ndef embedding_lookup_sparse(params, sp_ids: ragged_tensor.Ragged, sp_weights, partition_strategy='mod', name=None, combiner=None, max_norm=None, allow_fast_lookup=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Looks up embeddings for the given ids and weights from a list of tensors.\\n\\n  This op assumes that there is at least one id for each row in the dense tensor\\n  represented by sp_ids (i.e. there are no rows with empty features), and that\\n  all the indices of sp_ids are in canonical row-major order.\\n\\n  `sp_ids` and `sp_weights` (if not None) are `RaggedTensor`s with rank of 2.\\n  Embeddings are always aggregated along the last dimension.\\n\\n  It also assumes that all id values lie in the range [0, p0), where p0\\n  is the sum of the size of params along dimension 0.\\n\\n  Args:\\n    params: A single tensor representing the complete embedding tensor, or a\\n      list tensors all of same shape except for the first dimension,\\n      representing sharded embedding tensors. Alternatively, a\\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\\n      element must be appropriately sized for the given `partition_strategy`.\\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\\n      reasons.\\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\\n      containing float / double weights corresponding to `sparse_ids`, or `None`\\n      if all weights are assumed to be 1.0.\\n    partition_strategy: A string specifying the partitioning strategy, relevant\\n      if `len(params) > 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\\n      is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\\n    name: Optional name for the op.\\n    combiner: A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\\n      and \"sum\" are supported. \"sum\" computes the weighted sum of the embedding\\n      results for each row. \"mean\" is the weighted sum divided by the total\\n      weight. \"sqrtn\" is the weighted sum divided by the square root of the sum\\n      of the squares of the weights. Defaults to `mean`.\\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\\n      than this value, before combining.\\n    allow_fast_lookup: An optional boolean specifying whether to allow\\n      simplified embedding lookups when `params` is a single tensor and\\n      `max_norm` is `None`. Setting this flag to `True` during training can\\n      cause the use of dense gradients with increased memory footprint.\\n\\n  Returns:\\n    A dense tensor representing the combined embeddings for the\\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\\n    looks up the embeddings for all ids in that row, multiplies them by the\\n    corresponding weight, and combines these embeddings as specified.\\n\\n    In other words, if\\n\\n      `shape(combined params) = [p0, p1, ..., pm]`\\n\\n    and\\n\\n      `shape(sp_ids) = shape(sp_weights) = [d0, d1]`\\n\\n    then\\n\\n      `shape(output) = [d0, p1, ..., pm]`.\\n\\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\\n\\n      ```python\\n      [0, 0]: id 1, weight 2.0\\n      [0, 1]: id 3, weight 0.5\\n      [1, 0]: id 0, weight 1.0\\n      [2, 3]: id 1, weight 3.0\\n      ```\\n\\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\\n\\n      ```python\\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\\n      output[1, :] = (params[0, :] * 1.0) / 1.0\\n      output[2, :] = (params[1, :] * 3.0) / 3.0\\n      ```\\n\\n  Raises:\\n    TypeError: If `sp_weights` is neither `None` nor of the same type as\\n      `sp_ids`.\\n    ValueError: If `combiner` is not one of {\"mean\", \"sqrtn\", \"sum\"}.\\n  '\n    rt_ids = sp_ids\n    rt_weights = sp_weights\n    if combiner is None:\n        combiner = 'mean'\n    if combiner not in ('mean', 'sqrtn', 'sum'):\n        raise ValueError(f\"combiner must be one of 'mean', 'sqrtn' or 'sum', got {combiner}\")\n    if isinstance(params, variables.PartitionedVariable):\n        params = list(params)\n    if not isinstance(params, list):\n        params = [params]\n    ignore_weights = rt_weights is None\n    if not ignore_weights:\n        if not isinstance(rt_weights, ragged_tensor.RaggedTensor):\n            raise TypeError(f'sp_ids must be of the same type as sp_weights, received {{type(sp_ids).__name__!r}} for sp_ids and {{type(sp_weights).__name__!r}} for sp_weights.')\n        rt_ids.values.get_shape().assert_is_compatible_with(rt_weights.values.get_shape())\n        rt_ids.get_shape().assert_is_compatible_with(rt_weights.get_shape())\n    with ops.name_scope(name, 'embedding_lookup_sparse', params + [rt_ids]) as name:\n        segment_ids = rt_ids.value_rowids()\n        ids = rt_ids.flat_values\n        return embedding_ops.embedding_lookup_sparse_impl(params, segment_ids, sp_weights, ids, combiner, ignore_weights, max_norm, allow_fast_lookup, partition_strategy, name)",
            "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup_sparse)\ndef embedding_lookup_sparse(params, sp_ids: ragged_tensor.Ragged, sp_weights, partition_strategy='mod', name=None, combiner=None, max_norm=None, allow_fast_lookup=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Looks up embeddings for the given ids and weights from a list of tensors.\\n\\n  This op assumes that there is at least one id for each row in the dense tensor\\n  represented by sp_ids (i.e. there are no rows with empty features), and that\\n  all the indices of sp_ids are in canonical row-major order.\\n\\n  `sp_ids` and `sp_weights` (if not None) are `RaggedTensor`s with rank of 2.\\n  Embeddings are always aggregated along the last dimension.\\n\\n  It also assumes that all id values lie in the range [0, p0), where p0\\n  is the sum of the size of params along dimension 0.\\n\\n  Args:\\n    params: A single tensor representing the complete embedding tensor, or a\\n      list tensors all of same shape except for the first dimension,\\n      representing sharded embedding tensors. Alternatively, a\\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\\n      element must be appropriately sized for the given `partition_strategy`.\\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\\n      reasons.\\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\\n      containing float / double weights corresponding to `sparse_ids`, or `None`\\n      if all weights are assumed to be 1.0.\\n    partition_strategy: A string specifying the partitioning strategy, relevant\\n      if `len(params) > 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\\n      is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\\n    name: Optional name for the op.\\n    combiner: A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\\n      and \"sum\" are supported. \"sum\" computes the weighted sum of the embedding\\n      results for each row. \"mean\" is the weighted sum divided by the total\\n      weight. \"sqrtn\" is the weighted sum divided by the square root of the sum\\n      of the squares of the weights. Defaults to `mean`.\\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\\n      than this value, before combining.\\n    allow_fast_lookup: An optional boolean specifying whether to allow\\n      simplified embedding lookups when `params` is a single tensor and\\n      `max_norm` is `None`. Setting this flag to `True` during training can\\n      cause the use of dense gradients with increased memory footprint.\\n\\n  Returns:\\n    A dense tensor representing the combined embeddings for the\\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\\n    looks up the embeddings for all ids in that row, multiplies them by the\\n    corresponding weight, and combines these embeddings as specified.\\n\\n    In other words, if\\n\\n      `shape(combined params) = [p0, p1, ..., pm]`\\n\\n    and\\n\\n      `shape(sp_ids) = shape(sp_weights) = [d0, d1]`\\n\\n    then\\n\\n      `shape(output) = [d0, p1, ..., pm]`.\\n\\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\\n\\n      ```python\\n      [0, 0]: id 1, weight 2.0\\n      [0, 1]: id 3, weight 0.5\\n      [1, 0]: id 0, weight 1.0\\n      [2, 3]: id 1, weight 3.0\\n      ```\\n\\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\\n\\n      ```python\\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\\n      output[1, :] = (params[0, :] * 1.0) / 1.0\\n      output[2, :] = (params[1, :] * 3.0) / 3.0\\n      ```\\n\\n  Raises:\\n    TypeError: If `sp_weights` is neither `None` nor of the same type as\\n      `sp_ids`.\\n    ValueError: If `combiner` is not one of {\"mean\", \"sqrtn\", \"sum\"}.\\n  '\n    rt_ids = sp_ids\n    rt_weights = sp_weights\n    if combiner is None:\n        combiner = 'mean'\n    if combiner not in ('mean', 'sqrtn', 'sum'):\n        raise ValueError(f\"combiner must be one of 'mean', 'sqrtn' or 'sum', got {combiner}\")\n    if isinstance(params, variables.PartitionedVariable):\n        params = list(params)\n    if not isinstance(params, list):\n        params = [params]\n    ignore_weights = rt_weights is None\n    if not ignore_weights:\n        if not isinstance(rt_weights, ragged_tensor.RaggedTensor):\n            raise TypeError(f'sp_ids must be of the same type as sp_weights, received {{type(sp_ids).__name__!r}} for sp_ids and {{type(sp_weights).__name__!r}} for sp_weights.')\n        rt_ids.values.get_shape().assert_is_compatible_with(rt_weights.values.get_shape())\n        rt_ids.get_shape().assert_is_compatible_with(rt_weights.get_shape())\n    with ops.name_scope(name, 'embedding_lookup_sparse', params + [rt_ids]) as name:\n        segment_ids = rt_ids.value_rowids()\n        ids = rt_ids.flat_values\n        return embedding_ops.embedding_lookup_sparse_impl(params, segment_ids, sp_weights, ids, combiner, ignore_weights, max_norm, allow_fast_lookup, partition_strategy, name)",
            "@dispatch.dispatch_for_api(embedding_ops.embedding_lookup_sparse)\ndef embedding_lookup_sparse(params, sp_ids: ragged_tensor.Ragged, sp_weights, partition_strategy='mod', name=None, combiner=None, max_norm=None, allow_fast_lookup=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Looks up embeddings for the given ids and weights from a list of tensors.\\n\\n  This op assumes that there is at least one id for each row in the dense tensor\\n  represented by sp_ids (i.e. there are no rows with empty features), and that\\n  all the indices of sp_ids are in canonical row-major order.\\n\\n  `sp_ids` and `sp_weights` (if not None) are `RaggedTensor`s with rank of 2.\\n  Embeddings are always aggregated along the last dimension.\\n\\n  It also assumes that all id values lie in the range [0, p0), where p0\\n  is the sum of the size of params along dimension 0.\\n\\n  Args:\\n    params: A single tensor representing the complete embedding tensor, or a\\n      list tensors all of same shape except for the first dimension,\\n      representing sharded embedding tensors. Alternatively, a\\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\\n      element must be appropriately sized for the given `partition_strategy`.\\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\\n      reasons.\\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\\n      containing float / double weights corresponding to `sparse_ids`, or `None`\\n      if all weights are assumed to be 1.0.\\n    partition_strategy: A string specifying the partitioning strategy, relevant\\n      if `len(params) > 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\\n      is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\\n    name: Optional name for the op.\\n    combiner: A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\\n      and \"sum\" are supported. \"sum\" computes the weighted sum of the embedding\\n      results for each row. \"mean\" is the weighted sum divided by the total\\n      weight. \"sqrtn\" is the weighted sum divided by the square root of the sum\\n      of the squares of the weights. Defaults to `mean`.\\n    max_norm: If not `None`, each embedding is clipped if its l2-norm is larger\\n      than this value, before combining.\\n    allow_fast_lookup: An optional boolean specifying whether to allow\\n      simplified embedding lookups when `params` is a single tensor and\\n      `max_norm` is `None`. Setting this flag to `True` during training can\\n      cause the use of dense gradients with increased memory footprint.\\n\\n  Returns:\\n    A dense tensor representing the combined embeddings for the\\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\\n    looks up the embeddings for all ids in that row, multiplies them by the\\n    corresponding weight, and combines these embeddings as specified.\\n\\n    In other words, if\\n\\n      `shape(combined params) = [p0, p1, ..., pm]`\\n\\n    and\\n\\n      `shape(sp_ids) = shape(sp_weights) = [d0, d1]`\\n\\n    then\\n\\n      `shape(output) = [d0, p1, ..., pm]`.\\n\\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\\n\\n      ```python\\n      [0, 0]: id 1, weight 2.0\\n      [0, 1]: id 3, weight 0.5\\n      [1, 0]: id 0, weight 1.0\\n      [2, 3]: id 1, weight 3.0\\n      ```\\n\\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\\n\\n      ```python\\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\\n      output[1, :] = (params[0, :] * 1.0) / 1.0\\n      output[2, :] = (params[1, :] * 3.0) / 3.0\\n      ```\\n\\n  Raises:\\n    TypeError: If `sp_weights` is neither `None` nor of the same type as\\n      `sp_ids`.\\n    ValueError: If `combiner` is not one of {\"mean\", \"sqrtn\", \"sum\"}.\\n  '\n    rt_ids = sp_ids\n    rt_weights = sp_weights\n    if combiner is None:\n        combiner = 'mean'\n    if combiner not in ('mean', 'sqrtn', 'sum'):\n        raise ValueError(f\"combiner must be one of 'mean', 'sqrtn' or 'sum', got {combiner}\")\n    if isinstance(params, variables.PartitionedVariable):\n        params = list(params)\n    if not isinstance(params, list):\n        params = [params]\n    ignore_weights = rt_weights is None\n    if not ignore_weights:\n        if not isinstance(rt_weights, ragged_tensor.RaggedTensor):\n            raise TypeError(f'sp_ids must be of the same type as sp_weights, received {{type(sp_ids).__name__!r}} for sp_ids and {{type(sp_weights).__name__!r}} for sp_weights.')\n        rt_ids.values.get_shape().assert_is_compatible_with(rt_weights.values.get_shape())\n        rt_ids.get_shape().assert_is_compatible_with(rt_weights.get_shape())\n    with ops.name_scope(name, 'embedding_lookup_sparse', params + [rt_ids]) as name:\n        segment_ids = rt_ids.value_rowids()\n        ids = rt_ids.flat_values\n        return embedding_ops.embedding_lookup_sparse_impl(params, segment_ids, sp_weights, ids, combiner, ignore_weights, max_norm, allow_fast_lookup, partition_strategy, name)"
        ]
    },
    {
        "func_name": "safe_embedding_lookup_sparse",
        "original": "@dispatch.dispatch_for_api(embedding_ops.safe_embedding_lookup_sparse)\ndef safe_embedding_lookup_sparse(embedding_weights, sparse_ids: ragged_tensor.Ragged, sparse_weights=None, combiner='mean', default_id=None, name=None, partition_strategy='div', max_norm=None, allow_fast_lookup=False):\n    \"\"\"Lookup embedding results, accounting for invalid IDs and empty features.\n\n  The partitioned embedding in `embedding_weights` must all be the same shape\n  except for the first dimension. The first dimension is allowed to vary as the\n  vocabulary size is not necessarily a multiple of `P`.  `embedding_weights`\n  may be a `PartitionedVariable` as returned by using\n  `tf.compat.v1.get_variable()` with a\n  partitioner.\n\n  Invalid IDs (< 0) are pruned from input IDs and weights, as well as any IDs\n  with non-positive weight. For an entry with no features, the embedding vector\n  for `default_id` is returned, or the 0-vector if `default_id` is not supplied.\n\n  The ids and weights may be multi-dimensional `SparseTensor`s or\n  `RaggedTensor`s with rank of 2. For `SpareTensor`s with left-aligned non-zero\n  entries which can be described as `RaggedTensor`s, use of `RaggedTensor`s can\n  yield higher performance. Embeddings are always aggregated along the last\n  dimension.\n\n  Args:\n    embedding_weights: A single tensor representing the complete embedding\n      tensor, or a list tensors all of same shape except for the first\n      dimension, representing sharded embedding tensors. Alternatively, a\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\n      element must be appropriately sized for the given `partition_strategy`.\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\n      reasons.\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\n      containing float weights corresponding to `sparse_ids`, or `None` if all\n      weights are assumed to be 1.0.\n    combiner: A string specifying how to combine embedding results for each\n      entry. Currently \"mean\", \"sqrtn\" and \"sum\" are supported, with \"mean\" the\n      default.\n    default_id: The id to use for an entry with no features.\n    name: A name for this operation (optional).\n    partition_strategy: A string specifying the partitioning strategy. Currently\n      `\"div\"` and `\"mod\"` are supported. Default is `\"div\"`.\n    max_norm: If not `None`, all embeddings are l2-normalized to max_norm before\n      combining.\n    allow_fast_lookup: An optional boolean specifying whether to allow\n      simplified embedding lookups when `params` is a single tensor and\n      `max_norm` is `None`. Setting this flag to `True` during training can\n      cause the use of dense gradients with increased memory footprint.\n\n  Returns:\n    A dense tensor representing the combined embeddings for the\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\n    looks up the embeddings for all ids in that row, multiplies them by the\n    corresponding weight, and combines these embeddings as specified.\n\n    In other words, if\n\n      `shape(combined embedding_weights) = [p0, p1, ..., pm]`\n\n    and\n\n      `shape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]`\n\n    then\n\n      `shape(output) = [d0, d1, ... dn-1, p1, ..., pm]`.\n\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\n\n      ```python\n      [0, 0]: id 1, weight 2.0\n      [0, 1]: id 3, weight 0.5\n      [1, 0]: id -1, weight 1.0\n      [2, 3]: id 1, weight 3.0\n      ```\n\n    `default_id` is 0.\n\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\n\n      ```python\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\n      output[1, :] = (params[0, :] * 1.0) / 1.0\n      output[2, :] = (params[1, :] * 3.0) / 3.0\n      ```\n\n  Raises:\n    ValueError: if `embedding_weights` is empty.\n  \"\"\"\n    ragged_ids = sparse_ids\n    ragged_weights = sparse_weights\n    if embedding_weights is None:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    if isinstance(embedding_weights, variables.PartitionedVariable):\n        embedding_weights = list(embedding_weights)\n    if not isinstance(embedding_weights, list):\n        embedding_weights = [embedding_weights]\n    if len(embedding_weights) < 1:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    dtype = ragged_weights.dtype if ragged_weights is not None else None\n    embedding_weights = [w if resource_variable_ops.is_resource_variable(w) and dtype in (None, w.dtype) else ops.convert_to_tensor(w, dtype=dtype) for w in embedding_weights]\n    with ops.name_scope(name, 'embedding_lookup', embedding_weights + [ragged_ids, ragged_weights]) as scope:\n        (ragged_ids, ragged_weights) = _prune_invalid_ids_ragged(ragged_ids, ragged_weights)\n        if combiner != 'sum':\n            (ragged_ids, ragged_weights) = _prune_invalid_weights_ragged(ragged_ids, ragged_weights)\n        (ragged_ids, is_row_empty) = ragged_array_ops.fill_empty_rows(ragged_ids, default_id or 0)\n        if ragged_weights is not None:\n            (ragged_weights, _) = ragged_array_ops.fill_empty_rows(ragged_weights, 1.0)\n        result = embedding_lookup_sparse(embedding_weights, ragged_ids, ragged_weights, combiner=combiner, partition_strategy=partition_strategy, name=None if default_id is None else scope, max_norm=max_norm, allow_fast_lookup=allow_fast_lookup)\n        if default_id is None:\n            is_row_empty = array_ops.tile(array_ops.reshape(is_row_empty, [-1, 1]), array_ops_stack.stack([1, array_ops.shape(result)[1]]))\n            result = array_ops.where(is_row_empty, array_ops.zeros_like(result), result, name=scope)\n        return result",
        "mutated": [
            "@dispatch.dispatch_for_api(embedding_ops.safe_embedding_lookup_sparse)\ndef safe_embedding_lookup_sparse(embedding_weights, sparse_ids: ragged_tensor.Ragged, sparse_weights=None, combiner='mean', default_id=None, name=None, partition_strategy='div', max_norm=None, allow_fast_lookup=False):\n    if False:\n        i = 10\n    'Lookup embedding results, accounting for invalid IDs and empty features.\\n\\n  The partitioned embedding in `embedding_weights` must all be the same shape\\n  except for the first dimension. The first dimension is allowed to vary as the\\n  vocabulary size is not necessarily a multiple of `P`.  `embedding_weights`\\n  may be a `PartitionedVariable` as returned by using\\n  `tf.compat.v1.get_variable()` with a\\n  partitioner.\\n\\n  Invalid IDs (< 0) are pruned from input IDs and weights, as well as any IDs\\n  with non-positive weight. For an entry with no features, the embedding vector\\n  for `default_id` is returned, or the 0-vector if `default_id` is not supplied.\\n\\n  The ids and weights may be multi-dimensional `SparseTensor`s or\\n  `RaggedTensor`s with rank of 2. For `SpareTensor`s with left-aligned non-zero\\n  entries which can be described as `RaggedTensor`s, use of `RaggedTensor`s can\\n  yield higher performance. Embeddings are always aggregated along the last\\n  dimension.\\n\\n  Args:\\n    embedding_weights: A single tensor representing the complete embedding\\n      tensor, or a list tensors all of same shape except for the first\\n      dimension, representing sharded embedding tensors. Alternatively, a\\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\\n      element must be appropriately sized for the given `partition_strategy`.\\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\\n      reasons.\\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\\n      containing float weights corresponding to `sparse_ids`, or `None` if all\\n      weights are assumed to be 1.0.\\n    combiner: A string specifying how to combine embedding results for each\\n      entry. Currently \"mean\", \"sqrtn\" and \"sum\" are supported, with \"mean\" the\\n      default.\\n    default_id: The id to use for an entry with no features.\\n    name: A name for this operation (optional).\\n    partition_strategy: A string specifying the partitioning strategy. Currently\\n      `\"div\"` and `\"mod\"` are supported. Default is `\"div\"`.\\n    max_norm: If not `None`, all embeddings are l2-normalized to max_norm before\\n      combining.\\n    allow_fast_lookup: An optional boolean specifying whether to allow\\n      simplified embedding lookups when `params` is a single tensor and\\n      `max_norm` is `None`. Setting this flag to `True` during training can\\n      cause the use of dense gradients with increased memory footprint.\\n\\n  Returns:\\n    A dense tensor representing the combined embeddings for the\\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\\n    looks up the embeddings for all ids in that row, multiplies them by the\\n    corresponding weight, and combines these embeddings as specified.\\n\\n    In other words, if\\n\\n      `shape(combined embedding_weights) = [p0, p1, ..., pm]`\\n\\n    and\\n\\n      `shape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]`\\n\\n    then\\n\\n      `shape(output) = [d0, d1, ... dn-1, p1, ..., pm]`.\\n\\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\\n\\n      ```python\\n      [0, 0]: id 1, weight 2.0\\n      [0, 1]: id 3, weight 0.5\\n      [1, 0]: id -1, weight 1.0\\n      [2, 3]: id 1, weight 3.0\\n      ```\\n\\n    `default_id` is 0.\\n\\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\\n\\n      ```python\\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\\n      output[1, :] = (params[0, :] * 1.0) / 1.0\\n      output[2, :] = (params[1, :] * 3.0) / 3.0\\n      ```\\n\\n  Raises:\\n    ValueError: if `embedding_weights` is empty.\\n  '\n    ragged_ids = sparse_ids\n    ragged_weights = sparse_weights\n    if embedding_weights is None:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    if isinstance(embedding_weights, variables.PartitionedVariable):\n        embedding_weights = list(embedding_weights)\n    if not isinstance(embedding_weights, list):\n        embedding_weights = [embedding_weights]\n    if len(embedding_weights) < 1:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    dtype = ragged_weights.dtype if ragged_weights is not None else None\n    embedding_weights = [w if resource_variable_ops.is_resource_variable(w) and dtype in (None, w.dtype) else ops.convert_to_tensor(w, dtype=dtype) for w in embedding_weights]\n    with ops.name_scope(name, 'embedding_lookup', embedding_weights + [ragged_ids, ragged_weights]) as scope:\n        (ragged_ids, ragged_weights) = _prune_invalid_ids_ragged(ragged_ids, ragged_weights)\n        if combiner != 'sum':\n            (ragged_ids, ragged_weights) = _prune_invalid_weights_ragged(ragged_ids, ragged_weights)\n        (ragged_ids, is_row_empty) = ragged_array_ops.fill_empty_rows(ragged_ids, default_id or 0)\n        if ragged_weights is not None:\n            (ragged_weights, _) = ragged_array_ops.fill_empty_rows(ragged_weights, 1.0)\n        result = embedding_lookup_sparse(embedding_weights, ragged_ids, ragged_weights, combiner=combiner, partition_strategy=partition_strategy, name=None if default_id is None else scope, max_norm=max_norm, allow_fast_lookup=allow_fast_lookup)\n        if default_id is None:\n            is_row_empty = array_ops.tile(array_ops.reshape(is_row_empty, [-1, 1]), array_ops_stack.stack([1, array_ops.shape(result)[1]]))\n            result = array_ops.where(is_row_empty, array_ops.zeros_like(result), result, name=scope)\n        return result",
            "@dispatch.dispatch_for_api(embedding_ops.safe_embedding_lookup_sparse)\ndef safe_embedding_lookup_sparse(embedding_weights, sparse_ids: ragged_tensor.Ragged, sparse_weights=None, combiner='mean', default_id=None, name=None, partition_strategy='div', max_norm=None, allow_fast_lookup=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Lookup embedding results, accounting for invalid IDs and empty features.\\n\\n  The partitioned embedding in `embedding_weights` must all be the same shape\\n  except for the first dimension. The first dimension is allowed to vary as the\\n  vocabulary size is not necessarily a multiple of `P`.  `embedding_weights`\\n  may be a `PartitionedVariable` as returned by using\\n  `tf.compat.v1.get_variable()` with a\\n  partitioner.\\n\\n  Invalid IDs (< 0) are pruned from input IDs and weights, as well as any IDs\\n  with non-positive weight. For an entry with no features, the embedding vector\\n  for `default_id` is returned, or the 0-vector if `default_id` is not supplied.\\n\\n  The ids and weights may be multi-dimensional `SparseTensor`s or\\n  `RaggedTensor`s with rank of 2. For `SpareTensor`s with left-aligned non-zero\\n  entries which can be described as `RaggedTensor`s, use of `RaggedTensor`s can\\n  yield higher performance. Embeddings are always aggregated along the last\\n  dimension.\\n\\n  Args:\\n    embedding_weights: A single tensor representing the complete embedding\\n      tensor, or a list tensors all of same shape except for the first\\n      dimension, representing sharded embedding tensors. Alternatively, a\\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\\n      element must be appropriately sized for the given `partition_strategy`.\\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\\n      reasons.\\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\\n      containing float weights corresponding to `sparse_ids`, or `None` if all\\n      weights are assumed to be 1.0.\\n    combiner: A string specifying how to combine embedding results for each\\n      entry. Currently \"mean\", \"sqrtn\" and \"sum\" are supported, with \"mean\" the\\n      default.\\n    default_id: The id to use for an entry with no features.\\n    name: A name for this operation (optional).\\n    partition_strategy: A string specifying the partitioning strategy. Currently\\n      `\"div\"` and `\"mod\"` are supported. Default is `\"div\"`.\\n    max_norm: If not `None`, all embeddings are l2-normalized to max_norm before\\n      combining.\\n    allow_fast_lookup: An optional boolean specifying whether to allow\\n      simplified embedding lookups when `params` is a single tensor and\\n      `max_norm` is `None`. Setting this flag to `True` during training can\\n      cause the use of dense gradients with increased memory footprint.\\n\\n  Returns:\\n    A dense tensor representing the combined embeddings for the\\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\\n    looks up the embeddings for all ids in that row, multiplies them by the\\n    corresponding weight, and combines these embeddings as specified.\\n\\n    In other words, if\\n\\n      `shape(combined embedding_weights) = [p0, p1, ..., pm]`\\n\\n    and\\n\\n      `shape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]`\\n\\n    then\\n\\n      `shape(output) = [d0, d1, ... dn-1, p1, ..., pm]`.\\n\\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\\n\\n      ```python\\n      [0, 0]: id 1, weight 2.0\\n      [0, 1]: id 3, weight 0.5\\n      [1, 0]: id -1, weight 1.0\\n      [2, 3]: id 1, weight 3.0\\n      ```\\n\\n    `default_id` is 0.\\n\\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\\n\\n      ```python\\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\\n      output[1, :] = (params[0, :] * 1.0) / 1.0\\n      output[2, :] = (params[1, :] * 3.0) / 3.0\\n      ```\\n\\n  Raises:\\n    ValueError: if `embedding_weights` is empty.\\n  '\n    ragged_ids = sparse_ids\n    ragged_weights = sparse_weights\n    if embedding_weights is None:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    if isinstance(embedding_weights, variables.PartitionedVariable):\n        embedding_weights = list(embedding_weights)\n    if not isinstance(embedding_weights, list):\n        embedding_weights = [embedding_weights]\n    if len(embedding_weights) < 1:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    dtype = ragged_weights.dtype if ragged_weights is not None else None\n    embedding_weights = [w if resource_variable_ops.is_resource_variable(w) and dtype in (None, w.dtype) else ops.convert_to_tensor(w, dtype=dtype) for w in embedding_weights]\n    with ops.name_scope(name, 'embedding_lookup', embedding_weights + [ragged_ids, ragged_weights]) as scope:\n        (ragged_ids, ragged_weights) = _prune_invalid_ids_ragged(ragged_ids, ragged_weights)\n        if combiner != 'sum':\n            (ragged_ids, ragged_weights) = _prune_invalid_weights_ragged(ragged_ids, ragged_weights)\n        (ragged_ids, is_row_empty) = ragged_array_ops.fill_empty_rows(ragged_ids, default_id or 0)\n        if ragged_weights is not None:\n            (ragged_weights, _) = ragged_array_ops.fill_empty_rows(ragged_weights, 1.0)\n        result = embedding_lookup_sparse(embedding_weights, ragged_ids, ragged_weights, combiner=combiner, partition_strategy=partition_strategy, name=None if default_id is None else scope, max_norm=max_norm, allow_fast_lookup=allow_fast_lookup)\n        if default_id is None:\n            is_row_empty = array_ops.tile(array_ops.reshape(is_row_empty, [-1, 1]), array_ops_stack.stack([1, array_ops.shape(result)[1]]))\n            result = array_ops.where(is_row_empty, array_ops.zeros_like(result), result, name=scope)\n        return result",
            "@dispatch.dispatch_for_api(embedding_ops.safe_embedding_lookup_sparse)\ndef safe_embedding_lookup_sparse(embedding_weights, sparse_ids: ragged_tensor.Ragged, sparse_weights=None, combiner='mean', default_id=None, name=None, partition_strategy='div', max_norm=None, allow_fast_lookup=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Lookup embedding results, accounting for invalid IDs and empty features.\\n\\n  The partitioned embedding in `embedding_weights` must all be the same shape\\n  except for the first dimension. The first dimension is allowed to vary as the\\n  vocabulary size is not necessarily a multiple of `P`.  `embedding_weights`\\n  may be a `PartitionedVariable` as returned by using\\n  `tf.compat.v1.get_variable()` with a\\n  partitioner.\\n\\n  Invalid IDs (< 0) are pruned from input IDs and weights, as well as any IDs\\n  with non-positive weight. For an entry with no features, the embedding vector\\n  for `default_id` is returned, or the 0-vector if `default_id` is not supplied.\\n\\n  The ids and weights may be multi-dimensional `SparseTensor`s or\\n  `RaggedTensor`s with rank of 2. For `SpareTensor`s with left-aligned non-zero\\n  entries which can be described as `RaggedTensor`s, use of `RaggedTensor`s can\\n  yield higher performance. Embeddings are always aggregated along the last\\n  dimension.\\n\\n  Args:\\n    embedding_weights: A single tensor representing the complete embedding\\n      tensor, or a list tensors all of same shape except for the first\\n      dimension, representing sharded embedding tensors. Alternatively, a\\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\\n      element must be appropriately sized for the given `partition_strategy`.\\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\\n      reasons.\\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\\n      containing float weights corresponding to `sparse_ids`, or `None` if all\\n      weights are assumed to be 1.0.\\n    combiner: A string specifying how to combine embedding results for each\\n      entry. Currently \"mean\", \"sqrtn\" and \"sum\" are supported, with \"mean\" the\\n      default.\\n    default_id: The id to use for an entry with no features.\\n    name: A name for this operation (optional).\\n    partition_strategy: A string specifying the partitioning strategy. Currently\\n      `\"div\"` and `\"mod\"` are supported. Default is `\"div\"`.\\n    max_norm: If not `None`, all embeddings are l2-normalized to max_norm before\\n      combining.\\n    allow_fast_lookup: An optional boolean specifying whether to allow\\n      simplified embedding lookups when `params` is a single tensor and\\n      `max_norm` is `None`. Setting this flag to `True` during training can\\n      cause the use of dense gradients with increased memory footprint.\\n\\n  Returns:\\n    A dense tensor representing the combined embeddings for the\\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\\n    looks up the embeddings for all ids in that row, multiplies them by the\\n    corresponding weight, and combines these embeddings as specified.\\n\\n    In other words, if\\n\\n      `shape(combined embedding_weights) = [p0, p1, ..., pm]`\\n\\n    and\\n\\n      `shape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]`\\n\\n    then\\n\\n      `shape(output) = [d0, d1, ... dn-1, p1, ..., pm]`.\\n\\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\\n\\n      ```python\\n      [0, 0]: id 1, weight 2.0\\n      [0, 1]: id 3, weight 0.5\\n      [1, 0]: id -1, weight 1.0\\n      [2, 3]: id 1, weight 3.0\\n      ```\\n\\n    `default_id` is 0.\\n\\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\\n\\n      ```python\\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\\n      output[1, :] = (params[0, :] * 1.0) / 1.0\\n      output[2, :] = (params[1, :] * 3.0) / 3.0\\n      ```\\n\\n  Raises:\\n    ValueError: if `embedding_weights` is empty.\\n  '\n    ragged_ids = sparse_ids\n    ragged_weights = sparse_weights\n    if embedding_weights is None:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    if isinstance(embedding_weights, variables.PartitionedVariable):\n        embedding_weights = list(embedding_weights)\n    if not isinstance(embedding_weights, list):\n        embedding_weights = [embedding_weights]\n    if len(embedding_weights) < 1:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    dtype = ragged_weights.dtype if ragged_weights is not None else None\n    embedding_weights = [w if resource_variable_ops.is_resource_variable(w) and dtype in (None, w.dtype) else ops.convert_to_tensor(w, dtype=dtype) for w in embedding_weights]\n    with ops.name_scope(name, 'embedding_lookup', embedding_weights + [ragged_ids, ragged_weights]) as scope:\n        (ragged_ids, ragged_weights) = _prune_invalid_ids_ragged(ragged_ids, ragged_weights)\n        if combiner != 'sum':\n            (ragged_ids, ragged_weights) = _prune_invalid_weights_ragged(ragged_ids, ragged_weights)\n        (ragged_ids, is_row_empty) = ragged_array_ops.fill_empty_rows(ragged_ids, default_id or 0)\n        if ragged_weights is not None:\n            (ragged_weights, _) = ragged_array_ops.fill_empty_rows(ragged_weights, 1.0)\n        result = embedding_lookup_sparse(embedding_weights, ragged_ids, ragged_weights, combiner=combiner, partition_strategy=partition_strategy, name=None if default_id is None else scope, max_norm=max_norm, allow_fast_lookup=allow_fast_lookup)\n        if default_id is None:\n            is_row_empty = array_ops.tile(array_ops.reshape(is_row_empty, [-1, 1]), array_ops_stack.stack([1, array_ops.shape(result)[1]]))\n            result = array_ops.where(is_row_empty, array_ops.zeros_like(result), result, name=scope)\n        return result",
            "@dispatch.dispatch_for_api(embedding_ops.safe_embedding_lookup_sparse)\ndef safe_embedding_lookup_sparse(embedding_weights, sparse_ids: ragged_tensor.Ragged, sparse_weights=None, combiner='mean', default_id=None, name=None, partition_strategy='div', max_norm=None, allow_fast_lookup=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Lookup embedding results, accounting for invalid IDs and empty features.\\n\\n  The partitioned embedding in `embedding_weights` must all be the same shape\\n  except for the first dimension. The first dimension is allowed to vary as the\\n  vocabulary size is not necessarily a multiple of `P`.  `embedding_weights`\\n  may be a `PartitionedVariable` as returned by using\\n  `tf.compat.v1.get_variable()` with a\\n  partitioner.\\n\\n  Invalid IDs (< 0) are pruned from input IDs and weights, as well as any IDs\\n  with non-positive weight. For an entry with no features, the embedding vector\\n  for `default_id` is returned, or the 0-vector if `default_id` is not supplied.\\n\\n  The ids and weights may be multi-dimensional `SparseTensor`s or\\n  `RaggedTensor`s with rank of 2. For `SpareTensor`s with left-aligned non-zero\\n  entries which can be described as `RaggedTensor`s, use of `RaggedTensor`s can\\n  yield higher performance. Embeddings are always aggregated along the last\\n  dimension.\\n\\n  Args:\\n    embedding_weights: A single tensor representing the complete embedding\\n      tensor, or a list tensors all of same shape except for the first\\n      dimension, representing sharded embedding tensors. Alternatively, a\\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\\n      element must be appropriately sized for the given `partition_strategy`.\\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\\n      reasons.\\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\\n      containing float weights corresponding to `sparse_ids`, or `None` if all\\n      weights are assumed to be 1.0.\\n    combiner: A string specifying how to combine embedding results for each\\n      entry. Currently \"mean\", \"sqrtn\" and \"sum\" are supported, with \"mean\" the\\n      default.\\n    default_id: The id to use for an entry with no features.\\n    name: A name for this operation (optional).\\n    partition_strategy: A string specifying the partitioning strategy. Currently\\n      `\"div\"` and `\"mod\"` are supported. Default is `\"div\"`.\\n    max_norm: If not `None`, all embeddings are l2-normalized to max_norm before\\n      combining.\\n    allow_fast_lookup: An optional boolean specifying whether to allow\\n      simplified embedding lookups when `params` is a single tensor and\\n      `max_norm` is `None`. Setting this flag to `True` during training can\\n      cause the use of dense gradients with increased memory footprint.\\n\\n  Returns:\\n    A dense tensor representing the combined embeddings for the\\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\\n    looks up the embeddings for all ids in that row, multiplies them by the\\n    corresponding weight, and combines these embeddings as specified.\\n\\n    In other words, if\\n\\n      `shape(combined embedding_weights) = [p0, p1, ..., pm]`\\n\\n    and\\n\\n      `shape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]`\\n\\n    then\\n\\n      `shape(output) = [d0, d1, ... dn-1, p1, ..., pm]`.\\n\\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\\n\\n      ```python\\n      [0, 0]: id 1, weight 2.0\\n      [0, 1]: id 3, weight 0.5\\n      [1, 0]: id -1, weight 1.0\\n      [2, 3]: id 1, weight 3.0\\n      ```\\n\\n    `default_id` is 0.\\n\\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\\n\\n      ```python\\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\\n      output[1, :] = (params[0, :] * 1.0) / 1.0\\n      output[2, :] = (params[1, :] * 3.0) / 3.0\\n      ```\\n\\n  Raises:\\n    ValueError: if `embedding_weights` is empty.\\n  '\n    ragged_ids = sparse_ids\n    ragged_weights = sparse_weights\n    if embedding_weights is None:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    if isinstance(embedding_weights, variables.PartitionedVariable):\n        embedding_weights = list(embedding_weights)\n    if not isinstance(embedding_weights, list):\n        embedding_weights = [embedding_weights]\n    if len(embedding_weights) < 1:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    dtype = ragged_weights.dtype if ragged_weights is not None else None\n    embedding_weights = [w if resource_variable_ops.is_resource_variable(w) and dtype in (None, w.dtype) else ops.convert_to_tensor(w, dtype=dtype) for w in embedding_weights]\n    with ops.name_scope(name, 'embedding_lookup', embedding_weights + [ragged_ids, ragged_weights]) as scope:\n        (ragged_ids, ragged_weights) = _prune_invalid_ids_ragged(ragged_ids, ragged_weights)\n        if combiner != 'sum':\n            (ragged_ids, ragged_weights) = _prune_invalid_weights_ragged(ragged_ids, ragged_weights)\n        (ragged_ids, is_row_empty) = ragged_array_ops.fill_empty_rows(ragged_ids, default_id or 0)\n        if ragged_weights is not None:\n            (ragged_weights, _) = ragged_array_ops.fill_empty_rows(ragged_weights, 1.0)\n        result = embedding_lookup_sparse(embedding_weights, ragged_ids, ragged_weights, combiner=combiner, partition_strategy=partition_strategy, name=None if default_id is None else scope, max_norm=max_norm, allow_fast_lookup=allow_fast_lookup)\n        if default_id is None:\n            is_row_empty = array_ops.tile(array_ops.reshape(is_row_empty, [-1, 1]), array_ops_stack.stack([1, array_ops.shape(result)[1]]))\n            result = array_ops.where(is_row_empty, array_ops.zeros_like(result), result, name=scope)\n        return result",
            "@dispatch.dispatch_for_api(embedding_ops.safe_embedding_lookup_sparse)\ndef safe_embedding_lookup_sparse(embedding_weights, sparse_ids: ragged_tensor.Ragged, sparse_weights=None, combiner='mean', default_id=None, name=None, partition_strategy='div', max_norm=None, allow_fast_lookup=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Lookup embedding results, accounting for invalid IDs and empty features.\\n\\n  The partitioned embedding in `embedding_weights` must all be the same shape\\n  except for the first dimension. The first dimension is allowed to vary as the\\n  vocabulary size is not necessarily a multiple of `P`.  `embedding_weights`\\n  may be a `PartitionedVariable` as returned by using\\n  `tf.compat.v1.get_variable()` with a\\n  partitioner.\\n\\n  Invalid IDs (< 0) are pruned from input IDs and weights, as well as any IDs\\n  with non-positive weight. For an entry with no features, the embedding vector\\n  for `default_id` is returned, or the 0-vector if `default_id` is not supplied.\\n\\n  The ids and weights may be multi-dimensional `SparseTensor`s or\\n  `RaggedTensor`s with rank of 2. For `SpareTensor`s with left-aligned non-zero\\n  entries which can be described as `RaggedTensor`s, use of `RaggedTensor`s can\\n  yield higher performance. Embeddings are always aggregated along the last\\n  dimension.\\n\\n  Args:\\n    embedding_weights: A single tensor representing the complete embedding\\n      tensor, or a list tensors all of same shape except for the first\\n      dimension, representing sharded embedding tensors. Alternatively, a\\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\\n      element must be appropriately sized for the given `partition_strategy`.\\n    sp_ids: `RaggedTensor` with rank 2. The rank is not verified for performance\\n      reasons.\\n    sparse_weights: `RaggedTensor` of same type and shape as `sparse_ids`,\\n      containing float weights corresponding to `sparse_ids`, or `None` if all\\n      weights are assumed to be 1.0.\\n    combiner: A string specifying how to combine embedding results for each\\n      entry. Currently \"mean\", \"sqrtn\" and \"sum\" are supported, with \"mean\" the\\n      default.\\n    default_id: The id to use for an entry with no features.\\n    name: A name for this operation (optional).\\n    partition_strategy: A string specifying the partitioning strategy. Currently\\n      `\"div\"` and `\"mod\"` are supported. Default is `\"div\"`.\\n    max_norm: If not `None`, all embeddings are l2-normalized to max_norm before\\n      combining.\\n    allow_fast_lookup: An optional boolean specifying whether to allow\\n      simplified embedding lookups when `params` is a single tensor and\\n      `max_norm` is `None`. Setting this flag to `True` during training can\\n      cause the use of dense gradients with increased memory footprint.\\n\\n  Returns:\\n    A dense tensor representing the combined embeddings for the\\n    sparse ids. For each row in the dense tensor represented by `sp_ids`, the op\\n    looks up the embeddings for all ids in that row, multiplies them by the\\n    corresponding weight, and combines these embeddings as specified.\\n\\n    In other words, if\\n\\n      `shape(combined embedding_weights) = [p0, p1, ..., pm]`\\n\\n    and\\n\\n      `shape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]`\\n\\n    then\\n\\n      `shape(output) = [d0, d1, ... dn-1, p1, ..., pm]`.\\n\\n    For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\\n\\n      ```python\\n      [0, 0]: id 1, weight 2.0\\n      [0, 1]: id 3, weight 0.5\\n      [1, 0]: id -1, weight 1.0\\n      [2, 3]: id 1, weight 3.0\\n      ```\\n\\n    `default_id` is 0.\\n\\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\\n\\n      ```python\\n      output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\\n      output[1, :] = (params[0, :] * 1.0) / 1.0\\n      output[2, :] = (params[1, :] * 3.0) / 3.0\\n      ```\\n\\n  Raises:\\n    ValueError: if `embedding_weights` is empty.\\n  '\n    ragged_ids = sparse_ids\n    ragged_weights = sparse_weights\n    if embedding_weights is None:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    if isinstance(embedding_weights, variables.PartitionedVariable):\n        embedding_weights = list(embedding_weights)\n    if not isinstance(embedding_weights, list):\n        embedding_weights = [embedding_weights]\n    if len(embedding_weights) < 1:\n        raise ValueError(f'Missing embedding_weights {embedding_weights}.')\n    dtype = ragged_weights.dtype if ragged_weights is not None else None\n    embedding_weights = [w if resource_variable_ops.is_resource_variable(w) and dtype in (None, w.dtype) else ops.convert_to_tensor(w, dtype=dtype) for w in embedding_weights]\n    with ops.name_scope(name, 'embedding_lookup', embedding_weights + [ragged_ids, ragged_weights]) as scope:\n        (ragged_ids, ragged_weights) = _prune_invalid_ids_ragged(ragged_ids, ragged_weights)\n        if combiner != 'sum':\n            (ragged_ids, ragged_weights) = _prune_invalid_weights_ragged(ragged_ids, ragged_weights)\n        (ragged_ids, is_row_empty) = ragged_array_ops.fill_empty_rows(ragged_ids, default_id or 0)\n        if ragged_weights is not None:\n            (ragged_weights, _) = ragged_array_ops.fill_empty_rows(ragged_weights, 1.0)\n        result = embedding_lookup_sparse(embedding_weights, ragged_ids, ragged_weights, combiner=combiner, partition_strategy=partition_strategy, name=None if default_id is None else scope, max_norm=max_norm, allow_fast_lookup=allow_fast_lookup)\n        if default_id is None:\n            is_row_empty = array_ops.tile(array_ops.reshape(is_row_empty, [-1, 1]), array_ops_stack.stack([1, array_ops.shape(result)[1]]))\n            result = array_ops.where(is_row_empty, array_ops.zeros_like(result), result, name=scope)\n        return result"
        ]
    },
    {
        "func_name": "_prune_invalid_ids_ragged",
        "original": "def _prune_invalid_ids_ragged(ids, weights):\n    \"\"\"Prune invalid IDs (< 0) from the input ids and weights.\"\"\"\n    is_id_valid = math_ops.greater_equal(ids.values, 0)\n    nrows = ids.nrows()\n    pruned_values = array_ops.boolean_mask_v2(ids.values, is_id_valid)\n    pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_id_valid)\n    ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n    if weights is not None:\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_id_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)",
        "mutated": [
            "def _prune_invalid_ids_ragged(ids, weights):\n    if False:\n        i = 10\n    'Prune invalid IDs (< 0) from the input ids and weights.'\n    is_id_valid = math_ops.greater_equal(ids.values, 0)\n    nrows = ids.nrows()\n    pruned_values = array_ops.boolean_mask_v2(ids.values, is_id_valid)\n    pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_id_valid)\n    ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n    if weights is not None:\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_id_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)",
            "def _prune_invalid_ids_ragged(ids, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Prune invalid IDs (< 0) from the input ids and weights.'\n    is_id_valid = math_ops.greater_equal(ids.values, 0)\n    nrows = ids.nrows()\n    pruned_values = array_ops.boolean_mask_v2(ids.values, is_id_valid)\n    pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_id_valid)\n    ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n    if weights is not None:\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_id_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)",
            "def _prune_invalid_ids_ragged(ids, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Prune invalid IDs (< 0) from the input ids and weights.'\n    is_id_valid = math_ops.greater_equal(ids.values, 0)\n    nrows = ids.nrows()\n    pruned_values = array_ops.boolean_mask_v2(ids.values, is_id_valid)\n    pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_id_valid)\n    ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n    if weights is not None:\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_id_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)",
            "def _prune_invalid_ids_ragged(ids, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Prune invalid IDs (< 0) from the input ids and weights.'\n    is_id_valid = math_ops.greater_equal(ids.values, 0)\n    nrows = ids.nrows()\n    pruned_values = array_ops.boolean_mask_v2(ids.values, is_id_valid)\n    pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_id_valid)\n    ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n    if weights is not None:\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_id_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)",
            "def _prune_invalid_ids_ragged(ids, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Prune invalid IDs (< 0) from the input ids and weights.'\n    is_id_valid = math_ops.greater_equal(ids.values, 0)\n    nrows = ids.nrows()\n    pruned_values = array_ops.boolean_mask_v2(ids.values, is_id_valid)\n    pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_id_valid)\n    ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n    if weights is not None:\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_id_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)"
        ]
    },
    {
        "func_name": "_prune_invalid_weights_ragged",
        "original": "def _prune_invalid_weights_ragged(ids, weights):\n    \"\"\"Prune invalid weights (< 0) from the input ids and weights.\"\"\"\n    if weights is not None:\n        is_weights_valid = math_ops.greater(weights.values, 0)\n        nrows = ids.nrows()\n        pruned_values = array_ops.boolean_mask_v2(ids.values, is_weights_valid)\n        pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_weights_valid)\n        ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_weights_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)",
        "mutated": [
            "def _prune_invalid_weights_ragged(ids, weights):\n    if False:\n        i = 10\n    'Prune invalid weights (< 0) from the input ids and weights.'\n    if weights is not None:\n        is_weights_valid = math_ops.greater(weights.values, 0)\n        nrows = ids.nrows()\n        pruned_values = array_ops.boolean_mask_v2(ids.values, is_weights_valid)\n        pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_weights_valid)\n        ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_weights_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)",
            "def _prune_invalid_weights_ragged(ids, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Prune invalid weights (< 0) from the input ids and weights.'\n    if weights is not None:\n        is_weights_valid = math_ops.greater(weights.values, 0)\n        nrows = ids.nrows()\n        pruned_values = array_ops.boolean_mask_v2(ids.values, is_weights_valid)\n        pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_weights_valid)\n        ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_weights_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)",
            "def _prune_invalid_weights_ragged(ids, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Prune invalid weights (< 0) from the input ids and weights.'\n    if weights is not None:\n        is_weights_valid = math_ops.greater(weights.values, 0)\n        nrows = ids.nrows()\n        pruned_values = array_ops.boolean_mask_v2(ids.values, is_weights_valid)\n        pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_weights_valid)\n        ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_weights_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)",
            "def _prune_invalid_weights_ragged(ids, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Prune invalid weights (< 0) from the input ids and weights.'\n    if weights is not None:\n        is_weights_valid = math_ops.greater(weights.values, 0)\n        nrows = ids.nrows()\n        pruned_values = array_ops.boolean_mask_v2(ids.values, is_weights_valid)\n        pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_weights_valid)\n        ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_weights_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)",
            "def _prune_invalid_weights_ragged(ids, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Prune invalid weights (< 0) from the input ids and weights.'\n    if weights is not None:\n        is_weights_valid = math_ops.greater(weights.values, 0)\n        nrows = ids.nrows()\n        pruned_values = array_ops.boolean_mask_v2(ids.values, is_weights_valid)\n        pruned_value_rowids = array_ops.boolean_mask_v2(ids.value_rowids(), is_weights_valid)\n        ids = ragged_tensor.RaggedTensor.from_value_rowids(pruned_values, pruned_value_rowids, nrows=nrows, validate=False)\n        pruned_weights_values = array_ops.boolean_mask_v2(weights.values, is_weights_valid)\n        weights = ragged_tensor.RaggedTensor.from_value_rowids(pruned_weights_values, pruned_value_rowids, nrows=nrows, validate=False)\n    return (ids, weights)"
        ]
    }
]