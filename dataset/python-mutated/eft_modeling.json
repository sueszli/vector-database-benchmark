[
    {
        "func_name": "load_peft_model",
        "original": "def load_peft_model(model, peft_model_path, tokenizer):\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.eos_token_id = tokenizer.eos_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model = PeftModel.from_pretrained(model, peft_model_path, torch_dtype=model.dtype)\n    model.eos_token_id = tokenizer.eos_token_id\n    try:\n        extra_embeds = hf_hub_download(peft_model_path, 'extra_embeddings.pt')\n        embed_weights = torch.load(extra_embeds, map_location=model.device)\n        model.base_model.model.model.embed_tokens.weight[len(tokenizer) - embed_weights.shape[0]:, :] = embed_weights.to(model.base_model.model.model.embed_tokens.weight.dtype)\n    except Exception:\n        print('Warning:Extra embeddings not added. This is expected if adapter file contains WTE')\n    return model",
        "mutated": [
            "def load_peft_model(model, peft_model_path, tokenizer):\n    if False:\n        i = 10\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.eos_token_id = tokenizer.eos_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model = PeftModel.from_pretrained(model, peft_model_path, torch_dtype=model.dtype)\n    model.eos_token_id = tokenizer.eos_token_id\n    try:\n        extra_embeds = hf_hub_download(peft_model_path, 'extra_embeddings.pt')\n        embed_weights = torch.load(extra_embeds, map_location=model.device)\n        model.base_model.model.model.embed_tokens.weight[len(tokenizer) - embed_weights.shape[0]:, :] = embed_weights.to(model.base_model.model.model.embed_tokens.weight.dtype)\n    except Exception:\n        print('Warning:Extra embeddings not added. This is expected if adapter file contains WTE')\n    return model",
            "def load_peft_model(model, peft_model_path, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.eos_token_id = tokenizer.eos_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model = PeftModel.from_pretrained(model, peft_model_path, torch_dtype=model.dtype)\n    model.eos_token_id = tokenizer.eos_token_id\n    try:\n        extra_embeds = hf_hub_download(peft_model_path, 'extra_embeddings.pt')\n        embed_weights = torch.load(extra_embeds, map_location=model.device)\n        model.base_model.model.model.embed_tokens.weight[len(tokenizer) - embed_weights.shape[0]:, :] = embed_weights.to(model.base_model.model.model.embed_tokens.weight.dtype)\n    except Exception:\n        print('Warning:Extra embeddings not added. This is expected if adapter file contains WTE')\n    return model",
            "def load_peft_model(model, peft_model_path, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.eos_token_id = tokenizer.eos_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model = PeftModel.from_pretrained(model, peft_model_path, torch_dtype=model.dtype)\n    model.eos_token_id = tokenizer.eos_token_id\n    try:\n        extra_embeds = hf_hub_download(peft_model_path, 'extra_embeddings.pt')\n        embed_weights = torch.load(extra_embeds, map_location=model.device)\n        model.base_model.model.model.embed_tokens.weight[len(tokenizer) - embed_weights.shape[0]:, :] = embed_weights.to(model.base_model.model.model.embed_tokens.weight.dtype)\n    except Exception:\n        print('Warning:Extra embeddings not added. This is expected if adapter file contains WTE')\n    return model",
            "def load_peft_model(model, peft_model_path, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.eos_token_id = tokenizer.eos_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model = PeftModel.from_pretrained(model, peft_model_path, torch_dtype=model.dtype)\n    model.eos_token_id = tokenizer.eos_token_id\n    try:\n        extra_embeds = hf_hub_download(peft_model_path, 'extra_embeddings.pt')\n        embed_weights = torch.load(extra_embeds, map_location=model.device)\n        model.base_model.model.model.embed_tokens.weight[len(tokenizer) - embed_weights.shape[0]:, :] = embed_weights.to(model.base_model.model.model.embed_tokens.weight.dtype)\n    except Exception:\n        print('Warning:Extra embeddings not added. This is expected if adapter file contains WTE')\n    return model",
            "def load_peft_model(model, peft_model_path, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.eos_token_id = tokenizer.eos_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model = PeftModel.from_pretrained(model, peft_model_path, torch_dtype=model.dtype)\n    model.eos_token_id = tokenizer.eos_token_id\n    try:\n        extra_embeds = hf_hub_download(peft_model_path, 'extra_embeddings.pt')\n        embed_weights = torch.load(extra_embeds, map_location=model.device)\n        model.base_model.model.model.embed_tokens.weight[len(tokenizer) - embed_weights.shape[0]:, :] = embed_weights.to(model.base_model.model.model.embed_tokens.weight.dtype)\n    except Exception:\n        print('Warning:Extra embeddings not added. This is expected if adapter file contains WTE')\n    return model"
        ]
    },
    {
        "func_name": "make_inputs_require_grad",
        "original": "def make_inputs_require_grad(module, input, output):\n    output.requires_grad_(True)",
        "mutated": [
            "def make_inputs_require_grad(module, input, output):\n    if False:\n        i = 10\n    output.requires_grad_(True)",
            "def make_inputs_require_grad(module, input, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    output.requires_grad_(True)",
            "def make_inputs_require_grad(module, input, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    output.requires_grad_(True)",
            "def make_inputs_require_grad(module, input, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    output.requires_grad_(True)",
            "def make_inputs_require_grad(module, input, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    output.requires_grad_(True)"
        ]
    },
    {
        "func_name": "prepare_model_for_gradient_checkpointing",
        "original": "def prepare_model_for_gradient_checkpointing(model):\n    \"\"\"\n    Prepares the model for gradient checkpointing if necessary\n    \"\"\"\n    if not getattr(model, 'is_loaded_in_8bit', False):\n        if hasattr(model, 'enable_input_require_grads'):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n    return model",
        "mutated": [
            "def prepare_model_for_gradient_checkpointing(model):\n    if False:\n        i = 10\n    '\\n    Prepares the model for gradient checkpointing if necessary\\n    '\n    if not getattr(model, 'is_loaded_in_8bit', False):\n        if hasattr(model, 'enable_input_require_grads'):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n    return model",
            "def prepare_model_for_gradient_checkpointing(model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Prepares the model for gradient checkpointing if necessary\\n    '\n    if not getattr(model, 'is_loaded_in_8bit', False):\n        if hasattr(model, 'enable_input_require_grads'):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n    return model",
            "def prepare_model_for_gradient_checkpointing(model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Prepares the model for gradient checkpointing if necessary\\n    '\n    if not getattr(model, 'is_loaded_in_8bit', False):\n        if hasattr(model, 'enable_input_require_grads'):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n    return model",
            "def prepare_model_for_gradient_checkpointing(model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Prepares the model for gradient checkpointing if necessary\\n    '\n    if not getattr(model, 'is_loaded_in_8bit', False):\n        if hasattr(model, 'enable_input_require_grads'):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n    return model",
            "def prepare_model_for_gradient_checkpointing(model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Prepares the model for gradient checkpointing if necessary\\n    '\n    if not getattr(model, 'is_loaded_in_8bit', False):\n        if hasattr(model, 'enable_input_require_grads'):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n    return model"
        ]
    },
    {
        "func_name": "peft_model",
        "original": "def peft_model(model, training_config):\n    peft_config = training_config.peft_config\n    peft_type = peft_config.pop('peft_type', 'lora')\n    if peft_type == 'lora':\n        default_args = {'r': 16, 'lora_alpha': 32, 'target_modules': 'all', 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'modules_to_save': ['wte', 'lm_head']}\n        kwargs = merge_dicts(default_args, peft_config)\n        if kwargs.get('target_modules') == 'all':\n            kwargs.update({'target_modules': get_all_linear_layers(model)})\n        config = LoraConfig(**kwargs)\n    elif peft_type == 'prefix-tuning':\n        default_args = {'num_virtual_tokens': 30, 'prefix_projection': True, 'encoder_hidden_size': 1024, 'task_type': 'CAUSAL_LM'}\n        kwargs = merge_dicts(default_args, peft_config)\n        config = PrefixTuningConfig(**kwargs)\n    else:\n        raise ValueError('peft_method config is lora or prefix-tuning')\n    model = get_peft_model(model, config)\n    if training_config.int8_training:\n        model = prepare_model_for_int8_training(model)\n    if training_config.gradient_checkpointing:\n        model = prepare_model_for_gradient_checkpointing(model)\n    model.print_trainable_parameters()\n    return model",
        "mutated": [
            "def peft_model(model, training_config):\n    if False:\n        i = 10\n    peft_config = training_config.peft_config\n    peft_type = peft_config.pop('peft_type', 'lora')\n    if peft_type == 'lora':\n        default_args = {'r': 16, 'lora_alpha': 32, 'target_modules': 'all', 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'modules_to_save': ['wte', 'lm_head']}\n        kwargs = merge_dicts(default_args, peft_config)\n        if kwargs.get('target_modules') == 'all':\n            kwargs.update({'target_modules': get_all_linear_layers(model)})\n        config = LoraConfig(**kwargs)\n    elif peft_type == 'prefix-tuning':\n        default_args = {'num_virtual_tokens': 30, 'prefix_projection': True, 'encoder_hidden_size': 1024, 'task_type': 'CAUSAL_LM'}\n        kwargs = merge_dicts(default_args, peft_config)\n        config = PrefixTuningConfig(**kwargs)\n    else:\n        raise ValueError('peft_method config is lora or prefix-tuning')\n    model = get_peft_model(model, config)\n    if training_config.int8_training:\n        model = prepare_model_for_int8_training(model)\n    if training_config.gradient_checkpointing:\n        model = prepare_model_for_gradient_checkpointing(model)\n    model.print_trainable_parameters()\n    return model",
            "def peft_model(model, training_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    peft_config = training_config.peft_config\n    peft_type = peft_config.pop('peft_type', 'lora')\n    if peft_type == 'lora':\n        default_args = {'r': 16, 'lora_alpha': 32, 'target_modules': 'all', 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'modules_to_save': ['wte', 'lm_head']}\n        kwargs = merge_dicts(default_args, peft_config)\n        if kwargs.get('target_modules') == 'all':\n            kwargs.update({'target_modules': get_all_linear_layers(model)})\n        config = LoraConfig(**kwargs)\n    elif peft_type == 'prefix-tuning':\n        default_args = {'num_virtual_tokens': 30, 'prefix_projection': True, 'encoder_hidden_size': 1024, 'task_type': 'CAUSAL_LM'}\n        kwargs = merge_dicts(default_args, peft_config)\n        config = PrefixTuningConfig(**kwargs)\n    else:\n        raise ValueError('peft_method config is lora or prefix-tuning')\n    model = get_peft_model(model, config)\n    if training_config.int8_training:\n        model = prepare_model_for_int8_training(model)\n    if training_config.gradient_checkpointing:\n        model = prepare_model_for_gradient_checkpointing(model)\n    model.print_trainable_parameters()\n    return model",
            "def peft_model(model, training_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    peft_config = training_config.peft_config\n    peft_type = peft_config.pop('peft_type', 'lora')\n    if peft_type == 'lora':\n        default_args = {'r': 16, 'lora_alpha': 32, 'target_modules': 'all', 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'modules_to_save': ['wte', 'lm_head']}\n        kwargs = merge_dicts(default_args, peft_config)\n        if kwargs.get('target_modules') == 'all':\n            kwargs.update({'target_modules': get_all_linear_layers(model)})\n        config = LoraConfig(**kwargs)\n    elif peft_type == 'prefix-tuning':\n        default_args = {'num_virtual_tokens': 30, 'prefix_projection': True, 'encoder_hidden_size': 1024, 'task_type': 'CAUSAL_LM'}\n        kwargs = merge_dicts(default_args, peft_config)\n        config = PrefixTuningConfig(**kwargs)\n    else:\n        raise ValueError('peft_method config is lora or prefix-tuning')\n    model = get_peft_model(model, config)\n    if training_config.int8_training:\n        model = prepare_model_for_int8_training(model)\n    if training_config.gradient_checkpointing:\n        model = prepare_model_for_gradient_checkpointing(model)\n    model.print_trainable_parameters()\n    return model",
            "def peft_model(model, training_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    peft_config = training_config.peft_config\n    peft_type = peft_config.pop('peft_type', 'lora')\n    if peft_type == 'lora':\n        default_args = {'r': 16, 'lora_alpha': 32, 'target_modules': 'all', 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'modules_to_save': ['wte', 'lm_head']}\n        kwargs = merge_dicts(default_args, peft_config)\n        if kwargs.get('target_modules') == 'all':\n            kwargs.update({'target_modules': get_all_linear_layers(model)})\n        config = LoraConfig(**kwargs)\n    elif peft_type == 'prefix-tuning':\n        default_args = {'num_virtual_tokens': 30, 'prefix_projection': True, 'encoder_hidden_size': 1024, 'task_type': 'CAUSAL_LM'}\n        kwargs = merge_dicts(default_args, peft_config)\n        config = PrefixTuningConfig(**kwargs)\n    else:\n        raise ValueError('peft_method config is lora or prefix-tuning')\n    model = get_peft_model(model, config)\n    if training_config.int8_training:\n        model = prepare_model_for_int8_training(model)\n    if training_config.gradient_checkpointing:\n        model = prepare_model_for_gradient_checkpointing(model)\n    model.print_trainable_parameters()\n    return model",
            "def peft_model(model, training_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    peft_config = training_config.peft_config\n    peft_type = peft_config.pop('peft_type', 'lora')\n    if peft_type == 'lora':\n        default_args = {'r': 16, 'lora_alpha': 32, 'target_modules': 'all', 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'modules_to_save': ['wte', 'lm_head']}\n        kwargs = merge_dicts(default_args, peft_config)\n        if kwargs.get('target_modules') == 'all':\n            kwargs.update({'target_modules': get_all_linear_layers(model)})\n        config = LoraConfig(**kwargs)\n    elif peft_type == 'prefix-tuning':\n        default_args = {'num_virtual_tokens': 30, 'prefix_projection': True, 'encoder_hidden_size': 1024, 'task_type': 'CAUSAL_LM'}\n        kwargs = merge_dicts(default_args, peft_config)\n        config = PrefixTuningConfig(**kwargs)\n    else:\n        raise ValueError('peft_method config is lora or prefix-tuning')\n    model = get_peft_model(model, config)\n    if training_config.int8_training:\n        model = prepare_model_for_int8_training(model)\n    if training_config.gradient_checkpointing:\n        model = prepare_model_for_gradient_checkpointing(model)\n    model.print_trainable_parameters()\n    return model"
        ]
    },
    {
        "func_name": "save_adapter_model_from_ckpt",
        "original": "def save_adapter_model_from_ckpt(save_config: SaveLoraConfig):\n    tokenizer = get_tokenizer(save_config)\n    model = get_model(save_config, tokenizer)\n    model = peft_model(model)\n    model.load_state_dict(torch.load(save_config.torch_ckpt_path))\n    vocab_size = tokenizer.vocab_size\n    num_special_tokens = len(tokenizer.additional_special_tokens)\n    new_embs = model.state_dict()['base_model.model.model.embed_tokens.weight'][vocab_size:vocab_size + num_special_tokens, :].clone()\n    new_embs = new_embs.to(save_config.dtype)\n    model.save_pretrained(save_config.adapter_save_path, torch_dtype=save_config.dtype)\n    tokenizer.save_pretrained(save_config.adapter_save_path)\n    torch.save(new_embs, Path(save_config.adapter_save_path).joinpath('extra_embeddings.pt'))",
        "mutated": [
            "def save_adapter_model_from_ckpt(save_config: SaveLoraConfig):\n    if False:\n        i = 10\n    tokenizer = get_tokenizer(save_config)\n    model = get_model(save_config, tokenizer)\n    model = peft_model(model)\n    model.load_state_dict(torch.load(save_config.torch_ckpt_path))\n    vocab_size = tokenizer.vocab_size\n    num_special_tokens = len(tokenizer.additional_special_tokens)\n    new_embs = model.state_dict()['base_model.model.model.embed_tokens.weight'][vocab_size:vocab_size + num_special_tokens, :].clone()\n    new_embs = new_embs.to(save_config.dtype)\n    model.save_pretrained(save_config.adapter_save_path, torch_dtype=save_config.dtype)\n    tokenizer.save_pretrained(save_config.adapter_save_path)\n    torch.save(new_embs, Path(save_config.adapter_save_path).joinpath('extra_embeddings.pt'))",
            "def save_adapter_model_from_ckpt(save_config: SaveLoraConfig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizer = get_tokenizer(save_config)\n    model = get_model(save_config, tokenizer)\n    model = peft_model(model)\n    model.load_state_dict(torch.load(save_config.torch_ckpt_path))\n    vocab_size = tokenizer.vocab_size\n    num_special_tokens = len(tokenizer.additional_special_tokens)\n    new_embs = model.state_dict()['base_model.model.model.embed_tokens.weight'][vocab_size:vocab_size + num_special_tokens, :].clone()\n    new_embs = new_embs.to(save_config.dtype)\n    model.save_pretrained(save_config.adapter_save_path, torch_dtype=save_config.dtype)\n    tokenizer.save_pretrained(save_config.adapter_save_path)\n    torch.save(new_embs, Path(save_config.adapter_save_path).joinpath('extra_embeddings.pt'))",
            "def save_adapter_model_from_ckpt(save_config: SaveLoraConfig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizer = get_tokenizer(save_config)\n    model = get_model(save_config, tokenizer)\n    model = peft_model(model)\n    model.load_state_dict(torch.load(save_config.torch_ckpt_path))\n    vocab_size = tokenizer.vocab_size\n    num_special_tokens = len(tokenizer.additional_special_tokens)\n    new_embs = model.state_dict()['base_model.model.model.embed_tokens.weight'][vocab_size:vocab_size + num_special_tokens, :].clone()\n    new_embs = new_embs.to(save_config.dtype)\n    model.save_pretrained(save_config.adapter_save_path, torch_dtype=save_config.dtype)\n    tokenizer.save_pretrained(save_config.adapter_save_path)\n    torch.save(new_embs, Path(save_config.adapter_save_path).joinpath('extra_embeddings.pt'))",
            "def save_adapter_model_from_ckpt(save_config: SaveLoraConfig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizer = get_tokenizer(save_config)\n    model = get_model(save_config, tokenizer)\n    model = peft_model(model)\n    model.load_state_dict(torch.load(save_config.torch_ckpt_path))\n    vocab_size = tokenizer.vocab_size\n    num_special_tokens = len(tokenizer.additional_special_tokens)\n    new_embs = model.state_dict()['base_model.model.model.embed_tokens.weight'][vocab_size:vocab_size + num_special_tokens, :].clone()\n    new_embs = new_embs.to(save_config.dtype)\n    model.save_pretrained(save_config.adapter_save_path, torch_dtype=save_config.dtype)\n    tokenizer.save_pretrained(save_config.adapter_save_path)\n    torch.save(new_embs, Path(save_config.adapter_save_path).joinpath('extra_embeddings.pt'))",
            "def save_adapter_model_from_ckpt(save_config: SaveLoraConfig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizer = get_tokenizer(save_config)\n    model = get_model(save_config, tokenizer)\n    model = peft_model(model)\n    model.load_state_dict(torch.load(save_config.torch_ckpt_path))\n    vocab_size = tokenizer.vocab_size\n    num_special_tokens = len(tokenizer.additional_special_tokens)\n    new_embs = model.state_dict()['base_model.model.model.embed_tokens.weight'][vocab_size:vocab_size + num_special_tokens, :].clone()\n    new_embs = new_embs.to(save_config.dtype)\n    model.save_pretrained(save_config.adapter_save_path, torch_dtype=save_config.dtype)\n    tokenizer.save_pretrained(save_config.adapter_save_path)\n    torch.save(new_embs, Path(save_config.adapter_save_path).joinpath('extra_embeddings.pt'))"
        ]
    }
]