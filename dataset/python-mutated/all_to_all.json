[
    {
        "func_name": "alltoall",
        "original": "def alltoall(in_tensor_list, out_tensor_list, group=None, sync_op=True):\n    \"\"\"\n    Scatter tensors in in_tensor_list to all participators averagely and gather the result tensors in out_tensor_list.\n    As shown below, the in_tensor_list in GPU0 includes 0_0 and 0_1, and GPU1 includes 1_0 and 1_1.\n    Through alltoall operator, the 0_0 in GPU0 will be sent to GPU0 and 0_1 to GPU1, 1_0 in GPU1 sent to GPU0 and 1_1 to GPU1.\n    Finally the out_tensor_list in GPU0 includes 0_0 and 1_0, and GPU1 includes 0_1 and 1_1.\n\n    .. image:: https://githubraw.cdn.bcebos.com/PaddlePaddle/docs/develop/docs/api/paddle/distributed/img/alltoall.png\n        :width: 800\n        :alt: alltoall\n        :align: center\n\n    Args:\n        in_tensor_list (List[Tensor]): List of tensors to scatter one per rank. The data type of each tensor\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\n        out_tensor_list (List[Tensor]): List of tensors to be gathered one per rank. The data type of each tensor should be the same as the input tensors.\n        group (Group, optional): The group instance return by new_group or None for global default group. Default: None.\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\n\n    Returns:\n        Return a task object.\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\n            >>> import paddle\n            >>> import paddle.distributed as dist\n\n            >>> dist.init_parallel_env()\n            >>> out_tensor_list = []\n            >>> if dist.get_rank() == 0:\n            ...     data1 = paddle.to_tensor([[1, 2, 3], [4, 5, 6]])\n            ...     data2 = paddle.to_tensor([[7, 8, 9], [10, 11, 12]])\n            >>> else:\n            ...     data1 = paddle.to_tensor([[13, 14, 15], [16, 17, 18]])\n            ...     data2 = paddle.to_tensor([[19, 20, 21], [22, 23, 24]])\n            >>> dist.alltoall([data1, data2], out_tensor_list)\n            >>> print(out_tensor_list)\n            >>> # [[[1, 2, 3], [4, 5, 6]], [[13, 14, 15], [16, 17, 18]]] (2 GPUs, out for rank 0)\n            >>> # [[[7, 8, 9], [10, 11, 12]], [[19, 20, 21], [22, 23, 24]]] (2 GPUs, out for rank 1)\n    \"\"\"\n    return stream.alltoall(out_tensor_list, in_tensor_list, group, sync_op, False)",
        "mutated": [
            "def alltoall(in_tensor_list, out_tensor_list, group=None, sync_op=True):\n    if False:\n        i = 10\n    '\\n    Scatter tensors in in_tensor_list to all participators averagely and gather the result tensors in out_tensor_list.\\n    As shown below, the in_tensor_list in GPU0 includes 0_0 and 0_1, and GPU1 includes 1_0 and 1_1.\\n    Through alltoall operator, the 0_0 in GPU0 will be sent to GPU0 and 0_1 to GPU1, 1_0 in GPU1 sent to GPU0 and 1_1 to GPU1.\\n    Finally the out_tensor_list in GPU0 includes 0_0 and 1_0, and GPU1 includes 0_1 and 1_1.\\n\\n    .. image:: https://githubraw.cdn.bcebos.com/PaddlePaddle/docs/develop/docs/api/paddle/distributed/img/alltoall.png\\n        :width: 800\\n        :alt: alltoall\\n        :align: center\\n\\n    Args:\\n        in_tensor_list (List[Tensor]): List of tensors to scatter one per rank. The data type of each tensor\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        out_tensor_list (List[Tensor]): List of tensors to be gathered one per rank. The data type of each tensor should be the same as the input tensors.\\n        group (Group, optional): The group instance return by new_group or None for global default group. Default: None.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> out_tensor_list = []\\n            >>> if dist.get_rank() == 0:\\n            ...     data1 = paddle.to_tensor([[1, 2, 3], [4, 5, 6]])\\n            ...     data2 = paddle.to_tensor([[7, 8, 9], [10, 11, 12]])\\n            >>> else:\\n            ...     data1 = paddle.to_tensor([[13, 14, 15], [16, 17, 18]])\\n            ...     data2 = paddle.to_tensor([[19, 20, 21], [22, 23, 24]])\\n            >>> dist.alltoall([data1, data2], out_tensor_list)\\n            >>> print(out_tensor_list)\\n            >>> # [[[1, 2, 3], [4, 5, 6]], [[13, 14, 15], [16, 17, 18]]] (2 GPUs, out for rank 0)\\n            >>> # [[[7, 8, 9], [10, 11, 12]], [[19, 20, 21], [22, 23, 24]]] (2 GPUs, out for rank 1)\\n    '\n    return stream.alltoall(out_tensor_list, in_tensor_list, group, sync_op, False)",
            "def alltoall(in_tensor_list, out_tensor_list, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Scatter tensors in in_tensor_list to all participators averagely and gather the result tensors in out_tensor_list.\\n    As shown below, the in_tensor_list in GPU0 includes 0_0 and 0_1, and GPU1 includes 1_0 and 1_1.\\n    Through alltoall operator, the 0_0 in GPU0 will be sent to GPU0 and 0_1 to GPU1, 1_0 in GPU1 sent to GPU0 and 1_1 to GPU1.\\n    Finally the out_tensor_list in GPU0 includes 0_0 and 1_0, and GPU1 includes 0_1 and 1_1.\\n\\n    .. image:: https://githubraw.cdn.bcebos.com/PaddlePaddle/docs/develop/docs/api/paddle/distributed/img/alltoall.png\\n        :width: 800\\n        :alt: alltoall\\n        :align: center\\n\\n    Args:\\n        in_tensor_list (List[Tensor]): List of tensors to scatter one per rank. The data type of each tensor\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        out_tensor_list (List[Tensor]): List of tensors to be gathered one per rank. The data type of each tensor should be the same as the input tensors.\\n        group (Group, optional): The group instance return by new_group or None for global default group. Default: None.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> out_tensor_list = []\\n            >>> if dist.get_rank() == 0:\\n            ...     data1 = paddle.to_tensor([[1, 2, 3], [4, 5, 6]])\\n            ...     data2 = paddle.to_tensor([[7, 8, 9], [10, 11, 12]])\\n            >>> else:\\n            ...     data1 = paddle.to_tensor([[13, 14, 15], [16, 17, 18]])\\n            ...     data2 = paddle.to_tensor([[19, 20, 21], [22, 23, 24]])\\n            >>> dist.alltoall([data1, data2], out_tensor_list)\\n            >>> print(out_tensor_list)\\n            >>> # [[[1, 2, 3], [4, 5, 6]], [[13, 14, 15], [16, 17, 18]]] (2 GPUs, out for rank 0)\\n            >>> # [[[7, 8, 9], [10, 11, 12]], [[19, 20, 21], [22, 23, 24]]] (2 GPUs, out for rank 1)\\n    '\n    return stream.alltoall(out_tensor_list, in_tensor_list, group, sync_op, False)",
            "def alltoall(in_tensor_list, out_tensor_list, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Scatter tensors in in_tensor_list to all participators averagely and gather the result tensors in out_tensor_list.\\n    As shown below, the in_tensor_list in GPU0 includes 0_0 and 0_1, and GPU1 includes 1_0 and 1_1.\\n    Through alltoall operator, the 0_0 in GPU0 will be sent to GPU0 and 0_1 to GPU1, 1_0 in GPU1 sent to GPU0 and 1_1 to GPU1.\\n    Finally the out_tensor_list in GPU0 includes 0_0 and 1_0, and GPU1 includes 0_1 and 1_1.\\n\\n    .. image:: https://githubraw.cdn.bcebos.com/PaddlePaddle/docs/develop/docs/api/paddle/distributed/img/alltoall.png\\n        :width: 800\\n        :alt: alltoall\\n        :align: center\\n\\n    Args:\\n        in_tensor_list (List[Tensor]): List of tensors to scatter one per rank. The data type of each tensor\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        out_tensor_list (List[Tensor]): List of tensors to be gathered one per rank. The data type of each tensor should be the same as the input tensors.\\n        group (Group, optional): The group instance return by new_group or None for global default group. Default: None.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> out_tensor_list = []\\n            >>> if dist.get_rank() == 0:\\n            ...     data1 = paddle.to_tensor([[1, 2, 3], [4, 5, 6]])\\n            ...     data2 = paddle.to_tensor([[7, 8, 9], [10, 11, 12]])\\n            >>> else:\\n            ...     data1 = paddle.to_tensor([[13, 14, 15], [16, 17, 18]])\\n            ...     data2 = paddle.to_tensor([[19, 20, 21], [22, 23, 24]])\\n            >>> dist.alltoall([data1, data2], out_tensor_list)\\n            >>> print(out_tensor_list)\\n            >>> # [[[1, 2, 3], [4, 5, 6]], [[13, 14, 15], [16, 17, 18]]] (2 GPUs, out for rank 0)\\n            >>> # [[[7, 8, 9], [10, 11, 12]], [[19, 20, 21], [22, 23, 24]]] (2 GPUs, out for rank 1)\\n    '\n    return stream.alltoall(out_tensor_list, in_tensor_list, group, sync_op, False)",
            "def alltoall(in_tensor_list, out_tensor_list, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Scatter tensors in in_tensor_list to all participators averagely and gather the result tensors in out_tensor_list.\\n    As shown below, the in_tensor_list in GPU0 includes 0_0 and 0_1, and GPU1 includes 1_0 and 1_1.\\n    Through alltoall operator, the 0_0 in GPU0 will be sent to GPU0 and 0_1 to GPU1, 1_0 in GPU1 sent to GPU0 and 1_1 to GPU1.\\n    Finally the out_tensor_list in GPU0 includes 0_0 and 1_0, and GPU1 includes 0_1 and 1_1.\\n\\n    .. image:: https://githubraw.cdn.bcebos.com/PaddlePaddle/docs/develop/docs/api/paddle/distributed/img/alltoall.png\\n        :width: 800\\n        :alt: alltoall\\n        :align: center\\n\\n    Args:\\n        in_tensor_list (List[Tensor]): List of tensors to scatter one per rank. The data type of each tensor\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        out_tensor_list (List[Tensor]): List of tensors to be gathered one per rank. The data type of each tensor should be the same as the input tensors.\\n        group (Group, optional): The group instance return by new_group or None for global default group. Default: None.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> out_tensor_list = []\\n            >>> if dist.get_rank() == 0:\\n            ...     data1 = paddle.to_tensor([[1, 2, 3], [4, 5, 6]])\\n            ...     data2 = paddle.to_tensor([[7, 8, 9], [10, 11, 12]])\\n            >>> else:\\n            ...     data1 = paddle.to_tensor([[13, 14, 15], [16, 17, 18]])\\n            ...     data2 = paddle.to_tensor([[19, 20, 21], [22, 23, 24]])\\n            >>> dist.alltoall([data1, data2], out_tensor_list)\\n            >>> print(out_tensor_list)\\n            >>> # [[[1, 2, 3], [4, 5, 6]], [[13, 14, 15], [16, 17, 18]]] (2 GPUs, out for rank 0)\\n            >>> # [[[7, 8, 9], [10, 11, 12]], [[19, 20, 21], [22, 23, 24]]] (2 GPUs, out for rank 1)\\n    '\n    return stream.alltoall(out_tensor_list, in_tensor_list, group, sync_op, False)",
            "def alltoall(in_tensor_list, out_tensor_list, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Scatter tensors in in_tensor_list to all participators averagely and gather the result tensors in out_tensor_list.\\n    As shown below, the in_tensor_list in GPU0 includes 0_0 and 0_1, and GPU1 includes 1_0 and 1_1.\\n    Through alltoall operator, the 0_0 in GPU0 will be sent to GPU0 and 0_1 to GPU1, 1_0 in GPU1 sent to GPU0 and 1_1 to GPU1.\\n    Finally the out_tensor_list in GPU0 includes 0_0 and 1_0, and GPU1 includes 0_1 and 1_1.\\n\\n    .. image:: https://githubraw.cdn.bcebos.com/PaddlePaddle/docs/develop/docs/api/paddle/distributed/img/alltoall.png\\n        :width: 800\\n        :alt: alltoall\\n        :align: center\\n\\n    Args:\\n        in_tensor_list (List[Tensor]): List of tensors to scatter one per rank. The data type of each tensor\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        out_tensor_list (List[Tensor]): List of tensors to be gathered one per rank. The data type of each tensor should be the same as the input tensors.\\n        group (Group, optional): The group instance return by new_group or None for global default group. Default: None.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> out_tensor_list = []\\n            >>> if dist.get_rank() == 0:\\n            ...     data1 = paddle.to_tensor([[1, 2, 3], [4, 5, 6]])\\n            ...     data2 = paddle.to_tensor([[7, 8, 9], [10, 11, 12]])\\n            >>> else:\\n            ...     data1 = paddle.to_tensor([[13, 14, 15], [16, 17, 18]])\\n            ...     data2 = paddle.to_tensor([[19, 20, 21], [22, 23, 24]])\\n            >>> dist.alltoall([data1, data2], out_tensor_list)\\n            >>> print(out_tensor_list)\\n            >>> # [[[1, 2, 3], [4, 5, 6]], [[13, 14, 15], [16, 17, 18]]] (2 GPUs, out for rank 0)\\n            >>> # [[[7, 8, 9], [10, 11, 12]], [[19, 20, 21], [22, 23, 24]]] (2 GPUs, out for rank 1)\\n    '\n    return stream.alltoall(out_tensor_list, in_tensor_list, group, sync_op, False)"
        ]
    },
    {
        "func_name": "alltoall_single",
        "original": "def alltoall_single(in_tensor, out_tensor, in_split_sizes=None, out_split_sizes=None, group=None, sync_op=True):\n    \"\"\"\n    Scatter a single input tensor to all participators and gather the received tensors in out_tensor.\n\n    Note:\n        ``alltoall_single`` is only supported in eager mode.\n\n    Args:\n        in_tensor (Tensor): Input tensor. The data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\n        out_tensor (Tensor): Output Tensor. The data type should be the same as the data type of the input Tensor.\n        in_split_sizes (list[int], optional): Split sizes of ``in_tensor`` for dim[0]. If not given, dim[0] of ``in_tensor``\n            must be divisible by group size and ``in_tensor`` will be scattered averagely to all participators. Default: None.\n        out_split_sizes (list[int], optional): Split sizes of ``out_tensor`` for dim[0]. If not given, dim[0] of ``out_tensor``\n            must be divisible by group size and ``out_tensor`` will be gathered averagely from all participators. Default: None.\n        group (Group, optional): The group instance return by ``new_group`` or None for global default group. Default: None.\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\n\n    Returns:\n        Return a task object.\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\n            >>> import paddle\n            >>> import paddle.distributed as dist\n\n            >>> dist.init_parallel_env()\n            >>> rank = dist.get_rank()\n            >>> size = dist.get_world_size()\n\n            >>> # case 1 (2 GPUs)\n            >>> data = paddle.arange(2, dtype='int64') + rank * 2\n            >>> # data for rank 0: [0, 1]\n            >>> # data for rank 1: [2, 3]\n            >>> output = paddle.empty([2], dtype='int64')\n            >>> dist.alltoall_single(data, output)\n            >>> print(output)\n            >>> # output for rank 0: [0, 2]\n            >>> # output for rank 1: [1, 3]\n\n            >>> # case 2 (2 GPUs)\n            >>> in_split_sizes = [i + 1 for i in range(size)]\n            >>> # in_split_sizes for rank 0: [1, 2]\n            >>> # in_split_sizes for rank 1: [1, 2]\n            >>> out_split_sizes = [rank + 1 for i in range(size)]\n            >>> # out_split_sizes for rank 0: [1, 1]\n            >>> # out_split_sizes for rank 1: [2, 2]\n            >>> data = paddle.ones([sum(in_split_sizes), size], dtype='float32') * rank\n            >>> # data for rank 0: [[0., 0.], [0., 0.], [0., 0.]]\n            >>> # data for rank 1: [[1., 1.], [1., 1.], [1., 1.]]\n            >>> output = paddle.empty([(rank + 1) * size, size], dtype='float32')\n            >>> group = dist.new_group([0, 1])\n            >>> task = dist.alltoall_single(data,\n            ...                             output,\n            ...                             in_split_sizes,\n            ...                             out_split_sizes,\n            ...                             sync_op=False,\n            ...                             group=group)\n            >>> task.wait()\n            >>> print(output)\n            >>> # output for rank 0: [[0., 0.], [1., 1.]]\n            >>> # output for rank 1: [[0., 0.], [0., 0.], [1., 1.], [1., 1.]]\n\n    \"\"\"\n    return stream.alltoall_single(out_tensor, in_tensor, out_split_sizes, in_split_sizes, group, sync_op, False)",
        "mutated": [
            "def alltoall_single(in_tensor, out_tensor, in_split_sizes=None, out_split_sizes=None, group=None, sync_op=True):\n    if False:\n        i = 10\n    \"\\n    Scatter a single input tensor to all participators and gather the received tensors in out_tensor.\\n\\n    Note:\\n        ``alltoall_single`` is only supported in eager mode.\\n\\n    Args:\\n        in_tensor (Tensor): Input tensor. The data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        out_tensor (Tensor): Output Tensor. The data type should be the same as the data type of the input Tensor.\\n        in_split_sizes (list[int], optional): Split sizes of ``in_tensor`` for dim[0]. If not given, dim[0] of ``in_tensor``\\n            must be divisible by group size and ``in_tensor`` will be scattered averagely to all participators. Default: None.\\n        out_split_sizes (list[int], optional): Split sizes of ``out_tensor`` for dim[0]. If not given, dim[0] of ``out_tensor``\\n            must be divisible by group size and ``out_tensor`` will be gathered averagely from all participators. Default: None.\\n        group (Group, optional): The group instance return by ``new_group`` or None for global default group. Default: None.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> rank = dist.get_rank()\\n            >>> size = dist.get_world_size()\\n\\n            >>> # case 1 (2 GPUs)\\n            >>> data = paddle.arange(2, dtype='int64') + rank * 2\\n            >>> # data for rank 0: [0, 1]\\n            >>> # data for rank 1: [2, 3]\\n            >>> output = paddle.empty([2], dtype='int64')\\n            >>> dist.alltoall_single(data, output)\\n            >>> print(output)\\n            >>> # output for rank 0: [0, 2]\\n            >>> # output for rank 1: [1, 3]\\n\\n            >>> # case 2 (2 GPUs)\\n            >>> in_split_sizes = [i + 1 for i in range(size)]\\n            >>> # in_split_sizes for rank 0: [1, 2]\\n            >>> # in_split_sizes for rank 1: [1, 2]\\n            >>> out_split_sizes = [rank + 1 for i in range(size)]\\n            >>> # out_split_sizes for rank 0: [1, 1]\\n            >>> # out_split_sizes for rank 1: [2, 2]\\n            >>> data = paddle.ones([sum(in_split_sizes), size], dtype='float32') * rank\\n            >>> # data for rank 0: [[0., 0.], [0., 0.], [0., 0.]]\\n            >>> # data for rank 1: [[1., 1.], [1., 1.], [1., 1.]]\\n            >>> output = paddle.empty([(rank + 1) * size, size], dtype='float32')\\n            >>> group = dist.new_group([0, 1])\\n            >>> task = dist.alltoall_single(data,\\n            ...                             output,\\n            ...                             in_split_sizes,\\n            ...                             out_split_sizes,\\n            ...                             sync_op=False,\\n            ...                             group=group)\\n            >>> task.wait()\\n            >>> print(output)\\n            >>> # output for rank 0: [[0., 0.], [1., 1.]]\\n            >>> # output for rank 1: [[0., 0.], [0., 0.], [1., 1.], [1., 1.]]\\n\\n    \"\n    return stream.alltoall_single(out_tensor, in_tensor, out_split_sizes, in_split_sizes, group, sync_op, False)",
            "def alltoall_single(in_tensor, out_tensor, in_split_sizes=None, out_split_sizes=None, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Scatter a single input tensor to all participators and gather the received tensors in out_tensor.\\n\\n    Note:\\n        ``alltoall_single`` is only supported in eager mode.\\n\\n    Args:\\n        in_tensor (Tensor): Input tensor. The data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        out_tensor (Tensor): Output Tensor. The data type should be the same as the data type of the input Tensor.\\n        in_split_sizes (list[int], optional): Split sizes of ``in_tensor`` for dim[0]. If not given, dim[0] of ``in_tensor``\\n            must be divisible by group size and ``in_tensor`` will be scattered averagely to all participators. Default: None.\\n        out_split_sizes (list[int], optional): Split sizes of ``out_tensor`` for dim[0]. If not given, dim[0] of ``out_tensor``\\n            must be divisible by group size and ``out_tensor`` will be gathered averagely from all participators. Default: None.\\n        group (Group, optional): The group instance return by ``new_group`` or None for global default group. Default: None.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> rank = dist.get_rank()\\n            >>> size = dist.get_world_size()\\n\\n            >>> # case 1 (2 GPUs)\\n            >>> data = paddle.arange(2, dtype='int64') + rank * 2\\n            >>> # data for rank 0: [0, 1]\\n            >>> # data for rank 1: [2, 3]\\n            >>> output = paddle.empty([2], dtype='int64')\\n            >>> dist.alltoall_single(data, output)\\n            >>> print(output)\\n            >>> # output for rank 0: [0, 2]\\n            >>> # output for rank 1: [1, 3]\\n\\n            >>> # case 2 (2 GPUs)\\n            >>> in_split_sizes = [i + 1 for i in range(size)]\\n            >>> # in_split_sizes for rank 0: [1, 2]\\n            >>> # in_split_sizes for rank 1: [1, 2]\\n            >>> out_split_sizes = [rank + 1 for i in range(size)]\\n            >>> # out_split_sizes for rank 0: [1, 1]\\n            >>> # out_split_sizes for rank 1: [2, 2]\\n            >>> data = paddle.ones([sum(in_split_sizes), size], dtype='float32') * rank\\n            >>> # data for rank 0: [[0., 0.], [0., 0.], [0., 0.]]\\n            >>> # data for rank 1: [[1., 1.], [1., 1.], [1., 1.]]\\n            >>> output = paddle.empty([(rank + 1) * size, size], dtype='float32')\\n            >>> group = dist.new_group([0, 1])\\n            >>> task = dist.alltoall_single(data,\\n            ...                             output,\\n            ...                             in_split_sizes,\\n            ...                             out_split_sizes,\\n            ...                             sync_op=False,\\n            ...                             group=group)\\n            >>> task.wait()\\n            >>> print(output)\\n            >>> # output for rank 0: [[0., 0.], [1., 1.]]\\n            >>> # output for rank 1: [[0., 0.], [0., 0.], [1., 1.], [1., 1.]]\\n\\n    \"\n    return stream.alltoall_single(out_tensor, in_tensor, out_split_sizes, in_split_sizes, group, sync_op, False)",
            "def alltoall_single(in_tensor, out_tensor, in_split_sizes=None, out_split_sizes=None, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Scatter a single input tensor to all participators and gather the received tensors in out_tensor.\\n\\n    Note:\\n        ``alltoall_single`` is only supported in eager mode.\\n\\n    Args:\\n        in_tensor (Tensor): Input tensor. The data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        out_tensor (Tensor): Output Tensor. The data type should be the same as the data type of the input Tensor.\\n        in_split_sizes (list[int], optional): Split sizes of ``in_tensor`` for dim[0]. If not given, dim[0] of ``in_tensor``\\n            must be divisible by group size and ``in_tensor`` will be scattered averagely to all participators. Default: None.\\n        out_split_sizes (list[int], optional): Split sizes of ``out_tensor`` for dim[0]. If not given, dim[0] of ``out_tensor``\\n            must be divisible by group size and ``out_tensor`` will be gathered averagely from all participators. Default: None.\\n        group (Group, optional): The group instance return by ``new_group`` or None for global default group. Default: None.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> rank = dist.get_rank()\\n            >>> size = dist.get_world_size()\\n\\n            >>> # case 1 (2 GPUs)\\n            >>> data = paddle.arange(2, dtype='int64') + rank * 2\\n            >>> # data for rank 0: [0, 1]\\n            >>> # data for rank 1: [2, 3]\\n            >>> output = paddle.empty([2], dtype='int64')\\n            >>> dist.alltoall_single(data, output)\\n            >>> print(output)\\n            >>> # output for rank 0: [0, 2]\\n            >>> # output for rank 1: [1, 3]\\n\\n            >>> # case 2 (2 GPUs)\\n            >>> in_split_sizes = [i + 1 for i in range(size)]\\n            >>> # in_split_sizes for rank 0: [1, 2]\\n            >>> # in_split_sizes for rank 1: [1, 2]\\n            >>> out_split_sizes = [rank + 1 for i in range(size)]\\n            >>> # out_split_sizes for rank 0: [1, 1]\\n            >>> # out_split_sizes for rank 1: [2, 2]\\n            >>> data = paddle.ones([sum(in_split_sizes), size], dtype='float32') * rank\\n            >>> # data for rank 0: [[0., 0.], [0., 0.], [0., 0.]]\\n            >>> # data for rank 1: [[1., 1.], [1., 1.], [1., 1.]]\\n            >>> output = paddle.empty([(rank + 1) * size, size], dtype='float32')\\n            >>> group = dist.new_group([0, 1])\\n            >>> task = dist.alltoall_single(data,\\n            ...                             output,\\n            ...                             in_split_sizes,\\n            ...                             out_split_sizes,\\n            ...                             sync_op=False,\\n            ...                             group=group)\\n            >>> task.wait()\\n            >>> print(output)\\n            >>> # output for rank 0: [[0., 0.], [1., 1.]]\\n            >>> # output for rank 1: [[0., 0.], [0., 0.], [1., 1.], [1., 1.]]\\n\\n    \"\n    return stream.alltoall_single(out_tensor, in_tensor, out_split_sizes, in_split_sizes, group, sync_op, False)",
            "def alltoall_single(in_tensor, out_tensor, in_split_sizes=None, out_split_sizes=None, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Scatter a single input tensor to all participators and gather the received tensors in out_tensor.\\n\\n    Note:\\n        ``alltoall_single`` is only supported in eager mode.\\n\\n    Args:\\n        in_tensor (Tensor): Input tensor. The data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        out_tensor (Tensor): Output Tensor. The data type should be the same as the data type of the input Tensor.\\n        in_split_sizes (list[int], optional): Split sizes of ``in_tensor`` for dim[0]. If not given, dim[0] of ``in_tensor``\\n            must be divisible by group size and ``in_tensor`` will be scattered averagely to all participators. Default: None.\\n        out_split_sizes (list[int], optional): Split sizes of ``out_tensor`` for dim[0]. If not given, dim[0] of ``out_tensor``\\n            must be divisible by group size and ``out_tensor`` will be gathered averagely from all participators. Default: None.\\n        group (Group, optional): The group instance return by ``new_group`` or None for global default group. Default: None.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> rank = dist.get_rank()\\n            >>> size = dist.get_world_size()\\n\\n            >>> # case 1 (2 GPUs)\\n            >>> data = paddle.arange(2, dtype='int64') + rank * 2\\n            >>> # data for rank 0: [0, 1]\\n            >>> # data for rank 1: [2, 3]\\n            >>> output = paddle.empty([2], dtype='int64')\\n            >>> dist.alltoall_single(data, output)\\n            >>> print(output)\\n            >>> # output for rank 0: [0, 2]\\n            >>> # output for rank 1: [1, 3]\\n\\n            >>> # case 2 (2 GPUs)\\n            >>> in_split_sizes = [i + 1 for i in range(size)]\\n            >>> # in_split_sizes for rank 0: [1, 2]\\n            >>> # in_split_sizes for rank 1: [1, 2]\\n            >>> out_split_sizes = [rank + 1 for i in range(size)]\\n            >>> # out_split_sizes for rank 0: [1, 1]\\n            >>> # out_split_sizes for rank 1: [2, 2]\\n            >>> data = paddle.ones([sum(in_split_sizes), size], dtype='float32') * rank\\n            >>> # data for rank 0: [[0., 0.], [0., 0.], [0., 0.]]\\n            >>> # data for rank 1: [[1., 1.], [1., 1.], [1., 1.]]\\n            >>> output = paddle.empty([(rank + 1) * size, size], dtype='float32')\\n            >>> group = dist.new_group([0, 1])\\n            >>> task = dist.alltoall_single(data,\\n            ...                             output,\\n            ...                             in_split_sizes,\\n            ...                             out_split_sizes,\\n            ...                             sync_op=False,\\n            ...                             group=group)\\n            >>> task.wait()\\n            >>> print(output)\\n            >>> # output for rank 0: [[0., 0.], [1., 1.]]\\n            >>> # output for rank 1: [[0., 0.], [0., 0.], [1., 1.], [1., 1.]]\\n\\n    \"\n    return stream.alltoall_single(out_tensor, in_tensor, out_split_sizes, in_split_sizes, group, sync_op, False)",
            "def alltoall_single(in_tensor, out_tensor, in_split_sizes=None, out_split_sizes=None, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Scatter a single input tensor to all participators and gather the received tensors in out_tensor.\\n\\n    Note:\\n        ``alltoall_single`` is only supported in eager mode.\\n\\n    Args:\\n        in_tensor (Tensor): Input tensor. The data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        out_tensor (Tensor): Output Tensor. The data type should be the same as the data type of the input Tensor.\\n        in_split_sizes (list[int], optional): Split sizes of ``in_tensor`` for dim[0]. If not given, dim[0] of ``in_tensor``\\n            must be divisible by group size and ``in_tensor`` will be scattered averagely to all participators. Default: None.\\n        out_split_sizes (list[int], optional): Split sizes of ``out_tensor`` for dim[0]. If not given, dim[0] of ``out_tensor``\\n            must be divisible by group size and ``out_tensor`` will be gathered averagely from all participators. Default: None.\\n        group (Group, optional): The group instance return by ``new_group`` or None for global default group. Default: None.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> rank = dist.get_rank()\\n            >>> size = dist.get_world_size()\\n\\n            >>> # case 1 (2 GPUs)\\n            >>> data = paddle.arange(2, dtype='int64') + rank * 2\\n            >>> # data for rank 0: [0, 1]\\n            >>> # data for rank 1: [2, 3]\\n            >>> output = paddle.empty([2], dtype='int64')\\n            >>> dist.alltoall_single(data, output)\\n            >>> print(output)\\n            >>> # output for rank 0: [0, 2]\\n            >>> # output for rank 1: [1, 3]\\n\\n            >>> # case 2 (2 GPUs)\\n            >>> in_split_sizes = [i + 1 for i in range(size)]\\n            >>> # in_split_sizes for rank 0: [1, 2]\\n            >>> # in_split_sizes for rank 1: [1, 2]\\n            >>> out_split_sizes = [rank + 1 for i in range(size)]\\n            >>> # out_split_sizes for rank 0: [1, 1]\\n            >>> # out_split_sizes for rank 1: [2, 2]\\n            >>> data = paddle.ones([sum(in_split_sizes), size], dtype='float32') * rank\\n            >>> # data for rank 0: [[0., 0.], [0., 0.], [0., 0.]]\\n            >>> # data for rank 1: [[1., 1.], [1., 1.], [1., 1.]]\\n            >>> output = paddle.empty([(rank + 1) * size, size], dtype='float32')\\n            >>> group = dist.new_group([0, 1])\\n            >>> task = dist.alltoall_single(data,\\n            ...                             output,\\n            ...                             in_split_sizes,\\n            ...                             out_split_sizes,\\n            ...                             sync_op=False,\\n            ...                             group=group)\\n            >>> task.wait()\\n            >>> print(output)\\n            >>> # output for rank 0: [[0., 0.], [1., 1.]]\\n            >>> # output for rank 1: [[0., 0.], [0., 0.], [1., 1.], [1., 1.]]\\n\\n    \"\n    return stream.alltoall_single(out_tensor, in_tensor, out_split_sizes, in_split_sizes, group, sync_op, False)"
        ]
    }
]