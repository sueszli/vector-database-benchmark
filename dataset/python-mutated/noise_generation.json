[
    {
        "func_name": "noise_matrix_is_valid",
        "original": "def noise_matrix_is_valid(noise_matrix, py, *, verbose=False) -> bool:\n    \"\"\"Given a prior `py` representing ``p(true_label=k)``, checks if the given `noise_matrix` is a\n    learnable matrix. Learnability means that it is possible to achieve\n    better than random performance, on average, for the amount of noise in\n    `noise_matrix`.\n\n    Parameters\n    ----------\n    noise_matrix : np.ndarray\n      An array of shape ``(K, K)`` representing the conditional probability\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\n      examples in every class, labeled as every other class. Assumes columns of\n      `noise_matrix` sum to 1.\n\n    py : np.ndarray\n      An array of shape ``(K,)`` representing the fraction (prior probability)\n      of each true class label, ``P(true_label = k)``.\n\n    Returns\n    -------\n    is_valid : bool\n      Whether the noise matrix is a learnable matrix.\n    \"\"\"\n    K = len(py)\n    N = float(10000)\n    ps = np.dot(noise_matrix, py)\n    joint_noise = np.multiply(noise_matrix, py)\n    if not abs(joint_noise.sum() - 1.0) < FLOATING_POINT_COMPARISON:\n        return False\n    for i in range(K):\n        C = N * joint_noise[i][i]\n        E1 = N * joint_noise[i].sum() - C\n        E2 = N * joint_noise.T[i].sum() - C\n        O = N - E1 - E2 - C\n        if verbose:\n            print('E1E2/C', round(E1 * E2 / C), 'E1', round(E1), 'E2', round(E2), 'C', round(C), '|', round(E1 * E2 / C + E1 + E2 + C), '|', round(E1 * E2 / C), '<', round(O))\n            print(round(ps[i] * py[i]), '<', round(joint_noise[i][i]), ':', ps[i] * py[i] < joint_noise[i][i])\n        if not ps[i] * py[i] < joint_noise[i][i]:\n            return False\n    return True",
        "mutated": [
            "def noise_matrix_is_valid(noise_matrix, py, *, verbose=False) -> bool:\n    if False:\n        i = 10\n    'Given a prior `py` representing ``p(true_label=k)``, checks if the given `noise_matrix` is a\\n    learnable matrix. Learnability means that it is possible to achieve\\n    better than random performance, on average, for the amount of noise in\\n    `noise_matrix`.\\n\\n    Parameters\\n    ----------\\n    noise_matrix : np.ndarray\\n      An array of shape ``(K, K)`` representing the conditional probability\\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\\n      examples in every class, labeled as every other class. Assumes columns of\\n      `noise_matrix` sum to 1.\\n\\n    py : np.ndarray\\n      An array of shape ``(K,)`` representing the fraction (prior probability)\\n      of each true class label, ``P(true_label = k)``.\\n\\n    Returns\\n    -------\\n    is_valid : bool\\n      Whether the noise matrix is a learnable matrix.\\n    '\n    K = len(py)\n    N = float(10000)\n    ps = np.dot(noise_matrix, py)\n    joint_noise = np.multiply(noise_matrix, py)\n    if not abs(joint_noise.sum() - 1.0) < FLOATING_POINT_COMPARISON:\n        return False\n    for i in range(K):\n        C = N * joint_noise[i][i]\n        E1 = N * joint_noise[i].sum() - C\n        E2 = N * joint_noise.T[i].sum() - C\n        O = N - E1 - E2 - C\n        if verbose:\n            print('E1E2/C', round(E1 * E2 / C), 'E1', round(E1), 'E2', round(E2), 'C', round(C), '|', round(E1 * E2 / C + E1 + E2 + C), '|', round(E1 * E2 / C), '<', round(O))\n            print(round(ps[i] * py[i]), '<', round(joint_noise[i][i]), ':', ps[i] * py[i] < joint_noise[i][i])\n        if not ps[i] * py[i] < joint_noise[i][i]:\n            return False\n    return True",
            "def noise_matrix_is_valid(noise_matrix, py, *, verbose=False) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Given a prior `py` representing ``p(true_label=k)``, checks if the given `noise_matrix` is a\\n    learnable matrix. Learnability means that it is possible to achieve\\n    better than random performance, on average, for the amount of noise in\\n    `noise_matrix`.\\n\\n    Parameters\\n    ----------\\n    noise_matrix : np.ndarray\\n      An array of shape ``(K, K)`` representing the conditional probability\\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\\n      examples in every class, labeled as every other class. Assumes columns of\\n      `noise_matrix` sum to 1.\\n\\n    py : np.ndarray\\n      An array of shape ``(K,)`` representing the fraction (prior probability)\\n      of each true class label, ``P(true_label = k)``.\\n\\n    Returns\\n    -------\\n    is_valid : bool\\n      Whether the noise matrix is a learnable matrix.\\n    '\n    K = len(py)\n    N = float(10000)\n    ps = np.dot(noise_matrix, py)\n    joint_noise = np.multiply(noise_matrix, py)\n    if not abs(joint_noise.sum() - 1.0) < FLOATING_POINT_COMPARISON:\n        return False\n    for i in range(K):\n        C = N * joint_noise[i][i]\n        E1 = N * joint_noise[i].sum() - C\n        E2 = N * joint_noise.T[i].sum() - C\n        O = N - E1 - E2 - C\n        if verbose:\n            print('E1E2/C', round(E1 * E2 / C), 'E1', round(E1), 'E2', round(E2), 'C', round(C), '|', round(E1 * E2 / C + E1 + E2 + C), '|', round(E1 * E2 / C), '<', round(O))\n            print(round(ps[i] * py[i]), '<', round(joint_noise[i][i]), ':', ps[i] * py[i] < joint_noise[i][i])\n        if not ps[i] * py[i] < joint_noise[i][i]:\n            return False\n    return True",
            "def noise_matrix_is_valid(noise_matrix, py, *, verbose=False) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Given a prior `py` representing ``p(true_label=k)``, checks if the given `noise_matrix` is a\\n    learnable matrix. Learnability means that it is possible to achieve\\n    better than random performance, on average, for the amount of noise in\\n    `noise_matrix`.\\n\\n    Parameters\\n    ----------\\n    noise_matrix : np.ndarray\\n      An array of shape ``(K, K)`` representing the conditional probability\\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\\n      examples in every class, labeled as every other class. Assumes columns of\\n      `noise_matrix` sum to 1.\\n\\n    py : np.ndarray\\n      An array of shape ``(K,)`` representing the fraction (prior probability)\\n      of each true class label, ``P(true_label = k)``.\\n\\n    Returns\\n    -------\\n    is_valid : bool\\n      Whether the noise matrix is a learnable matrix.\\n    '\n    K = len(py)\n    N = float(10000)\n    ps = np.dot(noise_matrix, py)\n    joint_noise = np.multiply(noise_matrix, py)\n    if not abs(joint_noise.sum() - 1.0) < FLOATING_POINT_COMPARISON:\n        return False\n    for i in range(K):\n        C = N * joint_noise[i][i]\n        E1 = N * joint_noise[i].sum() - C\n        E2 = N * joint_noise.T[i].sum() - C\n        O = N - E1 - E2 - C\n        if verbose:\n            print('E1E2/C', round(E1 * E2 / C), 'E1', round(E1), 'E2', round(E2), 'C', round(C), '|', round(E1 * E2 / C + E1 + E2 + C), '|', round(E1 * E2 / C), '<', round(O))\n            print(round(ps[i] * py[i]), '<', round(joint_noise[i][i]), ':', ps[i] * py[i] < joint_noise[i][i])\n        if not ps[i] * py[i] < joint_noise[i][i]:\n            return False\n    return True",
            "def noise_matrix_is_valid(noise_matrix, py, *, verbose=False) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Given a prior `py` representing ``p(true_label=k)``, checks if the given `noise_matrix` is a\\n    learnable matrix. Learnability means that it is possible to achieve\\n    better than random performance, on average, for the amount of noise in\\n    `noise_matrix`.\\n\\n    Parameters\\n    ----------\\n    noise_matrix : np.ndarray\\n      An array of shape ``(K, K)`` representing the conditional probability\\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\\n      examples in every class, labeled as every other class. Assumes columns of\\n      `noise_matrix` sum to 1.\\n\\n    py : np.ndarray\\n      An array of shape ``(K,)`` representing the fraction (prior probability)\\n      of each true class label, ``P(true_label = k)``.\\n\\n    Returns\\n    -------\\n    is_valid : bool\\n      Whether the noise matrix is a learnable matrix.\\n    '\n    K = len(py)\n    N = float(10000)\n    ps = np.dot(noise_matrix, py)\n    joint_noise = np.multiply(noise_matrix, py)\n    if not abs(joint_noise.sum() - 1.0) < FLOATING_POINT_COMPARISON:\n        return False\n    for i in range(K):\n        C = N * joint_noise[i][i]\n        E1 = N * joint_noise[i].sum() - C\n        E2 = N * joint_noise.T[i].sum() - C\n        O = N - E1 - E2 - C\n        if verbose:\n            print('E1E2/C', round(E1 * E2 / C), 'E1', round(E1), 'E2', round(E2), 'C', round(C), '|', round(E1 * E2 / C + E1 + E2 + C), '|', round(E1 * E2 / C), '<', round(O))\n            print(round(ps[i] * py[i]), '<', round(joint_noise[i][i]), ':', ps[i] * py[i] < joint_noise[i][i])\n        if not ps[i] * py[i] < joint_noise[i][i]:\n            return False\n    return True",
            "def noise_matrix_is_valid(noise_matrix, py, *, verbose=False) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Given a prior `py` representing ``p(true_label=k)``, checks if the given `noise_matrix` is a\\n    learnable matrix. Learnability means that it is possible to achieve\\n    better than random performance, on average, for the amount of noise in\\n    `noise_matrix`.\\n\\n    Parameters\\n    ----------\\n    noise_matrix : np.ndarray\\n      An array of shape ``(K, K)`` representing the conditional probability\\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\\n      examples in every class, labeled as every other class. Assumes columns of\\n      `noise_matrix` sum to 1.\\n\\n    py : np.ndarray\\n      An array of shape ``(K,)`` representing the fraction (prior probability)\\n      of each true class label, ``P(true_label = k)``.\\n\\n    Returns\\n    -------\\n    is_valid : bool\\n      Whether the noise matrix is a learnable matrix.\\n    '\n    K = len(py)\n    N = float(10000)\n    ps = np.dot(noise_matrix, py)\n    joint_noise = np.multiply(noise_matrix, py)\n    if not abs(joint_noise.sum() - 1.0) < FLOATING_POINT_COMPARISON:\n        return False\n    for i in range(K):\n        C = N * joint_noise[i][i]\n        E1 = N * joint_noise[i].sum() - C\n        E2 = N * joint_noise.T[i].sum() - C\n        O = N - E1 - E2 - C\n        if verbose:\n            print('E1E2/C', round(E1 * E2 / C), 'E1', round(E1), 'E2', round(E2), 'C', round(C), '|', round(E1 * E2 / C + E1 + E2 + C), '|', round(E1 * E2 / C), '<', round(O))\n            print(round(ps[i] * py[i]), '<', round(joint_noise[i][i]), ':', ps[i] * py[i] < joint_noise[i][i])\n        if not ps[i] * py[i] < joint_noise[i][i]:\n            return False\n    return True"
        ]
    },
    {
        "func_name": "generate_noisy_labels",
        "original": "def generate_noisy_labels(true_labels, noise_matrix) -> np.ndarray:\n    \"\"\"Generates noisy `labels` from perfect labels `true_labels`,\n    \"exactly\" yielding the provided `noise_matrix` between `labels` and `true_labels`.\n\n    Below we provide a for loop implementation of what this function does.\n    We do not use this implementation as it is not a fast algorithm, but\n    it explains as Python pseudocode what is happening in this function.\n\n    Parameters\n    ----------\n    true_labels : np.ndarray\n      An array of shape ``(N,)`` representing perfect labels, without any\n      noise. Contains K distinct natural number classes, 0, 1, ..., K-1.\n\n    noise_matrix : np.ndarray\n      An array of shape ``(K, K)`` representing the conditional probability\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\n      examples in every class, labeled as every other class. Assumes columns of\n      `noise_matrix` sum to 1.\n\n    Returns\n    -------\n    labels : np.ndarray\n      An array of shape ``(N,)`` of noisy labels.\n\n    Examples\n    --------\n\n    .. code:: python\n\n        # Generate labels\n        count_joint = (noise_matrix * py * len(y)).round().astype(int)\n        labels = np.ndarray(y)\n        for k_s in range(K):\n            for k_y in range(K):\n                if k_s != k_y:\n                    idx_flip = np.where((labels==k_y)&(true_label==k_y))[0]\n                    if len(idx_flip): # pragma: no cover\n                        labels[np.random.choice(\n                            idx_flip,\n                            count_joint[k_s][k_y],\n                            replace=False,\n                        )] = k_s\n    \"\"\"\n    true_labels = np.asarray(true_labels)\n    K = len(noise_matrix)\n    py = value_counts(true_labels) / float(len(true_labels))\n    count_joint = (noise_matrix * py * len(true_labels)).astype(int)\n    np.fill_diagonal(count_joint, 0)\n    labels = np.array(true_labels)\n    for k in range(K):\n        labels_per_class = np.where(count_joint[:, k] != 0)[0]\n        label_counts = count_joint[labels_per_class, k]\n        noise = [labels_per_class[i] for (i, c) in enumerate(label_counts) for z in range(c)]\n        idx_flip = np.where((labels == k) & (true_labels == k))[0]\n        if len(idx_flip) and len(noise) and (len(idx_flip) >= len(noise)):\n            labels[np.random.choice(idx_flip, len(noise), replace=False)] = noise\n    return labels",
        "mutated": [
            "def generate_noisy_labels(true_labels, noise_matrix) -> np.ndarray:\n    if False:\n        i = 10\n    'Generates noisy `labels` from perfect labels `true_labels`,\\n    \"exactly\" yielding the provided `noise_matrix` between `labels` and `true_labels`.\\n\\n    Below we provide a for loop implementation of what this function does.\\n    We do not use this implementation as it is not a fast algorithm, but\\n    it explains as Python pseudocode what is happening in this function.\\n\\n    Parameters\\n    ----------\\n    true_labels : np.ndarray\\n      An array of shape ``(N,)`` representing perfect labels, without any\\n      noise. Contains K distinct natural number classes, 0, 1, ..., K-1.\\n\\n    noise_matrix : np.ndarray\\n      An array of shape ``(K, K)`` representing the conditional probability\\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\\n      examples in every class, labeled as every other class. Assumes columns of\\n      `noise_matrix` sum to 1.\\n\\n    Returns\\n    -------\\n    labels : np.ndarray\\n      An array of shape ``(N,)`` of noisy labels.\\n\\n    Examples\\n    --------\\n\\n    .. code:: python\\n\\n        # Generate labels\\n        count_joint = (noise_matrix * py * len(y)).round().astype(int)\\n        labels = np.ndarray(y)\\n        for k_s in range(K):\\n            for k_y in range(K):\\n                if k_s != k_y:\\n                    idx_flip = np.where((labels==k_y)&(true_label==k_y))[0]\\n                    if len(idx_flip): # pragma: no cover\\n                        labels[np.random.choice(\\n                            idx_flip,\\n                            count_joint[k_s][k_y],\\n                            replace=False,\\n                        )] = k_s\\n    '\n    true_labels = np.asarray(true_labels)\n    K = len(noise_matrix)\n    py = value_counts(true_labels) / float(len(true_labels))\n    count_joint = (noise_matrix * py * len(true_labels)).astype(int)\n    np.fill_diagonal(count_joint, 0)\n    labels = np.array(true_labels)\n    for k in range(K):\n        labels_per_class = np.where(count_joint[:, k] != 0)[0]\n        label_counts = count_joint[labels_per_class, k]\n        noise = [labels_per_class[i] for (i, c) in enumerate(label_counts) for z in range(c)]\n        idx_flip = np.where((labels == k) & (true_labels == k))[0]\n        if len(idx_flip) and len(noise) and (len(idx_flip) >= len(noise)):\n            labels[np.random.choice(idx_flip, len(noise), replace=False)] = noise\n    return labels",
            "def generate_noisy_labels(true_labels, noise_matrix) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generates noisy `labels` from perfect labels `true_labels`,\\n    \"exactly\" yielding the provided `noise_matrix` between `labels` and `true_labels`.\\n\\n    Below we provide a for loop implementation of what this function does.\\n    We do not use this implementation as it is not a fast algorithm, but\\n    it explains as Python pseudocode what is happening in this function.\\n\\n    Parameters\\n    ----------\\n    true_labels : np.ndarray\\n      An array of shape ``(N,)`` representing perfect labels, without any\\n      noise. Contains K distinct natural number classes, 0, 1, ..., K-1.\\n\\n    noise_matrix : np.ndarray\\n      An array of shape ``(K, K)`` representing the conditional probability\\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\\n      examples in every class, labeled as every other class. Assumes columns of\\n      `noise_matrix` sum to 1.\\n\\n    Returns\\n    -------\\n    labels : np.ndarray\\n      An array of shape ``(N,)`` of noisy labels.\\n\\n    Examples\\n    --------\\n\\n    .. code:: python\\n\\n        # Generate labels\\n        count_joint = (noise_matrix * py * len(y)).round().astype(int)\\n        labels = np.ndarray(y)\\n        for k_s in range(K):\\n            for k_y in range(K):\\n                if k_s != k_y:\\n                    idx_flip = np.where((labels==k_y)&(true_label==k_y))[0]\\n                    if len(idx_flip): # pragma: no cover\\n                        labels[np.random.choice(\\n                            idx_flip,\\n                            count_joint[k_s][k_y],\\n                            replace=False,\\n                        )] = k_s\\n    '\n    true_labels = np.asarray(true_labels)\n    K = len(noise_matrix)\n    py = value_counts(true_labels) / float(len(true_labels))\n    count_joint = (noise_matrix * py * len(true_labels)).astype(int)\n    np.fill_diagonal(count_joint, 0)\n    labels = np.array(true_labels)\n    for k in range(K):\n        labels_per_class = np.where(count_joint[:, k] != 0)[0]\n        label_counts = count_joint[labels_per_class, k]\n        noise = [labels_per_class[i] for (i, c) in enumerate(label_counts) for z in range(c)]\n        idx_flip = np.where((labels == k) & (true_labels == k))[0]\n        if len(idx_flip) and len(noise) and (len(idx_flip) >= len(noise)):\n            labels[np.random.choice(idx_flip, len(noise), replace=False)] = noise\n    return labels",
            "def generate_noisy_labels(true_labels, noise_matrix) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generates noisy `labels` from perfect labels `true_labels`,\\n    \"exactly\" yielding the provided `noise_matrix` between `labels` and `true_labels`.\\n\\n    Below we provide a for loop implementation of what this function does.\\n    We do not use this implementation as it is not a fast algorithm, but\\n    it explains as Python pseudocode what is happening in this function.\\n\\n    Parameters\\n    ----------\\n    true_labels : np.ndarray\\n      An array of shape ``(N,)`` representing perfect labels, without any\\n      noise. Contains K distinct natural number classes, 0, 1, ..., K-1.\\n\\n    noise_matrix : np.ndarray\\n      An array of shape ``(K, K)`` representing the conditional probability\\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\\n      examples in every class, labeled as every other class. Assumes columns of\\n      `noise_matrix` sum to 1.\\n\\n    Returns\\n    -------\\n    labels : np.ndarray\\n      An array of shape ``(N,)`` of noisy labels.\\n\\n    Examples\\n    --------\\n\\n    .. code:: python\\n\\n        # Generate labels\\n        count_joint = (noise_matrix * py * len(y)).round().astype(int)\\n        labels = np.ndarray(y)\\n        for k_s in range(K):\\n            for k_y in range(K):\\n                if k_s != k_y:\\n                    idx_flip = np.where((labels==k_y)&(true_label==k_y))[0]\\n                    if len(idx_flip): # pragma: no cover\\n                        labels[np.random.choice(\\n                            idx_flip,\\n                            count_joint[k_s][k_y],\\n                            replace=False,\\n                        )] = k_s\\n    '\n    true_labels = np.asarray(true_labels)\n    K = len(noise_matrix)\n    py = value_counts(true_labels) / float(len(true_labels))\n    count_joint = (noise_matrix * py * len(true_labels)).astype(int)\n    np.fill_diagonal(count_joint, 0)\n    labels = np.array(true_labels)\n    for k in range(K):\n        labels_per_class = np.where(count_joint[:, k] != 0)[0]\n        label_counts = count_joint[labels_per_class, k]\n        noise = [labels_per_class[i] for (i, c) in enumerate(label_counts) for z in range(c)]\n        idx_flip = np.where((labels == k) & (true_labels == k))[0]\n        if len(idx_flip) and len(noise) and (len(idx_flip) >= len(noise)):\n            labels[np.random.choice(idx_flip, len(noise), replace=False)] = noise\n    return labels",
            "def generate_noisy_labels(true_labels, noise_matrix) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generates noisy `labels` from perfect labels `true_labels`,\\n    \"exactly\" yielding the provided `noise_matrix` between `labels` and `true_labels`.\\n\\n    Below we provide a for loop implementation of what this function does.\\n    We do not use this implementation as it is not a fast algorithm, but\\n    it explains as Python pseudocode what is happening in this function.\\n\\n    Parameters\\n    ----------\\n    true_labels : np.ndarray\\n      An array of shape ``(N,)`` representing perfect labels, without any\\n      noise. Contains K distinct natural number classes, 0, 1, ..., K-1.\\n\\n    noise_matrix : np.ndarray\\n      An array of shape ``(K, K)`` representing the conditional probability\\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\\n      examples in every class, labeled as every other class. Assumes columns of\\n      `noise_matrix` sum to 1.\\n\\n    Returns\\n    -------\\n    labels : np.ndarray\\n      An array of shape ``(N,)`` of noisy labels.\\n\\n    Examples\\n    --------\\n\\n    .. code:: python\\n\\n        # Generate labels\\n        count_joint = (noise_matrix * py * len(y)).round().astype(int)\\n        labels = np.ndarray(y)\\n        for k_s in range(K):\\n            for k_y in range(K):\\n                if k_s != k_y:\\n                    idx_flip = np.where((labels==k_y)&(true_label==k_y))[0]\\n                    if len(idx_flip): # pragma: no cover\\n                        labels[np.random.choice(\\n                            idx_flip,\\n                            count_joint[k_s][k_y],\\n                            replace=False,\\n                        )] = k_s\\n    '\n    true_labels = np.asarray(true_labels)\n    K = len(noise_matrix)\n    py = value_counts(true_labels) / float(len(true_labels))\n    count_joint = (noise_matrix * py * len(true_labels)).astype(int)\n    np.fill_diagonal(count_joint, 0)\n    labels = np.array(true_labels)\n    for k in range(K):\n        labels_per_class = np.where(count_joint[:, k] != 0)[0]\n        label_counts = count_joint[labels_per_class, k]\n        noise = [labels_per_class[i] for (i, c) in enumerate(label_counts) for z in range(c)]\n        idx_flip = np.where((labels == k) & (true_labels == k))[0]\n        if len(idx_flip) and len(noise) and (len(idx_flip) >= len(noise)):\n            labels[np.random.choice(idx_flip, len(noise), replace=False)] = noise\n    return labels",
            "def generate_noisy_labels(true_labels, noise_matrix) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generates noisy `labels` from perfect labels `true_labels`,\\n    \"exactly\" yielding the provided `noise_matrix` between `labels` and `true_labels`.\\n\\n    Below we provide a for loop implementation of what this function does.\\n    We do not use this implementation as it is not a fast algorithm, but\\n    it explains as Python pseudocode what is happening in this function.\\n\\n    Parameters\\n    ----------\\n    true_labels : np.ndarray\\n      An array of shape ``(N,)`` representing perfect labels, without any\\n      noise. Contains K distinct natural number classes, 0, 1, ..., K-1.\\n\\n    noise_matrix : np.ndarray\\n      An array of shape ``(K, K)`` representing the conditional probability\\n      matrix ``P(label=k_s|true_label=k_y)`` containing the fraction of\\n      examples in every class, labeled as every other class. Assumes columns of\\n      `noise_matrix` sum to 1.\\n\\n    Returns\\n    -------\\n    labels : np.ndarray\\n      An array of shape ``(N,)`` of noisy labels.\\n\\n    Examples\\n    --------\\n\\n    .. code:: python\\n\\n        # Generate labels\\n        count_joint = (noise_matrix * py * len(y)).round().astype(int)\\n        labels = np.ndarray(y)\\n        for k_s in range(K):\\n            for k_y in range(K):\\n                if k_s != k_y:\\n                    idx_flip = np.where((labels==k_y)&(true_label==k_y))[0]\\n                    if len(idx_flip): # pragma: no cover\\n                        labels[np.random.choice(\\n                            idx_flip,\\n                            count_joint[k_s][k_y],\\n                            replace=False,\\n                        )] = k_s\\n    '\n    true_labels = np.asarray(true_labels)\n    K = len(noise_matrix)\n    py = value_counts(true_labels) / float(len(true_labels))\n    count_joint = (noise_matrix * py * len(true_labels)).astype(int)\n    np.fill_diagonal(count_joint, 0)\n    labels = np.array(true_labels)\n    for k in range(K):\n        labels_per_class = np.where(count_joint[:, k] != 0)[0]\n        label_counts = count_joint[labels_per_class, k]\n        noise = [labels_per_class[i] for (i, c) in enumerate(label_counts) for z in range(c)]\n        idx_flip = np.where((labels == k) & (true_labels == k))[0]\n        if len(idx_flip) and len(noise) and (len(idx_flip) >= len(noise)):\n            labels[np.random.choice(idx_flip, len(noise), replace=False)] = noise\n    return labels"
        ]
    },
    {
        "func_name": "generate_noise_matrix_from_trace",
        "original": "def generate_noise_matrix_from_trace(K, trace, *, max_trace_prob=1.0, min_trace_prob=1e-05, max_noise_rate=1 - 1e-05, min_noise_rate=0.0, valid_noise_matrix=True, py=None, frac_zero_noise_rates=0.0, seed=0, max_iter=10000) -> Optional[np.ndarray]:\n    \"\"\"Generates a ``K x K`` noise matrix ``P(label=k_s|true_label=k_y)`` with\n    ``np.sum(np.diagonal(noise_matrix))`` equal to the given `trace`.\n\n    Parameters\n    ----------\n    K : int\n      Creates a noise matrix of shape ``(K, K)``. Implies there are\n      K classes for learning with noisy labels.\n\n    trace : float\n      Sum of diagonal entries of array of random probabilities returned.\n\n    max_trace_prob : float\n      Maximum probability of any entry in the trace of the return matrix.\n\n    min_trace_prob : float\n      Minimum probability of any entry in the trace of the return matrix.\n\n    max_noise_rate : float\n      Maximum noise_rate (non-diagonal entry) in the returned np.ndarray.\n\n    min_noise_rate : float\n      Minimum noise_rate (non-diagonal entry) in the returned np.ndarray.\n\n    valid_noise_matrix : bool, default=True\n      If ``True``, returns a matrix having all necessary conditions for\n      learning with noisy labels. In particular, ``p(true_label=k)p(label=k) < p(true_label=k,label=k)``\n      is satisfied. This requires that ``trace > 1``.\n\n    py : np.ndarray\n      An array of shape ``(K,)`` representing the fraction (prior probability) of each true class label, ``P(true_label = k)``.\n      This argument is **required** when ``valid_noise_matrix=True``.\n\n    frac_zero_noise_rates : float\n      The fraction of the ``n*(n-1)`` noise rates\n      that will be set to 0. Note that if you set a high trace, it may be\n      impossible to also have a low fraction of zero noise rates without\n      forcing all non-1 diagonal values. Instead, when this happens we only\n      guarantee to produce a noise matrix with `frac_zero_noise_rates` *or\n      higher*. The opposite occurs with a small trace.\n\n    seed : int\n      Seeds the random number generator for numpy.\n\n    max_iter : int, default=10000\n      The max number of tries to produce a valid matrix before returning ``None``.\n\n    Returns\n    -------\n    noise_matrix : np.ndarray or None\n      An array of shape ``(K, K)`` representing the noise matrix ``P(label=k_s|true_label=k_y)`` with `trace`\n      equal to ``np.sum(np.diagonal(noise_matrix))``. This a conditional probability matrix and a\n      left stochastic matrix. Returns ``None`` if `max_iter` is exceeded.\n    \"\"\"\n    if valid_noise_matrix and trace <= 1:\n        raise ValueError('trace = {}. trace > 1 is necessary for a'.format(trace) + ' valid noise matrix to be returned (valid_noise_matrix == True)')\n    if valid_noise_matrix and py is None and (K > 2):\n        raise ValueError('py must be provided (not None) if the input parameter' + ' valid_noise_matrix == True')\n    if K <= 1:\n        raise ValueError('K must be >= 2, but K = {}.'.format(K))\n    if max_iter < 1:\n        return None\n    np.random.seed(seed)\n    if K == 2:\n        if frac_zero_noise_rates >= 0.5:\n            noise_mat = np.array([[1.0, 1 - (trace - 1.0)], [0.0, trace - 1.0]])\n            return noise_mat if np.random.rand() > 0.5 else np.rot90(noise_mat, k=2)\n        else:\n            diag = generate_n_rand_probabilities_that_sum_to_m(2, trace)\n            noise_matrix = np.array([[diag[0], 1 - diag[1]], [1 - diag[0], diag[1]]])\n            return noise_matrix\n    for z in range(max_iter):\n        noise_matrix = np.zeros(shape=(K, K))\n        nm_diagonal = generate_n_rand_probabilities_that_sum_to_m(n=K, m=trace, max_prob=max_trace_prob, min_prob=min_trace_prob)\n        np.fill_diagonal(noise_matrix, nm_diagonal)\n        num_col_with_noise = K - np.count_nonzero(1 == nm_diagonal)\n        num_zero_noise_rates = int(K * (K - 1) * frac_zero_noise_rates)\n        num_zero_noise_rates -= (K - num_col_with_noise) * (K - 1)\n        num_zero_noise_rates = np.maximum(num_zero_noise_rates, 0)\n        num_zero_noise_rates_per_col = randomly_distribute_N_balls_into_K_bins(N=num_zero_noise_rates, K=num_col_with_noise, max_balls_per_bin=K - 2, min_balls_per_bin=0) if K > 2 else np.array([0, 0])\n        stack_nonzero_noise_rates_per_col = list(K - 1 - num_zero_noise_rates_per_col)[::-1]\n        for col in np.arange(K)[nm_diagonal != 1]:\n            num_noise = stack_nonzero_noise_rates_per_col.pop()\n            noise_rates_col = list(generate_n_rand_probabilities_that_sum_to_m(n=num_noise, m=1 - nm_diagonal[col], max_prob=max_noise_rate, min_prob=min_noise_rate))\n            rows = np.random.choice([row for row in range(K) if row != col], num_noise, replace=False)\n            for row in rows:\n                noise_matrix[row][col] = noise_rates_col.pop()\n        if not valid_noise_matrix or noise_matrix_is_valid(noise_matrix, py):\n            return noise_matrix\n    return None",
        "mutated": [
            "def generate_noise_matrix_from_trace(K, trace, *, max_trace_prob=1.0, min_trace_prob=1e-05, max_noise_rate=1 - 1e-05, min_noise_rate=0.0, valid_noise_matrix=True, py=None, frac_zero_noise_rates=0.0, seed=0, max_iter=10000) -> Optional[np.ndarray]:\n    if False:\n        i = 10\n    'Generates a ``K x K`` noise matrix ``P(label=k_s|true_label=k_y)`` with\\n    ``np.sum(np.diagonal(noise_matrix))`` equal to the given `trace`.\\n\\n    Parameters\\n    ----------\\n    K : int\\n      Creates a noise matrix of shape ``(K, K)``. Implies there are\\n      K classes for learning with noisy labels.\\n\\n    trace : float\\n      Sum of diagonal entries of array of random probabilities returned.\\n\\n    max_trace_prob : float\\n      Maximum probability of any entry in the trace of the return matrix.\\n\\n    min_trace_prob : float\\n      Minimum probability of any entry in the trace of the return matrix.\\n\\n    max_noise_rate : float\\n      Maximum noise_rate (non-diagonal entry) in the returned np.ndarray.\\n\\n    min_noise_rate : float\\n      Minimum noise_rate (non-diagonal entry) in the returned np.ndarray.\\n\\n    valid_noise_matrix : bool, default=True\\n      If ``True``, returns a matrix having all necessary conditions for\\n      learning with noisy labels. In particular, ``p(true_label=k)p(label=k) < p(true_label=k,label=k)``\\n      is satisfied. This requires that ``trace > 1``.\\n\\n    py : np.ndarray\\n      An array of shape ``(K,)`` representing the fraction (prior probability) of each true class label, ``P(true_label = k)``.\\n      This argument is **required** when ``valid_noise_matrix=True``.\\n\\n    frac_zero_noise_rates : float\\n      The fraction of the ``n*(n-1)`` noise rates\\n      that will be set to 0. Note that if you set a high trace, it may be\\n      impossible to also have a low fraction of zero noise rates without\\n      forcing all non-1 diagonal values. Instead, when this happens we only\\n      guarantee to produce a noise matrix with `frac_zero_noise_rates` *or\\n      higher*. The opposite occurs with a small trace.\\n\\n    seed : int\\n      Seeds the random number generator for numpy.\\n\\n    max_iter : int, default=10000\\n      The max number of tries to produce a valid matrix before returning ``None``.\\n\\n    Returns\\n    -------\\n    noise_matrix : np.ndarray or None\\n      An array of shape ``(K, K)`` representing the noise matrix ``P(label=k_s|true_label=k_y)`` with `trace`\\n      equal to ``np.sum(np.diagonal(noise_matrix))``. This a conditional probability matrix and a\\n      left stochastic matrix. Returns ``None`` if `max_iter` is exceeded.\\n    '\n    if valid_noise_matrix and trace <= 1:\n        raise ValueError('trace = {}. trace > 1 is necessary for a'.format(trace) + ' valid noise matrix to be returned (valid_noise_matrix == True)')\n    if valid_noise_matrix and py is None and (K > 2):\n        raise ValueError('py must be provided (not None) if the input parameter' + ' valid_noise_matrix == True')\n    if K <= 1:\n        raise ValueError('K must be >= 2, but K = {}.'.format(K))\n    if max_iter < 1:\n        return None\n    np.random.seed(seed)\n    if K == 2:\n        if frac_zero_noise_rates >= 0.5:\n            noise_mat = np.array([[1.0, 1 - (trace - 1.0)], [0.0, trace - 1.0]])\n            return noise_mat if np.random.rand() > 0.5 else np.rot90(noise_mat, k=2)\n        else:\n            diag = generate_n_rand_probabilities_that_sum_to_m(2, trace)\n            noise_matrix = np.array([[diag[0], 1 - diag[1]], [1 - diag[0], diag[1]]])\n            return noise_matrix\n    for z in range(max_iter):\n        noise_matrix = np.zeros(shape=(K, K))\n        nm_diagonal = generate_n_rand_probabilities_that_sum_to_m(n=K, m=trace, max_prob=max_trace_prob, min_prob=min_trace_prob)\n        np.fill_diagonal(noise_matrix, nm_diagonal)\n        num_col_with_noise = K - np.count_nonzero(1 == nm_diagonal)\n        num_zero_noise_rates = int(K * (K - 1) * frac_zero_noise_rates)\n        num_zero_noise_rates -= (K - num_col_with_noise) * (K - 1)\n        num_zero_noise_rates = np.maximum(num_zero_noise_rates, 0)\n        num_zero_noise_rates_per_col = randomly_distribute_N_balls_into_K_bins(N=num_zero_noise_rates, K=num_col_with_noise, max_balls_per_bin=K - 2, min_balls_per_bin=0) if K > 2 else np.array([0, 0])\n        stack_nonzero_noise_rates_per_col = list(K - 1 - num_zero_noise_rates_per_col)[::-1]\n        for col in np.arange(K)[nm_diagonal != 1]:\n            num_noise = stack_nonzero_noise_rates_per_col.pop()\n            noise_rates_col = list(generate_n_rand_probabilities_that_sum_to_m(n=num_noise, m=1 - nm_diagonal[col], max_prob=max_noise_rate, min_prob=min_noise_rate))\n            rows = np.random.choice([row for row in range(K) if row != col], num_noise, replace=False)\n            for row in rows:\n                noise_matrix[row][col] = noise_rates_col.pop()\n        if not valid_noise_matrix or noise_matrix_is_valid(noise_matrix, py):\n            return noise_matrix\n    return None",
            "def generate_noise_matrix_from_trace(K, trace, *, max_trace_prob=1.0, min_trace_prob=1e-05, max_noise_rate=1 - 1e-05, min_noise_rate=0.0, valid_noise_matrix=True, py=None, frac_zero_noise_rates=0.0, seed=0, max_iter=10000) -> Optional[np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generates a ``K x K`` noise matrix ``P(label=k_s|true_label=k_y)`` with\\n    ``np.sum(np.diagonal(noise_matrix))`` equal to the given `trace`.\\n\\n    Parameters\\n    ----------\\n    K : int\\n      Creates a noise matrix of shape ``(K, K)``. Implies there are\\n      K classes for learning with noisy labels.\\n\\n    trace : float\\n      Sum of diagonal entries of array of random probabilities returned.\\n\\n    max_trace_prob : float\\n      Maximum probability of any entry in the trace of the return matrix.\\n\\n    min_trace_prob : float\\n      Minimum probability of any entry in the trace of the return matrix.\\n\\n    max_noise_rate : float\\n      Maximum noise_rate (non-diagonal entry) in the returned np.ndarray.\\n\\n    min_noise_rate : float\\n      Minimum noise_rate (non-diagonal entry) in the returned np.ndarray.\\n\\n    valid_noise_matrix : bool, default=True\\n      If ``True``, returns a matrix having all necessary conditions for\\n      learning with noisy labels. In particular, ``p(true_label=k)p(label=k) < p(true_label=k,label=k)``\\n      is satisfied. This requires that ``trace > 1``.\\n\\n    py : np.ndarray\\n      An array of shape ``(K,)`` representing the fraction (prior probability) of each true class label, ``P(true_label = k)``.\\n      This argument is **required** when ``valid_noise_matrix=True``.\\n\\n    frac_zero_noise_rates : float\\n      The fraction of the ``n*(n-1)`` noise rates\\n      that will be set to 0. Note that if you set a high trace, it may be\\n      impossible to also have a low fraction of zero noise rates without\\n      forcing all non-1 diagonal values. Instead, when this happens we only\\n      guarantee to produce a noise matrix with `frac_zero_noise_rates` *or\\n      higher*. The opposite occurs with a small trace.\\n\\n    seed : int\\n      Seeds the random number generator for numpy.\\n\\n    max_iter : int, default=10000\\n      The max number of tries to produce a valid matrix before returning ``None``.\\n\\n    Returns\\n    -------\\n    noise_matrix : np.ndarray or None\\n      An array of shape ``(K, K)`` representing the noise matrix ``P(label=k_s|true_label=k_y)`` with `trace`\\n      equal to ``np.sum(np.diagonal(noise_matrix))``. This a conditional probability matrix and a\\n      left stochastic matrix. Returns ``None`` if `max_iter` is exceeded.\\n    '\n    if valid_noise_matrix and trace <= 1:\n        raise ValueError('trace = {}. trace > 1 is necessary for a'.format(trace) + ' valid noise matrix to be returned (valid_noise_matrix == True)')\n    if valid_noise_matrix and py is None and (K > 2):\n        raise ValueError('py must be provided (not None) if the input parameter' + ' valid_noise_matrix == True')\n    if K <= 1:\n        raise ValueError('K must be >= 2, but K = {}.'.format(K))\n    if max_iter < 1:\n        return None\n    np.random.seed(seed)\n    if K == 2:\n        if frac_zero_noise_rates >= 0.5:\n            noise_mat = np.array([[1.0, 1 - (trace - 1.0)], [0.0, trace - 1.0]])\n            return noise_mat if np.random.rand() > 0.5 else np.rot90(noise_mat, k=2)\n        else:\n            diag = generate_n_rand_probabilities_that_sum_to_m(2, trace)\n            noise_matrix = np.array([[diag[0], 1 - diag[1]], [1 - diag[0], diag[1]]])\n            return noise_matrix\n    for z in range(max_iter):\n        noise_matrix = np.zeros(shape=(K, K))\n        nm_diagonal = generate_n_rand_probabilities_that_sum_to_m(n=K, m=trace, max_prob=max_trace_prob, min_prob=min_trace_prob)\n        np.fill_diagonal(noise_matrix, nm_diagonal)\n        num_col_with_noise = K - np.count_nonzero(1 == nm_diagonal)\n        num_zero_noise_rates = int(K * (K - 1) * frac_zero_noise_rates)\n        num_zero_noise_rates -= (K - num_col_with_noise) * (K - 1)\n        num_zero_noise_rates = np.maximum(num_zero_noise_rates, 0)\n        num_zero_noise_rates_per_col = randomly_distribute_N_balls_into_K_bins(N=num_zero_noise_rates, K=num_col_with_noise, max_balls_per_bin=K - 2, min_balls_per_bin=0) if K > 2 else np.array([0, 0])\n        stack_nonzero_noise_rates_per_col = list(K - 1 - num_zero_noise_rates_per_col)[::-1]\n        for col in np.arange(K)[nm_diagonal != 1]:\n            num_noise = stack_nonzero_noise_rates_per_col.pop()\n            noise_rates_col = list(generate_n_rand_probabilities_that_sum_to_m(n=num_noise, m=1 - nm_diagonal[col], max_prob=max_noise_rate, min_prob=min_noise_rate))\n            rows = np.random.choice([row for row in range(K) if row != col], num_noise, replace=False)\n            for row in rows:\n                noise_matrix[row][col] = noise_rates_col.pop()\n        if not valid_noise_matrix or noise_matrix_is_valid(noise_matrix, py):\n            return noise_matrix\n    return None",
            "def generate_noise_matrix_from_trace(K, trace, *, max_trace_prob=1.0, min_trace_prob=1e-05, max_noise_rate=1 - 1e-05, min_noise_rate=0.0, valid_noise_matrix=True, py=None, frac_zero_noise_rates=0.0, seed=0, max_iter=10000) -> Optional[np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generates a ``K x K`` noise matrix ``P(label=k_s|true_label=k_y)`` with\\n    ``np.sum(np.diagonal(noise_matrix))`` equal to the given `trace`.\\n\\n    Parameters\\n    ----------\\n    K : int\\n      Creates a noise matrix of shape ``(K, K)``. Implies there are\\n      K classes for learning with noisy labels.\\n\\n    trace : float\\n      Sum of diagonal entries of array of random probabilities returned.\\n\\n    max_trace_prob : float\\n      Maximum probability of any entry in the trace of the return matrix.\\n\\n    min_trace_prob : float\\n      Minimum probability of any entry in the trace of the return matrix.\\n\\n    max_noise_rate : float\\n      Maximum noise_rate (non-diagonal entry) in the returned np.ndarray.\\n\\n    min_noise_rate : float\\n      Minimum noise_rate (non-diagonal entry) in the returned np.ndarray.\\n\\n    valid_noise_matrix : bool, default=True\\n      If ``True``, returns a matrix having all necessary conditions for\\n      learning with noisy labels. In particular, ``p(true_label=k)p(label=k) < p(true_label=k,label=k)``\\n      is satisfied. This requires that ``trace > 1``.\\n\\n    py : np.ndarray\\n      An array of shape ``(K,)`` representing the fraction (prior probability) of each true class label, ``P(true_label = k)``.\\n      This argument is **required** when ``valid_noise_matrix=True``.\\n\\n    frac_zero_noise_rates : float\\n      The fraction of the ``n*(n-1)`` noise rates\\n      that will be set to 0. Note that if you set a high trace, it may be\\n      impossible to also have a low fraction of zero noise rates without\\n      forcing all non-1 diagonal values. Instead, when this happens we only\\n      guarantee to produce a noise matrix with `frac_zero_noise_rates` *or\\n      higher*. The opposite occurs with a small trace.\\n\\n    seed : int\\n      Seeds the random number generator for numpy.\\n\\n    max_iter : int, default=10000\\n      The max number of tries to produce a valid matrix before returning ``None``.\\n\\n    Returns\\n    -------\\n    noise_matrix : np.ndarray or None\\n      An array of shape ``(K, K)`` representing the noise matrix ``P(label=k_s|true_label=k_y)`` with `trace`\\n      equal to ``np.sum(np.diagonal(noise_matrix))``. This a conditional probability matrix and a\\n      left stochastic matrix. Returns ``None`` if `max_iter` is exceeded.\\n    '\n    if valid_noise_matrix and trace <= 1:\n        raise ValueError('trace = {}. trace > 1 is necessary for a'.format(trace) + ' valid noise matrix to be returned (valid_noise_matrix == True)')\n    if valid_noise_matrix and py is None and (K > 2):\n        raise ValueError('py must be provided (not None) if the input parameter' + ' valid_noise_matrix == True')\n    if K <= 1:\n        raise ValueError('K must be >= 2, but K = {}.'.format(K))\n    if max_iter < 1:\n        return None\n    np.random.seed(seed)\n    if K == 2:\n        if frac_zero_noise_rates >= 0.5:\n            noise_mat = np.array([[1.0, 1 - (trace - 1.0)], [0.0, trace - 1.0]])\n            return noise_mat if np.random.rand() > 0.5 else np.rot90(noise_mat, k=2)\n        else:\n            diag = generate_n_rand_probabilities_that_sum_to_m(2, trace)\n            noise_matrix = np.array([[diag[0], 1 - diag[1]], [1 - diag[0], diag[1]]])\n            return noise_matrix\n    for z in range(max_iter):\n        noise_matrix = np.zeros(shape=(K, K))\n        nm_diagonal = generate_n_rand_probabilities_that_sum_to_m(n=K, m=trace, max_prob=max_trace_prob, min_prob=min_trace_prob)\n        np.fill_diagonal(noise_matrix, nm_diagonal)\n        num_col_with_noise = K - np.count_nonzero(1 == nm_diagonal)\n        num_zero_noise_rates = int(K * (K - 1) * frac_zero_noise_rates)\n        num_zero_noise_rates -= (K - num_col_with_noise) * (K - 1)\n        num_zero_noise_rates = np.maximum(num_zero_noise_rates, 0)\n        num_zero_noise_rates_per_col = randomly_distribute_N_balls_into_K_bins(N=num_zero_noise_rates, K=num_col_with_noise, max_balls_per_bin=K - 2, min_balls_per_bin=0) if K > 2 else np.array([0, 0])\n        stack_nonzero_noise_rates_per_col = list(K - 1 - num_zero_noise_rates_per_col)[::-1]\n        for col in np.arange(K)[nm_diagonal != 1]:\n            num_noise = stack_nonzero_noise_rates_per_col.pop()\n            noise_rates_col = list(generate_n_rand_probabilities_that_sum_to_m(n=num_noise, m=1 - nm_diagonal[col], max_prob=max_noise_rate, min_prob=min_noise_rate))\n            rows = np.random.choice([row for row in range(K) if row != col], num_noise, replace=False)\n            for row in rows:\n                noise_matrix[row][col] = noise_rates_col.pop()\n        if not valid_noise_matrix or noise_matrix_is_valid(noise_matrix, py):\n            return noise_matrix\n    return None",
            "def generate_noise_matrix_from_trace(K, trace, *, max_trace_prob=1.0, min_trace_prob=1e-05, max_noise_rate=1 - 1e-05, min_noise_rate=0.0, valid_noise_matrix=True, py=None, frac_zero_noise_rates=0.0, seed=0, max_iter=10000) -> Optional[np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generates a ``K x K`` noise matrix ``P(label=k_s|true_label=k_y)`` with\\n    ``np.sum(np.diagonal(noise_matrix))`` equal to the given `trace`.\\n\\n    Parameters\\n    ----------\\n    K : int\\n      Creates a noise matrix of shape ``(K, K)``. Implies there are\\n      K classes for learning with noisy labels.\\n\\n    trace : float\\n      Sum of diagonal entries of array of random probabilities returned.\\n\\n    max_trace_prob : float\\n      Maximum probability of any entry in the trace of the return matrix.\\n\\n    min_trace_prob : float\\n      Minimum probability of any entry in the trace of the return matrix.\\n\\n    max_noise_rate : float\\n      Maximum noise_rate (non-diagonal entry) in the returned np.ndarray.\\n\\n    min_noise_rate : float\\n      Minimum noise_rate (non-diagonal entry) in the returned np.ndarray.\\n\\n    valid_noise_matrix : bool, default=True\\n      If ``True``, returns a matrix having all necessary conditions for\\n      learning with noisy labels. In particular, ``p(true_label=k)p(label=k) < p(true_label=k,label=k)``\\n      is satisfied. This requires that ``trace > 1``.\\n\\n    py : np.ndarray\\n      An array of shape ``(K,)`` representing the fraction (prior probability) of each true class label, ``P(true_label = k)``.\\n      This argument is **required** when ``valid_noise_matrix=True``.\\n\\n    frac_zero_noise_rates : float\\n      The fraction of the ``n*(n-1)`` noise rates\\n      that will be set to 0. Note that if you set a high trace, it may be\\n      impossible to also have a low fraction of zero noise rates without\\n      forcing all non-1 diagonal values. Instead, when this happens we only\\n      guarantee to produce a noise matrix with `frac_zero_noise_rates` *or\\n      higher*. The opposite occurs with a small trace.\\n\\n    seed : int\\n      Seeds the random number generator for numpy.\\n\\n    max_iter : int, default=10000\\n      The max number of tries to produce a valid matrix before returning ``None``.\\n\\n    Returns\\n    -------\\n    noise_matrix : np.ndarray or None\\n      An array of shape ``(K, K)`` representing the noise matrix ``P(label=k_s|true_label=k_y)`` with `trace`\\n      equal to ``np.sum(np.diagonal(noise_matrix))``. This a conditional probability matrix and a\\n      left stochastic matrix. Returns ``None`` if `max_iter` is exceeded.\\n    '\n    if valid_noise_matrix and trace <= 1:\n        raise ValueError('trace = {}. trace > 1 is necessary for a'.format(trace) + ' valid noise matrix to be returned (valid_noise_matrix == True)')\n    if valid_noise_matrix and py is None and (K > 2):\n        raise ValueError('py must be provided (not None) if the input parameter' + ' valid_noise_matrix == True')\n    if K <= 1:\n        raise ValueError('K must be >= 2, but K = {}.'.format(K))\n    if max_iter < 1:\n        return None\n    np.random.seed(seed)\n    if K == 2:\n        if frac_zero_noise_rates >= 0.5:\n            noise_mat = np.array([[1.0, 1 - (trace - 1.0)], [0.0, trace - 1.0]])\n            return noise_mat if np.random.rand() > 0.5 else np.rot90(noise_mat, k=2)\n        else:\n            diag = generate_n_rand_probabilities_that_sum_to_m(2, trace)\n            noise_matrix = np.array([[diag[0], 1 - diag[1]], [1 - diag[0], diag[1]]])\n            return noise_matrix\n    for z in range(max_iter):\n        noise_matrix = np.zeros(shape=(K, K))\n        nm_diagonal = generate_n_rand_probabilities_that_sum_to_m(n=K, m=trace, max_prob=max_trace_prob, min_prob=min_trace_prob)\n        np.fill_diagonal(noise_matrix, nm_diagonal)\n        num_col_with_noise = K - np.count_nonzero(1 == nm_diagonal)\n        num_zero_noise_rates = int(K * (K - 1) * frac_zero_noise_rates)\n        num_zero_noise_rates -= (K - num_col_with_noise) * (K - 1)\n        num_zero_noise_rates = np.maximum(num_zero_noise_rates, 0)\n        num_zero_noise_rates_per_col = randomly_distribute_N_balls_into_K_bins(N=num_zero_noise_rates, K=num_col_with_noise, max_balls_per_bin=K - 2, min_balls_per_bin=0) if K > 2 else np.array([0, 0])\n        stack_nonzero_noise_rates_per_col = list(K - 1 - num_zero_noise_rates_per_col)[::-1]\n        for col in np.arange(K)[nm_diagonal != 1]:\n            num_noise = stack_nonzero_noise_rates_per_col.pop()\n            noise_rates_col = list(generate_n_rand_probabilities_that_sum_to_m(n=num_noise, m=1 - nm_diagonal[col], max_prob=max_noise_rate, min_prob=min_noise_rate))\n            rows = np.random.choice([row for row in range(K) if row != col], num_noise, replace=False)\n            for row in rows:\n                noise_matrix[row][col] = noise_rates_col.pop()\n        if not valid_noise_matrix or noise_matrix_is_valid(noise_matrix, py):\n            return noise_matrix\n    return None",
            "def generate_noise_matrix_from_trace(K, trace, *, max_trace_prob=1.0, min_trace_prob=1e-05, max_noise_rate=1 - 1e-05, min_noise_rate=0.0, valid_noise_matrix=True, py=None, frac_zero_noise_rates=0.0, seed=0, max_iter=10000) -> Optional[np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generates a ``K x K`` noise matrix ``P(label=k_s|true_label=k_y)`` with\\n    ``np.sum(np.diagonal(noise_matrix))`` equal to the given `trace`.\\n\\n    Parameters\\n    ----------\\n    K : int\\n      Creates a noise matrix of shape ``(K, K)``. Implies there are\\n      K classes for learning with noisy labels.\\n\\n    trace : float\\n      Sum of diagonal entries of array of random probabilities returned.\\n\\n    max_trace_prob : float\\n      Maximum probability of any entry in the trace of the return matrix.\\n\\n    min_trace_prob : float\\n      Minimum probability of any entry in the trace of the return matrix.\\n\\n    max_noise_rate : float\\n      Maximum noise_rate (non-diagonal entry) in the returned np.ndarray.\\n\\n    min_noise_rate : float\\n      Minimum noise_rate (non-diagonal entry) in the returned np.ndarray.\\n\\n    valid_noise_matrix : bool, default=True\\n      If ``True``, returns a matrix having all necessary conditions for\\n      learning with noisy labels. In particular, ``p(true_label=k)p(label=k) < p(true_label=k,label=k)``\\n      is satisfied. This requires that ``trace > 1``.\\n\\n    py : np.ndarray\\n      An array of shape ``(K,)`` representing the fraction (prior probability) of each true class label, ``P(true_label = k)``.\\n      This argument is **required** when ``valid_noise_matrix=True``.\\n\\n    frac_zero_noise_rates : float\\n      The fraction of the ``n*(n-1)`` noise rates\\n      that will be set to 0. Note that if you set a high trace, it may be\\n      impossible to also have a low fraction of zero noise rates without\\n      forcing all non-1 diagonal values. Instead, when this happens we only\\n      guarantee to produce a noise matrix with `frac_zero_noise_rates` *or\\n      higher*. The opposite occurs with a small trace.\\n\\n    seed : int\\n      Seeds the random number generator for numpy.\\n\\n    max_iter : int, default=10000\\n      The max number of tries to produce a valid matrix before returning ``None``.\\n\\n    Returns\\n    -------\\n    noise_matrix : np.ndarray or None\\n      An array of shape ``(K, K)`` representing the noise matrix ``P(label=k_s|true_label=k_y)`` with `trace`\\n      equal to ``np.sum(np.diagonal(noise_matrix))``. This a conditional probability matrix and a\\n      left stochastic matrix. Returns ``None`` if `max_iter` is exceeded.\\n    '\n    if valid_noise_matrix and trace <= 1:\n        raise ValueError('trace = {}. trace > 1 is necessary for a'.format(trace) + ' valid noise matrix to be returned (valid_noise_matrix == True)')\n    if valid_noise_matrix and py is None and (K > 2):\n        raise ValueError('py must be provided (not None) if the input parameter' + ' valid_noise_matrix == True')\n    if K <= 1:\n        raise ValueError('K must be >= 2, but K = {}.'.format(K))\n    if max_iter < 1:\n        return None\n    np.random.seed(seed)\n    if K == 2:\n        if frac_zero_noise_rates >= 0.5:\n            noise_mat = np.array([[1.0, 1 - (trace - 1.0)], [0.0, trace - 1.0]])\n            return noise_mat if np.random.rand() > 0.5 else np.rot90(noise_mat, k=2)\n        else:\n            diag = generate_n_rand_probabilities_that_sum_to_m(2, trace)\n            noise_matrix = np.array([[diag[0], 1 - diag[1]], [1 - diag[0], diag[1]]])\n            return noise_matrix\n    for z in range(max_iter):\n        noise_matrix = np.zeros(shape=(K, K))\n        nm_diagonal = generate_n_rand_probabilities_that_sum_to_m(n=K, m=trace, max_prob=max_trace_prob, min_prob=min_trace_prob)\n        np.fill_diagonal(noise_matrix, nm_diagonal)\n        num_col_with_noise = K - np.count_nonzero(1 == nm_diagonal)\n        num_zero_noise_rates = int(K * (K - 1) * frac_zero_noise_rates)\n        num_zero_noise_rates -= (K - num_col_with_noise) * (K - 1)\n        num_zero_noise_rates = np.maximum(num_zero_noise_rates, 0)\n        num_zero_noise_rates_per_col = randomly_distribute_N_balls_into_K_bins(N=num_zero_noise_rates, K=num_col_with_noise, max_balls_per_bin=K - 2, min_balls_per_bin=0) if K > 2 else np.array([0, 0])\n        stack_nonzero_noise_rates_per_col = list(K - 1 - num_zero_noise_rates_per_col)[::-1]\n        for col in np.arange(K)[nm_diagonal != 1]:\n            num_noise = stack_nonzero_noise_rates_per_col.pop()\n            noise_rates_col = list(generate_n_rand_probabilities_that_sum_to_m(n=num_noise, m=1 - nm_diagonal[col], max_prob=max_noise_rate, min_prob=min_noise_rate))\n            rows = np.random.choice([row for row in range(K) if row != col], num_noise, replace=False)\n            for row in rows:\n                noise_matrix[row][col] = noise_rates_col.pop()\n        if not valid_noise_matrix or noise_matrix_is_valid(noise_matrix, py):\n            return noise_matrix\n    return None"
        ]
    },
    {
        "func_name": "generate_n_rand_probabilities_that_sum_to_m",
        "original": "def generate_n_rand_probabilities_that_sum_to_m(n, m, *, max_prob=1.0, min_prob=0.0) -> np.ndarray:\n    \"\"\"\n    Generates `n` random probabilities that sum to `m`.\n\n    When ``min_prob=0`` and ``max_prob = 1.0``, use\n    ``np.random.dirichlet(np.ones(n))*m`` instead.\n\n    Parameters\n    ----------\n    n : int\n      Length of array of random probabilities to be returned.\n\n    m : float\n      Sum of array of random probabilities that is returned.\n\n    max_prob : float, default=1.0\n      Maximum probability of any entry in the returned array. Must be between 0 and 1.\n\n    min_prob : float, default=0.0\n      Minimum probability of any entry in the returned array. Must be between 0 and 1.\n\n    Returns\n    -------\n    probabilities : np.ndarray\n      An array of probabilities.\n    \"\"\"\n    if n == 0:\n        return np.array([])\n    if max_prob + FLOATING_POINT_COMPARISON < m / float(n):\n        raise ValueError('max_prob must be greater or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    if min_prob > (m + FLOATING_POINT_COMPARISON) / float(n):\n        raise ValueError('min_prob must be less or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    result = np.random.dirichlet(np.ones(n)) * m\n    min_val = min(result)\n    max_val = max(result)\n    while max_val > max_prob + FLOATING_POINT_COMPARISON:\n        new_min = min_val + (max_val - max_prob)\n        adjustment = (max_prob - new_min) * np.random.rand()\n        result[np.argmin(result)] = new_min + adjustment\n        result[np.argmax(result)] = max_prob - adjustment\n        min_val = min(result)\n        max_val = max(result)\n    min_val = min(result)\n    max_val = max(result)\n    while min_val < min_prob - FLOATING_POINT_COMPARISON:\n        min_val = min(result)\n        max_val = max(result)\n        new_max = max_val - (min_prob - min_val)\n        adjustment = (new_max - min_prob) * np.random.rand()\n        result[np.argmax(result)] = new_max - adjustment\n        result[np.argmin(result)] = min_prob + adjustment\n        min_val = min(result)\n        max_val = max(result)\n    return result",
        "mutated": [
            "def generate_n_rand_probabilities_that_sum_to_m(n, m, *, max_prob=1.0, min_prob=0.0) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n    Generates `n` random probabilities that sum to `m`.\\n\\n    When ``min_prob=0`` and ``max_prob = 1.0``, use\\n    ``np.random.dirichlet(np.ones(n))*m`` instead.\\n\\n    Parameters\\n    ----------\\n    n : int\\n      Length of array of random probabilities to be returned.\\n\\n    m : float\\n      Sum of array of random probabilities that is returned.\\n\\n    max_prob : float, default=1.0\\n      Maximum probability of any entry in the returned array. Must be between 0 and 1.\\n\\n    min_prob : float, default=0.0\\n      Minimum probability of any entry in the returned array. Must be between 0 and 1.\\n\\n    Returns\\n    -------\\n    probabilities : np.ndarray\\n      An array of probabilities.\\n    '\n    if n == 0:\n        return np.array([])\n    if max_prob + FLOATING_POINT_COMPARISON < m / float(n):\n        raise ValueError('max_prob must be greater or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    if min_prob > (m + FLOATING_POINT_COMPARISON) / float(n):\n        raise ValueError('min_prob must be less or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    result = np.random.dirichlet(np.ones(n)) * m\n    min_val = min(result)\n    max_val = max(result)\n    while max_val > max_prob + FLOATING_POINT_COMPARISON:\n        new_min = min_val + (max_val - max_prob)\n        adjustment = (max_prob - new_min) * np.random.rand()\n        result[np.argmin(result)] = new_min + adjustment\n        result[np.argmax(result)] = max_prob - adjustment\n        min_val = min(result)\n        max_val = max(result)\n    min_val = min(result)\n    max_val = max(result)\n    while min_val < min_prob - FLOATING_POINT_COMPARISON:\n        min_val = min(result)\n        max_val = max(result)\n        new_max = max_val - (min_prob - min_val)\n        adjustment = (new_max - min_prob) * np.random.rand()\n        result[np.argmax(result)] = new_max - adjustment\n        result[np.argmin(result)] = min_prob + adjustment\n        min_val = min(result)\n        max_val = max(result)\n    return result",
            "def generate_n_rand_probabilities_that_sum_to_m(n, m, *, max_prob=1.0, min_prob=0.0) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Generates `n` random probabilities that sum to `m`.\\n\\n    When ``min_prob=0`` and ``max_prob = 1.0``, use\\n    ``np.random.dirichlet(np.ones(n))*m`` instead.\\n\\n    Parameters\\n    ----------\\n    n : int\\n      Length of array of random probabilities to be returned.\\n\\n    m : float\\n      Sum of array of random probabilities that is returned.\\n\\n    max_prob : float, default=1.0\\n      Maximum probability of any entry in the returned array. Must be between 0 and 1.\\n\\n    min_prob : float, default=0.0\\n      Minimum probability of any entry in the returned array. Must be between 0 and 1.\\n\\n    Returns\\n    -------\\n    probabilities : np.ndarray\\n      An array of probabilities.\\n    '\n    if n == 0:\n        return np.array([])\n    if max_prob + FLOATING_POINT_COMPARISON < m / float(n):\n        raise ValueError('max_prob must be greater or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    if min_prob > (m + FLOATING_POINT_COMPARISON) / float(n):\n        raise ValueError('min_prob must be less or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    result = np.random.dirichlet(np.ones(n)) * m\n    min_val = min(result)\n    max_val = max(result)\n    while max_val > max_prob + FLOATING_POINT_COMPARISON:\n        new_min = min_val + (max_val - max_prob)\n        adjustment = (max_prob - new_min) * np.random.rand()\n        result[np.argmin(result)] = new_min + adjustment\n        result[np.argmax(result)] = max_prob - adjustment\n        min_val = min(result)\n        max_val = max(result)\n    min_val = min(result)\n    max_val = max(result)\n    while min_val < min_prob - FLOATING_POINT_COMPARISON:\n        min_val = min(result)\n        max_val = max(result)\n        new_max = max_val - (min_prob - min_val)\n        adjustment = (new_max - min_prob) * np.random.rand()\n        result[np.argmax(result)] = new_max - adjustment\n        result[np.argmin(result)] = min_prob + adjustment\n        min_val = min(result)\n        max_val = max(result)\n    return result",
            "def generate_n_rand_probabilities_that_sum_to_m(n, m, *, max_prob=1.0, min_prob=0.0) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Generates `n` random probabilities that sum to `m`.\\n\\n    When ``min_prob=0`` and ``max_prob = 1.0``, use\\n    ``np.random.dirichlet(np.ones(n))*m`` instead.\\n\\n    Parameters\\n    ----------\\n    n : int\\n      Length of array of random probabilities to be returned.\\n\\n    m : float\\n      Sum of array of random probabilities that is returned.\\n\\n    max_prob : float, default=1.0\\n      Maximum probability of any entry in the returned array. Must be between 0 and 1.\\n\\n    min_prob : float, default=0.0\\n      Minimum probability of any entry in the returned array. Must be between 0 and 1.\\n\\n    Returns\\n    -------\\n    probabilities : np.ndarray\\n      An array of probabilities.\\n    '\n    if n == 0:\n        return np.array([])\n    if max_prob + FLOATING_POINT_COMPARISON < m / float(n):\n        raise ValueError('max_prob must be greater or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    if min_prob > (m + FLOATING_POINT_COMPARISON) / float(n):\n        raise ValueError('min_prob must be less or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    result = np.random.dirichlet(np.ones(n)) * m\n    min_val = min(result)\n    max_val = max(result)\n    while max_val > max_prob + FLOATING_POINT_COMPARISON:\n        new_min = min_val + (max_val - max_prob)\n        adjustment = (max_prob - new_min) * np.random.rand()\n        result[np.argmin(result)] = new_min + adjustment\n        result[np.argmax(result)] = max_prob - adjustment\n        min_val = min(result)\n        max_val = max(result)\n    min_val = min(result)\n    max_val = max(result)\n    while min_val < min_prob - FLOATING_POINT_COMPARISON:\n        min_val = min(result)\n        max_val = max(result)\n        new_max = max_val - (min_prob - min_val)\n        adjustment = (new_max - min_prob) * np.random.rand()\n        result[np.argmax(result)] = new_max - adjustment\n        result[np.argmin(result)] = min_prob + adjustment\n        min_val = min(result)\n        max_val = max(result)\n    return result",
            "def generate_n_rand_probabilities_that_sum_to_m(n, m, *, max_prob=1.0, min_prob=0.0) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Generates `n` random probabilities that sum to `m`.\\n\\n    When ``min_prob=0`` and ``max_prob = 1.0``, use\\n    ``np.random.dirichlet(np.ones(n))*m`` instead.\\n\\n    Parameters\\n    ----------\\n    n : int\\n      Length of array of random probabilities to be returned.\\n\\n    m : float\\n      Sum of array of random probabilities that is returned.\\n\\n    max_prob : float, default=1.0\\n      Maximum probability of any entry in the returned array. Must be between 0 and 1.\\n\\n    min_prob : float, default=0.0\\n      Minimum probability of any entry in the returned array. Must be between 0 and 1.\\n\\n    Returns\\n    -------\\n    probabilities : np.ndarray\\n      An array of probabilities.\\n    '\n    if n == 0:\n        return np.array([])\n    if max_prob + FLOATING_POINT_COMPARISON < m / float(n):\n        raise ValueError('max_prob must be greater or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    if min_prob > (m + FLOATING_POINT_COMPARISON) / float(n):\n        raise ValueError('min_prob must be less or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    result = np.random.dirichlet(np.ones(n)) * m\n    min_val = min(result)\n    max_val = max(result)\n    while max_val > max_prob + FLOATING_POINT_COMPARISON:\n        new_min = min_val + (max_val - max_prob)\n        adjustment = (max_prob - new_min) * np.random.rand()\n        result[np.argmin(result)] = new_min + adjustment\n        result[np.argmax(result)] = max_prob - adjustment\n        min_val = min(result)\n        max_val = max(result)\n    min_val = min(result)\n    max_val = max(result)\n    while min_val < min_prob - FLOATING_POINT_COMPARISON:\n        min_val = min(result)\n        max_val = max(result)\n        new_max = max_val - (min_prob - min_val)\n        adjustment = (new_max - min_prob) * np.random.rand()\n        result[np.argmax(result)] = new_max - adjustment\n        result[np.argmin(result)] = min_prob + adjustment\n        min_val = min(result)\n        max_val = max(result)\n    return result",
            "def generate_n_rand_probabilities_that_sum_to_m(n, m, *, max_prob=1.0, min_prob=0.0) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Generates `n` random probabilities that sum to `m`.\\n\\n    When ``min_prob=0`` and ``max_prob = 1.0``, use\\n    ``np.random.dirichlet(np.ones(n))*m`` instead.\\n\\n    Parameters\\n    ----------\\n    n : int\\n      Length of array of random probabilities to be returned.\\n\\n    m : float\\n      Sum of array of random probabilities that is returned.\\n\\n    max_prob : float, default=1.0\\n      Maximum probability of any entry in the returned array. Must be between 0 and 1.\\n\\n    min_prob : float, default=0.0\\n      Minimum probability of any entry in the returned array. Must be between 0 and 1.\\n\\n    Returns\\n    -------\\n    probabilities : np.ndarray\\n      An array of probabilities.\\n    '\n    if n == 0:\n        return np.array([])\n    if max_prob + FLOATING_POINT_COMPARISON < m / float(n):\n        raise ValueError('max_prob must be greater or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    if min_prob > (m + FLOATING_POINT_COMPARISON) / float(n):\n        raise ValueError('min_prob must be less or equal to m / n, but ' + 'max_prob = ' + str(max_prob) + ', m = ' + str(m) + ', n = ' + str(n) + ', m / n = ' + str(m / float(n)))\n    result = np.random.dirichlet(np.ones(n)) * m\n    min_val = min(result)\n    max_val = max(result)\n    while max_val > max_prob + FLOATING_POINT_COMPARISON:\n        new_min = min_val + (max_val - max_prob)\n        adjustment = (max_prob - new_min) * np.random.rand()\n        result[np.argmin(result)] = new_min + adjustment\n        result[np.argmax(result)] = max_prob - adjustment\n        min_val = min(result)\n        max_val = max(result)\n    min_val = min(result)\n    max_val = max(result)\n    while min_val < min_prob - FLOATING_POINT_COMPARISON:\n        min_val = min(result)\n        max_val = max(result)\n        new_max = max_val - (min_prob - min_val)\n        adjustment = (new_max - min_prob) * np.random.rand()\n        result[np.argmax(result)] = new_max - adjustment\n        result[np.argmin(result)] = min_prob + adjustment\n        min_val = min(result)\n        max_val = max(result)\n    return result"
        ]
    },
    {
        "func_name": "randomly_distribute_N_balls_into_K_bins",
        "original": "def randomly_distribute_N_balls_into_K_bins(N, K, *, max_balls_per_bin=None, min_balls_per_bin=None) -> np.ndarray:\n    \"\"\"Returns a uniformly random numpy integer array of length `N` that sums\n    to `K`.\n\n    Parameters\n    ----------\n    N : int\n      Number of balls.\n    K : int\n      Number of bins.\n    max_balls_per_bin : int\n      Ensure that each bin contains at most `max_balls_per_bin` balls.\n    min_balls_per_bin : int\n      Ensure that each bin contains at least `min_balls_per_bin` balls.\n\n    Returns\n    -------\n    int_array : np.array\n      Length `N` array that sums to `K`.\n    \"\"\"\n    if N == 0:\n        return np.zeros(K, dtype=int)\n    if max_balls_per_bin is None:\n        max_balls_per_bin = N\n    else:\n        max_balls_per_bin = min(max_balls_per_bin, N)\n    if min_balls_per_bin is None:\n        min_balls_per_bin = 0\n    else:\n        min_balls_per_bin = min(min_balls_per_bin, N / K)\n    if N / float(K) > max_balls_per_bin:\n        N = max_balls_per_bin * K\n    arr = np.round(generate_n_rand_probabilities_that_sum_to_m(n=K, m=1, max_prob=max_balls_per_bin / float(N), min_prob=min_balls_per_bin / float(N)) * N)\n    while sum(arr) != N:\n        while sum(arr) > N:\n            arr[np.argmax(arr)] -= 1\n        while sum(arr) < N:\n            arr[np.argmin(arr)] += 1\n    return arr.astype(int)",
        "mutated": [
            "def randomly_distribute_N_balls_into_K_bins(N, K, *, max_balls_per_bin=None, min_balls_per_bin=None) -> np.ndarray:\n    if False:\n        i = 10\n    'Returns a uniformly random numpy integer array of length `N` that sums\\n    to `K`.\\n\\n    Parameters\\n    ----------\\n    N : int\\n      Number of balls.\\n    K : int\\n      Number of bins.\\n    max_balls_per_bin : int\\n      Ensure that each bin contains at most `max_balls_per_bin` balls.\\n    min_balls_per_bin : int\\n      Ensure that each bin contains at least `min_balls_per_bin` balls.\\n\\n    Returns\\n    -------\\n    int_array : np.array\\n      Length `N` array that sums to `K`.\\n    '\n    if N == 0:\n        return np.zeros(K, dtype=int)\n    if max_balls_per_bin is None:\n        max_balls_per_bin = N\n    else:\n        max_balls_per_bin = min(max_balls_per_bin, N)\n    if min_balls_per_bin is None:\n        min_balls_per_bin = 0\n    else:\n        min_balls_per_bin = min(min_balls_per_bin, N / K)\n    if N / float(K) > max_balls_per_bin:\n        N = max_balls_per_bin * K\n    arr = np.round(generate_n_rand_probabilities_that_sum_to_m(n=K, m=1, max_prob=max_balls_per_bin / float(N), min_prob=min_balls_per_bin / float(N)) * N)\n    while sum(arr) != N:\n        while sum(arr) > N:\n            arr[np.argmax(arr)] -= 1\n        while sum(arr) < N:\n            arr[np.argmin(arr)] += 1\n    return arr.astype(int)",
            "def randomly_distribute_N_balls_into_K_bins(N, K, *, max_balls_per_bin=None, min_balls_per_bin=None) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a uniformly random numpy integer array of length `N` that sums\\n    to `K`.\\n\\n    Parameters\\n    ----------\\n    N : int\\n      Number of balls.\\n    K : int\\n      Number of bins.\\n    max_balls_per_bin : int\\n      Ensure that each bin contains at most `max_balls_per_bin` balls.\\n    min_balls_per_bin : int\\n      Ensure that each bin contains at least `min_balls_per_bin` balls.\\n\\n    Returns\\n    -------\\n    int_array : np.array\\n      Length `N` array that sums to `K`.\\n    '\n    if N == 0:\n        return np.zeros(K, dtype=int)\n    if max_balls_per_bin is None:\n        max_balls_per_bin = N\n    else:\n        max_balls_per_bin = min(max_balls_per_bin, N)\n    if min_balls_per_bin is None:\n        min_balls_per_bin = 0\n    else:\n        min_balls_per_bin = min(min_balls_per_bin, N / K)\n    if N / float(K) > max_balls_per_bin:\n        N = max_balls_per_bin * K\n    arr = np.round(generate_n_rand_probabilities_that_sum_to_m(n=K, m=1, max_prob=max_balls_per_bin / float(N), min_prob=min_balls_per_bin / float(N)) * N)\n    while sum(arr) != N:\n        while sum(arr) > N:\n            arr[np.argmax(arr)] -= 1\n        while sum(arr) < N:\n            arr[np.argmin(arr)] += 1\n    return arr.astype(int)",
            "def randomly_distribute_N_balls_into_K_bins(N, K, *, max_balls_per_bin=None, min_balls_per_bin=None) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a uniformly random numpy integer array of length `N` that sums\\n    to `K`.\\n\\n    Parameters\\n    ----------\\n    N : int\\n      Number of balls.\\n    K : int\\n      Number of bins.\\n    max_balls_per_bin : int\\n      Ensure that each bin contains at most `max_balls_per_bin` balls.\\n    min_balls_per_bin : int\\n      Ensure that each bin contains at least `min_balls_per_bin` balls.\\n\\n    Returns\\n    -------\\n    int_array : np.array\\n      Length `N` array that sums to `K`.\\n    '\n    if N == 0:\n        return np.zeros(K, dtype=int)\n    if max_balls_per_bin is None:\n        max_balls_per_bin = N\n    else:\n        max_balls_per_bin = min(max_balls_per_bin, N)\n    if min_balls_per_bin is None:\n        min_balls_per_bin = 0\n    else:\n        min_balls_per_bin = min(min_balls_per_bin, N / K)\n    if N / float(K) > max_balls_per_bin:\n        N = max_balls_per_bin * K\n    arr = np.round(generate_n_rand_probabilities_that_sum_to_m(n=K, m=1, max_prob=max_balls_per_bin / float(N), min_prob=min_balls_per_bin / float(N)) * N)\n    while sum(arr) != N:\n        while sum(arr) > N:\n            arr[np.argmax(arr)] -= 1\n        while sum(arr) < N:\n            arr[np.argmin(arr)] += 1\n    return arr.astype(int)",
            "def randomly_distribute_N_balls_into_K_bins(N, K, *, max_balls_per_bin=None, min_balls_per_bin=None) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a uniformly random numpy integer array of length `N` that sums\\n    to `K`.\\n\\n    Parameters\\n    ----------\\n    N : int\\n      Number of balls.\\n    K : int\\n      Number of bins.\\n    max_balls_per_bin : int\\n      Ensure that each bin contains at most `max_balls_per_bin` balls.\\n    min_balls_per_bin : int\\n      Ensure that each bin contains at least `min_balls_per_bin` balls.\\n\\n    Returns\\n    -------\\n    int_array : np.array\\n      Length `N` array that sums to `K`.\\n    '\n    if N == 0:\n        return np.zeros(K, dtype=int)\n    if max_balls_per_bin is None:\n        max_balls_per_bin = N\n    else:\n        max_balls_per_bin = min(max_balls_per_bin, N)\n    if min_balls_per_bin is None:\n        min_balls_per_bin = 0\n    else:\n        min_balls_per_bin = min(min_balls_per_bin, N / K)\n    if N / float(K) > max_balls_per_bin:\n        N = max_balls_per_bin * K\n    arr = np.round(generate_n_rand_probabilities_that_sum_to_m(n=K, m=1, max_prob=max_balls_per_bin / float(N), min_prob=min_balls_per_bin / float(N)) * N)\n    while sum(arr) != N:\n        while sum(arr) > N:\n            arr[np.argmax(arr)] -= 1\n        while sum(arr) < N:\n            arr[np.argmin(arr)] += 1\n    return arr.astype(int)",
            "def randomly_distribute_N_balls_into_K_bins(N, K, *, max_balls_per_bin=None, min_balls_per_bin=None) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a uniformly random numpy integer array of length `N` that sums\\n    to `K`.\\n\\n    Parameters\\n    ----------\\n    N : int\\n      Number of balls.\\n    K : int\\n      Number of bins.\\n    max_balls_per_bin : int\\n      Ensure that each bin contains at most `max_balls_per_bin` balls.\\n    min_balls_per_bin : int\\n      Ensure that each bin contains at least `min_balls_per_bin` balls.\\n\\n    Returns\\n    -------\\n    int_array : np.array\\n      Length `N` array that sums to `K`.\\n    '\n    if N == 0:\n        return np.zeros(K, dtype=int)\n    if max_balls_per_bin is None:\n        max_balls_per_bin = N\n    else:\n        max_balls_per_bin = min(max_balls_per_bin, N)\n    if min_balls_per_bin is None:\n        min_balls_per_bin = 0\n    else:\n        min_balls_per_bin = min(min_balls_per_bin, N / K)\n    if N / float(K) > max_balls_per_bin:\n        N = max_balls_per_bin * K\n    arr = np.round(generate_n_rand_probabilities_that_sum_to_m(n=K, m=1, max_prob=max_balls_per_bin / float(N), min_prob=min_balls_per_bin / float(N)) * N)\n    while sum(arr) != N:\n        while sum(arr) > N:\n            arr[np.argmax(arr)] -= 1\n        while sum(arr) < N:\n            arr[np.argmin(arr)] += 1\n    return arr.astype(int)"
        ]
    }
]