[
    {
        "func_name": "fused_ec_moe",
        "original": "def fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type):\n    \"\"\"\n    Applies fused ec_moe kernel.\n    This method requires SM_ARCH in sm75, sm80, sm86.\n\n    Args:\n        x (Tensor): the input Tensor. Its shape is [bsz, seq_len, d_model].\n        gate (Tensor): the gate Tensor to choose expert. Its shape is [bsz, seq_len, e].\n        bmm0_weight (Tensor): the first batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\n        bmm0_bias (Tensor): the first batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\n        bmm1_weight (Tensor): the second batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\n        bmm1_bias (Tensor): the second batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\n        act_type (string): the Activation Type. Currently only support `gelu`, `relu`.\n\n    Returns:\n        Tensor: the output Tensor.\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +REQUIRES(env:GPU)\n            >>> import paddle\n            >>> from paddle.incubate.nn.functional import fused_ec_moe\n\n            >>> paddle.set_device('gpu')\n            >>> x = paddle.randn([10, 128, 1024])\n            >>> gate = paddle.randn([10, 128, 8])\n            >>> bmm0_weight = paddle.randn([8, 1024, 4096])\n            >>> bmm0_bias = paddle.randn([8, 1024, 4096])\n            >>> bmm1_weight = paddle.randn([8, 1024, 4096])\n            >>> bmm1_bias = paddle.randn([8, 1024, 4096])\n            >>> out = fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type=\"gelu\")\n            >>> print(out.shape)\n            [10, 128, 1024]\n    \"\"\"\n    helper = LayerHelper('fused_moe', **locals())\n    out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    helper.append_op(type='moe', inputs={'X': x, 'Gate': gate, 'Bmm0': bmm0_weight, 'Bias0': bmm0_bias, 'Bmm1': bmm1_weight, 'Bias1': bmm1_bias}, outputs={'Out': out}, attrs={'act_type': act_type})\n    return out",
        "mutated": [
            "def fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type):\n    if False:\n        i = 10\n    '\\n    Applies fused ec_moe kernel.\\n    This method requires SM_ARCH in sm75, sm80, sm86.\\n\\n    Args:\\n        x (Tensor): the input Tensor. Its shape is [bsz, seq_len, d_model].\\n        gate (Tensor): the gate Tensor to choose expert. Its shape is [bsz, seq_len, e].\\n        bmm0_weight (Tensor): the first batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\\n        bmm0_bias (Tensor): the first batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\\n        bmm1_weight (Tensor): the second batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\\n        bmm1_bias (Tensor): the second batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\\n        act_type (string): the Activation Type. Currently only support `gelu`, `relu`.\\n\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import paddle\\n            >>> from paddle.incubate.nn.functional import fused_ec_moe\\n\\n            >>> paddle.set_device(\\'gpu\\')\\n            >>> x = paddle.randn([10, 128, 1024])\\n            >>> gate = paddle.randn([10, 128, 8])\\n            >>> bmm0_weight = paddle.randn([8, 1024, 4096])\\n            >>> bmm0_bias = paddle.randn([8, 1024, 4096])\\n            >>> bmm1_weight = paddle.randn([8, 1024, 4096])\\n            >>> bmm1_bias = paddle.randn([8, 1024, 4096])\\n            >>> out = fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type=\"gelu\")\\n            >>> print(out.shape)\\n            [10, 128, 1024]\\n    '\n    helper = LayerHelper('fused_moe', **locals())\n    out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    helper.append_op(type='moe', inputs={'X': x, 'Gate': gate, 'Bmm0': bmm0_weight, 'Bias0': bmm0_bias, 'Bmm1': bmm1_weight, 'Bias1': bmm1_bias}, outputs={'Out': out}, attrs={'act_type': act_type})\n    return out",
            "def fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Applies fused ec_moe kernel.\\n    This method requires SM_ARCH in sm75, sm80, sm86.\\n\\n    Args:\\n        x (Tensor): the input Tensor. Its shape is [bsz, seq_len, d_model].\\n        gate (Tensor): the gate Tensor to choose expert. Its shape is [bsz, seq_len, e].\\n        bmm0_weight (Tensor): the first batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\\n        bmm0_bias (Tensor): the first batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\\n        bmm1_weight (Tensor): the second batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\\n        bmm1_bias (Tensor): the second batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\\n        act_type (string): the Activation Type. Currently only support `gelu`, `relu`.\\n\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import paddle\\n            >>> from paddle.incubate.nn.functional import fused_ec_moe\\n\\n            >>> paddle.set_device(\\'gpu\\')\\n            >>> x = paddle.randn([10, 128, 1024])\\n            >>> gate = paddle.randn([10, 128, 8])\\n            >>> bmm0_weight = paddle.randn([8, 1024, 4096])\\n            >>> bmm0_bias = paddle.randn([8, 1024, 4096])\\n            >>> bmm1_weight = paddle.randn([8, 1024, 4096])\\n            >>> bmm1_bias = paddle.randn([8, 1024, 4096])\\n            >>> out = fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type=\"gelu\")\\n            >>> print(out.shape)\\n            [10, 128, 1024]\\n    '\n    helper = LayerHelper('fused_moe', **locals())\n    out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    helper.append_op(type='moe', inputs={'X': x, 'Gate': gate, 'Bmm0': bmm0_weight, 'Bias0': bmm0_bias, 'Bmm1': bmm1_weight, 'Bias1': bmm1_bias}, outputs={'Out': out}, attrs={'act_type': act_type})\n    return out",
            "def fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Applies fused ec_moe kernel.\\n    This method requires SM_ARCH in sm75, sm80, sm86.\\n\\n    Args:\\n        x (Tensor): the input Tensor. Its shape is [bsz, seq_len, d_model].\\n        gate (Tensor): the gate Tensor to choose expert. Its shape is [bsz, seq_len, e].\\n        bmm0_weight (Tensor): the first batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\\n        bmm0_bias (Tensor): the first batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\\n        bmm1_weight (Tensor): the second batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\\n        bmm1_bias (Tensor): the second batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\\n        act_type (string): the Activation Type. Currently only support `gelu`, `relu`.\\n\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import paddle\\n            >>> from paddle.incubate.nn.functional import fused_ec_moe\\n\\n            >>> paddle.set_device(\\'gpu\\')\\n            >>> x = paddle.randn([10, 128, 1024])\\n            >>> gate = paddle.randn([10, 128, 8])\\n            >>> bmm0_weight = paddle.randn([8, 1024, 4096])\\n            >>> bmm0_bias = paddle.randn([8, 1024, 4096])\\n            >>> bmm1_weight = paddle.randn([8, 1024, 4096])\\n            >>> bmm1_bias = paddle.randn([8, 1024, 4096])\\n            >>> out = fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type=\"gelu\")\\n            >>> print(out.shape)\\n            [10, 128, 1024]\\n    '\n    helper = LayerHelper('fused_moe', **locals())\n    out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    helper.append_op(type='moe', inputs={'X': x, 'Gate': gate, 'Bmm0': bmm0_weight, 'Bias0': bmm0_bias, 'Bmm1': bmm1_weight, 'Bias1': bmm1_bias}, outputs={'Out': out}, attrs={'act_type': act_type})\n    return out",
            "def fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Applies fused ec_moe kernel.\\n    This method requires SM_ARCH in sm75, sm80, sm86.\\n\\n    Args:\\n        x (Tensor): the input Tensor. Its shape is [bsz, seq_len, d_model].\\n        gate (Tensor): the gate Tensor to choose expert. Its shape is [bsz, seq_len, e].\\n        bmm0_weight (Tensor): the first batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\\n        bmm0_bias (Tensor): the first batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\\n        bmm1_weight (Tensor): the second batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\\n        bmm1_bias (Tensor): the second batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\\n        act_type (string): the Activation Type. Currently only support `gelu`, `relu`.\\n\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import paddle\\n            >>> from paddle.incubate.nn.functional import fused_ec_moe\\n\\n            >>> paddle.set_device(\\'gpu\\')\\n            >>> x = paddle.randn([10, 128, 1024])\\n            >>> gate = paddle.randn([10, 128, 8])\\n            >>> bmm0_weight = paddle.randn([8, 1024, 4096])\\n            >>> bmm0_bias = paddle.randn([8, 1024, 4096])\\n            >>> bmm1_weight = paddle.randn([8, 1024, 4096])\\n            >>> bmm1_bias = paddle.randn([8, 1024, 4096])\\n            >>> out = fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type=\"gelu\")\\n            >>> print(out.shape)\\n            [10, 128, 1024]\\n    '\n    helper = LayerHelper('fused_moe', **locals())\n    out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    helper.append_op(type='moe', inputs={'X': x, 'Gate': gate, 'Bmm0': bmm0_weight, 'Bias0': bmm0_bias, 'Bmm1': bmm1_weight, 'Bias1': bmm1_bias}, outputs={'Out': out}, attrs={'act_type': act_type})\n    return out",
            "def fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Applies fused ec_moe kernel.\\n    This method requires SM_ARCH in sm75, sm80, sm86.\\n\\n    Args:\\n        x (Tensor): the input Tensor. Its shape is [bsz, seq_len, d_model].\\n        gate (Tensor): the gate Tensor to choose expert. Its shape is [bsz, seq_len, e].\\n        bmm0_weight (Tensor): the first batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\\n        bmm0_bias (Tensor): the first batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\\n        bmm1_weight (Tensor): the second batch matrix matmul weight. Its shape is [e, d_model, d_feed_forward].\\n        bmm1_bias (Tensor): the second batch matrix matmul bias. Its shape is [e, 1, d_feed_forward].\\n        act_type (string): the Activation Type. Currently only support `gelu`, `relu`.\\n\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import paddle\\n            >>> from paddle.incubate.nn.functional import fused_ec_moe\\n\\n            >>> paddle.set_device(\\'gpu\\')\\n            >>> x = paddle.randn([10, 128, 1024])\\n            >>> gate = paddle.randn([10, 128, 8])\\n            >>> bmm0_weight = paddle.randn([8, 1024, 4096])\\n            >>> bmm0_bias = paddle.randn([8, 1024, 4096])\\n            >>> bmm1_weight = paddle.randn([8, 1024, 4096])\\n            >>> bmm1_bias = paddle.randn([8, 1024, 4096])\\n            >>> out = fused_ec_moe(x, gate, bmm0_weight, bmm0_bias, bmm1_weight, bmm1_bias, act_type=\"gelu\")\\n            >>> print(out.shape)\\n            [10, 128, 1024]\\n    '\n    helper = LayerHelper('fused_moe', **locals())\n    out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    helper.append_op(type='moe', inputs={'X': x, 'Gate': gate, 'Bmm0': bmm0_weight, 'Bias0': bmm0_bias, 'Bmm1': bmm1_weight, 'Bias1': bmm1_bias}, outputs={'Out': out}, attrs={'act_type': act_type})\n    return out"
        ]
    }
]