[
    {
        "func_name": "to8b",
        "original": "def to8b(x):\n    return (255 * np.clip(x, 0, 1)).astype(np.uint8)",
        "mutated": [
            "def to8b(x):\n    if False:\n        i = 10\n    return (255 * np.clip(x, 0, 1)).astype(np.uint8)",
            "def to8b(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return (255 * np.clip(x, 0, 1)).astype(np.uint8)",
            "def to8b(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return (255 * np.clip(x, 0, 1)).astype(np.uint8)",
            "def to8b(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return (255 * np.clip(x, 0, 1)).astype(np.uint8)",
            "def to8b(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return (255 * np.clip(x, 0, 1)).astype(np.uint8)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, model_dir, **kwargs):\n    super().__init__(model_dir, **kwargs)\n    if not torch.cuda.is_available():\n        raise Exception('GPU is required')\n    self.device = torch.device('cuda')\n    logger.info('model params:{}'.format(kwargs))\n    self.data_type = kwargs['data_type']\n    self.test_ray_chunk = kwargs['test_ray_chunk']\n    self.enc_ckpt_path = os.path.join(model_dir, 'fine_100000.tar')\n    if not os.path.exists(self.enc_ckpt_path):\n        raise Exception('encoder ckpt path not found')\n    self.dec_ckpt_path = os.path.join(model_dir, 'sresrnet_100000.pth')\n    if not os.path.exists(self.dec_ckpt_path):\n        raise Exception('decoder ckpt path not found')\n    self.ckpt_name = self.dec_ckpt_path.split('/')[-1][:-4]\n    self.ndc = True if self.data_type == 'llff' else False\n    self.sr_ratio = int(kwargs['factor'] / kwargs['load_sr'])\n    self.load_existed_model()\n    self.test_tile = kwargs['test_tile']\n    self.stepsize = kwargs['stepsize']",
        "mutated": [
            "def __init__(self, model_dir, **kwargs):\n    if False:\n        i = 10\n    super().__init__(model_dir, **kwargs)\n    if not torch.cuda.is_available():\n        raise Exception('GPU is required')\n    self.device = torch.device('cuda')\n    logger.info('model params:{}'.format(kwargs))\n    self.data_type = kwargs['data_type']\n    self.test_ray_chunk = kwargs['test_ray_chunk']\n    self.enc_ckpt_path = os.path.join(model_dir, 'fine_100000.tar')\n    if not os.path.exists(self.enc_ckpt_path):\n        raise Exception('encoder ckpt path not found')\n    self.dec_ckpt_path = os.path.join(model_dir, 'sresrnet_100000.pth')\n    if not os.path.exists(self.dec_ckpt_path):\n        raise Exception('decoder ckpt path not found')\n    self.ckpt_name = self.dec_ckpt_path.split('/')[-1][:-4]\n    self.ndc = True if self.data_type == 'llff' else False\n    self.sr_ratio = int(kwargs['factor'] / kwargs['load_sr'])\n    self.load_existed_model()\n    self.test_tile = kwargs['test_tile']\n    self.stepsize = kwargs['stepsize']",
            "def __init__(self, model_dir, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(model_dir, **kwargs)\n    if not torch.cuda.is_available():\n        raise Exception('GPU is required')\n    self.device = torch.device('cuda')\n    logger.info('model params:{}'.format(kwargs))\n    self.data_type = kwargs['data_type']\n    self.test_ray_chunk = kwargs['test_ray_chunk']\n    self.enc_ckpt_path = os.path.join(model_dir, 'fine_100000.tar')\n    if not os.path.exists(self.enc_ckpt_path):\n        raise Exception('encoder ckpt path not found')\n    self.dec_ckpt_path = os.path.join(model_dir, 'sresrnet_100000.pth')\n    if not os.path.exists(self.dec_ckpt_path):\n        raise Exception('decoder ckpt path not found')\n    self.ckpt_name = self.dec_ckpt_path.split('/')[-1][:-4]\n    self.ndc = True if self.data_type == 'llff' else False\n    self.sr_ratio = int(kwargs['factor'] / kwargs['load_sr'])\n    self.load_existed_model()\n    self.test_tile = kwargs['test_tile']\n    self.stepsize = kwargs['stepsize']",
            "def __init__(self, model_dir, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(model_dir, **kwargs)\n    if not torch.cuda.is_available():\n        raise Exception('GPU is required')\n    self.device = torch.device('cuda')\n    logger.info('model params:{}'.format(kwargs))\n    self.data_type = kwargs['data_type']\n    self.test_ray_chunk = kwargs['test_ray_chunk']\n    self.enc_ckpt_path = os.path.join(model_dir, 'fine_100000.tar')\n    if not os.path.exists(self.enc_ckpt_path):\n        raise Exception('encoder ckpt path not found')\n    self.dec_ckpt_path = os.path.join(model_dir, 'sresrnet_100000.pth')\n    if not os.path.exists(self.dec_ckpt_path):\n        raise Exception('decoder ckpt path not found')\n    self.ckpt_name = self.dec_ckpt_path.split('/')[-1][:-4]\n    self.ndc = True if self.data_type == 'llff' else False\n    self.sr_ratio = int(kwargs['factor'] / kwargs['load_sr'])\n    self.load_existed_model()\n    self.test_tile = kwargs['test_tile']\n    self.stepsize = kwargs['stepsize']",
            "def __init__(self, model_dir, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(model_dir, **kwargs)\n    if not torch.cuda.is_available():\n        raise Exception('GPU is required')\n    self.device = torch.device('cuda')\n    logger.info('model params:{}'.format(kwargs))\n    self.data_type = kwargs['data_type']\n    self.test_ray_chunk = kwargs['test_ray_chunk']\n    self.enc_ckpt_path = os.path.join(model_dir, 'fine_100000.tar')\n    if not os.path.exists(self.enc_ckpt_path):\n        raise Exception('encoder ckpt path not found')\n    self.dec_ckpt_path = os.path.join(model_dir, 'sresrnet_100000.pth')\n    if not os.path.exists(self.dec_ckpt_path):\n        raise Exception('decoder ckpt path not found')\n    self.ckpt_name = self.dec_ckpt_path.split('/')[-1][:-4]\n    self.ndc = True if self.data_type == 'llff' else False\n    self.sr_ratio = int(kwargs['factor'] / kwargs['load_sr'])\n    self.load_existed_model()\n    self.test_tile = kwargs['test_tile']\n    self.stepsize = kwargs['stepsize']",
            "def __init__(self, model_dir, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(model_dir, **kwargs)\n    if not torch.cuda.is_available():\n        raise Exception('GPU is required')\n    self.device = torch.device('cuda')\n    logger.info('model params:{}'.format(kwargs))\n    self.data_type = kwargs['data_type']\n    self.test_ray_chunk = kwargs['test_ray_chunk']\n    self.enc_ckpt_path = os.path.join(model_dir, 'fine_100000.tar')\n    if not os.path.exists(self.enc_ckpt_path):\n        raise Exception('encoder ckpt path not found')\n    self.dec_ckpt_path = os.path.join(model_dir, 'sresrnet_100000.pth')\n    if not os.path.exists(self.dec_ckpt_path):\n        raise Exception('decoder ckpt path not found')\n    self.ckpt_name = self.dec_ckpt_path.split('/')[-1][:-4]\n    self.ndc = True if self.data_type == 'llff' else False\n    self.sr_ratio = int(kwargs['factor'] / kwargs['load_sr'])\n    self.load_existed_model()\n    self.test_tile = kwargs['test_tile']\n    self.stepsize = kwargs['stepsize']"
        ]
    },
    {
        "func_name": "load_existed_model",
        "original": "def load_existed_model(self):\n    if self.ndc:\n        model_class = DirectMPIGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n    else:\n        model_class = DirectVoxGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n        ckpt['model_kwargs']['mask_cache_path'] = self.enc_ckpt_path\n    self.encoder = model_class(**ckpt['model_kwargs'])\n    self.encoder.load_state_dict(ckpt['model_state_dict'])\n    self.encoder = self.encoder.to(self.device)\n    self.decoder = SFTNet(n_in_colors=3, scale=self.sr_ratio, num_feat=64, num_block=5, num_grow_ch=32, num_cond=1, dswise=False).to(self.device)\n    self.decoder.load_network(load_path=self.dec_ckpt_path, device=self.device)\n    self.decoder.eval()",
        "mutated": [
            "def load_existed_model(self):\n    if False:\n        i = 10\n    if self.ndc:\n        model_class = DirectMPIGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n    else:\n        model_class = DirectVoxGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n        ckpt['model_kwargs']['mask_cache_path'] = self.enc_ckpt_path\n    self.encoder = model_class(**ckpt['model_kwargs'])\n    self.encoder.load_state_dict(ckpt['model_state_dict'])\n    self.encoder = self.encoder.to(self.device)\n    self.decoder = SFTNet(n_in_colors=3, scale=self.sr_ratio, num_feat=64, num_block=5, num_grow_ch=32, num_cond=1, dswise=False).to(self.device)\n    self.decoder.load_network(load_path=self.dec_ckpt_path, device=self.device)\n    self.decoder.eval()",
            "def load_existed_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.ndc:\n        model_class = DirectMPIGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n    else:\n        model_class = DirectVoxGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n        ckpt['model_kwargs']['mask_cache_path'] = self.enc_ckpt_path\n    self.encoder = model_class(**ckpt['model_kwargs'])\n    self.encoder.load_state_dict(ckpt['model_state_dict'])\n    self.encoder = self.encoder.to(self.device)\n    self.decoder = SFTNet(n_in_colors=3, scale=self.sr_ratio, num_feat=64, num_block=5, num_grow_ch=32, num_cond=1, dswise=False).to(self.device)\n    self.decoder.load_network(load_path=self.dec_ckpt_path, device=self.device)\n    self.decoder.eval()",
            "def load_existed_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.ndc:\n        model_class = DirectMPIGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n    else:\n        model_class = DirectVoxGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n        ckpt['model_kwargs']['mask_cache_path'] = self.enc_ckpt_path\n    self.encoder = model_class(**ckpt['model_kwargs'])\n    self.encoder.load_state_dict(ckpt['model_state_dict'])\n    self.encoder = self.encoder.to(self.device)\n    self.decoder = SFTNet(n_in_colors=3, scale=self.sr_ratio, num_feat=64, num_block=5, num_grow_ch=32, num_cond=1, dswise=False).to(self.device)\n    self.decoder.load_network(load_path=self.dec_ckpt_path, device=self.device)\n    self.decoder.eval()",
            "def load_existed_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.ndc:\n        model_class = DirectMPIGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n    else:\n        model_class = DirectVoxGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n        ckpt['model_kwargs']['mask_cache_path'] = self.enc_ckpt_path\n    self.encoder = model_class(**ckpt['model_kwargs'])\n    self.encoder.load_state_dict(ckpt['model_state_dict'])\n    self.encoder = self.encoder.to(self.device)\n    self.decoder = SFTNet(n_in_colors=3, scale=self.sr_ratio, num_feat=64, num_block=5, num_grow_ch=32, num_cond=1, dswise=False).to(self.device)\n    self.decoder.load_network(load_path=self.dec_ckpt_path, device=self.device)\n    self.decoder.eval()",
            "def load_existed_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.ndc:\n        model_class = DirectMPIGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n    else:\n        model_class = DirectVoxGO\n        ckpt = torch.load(self.enc_ckpt_path, map_location='cpu')\n        ckpt['model_kwargs']['mask_cache_path'] = self.enc_ckpt_path\n    self.encoder = model_class(**ckpt['model_kwargs'])\n    self.encoder.load_state_dict(ckpt['model_state_dict'])\n    self.encoder = self.encoder.to(self.device)\n    self.decoder = SFTNet(n_in_colors=3, scale=self.sr_ratio, num_feat=64, num_block=5, num_grow_ch=32, num_cond=1, dswise=False).to(self.device)\n    self.decoder.load_network(load_path=self.dec_ckpt_path, device=self.device)\n    self.decoder.eval()"
        ]
    },
    {
        "func_name": "nerf_reconstruction",
        "original": "def nerf_reconstruction(self, data_cfg, render_dir):\n    data_dict = load_everything(cfg_data=data_cfg)\n    self.render_viewpoints_kwargs = {'render_kwargs': {'near': data_dict['near'], 'far': data_dict['far'], 'bg': 1 if data_dict['white_bkgd'] else 0, 'stepsize': self.stepsize, 'inverse_y': False, 'flip_x': False, 'flip_y': False, 'render_depth': True}}\n    os.makedirs(render_dir, exist_ok=True)\n    print('All results are dumped into', render_dir)\n    (rgbs, depths, bgmaps, _, _, rgb_features) = self.render_viewpoints(render_poses=data_dict['poses'][data_dict['i_test']], HW=data_dict['HW'][data_dict['i_test']], Ks=data_dict['Ks'][data_dict['i_test']], gt_imgs=[data_dict['images'][i].cpu().numpy() for i in data_dict['i_test']], savedir=render_dir, dump_images=False, **self.render_viewpoints_kwargs)\n    rgbsr = []\n    for (idx, rgbsave) in enumerate(tqdm(rgb_features)):\n        rgbtest = torch.from_numpy(rgbsave).movedim(-1, 0).unsqueeze(0).to(self.device)\n        input_cond = torch.from_numpy(depths).movedim(-1, 1)\n        input_cond = input_cond[idx, :, :, :].to(self.device)\n        if self.test_tile:\n            rgb_srtest = self.decoder.tile_process(rgbtest, input_cond, tile_size=self.test_tile)\n        else:\n            rgb_srtest = self.decoder(rgbtest, input_cond).detach().to('cpu')\n        rgb_srsave = rgb_srtest.squeeze().movedim(0, -1).detach().clamp(0, 1).numpy()\n        rgbsr.append(rgb_srsave)\n    print('all inference process has done, saving images... because our images are\\n            4K (4032x3024), the saving process may be time-consuming.')\n    rgbsr = np.array(rgbsr)\n    for i in trange(len(rgbsr)):\n        rgb8 = to8b(rgbsr[i])\n        filename = os.path.join(render_dir, '{:03d}_dec.png'.format(i))\n        imageio.imwrite(filename, rgb8)\n    imageio.mimwrite(os.path.join(render_dir, 'result_dec.mp4'), to8b(rgbsr), fps=25, codec='libx264', quality=8)",
        "mutated": [
            "def nerf_reconstruction(self, data_cfg, render_dir):\n    if False:\n        i = 10\n    data_dict = load_everything(cfg_data=data_cfg)\n    self.render_viewpoints_kwargs = {'render_kwargs': {'near': data_dict['near'], 'far': data_dict['far'], 'bg': 1 if data_dict['white_bkgd'] else 0, 'stepsize': self.stepsize, 'inverse_y': False, 'flip_x': False, 'flip_y': False, 'render_depth': True}}\n    os.makedirs(render_dir, exist_ok=True)\n    print('All results are dumped into', render_dir)\n    (rgbs, depths, bgmaps, _, _, rgb_features) = self.render_viewpoints(render_poses=data_dict['poses'][data_dict['i_test']], HW=data_dict['HW'][data_dict['i_test']], Ks=data_dict['Ks'][data_dict['i_test']], gt_imgs=[data_dict['images'][i].cpu().numpy() for i in data_dict['i_test']], savedir=render_dir, dump_images=False, **self.render_viewpoints_kwargs)\n    rgbsr = []\n    for (idx, rgbsave) in enumerate(tqdm(rgb_features)):\n        rgbtest = torch.from_numpy(rgbsave).movedim(-1, 0).unsqueeze(0).to(self.device)\n        input_cond = torch.from_numpy(depths).movedim(-1, 1)\n        input_cond = input_cond[idx, :, :, :].to(self.device)\n        if self.test_tile:\n            rgb_srtest = self.decoder.tile_process(rgbtest, input_cond, tile_size=self.test_tile)\n        else:\n            rgb_srtest = self.decoder(rgbtest, input_cond).detach().to('cpu')\n        rgb_srsave = rgb_srtest.squeeze().movedim(0, -1).detach().clamp(0, 1).numpy()\n        rgbsr.append(rgb_srsave)\n    print('all inference process has done, saving images... because our images are\\n            4K (4032x3024), the saving process may be time-consuming.')\n    rgbsr = np.array(rgbsr)\n    for i in trange(len(rgbsr)):\n        rgb8 = to8b(rgbsr[i])\n        filename = os.path.join(render_dir, '{:03d}_dec.png'.format(i))\n        imageio.imwrite(filename, rgb8)\n    imageio.mimwrite(os.path.join(render_dir, 'result_dec.mp4'), to8b(rgbsr), fps=25, codec='libx264', quality=8)",
            "def nerf_reconstruction(self, data_cfg, render_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    data_dict = load_everything(cfg_data=data_cfg)\n    self.render_viewpoints_kwargs = {'render_kwargs': {'near': data_dict['near'], 'far': data_dict['far'], 'bg': 1 if data_dict['white_bkgd'] else 0, 'stepsize': self.stepsize, 'inverse_y': False, 'flip_x': False, 'flip_y': False, 'render_depth': True}}\n    os.makedirs(render_dir, exist_ok=True)\n    print('All results are dumped into', render_dir)\n    (rgbs, depths, bgmaps, _, _, rgb_features) = self.render_viewpoints(render_poses=data_dict['poses'][data_dict['i_test']], HW=data_dict['HW'][data_dict['i_test']], Ks=data_dict['Ks'][data_dict['i_test']], gt_imgs=[data_dict['images'][i].cpu().numpy() for i in data_dict['i_test']], savedir=render_dir, dump_images=False, **self.render_viewpoints_kwargs)\n    rgbsr = []\n    for (idx, rgbsave) in enumerate(tqdm(rgb_features)):\n        rgbtest = torch.from_numpy(rgbsave).movedim(-1, 0).unsqueeze(0).to(self.device)\n        input_cond = torch.from_numpy(depths).movedim(-1, 1)\n        input_cond = input_cond[idx, :, :, :].to(self.device)\n        if self.test_tile:\n            rgb_srtest = self.decoder.tile_process(rgbtest, input_cond, tile_size=self.test_tile)\n        else:\n            rgb_srtest = self.decoder(rgbtest, input_cond).detach().to('cpu')\n        rgb_srsave = rgb_srtest.squeeze().movedim(0, -1).detach().clamp(0, 1).numpy()\n        rgbsr.append(rgb_srsave)\n    print('all inference process has done, saving images... because our images are\\n            4K (4032x3024), the saving process may be time-consuming.')\n    rgbsr = np.array(rgbsr)\n    for i in trange(len(rgbsr)):\n        rgb8 = to8b(rgbsr[i])\n        filename = os.path.join(render_dir, '{:03d}_dec.png'.format(i))\n        imageio.imwrite(filename, rgb8)\n    imageio.mimwrite(os.path.join(render_dir, 'result_dec.mp4'), to8b(rgbsr), fps=25, codec='libx264', quality=8)",
            "def nerf_reconstruction(self, data_cfg, render_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    data_dict = load_everything(cfg_data=data_cfg)\n    self.render_viewpoints_kwargs = {'render_kwargs': {'near': data_dict['near'], 'far': data_dict['far'], 'bg': 1 if data_dict['white_bkgd'] else 0, 'stepsize': self.stepsize, 'inverse_y': False, 'flip_x': False, 'flip_y': False, 'render_depth': True}}\n    os.makedirs(render_dir, exist_ok=True)\n    print('All results are dumped into', render_dir)\n    (rgbs, depths, bgmaps, _, _, rgb_features) = self.render_viewpoints(render_poses=data_dict['poses'][data_dict['i_test']], HW=data_dict['HW'][data_dict['i_test']], Ks=data_dict['Ks'][data_dict['i_test']], gt_imgs=[data_dict['images'][i].cpu().numpy() for i in data_dict['i_test']], savedir=render_dir, dump_images=False, **self.render_viewpoints_kwargs)\n    rgbsr = []\n    for (idx, rgbsave) in enumerate(tqdm(rgb_features)):\n        rgbtest = torch.from_numpy(rgbsave).movedim(-1, 0).unsqueeze(0).to(self.device)\n        input_cond = torch.from_numpy(depths).movedim(-1, 1)\n        input_cond = input_cond[idx, :, :, :].to(self.device)\n        if self.test_tile:\n            rgb_srtest = self.decoder.tile_process(rgbtest, input_cond, tile_size=self.test_tile)\n        else:\n            rgb_srtest = self.decoder(rgbtest, input_cond).detach().to('cpu')\n        rgb_srsave = rgb_srtest.squeeze().movedim(0, -1).detach().clamp(0, 1).numpy()\n        rgbsr.append(rgb_srsave)\n    print('all inference process has done, saving images... because our images are\\n            4K (4032x3024), the saving process may be time-consuming.')\n    rgbsr = np.array(rgbsr)\n    for i in trange(len(rgbsr)):\n        rgb8 = to8b(rgbsr[i])\n        filename = os.path.join(render_dir, '{:03d}_dec.png'.format(i))\n        imageio.imwrite(filename, rgb8)\n    imageio.mimwrite(os.path.join(render_dir, 'result_dec.mp4'), to8b(rgbsr), fps=25, codec='libx264', quality=8)",
            "def nerf_reconstruction(self, data_cfg, render_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    data_dict = load_everything(cfg_data=data_cfg)\n    self.render_viewpoints_kwargs = {'render_kwargs': {'near': data_dict['near'], 'far': data_dict['far'], 'bg': 1 if data_dict['white_bkgd'] else 0, 'stepsize': self.stepsize, 'inverse_y': False, 'flip_x': False, 'flip_y': False, 'render_depth': True}}\n    os.makedirs(render_dir, exist_ok=True)\n    print('All results are dumped into', render_dir)\n    (rgbs, depths, bgmaps, _, _, rgb_features) = self.render_viewpoints(render_poses=data_dict['poses'][data_dict['i_test']], HW=data_dict['HW'][data_dict['i_test']], Ks=data_dict['Ks'][data_dict['i_test']], gt_imgs=[data_dict['images'][i].cpu().numpy() for i in data_dict['i_test']], savedir=render_dir, dump_images=False, **self.render_viewpoints_kwargs)\n    rgbsr = []\n    for (idx, rgbsave) in enumerate(tqdm(rgb_features)):\n        rgbtest = torch.from_numpy(rgbsave).movedim(-1, 0).unsqueeze(0).to(self.device)\n        input_cond = torch.from_numpy(depths).movedim(-1, 1)\n        input_cond = input_cond[idx, :, :, :].to(self.device)\n        if self.test_tile:\n            rgb_srtest = self.decoder.tile_process(rgbtest, input_cond, tile_size=self.test_tile)\n        else:\n            rgb_srtest = self.decoder(rgbtest, input_cond).detach().to('cpu')\n        rgb_srsave = rgb_srtest.squeeze().movedim(0, -1).detach().clamp(0, 1).numpy()\n        rgbsr.append(rgb_srsave)\n    print('all inference process has done, saving images... because our images are\\n            4K (4032x3024), the saving process may be time-consuming.')\n    rgbsr = np.array(rgbsr)\n    for i in trange(len(rgbsr)):\n        rgb8 = to8b(rgbsr[i])\n        filename = os.path.join(render_dir, '{:03d}_dec.png'.format(i))\n        imageio.imwrite(filename, rgb8)\n    imageio.mimwrite(os.path.join(render_dir, 'result_dec.mp4'), to8b(rgbsr), fps=25, codec='libx264', quality=8)",
            "def nerf_reconstruction(self, data_cfg, render_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    data_dict = load_everything(cfg_data=data_cfg)\n    self.render_viewpoints_kwargs = {'render_kwargs': {'near': data_dict['near'], 'far': data_dict['far'], 'bg': 1 if data_dict['white_bkgd'] else 0, 'stepsize': self.stepsize, 'inverse_y': False, 'flip_x': False, 'flip_y': False, 'render_depth': True}}\n    os.makedirs(render_dir, exist_ok=True)\n    print('All results are dumped into', render_dir)\n    (rgbs, depths, bgmaps, _, _, rgb_features) = self.render_viewpoints(render_poses=data_dict['poses'][data_dict['i_test']], HW=data_dict['HW'][data_dict['i_test']], Ks=data_dict['Ks'][data_dict['i_test']], gt_imgs=[data_dict['images'][i].cpu().numpy() for i in data_dict['i_test']], savedir=render_dir, dump_images=False, **self.render_viewpoints_kwargs)\n    rgbsr = []\n    for (idx, rgbsave) in enumerate(tqdm(rgb_features)):\n        rgbtest = torch.from_numpy(rgbsave).movedim(-1, 0).unsqueeze(0).to(self.device)\n        input_cond = torch.from_numpy(depths).movedim(-1, 1)\n        input_cond = input_cond[idx, :, :, :].to(self.device)\n        if self.test_tile:\n            rgb_srtest = self.decoder.tile_process(rgbtest, input_cond, tile_size=self.test_tile)\n        else:\n            rgb_srtest = self.decoder(rgbtest, input_cond).detach().to('cpu')\n        rgb_srsave = rgb_srtest.squeeze().movedim(0, -1).detach().clamp(0, 1).numpy()\n        rgbsr.append(rgb_srsave)\n    print('all inference process has done, saving images... because our images are\\n            4K (4032x3024), the saving process may be time-consuming.')\n    rgbsr = np.array(rgbsr)\n    for i in trange(len(rgbsr)):\n        rgb8 = to8b(rgbsr[i])\n        filename = os.path.join(render_dir, '{:03d}_dec.png'.format(i))\n        imageio.imwrite(filename, rgb8)\n    imageio.mimwrite(os.path.join(render_dir, 'result_dec.mp4'), to8b(rgbsr), fps=25, codec='libx264', quality=8)"
        ]
    },
    {
        "func_name": "render_viewpoints",
        "original": "@torch.no_grad()\ndef render_viewpoints(self, render_poses, HW, Ks, render_kwargs, gt_imgs=None, savedir=None, dump_images=False, render_factor=0, eval_ssim=False, eval_lpips_alex=False, eval_lpips_vgg=False):\n    \"\"\"Render images for the given viewpoints; run evaluation if gt given.\n        \"\"\"\n    assert len(render_poses) == len(HW) and len(HW) == len(Ks)\n    if render_factor != 0:\n        HW = np.copy(HW)\n        Ks = np.copy(Ks)\n        HW = (HW / render_factor).astype(int)\n        Ks[:, :2, :3] /= render_factor\n    rgbs = []\n    rgb_features = []\n    depths = []\n    bgmaps = []\n    psnrs = []\n    viewdirs_all = []\n    ssims = []\n    lpips_alex = []\n    lpips_vgg = []\n    for (i, c2w) in enumerate(tqdm(render_poses)):\n        (H, W) = HW[i]\n        K = Ks[i]\n        c2w = torch.Tensor(c2w)\n        (rays_o, rays_d, viewdirs) = get_rays_of_a_view(H, W, K, c2w, self.ndc, inverse_y=False, flip_x=False, flip_y=False)\n        keys = ['rgb_marched', 'depth', 'alphainv_last', 'rgb_feature']\n        rays_o = rays_o.flatten(0, -2).to('cuda')\n        rays_d = rays_d.flatten(0, -2).to('cuda')\n        viewdirs = viewdirs.flatten(0, -2).to('cuda')\n        time_rdstart = time.time()\n        render_result_chunks = [{k: v for (k, v) in self.encoder(ro, rd, vd, **render_kwargs).items() if k in keys} for (ro, rd, vd) in zip(rays_o.split(self.test_ray_chunk, 0), rays_d.split(self.test_ray_chunk, 0), viewdirs.split(self.test_ray_chunk, 0))]\n        render_result = {k: torch.cat([ret[k] for ret in render_result_chunks]).reshape(H, W, -1) for k in render_result_chunks[0].keys()}\n        print(f'render time is: {time.time() - time_rdstart}')\n        rgb = render_result['rgb_marched'].clamp(0, 1).cpu().numpy()\n        rgb_feature = render_result['rgb_feature'].cpu().numpy()\n        depth = render_result['depth'].cpu().numpy()\n        bgmap = render_result['alphainv_last'].cpu().numpy()\n        rgbs.append(rgb)\n        rgb_features.append(rgb_feature)\n        depths.append(depth)\n        bgmaps.append(bgmap)\n        viewdirs_all.append(viewdirs)\n        if i == 0:\n            print('Testing', rgb.shape)\n        if gt_imgs is not None and render_factor == 0:\n            p = -10.0 * np.log10(np.mean(np.square(rgb - gt_imgs[i])))\n            psnrs.append(p)\n    if len(psnrs):\n        print('Testing psnr', np.mean(psnrs), '(avg)')\n        if eval_ssim:\n            print('Testing ssim', np.mean(ssims), '(avg)')\n        if eval_lpips_vgg:\n            print('Testing lpips (vgg)', np.mean(lpips_vgg), '(avg)')\n        if eval_lpips_alex:\n            print('Testing lpips (alex)', np.mean(lpips_alex), '(avg)')\n    if savedir is not None and dump_images:\n        for i in trange(len(rgbs)):\n            rgb8 = to8b(rgbs[i])\n            filename = os.path.join(savedir, '{:03d}_enc.png'.format(i))\n            imageio.imwrite(filename, rgb8)\n    rgbs = np.array(rgbs)\n    rgb_features = np.array(rgb_features)\n    depths = np.array(depths)\n    bgmaps = np.array(bgmaps)\n    return (rgbs, depths, bgmaps, psnrs, viewdirs_all, rgb_features)",
        "mutated": [
            "@torch.no_grad()\ndef render_viewpoints(self, render_poses, HW, Ks, render_kwargs, gt_imgs=None, savedir=None, dump_images=False, render_factor=0, eval_ssim=False, eval_lpips_alex=False, eval_lpips_vgg=False):\n    if False:\n        i = 10\n    'Render images for the given viewpoints; run evaluation if gt given.\\n        '\n    assert len(render_poses) == len(HW) and len(HW) == len(Ks)\n    if render_factor != 0:\n        HW = np.copy(HW)\n        Ks = np.copy(Ks)\n        HW = (HW / render_factor).astype(int)\n        Ks[:, :2, :3] /= render_factor\n    rgbs = []\n    rgb_features = []\n    depths = []\n    bgmaps = []\n    psnrs = []\n    viewdirs_all = []\n    ssims = []\n    lpips_alex = []\n    lpips_vgg = []\n    for (i, c2w) in enumerate(tqdm(render_poses)):\n        (H, W) = HW[i]\n        K = Ks[i]\n        c2w = torch.Tensor(c2w)\n        (rays_o, rays_d, viewdirs) = get_rays_of_a_view(H, W, K, c2w, self.ndc, inverse_y=False, flip_x=False, flip_y=False)\n        keys = ['rgb_marched', 'depth', 'alphainv_last', 'rgb_feature']\n        rays_o = rays_o.flatten(0, -2).to('cuda')\n        rays_d = rays_d.flatten(0, -2).to('cuda')\n        viewdirs = viewdirs.flatten(0, -2).to('cuda')\n        time_rdstart = time.time()\n        render_result_chunks = [{k: v for (k, v) in self.encoder(ro, rd, vd, **render_kwargs).items() if k in keys} for (ro, rd, vd) in zip(rays_o.split(self.test_ray_chunk, 0), rays_d.split(self.test_ray_chunk, 0), viewdirs.split(self.test_ray_chunk, 0))]\n        render_result = {k: torch.cat([ret[k] for ret in render_result_chunks]).reshape(H, W, -1) for k in render_result_chunks[0].keys()}\n        print(f'render time is: {time.time() - time_rdstart}')\n        rgb = render_result['rgb_marched'].clamp(0, 1).cpu().numpy()\n        rgb_feature = render_result['rgb_feature'].cpu().numpy()\n        depth = render_result['depth'].cpu().numpy()\n        bgmap = render_result['alphainv_last'].cpu().numpy()\n        rgbs.append(rgb)\n        rgb_features.append(rgb_feature)\n        depths.append(depth)\n        bgmaps.append(bgmap)\n        viewdirs_all.append(viewdirs)\n        if i == 0:\n            print('Testing', rgb.shape)\n        if gt_imgs is not None and render_factor == 0:\n            p = -10.0 * np.log10(np.mean(np.square(rgb - gt_imgs[i])))\n            psnrs.append(p)\n    if len(psnrs):\n        print('Testing psnr', np.mean(psnrs), '(avg)')\n        if eval_ssim:\n            print('Testing ssim', np.mean(ssims), '(avg)')\n        if eval_lpips_vgg:\n            print('Testing lpips (vgg)', np.mean(lpips_vgg), '(avg)')\n        if eval_lpips_alex:\n            print('Testing lpips (alex)', np.mean(lpips_alex), '(avg)')\n    if savedir is not None and dump_images:\n        for i in trange(len(rgbs)):\n            rgb8 = to8b(rgbs[i])\n            filename = os.path.join(savedir, '{:03d}_enc.png'.format(i))\n            imageio.imwrite(filename, rgb8)\n    rgbs = np.array(rgbs)\n    rgb_features = np.array(rgb_features)\n    depths = np.array(depths)\n    bgmaps = np.array(bgmaps)\n    return (rgbs, depths, bgmaps, psnrs, viewdirs_all, rgb_features)",
            "@torch.no_grad()\ndef render_viewpoints(self, render_poses, HW, Ks, render_kwargs, gt_imgs=None, savedir=None, dump_images=False, render_factor=0, eval_ssim=False, eval_lpips_alex=False, eval_lpips_vgg=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Render images for the given viewpoints; run evaluation if gt given.\\n        '\n    assert len(render_poses) == len(HW) and len(HW) == len(Ks)\n    if render_factor != 0:\n        HW = np.copy(HW)\n        Ks = np.copy(Ks)\n        HW = (HW / render_factor).astype(int)\n        Ks[:, :2, :3] /= render_factor\n    rgbs = []\n    rgb_features = []\n    depths = []\n    bgmaps = []\n    psnrs = []\n    viewdirs_all = []\n    ssims = []\n    lpips_alex = []\n    lpips_vgg = []\n    for (i, c2w) in enumerate(tqdm(render_poses)):\n        (H, W) = HW[i]\n        K = Ks[i]\n        c2w = torch.Tensor(c2w)\n        (rays_o, rays_d, viewdirs) = get_rays_of_a_view(H, W, K, c2w, self.ndc, inverse_y=False, flip_x=False, flip_y=False)\n        keys = ['rgb_marched', 'depth', 'alphainv_last', 'rgb_feature']\n        rays_o = rays_o.flatten(0, -2).to('cuda')\n        rays_d = rays_d.flatten(0, -2).to('cuda')\n        viewdirs = viewdirs.flatten(0, -2).to('cuda')\n        time_rdstart = time.time()\n        render_result_chunks = [{k: v for (k, v) in self.encoder(ro, rd, vd, **render_kwargs).items() if k in keys} for (ro, rd, vd) in zip(rays_o.split(self.test_ray_chunk, 0), rays_d.split(self.test_ray_chunk, 0), viewdirs.split(self.test_ray_chunk, 0))]\n        render_result = {k: torch.cat([ret[k] for ret in render_result_chunks]).reshape(H, W, -1) for k in render_result_chunks[0].keys()}\n        print(f'render time is: {time.time() - time_rdstart}')\n        rgb = render_result['rgb_marched'].clamp(0, 1).cpu().numpy()\n        rgb_feature = render_result['rgb_feature'].cpu().numpy()\n        depth = render_result['depth'].cpu().numpy()\n        bgmap = render_result['alphainv_last'].cpu().numpy()\n        rgbs.append(rgb)\n        rgb_features.append(rgb_feature)\n        depths.append(depth)\n        bgmaps.append(bgmap)\n        viewdirs_all.append(viewdirs)\n        if i == 0:\n            print('Testing', rgb.shape)\n        if gt_imgs is not None and render_factor == 0:\n            p = -10.0 * np.log10(np.mean(np.square(rgb - gt_imgs[i])))\n            psnrs.append(p)\n    if len(psnrs):\n        print('Testing psnr', np.mean(psnrs), '(avg)')\n        if eval_ssim:\n            print('Testing ssim', np.mean(ssims), '(avg)')\n        if eval_lpips_vgg:\n            print('Testing lpips (vgg)', np.mean(lpips_vgg), '(avg)')\n        if eval_lpips_alex:\n            print('Testing lpips (alex)', np.mean(lpips_alex), '(avg)')\n    if savedir is not None and dump_images:\n        for i in trange(len(rgbs)):\n            rgb8 = to8b(rgbs[i])\n            filename = os.path.join(savedir, '{:03d}_enc.png'.format(i))\n            imageio.imwrite(filename, rgb8)\n    rgbs = np.array(rgbs)\n    rgb_features = np.array(rgb_features)\n    depths = np.array(depths)\n    bgmaps = np.array(bgmaps)\n    return (rgbs, depths, bgmaps, psnrs, viewdirs_all, rgb_features)",
            "@torch.no_grad()\ndef render_viewpoints(self, render_poses, HW, Ks, render_kwargs, gt_imgs=None, savedir=None, dump_images=False, render_factor=0, eval_ssim=False, eval_lpips_alex=False, eval_lpips_vgg=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Render images for the given viewpoints; run evaluation if gt given.\\n        '\n    assert len(render_poses) == len(HW) and len(HW) == len(Ks)\n    if render_factor != 0:\n        HW = np.copy(HW)\n        Ks = np.copy(Ks)\n        HW = (HW / render_factor).astype(int)\n        Ks[:, :2, :3] /= render_factor\n    rgbs = []\n    rgb_features = []\n    depths = []\n    bgmaps = []\n    psnrs = []\n    viewdirs_all = []\n    ssims = []\n    lpips_alex = []\n    lpips_vgg = []\n    for (i, c2w) in enumerate(tqdm(render_poses)):\n        (H, W) = HW[i]\n        K = Ks[i]\n        c2w = torch.Tensor(c2w)\n        (rays_o, rays_d, viewdirs) = get_rays_of_a_view(H, W, K, c2w, self.ndc, inverse_y=False, flip_x=False, flip_y=False)\n        keys = ['rgb_marched', 'depth', 'alphainv_last', 'rgb_feature']\n        rays_o = rays_o.flatten(0, -2).to('cuda')\n        rays_d = rays_d.flatten(0, -2).to('cuda')\n        viewdirs = viewdirs.flatten(0, -2).to('cuda')\n        time_rdstart = time.time()\n        render_result_chunks = [{k: v for (k, v) in self.encoder(ro, rd, vd, **render_kwargs).items() if k in keys} for (ro, rd, vd) in zip(rays_o.split(self.test_ray_chunk, 0), rays_d.split(self.test_ray_chunk, 0), viewdirs.split(self.test_ray_chunk, 0))]\n        render_result = {k: torch.cat([ret[k] for ret in render_result_chunks]).reshape(H, W, -1) for k in render_result_chunks[0].keys()}\n        print(f'render time is: {time.time() - time_rdstart}')\n        rgb = render_result['rgb_marched'].clamp(0, 1).cpu().numpy()\n        rgb_feature = render_result['rgb_feature'].cpu().numpy()\n        depth = render_result['depth'].cpu().numpy()\n        bgmap = render_result['alphainv_last'].cpu().numpy()\n        rgbs.append(rgb)\n        rgb_features.append(rgb_feature)\n        depths.append(depth)\n        bgmaps.append(bgmap)\n        viewdirs_all.append(viewdirs)\n        if i == 0:\n            print('Testing', rgb.shape)\n        if gt_imgs is not None and render_factor == 0:\n            p = -10.0 * np.log10(np.mean(np.square(rgb - gt_imgs[i])))\n            psnrs.append(p)\n    if len(psnrs):\n        print('Testing psnr', np.mean(psnrs), '(avg)')\n        if eval_ssim:\n            print('Testing ssim', np.mean(ssims), '(avg)')\n        if eval_lpips_vgg:\n            print('Testing lpips (vgg)', np.mean(lpips_vgg), '(avg)')\n        if eval_lpips_alex:\n            print('Testing lpips (alex)', np.mean(lpips_alex), '(avg)')\n    if savedir is not None and dump_images:\n        for i in trange(len(rgbs)):\n            rgb8 = to8b(rgbs[i])\n            filename = os.path.join(savedir, '{:03d}_enc.png'.format(i))\n            imageio.imwrite(filename, rgb8)\n    rgbs = np.array(rgbs)\n    rgb_features = np.array(rgb_features)\n    depths = np.array(depths)\n    bgmaps = np.array(bgmaps)\n    return (rgbs, depths, bgmaps, psnrs, viewdirs_all, rgb_features)",
            "@torch.no_grad()\ndef render_viewpoints(self, render_poses, HW, Ks, render_kwargs, gt_imgs=None, savedir=None, dump_images=False, render_factor=0, eval_ssim=False, eval_lpips_alex=False, eval_lpips_vgg=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Render images for the given viewpoints; run evaluation if gt given.\\n        '\n    assert len(render_poses) == len(HW) and len(HW) == len(Ks)\n    if render_factor != 0:\n        HW = np.copy(HW)\n        Ks = np.copy(Ks)\n        HW = (HW / render_factor).astype(int)\n        Ks[:, :2, :3] /= render_factor\n    rgbs = []\n    rgb_features = []\n    depths = []\n    bgmaps = []\n    psnrs = []\n    viewdirs_all = []\n    ssims = []\n    lpips_alex = []\n    lpips_vgg = []\n    for (i, c2w) in enumerate(tqdm(render_poses)):\n        (H, W) = HW[i]\n        K = Ks[i]\n        c2w = torch.Tensor(c2w)\n        (rays_o, rays_d, viewdirs) = get_rays_of_a_view(H, W, K, c2w, self.ndc, inverse_y=False, flip_x=False, flip_y=False)\n        keys = ['rgb_marched', 'depth', 'alphainv_last', 'rgb_feature']\n        rays_o = rays_o.flatten(0, -2).to('cuda')\n        rays_d = rays_d.flatten(0, -2).to('cuda')\n        viewdirs = viewdirs.flatten(0, -2).to('cuda')\n        time_rdstart = time.time()\n        render_result_chunks = [{k: v for (k, v) in self.encoder(ro, rd, vd, **render_kwargs).items() if k in keys} for (ro, rd, vd) in zip(rays_o.split(self.test_ray_chunk, 0), rays_d.split(self.test_ray_chunk, 0), viewdirs.split(self.test_ray_chunk, 0))]\n        render_result = {k: torch.cat([ret[k] for ret in render_result_chunks]).reshape(H, W, -1) for k in render_result_chunks[0].keys()}\n        print(f'render time is: {time.time() - time_rdstart}')\n        rgb = render_result['rgb_marched'].clamp(0, 1).cpu().numpy()\n        rgb_feature = render_result['rgb_feature'].cpu().numpy()\n        depth = render_result['depth'].cpu().numpy()\n        bgmap = render_result['alphainv_last'].cpu().numpy()\n        rgbs.append(rgb)\n        rgb_features.append(rgb_feature)\n        depths.append(depth)\n        bgmaps.append(bgmap)\n        viewdirs_all.append(viewdirs)\n        if i == 0:\n            print('Testing', rgb.shape)\n        if gt_imgs is not None and render_factor == 0:\n            p = -10.0 * np.log10(np.mean(np.square(rgb - gt_imgs[i])))\n            psnrs.append(p)\n    if len(psnrs):\n        print('Testing psnr', np.mean(psnrs), '(avg)')\n        if eval_ssim:\n            print('Testing ssim', np.mean(ssims), '(avg)')\n        if eval_lpips_vgg:\n            print('Testing lpips (vgg)', np.mean(lpips_vgg), '(avg)')\n        if eval_lpips_alex:\n            print('Testing lpips (alex)', np.mean(lpips_alex), '(avg)')\n    if savedir is not None and dump_images:\n        for i in trange(len(rgbs)):\n            rgb8 = to8b(rgbs[i])\n            filename = os.path.join(savedir, '{:03d}_enc.png'.format(i))\n            imageio.imwrite(filename, rgb8)\n    rgbs = np.array(rgbs)\n    rgb_features = np.array(rgb_features)\n    depths = np.array(depths)\n    bgmaps = np.array(bgmaps)\n    return (rgbs, depths, bgmaps, psnrs, viewdirs_all, rgb_features)",
            "@torch.no_grad()\ndef render_viewpoints(self, render_poses, HW, Ks, render_kwargs, gt_imgs=None, savedir=None, dump_images=False, render_factor=0, eval_ssim=False, eval_lpips_alex=False, eval_lpips_vgg=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Render images for the given viewpoints; run evaluation if gt given.\\n        '\n    assert len(render_poses) == len(HW) and len(HW) == len(Ks)\n    if render_factor != 0:\n        HW = np.copy(HW)\n        Ks = np.copy(Ks)\n        HW = (HW / render_factor).astype(int)\n        Ks[:, :2, :3] /= render_factor\n    rgbs = []\n    rgb_features = []\n    depths = []\n    bgmaps = []\n    psnrs = []\n    viewdirs_all = []\n    ssims = []\n    lpips_alex = []\n    lpips_vgg = []\n    for (i, c2w) in enumerate(tqdm(render_poses)):\n        (H, W) = HW[i]\n        K = Ks[i]\n        c2w = torch.Tensor(c2w)\n        (rays_o, rays_d, viewdirs) = get_rays_of_a_view(H, W, K, c2w, self.ndc, inverse_y=False, flip_x=False, flip_y=False)\n        keys = ['rgb_marched', 'depth', 'alphainv_last', 'rgb_feature']\n        rays_o = rays_o.flatten(0, -2).to('cuda')\n        rays_d = rays_d.flatten(0, -2).to('cuda')\n        viewdirs = viewdirs.flatten(0, -2).to('cuda')\n        time_rdstart = time.time()\n        render_result_chunks = [{k: v for (k, v) in self.encoder(ro, rd, vd, **render_kwargs).items() if k in keys} for (ro, rd, vd) in zip(rays_o.split(self.test_ray_chunk, 0), rays_d.split(self.test_ray_chunk, 0), viewdirs.split(self.test_ray_chunk, 0))]\n        render_result = {k: torch.cat([ret[k] for ret in render_result_chunks]).reshape(H, W, -1) for k in render_result_chunks[0].keys()}\n        print(f'render time is: {time.time() - time_rdstart}')\n        rgb = render_result['rgb_marched'].clamp(0, 1).cpu().numpy()\n        rgb_feature = render_result['rgb_feature'].cpu().numpy()\n        depth = render_result['depth'].cpu().numpy()\n        bgmap = render_result['alphainv_last'].cpu().numpy()\n        rgbs.append(rgb)\n        rgb_features.append(rgb_feature)\n        depths.append(depth)\n        bgmaps.append(bgmap)\n        viewdirs_all.append(viewdirs)\n        if i == 0:\n            print('Testing', rgb.shape)\n        if gt_imgs is not None and render_factor == 0:\n            p = -10.0 * np.log10(np.mean(np.square(rgb - gt_imgs[i])))\n            psnrs.append(p)\n    if len(psnrs):\n        print('Testing psnr', np.mean(psnrs), '(avg)')\n        if eval_ssim:\n            print('Testing ssim', np.mean(ssims), '(avg)')\n        if eval_lpips_vgg:\n            print('Testing lpips (vgg)', np.mean(lpips_vgg), '(avg)')\n        if eval_lpips_alex:\n            print('Testing lpips (alex)', np.mean(lpips_alex), '(avg)')\n    if savedir is not None and dump_images:\n        for i in trange(len(rgbs)):\n            rgb8 = to8b(rgbs[i])\n            filename = os.path.join(savedir, '{:03d}_enc.png'.format(i))\n            imageio.imwrite(filename, rgb8)\n    rgbs = np.array(rgbs)\n    rgb_features = np.array(rgb_features)\n    depths = np.array(depths)\n    bgmaps = np.array(bgmaps)\n    return (rgbs, depths, bgmaps, psnrs, viewdirs_all, rgb_features)"
        ]
    },
    {
        "func_name": "load_everything",
        "original": "def load_everything(cfg_data):\n    \"\"\"Load images / poses / camera settings / data split.\n    \"\"\"\n    cfg_data = mmcv.Config(cfg_data)\n    data_dict = load_data(cfg_data)\n    kept_keys = {'hwf', 'HW', 'Ks', 'near', 'far', 'near_clip', 'i_train', 'i_val', 'i_test', 'irregular_shape', 'poses', 'render_poses', 'images', 'white_bkgd'}\n    kept_keys.add('srgt')\n    kept_keys.add('w2c')\n    data_dict['srgt'] = torch.FloatTensor(data_dict['srgt'], device='cpu')\n    data_dict['w2c'] = torch.FloatTensor(data_dict['w2c'], device='cpu')\n    for k in list(data_dict.keys()):\n        if k not in kept_keys:\n            data_dict.pop(k)\n    if data_dict['irregular_shape']:\n        data_dict['images'] = [torch.FloatTensor(im, device='cpu') for im in data_dict['images']]\n    else:\n        data_dict['images'] = torch.FloatTensor(data_dict['images'], device='cpu')\n    data_dict['poses'] = torch.Tensor(data_dict['poses'])\n    return data_dict",
        "mutated": [
            "def load_everything(cfg_data):\n    if False:\n        i = 10\n    'Load images / poses / camera settings / data split.\\n    '\n    cfg_data = mmcv.Config(cfg_data)\n    data_dict = load_data(cfg_data)\n    kept_keys = {'hwf', 'HW', 'Ks', 'near', 'far', 'near_clip', 'i_train', 'i_val', 'i_test', 'irregular_shape', 'poses', 'render_poses', 'images', 'white_bkgd'}\n    kept_keys.add('srgt')\n    kept_keys.add('w2c')\n    data_dict['srgt'] = torch.FloatTensor(data_dict['srgt'], device='cpu')\n    data_dict['w2c'] = torch.FloatTensor(data_dict['w2c'], device='cpu')\n    for k in list(data_dict.keys()):\n        if k not in kept_keys:\n            data_dict.pop(k)\n    if data_dict['irregular_shape']:\n        data_dict['images'] = [torch.FloatTensor(im, device='cpu') for im in data_dict['images']]\n    else:\n        data_dict['images'] = torch.FloatTensor(data_dict['images'], device='cpu')\n    data_dict['poses'] = torch.Tensor(data_dict['poses'])\n    return data_dict",
            "def load_everything(cfg_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Load images / poses / camera settings / data split.\\n    '\n    cfg_data = mmcv.Config(cfg_data)\n    data_dict = load_data(cfg_data)\n    kept_keys = {'hwf', 'HW', 'Ks', 'near', 'far', 'near_clip', 'i_train', 'i_val', 'i_test', 'irregular_shape', 'poses', 'render_poses', 'images', 'white_bkgd'}\n    kept_keys.add('srgt')\n    kept_keys.add('w2c')\n    data_dict['srgt'] = torch.FloatTensor(data_dict['srgt'], device='cpu')\n    data_dict['w2c'] = torch.FloatTensor(data_dict['w2c'], device='cpu')\n    for k in list(data_dict.keys()):\n        if k not in kept_keys:\n            data_dict.pop(k)\n    if data_dict['irregular_shape']:\n        data_dict['images'] = [torch.FloatTensor(im, device='cpu') for im in data_dict['images']]\n    else:\n        data_dict['images'] = torch.FloatTensor(data_dict['images'], device='cpu')\n    data_dict['poses'] = torch.Tensor(data_dict['poses'])\n    return data_dict",
            "def load_everything(cfg_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Load images / poses / camera settings / data split.\\n    '\n    cfg_data = mmcv.Config(cfg_data)\n    data_dict = load_data(cfg_data)\n    kept_keys = {'hwf', 'HW', 'Ks', 'near', 'far', 'near_clip', 'i_train', 'i_val', 'i_test', 'irregular_shape', 'poses', 'render_poses', 'images', 'white_bkgd'}\n    kept_keys.add('srgt')\n    kept_keys.add('w2c')\n    data_dict['srgt'] = torch.FloatTensor(data_dict['srgt'], device='cpu')\n    data_dict['w2c'] = torch.FloatTensor(data_dict['w2c'], device='cpu')\n    for k in list(data_dict.keys()):\n        if k not in kept_keys:\n            data_dict.pop(k)\n    if data_dict['irregular_shape']:\n        data_dict['images'] = [torch.FloatTensor(im, device='cpu') for im in data_dict['images']]\n    else:\n        data_dict['images'] = torch.FloatTensor(data_dict['images'], device='cpu')\n    data_dict['poses'] = torch.Tensor(data_dict['poses'])\n    return data_dict",
            "def load_everything(cfg_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Load images / poses / camera settings / data split.\\n    '\n    cfg_data = mmcv.Config(cfg_data)\n    data_dict = load_data(cfg_data)\n    kept_keys = {'hwf', 'HW', 'Ks', 'near', 'far', 'near_clip', 'i_train', 'i_val', 'i_test', 'irregular_shape', 'poses', 'render_poses', 'images', 'white_bkgd'}\n    kept_keys.add('srgt')\n    kept_keys.add('w2c')\n    data_dict['srgt'] = torch.FloatTensor(data_dict['srgt'], device='cpu')\n    data_dict['w2c'] = torch.FloatTensor(data_dict['w2c'], device='cpu')\n    for k in list(data_dict.keys()):\n        if k not in kept_keys:\n            data_dict.pop(k)\n    if data_dict['irregular_shape']:\n        data_dict['images'] = [torch.FloatTensor(im, device='cpu') for im in data_dict['images']]\n    else:\n        data_dict['images'] = torch.FloatTensor(data_dict['images'], device='cpu')\n    data_dict['poses'] = torch.Tensor(data_dict['poses'])\n    return data_dict",
            "def load_everything(cfg_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Load images / poses / camera settings / data split.\\n    '\n    cfg_data = mmcv.Config(cfg_data)\n    data_dict = load_data(cfg_data)\n    kept_keys = {'hwf', 'HW', 'Ks', 'near', 'far', 'near_clip', 'i_train', 'i_val', 'i_test', 'irregular_shape', 'poses', 'render_poses', 'images', 'white_bkgd'}\n    kept_keys.add('srgt')\n    kept_keys.add('w2c')\n    data_dict['srgt'] = torch.FloatTensor(data_dict['srgt'], device='cpu')\n    data_dict['w2c'] = torch.FloatTensor(data_dict['w2c'], device='cpu')\n    for k in list(data_dict.keys()):\n        if k not in kept_keys:\n            data_dict.pop(k)\n    if data_dict['irregular_shape']:\n        data_dict['images'] = [torch.FloatTensor(im, device='cpu') for im in data_dict['images']]\n    else:\n        data_dict['images'] = torch.FloatTensor(data_dict['images'], device='cpu')\n    data_dict['poses'] = torch.Tensor(data_dict['poses'])\n    return data_dict"
        ]
    }
]