[
    {
        "func_name": "_copy_objects",
        "original": "@engine.dispatch_on_engine\ndef _copy_objects(s3_client: Optional['S3Client'], batch: List[Tuple[str, str]], use_threads: Union[bool, int], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    _logger.debug('Copying %s objects', len(batch))\n    s3_client = s3_client if s3_client else _utils.client(service_name='s3')\n    for (source, target) in batch:\n        (source_bucket, source_key) = _utils.parse_path(path=source)\n        copy_source: CopySourceTypeDef = {'Bucket': source_bucket, 'Key': source_key}\n        (target_bucket, target_key) = _utils.parse_path(path=target)\n        s3_client.copy(CopySource=copy_source, Bucket=target_bucket, Key=target_key, ExtraArgs=s3_additional_kwargs, Config=TransferConfig(num_download_attempts=10, use_threads=use_threads))",
        "mutated": [
            "@engine.dispatch_on_engine\ndef _copy_objects(s3_client: Optional['S3Client'], batch: List[Tuple[str, str]], use_threads: Union[bool, int], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    if False:\n        i = 10\n    _logger.debug('Copying %s objects', len(batch))\n    s3_client = s3_client if s3_client else _utils.client(service_name='s3')\n    for (source, target) in batch:\n        (source_bucket, source_key) = _utils.parse_path(path=source)\n        copy_source: CopySourceTypeDef = {'Bucket': source_bucket, 'Key': source_key}\n        (target_bucket, target_key) = _utils.parse_path(path=target)\n        s3_client.copy(CopySource=copy_source, Bucket=target_bucket, Key=target_key, ExtraArgs=s3_additional_kwargs, Config=TransferConfig(num_download_attempts=10, use_threads=use_threads))",
            "@engine.dispatch_on_engine\ndef _copy_objects(s3_client: Optional['S3Client'], batch: List[Tuple[str, str]], use_threads: Union[bool, int], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    _logger.debug('Copying %s objects', len(batch))\n    s3_client = s3_client if s3_client else _utils.client(service_name='s3')\n    for (source, target) in batch:\n        (source_bucket, source_key) = _utils.parse_path(path=source)\n        copy_source: CopySourceTypeDef = {'Bucket': source_bucket, 'Key': source_key}\n        (target_bucket, target_key) = _utils.parse_path(path=target)\n        s3_client.copy(CopySource=copy_source, Bucket=target_bucket, Key=target_key, ExtraArgs=s3_additional_kwargs, Config=TransferConfig(num_download_attempts=10, use_threads=use_threads))",
            "@engine.dispatch_on_engine\ndef _copy_objects(s3_client: Optional['S3Client'], batch: List[Tuple[str, str]], use_threads: Union[bool, int], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    _logger.debug('Copying %s objects', len(batch))\n    s3_client = s3_client if s3_client else _utils.client(service_name='s3')\n    for (source, target) in batch:\n        (source_bucket, source_key) = _utils.parse_path(path=source)\n        copy_source: CopySourceTypeDef = {'Bucket': source_bucket, 'Key': source_key}\n        (target_bucket, target_key) = _utils.parse_path(path=target)\n        s3_client.copy(CopySource=copy_source, Bucket=target_bucket, Key=target_key, ExtraArgs=s3_additional_kwargs, Config=TransferConfig(num_download_attempts=10, use_threads=use_threads))",
            "@engine.dispatch_on_engine\ndef _copy_objects(s3_client: Optional['S3Client'], batch: List[Tuple[str, str]], use_threads: Union[bool, int], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    _logger.debug('Copying %s objects', len(batch))\n    s3_client = s3_client if s3_client else _utils.client(service_name='s3')\n    for (source, target) in batch:\n        (source_bucket, source_key) = _utils.parse_path(path=source)\n        copy_source: CopySourceTypeDef = {'Bucket': source_bucket, 'Key': source_key}\n        (target_bucket, target_key) = _utils.parse_path(path=target)\n        s3_client.copy(CopySource=copy_source, Bucket=target_bucket, Key=target_key, ExtraArgs=s3_additional_kwargs, Config=TransferConfig(num_download_attempts=10, use_threads=use_threads))",
            "@engine.dispatch_on_engine\ndef _copy_objects(s3_client: Optional['S3Client'], batch: List[Tuple[str, str]], use_threads: Union[bool, int], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    _logger.debug('Copying %s objects', len(batch))\n    s3_client = s3_client if s3_client else _utils.client(service_name='s3')\n    for (source, target) in batch:\n        (source_bucket, source_key) = _utils.parse_path(path=source)\n        copy_source: CopySourceTypeDef = {'Bucket': source_bucket, 'Key': source_key}\n        (target_bucket, target_key) = _utils.parse_path(path=target)\n        s3_client.copy(CopySource=copy_source, Bucket=target_bucket, Key=target_key, ExtraArgs=s3_additional_kwargs, Config=TransferConfig(num_download_attempts=10, use_threads=use_threads))"
        ]
    },
    {
        "func_name": "_copy",
        "original": "def _copy(batches: List[List[Tuple[str, str]]], use_threads: Union[bool, int], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if s3_additional_kwargs is None:\n        boto3_kwargs: Optional[Dict[str, Any]] = None\n    else:\n        boto3_kwargs = get_botocore_valid_kwargs(function_name='copy_object', s3_additional_kwargs=s3_additional_kwargs)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    ray_get(executor.map(_copy_objects, s3_client, batches, itertools.repeat(use_threads), itertools.repeat(boto3_kwargs)))",
        "mutated": [
            "def _copy(batches: List[List[Tuple[str, str]]], use_threads: Union[bool, int], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    if False:\n        i = 10\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if s3_additional_kwargs is None:\n        boto3_kwargs: Optional[Dict[str, Any]] = None\n    else:\n        boto3_kwargs = get_botocore_valid_kwargs(function_name='copy_object', s3_additional_kwargs=s3_additional_kwargs)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    ray_get(executor.map(_copy_objects, s3_client, batches, itertools.repeat(use_threads), itertools.repeat(boto3_kwargs)))",
            "def _copy(batches: List[List[Tuple[str, str]]], use_threads: Union[bool, int], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if s3_additional_kwargs is None:\n        boto3_kwargs: Optional[Dict[str, Any]] = None\n    else:\n        boto3_kwargs = get_botocore_valid_kwargs(function_name='copy_object', s3_additional_kwargs=s3_additional_kwargs)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    ray_get(executor.map(_copy_objects, s3_client, batches, itertools.repeat(use_threads), itertools.repeat(boto3_kwargs)))",
            "def _copy(batches: List[List[Tuple[str, str]]], use_threads: Union[bool, int], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if s3_additional_kwargs is None:\n        boto3_kwargs: Optional[Dict[str, Any]] = None\n    else:\n        boto3_kwargs = get_botocore_valid_kwargs(function_name='copy_object', s3_additional_kwargs=s3_additional_kwargs)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    ray_get(executor.map(_copy_objects, s3_client, batches, itertools.repeat(use_threads), itertools.repeat(boto3_kwargs)))",
            "def _copy(batches: List[List[Tuple[str, str]]], use_threads: Union[bool, int], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if s3_additional_kwargs is None:\n        boto3_kwargs: Optional[Dict[str, Any]] = None\n    else:\n        boto3_kwargs = get_botocore_valid_kwargs(function_name='copy_object', s3_additional_kwargs=s3_additional_kwargs)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    ray_get(executor.map(_copy_objects, s3_client, batches, itertools.repeat(use_threads), itertools.repeat(boto3_kwargs)))",
            "def _copy(batches: List[List[Tuple[str, str]]], use_threads: Union[bool, int], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if s3_additional_kwargs is None:\n        boto3_kwargs: Optional[Dict[str, Any]] = None\n    else:\n        boto3_kwargs = get_botocore_valid_kwargs(function_name='copy_object', s3_additional_kwargs=s3_additional_kwargs)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    ray_get(executor.map(_copy_objects, s3_client, batches, itertools.repeat(use_threads), itertools.repeat(boto3_kwargs)))"
        ]
    },
    {
        "func_name": "merge_datasets",
        "original": "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef merge_datasets(source_path: str, target_path: str, mode: Literal['append', 'overwrite', 'overwrite_partitions']='append', ignore_empty: bool=False, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    \"\"\"Merge a source dataset into a target dataset.\n\n    This function accepts Unix shell-style wildcards in the source_path argument.\n    * (matches everything), ? (matches any single character),\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\n    you can use `glob.escape(source_path)` before passing the path to this function.\n\n    Note\n    ----\n    If you are merging tables (S3 datasets + Glue Catalog metadata),\n    remember that you will also need to update your partitions metadata in some cases.\n    (e.g. wr.athena.repair_table(table='...', database='...'))\n\n    Note\n    ----\n    In case of `use_threads=True` the number of threads\n    that will be spawned will be gotten from os.cpu_count().\n\n    Parameters\n    ----------\n    source_path : str,\n        S3 Path for the source directory.\n    target_path : str,\n        S3 Path for the target directory.\n    mode: str, optional\n        ``append`` (Default), ``overwrite``, ``overwrite_partitions``.\n    ignore_empty: bool\n        Ignore files with 0 bytes.\n    use_threads : bool, int\n        True to enable concurrent requests, False to disable multiple threads.\n        If enabled os.cpu_count() will be used as the max number of threads.\n        If integer is provided, specified number is used.\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    s3_additional_kwargs : Optional[Dict[str, Any]]\n        Forwarded to botocore requests.\n        e.g. s3_additional_kwargs={'ServerSideEncryption': 'aws:kms', 'SSEKMSKeyId': 'YOUR_KMS_KEY_ARN'}\n\n    Returns\n    -------\n    List[str]\n        List of new objects paths.\n\n    Examples\n    --------\n    Merging\n\n    >>> import awswrangler as wr\n    >>> wr.s3.merge_datasets(\n    ...     source_path=\"s3://bucket0/dir0/\",\n    ...     target_path=\"s3://bucket1/dir1/\",\n    ...     mode=\"append\"\n    ... )\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\n\n    Merging with a KMS key\n\n    >>> import awswrangler as wr\n    >>> wr.s3.merge_datasets(\n    ...     source_path=\"s3://bucket0/dir0/\",\n    ...     target_path=\"s3://bucket1/dir1/\",\n    ...     mode=\"append\",\n    ...     s3_additional_kwargs={\n    ...         'ServerSideEncryption': 'aws:kms',\n    ...         'SSEKMSKeyId': 'YOUR_KMS_KEY_ARN'\n    ...     }\n    ... )\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\n\n    \"\"\"\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    paths: List[str] = list_objects(path=f'{source_path}/', ignore_empty=ignore_empty, boto3_session=boto3_session)\n    if len(paths) < 1:\n        return []\n    if mode == 'overwrite':\n        _logger.debug('Deleting to overwrite: %s/', target_path)\n        delete_objects(path=f'{target_path}/', use_threads=use_threads, boto3_session=boto3_session)\n    elif mode == 'overwrite_partitions':\n        paths_wo_prefix: List[str] = [x.replace(f'{source_path}/', '') for x in paths]\n        paths_wo_filename: List[str] = [f\"{x.rpartition('/')[0]}/\" for x in paths_wo_prefix]\n        partitions_paths: List[str] = list(set(paths_wo_filename))\n        target_partitions_paths = [f'{target_path}/{x}' for x in partitions_paths]\n        for path in target_partitions_paths:\n            _logger.debug('Deleting to overwrite_partitions: %s', path)\n            delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session)\n    elif mode != 'append':\n        raise exceptions.InvalidArgumentValue(f'{mode} is a invalid mode option.')\n    new_objects: List[str] = copy_objects(paths=paths, source_path=source_path, target_path=target_path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects",
        "mutated": [
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef merge_datasets(source_path: str, target_path: str, mode: Literal['append', 'overwrite', 'overwrite_partitions']='append', ignore_empty: bool=False, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    if False:\n        i = 10\n    'Merge a source dataset into a target dataset.\\n\\n    This function accepts Unix shell-style wildcards in the source_path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(source_path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    If you are merging tables (S3 datasets + Glue Catalog metadata),\\n    remember that you will also need to update your partitions metadata in some cases.\\n    (e.g. wr.athena.repair_table(table=\\'...\\', database=\\'...\\'))\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    source_path : str,\\n        S3 Path for the source directory.\\n    target_path : str,\\n        S3 Path for the target directory.\\n    mode: str, optional\\n        ``append`` (Default), ``overwrite``, ``overwrite_partitions``.\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forwarded to botocore requests.\\n        e.g. s3_additional_kwargs={\\'ServerSideEncryption\\': \\'aws:kms\\', \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'}\\n\\n    Returns\\n    -------\\n    List[str]\\n        List of new objects paths.\\n\\n    Examples\\n    --------\\n    Merging\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.merge_datasets(\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     mode=\"append\"\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    Merging with a KMS key\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.merge_datasets(\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     mode=\"append\",\\n    ...     s3_additional_kwargs={\\n    ...         \\'ServerSideEncryption\\': \\'aws:kms\\',\\n    ...         \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'\\n    ...     }\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    '\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    paths: List[str] = list_objects(path=f'{source_path}/', ignore_empty=ignore_empty, boto3_session=boto3_session)\n    if len(paths) < 1:\n        return []\n    if mode == 'overwrite':\n        _logger.debug('Deleting to overwrite: %s/', target_path)\n        delete_objects(path=f'{target_path}/', use_threads=use_threads, boto3_session=boto3_session)\n    elif mode == 'overwrite_partitions':\n        paths_wo_prefix: List[str] = [x.replace(f'{source_path}/', '') for x in paths]\n        paths_wo_filename: List[str] = [f\"{x.rpartition('/')[0]}/\" for x in paths_wo_prefix]\n        partitions_paths: List[str] = list(set(paths_wo_filename))\n        target_partitions_paths = [f'{target_path}/{x}' for x in partitions_paths]\n        for path in target_partitions_paths:\n            _logger.debug('Deleting to overwrite_partitions: %s', path)\n            delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session)\n    elif mode != 'append':\n        raise exceptions.InvalidArgumentValue(f'{mode} is a invalid mode option.')\n    new_objects: List[str] = copy_objects(paths=paths, source_path=source_path, target_path=target_path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef merge_datasets(source_path: str, target_path: str, mode: Literal['append', 'overwrite', 'overwrite_partitions']='append', ignore_empty: bool=False, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Merge a source dataset into a target dataset.\\n\\n    This function accepts Unix shell-style wildcards in the source_path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(source_path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    If you are merging tables (S3 datasets + Glue Catalog metadata),\\n    remember that you will also need to update your partitions metadata in some cases.\\n    (e.g. wr.athena.repair_table(table=\\'...\\', database=\\'...\\'))\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    source_path : str,\\n        S3 Path for the source directory.\\n    target_path : str,\\n        S3 Path for the target directory.\\n    mode: str, optional\\n        ``append`` (Default), ``overwrite``, ``overwrite_partitions``.\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forwarded to botocore requests.\\n        e.g. s3_additional_kwargs={\\'ServerSideEncryption\\': \\'aws:kms\\', \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'}\\n\\n    Returns\\n    -------\\n    List[str]\\n        List of new objects paths.\\n\\n    Examples\\n    --------\\n    Merging\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.merge_datasets(\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     mode=\"append\"\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    Merging with a KMS key\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.merge_datasets(\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     mode=\"append\",\\n    ...     s3_additional_kwargs={\\n    ...         \\'ServerSideEncryption\\': \\'aws:kms\\',\\n    ...         \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'\\n    ...     }\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    '\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    paths: List[str] = list_objects(path=f'{source_path}/', ignore_empty=ignore_empty, boto3_session=boto3_session)\n    if len(paths) < 1:\n        return []\n    if mode == 'overwrite':\n        _logger.debug('Deleting to overwrite: %s/', target_path)\n        delete_objects(path=f'{target_path}/', use_threads=use_threads, boto3_session=boto3_session)\n    elif mode == 'overwrite_partitions':\n        paths_wo_prefix: List[str] = [x.replace(f'{source_path}/', '') for x in paths]\n        paths_wo_filename: List[str] = [f\"{x.rpartition('/')[0]}/\" for x in paths_wo_prefix]\n        partitions_paths: List[str] = list(set(paths_wo_filename))\n        target_partitions_paths = [f'{target_path}/{x}' for x in partitions_paths]\n        for path in target_partitions_paths:\n            _logger.debug('Deleting to overwrite_partitions: %s', path)\n            delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session)\n    elif mode != 'append':\n        raise exceptions.InvalidArgumentValue(f'{mode} is a invalid mode option.')\n    new_objects: List[str] = copy_objects(paths=paths, source_path=source_path, target_path=target_path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef merge_datasets(source_path: str, target_path: str, mode: Literal['append', 'overwrite', 'overwrite_partitions']='append', ignore_empty: bool=False, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Merge a source dataset into a target dataset.\\n\\n    This function accepts Unix shell-style wildcards in the source_path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(source_path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    If you are merging tables (S3 datasets + Glue Catalog metadata),\\n    remember that you will also need to update your partitions metadata in some cases.\\n    (e.g. wr.athena.repair_table(table=\\'...\\', database=\\'...\\'))\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    source_path : str,\\n        S3 Path for the source directory.\\n    target_path : str,\\n        S3 Path for the target directory.\\n    mode: str, optional\\n        ``append`` (Default), ``overwrite``, ``overwrite_partitions``.\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forwarded to botocore requests.\\n        e.g. s3_additional_kwargs={\\'ServerSideEncryption\\': \\'aws:kms\\', \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'}\\n\\n    Returns\\n    -------\\n    List[str]\\n        List of new objects paths.\\n\\n    Examples\\n    --------\\n    Merging\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.merge_datasets(\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     mode=\"append\"\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    Merging with a KMS key\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.merge_datasets(\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     mode=\"append\",\\n    ...     s3_additional_kwargs={\\n    ...         \\'ServerSideEncryption\\': \\'aws:kms\\',\\n    ...         \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'\\n    ...     }\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    '\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    paths: List[str] = list_objects(path=f'{source_path}/', ignore_empty=ignore_empty, boto3_session=boto3_session)\n    if len(paths) < 1:\n        return []\n    if mode == 'overwrite':\n        _logger.debug('Deleting to overwrite: %s/', target_path)\n        delete_objects(path=f'{target_path}/', use_threads=use_threads, boto3_session=boto3_session)\n    elif mode == 'overwrite_partitions':\n        paths_wo_prefix: List[str] = [x.replace(f'{source_path}/', '') for x in paths]\n        paths_wo_filename: List[str] = [f\"{x.rpartition('/')[0]}/\" for x in paths_wo_prefix]\n        partitions_paths: List[str] = list(set(paths_wo_filename))\n        target_partitions_paths = [f'{target_path}/{x}' for x in partitions_paths]\n        for path in target_partitions_paths:\n            _logger.debug('Deleting to overwrite_partitions: %s', path)\n            delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session)\n    elif mode != 'append':\n        raise exceptions.InvalidArgumentValue(f'{mode} is a invalid mode option.')\n    new_objects: List[str] = copy_objects(paths=paths, source_path=source_path, target_path=target_path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef merge_datasets(source_path: str, target_path: str, mode: Literal['append', 'overwrite', 'overwrite_partitions']='append', ignore_empty: bool=False, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Merge a source dataset into a target dataset.\\n\\n    This function accepts Unix shell-style wildcards in the source_path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(source_path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    If you are merging tables (S3 datasets + Glue Catalog metadata),\\n    remember that you will also need to update your partitions metadata in some cases.\\n    (e.g. wr.athena.repair_table(table=\\'...\\', database=\\'...\\'))\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    source_path : str,\\n        S3 Path for the source directory.\\n    target_path : str,\\n        S3 Path for the target directory.\\n    mode: str, optional\\n        ``append`` (Default), ``overwrite``, ``overwrite_partitions``.\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forwarded to botocore requests.\\n        e.g. s3_additional_kwargs={\\'ServerSideEncryption\\': \\'aws:kms\\', \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'}\\n\\n    Returns\\n    -------\\n    List[str]\\n        List of new objects paths.\\n\\n    Examples\\n    --------\\n    Merging\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.merge_datasets(\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     mode=\"append\"\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    Merging with a KMS key\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.merge_datasets(\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     mode=\"append\",\\n    ...     s3_additional_kwargs={\\n    ...         \\'ServerSideEncryption\\': \\'aws:kms\\',\\n    ...         \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'\\n    ...     }\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    '\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    paths: List[str] = list_objects(path=f'{source_path}/', ignore_empty=ignore_empty, boto3_session=boto3_session)\n    if len(paths) < 1:\n        return []\n    if mode == 'overwrite':\n        _logger.debug('Deleting to overwrite: %s/', target_path)\n        delete_objects(path=f'{target_path}/', use_threads=use_threads, boto3_session=boto3_session)\n    elif mode == 'overwrite_partitions':\n        paths_wo_prefix: List[str] = [x.replace(f'{source_path}/', '') for x in paths]\n        paths_wo_filename: List[str] = [f\"{x.rpartition('/')[0]}/\" for x in paths_wo_prefix]\n        partitions_paths: List[str] = list(set(paths_wo_filename))\n        target_partitions_paths = [f'{target_path}/{x}' for x in partitions_paths]\n        for path in target_partitions_paths:\n            _logger.debug('Deleting to overwrite_partitions: %s', path)\n            delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session)\n    elif mode != 'append':\n        raise exceptions.InvalidArgumentValue(f'{mode} is a invalid mode option.')\n    new_objects: List[str] = copy_objects(paths=paths, source_path=source_path, target_path=target_path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef merge_datasets(source_path: str, target_path: str, mode: Literal['append', 'overwrite', 'overwrite_partitions']='append', ignore_empty: bool=False, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Merge a source dataset into a target dataset.\\n\\n    This function accepts Unix shell-style wildcards in the source_path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(source_path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    If you are merging tables (S3 datasets + Glue Catalog metadata),\\n    remember that you will also need to update your partitions metadata in some cases.\\n    (e.g. wr.athena.repair_table(table=\\'...\\', database=\\'...\\'))\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    source_path : str,\\n        S3 Path for the source directory.\\n    target_path : str,\\n        S3 Path for the target directory.\\n    mode: str, optional\\n        ``append`` (Default), ``overwrite``, ``overwrite_partitions``.\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forwarded to botocore requests.\\n        e.g. s3_additional_kwargs={\\'ServerSideEncryption\\': \\'aws:kms\\', \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'}\\n\\n    Returns\\n    -------\\n    List[str]\\n        List of new objects paths.\\n\\n    Examples\\n    --------\\n    Merging\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.merge_datasets(\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     mode=\"append\"\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    Merging with a KMS key\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.merge_datasets(\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     mode=\"append\",\\n    ...     s3_additional_kwargs={\\n    ...         \\'ServerSideEncryption\\': \\'aws:kms\\',\\n    ...         \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'\\n    ...     }\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    '\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    paths: List[str] = list_objects(path=f'{source_path}/', ignore_empty=ignore_empty, boto3_session=boto3_session)\n    if len(paths) < 1:\n        return []\n    if mode == 'overwrite':\n        _logger.debug('Deleting to overwrite: %s/', target_path)\n        delete_objects(path=f'{target_path}/', use_threads=use_threads, boto3_session=boto3_session)\n    elif mode == 'overwrite_partitions':\n        paths_wo_prefix: List[str] = [x.replace(f'{source_path}/', '') for x in paths]\n        paths_wo_filename: List[str] = [f\"{x.rpartition('/')[0]}/\" for x in paths_wo_prefix]\n        partitions_paths: List[str] = list(set(paths_wo_filename))\n        target_partitions_paths = [f'{target_path}/{x}' for x in partitions_paths]\n        for path in target_partitions_paths:\n            _logger.debug('Deleting to overwrite_partitions: %s', path)\n            delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session)\n    elif mode != 'append':\n        raise exceptions.InvalidArgumentValue(f'{mode} is a invalid mode option.')\n    new_objects: List[str] = copy_objects(paths=paths, source_path=source_path, target_path=target_path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects"
        ]
    },
    {
        "func_name": "copy_objects",
        "original": "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef copy_objects(paths: List[str], source_path: str, target_path: str, replace_filenames: Optional[Dict[str, str]]=None, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    \"\"\"Copy a list of S3 objects to another S3 directory.\n\n    Note\n    ----\n    In case of `use_threads=True` the number of threads\n    that will be spawned will be gotten from os.cpu_count().\n\n    Parameters\n    ----------\n    paths : List[str]\n        List of S3 objects paths (e.g. [s3://bucket/dir0/key0, s3://bucket/dir0/key1]).\n    source_path : str,\n        S3 Path for the source directory.\n    target_path : str,\n        S3 Path for the target directory.\n    replace_filenames : Dict[str, str], optional\n        e.g. {\"old_name.csv\": \"new_name.csv\", \"old_name2.csv\": \"new_name2.csv\"}\n    use_threads : bool, int\n        True to enable concurrent requests, False to disable multiple threads.\n        If enabled os.cpu_count() will be used as the max number of threads.\n        If integer is provided, specified number is used.\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    s3_additional_kwargs : Optional[Dict[str, Any]]\n        Forwarded to botocore requests.\n        e.g. s3_additional_kwargs={'ServerSideEncryption': 'aws:kms', 'SSEKMSKeyId': 'YOUR_KMS_KEY_ARN'}\n\n    Returns\n    -------\n    List[str]\n        List of new objects paths.\n\n    Examples\n    --------\n    Copying\n\n    >>> import awswrangler as wr\n    >>> wr.s3.copy_objects(\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\n    ...     source_path=\"s3://bucket0/dir0/\",\n    ...     target_path=\"s3://bucket1/dir1/\"\n    ... )\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\n\n    Copying with a KMS key\n\n    >>> import awswrangler as wr\n    >>> wr.s3.copy_objects(\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\n    ...     source_path=\"s3://bucket0/dir0/\",\n    ...     target_path=\"s3://bucket1/dir1/\",\n    ...     s3_additional_kwargs={\n    ...         'ServerSideEncryption': 'aws:kms',\n    ...         'SSEKMSKeyId': 'YOUR_KMS_KEY_ARN'\n    ...     }\n    ... )\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\n\n    \"\"\"\n    if len(paths) < 1:\n        return []\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    batch: List[Tuple[str, str]] = []\n    new_objects: List[str] = []\n    for path in paths:\n        path_wo_prefix: str = path.replace(f'{source_path}/', '')\n        path_final: str = f'{target_path}/{path_wo_prefix}'\n        if replace_filenames is not None:\n            parts: List[str] = path_final.rsplit(sep='/', maxsplit=1)\n            if len(parts) == 2:\n                path_wo_filename: str = parts[0]\n                filename: str = parts[1]\n                if filename in replace_filenames:\n                    new_filename: str = replace_filenames[filename]\n                    _logger.debug('Replacing filename: %s -> %s', filename, new_filename)\n                    path_final = f'{path_wo_filename}/{new_filename}'\n        new_objects.append(path_final)\n        batch.append((path, path_final))\n    _logger.debug('Creating %s new objects', len(new_objects))\n    _copy(batches=_utils.chunkify(lst=batch, max_length=1000), use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects",
        "mutated": [
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef copy_objects(paths: List[str], source_path: str, target_path: str, replace_filenames: Optional[Dict[str, str]]=None, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    if False:\n        i = 10\n    'Copy a list of S3 objects to another S3 directory.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    paths : List[str]\\n        List of S3 objects paths (e.g. [s3://bucket/dir0/key0, s3://bucket/dir0/key1]).\\n    source_path : str,\\n        S3 Path for the source directory.\\n    target_path : str,\\n        S3 Path for the target directory.\\n    replace_filenames : Dict[str, str], optional\\n        e.g. {\"old_name.csv\": \"new_name.csv\", \"old_name2.csv\": \"new_name2.csv\"}\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forwarded to botocore requests.\\n        e.g. s3_additional_kwargs={\\'ServerSideEncryption\\': \\'aws:kms\\', \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'}\\n\\n    Returns\\n    -------\\n    List[str]\\n        List of new objects paths.\\n\\n    Examples\\n    --------\\n    Copying\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.copy_objects(\\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\"\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    Copying with a KMS key\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.copy_objects(\\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     s3_additional_kwargs={\\n    ...         \\'ServerSideEncryption\\': \\'aws:kms\\',\\n    ...         \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'\\n    ...     }\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    '\n    if len(paths) < 1:\n        return []\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    batch: List[Tuple[str, str]] = []\n    new_objects: List[str] = []\n    for path in paths:\n        path_wo_prefix: str = path.replace(f'{source_path}/', '')\n        path_final: str = f'{target_path}/{path_wo_prefix}'\n        if replace_filenames is not None:\n            parts: List[str] = path_final.rsplit(sep='/', maxsplit=1)\n            if len(parts) == 2:\n                path_wo_filename: str = parts[0]\n                filename: str = parts[1]\n                if filename in replace_filenames:\n                    new_filename: str = replace_filenames[filename]\n                    _logger.debug('Replacing filename: %s -> %s', filename, new_filename)\n                    path_final = f'{path_wo_filename}/{new_filename}'\n        new_objects.append(path_final)\n        batch.append((path, path_final))\n    _logger.debug('Creating %s new objects', len(new_objects))\n    _copy(batches=_utils.chunkify(lst=batch, max_length=1000), use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef copy_objects(paths: List[str], source_path: str, target_path: str, replace_filenames: Optional[Dict[str, str]]=None, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Copy a list of S3 objects to another S3 directory.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    paths : List[str]\\n        List of S3 objects paths (e.g. [s3://bucket/dir0/key0, s3://bucket/dir0/key1]).\\n    source_path : str,\\n        S3 Path for the source directory.\\n    target_path : str,\\n        S3 Path for the target directory.\\n    replace_filenames : Dict[str, str], optional\\n        e.g. {\"old_name.csv\": \"new_name.csv\", \"old_name2.csv\": \"new_name2.csv\"}\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forwarded to botocore requests.\\n        e.g. s3_additional_kwargs={\\'ServerSideEncryption\\': \\'aws:kms\\', \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'}\\n\\n    Returns\\n    -------\\n    List[str]\\n        List of new objects paths.\\n\\n    Examples\\n    --------\\n    Copying\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.copy_objects(\\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\"\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    Copying with a KMS key\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.copy_objects(\\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     s3_additional_kwargs={\\n    ...         \\'ServerSideEncryption\\': \\'aws:kms\\',\\n    ...         \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'\\n    ...     }\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    '\n    if len(paths) < 1:\n        return []\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    batch: List[Tuple[str, str]] = []\n    new_objects: List[str] = []\n    for path in paths:\n        path_wo_prefix: str = path.replace(f'{source_path}/', '')\n        path_final: str = f'{target_path}/{path_wo_prefix}'\n        if replace_filenames is not None:\n            parts: List[str] = path_final.rsplit(sep='/', maxsplit=1)\n            if len(parts) == 2:\n                path_wo_filename: str = parts[0]\n                filename: str = parts[1]\n                if filename in replace_filenames:\n                    new_filename: str = replace_filenames[filename]\n                    _logger.debug('Replacing filename: %s -> %s', filename, new_filename)\n                    path_final = f'{path_wo_filename}/{new_filename}'\n        new_objects.append(path_final)\n        batch.append((path, path_final))\n    _logger.debug('Creating %s new objects', len(new_objects))\n    _copy(batches=_utils.chunkify(lst=batch, max_length=1000), use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef copy_objects(paths: List[str], source_path: str, target_path: str, replace_filenames: Optional[Dict[str, str]]=None, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Copy a list of S3 objects to another S3 directory.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    paths : List[str]\\n        List of S3 objects paths (e.g. [s3://bucket/dir0/key0, s3://bucket/dir0/key1]).\\n    source_path : str,\\n        S3 Path for the source directory.\\n    target_path : str,\\n        S3 Path for the target directory.\\n    replace_filenames : Dict[str, str], optional\\n        e.g. {\"old_name.csv\": \"new_name.csv\", \"old_name2.csv\": \"new_name2.csv\"}\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forwarded to botocore requests.\\n        e.g. s3_additional_kwargs={\\'ServerSideEncryption\\': \\'aws:kms\\', \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'}\\n\\n    Returns\\n    -------\\n    List[str]\\n        List of new objects paths.\\n\\n    Examples\\n    --------\\n    Copying\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.copy_objects(\\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\"\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    Copying with a KMS key\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.copy_objects(\\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     s3_additional_kwargs={\\n    ...         \\'ServerSideEncryption\\': \\'aws:kms\\',\\n    ...         \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'\\n    ...     }\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    '\n    if len(paths) < 1:\n        return []\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    batch: List[Tuple[str, str]] = []\n    new_objects: List[str] = []\n    for path in paths:\n        path_wo_prefix: str = path.replace(f'{source_path}/', '')\n        path_final: str = f'{target_path}/{path_wo_prefix}'\n        if replace_filenames is not None:\n            parts: List[str] = path_final.rsplit(sep='/', maxsplit=1)\n            if len(parts) == 2:\n                path_wo_filename: str = parts[0]\n                filename: str = parts[1]\n                if filename in replace_filenames:\n                    new_filename: str = replace_filenames[filename]\n                    _logger.debug('Replacing filename: %s -> %s', filename, new_filename)\n                    path_final = f'{path_wo_filename}/{new_filename}'\n        new_objects.append(path_final)\n        batch.append((path, path_final))\n    _logger.debug('Creating %s new objects', len(new_objects))\n    _copy(batches=_utils.chunkify(lst=batch, max_length=1000), use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef copy_objects(paths: List[str], source_path: str, target_path: str, replace_filenames: Optional[Dict[str, str]]=None, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Copy a list of S3 objects to another S3 directory.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    paths : List[str]\\n        List of S3 objects paths (e.g. [s3://bucket/dir0/key0, s3://bucket/dir0/key1]).\\n    source_path : str,\\n        S3 Path for the source directory.\\n    target_path : str,\\n        S3 Path for the target directory.\\n    replace_filenames : Dict[str, str], optional\\n        e.g. {\"old_name.csv\": \"new_name.csv\", \"old_name2.csv\": \"new_name2.csv\"}\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forwarded to botocore requests.\\n        e.g. s3_additional_kwargs={\\'ServerSideEncryption\\': \\'aws:kms\\', \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'}\\n\\n    Returns\\n    -------\\n    List[str]\\n        List of new objects paths.\\n\\n    Examples\\n    --------\\n    Copying\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.copy_objects(\\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\"\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    Copying with a KMS key\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.copy_objects(\\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     s3_additional_kwargs={\\n    ...         \\'ServerSideEncryption\\': \\'aws:kms\\',\\n    ...         \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'\\n    ...     }\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    '\n    if len(paths) < 1:\n        return []\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    batch: List[Tuple[str, str]] = []\n    new_objects: List[str] = []\n    for path in paths:\n        path_wo_prefix: str = path.replace(f'{source_path}/', '')\n        path_final: str = f'{target_path}/{path_wo_prefix}'\n        if replace_filenames is not None:\n            parts: List[str] = path_final.rsplit(sep='/', maxsplit=1)\n            if len(parts) == 2:\n                path_wo_filename: str = parts[0]\n                filename: str = parts[1]\n                if filename in replace_filenames:\n                    new_filename: str = replace_filenames[filename]\n                    _logger.debug('Replacing filename: %s -> %s', filename, new_filename)\n                    path_final = f'{path_wo_filename}/{new_filename}'\n        new_objects.append(path_final)\n        batch.append((path, path_final))\n    _logger.debug('Creating %s new objects', len(new_objects))\n    _copy(batches=_utils.chunkify(lst=batch, max_length=1000), use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef copy_objects(paths: List[str], source_path: str, target_path: str, replace_filenames: Optional[Dict[str, str]]=None, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Copy a list of S3 objects to another S3 directory.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    paths : List[str]\\n        List of S3 objects paths (e.g. [s3://bucket/dir0/key0, s3://bucket/dir0/key1]).\\n    source_path : str,\\n        S3 Path for the source directory.\\n    target_path : str,\\n        S3 Path for the target directory.\\n    replace_filenames : Dict[str, str], optional\\n        e.g. {\"old_name.csv\": \"new_name.csv\", \"old_name2.csv\": \"new_name2.csv\"}\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forwarded to botocore requests.\\n        e.g. s3_additional_kwargs={\\'ServerSideEncryption\\': \\'aws:kms\\', \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'}\\n\\n    Returns\\n    -------\\n    List[str]\\n        List of new objects paths.\\n\\n    Examples\\n    --------\\n    Copying\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.copy_objects(\\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\"\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    Copying with a KMS key\\n\\n    >>> import awswrangler as wr\\n    >>> wr.s3.copy_objects(\\n    ...     paths=[\"s3://bucket0/dir0/key0\", \"s3://bucket0/dir0/key1\"],\\n    ...     source_path=\"s3://bucket0/dir0/\",\\n    ...     target_path=\"s3://bucket1/dir1/\",\\n    ...     s3_additional_kwargs={\\n    ...         \\'ServerSideEncryption\\': \\'aws:kms\\',\\n    ...         \\'SSEKMSKeyId\\': \\'YOUR_KMS_KEY_ARN\\'\\n    ...     }\\n    ... )\\n    [\"s3://bucket1/dir1/key0\", \"s3://bucket1/dir1/key1\"]\\n\\n    '\n    if len(paths) < 1:\n        return []\n    source_path = source_path[:-1] if source_path[-1] == '/' else source_path\n    target_path = target_path[:-1] if target_path[-1] == '/' else target_path\n    batch: List[Tuple[str, str]] = []\n    new_objects: List[str] = []\n    for path in paths:\n        path_wo_prefix: str = path.replace(f'{source_path}/', '')\n        path_final: str = f'{target_path}/{path_wo_prefix}'\n        if replace_filenames is not None:\n            parts: List[str] = path_final.rsplit(sep='/', maxsplit=1)\n            if len(parts) == 2:\n                path_wo_filename: str = parts[0]\n                filename: str = parts[1]\n                if filename in replace_filenames:\n                    new_filename: str = replace_filenames[filename]\n                    _logger.debug('Replacing filename: %s -> %s', filename, new_filename)\n                    path_final = f'{path_wo_filename}/{new_filename}'\n        new_objects.append(path_final)\n        batch.append((path, path_final))\n    _logger.debug('Creating %s new objects', len(new_objects))\n    _copy(batches=_utils.chunkify(lst=batch, max_length=1000), use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n    return new_objects"
        ]
    }
]