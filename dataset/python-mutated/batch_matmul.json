[
    {
        "func_name": "get_template_op",
        "original": "@autotvm.template\ndef get_template_op(**kargs):\n    batch = op_attributes['B']\n    M = op_attributes['N']\n    K = op_attributes['K']\n    N = op_attributes['M']\n    pose = op_attributes['P']\n    if pose == 'NN':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, k, j], axis=k), name='C')\n    elif pose == 'NT':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, j, k], axis=k), name='C')\n    elif pose == 'TN':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, k, j], axis=k), name='C')\n    elif pose == 'TT':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, j, k], axis=k), name='C')\n    else:\n        raise\n    cfg = autotvm.get_config()\n    s = tvm.create_schedule(C.op)\n    AA = s.cache_read(A, 'shared', [C])\n    AL = s.cache_read(AA, 'local', [C])\n    BB = s.cache_read(B, 'shared', [C])\n    BL = s.cache_read(BB, 'local', [C])\n    CC = s.cache_write(C, 'local')\n    (b, y, x) = C.op.axis\n    k = CC.op.reduce_axis[0]\n    cfg.define_split('B', cfg.axis(b), num_outputs=2)\n    (bo, bi) = cfg['B'].apply(s, C, b)\n    cfg.define_split('K', cfg.axis(k), num_outputs=3)\n    (ko, kt, ki) = cfg['K'].apply(s, CC, k)\n    block_x = tvm.thread_axis('blockIdx.x')\n    block_y = tvm.thread_axis('blockIdx.y')\n    block_z = tvm.thread_axis('blockIdx.z')\n    thread_x = tvm.thread_axis('threadIdx.x')\n    thread_y = tvm.thread_axis('threadIdx.y')\n    thread_z = tvm.thread_axis('threadIdx.z')\n    cfg.define_split('X', cfg.axis(y), num_outputs=4)\n    cfg.define_split('Y', cfg.axis(x), num_outputs=4)\n    (by, tyz, ty, yi) = cfg['X'].apply(s, C, y)\n    (bx, txz, tx, xi) = cfg['Y'].apply(s, C, x)\n    s[C].bind(bo, block_z)\n    s[C].bind(by, block_y)\n    s[C].bind(bx, block_x)\n    s[C].bind(tyz, tvm.thread_axis('vthread'))\n    s[C].bind(txz, tvm.thread_axis('vthread'))\n    s[C].bind(bi, thread_z)\n    s[C].bind(ty, thread_y)\n    s[C].bind(tx, thread_x)\n    s[C].reorder(by, bx, tyz, txz, ty, tx, yi, xi)\n    s[CC].compute_at(s[C], tx)\n    (bo, yo, xo) = CC.op.axis\n    s[CC].reorder(ko, kt, yo, xo, ki)\n    s[CC].unroll(kt)\n    for stage in [AL, BL]:\n        s[stage].compute_at(s[CC], kt)\n        s[stage].double_buffer()\n    for stage in [AA, BB]:\n        s[stage].compute_at(s[CC], ko)\n        fused = s[stage].fuse(*s[stage].op.axis)\n        (ty, tx) = s[stage].split(fused, nparts=cfg['X'].size[2])\n        (tx, xi) = s[stage].split(tx, nparts=cfg['Y'].size[2])\n        (_, xi) = s[stage].split(xi, factor=4)\n        s[stage].bind(ty, thread_y)\n        s[stage].bind(tx, thread_x)\n        s[stage].vectorize(xi)\n        s[stage].double_buffer()\n    cfg.add_flop(batch * M * K * N * 2.0)\n    return (s, [A, B, C])",
        "mutated": [
            "@autotvm.template\ndef get_template_op(**kargs):\n    if False:\n        i = 10\n    batch = op_attributes['B']\n    M = op_attributes['N']\n    K = op_attributes['K']\n    N = op_attributes['M']\n    pose = op_attributes['P']\n    if pose == 'NN':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, k, j], axis=k), name='C')\n    elif pose == 'NT':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, j, k], axis=k), name='C')\n    elif pose == 'TN':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, k, j], axis=k), name='C')\n    elif pose == 'TT':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, j, k], axis=k), name='C')\n    else:\n        raise\n    cfg = autotvm.get_config()\n    s = tvm.create_schedule(C.op)\n    AA = s.cache_read(A, 'shared', [C])\n    AL = s.cache_read(AA, 'local', [C])\n    BB = s.cache_read(B, 'shared', [C])\n    BL = s.cache_read(BB, 'local', [C])\n    CC = s.cache_write(C, 'local')\n    (b, y, x) = C.op.axis\n    k = CC.op.reduce_axis[0]\n    cfg.define_split('B', cfg.axis(b), num_outputs=2)\n    (bo, bi) = cfg['B'].apply(s, C, b)\n    cfg.define_split('K', cfg.axis(k), num_outputs=3)\n    (ko, kt, ki) = cfg['K'].apply(s, CC, k)\n    block_x = tvm.thread_axis('blockIdx.x')\n    block_y = tvm.thread_axis('blockIdx.y')\n    block_z = tvm.thread_axis('blockIdx.z')\n    thread_x = tvm.thread_axis('threadIdx.x')\n    thread_y = tvm.thread_axis('threadIdx.y')\n    thread_z = tvm.thread_axis('threadIdx.z')\n    cfg.define_split('X', cfg.axis(y), num_outputs=4)\n    cfg.define_split('Y', cfg.axis(x), num_outputs=4)\n    (by, tyz, ty, yi) = cfg['X'].apply(s, C, y)\n    (bx, txz, tx, xi) = cfg['Y'].apply(s, C, x)\n    s[C].bind(bo, block_z)\n    s[C].bind(by, block_y)\n    s[C].bind(bx, block_x)\n    s[C].bind(tyz, tvm.thread_axis('vthread'))\n    s[C].bind(txz, tvm.thread_axis('vthread'))\n    s[C].bind(bi, thread_z)\n    s[C].bind(ty, thread_y)\n    s[C].bind(tx, thread_x)\n    s[C].reorder(by, bx, tyz, txz, ty, tx, yi, xi)\n    s[CC].compute_at(s[C], tx)\n    (bo, yo, xo) = CC.op.axis\n    s[CC].reorder(ko, kt, yo, xo, ki)\n    s[CC].unroll(kt)\n    for stage in [AL, BL]:\n        s[stage].compute_at(s[CC], kt)\n        s[stage].double_buffer()\n    for stage in [AA, BB]:\n        s[stage].compute_at(s[CC], ko)\n        fused = s[stage].fuse(*s[stage].op.axis)\n        (ty, tx) = s[stage].split(fused, nparts=cfg['X'].size[2])\n        (tx, xi) = s[stage].split(tx, nparts=cfg['Y'].size[2])\n        (_, xi) = s[stage].split(xi, factor=4)\n        s[stage].bind(ty, thread_y)\n        s[stage].bind(tx, thread_x)\n        s[stage].vectorize(xi)\n        s[stage].double_buffer()\n    cfg.add_flop(batch * M * K * N * 2.0)\n    return (s, [A, B, C])",
            "@autotvm.template\ndef get_template_op(**kargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    batch = op_attributes['B']\n    M = op_attributes['N']\n    K = op_attributes['K']\n    N = op_attributes['M']\n    pose = op_attributes['P']\n    if pose == 'NN':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, k, j], axis=k), name='C')\n    elif pose == 'NT':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, j, k], axis=k), name='C')\n    elif pose == 'TN':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, k, j], axis=k), name='C')\n    elif pose == 'TT':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, j, k], axis=k), name='C')\n    else:\n        raise\n    cfg = autotvm.get_config()\n    s = tvm.create_schedule(C.op)\n    AA = s.cache_read(A, 'shared', [C])\n    AL = s.cache_read(AA, 'local', [C])\n    BB = s.cache_read(B, 'shared', [C])\n    BL = s.cache_read(BB, 'local', [C])\n    CC = s.cache_write(C, 'local')\n    (b, y, x) = C.op.axis\n    k = CC.op.reduce_axis[0]\n    cfg.define_split('B', cfg.axis(b), num_outputs=2)\n    (bo, bi) = cfg['B'].apply(s, C, b)\n    cfg.define_split('K', cfg.axis(k), num_outputs=3)\n    (ko, kt, ki) = cfg['K'].apply(s, CC, k)\n    block_x = tvm.thread_axis('blockIdx.x')\n    block_y = tvm.thread_axis('blockIdx.y')\n    block_z = tvm.thread_axis('blockIdx.z')\n    thread_x = tvm.thread_axis('threadIdx.x')\n    thread_y = tvm.thread_axis('threadIdx.y')\n    thread_z = tvm.thread_axis('threadIdx.z')\n    cfg.define_split('X', cfg.axis(y), num_outputs=4)\n    cfg.define_split('Y', cfg.axis(x), num_outputs=4)\n    (by, tyz, ty, yi) = cfg['X'].apply(s, C, y)\n    (bx, txz, tx, xi) = cfg['Y'].apply(s, C, x)\n    s[C].bind(bo, block_z)\n    s[C].bind(by, block_y)\n    s[C].bind(bx, block_x)\n    s[C].bind(tyz, tvm.thread_axis('vthread'))\n    s[C].bind(txz, tvm.thread_axis('vthread'))\n    s[C].bind(bi, thread_z)\n    s[C].bind(ty, thread_y)\n    s[C].bind(tx, thread_x)\n    s[C].reorder(by, bx, tyz, txz, ty, tx, yi, xi)\n    s[CC].compute_at(s[C], tx)\n    (bo, yo, xo) = CC.op.axis\n    s[CC].reorder(ko, kt, yo, xo, ki)\n    s[CC].unroll(kt)\n    for stage in [AL, BL]:\n        s[stage].compute_at(s[CC], kt)\n        s[stage].double_buffer()\n    for stage in [AA, BB]:\n        s[stage].compute_at(s[CC], ko)\n        fused = s[stage].fuse(*s[stage].op.axis)\n        (ty, tx) = s[stage].split(fused, nparts=cfg['X'].size[2])\n        (tx, xi) = s[stage].split(tx, nparts=cfg['Y'].size[2])\n        (_, xi) = s[stage].split(xi, factor=4)\n        s[stage].bind(ty, thread_y)\n        s[stage].bind(tx, thread_x)\n        s[stage].vectorize(xi)\n        s[stage].double_buffer()\n    cfg.add_flop(batch * M * K * N * 2.0)\n    return (s, [A, B, C])",
            "@autotvm.template\ndef get_template_op(**kargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    batch = op_attributes['B']\n    M = op_attributes['N']\n    K = op_attributes['K']\n    N = op_attributes['M']\n    pose = op_attributes['P']\n    if pose == 'NN':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, k, j], axis=k), name='C')\n    elif pose == 'NT':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, j, k], axis=k), name='C')\n    elif pose == 'TN':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, k, j], axis=k), name='C')\n    elif pose == 'TT':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, j, k], axis=k), name='C')\n    else:\n        raise\n    cfg = autotvm.get_config()\n    s = tvm.create_schedule(C.op)\n    AA = s.cache_read(A, 'shared', [C])\n    AL = s.cache_read(AA, 'local', [C])\n    BB = s.cache_read(B, 'shared', [C])\n    BL = s.cache_read(BB, 'local', [C])\n    CC = s.cache_write(C, 'local')\n    (b, y, x) = C.op.axis\n    k = CC.op.reduce_axis[0]\n    cfg.define_split('B', cfg.axis(b), num_outputs=2)\n    (bo, bi) = cfg['B'].apply(s, C, b)\n    cfg.define_split('K', cfg.axis(k), num_outputs=3)\n    (ko, kt, ki) = cfg['K'].apply(s, CC, k)\n    block_x = tvm.thread_axis('blockIdx.x')\n    block_y = tvm.thread_axis('blockIdx.y')\n    block_z = tvm.thread_axis('blockIdx.z')\n    thread_x = tvm.thread_axis('threadIdx.x')\n    thread_y = tvm.thread_axis('threadIdx.y')\n    thread_z = tvm.thread_axis('threadIdx.z')\n    cfg.define_split('X', cfg.axis(y), num_outputs=4)\n    cfg.define_split('Y', cfg.axis(x), num_outputs=4)\n    (by, tyz, ty, yi) = cfg['X'].apply(s, C, y)\n    (bx, txz, tx, xi) = cfg['Y'].apply(s, C, x)\n    s[C].bind(bo, block_z)\n    s[C].bind(by, block_y)\n    s[C].bind(bx, block_x)\n    s[C].bind(tyz, tvm.thread_axis('vthread'))\n    s[C].bind(txz, tvm.thread_axis('vthread'))\n    s[C].bind(bi, thread_z)\n    s[C].bind(ty, thread_y)\n    s[C].bind(tx, thread_x)\n    s[C].reorder(by, bx, tyz, txz, ty, tx, yi, xi)\n    s[CC].compute_at(s[C], tx)\n    (bo, yo, xo) = CC.op.axis\n    s[CC].reorder(ko, kt, yo, xo, ki)\n    s[CC].unroll(kt)\n    for stage in [AL, BL]:\n        s[stage].compute_at(s[CC], kt)\n        s[stage].double_buffer()\n    for stage in [AA, BB]:\n        s[stage].compute_at(s[CC], ko)\n        fused = s[stage].fuse(*s[stage].op.axis)\n        (ty, tx) = s[stage].split(fused, nparts=cfg['X'].size[2])\n        (tx, xi) = s[stage].split(tx, nparts=cfg['Y'].size[2])\n        (_, xi) = s[stage].split(xi, factor=4)\n        s[stage].bind(ty, thread_y)\n        s[stage].bind(tx, thread_x)\n        s[stage].vectorize(xi)\n        s[stage].double_buffer()\n    cfg.add_flop(batch * M * K * N * 2.0)\n    return (s, [A, B, C])",
            "@autotvm.template\ndef get_template_op(**kargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    batch = op_attributes['B']\n    M = op_attributes['N']\n    K = op_attributes['K']\n    N = op_attributes['M']\n    pose = op_attributes['P']\n    if pose == 'NN':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, k, j], axis=k), name='C')\n    elif pose == 'NT':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, j, k], axis=k), name='C')\n    elif pose == 'TN':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, k, j], axis=k), name='C')\n    elif pose == 'TT':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, j, k], axis=k), name='C')\n    else:\n        raise\n    cfg = autotvm.get_config()\n    s = tvm.create_schedule(C.op)\n    AA = s.cache_read(A, 'shared', [C])\n    AL = s.cache_read(AA, 'local', [C])\n    BB = s.cache_read(B, 'shared', [C])\n    BL = s.cache_read(BB, 'local', [C])\n    CC = s.cache_write(C, 'local')\n    (b, y, x) = C.op.axis\n    k = CC.op.reduce_axis[0]\n    cfg.define_split('B', cfg.axis(b), num_outputs=2)\n    (bo, bi) = cfg['B'].apply(s, C, b)\n    cfg.define_split('K', cfg.axis(k), num_outputs=3)\n    (ko, kt, ki) = cfg['K'].apply(s, CC, k)\n    block_x = tvm.thread_axis('blockIdx.x')\n    block_y = tvm.thread_axis('blockIdx.y')\n    block_z = tvm.thread_axis('blockIdx.z')\n    thread_x = tvm.thread_axis('threadIdx.x')\n    thread_y = tvm.thread_axis('threadIdx.y')\n    thread_z = tvm.thread_axis('threadIdx.z')\n    cfg.define_split('X', cfg.axis(y), num_outputs=4)\n    cfg.define_split('Y', cfg.axis(x), num_outputs=4)\n    (by, tyz, ty, yi) = cfg['X'].apply(s, C, y)\n    (bx, txz, tx, xi) = cfg['Y'].apply(s, C, x)\n    s[C].bind(bo, block_z)\n    s[C].bind(by, block_y)\n    s[C].bind(bx, block_x)\n    s[C].bind(tyz, tvm.thread_axis('vthread'))\n    s[C].bind(txz, tvm.thread_axis('vthread'))\n    s[C].bind(bi, thread_z)\n    s[C].bind(ty, thread_y)\n    s[C].bind(tx, thread_x)\n    s[C].reorder(by, bx, tyz, txz, ty, tx, yi, xi)\n    s[CC].compute_at(s[C], tx)\n    (bo, yo, xo) = CC.op.axis\n    s[CC].reorder(ko, kt, yo, xo, ki)\n    s[CC].unroll(kt)\n    for stage in [AL, BL]:\n        s[stage].compute_at(s[CC], kt)\n        s[stage].double_buffer()\n    for stage in [AA, BB]:\n        s[stage].compute_at(s[CC], ko)\n        fused = s[stage].fuse(*s[stage].op.axis)\n        (ty, tx) = s[stage].split(fused, nparts=cfg['X'].size[2])\n        (tx, xi) = s[stage].split(tx, nparts=cfg['Y'].size[2])\n        (_, xi) = s[stage].split(xi, factor=4)\n        s[stage].bind(ty, thread_y)\n        s[stage].bind(tx, thread_x)\n        s[stage].vectorize(xi)\n        s[stage].double_buffer()\n    cfg.add_flop(batch * M * K * N * 2.0)\n    return (s, [A, B, C])",
            "@autotvm.template\ndef get_template_op(**kargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    batch = op_attributes['B']\n    M = op_attributes['N']\n    K = op_attributes['K']\n    N = op_attributes['M']\n    pose = op_attributes['P']\n    if pose == 'NN':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, k, j], axis=k), name='C')\n    elif pose == 'NT':\n        A = tvm.placeholder((batch, M, K), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, i, k] * B[b, j, k], axis=k), name='C')\n    elif pose == 'TN':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, K, N), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, k, j], axis=k), name='C')\n    elif pose == 'TT':\n        A = tvm.placeholder((batch, K, M), name='A', dtype='float32')\n        B = tvm.placeholder((batch, N, K), name='B', dtype='float32')\n        k = tvm.reduce_axis((0, K), name='k')\n        C = tvm.compute((batch, M, N), lambda b, i, j: tvm.sum(A[b, k, i] * B[b, j, k], axis=k), name='C')\n    else:\n        raise\n    cfg = autotvm.get_config()\n    s = tvm.create_schedule(C.op)\n    AA = s.cache_read(A, 'shared', [C])\n    AL = s.cache_read(AA, 'local', [C])\n    BB = s.cache_read(B, 'shared', [C])\n    BL = s.cache_read(BB, 'local', [C])\n    CC = s.cache_write(C, 'local')\n    (b, y, x) = C.op.axis\n    k = CC.op.reduce_axis[0]\n    cfg.define_split('B', cfg.axis(b), num_outputs=2)\n    (bo, bi) = cfg['B'].apply(s, C, b)\n    cfg.define_split('K', cfg.axis(k), num_outputs=3)\n    (ko, kt, ki) = cfg['K'].apply(s, CC, k)\n    block_x = tvm.thread_axis('blockIdx.x')\n    block_y = tvm.thread_axis('blockIdx.y')\n    block_z = tvm.thread_axis('blockIdx.z')\n    thread_x = tvm.thread_axis('threadIdx.x')\n    thread_y = tvm.thread_axis('threadIdx.y')\n    thread_z = tvm.thread_axis('threadIdx.z')\n    cfg.define_split('X', cfg.axis(y), num_outputs=4)\n    cfg.define_split('Y', cfg.axis(x), num_outputs=4)\n    (by, tyz, ty, yi) = cfg['X'].apply(s, C, y)\n    (bx, txz, tx, xi) = cfg['Y'].apply(s, C, x)\n    s[C].bind(bo, block_z)\n    s[C].bind(by, block_y)\n    s[C].bind(bx, block_x)\n    s[C].bind(tyz, tvm.thread_axis('vthread'))\n    s[C].bind(txz, tvm.thread_axis('vthread'))\n    s[C].bind(bi, thread_z)\n    s[C].bind(ty, thread_y)\n    s[C].bind(tx, thread_x)\n    s[C].reorder(by, bx, tyz, txz, ty, tx, yi, xi)\n    s[CC].compute_at(s[C], tx)\n    (bo, yo, xo) = CC.op.axis\n    s[CC].reorder(ko, kt, yo, xo, ki)\n    s[CC].unroll(kt)\n    for stage in [AL, BL]:\n        s[stage].compute_at(s[CC], kt)\n        s[stage].double_buffer()\n    for stage in [AA, BB]:\n        s[stage].compute_at(s[CC], ko)\n        fused = s[stage].fuse(*s[stage].op.axis)\n        (ty, tx) = s[stage].split(fused, nparts=cfg['X'].size[2])\n        (tx, xi) = s[stage].split(tx, nparts=cfg['Y'].size[2])\n        (_, xi) = s[stage].split(xi, factor=4)\n        s[stage].bind(ty, thread_y)\n        s[stage].bind(tx, thread_x)\n        s[stage].vectorize(xi)\n        s[stage].double_buffer()\n    cfg.add_flop(batch * M * K * N * 2.0)\n    return (s, [A, B, C])"
        ]
    }
]