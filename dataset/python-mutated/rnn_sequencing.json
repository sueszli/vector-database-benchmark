[
    {
        "func_name": "pad_batch_to_sequences_of_same_size",
        "original": "@DeveloperAPI\ndef pad_batch_to_sequences_of_same_size(batch: SampleBatch, max_seq_len: int, shuffle: bool=False, batch_divisibility_req: int=1, feature_keys: Optional[List[str]]=None, view_requirements: Optional[ViewRequirementsDict]=None, _enable_new_api_stack: bool=False, padding: str='zero'):\n    \"\"\"Applies padding to `batch` so it's choppable into same-size sequences.\n\n    Shuffles `batch` (if desired), makes sure divisibility requirement is met,\n    then pads the batch ([B, ...]) into same-size chunks ([B, ...]) w/o\n    adding a time dimension (yet).\n    Padding depends on episodes found in batch and `max_seq_len`.\n\n    Args:\n        batch: The SampleBatch object. All values in here have\n            the shape [B, ...].\n        max_seq_len: The max. sequence length to use for chopping.\n        shuffle: Whether to shuffle batch sequences. Shuffle may\n            be done in-place. This only makes sense if you're further\n            applying minibatch SGD after getting the outputs.\n        batch_divisibility_req: The int by which the batch dimension\n            must be dividable.\n        feature_keys: An optional list of keys to apply sequence-chopping\n            to. If None, use all keys in batch that are not\n            \"state_in/out_\"-type keys.\n        view_requirements: An optional Policy ViewRequirements dict to\n            be able to infer whether e.g. dynamic max'ing should be\n            applied over the seq_lens.\n        _enable_new_api_stack: This is a temporary flag to enable the new RLModule API.\n            After a complete rollout of the new API, this flag will be removed.\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\n            will pad with zeros, last padding will pad with the last value.\n    \"\"\"\n    if batch.zero_padded:\n        return\n    batch.zero_padded = True\n    if batch_divisibility_req > 1:\n        meets_divisibility_reqs = len(batch[SampleBatch.CUR_OBS]) % batch_divisibility_req == 0 and max(batch[SampleBatch.AGENT_INDEX]) == 0\n    else:\n        meets_divisibility_reqs = True\n    states_already_reduced_to_init = False\n    if _enable_new_api_stack and ('state_in' in batch or 'state_out' in batch):\n        seq_lens = batch.get(SampleBatch.SEQ_LENS)\n        state_ins = tree.flatten(batch['state_in'])\n        if state_ins:\n            assert all((len(state_in) == len(state_ins[0]) for state_in in state_ins)), 'All state_in tensors should have the same batch_dim size.'\n            if len(state_ins[0]) == len(seq_lens):\n                states_already_reduced_to_init = True\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not _enable_new_api_stack and ('state_in_0' in batch or 'state_out_0' in batch):\n        if batch.get(SampleBatch.SEQ_LENS) is not None and len(batch['state_in_0']) == len(batch[SampleBatch.SEQ_LENS]):\n            states_already_reduced_to_init = True\n        if view_requirements['state_in_0'].shift_from is None:\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not meets_divisibility_reqs:\n        max_seq_len = batch_divisibility_req\n        dynamic_max = False\n        batch.max_seq_len = max_seq_len\n    else:\n        if shuffle:\n            batch.shuffle()\n        return\n    state_keys = []\n    feature_keys_ = feature_keys or []\n    for (k, v) in batch.items():\n        if k.startswith('state_in'):\n            state_keys.append(k)\n        elif not feature_keys and (not k.startswith('state_out') if not _enable_new_api_stack else True) and (k not in [SampleBatch.SEQ_LENS]):\n            feature_keys_.append(k)\n    (feature_sequences, initial_states, seq_lens) = chop_into_sequences(feature_columns=[batch[k] for k in feature_keys_], state_columns=[batch[k] for k in state_keys], episode_ids=batch.get(SampleBatch.EPS_ID), unroll_ids=batch.get(SampleBatch.UNROLL_ID), agent_indices=batch.get(SampleBatch.AGENT_INDEX), seq_lens=batch.get(SampleBatch.SEQ_LENS), max_seq_len=max_seq_len, dynamic_max=dynamic_max, states_already_reduced_to_init=states_already_reduced_to_init, shuffle=shuffle, handle_nested_data=True, padding=padding)\n    for (i, k) in enumerate(feature_keys_):\n        batch[k] = tree.unflatten_as(batch[k], feature_sequences[i])\n    for (i, k) in enumerate(state_keys):\n        batch[k] = initial_states[i]\n    batch[SampleBatch.SEQ_LENS] = np.array(seq_lens)\n    if dynamic_max:\n        batch.max_seq_len = max(seq_lens)\n    if log_once('rnn_ma_feed_dict'):\n        logger.info('Padded input for RNN/Attn.Nets/MA:\\n\\n{}\\n'.format(summarize({'features': feature_sequences, 'initial_states': initial_states, 'seq_lens': seq_lens, 'max_seq_len': max_seq_len})))",
        "mutated": [
            "@DeveloperAPI\ndef pad_batch_to_sequences_of_same_size(batch: SampleBatch, max_seq_len: int, shuffle: bool=False, batch_divisibility_req: int=1, feature_keys: Optional[List[str]]=None, view_requirements: Optional[ViewRequirementsDict]=None, _enable_new_api_stack: bool=False, padding: str='zero'):\n    if False:\n        i = 10\n    'Applies padding to `batch` so it\\'s choppable into same-size sequences.\\n\\n    Shuffles `batch` (if desired), makes sure divisibility requirement is met,\\n    then pads the batch ([B, ...]) into same-size chunks ([B, ...]) w/o\\n    adding a time dimension (yet).\\n    Padding depends on episodes found in batch and `max_seq_len`.\\n\\n    Args:\\n        batch: The SampleBatch object. All values in here have\\n            the shape [B, ...].\\n        max_seq_len: The max. sequence length to use for chopping.\\n        shuffle: Whether to shuffle batch sequences. Shuffle may\\n            be done in-place. This only makes sense if you\\'re further\\n            applying minibatch SGD after getting the outputs.\\n        batch_divisibility_req: The int by which the batch dimension\\n            must be dividable.\\n        feature_keys: An optional list of keys to apply sequence-chopping\\n            to. If None, use all keys in batch that are not\\n            \"state_in/out_\"-type keys.\\n        view_requirements: An optional Policy ViewRequirements dict to\\n            be able to infer whether e.g. dynamic max\\'ing should be\\n            applied over the seq_lens.\\n        _enable_new_api_stack: This is a temporary flag to enable the new RLModule API.\\n            After a complete rollout of the new API, this flag will be removed.\\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\\n            will pad with zeros, last padding will pad with the last value.\\n    '\n    if batch.zero_padded:\n        return\n    batch.zero_padded = True\n    if batch_divisibility_req > 1:\n        meets_divisibility_reqs = len(batch[SampleBatch.CUR_OBS]) % batch_divisibility_req == 0 and max(batch[SampleBatch.AGENT_INDEX]) == 0\n    else:\n        meets_divisibility_reqs = True\n    states_already_reduced_to_init = False\n    if _enable_new_api_stack and ('state_in' in batch or 'state_out' in batch):\n        seq_lens = batch.get(SampleBatch.SEQ_LENS)\n        state_ins = tree.flatten(batch['state_in'])\n        if state_ins:\n            assert all((len(state_in) == len(state_ins[0]) for state_in in state_ins)), 'All state_in tensors should have the same batch_dim size.'\n            if len(state_ins[0]) == len(seq_lens):\n                states_already_reduced_to_init = True\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not _enable_new_api_stack and ('state_in_0' in batch or 'state_out_0' in batch):\n        if batch.get(SampleBatch.SEQ_LENS) is not None and len(batch['state_in_0']) == len(batch[SampleBatch.SEQ_LENS]):\n            states_already_reduced_to_init = True\n        if view_requirements['state_in_0'].shift_from is None:\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not meets_divisibility_reqs:\n        max_seq_len = batch_divisibility_req\n        dynamic_max = False\n        batch.max_seq_len = max_seq_len\n    else:\n        if shuffle:\n            batch.shuffle()\n        return\n    state_keys = []\n    feature_keys_ = feature_keys or []\n    for (k, v) in batch.items():\n        if k.startswith('state_in'):\n            state_keys.append(k)\n        elif not feature_keys and (not k.startswith('state_out') if not _enable_new_api_stack else True) and (k not in [SampleBatch.SEQ_LENS]):\n            feature_keys_.append(k)\n    (feature_sequences, initial_states, seq_lens) = chop_into_sequences(feature_columns=[batch[k] for k in feature_keys_], state_columns=[batch[k] for k in state_keys], episode_ids=batch.get(SampleBatch.EPS_ID), unroll_ids=batch.get(SampleBatch.UNROLL_ID), agent_indices=batch.get(SampleBatch.AGENT_INDEX), seq_lens=batch.get(SampleBatch.SEQ_LENS), max_seq_len=max_seq_len, dynamic_max=dynamic_max, states_already_reduced_to_init=states_already_reduced_to_init, shuffle=shuffle, handle_nested_data=True, padding=padding)\n    for (i, k) in enumerate(feature_keys_):\n        batch[k] = tree.unflatten_as(batch[k], feature_sequences[i])\n    for (i, k) in enumerate(state_keys):\n        batch[k] = initial_states[i]\n    batch[SampleBatch.SEQ_LENS] = np.array(seq_lens)\n    if dynamic_max:\n        batch.max_seq_len = max(seq_lens)\n    if log_once('rnn_ma_feed_dict'):\n        logger.info('Padded input for RNN/Attn.Nets/MA:\\n\\n{}\\n'.format(summarize({'features': feature_sequences, 'initial_states': initial_states, 'seq_lens': seq_lens, 'max_seq_len': max_seq_len})))",
            "@DeveloperAPI\ndef pad_batch_to_sequences_of_same_size(batch: SampleBatch, max_seq_len: int, shuffle: bool=False, batch_divisibility_req: int=1, feature_keys: Optional[List[str]]=None, view_requirements: Optional[ViewRequirementsDict]=None, _enable_new_api_stack: bool=False, padding: str='zero'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Applies padding to `batch` so it\\'s choppable into same-size sequences.\\n\\n    Shuffles `batch` (if desired), makes sure divisibility requirement is met,\\n    then pads the batch ([B, ...]) into same-size chunks ([B, ...]) w/o\\n    adding a time dimension (yet).\\n    Padding depends on episodes found in batch and `max_seq_len`.\\n\\n    Args:\\n        batch: The SampleBatch object. All values in here have\\n            the shape [B, ...].\\n        max_seq_len: The max. sequence length to use for chopping.\\n        shuffle: Whether to shuffle batch sequences. Shuffle may\\n            be done in-place. This only makes sense if you\\'re further\\n            applying minibatch SGD after getting the outputs.\\n        batch_divisibility_req: The int by which the batch dimension\\n            must be dividable.\\n        feature_keys: An optional list of keys to apply sequence-chopping\\n            to. If None, use all keys in batch that are not\\n            \"state_in/out_\"-type keys.\\n        view_requirements: An optional Policy ViewRequirements dict to\\n            be able to infer whether e.g. dynamic max\\'ing should be\\n            applied over the seq_lens.\\n        _enable_new_api_stack: This is a temporary flag to enable the new RLModule API.\\n            After a complete rollout of the new API, this flag will be removed.\\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\\n            will pad with zeros, last padding will pad with the last value.\\n    '\n    if batch.zero_padded:\n        return\n    batch.zero_padded = True\n    if batch_divisibility_req > 1:\n        meets_divisibility_reqs = len(batch[SampleBatch.CUR_OBS]) % batch_divisibility_req == 0 and max(batch[SampleBatch.AGENT_INDEX]) == 0\n    else:\n        meets_divisibility_reqs = True\n    states_already_reduced_to_init = False\n    if _enable_new_api_stack and ('state_in' in batch or 'state_out' in batch):\n        seq_lens = batch.get(SampleBatch.SEQ_LENS)\n        state_ins = tree.flatten(batch['state_in'])\n        if state_ins:\n            assert all((len(state_in) == len(state_ins[0]) for state_in in state_ins)), 'All state_in tensors should have the same batch_dim size.'\n            if len(state_ins[0]) == len(seq_lens):\n                states_already_reduced_to_init = True\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not _enable_new_api_stack and ('state_in_0' in batch or 'state_out_0' in batch):\n        if batch.get(SampleBatch.SEQ_LENS) is not None and len(batch['state_in_0']) == len(batch[SampleBatch.SEQ_LENS]):\n            states_already_reduced_to_init = True\n        if view_requirements['state_in_0'].shift_from is None:\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not meets_divisibility_reqs:\n        max_seq_len = batch_divisibility_req\n        dynamic_max = False\n        batch.max_seq_len = max_seq_len\n    else:\n        if shuffle:\n            batch.shuffle()\n        return\n    state_keys = []\n    feature_keys_ = feature_keys or []\n    for (k, v) in batch.items():\n        if k.startswith('state_in'):\n            state_keys.append(k)\n        elif not feature_keys and (not k.startswith('state_out') if not _enable_new_api_stack else True) and (k not in [SampleBatch.SEQ_LENS]):\n            feature_keys_.append(k)\n    (feature_sequences, initial_states, seq_lens) = chop_into_sequences(feature_columns=[batch[k] for k in feature_keys_], state_columns=[batch[k] for k in state_keys], episode_ids=batch.get(SampleBatch.EPS_ID), unroll_ids=batch.get(SampleBatch.UNROLL_ID), agent_indices=batch.get(SampleBatch.AGENT_INDEX), seq_lens=batch.get(SampleBatch.SEQ_LENS), max_seq_len=max_seq_len, dynamic_max=dynamic_max, states_already_reduced_to_init=states_already_reduced_to_init, shuffle=shuffle, handle_nested_data=True, padding=padding)\n    for (i, k) in enumerate(feature_keys_):\n        batch[k] = tree.unflatten_as(batch[k], feature_sequences[i])\n    for (i, k) in enumerate(state_keys):\n        batch[k] = initial_states[i]\n    batch[SampleBatch.SEQ_LENS] = np.array(seq_lens)\n    if dynamic_max:\n        batch.max_seq_len = max(seq_lens)\n    if log_once('rnn_ma_feed_dict'):\n        logger.info('Padded input for RNN/Attn.Nets/MA:\\n\\n{}\\n'.format(summarize({'features': feature_sequences, 'initial_states': initial_states, 'seq_lens': seq_lens, 'max_seq_len': max_seq_len})))",
            "@DeveloperAPI\ndef pad_batch_to_sequences_of_same_size(batch: SampleBatch, max_seq_len: int, shuffle: bool=False, batch_divisibility_req: int=1, feature_keys: Optional[List[str]]=None, view_requirements: Optional[ViewRequirementsDict]=None, _enable_new_api_stack: bool=False, padding: str='zero'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Applies padding to `batch` so it\\'s choppable into same-size sequences.\\n\\n    Shuffles `batch` (if desired), makes sure divisibility requirement is met,\\n    then pads the batch ([B, ...]) into same-size chunks ([B, ...]) w/o\\n    adding a time dimension (yet).\\n    Padding depends on episodes found in batch and `max_seq_len`.\\n\\n    Args:\\n        batch: The SampleBatch object. All values in here have\\n            the shape [B, ...].\\n        max_seq_len: The max. sequence length to use for chopping.\\n        shuffle: Whether to shuffle batch sequences. Shuffle may\\n            be done in-place. This only makes sense if you\\'re further\\n            applying minibatch SGD after getting the outputs.\\n        batch_divisibility_req: The int by which the batch dimension\\n            must be dividable.\\n        feature_keys: An optional list of keys to apply sequence-chopping\\n            to. If None, use all keys in batch that are not\\n            \"state_in/out_\"-type keys.\\n        view_requirements: An optional Policy ViewRequirements dict to\\n            be able to infer whether e.g. dynamic max\\'ing should be\\n            applied over the seq_lens.\\n        _enable_new_api_stack: This is a temporary flag to enable the new RLModule API.\\n            After a complete rollout of the new API, this flag will be removed.\\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\\n            will pad with zeros, last padding will pad with the last value.\\n    '\n    if batch.zero_padded:\n        return\n    batch.zero_padded = True\n    if batch_divisibility_req > 1:\n        meets_divisibility_reqs = len(batch[SampleBatch.CUR_OBS]) % batch_divisibility_req == 0 and max(batch[SampleBatch.AGENT_INDEX]) == 0\n    else:\n        meets_divisibility_reqs = True\n    states_already_reduced_to_init = False\n    if _enable_new_api_stack and ('state_in' in batch or 'state_out' in batch):\n        seq_lens = batch.get(SampleBatch.SEQ_LENS)\n        state_ins = tree.flatten(batch['state_in'])\n        if state_ins:\n            assert all((len(state_in) == len(state_ins[0]) for state_in in state_ins)), 'All state_in tensors should have the same batch_dim size.'\n            if len(state_ins[0]) == len(seq_lens):\n                states_already_reduced_to_init = True\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not _enable_new_api_stack and ('state_in_0' in batch or 'state_out_0' in batch):\n        if batch.get(SampleBatch.SEQ_LENS) is not None and len(batch['state_in_0']) == len(batch[SampleBatch.SEQ_LENS]):\n            states_already_reduced_to_init = True\n        if view_requirements['state_in_0'].shift_from is None:\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not meets_divisibility_reqs:\n        max_seq_len = batch_divisibility_req\n        dynamic_max = False\n        batch.max_seq_len = max_seq_len\n    else:\n        if shuffle:\n            batch.shuffle()\n        return\n    state_keys = []\n    feature_keys_ = feature_keys or []\n    for (k, v) in batch.items():\n        if k.startswith('state_in'):\n            state_keys.append(k)\n        elif not feature_keys and (not k.startswith('state_out') if not _enable_new_api_stack else True) and (k not in [SampleBatch.SEQ_LENS]):\n            feature_keys_.append(k)\n    (feature_sequences, initial_states, seq_lens) = chop_into_sequences(feature_columns=[batch[k] for k in feature_keys_], state_columns=[batch[k] for k in state_keys], episode_ids=batch.get(SampleBatch.EPS_ID), unroll_ids=batch.get(SampleBatch.UNROLL_ID), agent_indices=batch.get(SampleBatch.AGENT_INDEX), seq_lens=batch.get(SampleBatch.SEQ_LENS), max_seq_len=max_seq_len, dynamic_max=dynamic_max, states_already_reduced_to_init=states_already_reduced_to_init, shuffle=shuffle, handle_nested_data=True, padding=padding)\n    for (i, k) in enumerate(feature_keys_):\n        batch[k] = tree.unflatten_as(batch[k], feature_sequences[i])\n    for (i, k) in enumerate(state_keys):\n        batch[k] = initial_states[i]\n    batch[SampleBatch.SEQ_LENS] = np.array(seq_lens)\n    if dynamic_max:\n        batch.max_seq_len = max(seq_lens)\n    if log_once('rnn_ma_feed_dict'):\n        logger.info('Padded input for RNN/Attn.Nets/MA:\\n\\n{}\\n'.format(summarize({'features': feature_sequences, 'initial_states': initial_states, 'seq_lens': seq_lens, 'max_seq_len': max_seq_len})))",
            "@DeveloperAPI\ndef pad_batch_to_sequences_of_same_size(batch: SampleBatch, max_seq_len: int, shuffle: bool=False, batch_divisibility_req: int=1, feature_keys: Optional[List[str]]=None, view_requirements: Optional[ViewRequirementsDict]=None, _enable_new_api_stack: bool=False, padding: str='zero'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Applies padding to `batch` so it\\'s choppable into same-size sequences.\\n\\n    Shuffles `batch` (if desired), makes sure divisibility requirement is met,\\n    then pads the batch ([B, ...]) into same-size chunks ([B, ...]) w/o\\n    adding a time dimension (yet).\\n    Padding depends on episodes found in batch and `max_seq_len`.\\n\\n    Args:\\n        batch: The SampleBatch object. All values in here have\\n            the shape [B, ...].\\n        max_seq_len: The max. sequence length to use for chopping.\\n        shuffle: Whether to shuffle batch sequences. Shuffle may\\n            be done in-place. This only makes sense if you\\'re further\\n            applying minibatch SGD after getting the outputs.\\n        batch_divisibility_req: The int by which the batch dimension\\n            must be dividable.\\n        feature_keys: An optional list of keys to apply sequence-chopping\\n            to. If None, use all keys in batch that are not\\n            \"state_in/out_\"-type keys.\\n        view_requirements: An optional Policy ViewRequirements dict to\\n            be able to infer whether e.g. dynamic max\\'ing should be\\n            applied over the seq_lens.\\n        _enable_new_api_stack: This is a temporary flag to enable the new RLModule API.\\n            After a complete rollout of the new API, this flag will be removed.\\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\\n            will pad with zeros, last padding will pad with the last value.\\n    '\n    if batch.zero_padded:\n        return\n    batch.zero_padded = True\n    if batch_divisibility_req > 1:\n        meets_divisibility_reqs = len(batch[SampleBatch.CUR_OBS]) % batch_divisibility_req == 0 and max(batch[SampleBatch.AGENT_INDEX]) == 0\n    else:\n        meets_divisibility_reqs = True\n    states_already_reduced_to_init = False\n    if _enable_new_api_stack and ('state_in' in batch or 'state_out' in batch):\n        seq_lens = batch.get(SampleBatch.SEQ_LENS)\n        state_ins = tree.flatten(batch['state_in'])\n        if state_ins:\n            assert all((len(state_in) == len(state_ins[0]) for state_in in state_ins)), 'All state_in tensors should have the same batch_dim size.'\n            if len(state_ins[0]) == len(seq_lens):\n                states_already_reduced_to_init = True\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not _enable_new_api_stack and ('state_in_0' in batch or 'state_out_0' in batch):\n        if batch.get(SampleBatch.SEQ_LENS) is not None and len(batch['state_in_0']) == len(batch[SampleBatch.SEQ_LENS]):\n            states_already_reduced_to_init = True\n        if view_requirements['state_in_0'].shift_from is None:\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not meets_divisibility_reqs:\n        max_seq_len = batch_divisibility_req\n        dynamic_max = False\n        batch.max_seq_len = max_seq_len\n    else:\n        if shuffle:\n            batch.shuffle()\n        return\n    state_keys = []\n    feature_keys_ = feature_keys or []\n    for (k, v) in batch.items():\n        if k.startswith('state_in'):\n            state_keys.append(k)\n        elif not feature_keys and (not k.startswith('state_out') if not _enable_new_api_stack else True) and (k not in [SampleBatch.SEQ_LENS]):\n            feature_keys_.append(k)\n    (feature_sequences, initial_states, seq_lens) = chop_into_sequences(feature_columns=[batch[k] for k in feature_keys_], state_columns=[batch[k] for k in state_keys], episode_ids=batch.get(SampleBatch.EPS_ID), unroll_ids=batch.get(SampleBatch.UNROLL_ID), agent_indices=batch.get(SampleBatch.AGENT_INDEX), seq_lens=batch.get(SampleBatch.SEQ_LENS), max_seq_len=max_seq_len, dynamic_max=dynamic_max, states_already_reduced_to_init=states_already_reduced_to_init, shuffle=shuffle, handle_nested_data=True, padding=padding)\n    for (i, k) in enumerate(feature_keys_):\n        batch[k] = tree.unflatten_as(batch[k], feature_sequences[i])\n    for (i, k) in enumerate(state_keys):\n        batch[k] = initial_states[i]\n    batch[SampleBatch.SEQ_LENS] = np.array(seq_lens)\n    if dynamic_max:\n        batch.max_seq_len = max(seq_lens)\n    if log_once('rnn_ma_feed_dict'):\n        logger.info('Padded input for RNN/Attn.Nets/MA:\\n\\n{}\\n'.format(summarize({'features': feature_sequences, 'initial_states': initial_states, 'seq_lens': seq_lens, 'max_seq_len': max_seq_len})))",
            "@DeveloperAPI\ndef pad_batch_to_sequences_of_same_size(batch: SampleBatch, max_seq_len: int, shuffle: bool=False, batch_divisibility_req: int=1, feature_keys: Optional[List[str]]=None, view_requirements: Optional[ViewRequirementsDict]=None, _enable_new_api_stack: bool=False, padding: str='zero'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Applies padding to `batch` so it\\'s choppable into same-size sequences.\\n\\n    Shuffles `batch` (if desired), makes sure divisibility requirement is met,\\n    then pads the batch ([B, ...]) into same-size chunks ([B, ...]) w/o\\n    adding a time dimension (yet).\\n    Padding depends on episodes found in batch and `max_seq_len`.\\n\\n    Args:\\n        batch: The SampleBatch object. All values in here have\\n            the shape [B, ...].\\n        max_seq_len: The max. sequence length to use for chopping.\\n        shuffle: Whether to shuffle batch sequences. Shuffle may\\n            be done in-place. This only makes sense if you\\'re further\\n            applying minibatch SGD after getting the outputs.\\n        batch_divisibility_req: The int by which the batch dimension\\n            must be dividable.\\n        feature_keys: An optional list of keys to apply sequence-chopping\\n            to. If None, use all keys in batch that are not\\n            \"state_in/out_\"-type keys.\\n        view_requirements: An optional Policy ViewRequirements dict to\\n            be able to infer whether e.g. dynamic max\\'ing should be\\n            applied over the seq_lens.\\n        _enable_new_api_stack: This is a temporary flag to enable the new RLModule API.\\n            After a complete rollout of the new API, this flag will be removed.\\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\\n            will pad with zeros, last padding will pad with the last value.\\n    '\n    if batch.zero_padded:\n        return\n    batch.zero_padded = True\n    if batch_divisibility_req > 1:\n        meets_divisibility_reqs = len(batch[SampleBatch.CUR_OBS]) % batch_divisibility_req == 0 and max(batch[SampleBatch.AGENT_INDEX]) == 0\n    else:\n        meets_divisibility_reqs = True\n    states_already_reduced_to_init = False\n    if _enable_new_api_stack and ('state_in' in batch or 'state_out' in batch):\n        seq_lens = batch.get(SampleBatch.SEQ_LENS)\n        state_ins = tree.flatten(batch['state_in'])\n        if state_ins:\n            assert all((len(state_in) == len(state_ins[0]) for state_in in state_ins)), 'All state_in tensors should have the same batch_dim size.'\n            if len(state_ins[0]) == len(seq_lens):\n                states_already_reduced_to_init = True\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not _enable_new_api_stack and ('state_in_0' in batch or 'state_out_0' in batch):\n        if batch.get(SampleBatch.SEQ_LENS) is not None and len(batch['state_in_0']) == len(batch[SampleBatch.SEQ_LENS]):\n            states_already_reduced_to_init = True\n        if view_requirements['state_in_0'].shift_from is None:\n            dynamic_max = True\n        else:\n            dynamic_max = False\n    elif not meets_divisibility_reqs:\n        max_seq_len = batch_divisibility_req\n        dynamic_max = False\n        batch.max_seq_len = max_seq_len\n    else:\n        if shuffle:\n            batch.shuffle()\n        return\n    state_keys = []\n    feature_keys_ = feature_keys or []\n    for (k, v) in batch.items():\n        if k.startswith('state_in'):\n            state_keys.append(k)\n        elif not feature_keys and (not k.startswith('state_out') if not _enable_new_api_stack else True) and (k not in [SampleBatch.SEQ_LENS]):\n            feature_keys_.append(k)\n    (feature_sequences, initial_states, seq_lens) = chop_into_sequences(feature_columns=[batch[k] for k in feature_keys_], state_columns=[batch[k] for k in state_keys], episode_ids=batch.get(SampleBatch.EPS_ID), unroll_ids=batch.get(SampleBatch.UNROLL_ID), agent_indices=batch.get(SampleBatch.AGENT_INDEX), seq_lens=batch.get(SampleBatch.SEQ_LENS), max_seq_len=max_seq_len, dynamic_max=dynamic_max, states_already_reduced_to_init=states_already_reduced_to_init, shuffle=shuffle, handle_nested_data=True, padding=padding)\n    for (i, k) in enumerate(feature_keys_):\n        batch[k] = tree.unflatten_as(batch[k], feature_sequences[i])\n    for (i, k) in enumerate(state_keys):\n        batch[k] = initial_states[i]\n    batch[SampleBatch.SEQ_LENS] = np.array(seq_lens)\n    if dynamic_max:\n        batch.max_seq_len = max(seq_lens)\n    if log_once('rnn_ma_feed_dict'):\n        logger.info('Padded input for RNN/Attn.Nets/MA:\\n\\n{}\\n'.format(summarize({'features': feature_sequences, 'initial_states': initial_states, 'seq_lens': seq_lens, 'max_seq_len': max_seq_len})))"
        ]
    },
    {
        "func_name": "add_time_dimension",
        "original": "@DeveloperAPI\ndef add_time_dimension(padded_inputs: TensorType, *, seq_lens: TensorType, framework: str='tf', time_major: bool=False):\n    \"\"\"Adds a time dimension to padded inputs.\n\n    Args:\n        padded_inputs: a padded batch of sequences. That is,\n            for seq_lens=[1, 2, 2], then inputs=[A, *, B, B, C, C], where\n            A, B, C are sequence elements and * denotes padding.\n        seq_lens: A 1D tensor of sequence lengths, denoting the non-padded length\n            in timesteps of each rollout in the batch.\n        framework: The framework string (\"tf2\", \"tf\", \"torch\").\n        time_major: Whether data should be returned in time-major (TxB)\n            format or not (BxT).\n\n    Returns:\n        TensorType: Reshaped tensor of shape [B, T, ...] or [T, B, ...].\n    \"\"\"\n    if framework in ['tf2', 'tf']:\n        assert time_major is False, 'time-major not supported yet for tf!'\n        padded_inputs = tf.convert_to_tensor(padded_inputs)\n        padded_batch_size = tf.shape(padded_inputs)[0]\n        new_batch_size = tf.shape(seq_lens)[0]\n        time_size = padded_batch_size // new_batch_size\n        new_shape = tf.concat([tf.expand_dims(new_batch_size, axis=0), tf.expand_dims(time_size, axis=0), tf.shape(padded_inputs)[1:]], axis=0)\n        return tf.reshape(padded_inputs, new_shape)\n    elif framework == 'torch':\n        padded_inputs = torch.as_tensor(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.view(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs\n    else:\n        assert framework == 'np', 'Unknown framework: {}'.format(framework)\n        padded_inputs = np.asarray(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.reshape(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs",
        "mutated": [
            "@DeveloperAPI\ndef add_time_dimension(padded_inputs: TensorType, *, seq_lens: TensorType, framework: str='tf', time_major: bool=False):\n    if False:\n        i = 10\n    'Adds a time dimension to padded inputs.\\n\\n    Args:\\n        padded_inputs: a padded batch of sequences. That is,\\n            for seq_lens=[1, 2, 2], then inputs=[A, *, B, B, C, C], where\\n            A, B, C are sequence elements and * denotes padding.\\n        seq_lens: A 1D tensor of sequence lengths, denoting the non-padded length\\n            in timesteps of each rollout in the batch.\\n        framework: The framework string (\"tf2\", \"tf\", \"torch\").\\n        time_major: Whether data should be returned in time-major (TxB)\\n            format or not (BxT).\\n\\n    Returns:\\n        TensorType: Reshaped tensor of shape [B, T, ...] or [T, B, ...].\\n    '\n    if framework in ['tf2', 'tf']:\n        assert time_major is False, 'time-major not supported yet for tf!'\n        padded_inputs = tf.convert_to_tensor(padded_inputs)\n        padded_batch_size = tf.shape(padded_inputs)[0]\n        new_batch_size = tf.shape(seq_lens)[0]\n        time_size = padded_batch_size // new_batch_size\n        new_shape = tf.concat([tf.expand_dims(new_batch_size, axis=0), tf.expand_dims(time_size, axis=0), tf.shape(padded_inputs)[1:]], axis=0)\n        return tf.reshape(padded_inputs, new_shape)\n    elif framework == 'torch':\n        padded_inputs = torch.as_tensor(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.view(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs\n    else:\n        assert framework == 'np', 'Unknown framework: {}'.format(framework)\n        padded_inputs = np.asarray(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.reshape(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs",
            "@DeveloperAPI\ndef add_time_dimension(padded_inputs: TensorType, *, seq_lens: TensorType, framework: str='tf', time_major: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Adds a time dimension to padded inputs.\\n\\n    Args:\\n        padded_inputs: a padded batch of sequences. That is,\\n            for seq_lens=[1, 2, 2], then inputs=[A, *, B, B, C, C], where\\n            A, B, C are sequence elements and * denotes padding.\\n        seq_lens: A 1D tensor of sequence lengths, denoting the non-padded length\\n            in timesteps of each rollout in the batch.\\n        framework: The framework string (\"tf2\", \"tf\", \"torch\").\\n        time_major: Whether data should be returned in time-major (TxB)\\n            format or not (BxT).\\n\\n    Returns:\\n        TensorType: Reshaped tensor of shape [B, T, ...] or [T, B, ...].\\n    '\n    if framework in ['tf2', 'tf']:\n        assert time_major is False, 'time-major not supported yet for tf!'\n        padded_inputs = tf.convert_to_tensor(padded_inputs)\n        padded_batch_size = tf.shape(padded_inputs)[0]\n        new_batch_size = tf.shape(seq_lens)[0]\n        time_size = padded_batch_size // new_batch_size\n        new_shape = tf.concat([tf.expand_dims(new_batch_size, axis=0), tf.expand_dims(time_size, axis=0), tf.shape(padded_inputs)[1:]], axis=0)\n        return tf.reshape(padded_inputs, new_shape)\n    elif framework == 'torch':\n        padded_inputs = torch.as_tensor(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.view(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs\n    else:\n        assert framework == 'np', 'Unknown framework: {}'.format(framework)\n        padded_inputs = np.asarray(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.reshape(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs",
            "@DeveloperAPI\ndef add_time_dimension(padded_inputs: TensorType, *, seq_lens: TensorType, framework: str='tf', time_major: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Adds a time dimension to padded inputs.\\n\\n    Args:\\n        padded_inputs: a padded batch of sequences. That is,\\n            for seq_lens=[1, 2, 2], then inputs=[A, *, B, B, C, C], where\\n            A, B, C are sequence elements and * denotes padding.\\n        seq_lens: A 1D tensor of sequence lengths, denoting the non-padded length\\n            in timesteps of each rollout in the batch.\\n        framework: The framework string (\"tf2\", \"tf\", \"torch\").\\n        time_major: Whether data should be returned in time-major (TxB)\\n            format or not (BxT).\\n\\n    Returns:\\n        TensorType: Reshaped tensor of shape [B, T, ...] or [T, B, ...].\\n    '\n    if framework in ['tf2', 'tf']:\n        assert time_major is False, 'time-major not supported yet for tf!'\n        padded_inputs = tf.convert_to_tensor(padded_inputs)\n        padded_batch_size = tf.shape(padded_inputs)[0]\n        new_batch_size = tf.shape(seq_lens)[0]\n        time_size = padded_batch_size // new_batch_size\n        new_shape = tf.concat([tf.expand_dims(new_batch_size, axis=0), tf.expand_dims(time_size, axis=0), tf.shape(padded_inputs)[1:]], axis=0)\n        return tf.reshape(padded_inputs, new_shape)\n    elif framework == 'torch':\n        padded_inputs = torch.as_tensor(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.view(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs\n    else:\n        assert framework == 'np', 'Unknown framework: {}'.format(framework)\n        padded_inputs = np.asarray(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.reshape(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs",
            "@DeveloperAPI\ndef add_time_dimension(padded_inputs: TensorType, *, seq_lens: TensorType, framework: str='tf', time_major: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Adds a time dimension to padded inputs.\\n\\n    Args:\\n        padded_inputs: a padded batch of sequences. That is,\\n            for seq_lens=[1, 2, 2], then inputs=[A, *, B, B, C, C], where\\n            A, B, C are sequence elements and * denotes padding.\\n        seq_lens: A 1D tensor of sequence lengths, denoting the non-padded length\\n            in timesteps of each rollout in the batch.\\n        framework: The framework string (\"tf2\", \"tf\", \"torch\").\\n        time_major: Whether data should be returned in time-major (TxB)\\n            format or not (BxT).\\n\\n    Returns:\\n        TensorType: Reshaped tensor of shape [B, T, ...] or [T, B, ...].\\n    '\n    if framework in ['tf2', 'tf']:\n        assert time_major is False, 'time-major not supported yet for tf!'\n        padded_inputs = tf.convert_to_tensor(padded_inputs)\n        padded_batch_size = tf.shape(padded_inputs)[0]\n        new_batch_size = tf.shape(seq_lens)[0]\n        time_size = padded_batch_size // new_batch_size\n        new_shape = tf.concat([tf.expand_dims(new_batch_size, axis=0), tf.expand_dims(time_size, axis=0), tf.shape(padded_inputs)[1:]], axis=0)\n        return tf.reshape(padded_inputs, new_shape)\n    elif framework == 'torch':\n        padded_inputs = torch.as_tensor(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.view(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs\n    else:\n        assert framework == 'np', 'Unknown framework: {}'.format(framework)\n        padded_inputs = np.asarray(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.reshape(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs",
            "@DeveloperAPI\ndef add_time_dimension(padded_inputs: TensorType, *, seq_lens: TensorType, framework: str='tf', time_major: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Adds a time dimension to padded inputs.\\n\\n    Args:\\n        padded_inputs: a padded batch of sequences. That is,\\n            for seq_lens=[1, 2, 2], then inputs=[A, *, B, B, C, C], where\\n            A, B, C are sequence elements and * denotes padding.\\n        seq_lens: A 1D tensor of sequence lengths, denoting the non-padded length\\n            in timesteps of each rollout in the batch.\\n        framework: The framework string (\"tf2\", \"tf\", \"torch\").\\n        time_major: Whether data should be returned in time-major (TxB)\\n            format or not (BxT).\\n\\n    Returns:\\n        TensorType: Reshaped tensor of shape [B, T, ...] or [T, B, ...].\\n    '\n    if framework in ['tf2', 'tf']:\n        assert time_major is False, 'time-major not supported yet for tf!'\n        padded_inputs = tf.convert_to_tensor(padded_inputs)\n        padded_batch_size = tf.shape(padded_inputs)[0]\n        new_batch_size = tf.shape(seq_lens)[0]\n        time_size = padded_batch_size // new_batch_size\n        new_shape = tf.concat([tf.expand_dims(new_batch_size, axis=0), tf.expand_dims(time_size, axis=0), tf.shape(padded_inputs)[1:]], axis=0)\n        return tf.reshape(padded_inputs, new_shape)\n    elif framework == 'torch':\n        padded_inputs = torch.as_tensor(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.view(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs\n    else:\n        assert framework == 'np', 'Unknown framework: {}'.format(framework)\n        padded_inputs = np.asarray(padded_inputs)\n        padded_batch_size = padded_inputs.shape[0]\n        new_batch_size = seq_lens.shape[0]\n        time_size = padded_batch_size // new_batch_size\n        batch_major_shape = (new_batch_size, time_size) + padded_inputs.shape[1:]\n        padded_outputs = padded_inputs.reshape(batch_major_shape)\n        if time_major:\n            padded_outputs = padded_outputs.transpose(0, 1)\n        return padded_outputs"
        ]
    },
    {
        "func_name": "chop_into_sequences",
        "original": "@DeveloperAPI\ndef chop_into_sequences(*, feature_columns, state_columns, max_seq_len, episode_ids=None, unroll_ids=None, agent_indices=None, dynamic_max=True, shuffle=False, seq_lens=None, states_already_reduced_to_init=False, handle_nested_data=False, _extra_padding=0, padding: str='zero'):\n    \"\"\"Truncate and pad experiences into fixed-length sequences.\n\n    Args:\n        feature_columns: List of arrays containing features.\n        state_columns: List of arrays containing LSTM state values.\n        max_seq_len: Max length of sequences. Sequences longer than max_seq_len\n            will be split into subsequences that span the batch dimension\n            and sum to max_seq_len.\n        episode_ids (List[EpisodeID]): List of episode ids for each step.\n        unroll_ids (List[UnrollID]): List of identifiers for the sample batch.\n            This is used to make sure sequences are cut between sample batches.\n        agent_indices (List[AgentID]): List of agent ids for each step. Note\n            that this has to be combined with episode_ids for uniqueness.\n        dynamic_max: Whether to dynamically shrink the max seq len.\n            For example, if max len is 20 and the actual max seq len in the\n            data is 7, it will be shrunk to 7.\n        shuffle: Whether to shuffle the sequence outputs.\n        handle_nested_data: If True, assume that the data in\n            `feature_columns` could be nested structures (of data).\n            If False, assumes that all items in `feature_columns` are\n            only np.ndarrays (no nested structured of np.ndarrays).\n        _extra_padding: Add extra padding to the end of sequences.\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\n            will pad with zeros, last padding will pad with the last value.\n\n    Returns:\n        f_pad: Padded feature columns. These will be of shape\n            [NUM_SEQUENCES * MAX_SEQ_LEN, ...].\n        s_init: Initial states for each sequence, of shape\n            [NUM_SEQUENCES, ...].\n        seq_lens: List of sequence lengths, of shape [NUM_SEQUENCES].\n\n    .. testcode::\n        :skipif: True\n\n        from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n        f_pad, s_init, seq_lens = chop_into_sequences(\n            episode_ids=[1, 1, 5, 5, 5, 5],\n            unroll_ids=[4, 4, 4, 4, 4, 4],\n            agent_indices=[0, 0, 0, 0, 0, 0],\n            feature_columns=[[4, 4, 8, 8, 8, 8],\n                             [1, 1, 0, 1, 1, 0]],\n            state_columns=[[4, 5, 4, 5, 5, 5]],\n            max_seq_len=3)\n        print(f_pad)\n        print(s_init)\n        print(seq_lens)\n\n\n    .. testoutput::\n\n        [[4, 4, 0, 8, 8, 8, 8, 0, 0],\n         [1, 1, 0, 0, 1, 1, 0, 0, 0]]\n        [[4, 4, 5]]\n        [2, 3, 1]\n    \"\"\"\n    if seq_lens is None or len(seq_lens) == 0:\n        prev_id = None\n        seq_lens = []\n        seq_len = 0\n        unique_ids = np.add(np.add(episode_ids, agent_indices), np.array(unroll_ids, dtype=np.int64) << 32)\n        for uid in unique_ids:\n            if prev_id is not None and uid != prev_id or seq_len >= max_seq_len:\n                seq_lens.append(seq_len)\n                seq_len = 0\n            seq_len += 1\n            prev_id = uid\n        if seq_len:\n            seq_lens.append(seq_len)\n        seq_lens = np.array(seq_lens, dtype=np.int32)\n    if dynamic_max:\n        max_seq_len = max(seq_lens) + _extra_padding\n    feature_sequences = []\n    for col in feature_columns:\n        if isinstance(col, list):\n            col = np.array(col)\n        feature_sequences.append([])\n        for f in tree.flatten(col):\n            if not isinstance(f, np.ndarray):\n                f = np.array(f)\n            length = len(seq_lens) * max_seq_len\n            if f.dtype == object or f.dtype.type is np.str_:\n                f_pad = [None] * length\n            else:\n                f_pad = np.zeros((length,) + np.shape(f)[1:], dtype=f.dtype)\n            seq_base = 0\n            i = 0\n            for len_ in seq_lens:\n                for seq_offset in range(len_):\n                    f_pad[seq_base + seq_offset] = f[i]\n                    i += 1\n                if padding == 'last':\n                    for seq_offset in range(len_, max_seq_len):\n                        f_pad[seq_base + seq_offset] = f[i - 1]\n                seq_base += max_seq_len\n            assert i == len(f), f\n            feature_sequences[-1].append(f_pad)\n    if states_already_reduced_to_init:\n        initial_states = state_columns\n    else:\n        initial_states = []\n        for state_column in state_columns:\n            if isinstance(state_column, list):\n                state_column = np.array(state_column)\n            initial_state_flat = []\n            for s in tree.flatten(state_column):\n                if not isinstance(s, np.ndarray):\n                    s = np.array(s)\n                s_init = []\n                i = 0\n                for len_ in seq_lens:\n                    s_init.append(s[i])\n                    i += len_\n                initial_state_flat.append(np.array(s_init))\n            initial_states.append(tree.unflatten_as(state_column, initial_state_flat))\n    if shuffle:\n        permutation = np.random.permutation(len(seq_lens))\n        for (i, f) in enumerate(tree.flatten(feature_sequences)):\n            orig_shape = f.shape\n            f = np.reshape(f, (len(seq_lens), -1) + f.shape[1:])\n            f = f[permutation]\n            f = np.reshape(f, orig_shape)\n            feature_sequences[i] = f\n        for (i, s) in enumerate(initial_states):\n            s = s[permutation]\n            initial_states[i] = s\n        seq_lens = seq_lens[permutation]\n    if not handle_nested_data:\n        feature_sequences = [f[0] for f in feature_sequences]\n    return (feature_sequences, initial_states, seq_lens)",
        "mutated": [
            "@DeveloperAPI\ndef chop_into_sequences(*, feature_columns, state_columns, max_seq_len, episode_ids=None, unroll_ids=None, agent_indices=None, dynamic_max=True, shuffle=False, seq_lens=None, states_already_reduced_to_init=False, handle_nested_data=False, _extra_padding=0, padding: str='zero'):\n    if False:\n        i = 10\n    'Truncate and pad experiences into fixed-length sequences.\\n\\n    Args:\\n        feature_columns: List of arrays containing features.\\n        state_columns: List of arrays containing LSTM state values.\\n        max_seq_len: Max length of sequences. Sequences longer than max_seq_len\\n            will be split into subsequences that span the batch dimension\\n            and sum to max_seq_len.\\n        episode_ids (List[EpisodeID]): List of episode ids for each step.\\n        unroll_ids (List[UnrollID]): List of identifiers for the sample batch.\\n            This is used to make sure sequences are cut between sample batches.\\n        agent_indices (List[AgentID]): List of agent ids for each step. Note\\n            that this has to be combined with episode_ids for uniqueness.\\n        dynamic_max: Whether to dynamically shrink the max seq len.\\n            For example, if max len is 20 and the actual max seq len in the\\n            data is 7, it will be shrunk to 7.\\n        shuffle: Whether to shuffle the sequence outputs.\\n        handle_nested_data: If True, assume that the data in\\n            `feature_columns` could be nested structures (of data).\\n            If False, assumes that all items in `feature_columns` are\\n            only np.ndarrays (no nested structured of np.ndarrays).\\n        _extra_padding: Add extra padding to the end of sequences.\\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\\n            will pad with zeros, last padding will pad with the last value.\\n\\n    Returns:\\n        f_pad: Padded feature columns. These will be of shape\\n            [NUM_SEQUENCES * MAX_SEQ_LEN, ...].\\n        s_init: Initial states for each sequence, of shape\\n            [NUM_SEQUENCES, ...].\\n        seq_lens: List of sequence lengths, of shape [NUM_SEQUENCES].\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.policy.rnn_sequencing import chop_into_sequences\\n        f_pad, s_init, seq_lens = chop_into_sequences(\\n            episode_ids=[1, 1, 5, 5, 5, 5],\\n            unroll_ids=[4, 4, 4, 4, 4, 4],\\n            agent_indices=[0, 0, 0, 0, 0, 0],\\n            feature_columns=[[4, 4, 8, 8, 8, 8],\\n                             [1, 1, 0, 1, 1, 0]],\\n            state_columns=[[4, 5, 4, 5, 5, 5]],\\n            max_seq_len=3)\\n        print(f_pad)\\n        print(s_init)\\n        print(seq_lens)\\n\\n\\n    .. testoutput::\\n\\n        [[4, 4, 0, 8, 8, 8, 8, 0, 0],\\n         [1, 1, 0, 0, 1, 1, 0, 0, 0]]\\n        [[4, 4, 5]]\\n        [2, 3, 1]\\n    '\n    if seq_lens is None or len(seq_lens) == 0:\n        prev_id = None\n        seq_lens = []\n        seq_len = 0\n        unique_ids = np.add(np.add(episode_ids, agent_indices), np.array(unroll_ids, dtype=np.int64) << 32)\n        for uid in unique_ids:\n            if prev_id is not None and uid != prev_id or seq_len >= max_seq_len:\n                seq_lens.append(seq_len)\n                seq_len = 0\n            seq_len += 1\n            prev_id = uid\n        if seq_len:\n            seq_lens.append(seq_len)\n        seq_lens = np.array(seq_lens, dtype=np.int32)\n    if dynamic_max:\n        max_seq_len = max(seq_lens) + _extra_padding\n    feature_sequences = []\n    for col in feature_columns:\n        if isinstance(col, list):\n            col = np.array(col)\n        feature_sequences.append([])\n        for f in tree.flatten(col):\n            if not isinstance(f, np.ndarray):\n                f = np.array(f)\n            length = len(seq_lens) * max_seq_len\n            if f.dtype == object or f.dtype.type is np.str_:\n                f_pad = [None] * length\n            else:\n                f_pad = np.zeros((length,) + np.shape(f)[1:], dtype=f.dtype)\n            seq_base = 0\n            i = 0\n            for len_ in seq_lens:\n                for seq_offset in range(len_):\n                    f_pad[seq_base + seq_offset] = f[i]\n                    i += 1\n                if padding == 'last':\n                    for seq_offset in range(len_, max_seq_len):\n                        f_pad[seq_base + seq_offset] = f[i - 1]\n                seq_base += max_seq_len\n            assert i == len(f), f\n            feature_sequences[-1].append(f_pad)\n    if states_already_reduced_to_init:\n        initial_states = state_columns\n    else:\n        initial_states = []\n        for state_column in state_columns:\n            if isinstance(state_column, list):\n                state_column = np.array(state_column)\n            initial_state_flat = []\n            for s in tree.flatten(state_column):\n                if not isinstance(s, np.ndarray):\n                    s = np.array(s)\n                s_init = []\n                i = 0\n                for len_ in seq_lens:\n                    s_init.append(s[i])\n                    i += len_\n                initial_state_flat.append(np.array(s_init))\n            initial_states.append(tree.unflatten_as(state_column, initial_state_flat))\n    if shuffle:\n        permutation = np.random.permutation(len(seq_lens))\n        for (i, f) in enumerate(tree.flatten(feature_sequences)):\n            orig_shape = f.shape\n            f = np.reshape(f, (len(seq_lens), -1) + f.shape[1:])\n            f = f[permutation]\n            f = np.reshape(f, orig_shape)\n            feature_sequences[i] = f\n        for (i, s) in enumerate(initial_states):\n            s = s[permutation]\n            initial_states[i] = s\n        seq_lens = seq_lens[permutation]\n    if not handle_nested_data:\n        feature_sequences = [f[0] for f in feature_sequences]\n    return (feature_sequences, initial_states, seq_lens)",
            "@DeveloperAPI\ndef chop_into_sequences(*, feature_columns, state_columns, max_seq_len, episode_ids=None, unroll_ids=None, agent_indices=None, dynamic_max=True, shuffle=False, seq_lens=None, states_already_reduced_to_init=False, handle_nested_data=False, _extra_padding=0, padding: str='zero'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Truncate and pad experiences into fixed-length sequences.\\n\\n    Args:\\n        feature_columns: List of arrays containing features.\\n        state_columns: List of arrays containing LSTM state values.\\n        max_seq_len: Max length of sequences. Sequences longer than max_seq_len\\n            will be split into subsequences that span the batch dimension\\n            and sum to max_seq_len.\\n        episode_ids (List[EpisodeID]): List of episode ids for each step.\\n        unroll_ids (List[UnrollID]): List of identifiers for the sample batch.\\n            This is used to make sure sequences are cut between sample batches.\\n        agent_indices (List[AgentID]): List of agent ids for each step. Note\\n            that this has to be combined with episode_ids for uniqueness.\\n        dynamic_max: Whether to dynamically shrink the max seq len.\\n            For example, if max len is 20 and the actual max seq len in the\\n            data is 7, it will be shrunk to 7.\\n        shuffle: Whether to shuffle the sequence outputs.\\n        handle_nested_data: If True, assume that the data in\\n            `feature_columns` could be nested structures (of data).\\n            If False, assumes that all items in `feature_columns` are\\n            only np.ndarrays (no nested structured of np.ndarrays).\\n        _extra_padding: Add extra padding to the end of sequences.\\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\\n            will pad with zeros, last padding will pad with the last value.\\n\\n    Returns:\\n        f_pad: Padded feature columns. These will be of shape\\n            [NUM_SEQUENCES * MAX_SEQ_LEN, ...].\\n        s_init: Initial states for each sequence, of shape\\n            [NUM_SEQUENCES, ...].\\n        seq_lens: List of sequence lengths, of shape [NUM_SEQUENCES].\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.policy.rnn_sequencing import chop_into_sequences\\n        f_pad, s_init, seq_lens = chop_into_sequences(\\n            episode_ids=[1, 1, 5, 5, 5, 5],\\n            unroll_ids=[4, 4, 4, 4, 4, 4],\\n            agent_indices=[0, 0, 0, 0, 0, 0],\\n            feature_columns=[[4, 4, 8, 8, 8, 8],\\n                             [1, 1, 0, 1, 1, 0]],\\n            state_columns=[[4, 5, 4, 5, 5, 5]],\\n            max_seq_len=3)\\n        print(f_pad)\\n        print(s_init)\\n        print(seq_lens)\\n\\n\\n    .. testoutput::\\n\\n        [[4, 4, 0, 8, 8, 8, 8, 0, 0],\\n         [1, 1, 0, 0, 1, 1, 0, 0, 0]]\\n        [[4, 4, 5]]\\n        [2, 3, 1]\\n    '\n    if seq_lens is None or len(seq_lens) == 0:\n        prev_id = None\n        seq_lens = []\n        seq_len = 0\n        unique_ids = np.add(np.add(episode_ids, agent_indices), np.array(unroll_ids, dtype=np.int64) << 32)\n        for uid in unique_ids:\n            if prev_id is not None and uid != prev_id or seq_len >= max_seq_len:\n                seq_lens.append(seq_len)\n                seq_len = 0\n            seq_len += 1\n            prev_id = uid\n        if seq_len:\n            seq_lens.append(seq_len)\n        seq_lens = np.array(seq_lens, dtype=np.int32)\n    if dynamic_max:\n        max_seq_len = max(seq_lens) + _extra_padding\n    feature_sequences = []\n    for col in feature_columns:\n        if isinstance(col, list):\n            col = np.array(col)\n        feature_sequences.append([])\n        for f in tree.flatten(col):\n            if not isinstance(f, np.ndarray):\n                f = np.array(f)\n            length = len(seq_lens) * max_seq_len\n            if f.dtype == object or f.dtype.type is np.str_:\n                f_pad = [None] * length\n            else:\n                f_pad = np.zeros((length,) + np.shape(f)[1:], dtype=f.dtype)\n            seq_base = 0\n            i = 0\n            for len_ in seq_lens:\n                for seq_offset in range(len_):\n                    f_pad[seq_base + seq_offset] = f[i]\n                    i += 1\n                if padding == 'last':\n                    for seq_offset in range(len_, max_seq_len):\n                        f_pad[seq_base + seq_offset] = f[i - 1]\n                seq_base += max_seq_len\n            assert i == len(f), f\n            feature_sequences[-1].append(f_pad)\n    if states_already_reduced_to_init:\n        initial_states = state_columns\n    else:\n        initial_states = []\n        for state_column in state_columns:\n            if isinstance(state_column, list):\n                state_column = np.array(state_column)\n            initial_state_flat = []\n            for s in tree.flatten(state_column):\n                if not isinstance(s, np.ndarray):\n                    s = np.array(s)\n                s_init = []\n                i = 0\n                for len_ in seq_lens:\n                    s_init.append(s[i])\n                    i += len_\n                initial_state_flat.append(np.array(s_init))\n            initial_states.append(tree.unflatten_as(state_column, initial_state_flat))\n    if shuffle:\n        permutation = np.random.permutation(len(seq_lens))\n        for (i, f) in enumerate(tree.flatten(feature_sequences)):\n            orig_shape = f.shape\n            f = np.reshape(f, (len(seq_lens), -1) + f.shape[1:])\n            f = f[permutation]\n            f = np.reshape(f, orig_shape)\n            feature_sequences[i] = f\n        for (i, s) in enumerate(initial_states):\n            s = s[permutation]\n            initial_states[i] = s\n        seq_lens = seq_lens[permutation]\n    if not handle_nested_data:\n        feature_sequences = [f[0] for f in feature_sequences]\n    return (feature_sequences, initial_states, seq_lens)",
            "@DeveloperAPI\ndef chop_into_sequences(*, feature_columns, state_columns, max_seq_len, episode_ids=None, unroll_ids=None, agent_indices=None, dynamic_max=True, shuffle=False, seq_lens=None, states_already_reduced_to_init=False, handle_nested_data=False, _extra_padding=0, padding: str='zero'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Truncate and pad experiences into fixed-length sequences.\\n\\n    Args:\\n        feature_columns: List of arrays containing features.\\n        state_columns: List of arrays containing LSTM state values.\\n        max_seq_len: Max length of sequences. Sequences longer than max_seq_len\\n            will be split into subsequences that span the batch dimension\\n            and sum to max_seq_len.\\n        episode_ids (List[EpisodeID]): List of episode ids for each step.\\n        unroll_ids (List[UnrollID]): List of identifiers for the sample batch.\\n            This is used to make sure sequences are cut between sample batches.\\n        agent_indices (List[AgentID]): List of agent ids for each step. Note\\n            that this has to be combined with episode_ids for uniqueness.\\n        dynamic_max: Whether to dynamically shrink the max seq len.\\n            For example, if max len is 20 and the actual max seq len in the\\n            data is 7, it will be shrunk to 7.\\n        shuffle: Whether to shuffle the sequence outputs.\\n        handle_nested_data: If True, assume that the data in\\n            `feature_columns` could be nested structures (of data).\\n            If False, assumes that all items in `feature_columns` are\\n            only np.ndarrays (no nested structured of np.ndarrays).\\n        _extra_padding: Add extra padding to the end of sequences.\\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\\n            will pad with zeros, last padding will pad with the last value.\\n\\n    Returns:\\n        f_pad: Padded feature columns. These will be of shape\\n            [NUM_SEQUENCES * MAX_SEQ_LEN, ...].\\n        s_init: Initial states for each sequence, of shape\\n            [NUM_SEQUENCES, ...].\\n        seq_lens: List of sequence lengths, of shape [NUM_SEQUENCES].\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.policy.rnn_sequencing import chop_into_sequences\\n        f_pad, s_init, seq_lens = chop_into_sequences(\\n            episode_ids=[1, 1, 5, 5, 5, 5],\\n            unroll_ids=[4, 4, 4, 4, 4, 4],\\n            agent_indices=[0, 0, 0, 0, 0, 0],\\n            feature_columns=[[4, 4, 8, 8, 8, 8],\\n                             [1, 1, 0, 1, 1, 0]],\\n            state_columns=[[4, 5, 4, 5, 5, 5]],\\n            max_seq_len=3)\\n        print(f_pad)\\n        print(s_init)\\n        print(seq_lens)\\n\\n\\n    .. testoutput::\\n\\n        [[4, 4, 0, 8, 8, 8, 8, 0, 0],\\n         [1, 1, 0, 0, 1, 1, 0, 0, 0]]\\n        [[4, 4, 5]]\\n        [2, 3, 1]\\n    '\n    if seq_lens is None or len(seq_lens) == 0:\n        prev_id = None\n        seq_lens = []\n        seq_len = 0\n        unique_ids = np.add(np.add(episode_ids, agent_indices), np.array(unroll_ids, dtype=np.int64) << 32)\n        for uid in unique_ids:\n            if prev_id is not None and uid != prev_id or seq_len >= max_seq_len:\n                seq_lens.append(seq_len)\n                seq_len = 0\n            seq_len += 1\n            prev_id = uid\n        if seq_len:\n            seq_lens.append(seq_len)\n        seq_lens = np.array(seq_lens, dtype=np.int32)\n    if dynamic_max:\n        max_seq_len = max(seq_lens) + _extra_padding\n    feature_sequences = []\n    for col in feature_columns:\n        if isinstance(col, list):\n            col = np.array(col)\n        feature_sequences.append([])\n        for f in tree.flatten(col):\n            if not isinstance(f, np.ndarray):\n                f = np.array(f)\n            length = len(seq_lens) * max_seq_len\n            if f.dtype == object or f.dtype.type is np.str_:\n                f_pad = [None] * length\n            else:\n                f_pad = np.zeros((length,) + np.shape(f)[1:], dtype=f.dtype)\n            seq_base = 0\n            i = 0\n            for len_ in seq_lens:\n                for seq_offset in range(len_):\n                    f_pad[seq_base + seq_offset] = f[i]\n                    i += 1\n                if padding == 'last':\n                    for seq_offset in range(len_, max_seq_len):\n                        f_pad[seq_base + seq_offset] = f[i - 1]\n                seq_base += max_seq_len\n            assert i == len(f), f\n            feature_sequences[-1].append(f_pad)\n    if states_already_reduced_to_init:\n        initial_states = state_columns\n    else:\n        initial_states = []\n        for state_column in state_columns:\n            if isinstance(state_column, list):\n                state_column = np.array(state_column)\n            initial_state_flat = []\n            for s in tree.flatten(state_column):\n                if not isinstance(s, np.ndarray):\n                    s = np.array(s)\n                s_init = []\n                i = 0\n                for len_ in seq_lens:\n                    s_init.append(s[i])\n                    i += len_\n                initial_state_flat.append(np.array(s_init))\n            initial_states.append(tree.unflatten_as(state_column, initial_state_flat))\n    if shuffle:\n        permutation = np.random.permutation(len(seq_lens))\n        for (i, f) in enumerate(tree.flatten(feature_sequences)):\n            orig_shape = f.shape\n            f = np.reshape(f, (len(seq_lens), -1) + f.shape[1:])\n            f = f[permutation]\n            f = np.reshape(f, orig_shape)\n            feature_sequences[i] = f\n        for (i, s) in enumerate(initial_states):\n            s = s[permutation]\n            initial_states[i] = s\n        seq_lens = seq_lens[permutation]\n    if not handle_nested_data:\n        feature_sequences = [f[0] for f in feature_sequences]\n    return (feature_sequences, initial_states, seq_lens)",
            "@DeveloperAPI\ndef chop_into_sequences(*, feature_columns, state_columns, max_seq_len, episode_ids=None, unroll_ids=None, agent_indices=None, dynamic_max=True, shuffle=False, seq_lens=None, states_already_reduced_to_init=False, handle_nested_data=False, _extra_padding=0, padding: str='zero'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Truncate and pad experiences into fixed-length sequences.\\n\\n    Args:\\n        feature_columns: List of arrays containing features.\\n        state_columns: List of arrays containing LSTM state values.\\n        max_seq_len: Max length of sequences. Sequences longer than max_seq_len\\n            will be split into subsequences that span the batch dimension\\n            and sum to max_seq_len.\\n        episode_ids (List[EpisodeID]): List of episode ids for each step.\\n        unroll_ids (List[UnrollID]): List of identifiers for the sample batch.\\n            This is used to make sure sequences are cut between sample batches.\\n        agent_indices (List[AgentID]): List of agent ids for each step. Note\\n            that this has to be combined with episode_ids for uniqueness.\\n        dynamic_max: Whether to dynamically shrink the max seq len.\\n            For example, if max len is 20 and the actual max seq len in the\\n            data is 7, it will be shrunk to 7.\\n        shuffle: Whether to shuffle the sequence outputs.\\n        handle_nested_data: If True, assume that the data in\\n            `feature_columns` could be nested structures (of data).\\n            If False, assumes that all items in `feature_columns` are\\n            only np.ndarrays (no nested structured of np.ndarrays).\\n        _extra_padding: Add extra padding to the end of sequences.\\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\\n            will pad with zeros, last padding will pad with the last value.\\n\\n    Returns:\\n        f_pad: Padded feature columns. These will be of shape\\n            [NUM_SEQUENCES * MAX_SEQ_LEN, ...].\\n        s_init: Initial states for each sequence, of shape\\n            [NUM_SEQUENCES, ...].\\n        seq_lens: List of sequence lengths, of shape [NUM_SEQUENCES].\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.policy.rnn_sequencing import chop_into_sequences\\n        f_pad, s_init, seq_lens = chop_into_sequences(\\n            episode_ids=[1, 1, 5, 5, 5, 5],\\n            unroll_ids=[4, 4, 4, 4, 4, 4],\\n            agent_indices=[0, 0, 0, 0, 0, 0],\\n            feature_columns=[[4, 4, 8, 8, 8, 8],\\n                             [1, 1, 0, 1, 1, 0]],\\n            state_columns=[[4, 5, 4, 5, 5, 5]],\\n            max_seq_len=3)\\n        print(f_pad)\\n        print(s_init)\\n        print(seq_lens)\\n\\n\\n    .. testoutput::\\n\\n        [[4, 4, 0, 8, 8, 8, 8, 0, 0],\\n         [1, 1, 0, 0, 1, 1, 0, 0, 0]]\\n        [[4, 4, 5]]\\n        [2, 3, 1]\\n    '\n    if seq_lens is None or len(seq_lens) == 0:\n        prev_id = None\n        seq_lens = []\n        seq_len = 0\n        unique_ids = np.add(np.add(episode_ids, agent_indices), np.array(unroll_ids, dtype=np.int64) << 32)\n        for uid in unique_ids:\n            if prev_id is not None and uid != prev_id or seq_len >= max_seq_len:\n                seq_lens.append(seq_len)\n                seq_len = 0\n            seq_len += 1\n            prev_id = uid\n        if seq_len:\n            seq_lens.append(seq_len)\n        seq_lens = np.array(seq_lens, dtype=np.int32)\n    if dynamic_max:\n        max_seq_len = max(seq_lens) + _extra_padding\n    feature_sequences = []\n    for col in feature_columns:\n        if isinstance(col, list):\n            col = np.array(col)\n        feature_sequences.append([])\n        for f in tree.flatten(col):\n            if not isinstance(f, np.ndarray):\n                f = np.array(f)\n            length = len(seq_lens) * max_seq_len\n            if f.dtype == object or f.dtype.type is np.str_:\n                f_pad = [None] * length\n            else:\n                f_pad = np.zeros((length,) + np.shape(f)[1:], dtype=f.dtype)\n            seq_base = 0\n            i = 0\n            for len_ in seq_lens:\n                for seq_offset in range(len_):\n                    f_pad[seq_base + seq_offset] = f[i]\n                    i += 1\n                if padding == 'last':\n                    for seq_offset in range(len_, max_seq_len):\n                        f_pad[seq_base + seq_offset] = f[i - 1]\n                seq_base += max_seq_len\n            assert i == len(f), f\n            feature_sequences[-1].append(f_pad)\n    if states_already_reduced_to_init:\n        initial_states = state_columns\n    else:\n        initial_states = []\n        for state_column in state_columns:\n            if isinstance(state_column, list):\n                state_column = np.array(state_column)\n            initial_state_flat = []\n            for s in tree.flatten(state_column):\n                if not isinstance(s, np.ndarray):\n                    s = np.array(s)\n                s_init = []\n                i = 0\n                for len_ in seq_lens:\n                    s_init.append(s[i])\n                    i += len_\n                initial_state_flat.append(np.array(s_init))\n            initial_states.append(tree.unflatten_as(state_column, initial_state_flat))\n    if shuffle:\n        permutation = np.random.permutation(len(seq_lens))\n        for (i, f) in enumerate(tree.flatten(feature_sequences)):\n            orig_shape = f.shape\n            f = np.reshape(f, (len(seq_lens), -1) + f.shape[1:])\n            f = f[permutation]\n            f = np.reshape(f, orig_shape)\n            feature_sequences[i] = f\n        for (i, s) in enumerate(initial_states):\n            s = s[permutation]\n            initial_states[i] = s\n        seq_lens = seq_lens[permutation]\n    if not handle_nested_data:\n        feature_sequences = [f[0] for f in feature_sequences]\n    return (feature_sequences, initial_states, seq_lens)",
            "@DeveloperAPI\ndef chop_into_sequences(*, feature_columns, state_columns, max_seq_len, episode_ids=None, unroll_ids=None, agent_indices=None, dynamic_max=True, shuffle=False, seq_lens=None, states_already_reduced_to_init=False, handle_nested_data=False, _extra_padding=0, padding: str='zero'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Truncate and pad experiences into fixed-length sequences.\\n\\n    Args:\\n        feature_columns: List of arrays containing features.\\n        state_columns: List of arrays containing LSTM state values.\\n        max_seq_len: Max length of sequences. Sequences longer than max_seq_len\\n            will be split into subsequences that span the batch dimension\\n            and sum to max_seq_len.\\n        episode_ids (List[EpisodeID]): List of episode ids for each step.\\n        unroll_ids (List[UnrollID]): List of identifiers for the sample batch.\\n            This is used to make sure sequences are cut between sample batches.\\n        agent_indices (List[AgentID]): List of agent ids for each step. Note\\n            that this has to be combined with episode_ids for uniqueness.\\n        dynamic_max: Whether to dynamically shrink the max seq len.\\n            For example, if max len is 20 and the actual max seq len in the\\n            data is 7, it will be shrunk to 7.\\n        shuffle: Whether to shuffle the sequence outputs.\\n        handle_nested_data: If True, assume that the data in\\n            `feature_columns` could be nested structures (of data).\\n            If False, assumes that all items in `feature_columns` are\\n            only np.ndarrays (no nested structured of np.ndarrays).\\n        _extra_padding: Add extra padding to the end of sequences.\\n        padding: Padding type to use. Either \"zero\" or \"last\". Zero padding\\n            will pad with zeros, last padding will pad with the last value.\\n\\n    Returns:\\n        f_pad: Padded feature columns. These will be of shape\\n            [NUM_SEQUENCES * MAX_SEQ_LEN, ...].\\n        s_init: Initial states for each sequence, of shape\\n            [NUM_SEQUENCES, ...].\\n        seq_lens: List of sequence lengths, of shape [NUM_SEQUENCES].\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.policy.rnn_sequencing import chop_into_sequences\\n        f_pad, s_init, seq_lens = chop_into_sequences(\\n            episode_ids=[1, 1, 5, 5, 5, 5],\\n            unroll_ids=[4, 4, 4, 4, 4, 4],\\n            agent_indices=[0, 0, 0, 0, 0, 0],\\n            feature_columns=[[4, 4, 8, 8, 8, 8],\\n                             [1, 1, 0, 1, 1, 0]],\\n            state_columns=[[4, 5, 4, 5, 5, 5]],\\n            max_seq_len=3)\\n        print(f_pad)\\n        print(s_init)\\n        print(seq_lens)\\n\\n\\n    .. testoutput::\\n\\n        [[4, 4, 0, 8, 8, 8, 8, 0, 0],\\n         [1, 1, 0, 0, 1, 1, 0, 0, 0]]\\n        [[4, 4, 5]]\\n        [2, 3, 1]\\n    '\n    if seq_lens is None or len(seq_lens) == 0:\n        prev_id = None\n        seq_lens = []\n        seq_len = 0\n        unique_ids = np.add(np.add(episode_ids, agent_indices), np.array(unroll_ids, dtype=np.int64) << 32)\n        for uid in unique_ids:\n            if prev_id is not None and uid != prev_id or seq_len >= max_seq_len:\n                seq_lens.append(seq_len)\n                seq_len = 0\n            seq_len += 1\n            prev_id = uid\n        if seq_len:\n            seq_lens.append(seq_len)\n        seq_lens = np.array(seq_lens, dtype=np.int32)\n    if dynamic_max:\n        max_seq_len = max(seq_lens) + _extra_padding\n    feature_sequences = []\n    for col in feature_columns:\n        if isinstance(col, list):\n            col = np.array(col)\n        feature_sequences.append([])\n        for f in tree.flatten(col):\n            if not isinstance(f, np.ndarray):\n                f = np.array(f)\n            length = len(seq_lens) * max_seq_len\n            if f.dtype == object or f.dtype.type is np.str_:\n                f_pad = [None] * length\n            else:\n                f_pad = np.zeros((length,) + np.shape(f)[1:], dtype=f.dtype)\n            seq_base = 0\n            i = 0\n            for len_ in seq_lens:\n                for seq_offset in range(len_):\n                    f_pad[seq_base + seq_offset] = f[i]\n                    i += 1\n                if padding == 'last':\n                    for seq_offset in range(len_, max_seq_len):\n                        f_pad[seq_base + seq_offset] = f[i - 1]\n                seq_base += max_seq_len\n            assert i == len(f), f\n            feature_sequences[-1].append(f_pad)\n    if states_already_reduced_to_init:\n        initial_states = state_columns\n    else:\n        initial_states = []\n        for state_column in state_columns:\n            if isinstance(state_column, list):\n                state_column = np.array(state_column)\n            initial_state_flat = []\n            for s in tree.flatten(state_column):\n                if not isinstance(s, np.ndarray):\n                    s = np.array(s)\n                s_init = []\n                i = 0\n                for len_ in seq_lens:\n                    s_init.append(s[i])\n                    i += len_\n                initial_state_flat.append(np.array(s_init))\n            initial_states.append(tree.unflatten_as(state_column, initial_state_flat))\n    if shuffle:\n        permutation = np.random.permutation(len(seq_lens))\n        for (i, f) in enumerate(tree.flatten(feature_sequences)):\n            orig_shape = f.shape\n            f = np.reshape(f, (len(seq_lens), -1) + f.shape[1:])\n            f = f[permutation]\n            f = np.reshape(f, orig_shape)\n            feature_sequences[i] = f\n        for (i, s) in enumerate(initial_states):\n            s = s[permutation]\n            initial_states[i] = s\n        seq_lens = seq_lens[permutation]\n    if not handle_nested_data:\n        feature_sequences = [f[0] for f in feature_sequences]\n    return (feature_sequences, initial_states, seq_lens)"
        ]
    },
    {
        "func_name": "timeslice_along_seq_lens_with_overlap",
        "original": "@DeveloperAPI\ndef timeslice_along_seq_lens_with_overlap(sample_batch: SampleBatchType, seq_lens: Optional[List[int]]=None, zero_pad_max_seq_len: int=0, pre_overlap: int=0, zero_init_states: bool=True) -> List['SampleBatch']:\n    \"\"\"Slices batch along `seq_lens` (each seq-len item produces one batch).\n\n    Args:\n        sample_batch: The SampleBatch to timeslice.\n        seq_lens (Optional[List[int]]): An optional list of seq_lens to slice\n            at. If None, use `sample_batch[SampleBatch.SEQ_LENS]`.\n        zero_pad_max_seq_len: If >0, already zero-pad the resulting\n            slices up to this length. NOTE: This max-len will include the\n            additional timesteps gained via setting pre_overlap (see Example).\n        pre_overlap: If >0, will overlap each two consecutive slices by\n            this many timesteps (toward the left side). This will cause\n            zero-padding at the very beginning of the batch.\n        zero_init_states: Whether initial states should always be\n            zero'd. If False, will use the state_outs of the batch to\n            populate state_in values.\n\n    Returns:\n        List[SampleBatch]: The list of (new) SampleBatches.\n\n    Examples:\n        assert seq_lens == [5, 5, 2]\n        assert sample_batch.count == 12\n        # self = 0 1 2 3 4 | 5 6 7 8 9 | 10 11 <- timesteps\n        slices = timeslice_along_seq_lens_with_overlap(\n            sample_batch=sample_batch.\n            zero_pad_max_seq_len=10,\n            pre_overlap=3)\n        # Z = zero padding (at beginning or end).\n        #             |pre (3)|     seq     | max-seq-len (up to 10)\n        # slices[0] = | Z Z Z |  0  1 2 3 4 | Z Z\n        # slices[1] = | 2 3 4 |  5  6 7 8 9 | Z Z\n        # slices[2] = | 7 8 9 | 10 11 Z Z Z | Z Z\n        # Note that `zero_pad_max_seq_len=10` includes the 3 pre-overlaps\n        #  count (makes sure each slice has exactly length 10).\n    \"\"\"\n    if seq_lens is None:\n        seq_lens = sample_batch.get(SampleBatch.SEQ_LENS)\n    elif sample_batch.get(SampleBatch.SEQ_LENS) is not None and log_once('overriding_sequencing_information'):\n        logger.warning('Found sequencing information in a batch that will be ignored when slicing. Ignore this warning if you know what you are doing.')\n    if seq_lens is None:\n        max_seq_len = zero_pad_max_seq_len - pre_overlap\n        if log_once('no_sequence_lengths_available_for_time_slicing'):\n            logger.warning('Trying to slice a batch along sequences without sequence lengths being provided in the batch. Batch will be sliced into slices of size {} = {} - {} = zero_pad_max_seq_len - pre_overlap.'.format(max_seq_len, zero_pad_max_seq_len, pre_overlap))\n        (num_seq_lens, last_seq_len) = divmod(len(sample_batch), max_seq_len)\n        seq_lens = [zero_pad_max_seq_len] * num_seq_lens + ([last_seq_len] if last_seq_len else [])\n    assert seq_lens is not None and len(seq_lens) > 0, 'Cannot timeslice along `seq_lens` when `seq_lens` is empty or None!'\n    start = 0\n    slices = []\n    for seq_len in seq_lens:\n        pre_begin = start - pre_overlap\n        slice_begin = start\n        end = start + seq_len\n        slices.append((pre_begin, slice_begin, end))\n        start += seq_len\n    timeslices = []\n    for (begin, slice_begin, end) in slices:\n        zero_length = None\n        data_begin = 0\n        zero_init_states_ = zero_init_states\n        if begin < 0:\n            zero_length = pre_overlap\n            data_begin = slice_begin\n            zero_init_states_ = True\n        else:\n            eps_ids = sample_batch[SampleBatch.EPS_ID][begin if begin >= 0 else 0:end]\n            is_last_episode_ids = eps_ids == eps_ids[-1]\n            if not is_last_episode_ids[0]:\n                zero_length = int(sum(1.0 - is_last_episode_ids))\n                data_begin = begin + zero_length\n                zero_init_states_ = True\n        if zero_length is not None:\n            data = {k: np.concatenate([np.zeros(shape=(zero_length,) + v.shape[1:], dtype=v.dtype), v[data_begin:end]]) for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        else:\n            data = {k: v[begin:end] for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        if zero_init_states_:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = np.zeros_like(sample_batch[key][0:1])\n                data.pop('state_out_{}'.format(i), None)\n                i += 1\n                key = 'state_in_{}'.format(i)\n        else:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = sample_batch['state_out_{}'.format(i)][begin - 1:begin]\n                del data['state_out_{}'.format(i)]\n                i += 1\n                key = 'state_in_{}'.format(i)\n        timeslices.append(SampleBatch(data, seq_lens=[end - begin]))\n    if zero_pad_max_seq_len > 0:\n        for ts in timeslices:\n            ts.right_zero_pad(max_seq_len=zero_pad_max_seq_len, exclude_states=True)\n    return timeslices",
        "mutated": [
            "@DeveloperAPI\ndef timeslice_along_seq_lens_with_overlap(sample_batch: SampleBatchType, seq_lens: Optional[List[int]]=None, zero_pad_max_seq_len: int=0, pre_overlap: int=0, zero_init_states: bool=True) -> List['SampleBatch']:\n    if False:\n        i = 10\n    \"Slices batch along `seq_lens` (each seq-len item produces one batch).\\n\\n    Args:\\n        sample_batch: The SampleBatch to timeslice.\\n        seq_lens (Optional[List[int]]): An optional list of seq_lens to slice\\n            at. If None, use `sample_batch[SampleBatch.SEQ_LENS]`.\\n        zero_pad_max_seq_len: If >0, already zero-pad the resulting\\n            slices up to this length. NOTE: This max-len will include the\\n            additional timesteps gained via setting pre_overlap (see Example).\\n        pre_overlap: If >0, will overlap each two consecutive slices by\\n            this many timesteps (toward the left side). This will cause\\n            zero-padding at the very beginning of the batch.\\n        zero_init_states: Whether initial states should always be\\n            zero'd. If False, will use the state_outs of the batch to\\n            populate state_in values.\\n\\n    Returns:\\n        List[SampleBatch]: The list of (new) SampleBatches.\\n\\n    Examples:\\n        assert seq_lens == [5, 5, 2]\\n        assert sample_batch.count == 12\\n        # self = 0 1 2 3 4 | 5 6 7 8 9 | 10 11 <- timesteps\\n        slices = timeslice_along_seq_lens_with_overlap(\\n            sample_batch=sample_batch.\\n            zero_pad_max_seq_len=10,\\n            pre_overlap=3)\\n        # Z = zero padding (at beginning or end).\\n        #             |pre (3)|     seq     | max-seq-len (up to 10)\\n        # slices[0] = | Z Z Z |  0  1 2 3 4 | Z Z\\n        # slices[1] = | 2 3 4 |  5  6 7 8 9 | Z Z\\n        # slices[2] = | 7 8 9 | 10 11 Z Z Z | Z Z\\n        # Note that `zero_pad_max_seq_len=10` includes the 3 pre-overlaps\\n        #  count (makes sure each slice has exactly length 10).\\n    \"\n    if seq_lens is None:\n        seq_lens = sample_batch.get(SampleBatch.SEQ_LENS)\n    elif sample_batch.get(SampleBatch.SEQ_LENS) is not None and log_once('overriding_sequencing_information'):\n        logger.warning('Found sequencing information in a batch that will be ignored when slicing. Ignore this warning if you know what you are doing.')\n    if seq_lens is None:\n        max_seq_len = zero_pad_max_seq_len - pre_overlap\n        if log_once('no_sequence_lengths_available_for_time_slicing'):\n            logger.warning('Trying to slice a batch along sequences without sequence lengths being provided in the batch. Batch will be sliced into slices of size {} = {} - {} = zero_pad_max_seq_len - pre_overlap.'.format(max_seq_len, zero_pad_max_seq_len, pre_overlap))\n        (num_seq_lens, last_seq_len) = divmod(len(sample_batch), max_seq_len)\n        seq_lens = [zero_pad_max_seq_len] * num_seq_lens + ([last_seq_len] if last_seq_len else [])\n    assert seq_lens is not None and len(seq_lens) > 0, 'Cannot timeslice along `seq_lens` when `seq_lens` is empty or None!'\n    start = 0\n    slices = []\n    for seq_len in seq_lens:\n        pre_begin = start - pre_overlap\n        slice_begin = start\n        end = start + seq_len\n        slices.append((pre_begin, slice_begin, end))\n        start += seq_len\n    timeslices = []\n    for (begin, slice_begin, end) in slices:\n        zero_length = None\n        data_begin = 0\n        zero_init_states_ = zero_init_states\n        if begin < 0:\n            zero_length = pre_overlap\n            data_begin = slice_begin\n            zero_init_states_ = True\n        else:\n            eps_ids = sample_batch[SampleBatch.EPS_ID][begin if begin >= 0 else 0:end]\n            is_last_episode_ids = eps_ids == eps_ids[-1]\n            if not is_last_episode_ids[0]:\n                zero_length = int(sum(1.0 - is_last_episode_ids))\n                data_begin = begin + zero_length\n                zero_init_states_ = True\n        if zero_length is not None:\n            data = {k: np.concatenate([np.zeros(shape=(zero_length,) + v.shape[1:], dtype=v.dtype), v[data_begin:end]]) for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        else:\n            data = {k: v[begin:end] for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        if zero_init_states_:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = np.zeros_like(sample_batch[key][0:1])\n                data.pop('state_out_{}'.format(i), None)\n                i += 1\n                key = 'state_in_{}'.format(i)\n        else:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = sample_batch['state_out_{}'.format(i)][begin - 1:begin]\n                del data['state_out_{}'.format(i)]\n                i += 1\n                key = 'state_in_{}'.format(i)\n        timeslices.append(SampleBatch(data, seq_lens=[end - begin]))\n    if zero_pad_max_seq_len > 0:\n        for ts in timeslices:\n            ts.right_zero_pad(max_seq_len=zero_pad_max_seq_len, exclude_states=True)\n    return timeslices",
            "@DeveloperAPI\ndef timeslice_along_seq_lens_with_overlap(sample_batch: SampleBatchType, seq_lens: Optional[List[int]]=None, zero_pad_max_seq_len: int=0, pre_overlap: int=0, zero_init_states: bool=True) -> List['SampleBatch']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Slices batch along `seq_lens` (each seq-len item produces one batch).\\n\\n    Args:\\n        sample_batch: The SampleBatch to timeslice.\\n        seq_lens (Optional[List[int]]): An optional list of seq_lens to slice\\n            at. If None, use `sample_batch[SampleBatch.SEQ_LENS]`.\\n        zero_pad_max_seq_len: If >0, already zero-pad the resulting\\n            slices up to this length. NOTE: This max-len will include the\\n            additional timesteps gained via setting pre_overlap (see Example).\\n        pre_overlap: If >0, will overlap each two consecutive slices by\\n            this many timesteps (toward the left side). This will cause\\n            zero-padding at the very beginning of the batch.\\n        zero_init_states: Whether initial states should always be\\n            zero'd. If False, will use the state_outs of the batch to\\n            populate state_in values.\\n\\n    Returns:\\n        List[SampleBatch]: The list of (new) SampleBatches.\\n\\n    Examples:\\n        assert seq_lens == [5, 5, 2]\\n        assert sample_batch.count == 12\\n        # self = 0 1 2 3 4 | 5 6 7 8 9 | 10 11 <- timesteps\\n        slices = timeslice_along_seq_lens_with_overlap(\\n            sample_batch=sample_batch.\\n            zero_pad_max_seq_len=10,\\n            pre_overlap=3)\\n        # Z = zero padding (at beginning or end).\\n        #             |pre (3)|     seq     | max-seq-len (up to 10)\\n        # slices[0] = | Z Z Z |  0  1 2 3 4 | Z Z\\n        # slices[1] = | 2 3 4 |  5  6 7 8 9 | Z Z\\n        # slices[2] = | 7 8 9 | 10 11 Z Z Z | Z Z\\n        # Note that `zero_pad_max_seq_len=10` includes the 3 pre-overlaps\\n        #  count (makes sure each slice has exactly length 10).\\n    \"\n    if seq_lens is None:\n        seq_lens = sample_batch.get(SampleBatch.SEQ_LENS)\n    elif sample_batch.get(SampleBatch.SEQ_LENS) is not None and log_once('overriding_sequencing_information'):\n        logger.warning('Found sequencing information in a batch that will be ignored when slicing. Ignore this warning if you know what you are doing.')\n    if seq_lens is None:\n        max_seq_len = zero_pad_max_seq_len - pre_overlap\n        if log_once('no_sequence_lengths_available_for_time_slicing'):\n            logger.warning('Trying to slice a batch along sequences without sequence lengths being provided in the batch. Batch will be sliced into slices of size {} = {} - {} = zero_pad_max_seq_len - pre_overlap.'.format(max_seq_len, zero_pad_max_seq_len, pre_overlap))\n        (num_seq_lens, last_seq_len) = divmod(len(sample_batch), max_seq_len)\n        seq_lens = [zero_pad_max_seq_len] * num_seq_lens + ([last_seq_len] if last_seq_len else [])\n    assert seq_lens is not None and len(seq_lens) > 0, 'Cannot timeslice along `seq_lens` when `seq_lens` is empty or None!'\n    start = 0\n    slices = []\n    for seq_len in seq_lens:\n        pre_begin = start - pre_overlap\n        slice_begin = start\n        end = start + seq_len\n        slices.append((pre_begin, slice_begin, end))\n        start += seq_len\n    timeslices = []\n    for (begin, slice_begin, end) in slices:\n        zero_length = None\n        data_begin = 0\n        zero_init_states_ = zero_init_states\n        if begin < 0:\n            zero_length = pre_overlap\n            data_begin = slice_begin\n            zero_init_states_ = True\n        else:\n            eps_ids = sample_batch[SampleBatch.EPS_ID][begin if begin >= 0 else 0:end]\n            is_last_episode_ids = eps_ids == eps_ids[-1]\n            if not is_last_episode_ids[0]:\n                zero_length = int(sum(1.0 - is_last_episode_ids))\n                data_begin = begin + zero_length\n                zero_init_states_ = True\n        if zero_length is not None:\n            data = {k: np.concatenate([np.zeros(shape=(zero_length,) + v.shape[1:], dtype=v.dtype), v[data_begin:end]]) for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        else:\n            data = {k: v[begin:end] for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        if zero_init_states_:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = np.zeros_like(sample_batch[key][0:1])\n                data.pop('state_out_{}'.format(i), None)\n                i += 1\n                key = 'state_in_{}'.format(i)\n        else:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = sample_batch['state_out_{}'.format(i)][begin - 1:begin]\n                del data['state_out_{}'.format(i)]\n                i += 1\n                key = 'state_in_{}'.format(i)\n        timeslices.append(SampleBatch(data, seq_lens=[end - begin]))\n    if zero_pad_max_seq_len > 0:\n        for ts in timeslices:\n            ts.right_zero_pad(max_seq_len=zero_pad_max_seq_len, exclude_states=True)\n    return timeslices",
            "@DeveloperAPI\ndef timeslice_along_seq_lens_with_overlap(sample_batch: SampleBatchType, seq_lens: Optional[List[int]]=None, zero_pad_max_seq_len: int=0, pre_overlap: int=0, zero_init_states: bool=True) -> List['SampleBatch']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Slices batch along `seq_lens` (each seq-len item produces one batch).\\n\\n    Args:\\n        sample_batch: The SampleBatch to timeslice.\\n        seq_lens (Optional[List[int]]): An optional list of seq_lens to slice\\n            at. If None, use `sample_batch[SampleBatch.SEQ_LENS]`.\\n        zero_pad_max_seq_len: If >0, already zero-pad the resulting\\n            slices up to this length. NOTE: This max-len will include the\\n            additional timesteps gained via setting pre_overlap (see Example).\\n        pre_overlap: If >0, will overlap each two consecutive slices by\\n            this many timesteps (toward the left side). This will cause\\n            zero-padding at the very beginning of the batch.\\n        zero_init_states: Whether initial states should always be\\n            zero'd. If False, will use the state_outs of the batch to\\n            populate state_in values.\\n\\n    Returns:\\n        List[SampleBatch]: The list of (new) SampleBatches.\\n\\n    Examples:\\n        assert seq_lens == [5, 5, 2]\\n        assert sample_batch.count == 12\\n        # self = 0 1 2 3 4 | 5 6 7 8 9 | 10 11 <- timesteps\\n        slices = timeslice_along_seq_lens_with_overlap(\\n            sample_batch=sample_batch.\\n            zero_pad_max_seq_len=10,\\n            pre_overlap=3)\\n        # Z = zero padding (at beginning or end).\\n        #             |pre (3)|     seq     | max-seq-len (up to 10)\\n        # slices[0] = | Z Z Z |  0  1 2 3 4 | Z Z\\n        # slices[1] = | 2 3 4 |  5  6 7 8 9 | Z Z\\n        # slices[2] = | 7 8 9 | 10 11 Z Z Z | Z Z\\n        # Note that `zero_pad_max_seq_len=10` includes the 3 pre-overlaps\\n        #  count (makes sure each slice has exactly length 10).\\n    \"\n    if seq_lens is None:\n        seq_lens = sample_batch.get(SampleBatch.SEQ_LENS)\n    elif sample_batch.get(SampleBatch.SEQ_LENS) is not None and log_once('overriding_sequencing_information'):\n        logger.warning('Found sequencing information in a batch that will be ignored when slicing. Ignore this warning if you know what you are doing.')\n    if seq_lens is None:\n        max_seq_len = zero_pad_max_seq_len - pre_overlap\n        if log_once('no_sequence_lengths_available_for_time_slicing'):\n            logger.warning('Trying to slice a batch along sequences without sequence lengths being provided in the batch. Batch will be sliced into slices of size {} = {} - {} = zero_pad_max_seq_len - pre_overlap.'.format(max_seq_len, zero_pad_max_seq_len, pre_overlap))\n        (num_seq_lens, last_seq_len) = divmod(len(sample_batch), max_seq_len)\n        seq_lens = [zero_pad_max_seq_len] * num_seq_lens + ([last_seq_len] if last_seq_len else [])\n    assert seq_lens is not None and len(seq_lens) > 0, 'Cannot timeslice along `seq_lens` when `seq_lens` is empty or None!'\n    start = 0\n    slices = []\n    for seq_len in seq_lens:\n        pre_begin = start - pre_overlap\n        slice_begin = start\n        end = start + seq_len\n        slices.append((pre_begin, slice_begin, end))\n        start += seq_len\n    timeslices = []\n    for (begin, slice_begin, end) in slices:\n        zero_length = None\n        data_begin = 0\n        zero_init_states_ = zero_init_states\n        if begin < 0:\n            zero_length = pre_overlap\n            data_begin = slice_begin\n            zero_init_states_ = True\n        else:\n            eps_ids = sample_batch[SampleBatch.EPS_ID][begin if begin >= 0 else 0:end]\n            is_last_episode_ids = eps_ids == eps_ids[-1]\n            if not is_last_episode_ids[0]:\n                zero_length = int(sum(1.0 - is_last_episode_ids))\n                data_begin = begin + zero_length\n                zero_init_states_ = True\n        if zero_length is not None:\n            data = {k: np.concatenate([np.zeros(shape=(zero_length,) + v.shape[1:], dtype=v.dtype), v[data_begin:end]]) for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        else:\n            data = {k: v[begin:end] for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        if zero_init_states_:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = np.zeros_like(sample_batch[key][0:1])\n                data.pop('state_out_{}'.format(i), None)\n                i += 1\n                key = 'state_in_{}'.format(i)\n        else:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = sample_batch['state_out_{}'.format(i)][begin - 1:begin]\n                del data['state_out_{}'.format(i)]\n                i += 1\n                key = 'state_in_{}'.format(i)\n        timeslices.append(SampleBatch(data, seq_lens=[end - begin]))\n    if zero_pad_max_seq_len > 0:\n        for ts in timeslices:\n            ts.right_zero_pad(max_seq_len=zero_pad_max_seq_len, exclude_states=True)\n    return timeslices",
            "@DeveloperAPI\ndef timeslice_along_seq_lens_with_overlap(sample_batch: SampleBatchType, seq_lens: Optional[List[int]]=None, zero_pad_max_seq_len: int=0, pre_overlap: int=0, zero_init_states: bool=True) -> List['SampleBatch']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Slices batch along `seq_lens` (each seq-len item produces one batch).\\n\\n    Args:\\n        sample_batch: The SampleBatch to timeslice.\\n        seq_lens (Optional[List[int]]): An optional list of seq_lens to slice\\n            at. If None, use `sample_batch[SampleBatch.SEQ_LENS]`.\\n        zero_pad_max_seq_len: If >0, already zero-pad the resulting\\n            slices up to this length. NOTE: This max-len will include the\\n            additional timesteps gained via setting pre_overlap (see Example).\\n        pre_overlap: If >0, will overlap each two consecutive slices by\\n            this many timesteps (toward the left side). This will cause\\n            zero-padding at the very beginning of the batch.\\n        zero_init_states: Whether initial states should always be\\n            zero'd. If False, will use the state_outs of the batch to\\n            populate state_in values.\\n\\n    Returns:\\n        List[SampleBatch]: The list of (new) SampleBatches.\\n\\n    Examples:\\n        assert seq_lens == [5, 5, 2]\\n        assert sample_batch.count == 12\\n        # self = 0 1 2 3 4 | 5 6 7 8 9 | 10 11 <- timesteps\\n        slices = timeslice_along_seq_lens_with_overlap(\\n            sample_batch=sample_batch.\\n            zero_pad_max_seq_len=10,\\n            pre_overlap=3)\\n        # Z = zero padding (at beginning or end).\\n        #             |pre (3)|     seq     | max-seq-len (up to 10)\\n        # slices[0] = | Z Z Z |  0  1 2 3 4 | Z Z\\n        # slices[1] = | 2 3 4 |  5  6 7 8 9 | Z Z\\n        # slices[2] = | 7 8 9 | 10 11 Z Z Z | Z Z\\n        # Note that `zero_pad_max_seq_len=10` includes the 3 pre-overlaps\\n        #  count (makes sure each slice has exactly length 10).\\n    \"\n    if seq_lens is None:\n        seq_lens = sample_batch.get(SampleBatch.SEQ_LENS)\n    elif sample_batch.get(SampleBatch.SEQ_LENS) is not None and log_once('overriding_sequencing_information'):\n        logger.warning('Found sequencing information in a batch that will be ignored when slicing. Ignore this warning if you know what you are doing.')\n    if seq_lens is None:\n        max_seq_len = zero_pad_max_seq_len - pre_overlap\n        if log_once('no_sequence_lengths_available_for_time_slicing'):\n            logger.warning('Trying to slice a batch along sequences without sequence lengths being provided in the batch. Batch will be sliced into slices of size {} = {} - {} = zero_pad_max_seq_len - pre_overlap.'.format(max_seq_len, zero_pad_max_seq_len, pre_overlap))\n        (num_seq_lens, last_seq_len) = divmod(len(sample_batch), max_seq_len)\n        seq_lens = [zero_pad_max_seq_len] * num_seq_lens + ([last_seq_len] if last_seq_len else [])\n    assert seq_lens is not None and len(seq_lens) > 0, 'Cannot timeslice along `seq_lens` when `seq_lens` is empty or None!'\n    start = 0\n    slices = []\n    for seq_len in seq_lens:\n        pre_begin = start - pre_overlap\n        slice_begin = start\n        end = start + seq_len\n        slices.append((pre_begin, slice_begin, end))\n        start += seq_len\n    timeslices = []\n    for (begin, slice_begin, end) in slices:\n        zero_length = None\n        data_begin = 0\n        zero_init_states_ = zero_init_states\n        if begin < 0:\n            zero_length = pre_overlap\n            data_begin = slice_begin\n            zero_init_states_ = True\n        else:\n            eps_ids = sample_batch[SampleBatch.EPS_ID][begin if begin >= 0 else 0:end]\n            is_last_episode_ids = eps_ids == eps_ids[-1]\n            if not is_last_episode_ids[0]:\n                zero_length = int(sum(1.0 - is_last_episode_ids))\n                data_begin = begin + zero_length\n                zero_init_states_ = True\n        if zero_length is not None:\n            data = {k: np.concatenate([np.zeros(shape=(zero_length,) + v.shape[1:], dtype=v.dtype), v[data_begin:end]]) for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        else:\n            data = {k: v[begin:end] for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        if zero_init_states_:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = np.zeros_like(sample_batch[key][0:1])\n                data.pop('state_out_{}'.format(i), None)\n                i += 1\n                key = 'state_in_{}'.format(i)\n        else:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = sample_batch['state_out_{}'.format(i)][begin - 1:begin]\n                del data['state_out_{}'.format(i)]\n                i += 1\n                key = 'state_in_{}'.format(i)\n        timeslices.append(SampleBatch(data, seq_lens=[end - begin]))\n    if zero_pad_max_seq_len > 0:\n        for ts in timeslices:\n            ts.right_zero_pad(max_seq_len=zero_pad_max_seq_len, exclude_states=True)\n    return timeslices",
            "@DeveloperAPI\ndef timeslice_along_seq_lens_with_overlap(sample_batch: SampleBatchType, seq_lens: Optional[List[int]]=None, zero_pad_max_seq_len: int=0, pre_overlap: int=0, zero_init_states: bool=True) -> List['SampleBatch']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Slices batch along `seq_lens` (each seq-len item produces one batch).\\n\\n    Args:\\n        sample_batch: The SampleBatch to timeslice.\\n        seq_lens (Optional[List[int]]): An optional list of seq_lens to slice\\n            at. If None, use `sample_batch[SampleBatch.SEQ_LENS]`.\\n        zero_pad_max_seq_len: If >0, already zero-pad the resulting\\n            slices up to this length. NOTE: This max-len will include the\\n            additional timesteps gained via setting pre_overlap (see Example).\\n        pre_overlap: If >0, will overlap each two consecutive slices by\\n            this many timesteps (toward the left side). This will cause\\n            zero-padding at the very beginning of the batch.\\n        zero_init_states: Whether initial states should always be\\n            zero'd. If False, will use the state_outs of the batch to\\n            populate state_in values.\\n\\n    Returns:\\n        List[SampleBatch]: The list of (new) SampleBatches.\\n\\n    Examples:\\n        assert seq_lens == [5, 5, 2]\\n        assert sample_batch.count == 12\\n        # self = 0 1 2 3 4 | 5 6 7 8 9 | 10 11 <- timesteps\\n        slices = timeslice_along_seq_lens_with_overlap(\\n            sample_batch=sample_batch.\\n            zero_pad_max_seq_len=10,\\n            pre_overlap=3)\\n        # Z = zero padding (at beginning or end).\\n        #             |pre (3)|     seq     | max-seq-len (up to 10)\\n        # slices[0] = | Z Z Z |  0  1 2 3 4 | Z Z\\n        # slices[1] = | 2 3 4 |  5  6 7 8 9 | Z Z\\n        # slices[2] = | 7 8 9 | 10 11 Z Z Z | Z Z\\n        # Note that `zero_pad_max_seq_len=10` includes the 3 pre-overlaps\\n        #  count (makes sure each slice has exactly length 10).\\n    \"\n    if seq_lens is None:\n        seq_lens = sample_batch.get(SampleBatch.SEQ_LENS)\n    elif sample_batch.get(SampleBatch.SEQ_LENS) is not None and log_once('overriding_sequencing_information'):\n        logger.warning('Found sequencing information in a batch that will be ignored when slicing. Ignore this warning if you know what you are doing.')\n    if seq_lens is None:\n        max_seq_len = zero_pad_max_seq_len - pre_overlap\n        if log_once('no_sequence_lengths_available_for_time_slicing'):\n            logger.warning('Trying to slice a batch along sequences without sequence lengths being provided in the batch. Batch will be sliced into slices of size {} = {} - {} = zero_pad_max_seq_len - pre_overlap.'.format(max_seq_len, zero_pad_max_seq_len, pre_overlap))\n        (num_seq_lens, last_seq_len) = divmod(len(sample_batch), max_seq_len)\n        seq_lens = [zero_pad_max_seq_len] * num_seq_lens + ([last_seq_len] if last_seq_len else [])\n    assert seq_lens is not None and len(seq_lens) > 0, 'Cannot timeslice along `seq_lens` when `seq_lens` is empty or None!'\n    start = 0\n    slices = []\n    for seq_len in seq_lens:\n        pre_begin = start - pre_overlap\n        slice_begin = start\n        end = start + seq_len\n        slices.append((pre_begin, slice_begin, end))\n        start += seq_len\n    timeslices = []\n    for (begin, slice_begin, end) in slices:\n        zero_length = None\n        data_begin = 0\n        zero_init_states_ = zero_init_states\n        if begin < 0:\n            zero_length = pre_overlap\n            data_begin = slice_begin\n            zero_init_states_ = True\n        else:\n            eps_ids = sample_batch[SampleBatch.EPS_ID][begin if begin >= 0 else 0:end]\n            is_last_episode_ids = eps_ids == eps_ids[-1]\n            if not is_last_episode_ids[0]:\n                zero_length = int(sum(1.0 - is_last_episode_ids))\n                data_begin = begin + zero_length\n                zero_init_states_ = True\n        if zero_length is not None:\n            data = {k: np.concatenate([np.zeros(shape=(zero_length,) + v.shape[1:], dtype=v.dtype), v[data_begin:end]]) for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        else:\n            data = {k: v[begin:end] for (k, v) in sample_batch.items() if k != SampleBatch.SEQ_LENS}\n        if zero_init_states_:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = np.zeros_like(sample_batch[key][0:1])\n                data.pop('state_out_{}'.format(i), None)\n                i += 1\n                key = 'state_in_{}'.format(i)\n        else:\n            i = 0\n            key = 'state_in_{}'.format(i)\n            while key in data:\n                data[key] = sample_batch['state_out_{}'.format(i)][begin - 1:begin]\n                del data['state_out_{}'.format(i)]\n                i += 1\n                key = 'state_in_{}'.format(i)\n        timeslices.append(SampleBatch(data, seq_lens=[end - begin]))\n    if zero_pad_max_seq_len > 0:\n        for ts in timeslices:\n            ts.right_zero_pad(max_seq_len=zero_pad_max_seq_len, exclude_states=True)\n    return timeslices"
        ]
    },
    {
        "func_name": "fold_mapping",
        "original": "def fold_mapping(item):\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = tf.shape(item)\n    other_dims = shape[2:]\n    return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))",
        "mutated": [
            "def fold_mapping(item):\n    if False:\n        i = 10\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = tf.shape(item)\n    other_dims = shape[2:]\n    return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))",
            "def fold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = tf.shape(item)\n    other_dims = shape[2:]\n    return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))",
            "def fold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = tf.shape(item)\n    other_dims = shape[2:]\n    return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))",
            "def fold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = tf.shape(item)\n    other_dims = shape[2:]\n    return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))",
            "def fold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = tf.shape(item)\n    other_dims = shape[2:]\n    return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))"
        ]
    },
    {
        "func_name": "unfold_mapping",
        "original": "def unfold_mapping(item):\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = item.shape\n    other_dims = shape[1:]\n    return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))",
        "mutated": [
            "def unfold_mapping(item):\n    if False:\n        i = 10\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = item.shape\n    other_dims = shape[1:]\n    return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))",
            "def unfold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = item.shape\n    other_dims = shape[1:]\n    return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))",
            "def unfold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = item.shape\n    other_dims = shape[1:]\n    return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))",
            "def unfold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = item.shape\n    other_dims = shape[1:]\n    return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))",
            "def unfold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if item is None:\n        return item\n    item = tf.convert_to_tensor(item)\n    shape = item.shape\n    other_dims = shape[1:]\n    return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))"
        ]
    },
    {
        "func_name": "fold_mapping",
        "original": "def fold_mapping(item):\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    (current_b_dim, current_t_dim) = list(size[:2])\n    assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n    other_dims = size[2:]\n    return item.reshape([b_dim * t_dim] + other_dims)",
        "mutated": [
            "def fold_mapping(item):\n    if False:\n        i = 10\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    (current_b_dim, current_t_dim) = list(size[:2])\n    assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n    other_dims = size[2:]\n    return item.reshape([b_dim * t_dim] + other_dims)",
            "def fold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    (current_b_dim, current_t_dim) = list(size[:2])\n    assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n    other_dims = size[2:]\n    return item.reshape([b_dim * t_dim] + other_dims)",
            "def fold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    (current_b_dim, current_t_dim) = list(size[:2])\n    assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n    other_dims = size[2:]\n    return item.reshape([b_dim * t_dim] + other_dims)",
            "def fold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    (current_b_dim, current_t_dim) = list(size[:2])\n    assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n    other_dims = size[2:]\n    return item.reshape([b_dim * t_dim] + other_dims)",
            "def fold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    (current_b_dim, current_t_dim) = list(size[:2])\n    assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n    other_dims = size[2:]\n    return item.reshape([b_dim * t_dim] + other_dims)"
        ]
    },
    {
        "func_name": "unfold_mapping",
        "original": "def unfold_mapping(item):\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    current_b_dim = size[0]\n    other_dims = size[1:]\n    assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n    return item.reshape([b_dim, t_dim] + other_dims)",
        "mutated": [
            "def unfold_mapping(item):\n    if False:\n        i = 10\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    current_b_dim = size[0]\n    other_dims = size[1:]\n    assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n    return item.reshape([b_dim, t_dim] + other_dims)",
            "def unfold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    current_b_dim = size[0]\n    other_dims = size[1:]\n    assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n    return item.reshape([b_dim, t_dim] + other_dims)",
            "def unfold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    current_b_dim = size[0]\n    other_dims = size[1:]\n    assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n    return item.reshape([b_dim, t_dim] + other_dims)",
            "def unfold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    current_b_dim = size[0]\n    other_dims = size[1:]\n    assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n    return item.reshape([b_dim, t_dim] + other_dims)",
            "def unfold_mapping(item):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if item is None:\n        return item\n    item = torch.as_tensor(item)\n    size = list(item.size())\n    current_b_dim = size[0]\n    other_dims = size[1:]\n    assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n    return item.reshape([b_dim, t_dim] + other_dims)"
        ]
    },
    {
        "func_name": "get_fold_unfold_fns",
        "original": "@DeveloperAPI\ndef get_fold_unfold_fns(b_dim: int, t_dim: int, framework: str):\n    \"\"\"Produces two functions to fold/unfold any Tensors in a struct.\n\n    Args:\n        b_dim: The batch dimension to use for folding.\n        t_dim: The time dimension to use for folding.\n        framework: The framework to use for folding. One of \"tf2\" or \"torch\".\n\n    Returns:\n        fold: A function that takes a struct of torch.Tensors and reshapes\n            them to have a first dimension of `b_dim * t_dim`.\n        unfold: A function that takes a struct of torch.Tensors and reshapes\n            them to have a first dimension of `b_dim` and a second dimension\n            of `t_dim`.\n    \"\"\"\n    if framework in 'tf2':\n        b_dim = tf.convert_to_tensor(b_dim)\n        t_dim = tf.convert_to_tensor(t_dim)\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = tf.shape(item)\n            other_dims = shape[2:]\n            return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = item.shape\n            other_dims = shape[1:]\n            return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))\n    elif framework == 'torch':\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            (current_b_dim, current_t_dim) = list(size[:2])\n            assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n            other_dims = size[2:]\n            return item.reshape([b_dim * t_dim] + other_dims)\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            current_b_dim = size[0]\n            other_dims = size[1:]\n            assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n            return item.reshape([b_dim, t_dim] + other_dims)\n    else:\n        raise ValueError(f'framework {framework} not implemented!')\n    return (functools.partial(tree.map_structure, fold_mapping), functools.partial(tree.map_structure, unfold_mapping))",
        "mutated": [
            "@DeveloperAPI\ndef get_fold_unfold_fns(b_dim: int, t_dim: int, framework: str):\n    if False:\n        i = 10\n    'Produces two functions to fold/unfold any Tensors in a struct.\\n\\n    Args:\\n        b_dim: The batch dimension to use for folding.\\n        t_dim: The time dimension to use for folding.\\n        framework: The framework to use for folding. One of \"tf2\" or \"torch\".\\n\\n    Returns:\\n        fold: A function that takes a struct of torch.Tensors and reshapes\\n            them to have a first dimension of `b_dim * t_dim`.\\n        unfold: A function that takes a struct of torch.Tensors and reshapes\\n            them to have a first dimension of `b_dim` and a second dimension\\n            of `t_dim`.\\n    '\n    if framework in 'tf2':\n        b_dim = tf.convert_to_tensor(b_dim)\n        t_dim = tf.convert_to_tensor(t_dim)\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = tf.shape(item)\n            other_dims = shape[2:]\n            return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = item.shape\n            other_dims = shape[1:]\n            return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))\n    elif framework == 'torch':\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            (current_b_dim, current_t_dim) = list(size[:2])\n            assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n            other_dims = size[2:]\n            return item.reshape([b_dim * t_dim] + other_dims)\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            current_b_dim = size[0]\n            other_dims = size[1:]\n            assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n            return item.reshape([b_dim, t_dim] + other_dims)\n    else:\n        raise ValueError(f'framework {framework} not implemented!')\n    return (functools.partial(tree.map_structure, fold_mapping), functools.partial(tree.map_structure, unfold_mapping))",
            "@DeveloperAPI\ndef get_fold_unfold_fns(b_dim: int, t_dim: int, framework: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Produces two functions to fold/unfold any Tensors in a struct.\\n\\n    Args:\\n        b_dim: The batch dimension to use for folding.\\n        t_dim: The time dimension to use for folding.\\n        framework: The framework to use for folding. One of \"tf2\" or \"torch\".\\n\\n    Returns:\\n        fold: A function that takes a struct of torch.Tensors and reshapes\\n            them to have a first dimension of `b_dim * t_dim`.\\n        unfold: A function that takes a struct of torch.Tensors and reshapes\\n            them to have a first dimension of `b_dim` and a second dimension\\n            of `t_dim`.\\n    '\n    if framework in 'tf2':\n        b_dim = tf.convert_to_tensor(b_dim)\n        t_dim = tf.convert_to_tensor(t_dim)\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = tf.shape(item)\n            other_dims = shape[2:]\n            return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = item.shape\n            other_dims = shape[1:]\n            return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))\n    elif framework == 'torch':\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            (current_b_dim, current_t_dim) = list(size[:2])\n            assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n            other_dims = size[2:]\n            return item.reshape([b_dim * t_dim] + other_dims)\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            current_b_dim = size[0]\n            other_dims = size[1:]\n            assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n            return item.reshape([b_dim, t_dim] + other_dims)\n    else:\n        raise ValueError(f'framework {framework} not implemented!')\n    return (functools.partial(tree.map_structure, fold_mapping), functools.partial(tree.map_structure, unfold_mapping))",
            "@DeveloperAPI\ndef get_fold_unfold_fns(b_dim: int, t_dim: int, framework: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Produces two functions to fold/unfold any Tensors in a struct.\\n\\n    Args:\\n        b_dim: The batch dimension to use for folding.\\n        t_dim: The time dimension to use for folding.\\n        framework: The framework to use for folding. One of \"tf2\" or \"torch\".\\n\\n    Returns:\\n        fold: A function that takes a struct of torch.Tensors and reshapes\\n            them to have a first dimension of `b_dim * t_dim`.\\n        unfold: A function that takes a struct of torch.Tensors and reshapes\\n            them to have a first dimension of `b_dim` and a second dimension\\n            of `t_dim`.\\n    '\n    if framework in 'tf2':\n        b_dim = tf.convert_to_tensor(b_dim)\n        t_dim = tf.convert_to_tensor(t_dim)\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = tf.shape(item)\n            other_dims = shape[2:]\n            return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = item.shape\n            other_dims = shape[1:]\n            return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))\n    elif framework == 'torch':\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            (current_b_dim, current_t_dim) = list(size[:2])\n            assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n            other_dims = size[2:]\n            return item.reshape([b_dim * t_dim] + other_dims)\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            current_b_dim = size[0]\n            other_dims = size[1:]\n            assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n            return item.reshape([b_dim, t_dim] + other_dims)\n    else:\n        raise ValueError(f'framework {framework} not implemented!')\n    return (functools.partial(tree.map_structure, fold_mapping), functools.partial(tree.map_structure, unfold_mapping))",
            "@DeveloperAPI\ndef get_fold_unfold_fns(b_dim: int, t_dim: int, framework: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Produces two functions to fold/unfold any Tensors in a struct.\\n\\n    Args:\\n        b_dim: The batch dimension to use for folding.\\n        t_dim: The time dimension to use for folding.\\n        framework: The framework to use for folding. One of \"tf2\" or \"torch\".\\n\\n    Returns:\\n        fold: A function that takes a struct of torch.Tensors and reshapes\\n            them to have a first dimension of `b_dim * t_dim`.\\n        unfold: A function that takes a struct of torch.Tensors and reshapes\\n            them to have a first dimension of `b_dim` and a second dimension\\n            of `t_dim`.\\n    '\n    if framework in 'tf2':\n        b_dim = tf.convert_to_tensor(b_dim)\n        t_dim = tf.convert_to_tensor(t_dim)\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = tf.shape(item)\n            other_dims = shape[2:]\n            return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = item.shape\n            other_dims = shape[1:]\n            return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))\n    elif framework == 'torch':\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            (current_b_dim, current_t_dim) = list(size[:2])\n            assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n            other_dims = size[2:]\n            return item.reshape([b_dim * t_dim] + other_dims)\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            current_b_dim = size[0]\n            other_dims = size[1:]\n            assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n            return item.reshape([b_dim, t_dim] + other_dims)\n    else:\n        raise ValueError(f'framework {framework} not implemented!')\n    return (functools.partial(tree.map_structure, fold_mapping), functools.partial(tree.map_structure, unfold_mapping))",
            "@DeveloperAPI\ndef get_fold_unfold_fns(b_dim: int, t_dim: int, framework: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Produces two functions to fold/unfold any Tensors in a struct.\\n\\n    Args:\\n        b_dim: The batch dimension to use for folding.\\n        t_dim: The time dimension to use for folding.\\n        framework: The framework to use for folding. One of \"tf2\" or \"torch\".\\n\\n    Returns:\\n        fold: A function that takes a struct of torch.Tensors and reshapes\\n            them to have a first dimension of `b_dim * t_dim`.\\n        unfold: A function that takes a struct of torch.Tensors and reshapes\\n            them to have a first dimension of `b_dim` and a second dimension\\n            of `t_dim`.\\n    '\n    if framework in 'tf2':\n        b_dim = tf.convert_to_tensor(b_dim)\n        t_dim = tf.convert_to_tensor(t_dim)\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = tf.shape(item)\n            other_dims = shape[2:]\n            return tf.reshape(item, tf.concat([[b_dim * t_dim], other_dims], axis=0))\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = tf.convert_to_tensor(item)\n            shape = item.shape\n            other_dims = shape[1:]\n            return tf.reshape(item, tf.concat([[b_dim], [t_dim], other_dims], axis=0))\n    elif framework == 'torch':\n\n        def fold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            (current_b_dim, current_t_dim) = list(size[:2])\n            assert (b_dim, t_dim) == (current_b_dim, current_t_dim), 'All tensors in the struct must have the same batch and time dimensions. Got {} and {}.'.format((b_dim, t_dim), (current_b_dim, current_t_dim))\n            other_dims = size[2:]\n            return item.reshape([b_dim * t_dim] + other_dims)\n\n        def unfold_mapping(item):\n            if item is None:\n                return item\n            item = torch.as_tensor(item)\n            size = list(item.size())\n            current_b_dim = size[0]\n            other_dims = size[1:]\n            assert current_b_dim == b_dim * t_dim, 'The first dimension of the tensor must be equal to the product of the desired batch and time dimensions. Got {} and {}.'.format(current_b_dim, b_dim * t_dim)\n            return item.reshape([b_dim, t_dim] + other_dims)\n    else:\n        raise ValueError(f'framework {framework} not implemented!')\n    return (functools.partial(tree.map_structure, fold_mapping), functools.partial(tree.map_structure, unfold_mapping))"
        ]
    }
]