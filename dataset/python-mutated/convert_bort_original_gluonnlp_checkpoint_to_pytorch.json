[
    {
        "func_name": "to_torch",
        "original": "def to_torch(mx_array) -> nn.Parameter:\n    return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))",
        "mutated": [
            "def to_torch(mx_array) -> nn.Parameter:\n    if False:\n        i = 10\n    return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))",
            "def to_torch(mx_array) -> nn.Parameter:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))",
            "def to_torch(mx_array) -> nn.Parameter:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))",
            "def to_torch(mx_array) -> nn.Parameter:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))",
            "def to_torch(mx_array) -> nn.Parameter:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))"
        ]
    },
    {
        "func_name": "check_and_map_params",
        "original": "def check_and_map_params(hf_param, gluon_param):\n    shape_hf = hf_param.shape\n    gluon_param = to_torch(params[gluon_param])\n    shape_gluon = gluon_param.shape\n    assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n    return gluon_param",
        "mutated": [
            "def check_and_map_params(hf_param, gluon_param):\n    if False:\n        i = 10\n    shape_hf = hf_param.shape\n    gluon_param = to_torch(params[gluon_param])\n    shape_gluon = gluon_param.shape\n    assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n    return gluon_param",
            "def check_and_map_params(hf_param, gluon_param):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    shape_hf = hf_param.shape\n    gluon_param = to_torch(params[gluon_param])\n    shape_gluon = gluon_param.shape\n    assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n    return gluon_param",
            "def check_and_map_params(hf_param, gluon_param):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    shape_hf = hf_param.shape\n    gluon_param = to_torch(params[gluon_param])\n    shape_gluon = gluon_param.shape\n    assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n    return gluon_param",
            "def check_and_map_params(hf_param, gluon_param):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    shape_hf = hf_param.shape\n    gluon_param = to_torch(params[gluon_param])\n    shape_gluon = gluon_param.shape\n    assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n    return gluon_param",
            "def check_and_map_params(hf_param, gluon_param):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    shape_hf = hf_param.shape\n    gluon_param = to_torch(params[gluon_param])\n    shape_gluon = gluon_param.shape\n    assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n    return gluon_param"
        ]
    },
    {
        "func_name": "convert_bort_checkpoint_to_pytorch",
        "original": "def convert_bort_checkpoint_to_pytorch(bort_checkpoint_path: str, pytorch_dump_folder_path: str):\n    \"\"\"\n    Convert the original Bort checkpoint (based on MXNET and Gluonnlp) to our BERT structure-\n    \"\"\"\n    bort_4_8_768_1024_hparams = {'attention_cell': 'multi_head', 'num_layers': 4, 'units': 1024, 'hidden_size': 768, 'max_length': 512, 'num_heads': 8, 'scaled': True, 'dropout': 0.1, 'use_residual': True, 'embed_size': 1024, 'embed_dropout': 0.1, 'word_embed': None, 'layer_norm_eps': 1e-05, 'token_type_vocab_size': 2}\n    predefined_args = bort_4_8_768_1024_hparams\n    encoder = BERTEncoder(attention_cell=predefined_args['attention_cell'], num_layers=predefined_args['num_layers'], units=predefined_args['units'], hidden_size=predefined_args['hidden_size'], max_length=predefined_args['max_length'], num_heads=predefined_args['num_heads'], scaled=predefined_args['scaled'], dropout=predefined_args['dropout'], output_attention=False, output_all_encodings=False, use_residual=predefined_args['use_residual'], activation=predefined_args.get('activation', 'gelu'), layer_norm_eps=predefined_args.get('layer_norm_eps', None))\n    vocab_name = 'openwebtext_ccnews_stories_books_cased'\n    gluon_cache_dir = os.path.join(get_home_dir(), 'models')\n    bort_vocab = _load_vocab(vocab_name, None, gluon_cache_dir, cls=Vocab)\n    original_bort = nlp.model.BERTModel(encoder, len(bort_vocab), units=predefined_args['units'], embed_size=predefined_args['embed_size'], embed_dropout=predefined_args['embed_dropout'], word_embed=predefined_args['word_embed'], use_pooler=False, use_token_type_embed=False, token_type_vocab_size=predefined_args['token_type_vocab_size'], use_classifier=False, use_decoder=False)\n    original_bort.load_parameters(bort_checkpoint_path, cast_dtype=True, ignore_extra=True)\n    params = original_bort._collect_params_with_prefix()\n    hf_bort_config_json = {'architectures': ['BertForMaskedLM'], 'attention_probs_dropout_prob': predefined_args['dropout'], 'hidden_act': 'gelu', 'hidden_dropout_prob': predefined_args['dropout'], 'hidden_size': predefined_args['embed_size'], 'initializer_range': 0.02, 'intermediate_size': predefined_args['hidden_size'], 'layer_norm_eps': predefined_args['layer_norm_eps'], 'max_position_embeddings': predefined_args['max_length'], 'model_type': 'bort', 'num_attention_heads': predefined_args['num_heads'], 'num_hidden_layers': predefined_args['num_layers'], 'pad_token_id': 1, 'type_vocab_size': 1, 'vocab_size': len(bort_vocab)}\n    hf_bort_config = BertConfig.from_dict(hf_bort_config_json)\n    hf_bort_model = BertForMaskedLM(hf_bort_config)\n    hf_bort_model.eval()\n\n    def to_torch(mx_array) -> nn.Parameter:\n        return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))\n\n    def check_and_map_params(hf_param, gluon_param):\n        shape_hf = hf_param.shape\n        gluon_param = to_torch(params[gluon_param])\n        shape_gluon = gluon_param.shape\n        assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n        return gluon_param\n    hf_bort_model.bert.embeddings.word_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.word_embeddings.weight, 'word_embed.0.weight')\n    hf_bort_model.bert.embeddings.position_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.position_embeddings.weight, 'encoder.position_weight')\n    hf_bort_model.bert.embeddings.LayerNorm.bias = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.bias, 'encoder.layer_norm.beta')\n    hf_bort_model.bert.embeddings.LayerNorm.weight = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.weight, 'encoder.layer_norm.gamma')\n    hf_bort_model.bert.embeddings.token_type_embeddings.weight.data = torch.zeros_like(hf_bort_model.bert.embeddings.token_type_embeddings.weight.data)\n    for i in range(hf_bort_config.num_hidden_layers):\n        layer: BertLayer = hf_bort_model.bert.encoder.layer[i]\n        self_attn: BertSelfAttention = layer.attention.self\n        self_attn.key.bias.data = check_and_map_params(self_attn.key.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.bias')\n        self_attn.key.weight.data = check_and_map_params(self_attn.key.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.weight')\n        self_attn.query.bias.data = check_and_map_params(self_attn.query.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.bias')\n        self_attn.query.weight.data = check_and_map_params(self_attn.query.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.weight')\n        self_attn.value.bias.data = check_and_map_params(self_attn.value.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.bias')\n        self_attn.value.weight.data = check_and_map_params(self_attn.value.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.weight')\n        self_output: BertSelfOutput = layer.attention.output\n        self_output.dense.bias = check_and_map_params(self_output.dense.bias, f'encoder.transformer_cells.{i}.proj.bias')\n        self_output.dense.weight = check_and_map_params(self_output.dense.weight, f'encoder.transformer_cells.{i}.proj.weight')\n        self_output.LayerNorm.bias = check_and_map_params(self_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.layer_norm.beta')\n        self_output.LayerNorm.weight = check_and_map_params(self_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.layer_norm.gamma')\n        intermediate: BertIntermediate = layer.intermediate\n        intermediate.dense.bias = check_and_map_params(intermediate.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_1.bias')\n        intermediate.dense.weight = check_and_map_params(intermediate.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_1.weight')\n        bert_output: BertOutput = layer.output\n        bert_output.dense.bias = check_and_map_params(bert_output.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_2.bias')\n        bert_output.dense.weight = check_and_map_params(bert_output.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_2.weight')\n        bert_output.LayerNorm.bias = check_and_map_params(bert_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.ffn.layer_norm.beta')\n        bert_output.LayerNorm.weight = check_and_map_params(bert_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.ffn.layer_norm.gamma')\n    hf_bort_model.half()\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT)['input_ids']\n    gluon_input_ids = mx.nd.array([input_ids])\n    output_gluon = original_bort(inputs=gluon_input_ids, token_types=[])\n    hf_bort_model.save_pretrained(pytorch_dump_folder_path)\n    hf_bort_model = BertModel.from_pretrained(pytorch_dump_folder_path)\n    hf_bort_model.eval()\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT, return_tensors='pt')\n    output_hf = hf_bort_model(**input_ids)[0]\n    gluon_layer = output_gluon[0].asnumpy()\n    hf_layer = output_hf[0].detach().numpy()\n    max_absolute_diff = np.max(np.abs(hf_layer - gluon_layer)).item()\n    success = np.allclose(gluon_layer, hf_layer, atol=0.001)\n    if success:\n        print('\u2714\ufe0f Both model do output the same tensors')\n    else:\n        print('\u274c Both model do **NOT** output the same tensors')\n        print('Absolute difference is:', max_absolute_diff)",
        "mutated": [
            "def convert_bort_checkpoint_to_pytorch(bort_checkpoint_path: str, pytorch_dump_folder_path: str):\n    if False:\n        i = 10\n    '\\n    Convert the original Bort checkpoint (based on MXNET and Gluonnlp) to our BERT structure-\\n    '\n    bort_4_8_768_1024_hparams = {'attention_cell': 'multi_head', 'num_layers': 4, 'units': 1024, 'hidden_size': 768, 'max_length': 512, 'num_heads': 8, 'scaled': True, 'dropout': 0.1, 'use_residual': True, 'embed_size': 1024, 'embed_dropout': 0.1, 'word_embed': None, 'layer_norm_eps': 1e-05, 'token_type_vocab_size': 2}\n    predefined_args = bort_4_8_768_1024_hparams\n    encoder = BERTEncoder(attention_cell=predefined_args['attention_cell'], num_layers=predefined_args['num_layers'], units=predefined_args['units'], hidden_size=predefined_args['hidden_size'], max_length=predefined_args['max_length'], num_heads=predefined_args['num_heads'], scaled=predefined_args['scaled'], dropout=predefined_args['dropout'], output_attention=False, output_all_encodings=False, use_residual=predefined_args['use_residual'], activation=predefined_args.get('activation', 'gelu'), layer_norm_eps=predefined_args.get('layer_norm_eps', None))\n    vocab_name = 'openwebtext_ccnews_stories_books_cased'\n    gluon_cache_dir = os.path.join(get_home_dir(), 'models')\n    bort_vocab = _load_vocab(vocab_name, None, gluon_cache_dir, cls=Vocab)\n    original_bort = nlp.model.BERTModel(encoder, len(bort_vocab), units=predefined_args['units'], embed_size=predefined_args['embed_size'], embed_dropout=predefined_args['embed_dropout'], word_embed=predefined_args['word_embed'], use_pooler=False, use_token_type_embed=False, token_type_vocab_size=predefined_args['token_type_vocab_size'], use_classifier=False, use_decoder=False)\n    original_bort.load_parameters(bort_checkpoint_path, cast_dtype=True, ignore_extra=True)\n    params = original_bort._collect_params_with_prefix()\n    hf_bort_config_json = {'architectures': ['BertForMaskedLM'], 'attention_probs_dropout_prob': predefined_args['dropout'], 'hidden_act': 'gelu', 'hidden_dropout_prob': predefined_args['dropout'], 'hidden_size': predefined_args['embed_size'], 'initializer_range': 0.02, 'intermediate_size': predefined_args['hidden_size'], 'layer_norm_eps': predefined_args['layer_norm_eps'], 'max_position_embeddings': predefined_args['max_length'], 'model_type': 'bort', 'num_attention_heads': predefined_args['num_heads'], 'num_hidden_layers': predefined_args['num_layers'], 'pad_token_id': 1, 'type_vocab_size': 1, 'vocab_size': len(bort_vocab)}\n    hf_bort_config = BertConfig.from_dict(hf_bort_config_json)\n    hf_bort_model = BertForMaskedLM(hf_bort_config)\n    hf_bort_model.eval()\n\n    def to_torch(mx_array) -> nn.Parameter:\n        return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))\n\n    def check_and_map_params(hf_param, gluon_param):\n        shape_hf = hf_param.shape\n        gluon_param = to_torch(params[gluon_param])\n        shape_gluon = gluon_param.shape\n        assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n        return gluon_param\n    hf_bort_model.bert.embeddings.word_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.word_embeddings.weight, 'word_embed.0.weight')\n    hf_bort_model.bert.embeddings.position_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.position_embeddings.weight, 'encoder.position_weight')\n    hf_bort_model.bert.embeddings.LayerNorm.bias = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.bias, 'encoder.layer_norm.beta')\n    hf_bort_model.bert.embeddings.LayerNorm.weight = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.weight, 'encoder.layer_norm.gamma')\n    hf_bort_model.bert.embeddings.token_type_embeddings.weight.data = torch.zeros_like(hf_bort_model.bert.embeddings.token_type_embeddings.weight.data)\n    for i in range(hf_bort_config.num_hidden_layers):\n        layer: BertLayer = hf_bort_model.bert.encoder.layer[i]\n        self_attn: BertSelfAttention = layer.attention.self\n        self_attn.key.bias.data = check_and_map_params(self_attn.key.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.bias')\n        self_attn.key.weight.data = check_and_map_params(self_attn.key.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.weight')\n        self_attn.query.bias.data = check_and_map_params(self_attn.query.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.bias')\n        self_attn.query.weight.data = check_and_map_params(self_attn.query.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.weight')\n        self_attn.value.bias.data = check_and_map_params(self_attn.value.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.bias')\n        self_attn.value.weight.data = check_and_map_params(self_attn.value.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.weight')\n        self_output: BertSelfOutput = layer.attention.output\n        self_output.dense.bias = check_and_map_params(self_output.dense.bias, f'encoder.transformer_cells.{i}.proj.bias')\n        self_output.dense.weight = check_and_map_params(self_output.dense.weight, f'encoder.transformer_cells.{i}.proj.weight')\n        self_output.LayerNorm.bias = check_and_map_params(self_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.layer_norm.beta')\n        self_output.LayerNorm.weight = check_and_map_params(self_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.layer_norm.gamma')\n        intermediate: BertIntermediate = layer.intermediate\n        intermediate.dense.bias = check_and_map_params(intermediate.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_1.bias')\n        intermediate.dense.weight = check_and_map_params(intermediate.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_1.weight')\n        bert_output: BertOutput = layer.output\n        bert_output.dense.bias = check_and_map_params(bert_output.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_2.bias')\n        bert_output.dense.weight = check_and_map_params(bert_output.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_2.weight')\n        bert_output.LayerNorm.bias = check_and_map_params(bert_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.ffn.layer_norm.beta')\n        bert_output.LayerNorm.weight = check_and_map_params(bert_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.ffn.layer_norm.gamma')\n    hf_bort_model.half()\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT)['input_ids']\n    gluon_input_ids = mx.nd.array([input_ids])\n    output_gluon = original_bort(inputs=gluon_input_ids, token_types=[])\n    hf_bort_model.save_pretrained(pytorch_dump_folder_path)\n    hf_bort_model = BertModel.from_pretrained(pytorch_dump_folder_path)\n    hf_bort_model.eval()\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT, return_tensors='pt')\n    output_hf = hf_bort_model(**input_ids)[0]\n    gluon_layer = output_gluon[0].asnumpy()\n    hf_layer = output_hf[0].detach().numpy()\n    max_absolute_diff = np.max(np.abs(hf_layer - gluon_layer)).item()\n    success = np.allclose(gluon_layer, hf_layer, atol=0.001)\n    if success:\n        print('\u2714\ufe0f Both model do output the same tensors')\n    else:\n        print('\u274c Both model do **NOT** output the same tensors')\n        print('Absolute difference is:', max_absolute_diff)",
            "def convert_bort_checkpoint_to_pytorch(bort_checkpoint_path: str, pytorch_dump_folder_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Convert the original Bort checkpoint (based on MXNET and Gluonnlp) to our BERT structure-\\n    '\n    bort_4_8_768_1024_hparams = {'attention_cell': 'multi_head', 'num_layers': 4, 'units': 1024, 'hidden_size': 768, 'max_length': 512, 'num_heads': 8, 'scaled': True, 'dropout': 0.1, 'use_residual': True, 'embed_size': 1024, 'embed_dropout': 0.1, 'word_embed': None, 'layer_norm_eps': 1e-05, 'token_type_vocab_size': 2}\n    predefined_args = bort_4_8_768_1024_hparams\n    encoder = BERTEncoder(attention_cell=predefined_args['attention_cell'], num_layers=predefined_args['num_layers'], units=predefined_args['units'], hidden_size=predefined_args['hidden_size'], max_length=predefined_args['max_length'], num_heads=predefined_args['num_heads'], scaled=predefined_args['scaled'], dropout=predefined_args['dropout'], output_attention=False, output_all_encodings=False, use_residual=predefined_args['use_residual'], activation=predefined_args.get('activation', 'gelu'), layer_norm_eps=predefined_args.get('layer_norm_eps', None))\n    vocab_name = 'openwebtext_ccnews_stories_books_cased'\n    gluon_cache_dir = os.path.join(get_home_dir(), 'models')\n    bort_vocab = _load_vocab(vocab_name, None, gluon_cache_dir, cls=Vocab)\n    original_bort = nlp.model.BERTModel(encoder, len(bort_vocab), units=predefined_args['units'], embed_size=predefined_args['embed_size'], embed_dropout=predefined_args['embed_dropout'], word_embed=predefined_args['word_embed'], use_pooler=False, use_token_type_embed=False, token_type_vocab_size=predefined_args['token_type_vocab_size'], use_classifier=False, use_decoder=False)\n    original_bort.load_parameters(bort_checkpoint_path, cast_dtype=True, ignore_extra=True)\n    params = original_bort._collect_params_with_prefix()\n    hf_bort_config_json = {'architectures': ['BertForMaskedLM'], 'attention_probs_dropout_prob': predefined_args['dropout'], 'hidden_act': 'gelu', 'hidden_dropout_prob': predefined_args['dropout'], 'hidden_size': predefined_args['embed_size'], 'initializer_range': 0.02, 'intermediate_size': predefined_args['hidden_size'], 'layer_norm_eps': predefined_args['layer_norm_eps'], 'max_position_embeddings': predefined_args['max_length'], 'model_type': 'bort', 'num_attention_heads': predefined_args['num_heads'], 'num_hidden_layers': predefined_args['num_layers'], 'pad_token_id': 1, 'type_vocab_size': 1, 'vocab_size': len(bort_vocab)}\n    hf_bort_config = BertConfig.from_dict(hf_bort_config_json)\n    hf_bort_model = BertForMaskedLM(hf_bort_config)\n    hf_bort_model.eval()\n\n    def to_torch(mx_array) -> nn.Parameter:\n        return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))\n\n    def check_and_map_params(hf_param, gluon_param):\n        shape_hf = hf_param.shape\n        gluon_param = to_torch(params[gluon_param])\n        shape_gluon = gluon_param.shape\n        assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n        return gluon_param\n    hf_bort_model.bert.embeddings.word_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.word_embeddings.weight, 'word_embed.0.weight')\n    hf_bort_model.bert.embeddings.position_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.position_embeddings.weight, 'encoder.position_weight')\n    hf_bort_model.bert.embeddings.LayerNorm.bias = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.bias, 'encoder.layer_norm.beta')\n    hf_bort_model.bert.embeddings.LayerNorm.weight = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.weight, 'encoder.layer_norm.gamma')\n    hf_bort_model.bert.embeddings.token_type_embeddings.weight.data = torch.zeros_like(hf_bort_model.bert.embeddings.token_type_embeddings.weight.data)\n    for i in range(hf_bort_config.num_hidden_layers):\n        layer: BertLayer = hf_bort_model.bert.encoder.layer[i]\n        self_attn: BertSelfAttention = layer.attention.self\n        self_attn.key.bias.data = check_and_map_params(self_attn.key.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.bias')\n        self_attn.key.weight.data = check_and_map_params(self_attn.key.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.weight')\n        self_attn.query.bias.data = check_and_map_params(self_attn.query.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.bias')\n        self_attn.query.weight.data = check_and_map_params(self_attn.query.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.weight')\n        self_attn.value.bias.data = check_and_map_params(self_attn.value.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.bias')\n        self_attn.value.weight.data = check_and_map_params(self_attn.value.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.weight')\n        self_output: BertSelfOutput = layer.attention.output\n        self_output.dense.bias = check_and_map_params(self_output.dense.bias, f'encoder.transformer_cells.{i}.proj.bias')\n        self_output.dense.weight = check_and_map_params(self_output.dense.weight, f'encoder.transformer_cells.{i}.proj.weight')\n        self_output.LayerNorm.bias = check_and_map_params(self_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.layer_norm.beta')\n        self_output.LayerNorm.weight = check_and_map_params(self_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.layer_norm.gamma')\n        intermediate: BertIntermediate = layer.intermediate\n        intermediate.dense.bias = check_and_map_params(intermediate.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_1.bias')\n        intermediate.dense.weight = check_and_map_params(intermediate.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_1.weight')\n        bert_output: BertOutput = layer.output\n        bert_output.dense.bias = check_and_map_params(bert_output.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_2.bias')\n        bert_output.dense.weight = check_and_map_params(bert_output.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_2.weight')\n        bert_output.LayerNorm.bias = check_and_map_params(bert_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.ffn.layer_norm.beta')\n        bert_output.LayerNorm.weight = check_and_map_params(bert_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.ffn.layer_norm.gamma')\n    hf_bort_model.half()\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT)['input_ids']\n    gluon_input_ids = mx.nd.array([input_ids])\n    output_gluon = original_bort(inputs=gluon_input_ids, token_types=[])\n    hf_bort_model.save_pretrained(pytorch_dump_folder_path)\n    hf_bort_model = BertModel.from_pretrained(pytorch_dump_folder_path)\n    hf_bort_model.eval()\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT, return_tensors='pt')\n    output_hf = hf_bort_model(**input_ids)[0]\n    gluon_layer = output_gluon[0].asnumpy()\n    hf_layer = output_hf[0].detach().numpy()\n    max_absolute_diff = np.max(np.abs(hf_layer - gluon_layer)).item()\n    success = np.allclose(gluon_layer, hf_layer, atol=0.001)\n    if success:\n        print('\u2714\ufe0f Both model do output the same tensors')\n    else:\n        print('\u274c Both model do **NOT** output the same tensors')\n        print('Absolute difference is:', max_absolute_diff)",
            "def convert_bort_checkpoint_to_pytorch(bort_checkpoint_path: str, pytorch_dump_folder_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Convert the original Bort checkpoint (based on MXNET and Gluonnlp) to our BERT structure-\\n    '\n    bort_4_8_768_1024_hparams = {'attention_cell': 'multi_head', 'num_layers': 4, 'units': 1024, 'hidden_size': 768, 'max_length': 512, 'num_heads': 8, 'scaled': True, 'dropout': 0.1, 'use_residual': True, 'embed_size': 1024, 'embed_dropout': 0.1, 'word_embed': None, 'layer_norm_eps': 1e-05, 'token_type_vocab_size': 2}\n    predefined_args = bort_4_8_768_1024_hparams\n    encoder = BERTEncoder(attention_cell=predefined_args['attention_cell'], num_layers=predefined_args['num_layers'], units=predefined_args['units'], hidden_size=predefined_args['hidden_size'], max_length=predefined_args['max_length'], num_heads=predefined_args['num_heads'], scaled=predefined_args['scaled'], dropout=predefined_args['dropout'], output_attention=False, output_all_encodings=False, use_residual=predefined_args['use_residual'], activation=predefined_args.get('activation', 'gelu'), layer_norm_eps=predefined_args.get('layer_norm_eps', None))\n    vocab_name = 'openwebtext_ccnews_stories_books_cased'\n    gluon_cache_dir = os.path.join(get_home_dir(), 'models')\n    bort_vocab = _load_vocab(vocab_name, None, gluon_cache_dir, cls=Vocab)\n    original_bort = nlp.model.BERTModel(encoder, len(bort_vocab), units=predefined_args['units'], embed_size=predefined_args['embed_size'], embed_dropout=predefined_args['embed_dropout'], word_embed=predefined_args['word_embed'], use_pooler=False, use_token_type_embed=False, token_type_vocab_size=predefined_args['token_type_vocab_size'], use_classifier=False, use_decoder=False)\n    original_bort.load_parameters(bort_checkpoint_path, cast_dtype=True, ignore_extra=True)\n    params = original_bort._collect_params_with_prefix()\n    hf_bort_config_json = {'architectures': ['BertForMaskedLM'], 'attention_probs_dropout_prob': predefined_args['dropout'], 'hidden_act': 'gelu', 'hidden_dropout_prob': predefined_args['dropout'], 'hidden_size': predefined_args['embed_size'], 'initializer_range': 0.02, 'intermediate_size': predefined_args['hidden_size'], 'layer_norm_eps': predefined_args['layer_norm_eps'], 'max_position_embeddings': predefined_args['max_length'], 'model_type': 'bort', 'num_attention_heads': predefined_args['num_heads'], 'num_hidden_layers': predefined_args['num_layers'], 'pad_token_id': 1, 'type_vocab_size': 1, 'vocab_size': len(bort_vocab)}\n    hf_bort_config = BertConfig.from_dict(hf_bort_config_json)\n    hf_bort_model = BertForMaskedLM(hf_bort_config)\n    hf_bort_model.eval()\n\n    def to_torch(mx_array) -> nn.Parameter:\n        return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))\n\n    def check_and_map_params(hf_param, gluon_param):\n        shape_hf = hf_param.shape\n        gluon_param = to_torch(params[gluon_param])\n        shape_gluon = gluon_param.shape\n        assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n        return gluon_param\n    hf_bort_model.bert.embeddings.word_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.word_embeddings.weight, 'word_embed.0.weight')\n    hf_bort_model.bert.embeddings.position_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.position_embeddings.weight, 'encoder.position_weight')\n    hf_bort_model.bert.embeddings.LayerNorm.bias = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.bias, 'encoder.layer_norm.beta')\n    hf_bort_model.bert.embeddings.LayerNorm.weight = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.weight, 'encoder.layer_norm.gamma')\n    hf_bort_model.bert.embeddings.token_type_embeddings.weight.data = torch.zeros_like(hf_bort_model.bert.embeddings.token_type_embeddings.weight.data)\n    for i in range(hf_bort_config.num_hidden_layers):\n        layer: BertLayer = hf_bort_model.bert.encoder.layer[i]\n        self_attn: BertSelfAttention = layer.attention.self\n        self_attn.key.bias.data = check_and_map_params(self_attn.key.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.bias')\n        self_attn.key.weight.data = check_and_map_params(self_attn.key.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.weight')\n        self_attn.query.bias.data = check_and_map_params(self_attn.query.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.bias')\n        self_attn.query.weight.data = check_and_map_params(self_attn.query.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.weight')\n        self_attn.value.bias.data = check_and_map_params(self_attn.value.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.bias')\n        self_attn.value.weight.data = check_and_map_params(self_attn.value.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.weight')\n        self_output: BertSelfOutput = layer.attention.output\n        self_output.dense.bias = check_and_map_params(self_output.dense.bias, f'encoder.transformer_cells.{i}.proj.bias')\n        self_output.dense.weight = check_and_map_params(self_output.dense.weight, f'encoder.transformer_cells.{i}.proj.weight')\n        self_output.LayerNorm.bias = check_and_map_params(self_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.layer_norm.beta')\n        self_output.LayerNorm.weight = check_and_map_params(self_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.layer_norm.gamma')\n        intermediate: BertIntermediate = layer.intermediate\n        intermediate.dense.bias = check_and_map_params(intermediate.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_1.bias')\n        intermediate.dense.weight = check_and_map_params(intermediate.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_1.weight')\n        bert_output: BertOutput = layer.output\n        bert_output.dense.bias = check_and_map_params(bert_output.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_2.bias')\n        bert_output.dense.weight = check_and_map_params(bert_output.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_2.weight')\n        bert_output.LayerNorm.bias = check_and_map_params(bert_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.ffn.layer_norm.beta')\n        bert_output.LayerNorm.weight = check_and_map_params(bert_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.ffn.layer_norm.gamma')\n    hf_bort_model.half()\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT)['input_ids']\n    gluon_input_ids = mx.nd.array([input_ids])\n    output_gluon = original_bort(inputs=gluon_input_ids, token_types=[])\n    hf_bort_model.save_pretrained(pytorch_dump_folder_path)\n    hf_bort_model = BertModel.from_pretrained(pytorch_dump_folder_path)\n    hf_bort_model.eval()\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT, return_tensors='pt')\n    output_hf = hf_bort_model(**input_ids)[0]\n    gluon_layer = output_gluon[0].asnumpy()\n    hf_layer = output_hf[0].detach().numpy()\n    max_absolute_diff = np.max(np.abs(hf_layer - gluon_layer)).item()\n    success = np.allclose(gluon_layer, hf_layer, atol=0.001)\n    if success:\n        print('\u2714\ufe0f Both model do output the same tensors')\n    else:\n        print('\u274c Both model do **NOT** output the same tensors')\n        print('Absolute difference is:', max_absolute_diff)",
            "def convert_bort_checkpoint_to_pytorch(bort_checkpoint_path: str, pytorch_dump_folder_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Convert the original Bort checkpoint (based on MXNET and Gluonnlp) to our BERT structure-\\n    '\n    bort_4_8_768_1024_hparams = {'attention_cell': 'multi_head', 'num_layers': 4, 'units': 1024, 'hidden_size': 768, 'max_length': 512, 'num_heads': 8, 'scaled': True, 'dropout': 0.1, 'use_residual': True, 'embed_size': 1024, 'embed_dropout': 0.1, 'word_embed': None, 'layer_norm_eps': 1e-05, 'token_type_vocab_size': 2}\n    predefined_args = bort_4_8_768_1024_hparams\n    encoder = BERTEncoder(attention_cell=predefined_args['attention_cell'], num_layers=predefined_args['num_layers'], units=predefined_args['units'], hidden_size=predefined_args['hidden_size'], max_length=predefined_args['max_length'], num_heads=predefined_args['num_heads'], scaled=predefined_args['scaled'], dropout=predefined_args['dropout'], output_attention=False, output_all_encodings=False, use_residual=predefined_args['use_residual'], activation=predefined_args.get('activation', 'gelu'), layer_norm_eps=predefined_args.get('layer_norm_eps', None))\n    vocab_name = 'openwebtext_ccnews_stories_books_cased'\n    gluon_cache_dir = os.path.join(get_home_dir(), 'models')\n    bort_vocab = _load_vocab(vocab_name, None, gluon_cache_dir, cls=Vocab)\n    original_bort = nlp.model.BERTModel(encoder, len(bort_vocab), units=predefined_args['units'], embed_size=predefined_args['embed_size'], embed_dropout=predefined_args['embed_dropout'], word_embed=predefined_args['word_embed'], use_pooler=False, use_token_type_embed=False, token_type_vocab_size=predefined_args['token_type_vocab_size'], use_classifier=False, use_decoder=False)\n    original_bort.load_parameters(bort_checkpoint_path, cast_dtype=True, ignore_extra=True)\n    params = original_bort._collect_params_with_prefix()\n    hf_bort_config_json = {'architectures': ['BertForMaskedLM'], 'attention_probs_dropout_prob': predefined_args['dropout'], 'hidden_act': 'gelu', 'hidden_dropout_prob': predefined_args['dropout'], 'hidden_size': predefined_args['embed_size'], 'initializer_range': 0.02, 'intermediate_size': predefined_args['hidden_size'], 'layer_norm_eps': predefined_args['layer_norm_eps'], 'max_position_embeddings': predefined_args['max_length'], 'model_type': 'bort', 'num_attention_heads': predefined_args['num_heads'], 'num_hidden_layers': predefined_args['num_layers'], 'pad_token_id': 1, 'type_vocab_size': 1, 'vocab_size': len(bort_vocab)}\n    hf_bort_config = BertConfig.from_dict(hf_bort_config_json)\n    hf_bort_model = BertForMaskedLM(hf_bort_config)\n    hf_bort_model.eval()\n\n    def to_torch(mx_array) -> nn.Parameter:\n        return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))\n\n    def check_and_map_params(hf_param, gluon_param):\n        shape_hf = hf_param.shape\n        gluon_param = to_torch(params[gluon_param])\n        shape_gluon = gluon_param.shape\n        assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n        return gluon_param\n    hf_bort_model.bert.embeddings.word_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.word_embeddings.weight, 'word_embed.0.weight')\n    hf_bort_model.bert.embeddings.position_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.position_embeddings.weight, 'encoder.position_weight')\n    hf_bort_model.bert.embeddings.LayerNorm.bias = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.bias, 'encoder.layer_norm.beta')\n    hf_bort_model.bert.embeddings.LayerNorm.weight = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.weight, 'encoder.layer_norm.gamma')\n    hf_bort_model.bert.embeddings.token_type_embeddings.weight.data = torch.zeros_like(hf_bort_model.bert.embeddings.token_type_embeddings.weight.data)\n    for i in range(hf_bort_config.num_hidden_layers):\n        layer: BertLayer = hf_bort_model.bert.encoder.layer[i]\n        self_attn: BertSelfAttention = layer.attention.self\n        self_attn.key.bias.data = check_and_map_params(self_attn.key.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.bias')\n        self_attn.key.weight.data = check_and_map_params(self_attn.key.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.weight')\n        self_attn.query.bias.data = check_and_map_params(self_attn.query.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.bias')\n        self_attn.query.weight.data = check_and_map_params(self_attn.query.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.weight')\n        self_attn.value.bias.data = check_and_map_params(self_attn.value.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.bias')\n        self_attn.value.weight.data = check_and_map_params(self_attn.value.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.weight')\n        self_output: BertSelfOutput = layer.attention.output\n        self_output.dense.bias = check_and_map_params(self_output.dense.bias, f'encoder.transformer_cells.{i}.proj.bias')\n        self_output.dense.weight = check_and_map_params(self_output.dense.weight, f'encoder.transformer_cells.{i}.proj.weight')\n        self_output.LayerNorm.bias = check_and_map_params(self_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.layer_norm.beta')\n        self_output.LayerNorm.weight = check_and_map_params(self_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.layer_norm.gamma')\n        intermediate: BertIntermediate = layer.intermediate\n        intermediate.dense.bias = check_and_map_params(intermediate.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_1.bias')\n        intermediate.dense.weight = check_and_map_params(intermediate.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_1.weight')\n        bert_output: BertOutput = layer.output\n        bert_output.dense.bias = check_and_map_params(bert_output.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_2.bias')\n        bert_output.dense.weight = check_and_map_params(bert_output.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_2.weight')\n        bert_output.LayerNorm.bias = check_and_map_params(bert_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.ffn.layer_norm.beta')\n        bert_output.LayerNorm.weight = check_and_map_params(bert_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.ffn.layer_norm.gamma')\n    hf_bort_model.half()\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT)['input_ids']\n    gluon_input_ids = mx.nd.array([input_ids])\n    output_gluon = original_bort(inputs=gluon_input_ids, token_types=[])\n    hf_bort_model.save_pretrained(pytorch_dump_folder_path)\n    hf_bort_model = BertModel.from_pretrained(pytorch_dump_folder_path)\n    hf_bort_model.eval()\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT, return_tensors='pt')\n    output_hf = hf_bort_model(**input_ids)[0]\n    gluon_layer = output_gluon[0].asnumpy()\n    hf_layer = output_hf[0].detach().numpy()\n    max_absolute_diff = np.max(np.abs(hf_layer - gluon_layer)).item()\n    success = np.allclose(gluon_layer, hf_layer, atol=0.001)\n    if success:\n        print('\u2714\ufe0f Both model do output the same tensors')\n    else:\n        print('\u274c Both model do **NOT** output the same tensors')\n        print('Absolute difference is:', max_absolute_diff)",
            "def convert_bort_checkpoint_to_pytorch(bort_checkpoint_path: str, pytorch_dump_folder_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Convert the original Bort checkpoint (based on MXNET and Gluonnlp) to our BERT structure-\\n    '\n    bort_4_8_768_1024_hparams = {'attention_cell': 'multi_head', 'num_layers': 4, 'units': 1024, 'hidden_size': 768, 'max_length': 512, 'num_heads': 8, 'scaled': True, 'dropout': 0.1, 'use_residual': True, 'embed_size': 1024, 'embed_dropout': 0.1, 'word_embed': None, 'layer_norm_eps': 1e-05, 'token_type_vocab_size': 2}\n    predefined_args = bort_4_8_768_1024_hparams\n    encoder = BERTEncoder(attention_cell=predefined_args['attention_cell'], num_layers=predefined_args['num_layers'], units=predefined_args['units'], hidden_size=predefined_args['hidden_size'], max_length=predefined_args['max_length'], num_heads=predefined_args['num_heads'], scaled=predefined_args['scaled'], dropout=predefined_args['dropout'], output_attention=False, output_all_encodings=False, use_residual=predefined_args['use_residual'], activation=predefined_args.get('activation', 'gelu'), layer_norm_eps=predefined_args.get('layer_norm_eps', None))\n    vocab_name = 'openwebtext_ccnews_stories_books_cased'\n    gluon_cache_dir = os.path.join(get_home_dir(), 'models')\n    bort_vocab = _load_vocab(vocab_name, None, gluon_cache_dir, cls=Vocab)\n    original_bort = nlp.model.BERTModel(encoder, len(bort_vocab), units=predefined_args['units'], embed_size=predefined_args['embed_size'], embed_dropout=predefined_args['embed_dropout'], word_embed=predefined_args['word_embed'], use_pooler=False, use_token_type_embed=False, token_type_vocab_size=predefined_args['token_type_vocab_size'], use_classifier=False, use_decoder=False)\n    original_bort.load_parameters(bort_checkpoint_path, cast_dtype=True, ignore_extra=True)\n    params = original_bort._collect_params_with_prefix()\n    hf_bort_config_json = {'architectures': ['BertForMaskedLM'], 'attention_probs_dropout_prob': predefined_args['dropout'], 'hidden_act': 'gelu', 'hidden_dropout_prob': predefined_args['dropout'], 'hidden_size': predefined_args['embed_size'], 'initializer_range': 0.02, 'intermediate_size': predefined_args['hidden_size'], 'layer_norm_eps': predefined_args['layer_norm_eps'], 'max_position_embeddings': predefined_args['max_length'], 'model_type': 'bort', 'num_attention_heads': predefined_args['num_heads'], 'num_hidden_layers': predefined_args['num_layers'], 'pad_token_id': 1, 'type_vocab_size': 1, 'vocab_size': len(bort_vocab)}\n    hf_bort_config = BertConfig.from_dict(hf_bort_config_json)\n    hf_bort_model = BertForMaskedLM(hf_bort_config)\n    hf_bort_model.eval()\n\n    def to_torch(mx_array) -> nn.Parameter:\n        return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))\n\n    def check_and_map_params(hf_param, gluon_param):\n        shape_hf = hf_param.shape\n        gluon_param = to_torch(params[gluon_param])\n        shape_gluon = gluon_param.shape\n        assert shape_hf == shape_gluon, f'The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers'\n        return gluon_param\n    hf_bort_model.bert.embeddings.word_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.word_embeddings.weight, 'word_embed.0.weight')\n    hf_bort_model.bert.embeddings.position_embeddings.weight = check_and_map_params(hf_bort_model.bert.embeddings.position_embeddings.weight, 'encoder.position_weight')\n    hf_bort_model.bert.embeddings.LayerNorm.bias = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.bias, 'encoder.layer_norm.beta')\n    hf_bort_model.bert.embeddings.LayerNorm.weight = check_and_map_params(hf_bort_model.bert.embeddings.LayerNorm.weight, 'encoder.layer_norm.gamma')\n    hf_bort_model.bert.embeddings.token_type_embeddings.weight.data = torch.zeros_like(hf_bort_model.bert.embeddings.token_type_embeddings.weight.data)\n    for i in range(hf_bort_config.num_hidden_layers):\n        layer: BertLayer = hf_bort_model.bert.encoder.layer[i]\n        self_attn: BertSelfAttention = layer.attention.self\n        self_attn.key.bias.data = check_and_map_params(self_attn.key.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.bias')\n        self_attn.key.weight.data = check_and_map_params(self_attn.key.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_key.weight')\n        self_attn.query.bias.data = check_and_map_params(self_attn.query.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.bias')\n        self_attn.query.weight.data = check_and_map_params(self_attn.query.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_query.weight')\n        self_attn.value.bias.data = check_and_map_params(self_attn.value.bias.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.bias')\n        self_attn.value.weight.data = check_and_map_params(self_attn.value.weight.data, f'encoder.transformer_cells.{i}.attention_cell.proj_value.weight')\n        self_output: BertSelfOutput = layer.attention.output\n        self_output.dense.bias = check_and_map_params(self_output.dense.bias, f'encoder.transformer_cells.{i}.proj.bias')\n        self_output.dense.weight = check_and_map_params(self_output.dense.weight, f'encoder.transformer_cells.{i}.proj.weight')\n        self_output.LayerNorm.bias = check_and_map_params(self_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.layer_norm.beta')\n        self_output.LayerNorm.weight = check_and_map_params(self_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.layer_norm.gamma')\n        intermediate: BertIntermediate = layer.intermediate\n        intermediate.dense.bias = check_and_map_params(intermediate.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_1.bias')\n        intermediate.dense.weight = check_and_map_params(intermediate.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_1.weight')\n        bert_output: BertOutput = layer.output\n        bert_output.dense.bias = check_and_map_params(bert_output.dense.bias, f'encoder.transformer_cells.{i}.ffn.ffn_2.bias')\n        bert_output.dense.weight = check_and_map_params(bert_output.dense.weight, f'encoder.transformer_cells.{i}.ffn.ffn_2.weight')\n        bert_output.LayerNorm.bias = check_and_map_params(bert_output.LayerNorm.bias, f'encoder.transformer_cells.{i}.ffn.layer_norm.beta')\n        bert_output.LayerNorm.weight = check_and_map_params(bert_output.LayerNorm.weight, f'encoder.transformer_cells.{i}.ffn.layer_norm.gamma')\n    hf_bort_model.half()\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT)['input_ids']\n    gluon_input_ids = mx.nd.array([input_ids])\n    output_gluon = original_bort(inputs=gluon_input_ids, token_types=[])\n    hf_bort_model.save_pretrained(pytorch_dump_folder_path)\n    hf_bort_model = BertModel.from_pretrained(pytorch_dump_folder_path)\n    hf_bort_model.eval()\n    input_ids = tokenizer.encode_plus(SAMPLE_TEXT, return_tensors='pt')\n    output_hf = hf_bort_model(**input_ids)[0]\n    gluon_layer = output_gluon[0].asnumpy()\n    hf_layer = output_hf[0].detach().numpy()\n    max_absolute_diff = np.max(np.abs(hf_layer - gluon_layer)).item()\n    success = np.allclose(gluon_layer, hf_layer, atol=0.001)\n    if success:\n        print('\u2714\ufe0f Both model do output the same tensors')\n    else:\n        print('\u274c Both model do **NOT** output the same tensors')\n        print('Absolute difference is:', max_absolute_diff)"
        ]
    }
]