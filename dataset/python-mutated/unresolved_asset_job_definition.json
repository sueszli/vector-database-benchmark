[
    {
        "func_name": "__new__",
        "original": "def __new__(cls, name: str, selection: 'AssetSelection', config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None):\n    from dagster._core.definitions import AssetSelection, ExecutorDefinition, HookDefinition, PartitionsDefinition\n    from dagster._core.definitions.run_config import convert_config_input\n    return super(UnresolvedAssetJobDefinition, cls).__new__(cls, name=check.str_param(name, 'name'), selection=check.inst_param(selection, 'selection', AssetSelection), config=convert_config_input(config), description=check.opt_str_param(description, 'description'), tags=check.opt_mapping_param(tags, 'tags'), metadata=check.opt_mapping_param(metadata, 'metadata'), partitions_def=check.opt_inst_param(partitions_def, 'partitions_def', PartitionsDefinition), executor_def=check.opt_inst_param(executor_def, 'partitions_def', ExecutorDefinition), hooks=check.opt_nullable_set_param(hooks, 'hooks', of_type=HookDefinition))",
        "mutated": [
            "def __new__(cls, name: str, selection: 'AssetSelection', config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None):\n    if False:\n        i = 10\n    from dagster._core.definitions import AssetSelection, ExecutorDefinition, HookDefinition, PartitionsDefinition\n    from dagster._core.definitions.run_config import convert_config_input\n    return super(UnresolvedAssetJobDefinition, cls).__new__(cls, name=check.str_param(name, 'name'), selection=check.inst_param(selection, 'selection', AssetSelection), config=convert_config_input(config), description=check.opt_str_param(description, 'description'), tags=check.opt_mapping_param(tags, 'tags'), metadata=check.opt_mapping_param(metadata, 'metadata'), partitions_def=check.opt_inst_param(partitions_def, 'partitions_def', PartitionsDefinition), executor_def=check.opt_inst_param(executor_def, 'partitions_def', ExecutorDefinition), hooks=check.opt_nullable_set_param(hooks, 'hooks', of_type=HookDefinition))",
            "def __new__(cls, name: str, selection: 'AssetSelection', config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from dagster._core.definitions import AssetSelection, ExecutorDefinition, HookDefinition, PartitionsDefinition\n    from dagster._core.definitions.run_config import convert_config_input\n    return super(UnresolvedAssetJobDefinition, cls).__new__(cls, name=check.str_param(name, 'name'), selection=check.inst_param(selection, 'selection', AssetSelection), config=convert_config_input(config), description=check.opt_str_param(description, 'description'), tags=check.opt_mapping_param(tags, 'tags'), metadata=check.opt_mapping_param(metadata, 'metadata'), partitions_def=check.opt_inst_param(partitions_def, 'partitions_def', PartitionsDefinition), executor_def=check.opt_inst_param(executor_def, 'partitions_def', ExecutorDefinition), hooks=check.opt_nullable_set_param(hooks, 'hooks', of_type=HookDefinition))",
            "def __new__(cls, name: str, selection: 'AssetSelection', config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from dagster._core.definitions import AssetSelection, ExecutorDefinition, HookDefinition, PartitionsDefinition\n    from dagster._core.definitions.run_config import convert_config_input\n    return super(UnresolvedAssetJobDefinition, cls).__new__(cls, name=check.str_param(name, 'name'), selection=check.inst_param(selection, 'selection', AssetSelection), config=convert_config_input(config), description=check.opt_str_param(description, 'description'), tags=check.opt_mapping_param(tags, 'tags'), metadata=check.opt_mapping_param(metadata, 'metadata'), partitions_def=check.opt_inst_param(partitions_def, 'partitions_def', PartitionsDefinition), executor_def=check.opt_inst_param(executor_def, 'partitions_def', ExecutorDefinition), hooks=check.opt_nullable_set_param(hooks, 'hooks', of_type=HookDefinition))",
            "def __new__(cls, name: str, selection: 'AssetSelection', config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from dagster._core.definitions import AssetSelection, ExecutorDefinition, HookDefinition, PartitionsDefinition\n    from dagster._core.definitions.run_config import convert_config_input\n    return super(UnresolvedAssetJobDefinition, cls).__new__(cls, name=check.str_param(name, 'name'), selection=check.inst_param(selection, 'selection', AssetSelection), config=convert_config_input(config), description=check.opt_str_param(description, 'description'), tags=check.opt_mapping_param(tags, 'tags'), metadata=check.opt_mapping_param(metadata, 'metadata'), partitions_def=check.opt_inst_param(partitions_def, 'partitions_def', PartitionsDefinition), executor_def=check.opt_inst_param(executor_def, 'partitions_def', ExecutorDefinition), hooks=check.opt_nullable_set_param(hooks, 'hooks', of_type=HookDefinition))",
            "def __new__(cls, name: str, selection: 'AssetSelection', config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from dagster._core.definitions import AssetSelection, ExecutorDefinition, HookDefinition, PartitionsDefinition\n    from dagster._core.definitions.run_config import convert_config_input\n    return super(UnresolvedAssetJobDefinition, cls).__new__(cls, name=check.str_param(name, 'name'), selection=check.inst_param(selection, 'selection', AssetSelection), config=convert_config_input(config), description=check.opt_str_param(description, 'description'), tags=check.opt_mapping_param(tags, 'tags'), metadata=check.opt_mapping_param(metadata, 'metadata'), partitions_def=check.opt_inst_param(partitions_def, 'partitions_def', PartitionsDefinition), executor_def=check.opt_inst_param(executor_def, 'partitions_def', ExecutorDefinition), hooks=check.opt_nullable_set_param(hooks, 'hooks', of_type=HookDefinition))"
        ]
    },
    {
        "func_name": "run_request_for_partition",
        "original": "@deprecated(breaking_version='2.0.0', additional_warn_text='Directly instantiate `RunRequest(partition_key=...)` instead.')\ndef run_request_for_partition(self, partition_key: str, run_key: Optional[str]=None, tags: Optional[Mapping[str, str]]=None, asset_selection: Optional[Sequence[AssetKey]]=None, run_config: Optional[Mapping[str, Any]]=None, current_time: Optional[datetime]=None, dynamic_partitions_store: Optional[DynamicPartitionsStore]=None) -> RunRequest:\n    \"\"\"Creates a RunRequest object for a run that processes the given partition.\n\n        Args:\n            partition_key: The key of the partition to request a run for.\n            run_key (Optional[str]): A string key to identify this launched run. For sensors, ensures that\n                only one run is created per run key across all sensor evaluations.  For schedules,\n                ensures that one run is created per tick, across failure recoveries. Passing in a `None`\n                value means that a run will always be launched per evaluation.\n            tags (Optional[Dict[str, str]]): A dictionary of tags (string key-value pairs) to attach\n                to the launched run.\n            run_config (Optional[Mapping[str, Any]]: Configuration for the run. If the job has\n                a :py:class:`PartitionedConfig`, this value will override replace the config\n                provided by it.\n            current_time (Optional[datetime]): Used to determine which time-partitions exist.\n                Defaults to now.\n            dynamic_partitions_store (Optional[DynamicPartitionsStore]): The DynamicPartitionsStore\n                object that is responsible for fetching dynamic partitions. Required when the\n                partitions definition is a DynamicPartitionsDefinition with a name defined. Users\n                can pass the DagsterInstance fetched via `context.instance` to this argument.\n\n        Returns:\n            RunRequest: an object that requests a run to process the given partition.\n        \"\"\"\n    from dagster._core.definitions.partition import DynamicPartitionsDefinition, PartitionedConfig\n    if not self.partitions_def:\n        check.failed('Called run_request_for_partition on a non-partitioned job')\n    partitioned_config = PartitionedConfig.from_flexible_config(self.config, self.partitions_def)\n    if isinstance(self.partitions_def, DynamicPartitionsDefinition) and self.partitions_def.name:\n        check.failed('run_request_for_partition is not supported for dynamic partitions. Instead, use RunRequest(partition_key=...)')\n    self.partitions_def.validate_partition_key(partition_key, current_time=current_time, dynamic_partitions_store=dynamic_partitions_store)\n    run_config = run_config if run_config is not None else partitioned_config.get_run_config_for_partition_key(partition_key)\n    run_request_tags = {**(tags or {}), **partitioned_config.get_tags_for_partition_key(partition_key)}\n    return RunRequest(job_name=self.name, run_key=run_key, run_config=run_config, tags=run_request_tags, asset_selection=asset_selection, partition_key=partition_key)",
        "mutated": [
            "@deprecated(breaking_version='2.0.0', additional_warn_text='Directly instantiate `RunRequest(partition_key=...)` instead.')\ndef run_request_for_partition(self, partition_key: str, run_key: Optional[str]=None, tags: Optional[Mapping[str, str]]=None, asset_selection: Optional[Sequence[AssetKey]]=None, run_config: Optional[Mapping[str, Any]]=None, current_time: Optional[datetime]=None, dynamic_partitions_store: Optional[DynamicPartitionsStore]=None) -> RunRequest:\n    if False:\n        i = 10\n    'Creates a RunRequest object for a run that processes the given partition.\\n\\n        Args:\\n            partition_key: The key of the partition to request a run for.\\n            run_key (Optional[str]): A string key to identify this launched run. For sensors, ensures that\\n                only one run is created per run key across all sensor evaluations.  For schedules,\\n                ensures that one run is created per tick, across failure recoveries. Passing in a `None`\\n                value means that a run will always be launched per evaluation.\\n            tags (Optional[Dict[str, str]]): A dictionary of tags (string key-value pairs) to attach\\n                to the launched run.\\n            run_config (Optional[Mapping[str, Any]]: Configuration for the run. If the job has\\n                a :py:class:`PartitionedConfig`, this value will override replace the config\\n                provided by it.\\n            current_time (Optional[datetime]): Used to determine which time-partitions exist.\\n                Defaults to now.\\n            dynamic_partitions_store (Optional[DynamicPartitionsStore]): The DynamicPartitionsStore\\n                object that is responsible for fetching dynamic partitions. Required when the\\n                partitions definition is a DynamicPartitionsDefinition with a name defined. Users\\n                can pass the DagsterInstance fetched via `context.instance` to this argument.\\n\\n        Returns:\\n            RunRequest: an object that requests a run to process the given partition.\\n        '\n    from dagster._core.definitions.partition import DynamicPartitionsDefinition, PartitionedConfig\n    if not self.partitions_def:\n        check.failed('Called run_request_for_partition on a non-partitioned job')\n    partitioned_config = PartitionedConfig.from_flexible_config(self.config, self.partitions_def)\n    if isinstance(self.partitions_def, DynamicPartitionsDefinition) and self.partitions_def.name:\n        check.failed('run_request_for_partition is not supported for dynamic partitions. Instead, use RunRequest(partition_key=...)')\n    self.partitions_def.validate_partition_key(partition_key, current_time=current_time, dynamic_partitions_store=dynamic_partitions_store)\n    run_config = run_config if run_config is not None else partitioned_config.get_run_config_for_partition_key(partition_key)\n    run_request_tags = {**(tags or {}), **partitioned_config.get_tags_for_partition_key(partition_key)}\n    return RunRequest(job_name=self.name, run_key=run_key, run_config=run_config, tags=run_request_tags, asset_selection=asset_selection, partition_key=partition_key)",
            "@deprecated(breaking_version='2.0.0', additional_warn_text='Directly instantiate `RunRequest(partition_key=...)` instead.')\ndef run_request_for_partition(self, partition_key: str, run_key: Optional[str]=None, tags: Optional[Mapping[str, str]]=None, asset_selection: Optional[Sequence[AssetKey]]=None, run_config: Optional[Mapping[str, Any]]=None, current_time: Optional[datetime]=None, dynamic_partitions_store: Optional[DynamicPartitionsStore]=None) -> RunRequest:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a RunRequest object for a run that processes the given partition.\\n\\n        Args:\\n            partition_key: The key of the partition to request a run for.\\n            run_key (Optional[str]): A string key to identify this launched run. For sensors, ensures that\\n                only one run is created per run key across all sensor evaluations.  For schedules,\\n                ensures that one run is created per tick, across failure recoveries. Passing in a `None`\\n                value means that a run will always be launched per evaluation.\\n            tags (Optional[Dict[str, str]]): A dictionary of tags (string key-value pairs) to attach\\n                to the launched run.\\n            run_config (Optional[Mapping[str, Any]]: Configuration for the run. If the job has\\n                a :py:class:`PartitionedConfig`, this value will override replace the config\\n                provided by it.\\n            current_time (Optional[datetime]): Used to determine which time-partitions exist.\\n                Defaults to now.\\n            dynamic_partitions_store (Optional[DynamicPartitionsStore]): The DynamicPartitionsStore\\n                object that is responsible for fetching dynamic partitions. Required when the\\n                partitions definition is a DynamicPartitionsDefinition with a name defined. Users\\n                can pass the DagsterInstance fetched via `context.instance` to this argument.\\n\\n        Returns:\\n            RunRequest: an object that requests a run to process the given partition.\\n        '\n    from dagster._core.definitions.partition import DynamicPartitionsDefinition, PartitionedConfig\n    if not self.partitions_def:\n        check.failed('Called run_request_for_partition on a non-partitioned job')\n    partitioned_config = PartitionedConfig.from_flexible_config(self.config, self.partitions_def)\n    if isinstance(self.partitions_def, DynamicPartitionsDefinition) and self.partitions_def.name:\n        check.failed('run_request_for_partition is not supported for dynamic partitions. Instead, use RunRequest(partition_key=...)')\n    self.partitions_def.validate_partition_key(partition_key, current_time=current_time, dynamic_partitions_store=dynamic_partitions_store)\n    run_config = run_config if run_config is not None else partitioned_config.get_run_config_for_partition_key(partition_key)\n    run_request_tags = {**(tags or {}), **partitioned_config.get_tags_for_partition_key(partition_key)}\n    return RunRequest(job_name=self.name, run_key=run_key, run_config=run_config, tags=run_request_tags, asset_selection=asset_selection, partition_key=partition_key)",
            "@deprecated(breaking_version='2.0.0', additional_warn_text='Directly instantiate `RunRequest(partition_key=...)` instead.')\ndef run_request_for_partition(self, partition_key: str, run_key: Optional[str]=None, tags: Optional[Mapping[str, str]]=None, asset_selection: Optional[Sequence[AssetKey]]=None, run_config: Optional[Mapping[str, Any]]=None, current_time: Optional[datetime]=None, dynamic_partitions_store: Optional[DynamicPartitionsStore]=None) -> RunRequest:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a RunRequest object for a run that processes the given partition.\\n\\n        Args:\\n            partition_key: The key of the partition to request a run for.\\n            run_key (Optional[str]): A string key to identify this launched run. For sensors, ensures that\\n                only one run is created per run key across all sensor evaluations.  For schedules,\\n                ensures that one run is created per tick, across failure recoveries. Passing in a `None`\\n                value means that a run will always be launched per evaluation.\\n            tags (Optional[Dict[str, str]]): A dictionary of tags (string key-value pairs) to attach\\n                to the launched run.\\n            run_config (Optional[Mapping[str, Any]]: Configuration for the run. If the job has\\n                a :py:class:`PartitionedConfig`, this value will override replace the config\\n                provided by it.\\n            current_time (Optional[datetime]): Used to determine which time-partitions exist.\\n                Defaults to now.\\n            dynamic_partitions_store (Optional[DynamicPartitionsStore]): The DynamicPartitionsStore\\n                object that is responsible for fetching dynamic partitions. Required when the\\n                partitions definition is a DynamicPartitionsDefinition with a name defined. Users\\n                can pass the DagsterInstance fetched via `context.instance` to this argument.\\n\\n        Returns:\\n            RunRequest: an object that requests a run to process the given partition.\\n        '\n    from dagster._core.definitions.partition import DynamicPartitionsDefinition, PartitionedConfig\n    if not self.partitions_def:\n        check.failed('Called run_request_for_partition on a non-partitioned job')\n    partitioned_config = PartitionedConfig.from_flexible_config(self.config, self.partitions_def)\n    if isinstance(self.partitions_def, DynamicPartitionsDefinition) and self.partitions_def.name:\n        check.failed('run_request_for_partition is not supported for dynamic partitions. Instead, use RunRequest(partition_key=...)')\n    self.partitions_def.validate_partition_key(partition_key, current_time=current_time, dynamic_partitions_store=dynamic_partitions_store)\n    run_config = run_config if run_config is not None else partitioned_config.get_run_config_for_partition_key(partition_key)\n    run_request_tags = {**(tags or {}), **partitioned_config.get_tags_for_partition_key(partition_key)}\n    return RunRequest(job_name=self.name, run_key=run_key, run_config=run_config, tags=run_request_tags, asset_selection=asset_selection, partition_key=partition_key)",
            "@deprecated(breaking_version='2.0.0', additional_warn_text='Directly instantiate `RunRequest(partition_key=...)` instead.')\ndef run_request_for_partition(self, partition_key: str, run_key: Optional[str]=None, tags: Optional[Mapping[str, str]]=None, asset_selection: Optional[Sequence[AssetKey]]=None, run_config: Optional[Mapping[str, Any]]=None, current_time: Optional[datetime]=None, dynamic_partitions_store: Optional[DynamicPartitionsStore]=None) -> RunRequest:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a RunRequest object for a run that processes the given partition.\\n\\n        Args:\\n            partition_key: The key of the partition to request a run for.\\n            run_key (Optional[str]): A string key to identify this launched run. For sensors, ensures that\\n                only one run is created per run key across all sensor evaluations.  For schedules,\\n                ensures that one run is created per tick, across failure recoveries. Passing in a `None`\\n                value means that a run will always be launched per evaluation.\\n            tags (Optional[Dict[str, str]]): A dictionary of tags (string key-value pairs) to attach\\n                to the launched run.\\n            run_config (Optional[Mapping[str, Any]]: Configuration for the run. If the job has\\n                a :py:class:`PartitionedConfig`, this value will override replace the config\\n                provided by it.\\n            current_time (Optional[datetime]): Used to determine which time-partitions exist.\\n                Defaults to now.\\n            dynamic_partitions_store (Optional[DynamicPartitionsStore]): The DynamicPartitionsStore\\n                object that is responsible for fetching dynamic partitions. Required when the\\n                partitions definition is a DynamicPartitionsDefinition with a name defined. Users\\n                can pass the DagsterInstance fetched via `context.instance` to this argument.\\n\\n        Returns:\\n            RunRequest: an object that requests a run to process the given partition.\\n        '\n    from dagster._core.definitions.partition import DynamicPartitionsDefinition, PartitionedConfig\n    if not self.partitions_def:\n        check.failed('Called run_request_for_partition on a non-partitioned job')\n    partitioned_config = PartitionedConfig.from_flexible_config(self.config, self.partitions_def)\n    if isinstance(self.partitions_def, DynamicPartitionsDefinition) and self.partitions_def.name:\n        check.failed('run_request_for_partition is not supported for dynamic partitions. Instead, use RunRequest(partition_key=...)')\n    self.partitions_def.validate_partition_key(partition_key, current_time=current_time, dynamic_partitions_store=dynamic_partitions_store)\n    run_config = run_config if run_config is not None else partitioned_config.get_run_config_for_partition_key(partition_key)\n    run_request_tags = {**(tags or {}), **partitioned_config.get_tags_for_partition_key(partition_key)}\n    return RunRequest(job_name=self.name, run_key=run_key, run_config=run_config, tags=run_request_tags, asset_selection=asset_selection, partition_key=partition_key)",
            "@deprecated(breaking_version='2.0.0', additional_warn_text='Directly instantiate `RunRequest(partition_key=...)` instead.')\ndef run_request_for_partition(self, partition_key: str, run_key: Optional[str]=None, tags: Optional[Mapping[str, str]]=None, asset_selection: Optional[Sequence[AssetKey]]=None, run_config: Optional[Mapping[str, Any]]=None, current_time: Optional[datetime]=None, dynamic_partitions_store: Optional[DynamicPartitionsStore]=None) -> RunRequest:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a RunRequest object for a run that processes the given partition.\\n\\n        Args:\\n            partition_key: The key of the partition to request a run for.\\n            run_key (Optional[str]): A string key to identify this launched run. For sensors, ensures that\\n                only one run is created per run key across all sensor evaluations.  For schedules,\\n                ensures that one run is created per tick, across failure recoveries. Passing in a `None`\\n                value means that a run will always be launched per evaluation.\\n            tags (Optional[Dict[str, str]]): A dictionary of tags (string key-value pairs) to attach\\n                to the launched run.\\n            run_config (Optional[Mapping[str, Any]]: Configuration for the run. If the job has\\n                a :py:class:`PartitionedConfig`, this value will override replace the config\\n                provided by it.\\n            current_time (Optional[datetime]): Used to determine which time-partitions exist.\\n                Defaults to now.\\n            dynamic_partitions_store (Optional[DynamicPartitionsStore]): The DynamicPartitionsStore\\n                object that is responsible for fetching dynamic partitions. Required when the\\n                partitions definition is a DynamicPartitionsDefinition with a name defined. Users\\n                can pass the DagsterInstance fetched via `context.instance` to this argument.\\n\\n        Returns:\\n            RunRequest: an object that requests a run to process the given partition.\\n        '\n    from dagster._core.definitions.partition import DynamicPartitionsDefinition, PartitionedConfig\n    if not self.partitions_def:\n        check.failed('Called run_request_for_partition on a non-partitioned job')\n    partitioned_config = PartitionedConfig.from_flexible_config(self.config, self.partitions_def)\n    if isinstance(self.partitions_def, DynamicPartitionsDefinition) and self.partitions_def.name:\n        check.failed('run_request_for_partition is not supported for dynamic partitions. Instead, use RunRequest(partition_key=...)')\n    self.partitions_def.validate_partition_key(partition_key, current_time=current_time, dynamic_partitions_store=dynamic_partitions_store)\n    run_config = run_config if run_config is not None else partitioned_config.get_run_config_for_partition_key(partition_key)\n    run_request_tags = {**(tags or {}), **partitioned_config.get_tags_for_partition_key(partition_key)}\n    return RunRequest(job_name=self.name, run_key=run_key, run_config=run_config, tags=run_request_tags, asset_selection=asset_selection, partition_key=partition_key)"
        ]
    },
    {
        "func_name": "resolve",
        "original": "def resolve(self, asset_graph: 'InternalAssetGraph', default_executor_def: Optional['ExecutorDefinition']=None, resource_defs: Optional[Mapping[str, 'ResourceDefinition']]=None) -> 'JobDefinition':\n    \"\"\"Resolve this UnresolvedAssetJobDefinition into a JobDefinition.\"\"\"\n    assets = asset_graph.assets\n    source_assets = asset_graph.source_assets\n    selected_asset_keys = self.selection.resolve(asset_graph)\n    selected_asset_checks = self.selection.resolve_checks(asset_graph)\n    asset_keys_by_partitions_def = defaultdict(set)\n    for asset_key in selected_asset_keys:\n        partitions_def = asset_graph.get_partitions_def(asset_key)\n        if partitions_def is not None:\n            asset_keys_by_partitions_def[partitions_def].add(asset_key)\n    if len(asset_keys_by_partitions_def) > 1:\n        keys_by_partitions_def_str = '\\n'.join((f'{partitions_def}: {asset_keys}' for (partitions_def, asset_keys) in asset_keys_by_partitions_def.items()))\n        raise DagsterInvalidDefinitionError(f\"Multiple partitioned assets exist in assets job '{self.name}'. Selected assets must have the same partitions definitions, but the selected assets have different partitions definitions: \\n{keys_by_partitions_def_str}\")\n    inferred_partitions_def = next(iter(asset_keys_by_partitions_def.keys())) if asset_keys_by_partitions_def else None\n    if inferred_partitions_def and self.partitions_def != inferred_partitions_def and (self.partitions_def is not None):\n        raise DagsterInvalidDefinitionError(f\"Job '{self.name}' received a partitions_def of {self.partitions_def}, but the selected assets {next(iter(asset_keys_by_partitions_def.values()))} have a non-matching partitions_def of {inferred_partitions_def}\")\n    return build_asset_selection_job(name=self.name, assets=assets, asset_checks=asset_graph.asset_checks, config=self.config, source_assets=source_assets, description=self.description, tags=self.tags, metadata=self.metadata, asset_selection=selected_asset_keys, asset_check_selection=selected_asset_checks, partitions_def=self.partitions_def if self.partitions_def else inferred_partitions_def, executor_def=self.executor_def or default_executor_def, hooks=self.hooks, resource_defs=resource_defs)",
        "mutated": [
            "def resolve(self, asset_graph: 'InternalAssetGraph', default_executor_def: Optional['ExecutorDefinition']=None, resource_defs: Optional[Mapping[str, 'ResourceDefinition']]=None) -> 'JobDefinition':\n    if False:\n        i = 10\n    'Resolve this UnresolvedAssetJobDefinition into a JobDefinition.'\n    assets = asset_graph.assets\n    source_assets = asset_graph.source_assets\n    selected_asset_keys = self.selection.resolve(asset_graph)\n    selected_asset_checks = self.selection.resolve_checks(asset_graph)\n    asset_keys_by_partitions_def = defaultdict(set)\n    for asset_key in selected_asset_keys:\n        partitions_def = asset_graph.get_partitions_def(asset_key)\n        if partitions_def is not None:\n            asset_keys_by_partitions_def[partitions_def].add(asset_key)\n    if len(asset_keys_by_partitions_def) > 1:\n        keys_by_partitions_def_str = '\\n'.join((f'{partitions_def}: {asset_keys}' for (partitions_def, asset_keys) in asset_keys_by_partitions_def.items()))\n        raise DagsterInvalidDefinitionError(f\"Multiple partitioned assets exist in assets job '{self.name}'. Selected assets must have the same partitions definitions, but the selected assets have different partitions definitions: \\n{keys_by_partitions_def_str}\")\n    inferred_partitions_def = next(iter(asset_keys_by_partitions_def.keys())) if asset_keys_by_partitions_def else None\n    if inferred_partitions_def and self.partitions_def != inferred_partitions_def and (self.partitions_def is not None):\n        raise DagsterInvalidDefinitionError(f\"Job '{self.name}' received a partitions_def of {self.partitions_def}, but the selected assets {next(iter(asset_keys_by_partitions_def.values()))} have a non-matching partitions_def of {inferred_partitions_def}\")\n    return build_asset_selection_job(name=self.name, assets=assets, asset_checks=asset_graph.asset_checks, config=self.config, source_assets=source_assets, description=self.description, tags=self.tags, metadata=self.metadata, asset_selection=selected_asset_keys, asset_check_selection=selected_asset_checks, partitions_def=self.partitions_def if self.partitions_def else inferred_partitions_def, executor_def=self.executor_def or default_executor_def, hooks=self.hooks, resource_defs=resource_defs)",
            "def resolve(self, asset_graph: 'InternalAssetGraph', default_executor_def: Optional['ExecutorDefinition']=None, resource_defs: Optional[Mapping[str, 'ResourceDefinition']]=None) -> 'JobDefinition':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Resolve this UnresolvedAssetJobDefinition into a JobDefinition.'\n    assets = asset_graph.assets\n    source_assets = asset_graph.source_assets\n    selected_asset_keys = self.selection.resolve(asset_graph)\n    selected_asset_checks = self.selection.resolve_checks(asset_graph)\n    asset_keys_by_partitions_def = defaultdict(set)\n    for asset_key in selected_asset_keys:\n        partitions_def = asset_graph.get_partitions_def(asset_key)\n        if partitions_def is not None:\n            asset_keys_by_partitions_def[partitions_def].add(asset_key)\n    if len(asset_keys_by_partitions_def) > 1:\n        keys_by_partitions_def_str = '\\n'.join((f'{partitions_def}: {asset_keys}' for (partitions_def, asset_keys) in asset_keys_by_partitions_def.items()))\n        raise DagsterInvalidDefinitionError(f\"Multiple partitioned assets exist in assets job '{self.name}'. Selected assets must have the same partitions definitions, but the selected assets have different partitions definitions: \\n{keys_by_partitions_def_str}\")\n    inferred_partitions_def = next(iter(asset_keys_by_partitions_def.keys())) if asset_keys_by_partitions_def else None\n    if inferred_partitions_def and self.partitions_def != inferred_partitions_def and (self.partitions_def is not None):\n        raise DagsterInvalidDefinitionError(f\"Job '{self.name}' received a partitions_def of {self.partitions_def}, but the selected assets {next(iter(asset_keys_by_partitions_def.values()))} have a non-matching partitions_def of {inferred_partitions_def}\")\n    return build_asset_selection_job(name=self.name, assets=assets, asset_checks=asset_graph.asset_checks, config=self.config, source_assets=source_assets, description=self.description, tags=self.tags, metadata=self.metadata, asset_selection=selected_asset_keys, asset_check_selection=selected_asset_checks, partitions_def=self.partitions_def if self.partitions_def else inferred_partitions_def, executor_def=self.executor_def or default_executor_def, hooks=self.hooks, resource_defs=resource_defs)",
            "def resolve(self, asset_graph: 'InternalAssetGraph', default_executor_def: Optional['ExecutorDefinition']=None, resource_defs: Optional[Mapping[str, 'ResourceDefinition']]=None) -> 'JobDefinition':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Resolve this UnresolvedAssetJobDefinition into a JobDefinition.'\n    assets = asset_graph.assets\n    source_assets = asset_graph.source_assets\n    selected_asset_keys = self.selection.resolve(asset_graph)\n    selected_asset_checks = self.selection.resolve_checks(asset_graph)\n    asset_keys_by_partitions_def = defaultdict(set)\n    for asset_key in selected_asset_keys:\n        partitions_def = asset_graph.get_partitions_def(asset_key)\n        if partitions_def is not None:\n            asset_keys_by_partitions_def[partitions_def].add(asset_key)\n    if len(asset_keys_by_partitions_def) > 1:\n        keys_by_partitions_def_str = '\\n'.join((f'{partitions_def}: {asset_keys}' for (partitions_def, asset_keys) in asset_keys_by_partitions_def.items()))\n        raise DagsterInvalidDefinitionError(f\"Multiple partitioned assets exist in assets job '{self.name}'. Selected assets must have the same partitions definitions, but the selected assets have different partitions definitions: \\n{keys_by_partitions_def_str}\")\n    inferred_partitions_def = next(iter(asset_keys_by_partitions_def.keys())) if asset_keys_by_partitions_def else None\n    if inferred_partitions_def and self.partitions_def != inferred_partitions_def and (self.partitions_def is not None):\n        raise DagsterInvalidDefinitionError(f\"Job '{self.name}' received a partitions_def of {self.partitions_def}, but the selected assets {next(iter(asset_keys_by_partitions_def.values()))} have a non-matching partitions_def of {inferred_partitions_def}\")\n    return build_asset_selection_job(name=self.name, assets=assets, asset_checks=asset_graph.asset_checks, config=self.config, source_assets=source_assets, description=self.description, tags=self.tags, metadata=self.metadata, asset_selection=selected_asset_keys, asset_check_selection=selected_asset_checks, partitions_def=self.partitions_def if self.partitions_def else inferred_partitions_def, executor_def=self.executor_def or default_executor_def, hooks=self.hooks, resource_defs=resource_defs)",
            "def resolve(self, asset_graph: 'InternalAssetGraph', default_executor_def: Optional['ExecutorDefinition']=None, resource_defs: Optional[Mapping[str, 'ResourceDefinition']]=None) -> 'JobDefinition':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Resolve this UnresolvedAssetJobDefinition into a JobDefinition.'\n    assets = asset_graph.assets\n    source_assets = asset_graph.source_assets\n    selected_asset_keys = self.selection.resolve(asset_graph)\n    selected_asset_checks = self.selection.resolve_checks(asset_graph)\n    asset_keys_by_partitions_def = defaultdict(set)\n    for asset_key in selected_asset_keys:\n        partitions_def = asset_graph.get_partitions_def(asset_key)\n        if partitions_def is not None:\n            asset_keys_by_partitions_def[partitions_def].add(asset_key)\n    if len(asset_keys_by_partitions_def) > 1:\n        keys_by_partitions_def_str = '\\n'.join((f'{partitions_def}: {asset_keys}' for (partitions_def, asset_keys) in asset_keys_by_partitions_def.items()))\n        raise DagsterInvalidDefinitionError(f\"Multiple partitioned assets exist in assets job '{self.name}'. Selected assets must have the same partitions definitions, but the selected assets have different partitions definitions: \\n{keys_by_partitions_def_str}\")\n    inferred_partitions_def = next(iter(asset_keys_by_partitions_def.keys())) if asset_keys_by_partitions_def else None\n    if inferred_partitions_def and self.partitions_def != inferred_partitions_def and (self.partitions_def is not None):\n        raise DagsterInvalidDefinitionError(f\"Job '{self.name}' received a partitions_def of {self.partitions_def}, but the selected assets {next(iter(asset_keys_by_partitions_def.values()))} have a non-matching partitions_def of {inferred_partitions_def}\")\n    return build_asset_selection_job(name=self.name, assets=assets, asset_checks=asset_graph.asset_checks, config=self.config, source_assets=source_assets, description=self.description, tags=self.tags, metadata=self.metadata, asset_selection=selected_asset_keys, asset_check_selection=selected_asset_checks, partitions_def=self.partitions_def if self.partitions_def else inferred_partitions_def, executor_def=self.executor_def or default_executor_def, hooks=self.hooks, resource_defs=resource_defs)",
            "def resolve(self, asset_graph: 'InternalAssetGraph', default_executor_def: Optional['ExecutorDefinition']=None, resource_defs: Optional[Mapping[str, 'ResourceDefinition']]=None) -> 'JobDefinition':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Resolve this UnresolvedAssetJobDefinition into a JobDefinition.'\n    assets = asset_graph.assets\n    source_assets = asset_graph.source_assets\n    selected_asset_keys = self.selection.resolve(asset_graph)\n    selected_asset_checks = self.selection.resolve_checks(asset_graph)\n    asset_keys_by_partitions_def = defaultdict(set)\n    for asset_key in selected_asset_keys:\n        partitions_def = asset_graph.get_partitions_def(asset_key)\n        if partitions_def is not None:\n            asset_keys_by_partitions_def[partitions_def].add(asset_key)\n    if len(asset_keys_by_partitions_def) > 1:\n        keys_by_partitions_def_str = '\\n'.join((f'{partitions_def}: {asset_keys}' for (partitions_def, asset_keys) in asset_keys_by_partitions_def.items()))\n        raise DagsterInvalidDefinitionError(f\"Multiple partitioned assets exist in assets job '{self.name}'. Selected assets must have the same partitions definitions, but the selected assets have different partitions definitions: \\n{keys_by_partitions_def_str}\")\n    inferred_partitions_def = next(iter(asset_keys_by_partitions_def.keys())) if asset_keys_by_partitions_def else None\n    if inferred_partitions_def and self.partitions_def != inferred_partitions_def and (self.partitions_def is not None):\n        raise DagsterInvalidDefinitionError(f\"Job '{self.name}' received a partitions_def of {self.partitions_def}, but the selected assets {next(iter(asset_keys_by_partitions_def.values()))} have a non-matching partitions_def of {inferred_partitions_def}\")\n    return build_asset_selection_job(name=self.name, assets=assets, asset_checks=asset_graph.asset_checks, config=self.config, source_assets=source_assets, description=self.description, tags=self.tags, metadata=self.metadata, asset_selection=selected_asset_keys, asset_check_selection=selected_asset_checks, partitions_def=self.partitions_def if self.partitions_def else inferred_partitions_def, executor_def=self.executor_def or default_executor_def, hooks=self.hooks, resource_defs=resource_defs)"
        ]
    },
    {
        "func_name": "define_asset_job",
        "original": "def define_asset_job(name: str, selection: Optional['CoercibleToAssetSelection']=None, config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None) -> UnresolvedAssetJobDefinition:\n    \"\"\"Creates a definition of a job which will either materialize a selection of assets or observe\n    a selection of source assets. This will only be resolved to a JobDefinition once placed in a\n    code location.\n\n    Args:\n        name (str):\n            The name for the job.\n        selection (Union[str, Sequence[str], Sequence[AssetKey], Sequence[Union[AssetsDefinition, SourceAsset]], AssetSelection]):\n            The assets that will be materialized or observed when the job is run.\n\n            The selected assets must all be included in the assets that are passed to the assets\n            argument of the Definitions object that this job is included on.\n\n            The string \"my_asset*\" selects my_asset and all downstream assets within the code\n            location. A list of strings represents the union of all assets selected by strings\n            within the list.\n\n            The selection will be resolved to a set of assets when the location is loaded. If the\n            selection resolves to all source assets, the created job will perform source asset\n            observations. If the selection resolves to all regular assets, the created job will\n            materialize assets. If the selection resolves to a mixed set of source assets and\n            regular assets, an error will be thrown.\n\n        config:\n            Describes how the Job is parameterized at runtime.\n\n            If no value is provided, then the schema for the job's run config is a standard\n            format based on its ops and resources.\n\n            If a dictionary is provided, then it must conform to the standard config schema, and\n            it will be used as the job's run config for the job whenever the job is executed.\n            The values provided will be viewable and editable in the Dagster UI, so be\n            careful with secrets.\n\n            If a :py:class:`ConfigMapping` object is provided, then the schema for the job's run config is\n            determined by the config mapping, and the ConfigMapping, which should return\n            configuration in the standard format to configure the job.\n        tags (Optional[Mapping[str, Any]]):\n            Arbitrary information that will be attached to the execution of the Job.\n            Values that are not strings will be json encoded and must meet the criteria that\n            `json.loads(json.dumps(value)) == value`.  These tag values may be overwritten by tag\n            values provided at invocation time.\n        metadata (Optional[Mapping[str, RawMetadataValue]]): Arbitrary metadata about the job.\n            Keys are displayed string labels, and values are one of the following: string, float,\n            int, JSON-serializable dict, JSON-serializable list, and one of the data classes\n            returned by a MetadataValue static method.\n        description (Optional[str]):\n            A description for the Job.\n        partitions_def (Optional[PartitionsDefinition]):\n            Defines the set of partitions for this job. All AssetDefinitions selected for this job\n            must have a matching PartitionsDefinition. If no PartitionsDefinition is provided, the\n            PartitionsDefinition will be inferred from the selected AssetDefinitions.\n        executor_def (Optional[ExecutorDefinition]):\n            How this Job will be executed. Defaults to :py:class:`multi_or_in_process_executor`,\n            which can be switched between multi-process and in-process modes of execution. The\n            default mode of execution is multi-process.\n\n\n    Returns:\n        UnresolvedAssetJobDefinition: The job, which can be placed inside a code location.\n\n    Examples:\n        .. code-block:: python\n\n            # A job that targets all assets in the code location:\n            @asset\n            def asset1():\n                ...\n\n            defs = Definitions(\n                assets=[asset1],\n                jobs=[define_asset_job(\"all_assets\")],\n            )\n\n            # A job that targets a single asset\n            @asset\n            def asset1():\n                ...\n\n            defs = Definitions(\n                assets=[asset1],\n                jobs=[define_asset_job(\"all_assets\", selection=[asset1])],\n            )\n\n            # A job that targets all the assets in a group:\n            defs = Definitions(\n                assets=assets,\n                jobs=[define_asset_job(\"marketing_job\", selection=AssetSelection.groups(\"marketing\"))],\n            )\n\n            @observable_source_asset\n            def source_asset():\n                ...\n\n            # A job that observes a source asset:\n            defs = Definitions(\n                assets=assets,\n                jobs=[define_asset_job(\"observation_job\", selection=[source_asset])],\n            )\n\n            # Resources are supplied to the assets, not the job:\n            @asset(required_resource_keys={\"slack_client\"})\n            def asset1():\n                ...\n\n            defs = Definitions(\n                assets=[asset1],\n                jobs=[define_asset_job(\"all_assets\")],\n                resources={\"slack_client\": prod_slack_client},\n            )\n\n    \"\"\"\n    from dagster._core.definitions import AssetSelection\n    if selection is None:\n        resolved_selection = AssetSelection.all()\n    else:\n        resolved_selection = AssetSelection.from_coercible(selection)\n    return UnresolvedAssetJobDefinition(name=name, selection=resolved_selection, config=config, description=description, tags=tags, metadata=metadata, partitions_def=partitions_def, executor_def=executor_def, hooks=hooks)",
        "mutated": [
            "def define_asset_job(name: str, selection: Optional['CoercibleToAssetSelection']=None, config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None) -> UnresolvedAssetJobDefinition:\n    if False:\n        i = 10\n    'Creates a definition of a job which will either materialize a selection of assets or observe\\n    a selection of source assets. This will only be resolved to a JobDefinition once placed in a\\n    code location.\\n\\n    Args:\\n        name (str):\\n            The name for the job.\\n        selection (Union[str, Sequence[str], Sequence[AssetKey], Sequence[Union[AssetsDefinition, SourceAsset]], AssetSelection]):\\n            The assets that will be materialized or observed when the job is run.\\n\\n            The selected assets must all be included in the assets that are passed to the assets\\n            argument of the Definitions object that this job is included on.\\n\\n            The string \"my_asset*\" selects my_asset and all downstream assets within the code\\n            location. A list of strings represents the union of all assets selected by strings\\n            within the list.\\n\\n            The selection will be resolved to a set of assets when the location is loaded. If the\\n            selection resolves to all source assets, the created job will perform source asset\\n            observations. If the selection resolves to all regular assets, the created job will\\n            materialize assets. If the selection resolves to a mixed set of source assets and\\n            regular assets, an error will be thrown.\\n\\n        config:\\n            Describes how the Job is parameterized at runtime.\\n\\n            If no value is provided, then the schema for the job\\'s run config is a standard\\n            format based on its ops and resources.\\n\\n            If a dictionary is provided, then it must conform to the standard config schema, and\\n            it will be used as the job\\'s run config for the job whenever the job is executed.\\n            The values provided will be viewable and editable in the Dagster UI, so be\\n            careful with secrets.\\n\\n            If a :py:class:`ConfigMapping` object is provided, then the schema for the job\\'s run config is\\n            determined by the config mapping, and the ConfigMapping, which should return\\n            configuration in the standard format to configure the job.\\n        tags (Optional[Mapping[str, Any]]):\\n            Arbitrary information that will be attached to the execution of the Job.\\n            Values that are not strings will be json encoded and must meet the criteria that\\n            `json.loads(json.dumps(value)) == value`.  These tag values may be overwritten by tag\\n            values provided at invocation time.\\n        metadata (Optional[Mapping[str, RawMetadataValue]]): Arbitrary metadata about the job.\\n            Keys are displayed string labels, and values are one of the following: string, float,\\n            int, JSON-serializable dict, JSON-serializable list, and one of the data classes\\n            returned by a MetadataValue static method.\\n        description (Optional[str]):\\n            A description for the Job.\\n        partitions_def (Optional[PartitionsDefinition]):\\n            Defines the set of partitions for this job. All AssetDefinitions selected for this job\\n            must have a matching PartitionsDefinition. If no PartitionsDefinition is provided, the\\n            PartitionsDefinition will be inferred from the selected AssetDefinitions.\\n        executor_def (Optional[ExecutorDefinition]):\\n            How this Job will be executed. Defaults to :py:class:`multi_or_in_process_executor`,\\n            which can be switched between multi-process and in-process modes of execution. The\\n            default mode of execution is multi-process.\\n\\n\\n    Returns:\\n        UnresolvedAssetJobDefinition: The job, which can be placed inside a code location.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            # A job that targets all assets in the code location:\\n            @asset\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\")],\\n            )\\n\\n            # A job that targets a single asset\\n            @asset\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\", selection=[asset1])],\\n            )\\n\\n            # A job that targets all the assets in a group:\\n            defs = Definitions(\\n                assets=assets,\\n                jobs=[define_asset_job(\"marketing_job\", selection=AssetSelection.groups(\"marketing\"))],\\n            )\\n\\n            @observable_source_asset\\n            def source_asset():\\n                ...\\n\\n            # A job that observes a source asset:\\n            defs = Definitions(\\n                assets=assets,\\n                jobs=[define_asset_job(\"observation_job\", selection=[source_asset])],\\n            )\\n\\n            # Resources are supplied to the assets, not the job:\\n            @asset(required_resource_keys={\"slack_client\"})\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\")],\\n                resources={\"slack_client\": prod_slack_client},\\n            )\\n\\n    '\n    from dagster._core.definitions import AssetSelection\n    if selection is None:\n        resolved_selection = AssetSelection.all()\n    else:\n        resolved_selection = AssetSelection.from_coercible(selection)\n    return UnresolvedAssetJobDefinition(name=name, selection=resolved_selection, config=config, description=description, tags=tags, metadata=metadata, partitions_def=partitions_def, executor_def=executor_def, hooks=hooks)",
            "def define_asset_job(name: str, selection: Optional['CoercibleToAssetSelection']=None, config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None) -> UnresolvedAssetJobDefinition:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a definition of a job which will either materialize a selection of assets or observe\\n    a selection of source assets. This will only be resolved to a JobDefinition once placed in a\\n    code location.\\n\\n    Args:\\n        name (str):\\n            The name for the job.\\n        selection (Union[str, Sequence[str], Sequence[AssetKey], Sequence[Union[AssetsDefinition, SourceAsset]], AssetSelection]):\\n            The assets that will be materialized or observed when the job is run.\\n\\n            The selected assets must all be included in the assets that are passed to the assets\\n            argument of the Definitions object that this job is included on.\\n\\n            The string \"my_asset*\" selects my_asset and all downstream assets within the code\\n            location. A list of strings represents the union of all assets selected by strings\\n            within the list.\\n\\n            The selection will be resolved to a set of assets when the location is loaded. If the\\n            selection resolves to all source assets, the created job will perform source asset\\n            observations. If the selection resolves to all regular assets, the created job will\\n            materialize assets. If the selection resolves to a mixed set of source assets and\\n            regular assets, an error will be thrown.\\n\\n        config:\\n            Describes how the Job is parameterized at runtime.\\n\\n            If no value is provided, then the schema for the job\\'s run config is a standard\\n            format based on its ops and resources.\\n\\n            If a dictionary is provided, then it must conform to the standard config schema, and\\n            it will be used as the job\\'s run config for the job whenever the job is executed.\\n            The values provided will be viewable and editable in the Dagster UI, so be\\n            careful with secrets.\\n\\n            If a :py:class:`ConfigMapping` object is provided, then the schema for the job\\'s run config is\\n            determined by the config mapping, and the ConfigMapping, which should return\\n            configuration in the standard format to configure the job.\\n        tags (Optional[Mapping[str, Any]]):\\n            Arbitrary information that will be attached to the execution of the Job.\\n            Values that are not strings will be json encoded and must meet the criteria that\\n            `json.loads(json.dumps(value)) == value`.  These tag values may be overwritten by tag\\n            values provided at invocation time.\\n        metadata (Optional[Mapping[str, RawMetadataValue]]): Arbitrary metadata about the job.\\n            Keys are displayed string labels, and values are one of the following: string, float,\\n            int, JSON-serializable dict, JSON-serializable list, and one of the data classes\\n            returned by a MetadataValue static method.\\n        description (Optional[str]):\\n            A description for the Job.\\n        partitions_def (Optional[PartitionsDefinition]):\\n            Defines the set of partitions for this job. All AssetDefinitions selected for this job\\n            must have a matching PartitionsDefinition. If no PartitionsDefinition is provided, the\\n            PartitionsDefinition will be inferred from the selected AssetDefinitions.\\n        executor_def (Optional[ExecutorDefinition]):\\n            How this Job will be executed. Defaults to :py:class:`multi_or_in_process_executor`,\\n            which can be switched between multi-process and in-process modes of execution. The\\n            default mode of execution is multi-process.\\n\\n\\n    Returns:\\n        UnresolvedAssetJobDefinition: The job, which can be placed inside a code location.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            # A job that targets all assets in the code location:\\n            @asset\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\")],\\n            )\\n\\n            # A job that targets a single asset\\n            @asset\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\", selection=[asset1])],\\n            )\\n\\n            # A job that targets all the assets in a group:\\n            defs = Definitions(\\n                assets=assets,\\n                jobs=[define_asset_job(\"marketing_job\", selection=AssetSelection.groups(\"marketing\"))],\\n            )\\n\\n            @observable_source_asset\\n            def source_asset():\\n                ...\\n\\n            # A job that observes a source asset:\\n            defs = Definitions(\\n                assets=assets,\\n                jobs=[define_asset_job(\"observation_job\", selection=[source_asset])],\\n            )\\n\\n            # Resources are supplied to the assets, not the job:\\n            @asset(required_resource_keys={\"slack_client\"})\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\")],\\n                resources={\"slack_client\": prod_slack_client},\\n            )\\n\\n    '\n    from dagster._core.definitions import AssetSelection\n    if selection is None:\n        resolved_selection = AssetSelection.all()\n    else:\n        resolved_selection = AssetSelection.from_coercible(selection)\n    return UnresolvedAssetJobDefinition(name=name, selection=resolved_selection, config=config, description=description, tags=tags, metadata=metadata, partitions_def=partitions_def, executor_def=executor_def, hooks=hooks)",
            "def define_asset_job(name: str, selection: Optional['CoercibleToAssetSelection']=None, config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None) -> UnresolvedAssetJobDefinition:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a definition of a job which will either materialize a selection of assets or observe\\n    a selection of source assets. This will only be resolved to a JobDefinition once placed in a\\n    code location.\\n\\n    Args:\\n        name (str):\\n            The name for the job.\\n        selection (Union[str, Sequence[str], Sequence[AssetKey], Sequence[Union[AssetsDefinition, SourceAsset]], AssetSelection]):\\n            The assets that will be materialized or observed when the job is run.\\n\\n            The selected assets must all be included in the assets that are passed to the assets\\n            argument of the Definitions object that this job is included on.\\n\\n            The string \"my_asset*\" selects my_asset and all downstream assets within the code\\n            location. A list of strings represents the union of all assets selected by strings\\n            within the list.\\n\\n            The selection will be resolved to a set of assets when the location is loaded. If the\\n            selection resolves to all source assets, the created job will perform source asset\\n            observations. If the selection resolves to all regular assets, the created job will\\n            materialize assets. If the selection resolves to a mixed set of source assets and\\n            regular assets, an error will be thrown.\\n\\n        config:\\n            Describes how the Job is parameterized at runtime.\\n\\n            If no value is provided, then the schema for the job\\'s run config is a standard\\n            format based on its ops and resources.\\n\\n            If a dictionary is provided, then it must conform to the standard config schema, and\\n            it will be used as the job\\'s run config for the job whenever the job is executed.\\n            The values provided will be viewable and editable in the Dagster UI, so be\\n            careful with secrets.\\n\\n            If a :py:class:`ConfigMapping` object is provided, then the schema for the job\\'s run config is\\n            determined by the config mapping, and the ConfigMapping, which should return\\n            configuration in the standard format to configure the job.\\n        tags (Optional[Mapping[str, Any]]):\\n            Arbitrary information that will be attached to the execution of the Job.\\n            Values that are not strings will be json encoded and must meet the criteria that\\n            `json.loads(json.dumps(value)) == value`.  These tag values may be overwritten by tag\\n            values provided at invocation time.\\n        metadata (Optional[Mapping[str, RawMetadataValue]]): Arbitrary metadata about the job.\\n            Keys are displayed string labels, and values are one of the following: string, float,\\n            int, JSON-serializable dict, JSON-serializable list, and one of the data classes\\n            returned by a MetadataValue static method.\\n        description (Optional[str]):\\n            A description for the Job.\\n        partitions_def (Optional[PartitionsDefinition]):\\n            Defines the set of partitions for this job. All AssetDefinitions selected for this job\\n            must have a matching PartitionsDefinition. If no PartitionsDefinition is provided, the\\n            PartitionsDefinition will be inferred from the selected AssetDefinitions.\\n        executor_def (Optional[ExecutorDefinition]):\\n            How this Job will be executed. Defaults to :py:class:`multi_or_in_process_executor`,\\n            which can be switched between multi-process and in-process modes of execution. The\\n            default mode of execution is multi-process.\\n\\n\\n    Returns:\\n        UnresolvedAssetJobDefinition: The job, which can be placed inside a code location.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            # A job that targets all assets in the code location:\\n            @asset\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\")],\\n            )\\n\\n            # A job that targets a single asset\\n            @asset\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\", selection=[asset1])],\\n            )\\n\\n            # A job that targets all the assets in a group:\\n            defs = Definitions(\\n                assets=assets,\\n                jobs=[define_asset_job(\"marketing_job\", selection=AssetSelection.groups(\"marketing\"))],\\n            )\\n\\n            @observable_source_asset\\n            def source_asset():\\n                ...\\n\\n            # A job that observes a source asset:\\n            defs = Definitions(\\n                assets=assets,\\n                jobs=[define_asset_job(\"observation_job\", selection=[source_asset])],\\n            )\\n\\n            # Resources are supplied to the assets, not the job:\\n            @asset(required_resource_keys={\"slack_client\"})\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\")],\\n                resources={\"slack_client\": prod_slack_client},\\n            )\\n\\n    '\n    from dagster._core.definitions import AssetSelection\n    if selection is None:\n        resolved_selection = AssetSelection.all()\n    else:\n        resolved_selection = AssetSelection.from_coercible(selection)\n    return UnresolvedAssetJobDefinition(name=name, selection=resolved_selection, config=config, description=description, tags=tags, metadata=metadata, partitions_def=partitions_def, executor_def=executor_def, hooks=hooks)",
            "def define_asset_job(name: str, selection: Optional['CoercibleToAssetSelection']=None, config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None) -> UnresolvedAssetJobDefinition:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a definition of a job which will either materialize a selection of assets or observe\\n    a selection of source assets. This will only be resolved to a JobDefinition once placed in a\\n    code location.\\n\\n    Args:\\n        name (str):\\n            The name for the job.\\n        selection (Union[str, Sequence[str], Sequence[AssetKey], Sequence[Union[AssetsDefinition, SourceAsset]], AssetSelection]):\\n            The assets that will be materialized or observed when the job is run.\\n\\n            The selected assets must all be included in the assets that are passed to the assets\\n            argument of the Definitions object that this job is included on.\\n\\n            The string \"my_asset*\" selects my_asset and all downstream assets within the code\\n            location. A list of strings represents the union of all assets selected by strings\\n            within the list.\\n\\n            The selection will be resolved to a set of assets when the location is loaded. If the\\n            selection resolves to all source assets, the created job will perform source asset\\n            observations. If the selection resolves to all regular assets, the created job will\\n            materialize assets. If the selection resolves to a mixed set of source assets and\\n            regular assets, an error will be thrown.\\n\\n        config:\\n            Describes how the Job is parameterized at runtime.\\n\\n            If no value is provided, then the schema for the job\\'s run config is a standard\\n            format based on its ops and resources.\\n\\n            If a dictionary is provided, then it must conform to the standard config schema, and\\n            it will be used as the job\\'s run config for the job whenever the job is executed.\\n            The values provided will be viewable and editable in the Dagster UI, so be\\n            careful with secrets.\\n\\n            If a :py:class:`ConfigMapping` object is provided, then the schema for the job\\'s run config is\\n            determined by the config mapping, and the ConfigMapping, which should return\\n            configuration in the standard format to configure the job.\\n        tags (Optional[Mapping[str, Any]]):\\n            Arbitrary information that will be attached to the execution of the Job.\\n            Values that are not strings will be json encoded and must meet the criteria that\\n            `json.loads(json.dumps(value)) == value`.  These tag values may be overwritten by tag\\n            values provided at invocation time.\\n        metadata (Optional[Mapping[str, RawMetadataValue]]): Arbitrary metadata about the job.\\n            Keys are displayed string labels, and values are one of the following: string, float,\\n            int, JSON-serializable dict, JSON-serializable list, and one of the data classes\\n            returned by a MetadataValue static method.\\n        description (Optional[str]):\\n            A description for the Job.\\n        partitions_def (Optional[PartitionsDefinition]):\\n            Defines the set of partitions for this job. All AssetDefinitions selected for this job\\n            must have a matching PartitionsDefinition. If no PartitionsDefinition is provided, the\\n            PartitionsDefinition will be inferred from the selected AssetDefinitions.\\n        executor_def (Optional[ExecutorDefinition]):\\n            How this Job will be executed. Defaults to :py:class:`multi_or_in_process_executor`,\\n            which can be switched between multi-process and in-process modes of execution. The\\n            default mode of execution is multi-process.\\n\\n\\n    Returns:\\n        UnresolvedAssetJobDefinition: The job, which can be placed inside a code location.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            # A job that targets all assets in the code location:\\n            @asset\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\")],\\n            )\\n\\n            # A job that targets a single asset\\n            @asset\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\", selection=[asset1])],\\n            )\\n\\n            # A job that targets all the assets in a group:\\n            defs = Definitions(\\n                assets=assets,\\n                jobs=[define_asset_job(\"marketing_job\", selection=AssetSelection.groups(\"marketing\"))],\\n            )\\n\\n            @observable_source_asset\\n            def source_asset():\\n                ...\\n\\n            # A job that observes a source asset:\\n            defs = Definitions(\\n                assets=assets,\\n                jobs=[define_asset_job(\"observation_job\", selection=[source_asset])],\\n            )\\n\\n            # Resources are supplied to the assets, not the job:\\n            @asset(required_resource_keys={\"slack_client\"})\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\")],\\n                resources={\"slack_client\": prod_slack_client},\\n            )\\n\\n    '\n    from dagster._core.definitions import AssetSelection\n    if selection is None:\n        resolved_selection = AssetSelection.all()\n    else:\n        resolved_selection = AssetSelection.from_coercible(selection)\n    return UnresolvedAssetJobDefinition(name=name, selection=resolved_selection, config=config, description=description, tags=tags, metadata=metadata, partitions_def=partitions_def, executor_def=executor_def, hooks=hooks)",
            "def define_asset_job(name: str, selection: Optional['CoercibleToAssetSelection']=None, config: Optional[Union[ConfigMapping, Mapping[str, Any], 'PartitionedConfig', 'RunConfig']]=None, description: Optional[str]=None, tags: Optional[Mapping[str, Any]]=None, metadata: Optional[Mapping[str, RawMetadataValue]]=None, partitions_def: Optional['PartitionsDefinition']=None, executor_def: Optional['ExecutorDefinition']=None, hooks: Optional[AbstractSet['HookDefinition']]=None) -> UnresolvedAssetJobDefinition:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a definition of a job which will either materialize a selection of assets or observe\\n    a selection of source assets. This will only be resolved to a JobDefinition once placed in a\\n    code location.\\n\\n    Args:\\n        name (str):\\n            The name for the job.\\n        selection (Union[str, Sequence[str], Sequence[AssetKey], Sequence[Union[AssetsDefinition, SourceAsset]], AssetSelection]):\\n            The assets that will be materialized or observed when the job is run.\\n\\n            The selected assets must all be included in the assets that are passed to the assets\\n            argument of the Definitions object that this job is included on.\\n\\n            The string \"my_asset*\" selects my_asset and all downstream assets within the code\\n            location. A list of strings represents the union of all assets selected by strings\\n            within the list.\\n\\n            The selection will be resolved to a set of assets when the location is loaded. If the\\n            selection resolves to all source assets, the created job will perform source asset\\n            observations. If the selection resolves to all regular assets, the created job will\\n            materialize assets. If the selection resolves to a mixed set of source assets and\\n            regular assets, an error will be thrown.\\n\\n        config:\\n            Describes how the Job is parameterized at runtime.\\n\\n            If no value is provided, then the schema for the job\\'s run config is a standard\\n            format based on its ops and resources.\\n\\n            If a dictionary is provided, then it must conform to the standard config schema, and\\n            it will be used as the job\\'s run config for the job whenever the job is executed.\\n            The values provided will be viewable and editable in the Dagster UI, so be\\n            careful with secrets.\\n\\n            If a :py:class:`ConfigMapping` object is provided, then the schema for the job\\'s run config is\\n            determined by the config mapping, and the ConfigMapping, which should return\\n            configuration in the standard format to configure the job.\\n        tags (Optional[Mapping[str, Any]]):\\n            Arbitrary information that will be attached to the execution of the Job.\\n            Values that are not strings will be json encoded and must meet the criteria that\\n            `json.loads(json.dumps(value)) == value`.  These tag values may be overwritten by tag\\n            values provided at invocation time.\\n        metadata (Optional[Mapping[str, RawMetadataValue]]): Arbitrary metadata about the job.\\n            Keys are displayed string labels, and values are one of the following: string, float,\\n            int, JSON-serializable dict, JSON-serializable list, and one of the data classes\\n            returned by a MetadataValue static method.\\n        description (Optional[str]):\\n            A description for the Job.\\n        partitions_def (Optional[PartitionsDefinition]):\\n            Defines the set of partitions for this job. All AssetDefinitions selected for this job\\n            must have a matching PartitionsDefinition. If no PartitionsDefinition is provided, the\\n            PartitionsDefinition will be inferred from the selected AssetDefinitions.\\n        executor_def (Optional[ExecutorDefinition]):\\n            How this Job will be executed. Defaults to :py:class:`multi_or_in_process_executor`,\\n            which can be switched between multi-process and in-process modes of execution. The\\n            default mode of execution is multi-process.\\n\\n\\n    Returns:\\n        UnresolvedAssetJobDefinition: The job, which can be placed inside a code location.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            # A job that targets all assets in the code location:\\n            @asset\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\")],\\n            )\\n\\n            # A job that targets a single asset\\n            @asset\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\", selection=[asset1])],\\n            )\\n\\n            # A job that targets all the assets in a group:\\n            defs = Definitions(\\n                assets=assets,\\n                jobs=[define_asset_job(\"marketing_job\", selection=AssetSelection.groups(\"marketing\"))],\\n            )\\n\\n            @observable_source_asset\\n            def source_asset():\\n                ...\\n\\n            # A job that observes a source asset:\\n            defs = Definitions(\\n                assets=assets,\\n                jobs=[define_asset_job(\"observation_job\", selection=[source_asset])],\\n            )\\n\\n            # Resources are supplied to the assets, not the job:\\n            @asset(required_resource_keys={\"slack_client\"})\\n            def asset1():\\n                ...\\n\\n            defs = Definitions(\\n                assets=[asset1],\\n                jobs=[define_asset_job(\"all_assets\")],\\n                resources={\"slack_client\": prod_slack_client},\\n            )\\n\\n    '\n    from dagster._core.definitions import AssetSelection\n    if selection is None:\n        resolved_selection = AssetSelection.all()\n    else:\n        resolved_selection = AssetSelection.from_coercible(selection)\n    return UnresolvedAssetJobDefinition(name=name, selection=resolved_selection, config=config, description=description, tags=tags, metadata=metadata, partitions_def=partitions_def, executor_def=executor_def, hooks=hooks)"
        ]
    }
]