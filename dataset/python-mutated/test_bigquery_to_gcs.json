[
    {
        "func_name": "test_execute",
        "original": "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute(self, mock_hook):\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, project_id=JOB_PROJECT_ID)\n    operator.execute(context=mock.MagicMock())\n    mock_hook.return_value.insert_job.assert_called_once_with(job_id='123456_hash', configuration=expected_configuration, project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=False)",
        "mutated": [
            "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute(self, mock_hook):\n    if False:\n        i = 10\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, project_id=JOB_PROJECT_ID)\n    operator.execute(context=mock.MagicMock())\n    mock_hook.return_value.insert_job.assert_called_once_with(job_id='123456_hash', configuration=expected_configuration, project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=False)",
            "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute(self, mock_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, project_id=JOB_PROJECT_ID)\n    operator.execute(context=mock.MagicMock())\n    mock_hook.return_value.insert_job.assert_called_once_with(job_id='123456_hash', configuration=expected_configuration, project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=False)",
            "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute(self, mock_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, project_id=JOB_PROJECT_ID)\n    operator.execute(context=mock.MagicMock())\n    mock_hook.return_value.insert_job.assert_called_once_with(job_id='123456_hash', configuration=expected_configuration, project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=False)",
            "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute(self, mock_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, project_id=JOB_PROJECT_ID)\n    operator.execute(context=mock.MagicMock())\n    mock_hook.return_value.insert_job.assert_called_once_with(job_id='123456_hash', configuration=expected_configuration, project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=False)",
            "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute(self, mock_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, project_id=JOB_PROJECT_ID)\n    operator.execute(context=mock.MagicMock())\n    mock_hook.return_value.insert_job.assert_called_once_with(job_id='123456_hash', configuration=expected_configuration, project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=False)"
        ]
    },
    {
        "func_name": "test_execute_deferrable_mode",
        "original": "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute_deferrable_mode(self, mock_hook):\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(project_id=JOB_PROJECT_ID, task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, deferrable=True)\n    with pytest.raises(TaskDeferred) as exc:\n        operator.execute(context=mock.MagicMock())\n    assert isinstance(exc.value.trigger, BigQueryInsertJobTrigger), 'Trigger is not a BigQueryInsertJobTrigger'\n    mock_hook.return_value.insert_job.assert_called_once_with(configuration=expected_configuration, job_id='123456_hash', project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=True)",
        "mutated": [
            "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute_deferrable_mode(self, mock_hook):\n    if False:\n        i = 10\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(project_id=JOB_PROJECT_ID, task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, deferrable=True)\n    with pytest.raises(TaskDeferred) as exc:\n        operator.execute(context=mock.MagicMock())\n    assert isinstance(exc.value.trigger, BigQueryInsertJobTrigger), 'Trigger is not a BigQueryInsertJobTrigger'\n    mock_hook.return_value.insert_job.assert_called_once_with(configuration=expected_configuration, job_id='123456_hash', project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=True)",
            "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute_deferrable_mode(self, mock_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(project_id=JOB_PROJECT_ID, task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, deferrable=True)\n    with pytest.raises(TaskDeferred) as exc:\n        operator.execute(context=mock.MagicMock())\n    assert isinstance(exc.value.trigger, BigQueryInsertJobTrigger), 'Trigger is not a BigQueryInsertJobTrigger'\n    mock_hook.return_value.insert_job.assert_called_once_with(configuration=expected_configuration, job_id='123456_hash', project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=True)",
            "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute_deferrable_mode(self, mock_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(project_id=JOB_PROJECT_ID, task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, deferrable=True)\n    with pytest.raises(TaskDeferred) as exc:\n        operator.execute(context=mock.MagicMock())\n    assert isinstance(exc.value.trigger, BigQueryInsertJobTrigger), 'Trigger is not a BigQueryInsertJobTrigger'\n    mock_hook.return_value.insert_job.assert_called_once_with(configuration=expected_configuration, job_id='123456_hash', project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=True)",
            "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute_deferrable_mode(self, mock_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(project_id=JOB_PROJECT_ID, task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, deferrable=True)\n    with pytest.raises(TaskDeferred) as exc:\n        operator.execute(context=mock.MagicMock())\n    assert isinstance(exc.value.trigger, BigQueryInsertJobTrigger), 'Trigger is not a BigQueryInsertJobTrigger'\n    mock_hook.return_value.insert_job.assert_called_once_with(configuration=expected_configuration, job_id='123456_hash', project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=True)",
            "@mock.patch('airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryHook')\ndef test_execute_deferrable_mode(self, mock_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    source_project_dataset_table = f'{PROJECT_ID}:{TEST_DATASET}.{TEST_TABLE_ID}'\n    destination_cloud_storage_uris = ['gs://some-bucket/some-file.txt']\n    compression = 'NONE'\n    export_format = 'CSV'\n    field_delimiter = ','\n    print_header = True\n    labels = {'k1': 'v1'}\n    job_id = '123456'\n    hash_ = 'hash'\n    real_job_id = f'{job_id}_{hash_}'\n    expected_configuration = {'extract': {'sourceTable': {'projectId': 'test-project-id', 'datasetId': 'test-dataset', 'tableId': 'test-table-id'}, 'compression': 'NONE', 'destinationUris': ['gs://some-bucket/some-file.txt'], 'destinationFormat': 'CSV', 'fieldDelimiter': ',', 'printHeader': True}, 'labels': {'k1': 'v1'}}\n    mock_hook.return_value.split_tablename.return_value = (PROJECT_ID, TEST_DATASET, TEST_TABLE_ID)\n    mock_hook.return_value.generate_job_id.return_value = real_job_id\n    mock_hook.return_value.insert_job.return_value = MagicMock(job_id='real_job_id', error_result=False)\n    mock_hook.return_value.project_id = JOB_PROJECT_ID\n    operator = BigQueryToGCSOperator(project_id=JOB_PROJECT_ID, task_id=TASK_ID, source_project_dataset_table=source_project_dataset_table, destination_cloud_storage_uris=destination_cloud_storage_uris, compression=compression, export_format=export_format, field_delimiter=field_delimiter, print_header=print_header, labels=labels, deferrable=True)\n    with pytest.raises(TaskDeferred) as exc:\n        operator.execute(context=mock.MagicMock())\n    assert isinstance(exc.value.trigger, BigQueryInsertJobTrigger), 'Trigger is not a BigQueryInsertJobTrigger'\n    mock_hook.return_value.insert_job.assert_called_once_with(configuration=expected_configuration, job_id='123456_hash', project_id=JOB_PROJECT_ID, location=None, timeout=None, retry=DEFAULT_RETRY, nowait=True)"
        ]
    }
]