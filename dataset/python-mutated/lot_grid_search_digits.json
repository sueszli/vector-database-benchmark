[
    {
        "func_name": "print_dataframe",
        "original": "def print_dataframe(filtered_cv_results):\n    \"\"\"Pretty print for filtered dataframe\"\"\"\n    for (mean_precision, std_precision, mean_recall, std_recall, params) in zip(filtered_cv_results['mean_test_precision'], filtered_cv_results['std_test_precision'], filtered_cv_results['mean_test_recall'], filtered_cv_results['std_test_recall'], filtered_cv_results['params']):\n        print(f'precision: {mean_precision:0.3f} (\u00b1{std_precision:0.03f}), recall: {mean_recall:0.3f} (\u00b1{std_recall:0.03f}), for {params}')\n    print()",
        "mutated": [
            "def print_dataframe(filtered_cv_results):\n    if False:\n        i = 10\n    'Pretty print for filtered dataframe'\n    for (mean_precision, std_precision, mean_recall, std_recall, params) in zip(filtered_cv_results['mean_test_precision'], filtered_cv_results['std_test_precision'], filtered_cv_results['mean_test_recall'], filtered_cv_results['std_test_recall'], filtered_cv_results['params']):\n        print(f'precision: {mean_precision:0.3f} (\u00b1{std_precision:0.03f}), recall: {mean_recall:0.3f} (\u00b1{std_recall:0.03f}), for {params}')\n    print()",
            "def print_dataframe(filtered_cv_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Pretty print for filtered dataframe'\n    for (mean_precision, std_precision, mean_recall, std_recall, params) in zip(filtered_cv_results['mean_test_precision'], filtered_cv_results['std_test_precision'], filtered_cv_results['mean_test_recall'], filtered_cv_results['std_test_recall'], filtered_cv_results['params']):\n        print(f'precision: {mean_precision:0.3f} (\u00b1{std_precision:0.03f}), recall: {mean_recall:0.3f} (\u00b1{std_recall:0.03f}), for {params}')\n    print()",
            "def print_dataframe(filtered_cv_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Pretty print for filtered dataframe'\n    for (mean_precision, std_precision, mean_recall, std_recall, params) in zip(filtered_cv_results['mean_test_precision'], filtered_cv_results['std_test_precision'], filtered_cv_results['mean_test_recall'], filtered_cv_results['std_test_recall'], filtered_cv_results['params']):\n        print(f'precision: {mean_precision:0.3f} (\u00b1{std_precision:0.03f}), recall: {mean_recall:0.3f} (\u00b1{std_recall:0.03f}), for {params}')\n    print()",
            "def print_dataframe(filtered_cv_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Pretty print for filtered dataframe'\n    for (mean_precision, std_precision, mean_recall, std_recall, params) in zip(filtered_cv_results['mean_test_precision'], filtered_cv_results['std_test_precision'], filtered_cv_results['mean_test_recall'], filtered_cv_results['std_test_recall'], filtered_cv_results['params']):\n        print(f'precision: {mean_precision:0.3f} (\u00b1{std_precision:0.03f}), recall: {mean_recall:0.3f} (\u00b1{std_recall:0.03f}), for {params}')\n    print()",
            "def print_dataframe(filtered_cv_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Pretty print for filtered dataframe'\n    for (mean_precision, std_precision, mean_recall, std_recall, params) in zip(filtered_cv_results['mean_test_precision'], filtered_cv_results['std_test_precision'], filtered_cv_results['mean_test_recall'], filtered_cv_results['std_test_recall'], filtered_cv_results['params']):\n        print(f'precision: {mean_precision:0.3f} (\u00b1{std_precision:0.03f}), recall: {mean_recall:0.3f} (\u00b1{std_recall:0.03f}), for {params}')\n    print()"
        ]
    },
    {
        "func_name": "refit_strategy",
        "original": "def refit_strategy(cv_results):\n    \"\"\"Define the strategy to select the best estimator.\n\n    The strategy defined here is to filter-out all results below a precision threshold\n    of 0.98, rank the remaining by recall and keep all models with one standard\n    deviation of the best by recall. Once these models are selected, we can select the\n    fastest model to predict.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy (masked) ndarrays\n        CV results as returned by the `GridSearchCV`.\n\n    Returns\n    -------\n    best_index : int\n        The index of the best estimator as it appears in `cv_results`.\n    \"\"\"\n    precision_threshold = 0.98\n    cv_results_ = pd.DataFrame(cv_results)\n    print('All grid-search results:')\n    print_dataframe(cv_results_)\n    high_precision_cv_results = cv_results_[cv_results_['mean_test_precision'] > precision_threshold]\n    print(f'Models with a precision higher than {precision_threshold}:')\n    print_dataframe(high_precision_cv_results)\n    high_precision_cv_results = high_precision_cv_results[['mean_score_time', 'mean_test_recall', 'std_test_recall', 'mean_test_precision', 'std_test_precision', 'rank_test_recall', 'rank_test_precision', 'params']]\n    best_recall_std = high_precision_cv_results['mean_test_recall'].std()\n    best_recall = high_precision_cv_results['mean_test_recall'].max()\n    best_recall_threshold = best_recall - best_recall_std\n    high_recall_cv_results = high_precision_cv_results[high_precision_cv_results['mean_test_recall'] > best_recall_threshold]\n    print('Out of the previously selected high precision models, we keep all the\\nthe models within one standard deviation of the highest recall model:')\n    print_dataframe(high_recall_cv_results)\n    fastest_top_recall_high_precision_index = high_recall_cv_results['mean_score_time'].idxmin()\n    print(f'\\nThe selected final model is the fastest to predict out of the previously\\nselected subset of best models based on precision and recall.\\nIts scoring time is:\\n\\n{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}')\n    return fastest_top_recall_high_precision_index",
        "mutated": [
            "def refit_strategy(cv_results):\n    if False:\n        i = 10\n    'Define the strategy to select the best estimator.\\n\\n    The strategy defined here is to filter-out all results below a precision threshold\\n    of 0.98, rank the remaining by recall and keep all models with one standard\\n    deviation of the best by recall. Once these models are selected, we can select the\\n    fastest model to predict.\\n\\n    Parameters\\n    ----------\\n    cv_results : dict of numpy (masked) ndarrays\\n        CV results as returned by the `GridSearchCV`.\\n\\n    Returns\\n    -------\\n    best_index : int\\n        The index of the best estimator as it appears in `cv_results`.\\n    '\n    precision_threshold = 0.98\n    cv_results_ = pd.DataFrame(cv_results)\n    print('All grid-search results:')\n    print_dataframe(cv_results_)\n    high_precision_cv_results = cv_results_[cv_results_['mean_test_precision'] > precision_threshold]\n    print(f'Models with a precision higher than {precision_threshold}:')\n    print_dataframe(high_precision_cv_results)\n    high_precision_cv_results = high_precision_cv_results[['mean_score_time', 'mean_test_recall', 'std_test_recall', 'mean_test_precision', 'std_test_precision', 'rank_test_recall', 'rank_test_precision', 'params']]\n    best_recall_std = high_precision_cv_results['mean_test_recall'].std()\n    best_recall = high_precision_cv_results['mean_test_recall'].max()\n    best_recall_threshold = best_recall - best_recall_std\n    high_recall_cv_results = high_precision_cv_results[high_precision_cv_results['mean_test_recall'] > best_recall_threshold]\n    print('Out of the previously selected high precision models, we keep all the\\nthe models within one standard deviation of the highest recall model:')\n    print_dataframe(high_recall_cv_results)\n    fastest_top_recall_high_precision_index = high_recall_cv_results['mean_score_time'].idxmin()\n    print(f'\\nThe selected final model is the fastest to predict out of the previously\\nselected subset of best models based on precision and recall.\\nIts scoring time is:\\n\\n{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}')\n    return fastest_top_recall_high_precision_index",
            "def refit_strategy(cv_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Define the strategy to select the best estimator.\\n\\n    The strategy defined here is to filter-out all results below a precision threshold\\n    of 0.98, rank the remaining by recall and keep all models with one standard\\n    deviation of the best by recall. Once these models are selected, we can select the\\n    fastest model to predict.\\n\\n    Parameters\\n    ----------\\n    cv_results : dict of numpy (masked) ndarrays\\n        CV results as returned by the `GridSearchCV`.\\n\\n    Returns\\n    -------\\n    best_index : int\\n        The index of the best estimator as it appears in `cv_results`.\\n    '\n    precision_threshold = 0.98\n    cv_results_ = pd.DataFrame(cv_results)\n    print('All grid-search results:')\n    print_dataframe(cv_results_)\n    high_precision_cv_results = cv_results_[cv_results_['mean_test_precision'] > precision_threshold]\n    print(f'Models with a precision higher than {precision_threshold}:')\n    print_dataframe(high_precision_cv_results)\n    high_precision_cv_results = high_precision_cv_results[['mean_score_time', 'mean_test_recall', 'std_test_recall', 'mean_test_precision', 'std_test_precision', 'rank_test_recall', 'rank_test_precision', 'params']]\n    best_recall_std = high_precision_cv_results['mean_test_recall'].std()\n    best_recall = high_precision_cv_results['mean_test_recall'].max()\n    best_recall_threshold = best_recall - best_recall_std\n    high_recall_cv_results = high_precision_cv_results[high_precision_cv_results['mean_test_recall'] > best_recall_threshold]\n    print('Out of the previously selected high precision models, we keep all the\\nthe models within one standard deviation of the highest recall model:')\n    print_dataframe(high_recall_cv_results)\n    fastest_top_recall_high_precision_index = high_recall_cv_results['mean_score_time'].idxmin()\n    print(f'\\nThe selected final model is the fastest to predict out of the previously\\nselected subset of best models based on precision and recall.\\nIts scoring time is:\\n\\n{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}')\n    return fastest_top_recall_high_precision_index",
            "def refit_strategy(cv_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Define the strategy to select the best estimator.\\n\\n    The strategy defined here is to filter-out all results below a precision threshold\\n    of 0.98, rank the remaining by recall and keep all models with one standard\\n    deviation of the best by recall. Once these models are selected, we can select the\\n    fastest model to predict.\\n\\n    Parameters\\n    ----------\\n    cv_results : dict of numpy (masked) ndarrays\\n        CV results as returned by the `GridSearchCV`.\\n\\n    Returns\\n    -------\\n    best_index : int\\n        The index of the best estimator as it appears in `cv_results`.\\n    '\n    precision_threshold = 0.98\n    cv_results_ = pd.DataFrame(cv_results)\n    print('All grid-search results:')\n    print_dataframe(cv_results_)\n    high_precision_cv_results = cv_results_[cv_results_['mean_test_precision'] > precision_threshold]\n    print(f'Models with a precision higher than {precision_threshold}:')\n    print_dataframe(high_precision_cv_results)\n    high_precision_cv_results = high_precision_cv_results[['mean_score_time', 'mean_test_recall', 'std_test_recall', 'mean_test_precision', 'std_test_precision', 'rank_test_recall', 'rank_test_precision', 'params']]\n    best_recall_std = high_precision_cv_results['mean_test_recall'].std()\n    best_recall = high_precision_cv_results['mean_test_recall'].max()\n    best_recall_threshold = best_recall - best_recall_std\n    high_recall_cv_results = high_precision_cv_results[high_precision_cv_results['mean_test_recall'] > best_recall_threshold]\n    print('Out of the previously selected high precision models, we keep all the\\nthe models within one standard deviation of the highest recall model:')\n    print_dataframe(high_recall_cv_results)\n    fastest_top_recall_high_precision_index = high_recall_cv_results['mean_score_time'].idxmin()\n    print(f'\\nThe selected final model is the fastest to predict out of the previously\\nselected subset of best models based on precision and recall.\\nIts scoring time is:\\n\\n{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}')\n    return fastest_top_recall_high_precision_index",
            "def refit_strategy(cv_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Define the strategy to select the best estimator.\\n\\n    The strategy defined here is to filter-out all results below a precision threshold\\n    of 0.98, rank the remaining by recall and keep all models with one standard\\n    deviation of the best by recall. Once these models are selected, we can select the\\n    fastest model to predict.\\n\\n    Parameters\\n    ----------\\n    cv_results : dict of numpy (masked) ndarrays\\n        CV results as returned by the `GridSearchCV`.\\n\\n    Returns\\n    -------\\n    best_index : int\\n        The index of the best estimator as it appears in `cv_results`.\\n    '\n    precision_threshold = 0.98\n    cv_results_ = pd.DataFrame(cv_results)\n    print('All grid-search results:')\n    print_dataframe(cv_results_)\n    high_precision_cv_results = cv_results_[cv_results_['mean_test_precision'] > precision_threshold]\n    print(f'Models with a precision higher than {precision_threshold}:')\n    print_dataframe(high_precision_cv_results)\n    high_precision_cv_results = high_precision_cv_results[['mean_score_time', 'mean_test_recall', 'std_test_recall', 'mean_test_precision', 'std_test_precision', 'rank_test_recall', 'rank_test_precision', 'params']]\n    best_recall_std = high_precision_cv_results['mean_test_recall'].std()\n    best_recall = high_precision_cv_results['mean_test_recall'].max()\n    best_recall_threshold = best_recall - best_recall_std\n    high_recall_cv_results = high_precision_cv_results[high_precision_cv_results['mean_test_recall'] > best_recall_threshold]\n    print('Out of the previously selected high precision models, we keep all the\\nthe models within one standard deviation of the highest recall model:')\n    print_dataframe(high_recall_cv_results)\n    fastest_top_recall_high_precision_index = high_recall_cv_results['mean_score_time'].idxmin()\n    print(f'\\nThe selected final model is the fastest to predict out of the previously\\nselected subset of best models based on precision and recall.\\nIts scoring time is:\\n\\n{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}')\n    return fastest_top_recall_high_precision_index",
            "def refit_strategy(cv_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Define the strategy to select the best estimator.\\n\\n    The strategy defined here is to filter-out all results below a precision threshold\\n    of 0.98, rank the remaining by recall and keep all models with one standard\\n    deviation of the best by recall. Once these models are selected, we can select the\\n    fastest model to predict.\\n\\n    Parameters\\n    ----------\\n    cv_results : dict of numpy (masked) ndarrays\\n        CV results as returned by the `GridSearchCV`.\\n\\n    Returns\\n    -------\\n    best_index : int\\n        The index of the best estimator as it appears in `cv_results`.\\n    '\n    precision_threshold = 0.98\n    cv_results_ = pd.DataFrame(cv_results)\n    print('All grid-search results:')\n    print_dataframe(cv_results_)\n    high_precision_cv_results = cv_results_[cv_results_['mean_test_precision'] > precision_threshold]\n    print(f'Models with a precision higher than {precision_threshold}:')\n    print_dataframe(high_precision_cv_results)\n    high_precision_cv_results = high_precision_cv_results[['mean_score_time', 'mean_test_recall', 'std_test_recall', 'mean_test_precision', 'std_test_precision', 'rank_test_recall', 'rank_test_precision', 'params']]\n    best_recall_std = high_precision_cv_results['mean_test_recall'].std()\n    best_recall = high_precision_cv_results['mean_test_recall'].max()\n    best_recall_threshold = best_recall - best_recall_std\n    high_recall_cv_results = high_precision_cv_results[high_precision_cv_results['mean_test_recall'] > best_recall_threshold]\n    print('Out of the previously selected high precision models, we keep all the\\nthe models within one standard deviation of the highest recall model:')\n    print_dataframe(high_recall_cv_results)\n    fastest_top_recall_high_precision_index = high_recall_cv_results['mean_score_time'].idxmin()\n    print(f'\\nThe selected final model is the fastest to predict out of the previously\\nselected subset of best models based on precision and recall.\\nIts scoring time is:\\n\\n{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}')\n    return fastest_top_recall_high_precision_index"
        ]
    }
]