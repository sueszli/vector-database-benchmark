[
    {
        "func_name": "__init__",
        "original": "def __init__(self, combine_method, tgt_dict, src_dict=None, beam_size=1, max_len_a=0, max_len_b=200, min_len=1, len_penalty=1.0, unk_penalty=0.0, retain_dropout=False, temperature=1.0, match_source_len=False, no_repeat_ngram_size=0, normalize_scores=True, channel_models=None, k2=10, ch_weight=1.0, channel_scoring_type='log_norm', top_k_vocab=0, lm_models=None, lm_dict=None, lm_weight=1.0, normalize_lm_scores_by_tgt_len=False):\n    \"\"\"Generates translations of a given source sentence,\n           using beam search with noisy channel decoding.\n\n        Args:\n            combine_method (string, optional): Method to combine direct, LM and\n                channel model scores (default: None)\n            tgt_dict (~fairseq.data.Dictionary): target dictionary\n            src_dict (~fairseq.data.Dictionary): source dictionary\n            beam_size (int, optional): beam width (default: 1)\n            max_len_a/b (int, optional): generate sequences of maximum length\n                ax + b, where x is the source length\n            min_len (int, optional): the minimum length of the generated output\n                (not including end-of-sentence)\n            len_penalty (float, optional): length penalty, where <1.0 favors\n                shorter, >1.0 favors longer sentences (default: 1.0)\n            unk_penalty (float, optional): unknown word penalty, where <0\n                produces more unks, >0 produces fewer (default: 0.0)\n            retain_dropout (bool, optional): use dropout when generating\n                (default: False)\n            temperature (float, optional): temperature, where values\n                >1.0 produce more uniform samples and values <1.0 produce\n                sharper samples (default: 1.0)\n            match_source_len (bool, optional): outputs should match the source\n                length (default: False)\n            no_repeat_ngram_size (int, optional): Size of n-grams that we avoid\n                repeating in the generation (default: 0)\n            normalize_scores (bool, optional): normalize scores by the length\n                of the output (default: True)\n            channel_models (List[~fairseq.models.FairseqModel]): ensemble of models\n                translating from the target to the source\n            k2 (int, optional): Top K2 candidates to score per beam at each step (default:10)\n            ch_weight (int, optional): Weight associated with the channel model score\n                assuming that the direct model score has weight 1.0 (default: 1.0)\n            channel_scoring_type (str, optional): String specifying how to score\n                the channel model (default: 'log_norm')\n            top_k_vocab (int, optional): If `channel_scoring_type` is `'src_vocab'` or\n                `'src_vocab_batched'`, then this parameter specifies the number of\n                most frequent tokens to include in the channel model output vocabulary,\n                in addition to the source tokens in the input batch (default: 0)\n            lm_models (List[~fairseq.models.FairseqModel]): ensemble of models\n                generating text in the target language\n            lm_dict (~fairseq.data.Dictionary): LM Model dictionary\n            lm_weight (int, optional): Weight associated with the LM model score\n                assuming that the direct model score has weight 1.0 (default: 1.0)\n            normalize_lm_scores_by_tgt_len (bool, optional): Should we normalize LM scores\n                by the target length? By default, we normalize the combination of\n                LM and channel model scores by the source length\n        \"\"\"\n    self.pad = tgt_dict.pad()\n    self.unk = tgt_dict.unk()\n    self.eos = tgt_dict.eos()\n    self.vocab_size = len(tgt_dict)\n    self.beam_size = beam_size\n    self.beam_size = min(beam_size, self.vocab_size - 1)\n    self.max_len_a = max_len_a\n    self.max_len_b = max_len_b\n    self.min_len = min_len\n    self.normalize_scores = normalize_scores\n    self.len_penalty = len_penalty\n    self.unk_penalty = unk_penalty\n    self.retain_dropout = retain_dropout\n    self.temperature = temperature\n    self.match_source_len = match_source_len\n    self.no_repeat_ngram_size = no_repeat_ngram_size\n    self.channel_models = channel_models\n    self.src_dict = src_dict\n    self.tgt_dict = tgt_dict\n    self.combine_method = combine_method\n    self.k2 = k2\n    self.ch_weight = ch_weight\n    self.channel_scoring_type = channel_scoring_type\n    self.top_k_vocab = top_k_vocab\n    self.lm_models = lm_models\n    self.lm_dict = lm_dict\n    self.lm_weight = lm_weight\n    self.log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n    self.normalize_lm_scores_by_tgt_len = normalize_lm_scores_by_tgt_len\n    self.share_tgt_dict = self.lm_dict == self.tgt_dict\n    self.tgt_to_lm = make_dict2dict(tgt_dict, lm_dict)\n    self.ch_scoring_bsz = 3072\n    assert temperature > 0, '--temperature must be greater than 0'\n    self.search = NoisyChannelBeamSearch(tgt_dict)",
        "mutated": [
            "def __init__(self, combine_method, tgt_dict, src_dict=None, beam_size=1, max_len_a=0, max_len_b=200, min_len=1, len_penalty=1.0, unk_penalty=0.0, retain_dropout=False, temperature=1.0, match_source_len=False, no_repeat_ngram_size=0, normalize_scores=True, channel_models=None, k2=10, ch_weight=1.0, channel_scoring_type='log_norm', top_k_vocab=0, lm_models=None, lm_dict=None, lm_weight=1.0, normalize_lm_scores_by_tgt_len=False):\n    if False:\n        i = 10\n    \"Generates translations of a given source sentence,\\n           using beam search with noisy channel decoding.\\n\\n        Args:\\n            combine_method (string, optional): Method to combine direct, LM and\\n                channel model scores (default: None)\\n            tgt_dict (~fairseq.data.Dictionary): target dictionary\\n            src_dict (~fairseq.data.Dictionary): source dictionary\\n            beam_size (int, optional): beam width (default: 1)\\n            max_len_a/b (int, optional): generate sequences of maximum length\\n                ax + b, where x is the source length\\n            min_len (int, optional): the minimum length of the generated output\\n                (not including end-of-sentence)\\n            len_penalty (float, optional): length penalty, where <1.0 favors\\n                shorter, >1.0 favors longer sentences (default: 1.0)\\n            unk_penalty (float, optional): unknown word penalty, where <0\\n                produces more unks, >0 produces fewer (default: 0.0)\\n            retain_dropout (bool, optional): use dropout when generating\\n                (default: False)\\n            temperature (float, optional): temperature, where values\\n                >1.0 produce more uniform samples and values <1.0 produce\\n                sharper samples (default: 1.0)\\n            match_source_len (bool, optional): outputs should match the source\\n                length (default: False)\\n            no_repeat_ngram_size (int, optional): Size of n-grams that we avoid\\n                repeating in the generation (default: 0)\\n            normalize_scores (bool, optional): normalize scores by the length\\n                of the output (default: True)\\n            channel_models (List[~fairseq.models.FairseqModel]): ensemble of models\\n                translating from the target to the source\\n            k2 (int, optional): Top K2 candidates to score per beam at each step (default:10)\\n            ch_weight (int, optional): Weight associated with the channel model score\\n                assuming that the direct model score has weight 1.0 (default: 1.0)\\n            channel_scoring_type (str, optional): String specifying how to score\\n                the channel model (default: 'log_norm')\\n            top_k_vocab (int, optional): If `channel_scoring_type` is `'src_vocab'` or\\n                `'src_vocab_batched'`, then this parameter specifies the number of\\n                most frequent tokens to include in the channel model output vocabulary,\\n                in addition to the source tokens in the input batch (default: 0)\\n            lm_models (List[~fairseq.models.FairseqModel]): ensemble of models\\n                generating text in the target language\\n            lm_dict (~fairseq.data.Dictionary): LM Model dictionary\\n            lm_weight (int, optional): Weight associated with the LM model score\\n                assuming that the direct model score has weight 1.0 (default: 1.0)\\n            normalize_lm_scores_by_tgt_len (bool, optional): Should we normalize LM scores\\n                by the target length? By default, we normalize the combination of\\n                LM and channel model scores by the source length\\n        \"\n    self.pad = tgt_dict.pad()\n    self.unk = tgt_dict.unk()\n    self.eos = tgt_dict.eos()\n    self.vocab_size = len(tgt_dict)\n    self.beam_size = beam_size\n    self.beam_size = min(beam_size, self.vocab_size - 1)\n    self.max_len_a = max_len_a\n    self.max_len_b = max_len_b\n    self.min_len = min_len\n    self.normalize_scores = normalize_scores\n    self.len_penalty = len_penalty\n    self.unk_penalty = unk_penalty\n    self.retain_dropout = retain_dropout\n    self.temperature = temperature\n    self.match_source_len = match_source_len\n    self.no_repeat_ngram_size = no_repeat_ngram_size\n    self.channel_models = channel_models\n    self.src_dict = src_dict\n    self.tgt_dict = tgt_dict\n    self.combine_method = combine_method\n    self.k2 = k2\n    self.ch_weight = ch_weight\n    self.channel_scoring_type = channel_scoring_type\n    self.top_k_vocab = top_k_vocab\n    self.lm_models = lm_models\n    self.lm_dict = lm_dict\n    self.lm_weight = lm_weight\n    self.log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n    self.normalize_lm_scores_by_tgt_len = normalize_lm_scores_by_tgt_len\n    self.share_tgt_dict = self.lm_dict == self.tgt_dict\n    self.tgt_to_lm = make_dict2dict(tgt_dict, lm_dict)\n    self.ch_scoring_bsz = 3072\n    assert temperature > 0, '--temperature must be greater than 0'\n    self.search = NoisyChannelBeamSearch(tgt_dict)",
            "def __init__(self, combine_method, tgt_dict, src_dict=None, beam_size=1, max_len_a=0, max_len_b=200, min_len=1, len_penalty=1.0, unk_penalty=0.0, retain_dropout=False, temperature=1.0, match_source_len=False, no_repeat_ngram_size=0, normalize_scores=True, channel_models=None, k2=10, ch_weight=1.0, channel_scoring_type='log_norm', top_k_vocab=0, lm_models=None, lm_dict=None, lm_weight=1.0, normalize_lm_scores_by_tgt_len=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Generates translations of a given source sentence,\\n           using beam search with noisy channel decoding.\\n\\n        Args:\\n            combine_method (string, optional): Method to combine direct, LM and\\n                channel model scores (default: None)\\n            tgt_dict (~fairseq.data.Dictionary): target dictionary\\n            src_dict (~fairseq.data.Dictionary): source dictionary\\n            beam_size (int, optional): beam width (default: 1)\\n            max_len_a/b (int, optional): generate sequences of maximum length\\n                ax + b, where x is the source length\\n            min_len (int, optional): the minimum length of the generated output\\n                (not including end-of-sentence)\\n            len_penalty (float, optional): length penalty, where <1.0 favors\\n                shorter, >1.0 favors longer sentences (default: 1.0)\\n            unk_penalty (float, optional): unknown word penalty, where <0\\n                produces more unks, >0 produces fewer (default: 0.0)\\n            retain_dropout (bool, optional): use dropout when generating\\n                (default: False)\\n            temperature (float, optional): temperature, where values\\n                >1.0 produce more uniform samples and values <1.0 produce\\n                sharper samples (default: 1.0)\\n            match_source_len (bool, optional): outputs should match the source\\n                length (default: False)\\n            no_repeat_ngram_size (int, optional): Size of n-grams that we avoid\\n                repeating in the generation (default: 0)\\n            normalize_scores (bool, optional): normalize scores by the length\\n                of the output (default: True)\\n            channel_models (List[~fairseq.models.FairseqModel]): ensemble of models\\n                translating from the target to the source\\n            k2 (int, optional): Top K2 candidates to score per beam at each step (default:10)\\n            ch_weight (int, optional): Weight associated with the channel model score\\n                assuming that the direct model score has weight 1.0 (default: 1.0)\\n            channel_scoring_type (str, optional): String specifying how to score\\n                the channel model (default: 'log_norm')\\n            top_k_vocab (int, optional): If `channel_scoring_type` is `'src_vocab'` or\\n                `'src_vocab_batched'`, then this parameter specifies the number of\\n                most frequent tokens to include in the channel model output vocabulary,\\n                in addition to the source tokens in the input batch (default: 0)\\n            lm_models (List[~fairseq.models.FairseqModel]): ensemble of models\\n                generating text in the target language\\n            lm_dict (~fairseq.data.Dictionary): LM Model dictionary\\n            lm_weight (int, optional): Weight associated with the LM model score\\n                assuming that the direct model score has weight 1.0 (default: 1.0)\\n            normalize_lm_scores_by_tgt_len (bool, optional): Should we normalize LM scores\\n                by the target length? By default, we normalize the combination of\\n                LM and channel model scores by the source length\\n        \"\n    self.pad = tgt_dict.pad()\n    self.unk = tgt_dict.unk()\n    self.eos = tgt_dict.eos()\n    self.vocab_size = len(tgt_dict)\n    self.beam_size = beam_size\n    self.beam_size = min(beam_size, self.vocab_size - 1)\n    self.max_len_a = max_len_a\n    self.max_len_b = max_len_b\n    self.min_len = min_len\n    self.normalize_scores = normalize_scores\n    self.len_penalty = len_penalty\n    self.unk_penalty = unk_penalty\n    self.retain_dropout = retain_dropout\n    self.temperature = temperature\n    self.match_source_len = match_source_len\n    self.no_repeat_ngram_size = no_repeat_ngram_size\n    self.channel_models = channel_models\n    self.src_dict = src_dict\n    self.tgt_dict = tgt_dict\n    self.combine_method = combine_method\n    self.k2 = k2\n    self.ch_weight = ch_weight\n    self.channel_scoring_type = channel_scoring_type\n    self.top_k_vocab = top_k_vocab\n    self.lm_models = lm_models\n    self.lm_dict = lm_dict\n    self.lm_weight = lm_weight\n    self.log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n    self.normalize_lm_scores_by_tgt_len = normalize_lm_scores_by_tgt_len\n    self.share_tgt_dict = self.lm_dict == self.tgt_dict\n    self.tgt_to_lm = make_dict2dict(tgt_dict, lm_dict)\n    self.ch_scoring_bsz = 3072\n    assert temperature > 0, '--temperature must be greater than 0'\n    self.search = NoisyChannelBeamSearch(tgt_dict)",
            "def __init__(self, combine_method, tgt_dict, src_dict=None, beam_size=1, max_len_a=0, max_len_b=200, min_len=1, len_penalty=1.0, unk_penalty=0.0, retain_dropout=False, temperature=1.0, match_source_len=False, no_repeat_ngram_size=0, normalize_scores=True, channel_models=None, k2=10, ch_weight=1.0, channel_scoring_type='log_norm', top_k_vocab=0, lm_models=None, lm_dict=None, lm_weight=1.0, normalize_lm_scores_by_tgt_len=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Generates translations of a given source sentence,\\n           using beam search with noisy channel decoding.\\n\\n        Args:\\n            combine_method (string, optional): Method to combine direct, LM and\\n                channel model scores (default: None)\\n            tgt_dict (~fairseq.data.Dictionary): target dictionary\\n            src_dict (~fairseq.data.Dictionary): source dictionary\\n            beam_size (int, optional): beam width (default: 1)\\n            max_len_a/b (int, optional): generate sequences of maximum length\\n                ax + b, where x is the source length\\n            min_len (int, optional): the minimum length of the generated output\\n                (not including end-of-sentence)\\n            len_penalty (float, optional): length penalty, where <1.0 favors\\n                shorter, >1.0 favors longer sentences (default: 1.0)\\n            unk_penalty (float, optional): unknown word penalty, where <0\\n                produces more unks, >0 produces fewer (default: 0.0)\\n            retain_dropout (bool, optional): use dropout when generating\\n                (default: False)\\n            temperature (float, optional): temperature, where values\\n                >1.0 produce more uniform samples and values <1.0 produce\\n                sharper samples (default: 1.0)\\n            match_source_len (bool, optional): outputs should match the source\\n                length (default: False)\\n            no_repeat_ngram_size (int, optional): Size of n-grams that we avoid\\n                repeating in the generation (default: 0)\\n            normalize_scores (bool, optional): normalize scores by the length\\n                of the output (default: True)\\n            channel_models (List[~fairseq.models.FairseqModel]): ensemble of models\\n                translating from the target to the source\\n            k2 (int, optional): Top K2 candidates to score per beam at each step (default:10)\\n            ch_weight (int, optional): Weight associated with the channel model score\\n                assuming that the direct model score has weight 1.0 (default: 1.0)\\n            channel_scoring_type (str, optional): String specifying how to score\\n                the channel model (default: 'log_norm')\\n            top_k_vocab (int, optional): If `channel_scoring_type` is `'src_vocab'` or\\n                `'src_vocab_batched'`, then this parameter specifies the number of\\n                most frequent tokens to include in the channel model output vocabulary,\\n                in addition to the source tokens in the input batch (default: 0)\\n            lm_models (List[~fairseq.models.FairseqModel]): ensemble of models\\n                generating text in the target language\\n            lm_dict (~fairseq.data.Dictionary): LM Model dictionary\\n            lm_weight (int, optional): Weight associated with the LM model score\\n                assuming that the direct model score has weight 1.0 (default: 1.0)\\n            normalize_lm_scores_by_tgt_len (bool, optional): Should we normalize LM scores\\n                by the target length? By default, we normalize the combination of\\n                LM and channel model scores by the source length\\n        \"\n    self.pad = tgt_dict.pad()\n    self.unk = tgt_dict.unk()\n    self.eos = tgt_dict.eos()\n    self.vocab_size = len(tgt_dict)\n    self.beam_size = beam_size\n    self.beam_size = min(beam_size, self.vocab_size - 1)\n    self.max_len_a = max_len_a\n    self.max_len_b = max_len_b\n    self.min_len = min_len\n    self.normalize_scores = normalize_scores\n    self.len_penalty = len_penalty\n    self.unk_penalty = unk_penalty\n    self.retain_dropout = retain_dropout\n    self.temperature = temperature\n    self.match_source_len = match_source_len\n    self.no_repeat_ngram_size = no_repeat_ngram_size\n    self.channel_models = channel_models\n    self.src_dict = src_dict\n    self.tgt_dict = tgt_dict\n    self.combine_method = combine_method\n    self.k2 = k2\n    self.ch_weight = ch_weight\n    self.channel_scoring_type = channel_scoring_type\n    self.top_k_vocab = top_k_vocab\n    self.lm_models = lm_models\n    self.lm_dict = lm_dict\n    self.lm_weight = lm_weight\n    self.log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n    self.normalize_lm_scores_by_tgt_len = normalize_lm_scores_by_tgt_len\n    self.share_tgt_dict = self.lm_dict == self.tgt_dict\n    self.tgt_to_lm = make_dict2dict(tgt_dict, lm_dict)\n    self.ch_scoring_bsz = 3072\n    assert temperature > 0, '--temperature must be greater than 0'\n    self.search = NoisyChannelBeamSearch(tgt_dict)",
            "def __init__(self, combine_method, tgt_dict, src_dict=None, beam_size=1, max_len_a=0, max_len_b=200, min_len=1, len_penalty=1.0, unk_penalty=0.0, retain_dropout=False, temperature=1.0, match_source_len=False, no_repeat_ngram_size=0, normalize_scores=True, channel_models=None, k2=10, ch_weight=1.0, channel_scoring_type='log_norm', top_k_vocab=0, lm_models=None, lm_dict=None, lm_weight=1.0, normalize_lm_scores_by_tgt_len=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Generates translations of a given source sentence,\\n           using beam search with noisy channel decoding.\\n\\n        Args:\\n            combine_method (string, optional): Method to combine direct, LM and\\n                channel model scores (default: None)\\n            tgt_dict (~fairseq.data.Dictionary): target dictionary\\n            src_dict (~fairseq.data.Dictionary): source dictionary\\n            beam_size (int, optional): beam width (default: 1)\\n            max_len_a/b (int, optional): generate sequences of maximum length\\n                ax + b, where x is the source length\\n            min_len (int, optional): the minimum length of the generated output\\n                (not including end-of-sentence)\\n            len_penalty (float, optional): length penalty, where <1.0 favors\\n                shorter, >1.0 favors longer sentences (default: 1.0)\\n            unk_penalty (float, optional): unknown word penalty, where <0\\n                produces more unks, >0 produces fewer (default: 0.0)\\n            retain_dropout (bool, optional): use dropout when generating\\n                (default: False)\\n            temperature (float, optional): temperature, where values\\n                >1.0 produce more uniform samples and values <1.0 produce\\n                sharper samples (default: 1.0)\\n            match_source_len (bool, optional): outputs should match the source\\n                length (default: False)\\n            no_repeat_ngram_size (int, optional): Size of n-grams that we avoid\\n                repeating in the generation (default: 0)\\n            normalize_scores (bool, optional): normalize scores by the length\\n                of the output (default: True)\\n            channel_models (List[~fairseq.models.FairseqModel]): ensemble of models\\n                translating from the target to the source\\n            k2 (int, optional): Top K2 candidates to score per beam at each step (default:10)\\n            ch_weight (int, optional): Weight associated with the channel model score\\n                assuming that the direct model score has weight 1.0 (default: 1.0)\\n            channel_scoring_type (str, optional): String specifying how to score\\n                the channel model (default: 'log_norm')\\n            top_k_vocab (int, optional): If `channel_scoring_type` is `'src_vocab'` or\\n                `'src_vocab_batched'`, then this parameter specifies the number of\\n                most frequent tokens to include in the channel model output vocabulary,\\n                in addition to the source tokens in the input batch (default: 0)\\n            lm_models (List[~fairseq.models.FairseqModel]): ensemble of models\\n                generating text in the target language\\n            lm_dict (~fairseq.data.Dictionary): LM Model dictionary\\n            lm_weight (int, optional): Weight associated with the LM model score\\n                assuming that the direct model score has weight 1.0 (default: 1.0)\\n            normalize_lm_scores_by_tgt_len (bool, optional): Should we normalize LM scores\\n                by the target length? By default, we normalize the combination of\\n                LM and channel model scores by the source length\\n        \"\n    self.pad = tgt_dict.pad()\n    self.unk = tgt_dict.unk()\n    self.eos = tgt_dict.eos()\n    self.vocab_size = len(tgt_dict)\n    self.beam_size = beam_size\n    self.beam_size = min(beam_size, self.vocab_size - 1)\n    self.max_len_a = max_len_a\n    self.max_len_b = max_len_b\n    self.min_len = min_len\n    self.normalize_scores = normalize_scores\n    self.len_penalty = len_penalty\n    self.unk_penalty = unk_penalty\n    self.retain_dropout = retain_dropout\n    self.temperature = temperature\n    self.match_source_len = match_source_len\n    self.no_repeat_ngram_size = no_repeat_ngram_size\n    self.channel_models = channel_models\n    self.src_dict = src_dict\n    self.tgt_dict = tgt_dict\n    self.combine_method = combine_method\n    self.k2 = k2\n    self.ch_weight = ch_weight\n    self.channel_scoring_type = channel_scoring_type\n    self.top_k_vocab = top_k_vocab\n    self.lm_models = lm_models\n    self.lm_dict = lm_dict\n    self.lm_weight = lm_weight\n    self.log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n    self.normalize_lm_scores_by_tgt_len = normalize_lm_scores_by_tgt_len\n    self.share_tgt_dict = self.lm_dict == self.tgt_dict\n    self.tgt_to_lm = make_dict2dict(tgt_dict, lm_dict)\n    self.ch_scoring_bsz = 3072\n    assert temperature > 0, '--temperature must be greater than 0'\n    self.search = NoisyChannelBeamSearch(tgt_dict)",
            "def __init__(self, combine_method, tgt_dict, src_dict=None, beam_size=1, max_len_a=0, max_len_b=200, min_len=1, len_penalty=1.0, unk_penalty=0.0, retain_dropout=False, temperature=1.0, match_source_len=False, no_repeat_ngram_size=0, normalize_scores=True, channel_models=None, k2=10, ch_weight=1.0, channel_scoring_type='log_norm', top_k_vocab=0, lm_models=None, lm_dict=None, lm_weight=1.0, normalize_lm_scores_by_tgt_len=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Generates translations of a given source sentence,\\n           using beam search with noisy channel decoding.\\n\\n        Args:\\n            combine_method (string, optional): Method to combine direct, LM and\\n                channel model scores (default: None)\\n            tgt_dict (~fairseq.data.Dictionary): target dictionary\\n            src_dict (~fairseq.data.Dictionary): source dictionary\\n            beam_size (int, optional): beam width (default: 1)\\n            max_len_a/b (int, optional): generate sequences of maximum length\\n                ax + b, where x is the source length\\n            min_len (int, optional): the minimum length of the generated output\\n                (not including end-of-sentence)\\n            len_penalty (float, optional): length penalty, where <1.0 favors\\n                shorter, >1.0 favors longer sentences (default: 1.0)\\n            unk_penalty (float, optional): unknown word penalty, where <0\\n                produces more unks, >0 produces fewer (default: 0.0)\\n            retain_dropout (bool, optional): use dropout when generating\\n                (default: False)\\n            temperature (float, optional): temperature, where values\\n                >1.0 produce more uniform samples and values <1.0 produce\\n                sharper samples (default: 1.0)\\n            match_source_len (bool, optional): outputs should match the source\\n                length (default: False)\\n            no_repeat_ngram_size (int, optional): Size of n-grams that we avoid\\n                repeating in the generation (default: 0)\\n            normalize_scores (bool, optional): normalize scores by the length\\n                of the output (default: True)\\n            channel_models (List[~fairseq.models.FairseqModel]): ensemble of models\\n                translating from the target to the source\\n            k2 (int, optional): Top K2 candidates to score per beam at each step (default:10)\\n            ch_weight (int, optional): Weight associated with the channel model score\\n                assuming that the direct model score has weight 1.0 (default: 1.0)\\n            channel_scoring_type (str, optional): String specifying how to score\\n                the channel model (default: 'log_norm')\\n            top_k_vocab (int, optional): If `channel_scoring_type` is `'src_vocab'` or\\n                `'src_vocab_batched'`, then this parameter specifies the number of\\n                most frequent tokens to include in the channel model output vocabulary,\\n                in addition to the source tokens in the input batch (default: 0)\\n            lm_models (List[~fairseq.models.FairseqModel]): ensemble of models\\n                generating text in the target language\\n            lm_dict (~fairseq.data.Dictionary): LM Model dictionary\\n            lm_weight (int, optional): Weight associated with the LM model score\\n                assuming that the direct model score has weight 1.0 (default: 1.0)\\n            normalize_lm_scores_by_tgt_len (bool, optional): Should we normalize LM scores\\n                by the target length? By default, we normalize the combination of\\n                LM and channel model scores by the source length\\n        \"\n    self.pad = tgt_dict.pad()\n    self.unk = tgt_dict.unk()\n    self.eos = tgt_dict.eos()\n    self.vocab_size = len(tgt_dict)\n    self.beam_size = beam_size\n    self.beam_size = min(beam_size, self.vocab_size - 1)\n    self.max_len_a = max_len_a\n    self.max_len_b = max_len_b\n    self.min_len = min_len\n    self.normalize_scores = normalize_scores\n    self.len_penalty = len_penalty\n    self.unk_penalty = unk_penalty\n    self.retain_dropout = retain_dropout\n    self.temperature = temperature\n    self.match_source_len = match_source_len\n    self.no_repeat_ngram_size = no_repeat_ngram_size\n    self.channel_models = channel_models\n    self.src_dict = src_dict\n    self.tgt_dict = tgt_dict\n    self.combine_method = combine_method\n    self.k2 = k2\n    self.ch_weight = ch_weight\n    self.channel_scoring_type = channel_scoring_type\n    self.top_k_vocab = top_k_vocab\n    self.lm_models = lm_models\n    self.lm_dict = lm_dict\n    self.lm_weight = lm_weight\n    self.log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n    self.normalize_lm_scores_by_tgt_len = normalize_lm_scores_by_tgt_len\n    self.share_tgt_dict = self.lm_dict == self.tgt_dict\n    self.tgt_to_lm = make_dict2dict(tgt_dict, lm_dict)\n    self.ch_scoring_bsz = 3072\n    assert temperature > 0, '--temperature must be greater than 0'\n    self.search = NoisyChannelBeamSearch(tgt_dict)"
        ]
    },
    {
        "func_name": "buffer",
        "original": "def buffer(name, type_of=tokens):\n    if name not in buffers:\n        buffers[name] = type_of.new()\n    return buffers[name]",
        "mutated": [
            "def buffer(name, type_of=tokens):\n    if False:\n        i = 10\n    if name not in buffers:\n        buffers[name] = type_of.new()\n    return buffers[name]",
            "def buffer(name, type_of=tokens):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if name not in buffers:\n        buffers[name] = type_of.new()\n    return buffers[name]",
            "def buffer(name, type_of=tokens):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if name not in buffers:\n        buffers[name] = type_of.new()\n    return buffers[name]",
            "def buffer(name, type_of=tokens):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if name not in buffers:\n        buffers[name] = type_of.new()\n    return buffers[name]",
            "def buffer(name, type_of=tokens):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if name not in buffers:\n        buffers[name] = type_of.new()\n    return buffers[name]"
        ]
    },
    {
        "func_name": "is_finished",
        "original": "def is_finished(sent, step, unfin_idx):\n    \"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"\n    assert len(finalized[sent]) <= beam_size\n    if len(finalized[sent]) == beam_size:\n        return True\n    return False",
        "mutated": [
            "def is_finished(sent, step, unfin_idx):\n    if False:\n        i = 10\n    \"\\n            Check whether we've finished generation for a given sentence, by\\n            comparing the worst score among finalized hypotheses to the best\\n            possible score among unfinalized hypotheses.\\n            \"\n    assert len(finalized[sent]) <= beam_size\n    if len(finalized[sent]) == beam_size:\n        return True\n    return False",
            "def is_finished(sent, step, unfin_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n            Check whether we've finished generation for a given sentence, by\\n            comparing the worst score among finalized hypotheses to the best\\n            possible score among unfinalized hypotheses.\\n            \"\n    assert len(finalized[sent]) <= beam_size\n    if len(finalized[sent]) == beam_size:\n        return True\n    return False",
            "def is_finished(sent, step, unfin_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n            Check whether we've finished generation for a given sentence, by\\n            comparing the worst score among finalized hypotheses to the best\\n            possible score among unfinalized hypotheses.\\n            \"\n    assert len(finalized[sent]) <= beam_size\n    if len(finalized[sent]) == beam_size:\n        return True\n    return False",
            "def is_finished(sent, step, unfin_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n            Check whether we've finished generation for a given sentence, by\\n            comparing the worst score among finalized hypotheses to the best\\n            possible score among unfinalized hypotheses.\\n            \"\n    assert len(finalized[sent]) <= beam_size\n    if len(finalized[sent]) == beam_size:\n        return True\n    return False",
            "def is_finished(sent, step, unfin_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n            Check whether we've finished generation for a given sentence, by\\n            comparing the worst score among finalized hypotheses to the best\\n            possible score among unfinalized hypotheses.\\n            \"\n    assert len(finalized[sent]) <= beam_size\n    if len(finalized[sent]) == beam_size:\n        return True\n    return False"
        ]
    },
    {
        "func_name": "get_hypo",
        "original": "def get_hypo():\n    if attn_clone is not None:\n        hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n        (_, alignment) = hypo_attn.max(dim=0)\n    else:\n        hypo_attn = None\n        alignment = None\n    return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}",
        "mutated": [
            "def get_hypo():\n    if False:\n        i = 10\n    if attn_clone is not None:\n        hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n        (_, alignment) = hypo_attn.max(dim=0)\n    else:\n        hypo_attn = None\n        alignment = None\n    return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}",
            "def get_hypo():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if attn_clone is not None:\n        hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n        (_, alignment) = hypo_attn.max(dim=0)\n    else:\n        hypo_attn = None\n        alignment = None\n    return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}",
            "def get_hypo():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if attn_clone is not None:\n        hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n        (_, alignment) = hypo_attn.max(dim=0)\n    else:\n        hypo_attn = None\n        alignment = None\n    return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}",
            "def get_hypo():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if attn_clone is not None:\n        hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n        (_, alignment) = hypo_attn.max(dim=0)\n    else:\n        hypo_attn = None\n        alignment = None\n    return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}",
            "def get_hypo():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if attn_clone is not None:\n        hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n        (_, alignment) = hypo_attn.max(dim=0)\n    else:\n        hypo_attn = None\n        alignment = None\n    return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}"
        ]
    },
    {
        "func_name": "finalize_hypos",
        "original": "def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n    \"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    fw scores for each hypothesis\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\n                    combined noisy channel scores for each hypothesis\n            \"\"\"\n    assert bbsz_idx.numel() == eos_scores.numel()\n    tokens_clone = tokens.index_select(0, bbsz_idx)\n    tokens_clone = tokens_clone[:, 1:step + 2]\n    assert not tokens_clone.eq(self.eos).any()\n    tokens_clone[:, step] = self.eos\n    attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n    pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n    pos_scores[:, step] = eos_scores\n    pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n    if self.normalize_scores:\n        combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n    cum_unfin = []\n    prev = 0\n    for f in finished:\n        if f:\n            prev += 1\n        else:\n            cum_unfin.append(prev)\n    sents_seen = set()\n    for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n        unfin_idx = idx // beam_size\n        sent = unfin_idx + cum_unfin[unfin_idx]\n        sents_seen.add((sent, unfin_idx))\n        if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n            score = -math.inf\n\n        def get_hypo():\n            if attn_clone is not None:\n                hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                (_, alignment) = hypo_attn.max(dim=0)\n            else:\n                hypo_attn = None\n                alignment = None\n            return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n        if len(finalized[sent]) < beam_size:\n            finalized[sent].append(get_hypo())\n    newly_finished = []\n    for (sent, unfin_idx) in sents_seen:\n        if not finished[sent] and is_finished(sent, step, unfin_idx):\n            finished[sent] = True\n            newly_finished.append(unfin_idx)\n    return newly_finished",
        "mutated": [
            "def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n    if False:\n        i = 10\n    '\\n            Finalize the given hypotheses at this step, while keeping the total\\n            number of finalized hypotheses per sentence <= beam_size.\\n\\n            Note: the input must be in the desired finalization order, so that\\n            hypotheses that appear earlier in the input are preferred to those\\n            that appear later.\\n\\n            Args:\\n                step: current time step\\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\\n                    indicating which hypotheses to finalize\\n                eos_scores: A vector of the same size as bbsz_idx containing\\n                    fw scores for each hypothesis\\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\\n                    combined noisy channel scores for each hypothesis\\n            '\n    assert bbsz_idx.numel() == eos_scores.numel()\n    tokens_clone = tokens.index_select(0, bbsz_idx)\n    tokens_clone = tokens_clone[:, 1:step + 2]\n    assert not tokens_clone.eq(self.eos).any()\n    tokens_clone[:, step] = self.eos\n    attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n    pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n    pos_scores[:, step] = eos_scores\n    pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n    if self.normalize_scores:\n        combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n    cum_unfin = []\n    prev = 0\n    for f in finished:\n        if f:\n            prev += 1\n        else:\n            cum_unfin.append(prev)\n    sents_seen = set()\n    for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n        unfin_idx = idx // beam_size\n        sent = unfin_idx + cum_unfin[unfin_idx]\n        sents_seen.add((sent, unfin_idx))\n        if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n            score = -math.inf\n\n        def get_hypo():\n            if attn_clone is not None:\n                hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                (_, alignment) = hypo_attn.max(dim=0)\n            else:\n                hypo_attn = None\n                alignment = None\n            return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n        if len(finalized[sent]) < beam_size:\n            finalized[sent].append(get_hypo())\n    newly_finished = []\n    for (sent, unfin_idx) in sents_seen:\n        if not finished[sent] and is_finished(sent, step, unfin_idx):\n            finished[sent] = True\n            newly_finished.append(unfin_idx)\n    return newly_finished",
            "def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n            Finalize the given hypotheses at this step, while keeping the total\\n            number of finalized hypotheses per sentence <= beam_size.\\n\\n            Note: the input must be in the desired finalization order, so that\\n            hypotheses that appear earlier in the input are preferred to those\\n            that appear later.\\n\\n            Args:\\n                step: current time step\\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\\n                    indicating which hypotheses to finalize\\n                eos_scores: A vector of the same size as bbsz_idx containing\\n                    fw scores for each hypothesis\\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\\n                    combined noisy channel scores for each hypothesis\\n            '\n    assert bbsz_idx.numel() == eos_scores.numel()\n    tokens_clone = tokens.index_select(0, bbsz_idx)\n    tokens_clone = tokens_clone[:, 1:step + 2]\n    assert not tokens_clone.eq(self.eos).any()\n    tokens_clone[:, step] = self.eos\n    attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n    pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n    pos_scores[:, step] = eos_scores\n    pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n    if self.normalize_scores:\n        combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n    cum_unfin = []\n    prev = 0\n    for f in finished:\n        if f:\n            prev += 1\n        else:\n            cum_unfin.append(prev)\n    sents_seen = set()\n    for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n        unfin_idx = idx // beam_size\n        sent = unfin_idx + cum_unfin[unfin_idx]\n        sents_seen.add((sent, unfin_idx))\n        if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n            score = -math.inf\n\n        def get_hypo():\n            if attn_clone is not None:\n                hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                (_, alignment) = hypo_attn.max(dim=0)\n            else:\n                hypo_attn = None\n                alignment = None\n            return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n        if len(finalized[sent]) < beam_size:\n            finalized[sent].append(get_hypo())\n    newly_finished = []\n    for (sent, unfin_idx) in sents_seen:\n        if not finished[sent] and is_finished(sent, step, unfin_idx):\n            finished[sent] = True\n            newly_finished.append(unfin_idx)\n    return newly_finished",
            "def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n            Finalize the given hypotheses at this step, while keeping the total\\n            number of finalized hypotheses per sentence <= beam_size.\\n\\n            Note: the input must be in the desired finalization order, so that\\n            hypotheses that appear earlier in the input are preferred to those\\n            that appear later.\\n\\n            Args:\\n                step: current time step\\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\\n                    indicating which hypotheses to finalize\\n                eos_scores: A vector of the same size as bbsz_idx containing\\n                    fw scores for each hypothesis\\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\\n                    combined noisy channel scores for each hypothesis\\n            '\n    assert bbsz_idx.numel() == eos_scores.numel()\n    tokens_clone = tokens.index_select(0, bbsz_idx)\n    tokens_clone = tokens_clone[:, 1:step + 2]\n    assert not tokens_clone.eq(self.eos).any()\n    tokens_clone[:, step] = self.eos\n    attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n    pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n    pos_scores[:, step] = eos_scores\n    pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n    if self.normalize_scores:\n        combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n    cum_unfin = []\n    prev = 0\n    for f in finished:\n        if f:\n            prev += 1\n        else:\n            cum_unfin.append(prev)\n    sents_seen = set()\n    for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n        unfin_idx = idx // beam_size\n        sent = unfin_idx + cum_unfin[unfin_idx]\n        sents_seen.add((sent, unfin_idx))\n        if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n            score = -math.inf\n\n        def get_hypo():\n            if attn_clone is not None:\n                hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                (_, alignment) = hypo_attn.max(dim=0)\n            else:\n                hypo_attn = None\n                alignment = None\n            return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n        if len(finalized[sent]) < beam_size:\n            finalized[sent].append(get_hypo())\n    newly_finished = []\n    for (sent, unfin_idx) in sents_seen:\n        if not finished[sent] and is_finished(sent, step, unfin_idx):\n            finished[sent] = True\n            newly_finished.append(unfin_idx)\n    return newly_finished",
            "def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n            Finalize the given hypotheses at this step, while keeping the total\\n            number of finalized hypotheses per sentence <= beam_size.\\n\\n            Note: the input must be in the desired finalization order, so that\\n            hypotheses that appear earlier in the input are preferred to those\\n            that appear later.\\n\\n            Args:\\n                step: current time step\\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\\n                    indicating which hypotheses to finalize\\n                eos_scores: A vector of the same size as bbsz_idx containing\\n                    fw scores for each hypothesis\\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\\n                    combined noisy channel scores for each hypothesis\\n            '\n    assert bbsz_idx.numel() == eos_scores.numel()\n    tokens_clone = tokens.index_select(0, bbsz_idx)\n    tokens_clone = tokens_clone[:, 1:step + 2]\n    assert not tokens_clone.eq(self.eos).any()\n    tokens_clone[:, step] = self.eos\n    attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n    pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n    pos_scores[:, step] = eos_scores\n    pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n    if self.normalize_scores:\n        combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n    cum_unfin = []\n    prev = 0\n    for f in finished:\n        if f:\n            prev += 1\n        else:\n            cum_unfin.append(prev)\n    sents_seen = set()\n    for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n        unfin_idx = idx // beam_size\n        sent = unfin_idx + cum_unfin[unfin_idx]\n        sents_seen.add((sent, unfin_idx))\n        if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n            score = -math.inf\n\n        def get_hypo():\n            if attn_clone is not None:\n                hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                (_, alignment) = hypo_attn.max(dim=0)\n            else:\n                hypo_attn = None\n                alignment = None\n            return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n        if len(finalized[sent]) < beam_size:\n            finalized[sent].append(get_hypo())\n    newly_finished = []\n    for (sent, unfin_idx) in sents_seen:\n        if not finished[sent] and is_finished(sent, step, unfin_idx):\n            finished[sent] = True\n            newly_finished.append(unfin_idx)\n    return newly_finished",
            "def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n            Finalize the given hypotheses at this step, while keeping the total\\n            number of finalized hypotheses per sentence <= beam_size.\\n\\n            Note: the input must be in the desired finalization order, so that\\n            hypotheses that appear earlier in the input are preferred to those\\n            that appear later.\\n\\n            Args:\\n                step: current time step\\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\\n                    indicating which hypotheses to finalize\\n                eos_scores: A vector of the same size as bbsz_idx containing\\n                    fw scores for each hypothesis\\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\\n                    combined noisy channel scores for each hypothesis\\n            '\n    assert bbsz_idx.numel() == eos_scores.numel()\n    tokens_clone = tokens.index_select(0, bbsz_idx)\n    tokens_clone = tokens_clone[:, 1:step + 2]\n    assert not tokens_clone.eq(self.eos).any()\n    tokens_clone[:, step] = self.eos\n    attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n    pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n    pos_scores[:, step] = eos_scores\n    pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n    if self.normalize_scores:\n        combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n    cum_unfin = []\n    prev = 0\n    for f in finished:\n        if f:\n            prev += 1\n        else:\n            cum_unfin.append(prev)\n    sents_seen = set()\n    for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n        unfin_idx = idx // beam_size\n        sent = unfin_idx + cum_unfin[unfin_idx]\n        sents_seen.add((sent, unfin_idx))\n        if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n            score = -math.inf\n\n        def get_hypo():\n            if attn_clone is not None:\n                hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                (_, alignment) = hypo_attn.max(dim=0)\n            else:\n                hypo_attn = None\n                alignment = None\n            return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n        if len(finalized[sent]) < beam_size:\n            finalized[sent].append(get_hypo())\n    newly_finished = []\n    for (sent, unfin_idx) in sents_seen:\n        if not finished[sent] and is_finished(sent, step, unfin_idx):\n            finished[sent] = True\n            newly_finished.append(unfin_idx)\n    return newly_finished"
        ]
    },
    {
        "func_name": "noisy_channel_rescoring",
        "original": "def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n    \"\"\"Rescore the top k hypothesis from each beam using noisy channel modeling\n            Returns:\n                new_fw_lprobs: the direct model probabilities after pruning the top k\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\n                new_lm_lprobs: the language model probabilities after pruning the top k\n            \"\"\"\n    with torch.no_grad():\n        lprobs_size = lprobs.size()\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n            cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n            cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n            fw_top_k = cand_scores\n            fw_top_k_idx = cand_indices\n            k = 1\n        else:\n            (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n        eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n        ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n        src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n        if self.combine_method != 'lm_only':\n            temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n            not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n            cur_tgt_size = step + 2\n            eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n            eos_tokens[eos_idx] = self.tgt_dict.pad_index\n            if step == 0:\n                channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            else:\n                channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n            ch_input_lengths[eos_idx] = cur_tgt_size - 1\n            if self.channel_scoring_type == 'unnormalized':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                ch_intermed_scores = ch_intermed_scores.float()\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'k2_separate':\n                for k_idx in range(k):\n                    k_eos_tokens = eos_tokens[k_idx::k, :]\n                    if step == 0:\n                        k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    else:\n                        k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                    k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                    k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                    k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                    k_ch_intermed_scores *= not_padding.float()\n                    ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'src_vocab':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            elif self.channel_scoring_type == 'src_vocab_batched':\n                ch_bsz_size = temp_src_tokens_full.shape[0]\n                ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                    end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                    temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                    channel_input_batch = channel_input[start_idx:end_idx, :]\n                    ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                    ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                    (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                    ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            else:\n                ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n        else:\n            cur_tgt_size = 0\n        ch_scores = ch_scores.view(bsz * beam_size, k)\n        expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n        if self.share_tgt_dict:\n            lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        else:\n            new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n            new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n            lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        lm_scores.add_(expanded_lm_prefix_scores)\n        ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n        new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_fw_lprobs[:, self.pad] = -math.inf\n        new_ch_lm_lprobs[:, self.pad] = -math.inf\n        new_lm_lprobs[:, self.pad] = -math.inf\n        new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n        new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n        new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n        return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)",
        "mutated": [
            "def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n    if False:\n        i = 10\n    'Rescore the top k hypothesis from each beam using noisy channel modeling\\n            Returns:\\n                new_fw_lprobs: the direct model probabilities after pruning the top k\\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\\n                new_lm_lprobs: the language model probabilities after pruning the top k\\n            '\n    with torch.no_grad():\n        lprobs_size = lprobs.size()\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n            cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n            cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n            fw_top_k = cand_scores\n            fw_top_k_idx = cand_indices\n            k = 1\n        else:\n            (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n        eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n        ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n        src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n        if self.combine_method != 'lm_only':\n            temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n            not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n            cur_tgt_size = step + 2\n            eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n            eos_tokens[eos_idx] = self.tgt_dict.pad_index\n            if step == 0:\n                channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            else:\n                channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n            ch_input_lengths[eos_idx] = cur_tgt_size - 1\n            if self.channel_scoring_type == 'unnormalized':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                ch_intermed_scores = ch_intermed_scores.float()\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'k2_separate':\n                for k_idx in range(k):\n                    k_eos_tokens = eos_tokens[k_idx::k, :]\n                    if step == 0:\n                        k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    else:\n                        k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                    k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                    k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                    k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                    k_ch_intermed_scores *= not_padding.float()\n                    ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'src_vocab':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            elif self.channel_scoring_type == 'src_vocab_batched':\n                ch_bsz_size = temp_src_tokens_full.shape[0]\n                ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                    end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                    temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                    channel_input_batch = channel_input[start_idx:end_idx, :]\n                    ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                    ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                    (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                    ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            else:\n                ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n        else:\n            cur_tgt_size = 0\n        ch_scores = ch_scores.view(bsz * beam_size, k)\n        expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n        if self.share_tgt_dict:\n            lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        else:\n            new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n            new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n            lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        lm_scores.add_(expanded_lm_prefix_scores)\n        ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n        new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_fw_lprobs[:, self.pad] = -math.inf\n        new_ch_lm_lprobs[:, self.pad] = -math.inf\n        new_lm_lprobs[:, self.pad] = -math.inf\n        new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n        new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n        new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n        return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)",
            "def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Rescore the top k hypothesis from each beam using noisy channel modeling\\n            Returns:\\n                new_fw_lprobs: the direct model probabilities after pruning the top k\\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\\n                new_lm_lprobs: the language model probabilities after pruning the top k\\n            '\n    with torch.no_grad():\n        lprobs_size = lprobs.size()\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n            cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n            cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n            fw_top_k = cand_scores\n            fw_top_k_idx = cand_indices\n            k = 1\n        else:\n            (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n        eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n        ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n        src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n        if self.combine_method != 'lm_only':\n            temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n            not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n            cur_tgt_size = step + 2\n            eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n            eos_tokens[eos_idx] = self.tgt_dict.pad_index\n            if step == 0:\n                channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            else:\n                channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n            ch_input_lengths[eos_idx] = cur_tgt_size - 1\n            if self.channel_scoring_type == 'unnormalized':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                ch_intermed_scores = ch_intermed_scores.float()\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'k2_separate':\n                for k_idx in range(k):\n                    k_eos_tokens = eos_tokens[k_idx::k, :]\n                    if step == 0:\n                        k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    else:\n                        k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                    k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                    k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                    k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                    k_ch_intermed_scores *= not_padding.float()\n                    ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'src_vocab':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            elif self.channel_scoring_type == 'src_vocab_batched':\n                ch_bsz_size = temp_src_tokens_full.shape[0]\n                ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                    end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                    temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                    channel_input_batch = channel_input[start_idx:end_idx, :]\n                    ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                    ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                    (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                    ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            else:\n                ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n        else:\n            cur_tgt_size = 0\n        ch_scores = ch_scores.view(bsz * beam_size, k)\n        expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n        if self.share_tgt_dict:\n            lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        else:\n            new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n            new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n            lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        lm_scores.add_(expanded_lm_prefix_scores)\n        ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n        new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_fw_lprobs[:, self.pad] = -math.inf\n        new_ch_lm_lprobs[:, self.pad] = -math.inf\n        new_lm_lprobs[:, self.pad] = -math.inf\n        new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n        new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n        new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n        return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)",
            "def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Rescore the top k hypothesis from each beam using noisy channel modeling\\n            Returns:\\n                new_fw_lprobs: the direct model probabilities after pruning the top k\\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\\n                new_lm_lprobs: the language model probabilities after pruning the top k\\n            '\n    with torch.no_grad():\n        lprobs_size = lprobs.size()\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n            cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n            cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n            fw_top_k = cand_scores\n            fw_top_k_idx = cand_indices\n            k = 1\n        else:\n            (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n        eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n        ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n        src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n        if self.combine_method != 'lm_only':\n            temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n            not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n            cur_tgt_size = step + 2\n            eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n            eos_tokens[eos_idx] = self.tgt_dict.pad_index\n            if step == 0:\n                channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            else:\n                channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n            ch_input_lengths[eos_idx] = cur_tgt_size - 1\n            if self.channel_scoring_type == 'unnormalized':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                ch_intermed_scores = ch_intermed_scores.float()\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'k2_separate':\n                for k_idx in range(k):\n                    k_eos_tokens = eos_tokens[k_idx::k, :]\n                    if step == 0:\n                        k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    else:\n                        k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                    k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                    k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                    k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                    k_ch_intermed_scores *= not_padding.float()\n                    ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'src_vocab':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            elif self.channel_scoring_type == 'src_vocab_batched':\n                ch_bsz_size = temp_src_tokens_full.shape[0]\n                ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                    end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                    temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                    channel_input_batch = channel_input[start_idx:end_idx, :]\n                    ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                    ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                    (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                    ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            else:\n                ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n        else:\n            cur_tgt_size = 0\n        ch_scores = ch_scores.view(bsz * beam_size, k)\n        expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n        if self.share_tgt_dict:\n            lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        else:\n            new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n            new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n            lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        lm_scores.add_(expanded_lm_prefix_scores)\n        ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n        new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_fw_lprobs[:, self.pad] = -math.inf\n        new_ch_lm_lprobs[:, self.pad] = -math.inf\n        new_lm_lprobs[:, self.pad] = -math.inf\n        new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n        new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n        new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n        return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)",
            "def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Rescore the top k hypothesis from each beam using noisy channel modeling\\n            Returns:\\n                new_fw_lprobs: the direct model probabilities after pruning the top k\\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\\n                new_lm_lprobs: the language model probabilities after pruning the top k\\n            '\n    with torch.no_grad():\n        lprobs_size = lprobs.size()\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n            cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n            cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n            fw_top_k = cand_scores\n            fw_top_k_idx = cand_indices\n            k = 1\n        else:\n            (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n        eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n        ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n        src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n        if self.combine_method != 'lm_only':\n            temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n            not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n            cur_tgt_size = step + 2\n            eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n            eos_tokens[eos_idx] = self.tgt_dict.pad_index\n            if step == 0:\n                channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            else:\n                channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n            ch_input_lengths[eos_idx] = cur_tgt_size - 1\n            if self.channel_scoring_type == 'unnormalized':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                ch_intermed_scores = ch_intermed_scores.float()\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'k2_separate':\n                for k_idx in range(k):\n                    k_eos_tokens = eos_tokens[k_idx::k, :]\n                    if step == 0:\n                        k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    else:\n                        k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                    k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                    k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                    k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                    k_ch_intermed_scores *= not_padding.float()\n                    ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'src_vocab':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            elif self.channel_scoring_type == 'src_vocab_batched':\n                ch_bsz_size = temp_src_tokens_full.shape[0]\n                ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                    end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                    temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                    channel_input_batch = channel_input[start_idx:end_idx, :]\n                    ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                    ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                    (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                    ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            else:\n                ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n        else:\n            cur_tgt_size = 0\n        ch_scores = ch_scores.view(bsz * beam_size, k)\n        expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n        if self.share_tgt_dict:\n            lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        else:\n            new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n            new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n            lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        lm_scores.add_(expanded_lm_prefix_scores)\n        ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n        new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_fw_lprobs[:, self.pad] = -math.inf\n        new_ch_lm_lprobs[:, self.pad] = -math.inf\n        new_lm_lprobs[:, self.pad] = -math.inf\n        new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n        new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n        new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n        return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)",
            "def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Rescore the top k hypothesis from each beam using noisy channel modeling\\n            Returns:\\n                new_fw_lprobs: the direct model probabilities after pruning the top k\\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\\n                new_lm_lprobs: the language model probabilities after pruning the top k\\n            '\n    with torch.no_grad():\n        lprobs_size = lprobs.size()\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n            cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n            cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n            fw_top_k = cand_scores\n            fw_top_k_idx = cand_indices\n            k = 1\n        else:\n            (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n        eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n        ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n        src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n        if self.combine_method != 'lm_only':\n            temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n            not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n            cur_tgt_size = step + 2\n            eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n            eos_tokens[eos_idx] = self.tgt_dict.pad_index\n            if step == 0:\n                channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            else:\n                channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n            ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n            ch_input_lengths[eos_idx] = cur_tgt_size - 1\n            if self.channel_scoring_type == 'unnormalized':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                ch_intermed_scores = ch_intermed_scores.float()\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'k2_separate':\n                for k_idx in range(k):\n                    k_eos_tokens = eos_tokens[k_idx::k, :]\n                    if step == 0:\n                        k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    else:\n                        k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                    k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                    k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                    k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                    k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                    k_ch_intermed_scores *= not_padding.float()\n                    ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n            elif self.channel_scoring_type == 'src_vocab':\n                ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                del ch_encoder_output\n                ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            elif self.channel_scoring_type == 'src_vocab_batched':\n                ch_bsz_size = temp_src_tokens_full.shape[0]\n                ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                    end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                    temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                    channel_input_batch = channel_input[start_idx:end_idx, :]\n                    ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                    ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                    (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                    ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                ch_scores = torch.sum(ch_lprobs, dim=1)\n            else:\n                ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                ch_intermed_scores *= not_padding.float()\n                ch_scores = torch.sum(ch_intermed_scores, dim=1)\n        else:\n            cur_tgt_size = 0\n        ch_scores = ch_scores.view(bsz * beam_size, k)\n        expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n        if self.share_tgt_dict:\n            lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        else:\n            new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n            new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n            lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n        lm_scores.add_(expanded_lm_prefix_scores)\n        ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n        new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n        new_fw_lprobs[:, self.pad] = -math.inf\n        new_ch_lm_lprobs[:, self.pad] = -math.inf\n        new_lm_lprobs[:, self.pad] = -math.inf\n        new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n        new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n        new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n        return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)"
        ]
    },
    {
        "func_name": "combine_ch_lm",
        "original": "def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n    if self.channel_scoring_type == 'unnormalized':\n        ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n    ch_scores = ch_scores * self.ch_weight\n    lm_scores1 = lm_scores1 * self.lm_weight\n    if combine_type == 'lm_only':\n        ch_scores = lm_scores1.view(ch_scores.size())\n    elif combine_type == 'noisy_channel':\n        if self.normalize_lm_scores_by_tgt_len:\n            ch_scores.div_(src_size)\n            lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n            ch_scores.add_(lm_scores_norm)\n        else:\n            ch_scores.add_(lm_scores1.view(ch_scores.size()))\n            ch_scores.div_(src_size)\n    return ch_scores",
        "mutated": [
            "def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n    if False:\n        i = 10\n    if self.channel_scoring_type == 'unnormalized':\n        ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n    ch_scores = ch_scores * self.ch_weight\n    lm_scores1 = lm_scores1 * self.lm_weight\n    if combine_type == 'lm_only':\n        ch_scores = lm_scores1.view(ch_scores.size())\n    elif combine_type == 'noisy_channel':\n        if self.normalize_lm_scores_by_tgt_len:\n            ch_scores.div_(src_size)\n            lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n            ch_scores.add_(lm_scores_norm)\n        else:\n            ch_scores.add_(lm_scores1.view(ch_scores.size()))\n            ch_scores.div_(src_size)\n    return ch_scores",
            "def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.channel_scoring_type == 'unnormalized':\n        ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n    ch_scores = ch_scores * self.ch_weight\n    lm_scores1 = lm_scores1 * self.lm_weight\n    if combine_type == 'lm_only':\n        ch_scores = lm_scores1.view(ch_scores.size())\n    elif combine_type == 'noisy_channel':\n        if self.normalize_lm_scores_by_tgt_len:\n            ch_scores.div_(src_size)\n            lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n            ch_scores.add_(lm_scores_norm)\n        else:\n            ch_scores.add_(lm_scores1.view(ch_scores.size()))\n            ch_scores.div_(src_size)\n    return ch_scores",
            "def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.channel_scoring_type == 'unnormalized':\n        ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n    ch_scores = ch_scores * self.ch_weight\n    lm_scores1 = lm_scores1 * self.lm_weight\n    if combine_type == 'lm_only':\n        ch_scores = lm_scores1.view(ch_scores.size())\n    elif combine_type == 'noisy_channel':\n        if self.normalize_lm_scores_by_tgt_len:\n            ch_scores.div_(src_size)\n            lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n            ch_scores.add_(lm_scores_norm)\n        else:\n            ch_scores.add_(lm_scores1.view(ch_scores.size()))\n            ch_scores.div_(src_size)\n    return ch_scores",
            "def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.channel_scoring_type == 'unnormalized':\n        ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n    ch_scores = ch_scores * self.ch_weight\n    lm_scores1 = lm_scores1 * self.lm_weight\n    if combine_type == 'lm_only':\n        ch_scores = lm_scores1.view(ch_scores.size())\n    elif combine_type == 'noisy_channel':\n        if self.normalize_lm_scores_by_tgt_len:\n            ch_scores.div_(src_size)\n            lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n            ch_scores.add_(lm_scores_norm)\n        else:\n            ch_scores.add_(lm_scores1.view(ch_scores.size()))\n            ch_scores.div_(src_size)\n    return ch_scores",
            "def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.channel_scoring_type == 'unnormalized':\n        ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n    ch_scores = ch_scores * self.ch_weight\n    lm_scores1 = lm_scores1 * self.lm_weight\n    if combine_type == 'lm_only':\n        ch_scores = lm_scores1.view(ch_scores.size())\n    elif combine_type == 'noisy_channel':\n        if self.normalize_lm_scores_by_tgt_len:\n            ch_scores.div_(src_size)\n            lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n            ch_scores.add_(lm_scores_norm)\n        else:\n            ch_scores.add_(lm_scores1.view(ch_scores.size()))\n            ch_scores.div_(src_size)\n    return ch_scores"
        ]
    },
    {
        "func_name": "replicate_first_beam",
        "original": "def replicate_first_beam(tensor, mask):\n    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n    tensor[mask] = tensor[mask][:, :1, :]\n    return tensor.view(-1, tensor.size(-1))",
        "mutated": [
            "def replicate_first_beam(tensor, mask):\n    if False:\n        i = 10\n    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n    tensor[mask] = tensor[mask][:, :1, :]\n    return tensor.view(-1, tensor.size(-1))",
            "def replicate_first_beam(tensor, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n    tensor[mask] = tensor[mask][:, :1, :]\n    return tensor.view(-1, tensor.size(-1))",
            "def replicate_first_beam(tensor, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n    tensor[mask] = tensor[mask][:, :1, :]\n    return tensor.view(-1, tensor.size(-1))",
            "def replicate_first_beam(tensor, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n    tensor[mask] = tensor[mask][:, :1, :]\n    return tensor.view(-1, tensor.size(-1))",
            "def replicate_first_beam(tensor, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n    tensor[mask] = tensor[mask][:, :1, :]\n    return tensor.view(-1, tensor.size(-1))"
        ]
    },
    {
        "func_name": "calculate_banned_tokens",
        "original": "def calculate_banned_tokens(bbsz_idx):\n    ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n    return gen_ngrams[bbsz_idx].get(ngram_index, [])",
        "mutated": [
            "def calculate_banned_tokens(bbsz_idx):\n    if False:\n        i = 10\n    ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n    return gen_ngrams[bbsz_idx].get(ngram_index, [])",
            "def calculate_banned_tokens(bbsz_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n    return gen_ngrams[bbsz_idx].get(ngram_index, [])",
            "def calculate_banned_tokens(bbsz_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n    return gen_ngrams[bbsz_idx].get(ngram_index, [])",
            "def calculate_banned_tokens(bbsz_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n    return gen_ngrams[bbsz_idx].get(ngram_index, [])",
            "def calculate_banned_tokens(bbsz_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n    return gen_ngrams[bbsz_idx].get(ngram_index, [])"
        ]
    },
    {
        "func_name": "generate",
        "original": "@torch.no_grad()\ndef generate(self, models, sample, prefix_tokens=None, bos_token=None, **kwargs):\n    \"\"\"Generate a batch of translations.\n        Args:\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n        \"\"\"\n    model = EnsembleModel(models)\n    incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(model.models_size)])\n    if not self.retain_dropout:\n        model.eval()\n    encoder_input = {k: v for (k, v) in sample['net_input'].items() if k != 'prev_output_tokens'}\n    src_tokens = encoder_input['src_tokens']\n    src_lengths_no_eos = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n    input_size = src_tokens.size()\n    bsz = input_size[0]\n    src_len = input_size[1]\n    beam_size = self.beam_size\n    if self.match_source_len:\n        max_len = src_lengths_no_eos.max().item()\n    else:\n        max_len = min(int(self.max_len_a * src_len + self.max_len_b), model.max_decoder_positions() - 1)\n    encoder_outs = model.forward_encoder(encoder_input)\n    new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n    new_order = new_order.to(src_tokens.device).long()\n    encoder_outs = model.reorder_encoder_out(encoder_outs, new_order)\n    src_lengths = encoder_input['src_lengths']\n    scores = src_tokens.new(bsz * beam_size, max_len + 1).float().fill_(0)\n    lm_prefix_scores = src_tokens.new(bsz * beam_size).float().fill_(0)\n    scores_buf = scores.clone()\n    tokens = src_tokens.new(bsz * beam_size, max_len + 2).long().fill_(self.pad)\n    tokens_buf = tokens.clone()\n    tokens[:, 0] = self.eos if bos_token is None else bos_token\n    src_tokens = reorder_all_tokens(src_tokens, src_lengths, self.src_dict.eos_index)\n    src_tokens = src_tokens.repeat(1, beam_size).view(-1, src_len)\n    src_lengths = src_lengths.view(bsz, -1).repeat(1, beam_size).view(bsz * beam_size, -1)\n    (attn, attn_buf) = (None, None)\n    nonpad_idxs = None\n    cands_to_ignore = src_tokens.new_zeros(bsz, beam_size).eq(-1)\n    finalized = [[] for i in range(bsz)]\n    finished = [False for i in range(bsz)]\n    num_remaining_sent = bsz\n    cand_size = 2 * beam_size\n    bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n    cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n    buffers = {}\n\n    def buffer(name, type_of=tokens):\n        if name not in buffers:\n            buffers[name] = type_of.new()\n        return buffers[name]\n\n    def is_finished(sent, step, unfin_idx):\n        \"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"\n        assert len(finalized[sent]) <= beam_size\n        if len(finalized[sent]) == beam_size:\n            return True\n        return False\n\n    def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n        \"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    fw scores for each hypothesis\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\n                    combined noisy channel scores for each hypothesis\n            \"\"\"\n        assert bbsz_idx.numel() == eos_scores.numel()\n        tokens_clone = tokens.index_select(0, bbsz_idx)\n        tokens_clone = tokens_clone[:, 1:step + 2]\n        assert not tokens_clone.eq(self.eos).any()\n        tokens_clone[:, step] = self.eos\n        attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n        pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n        pos_scores[:, step] = eos_scores\n        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n        if self.normalize_scores:\n            combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n        cum_unfin = []\n        prev = 0\n        for f in finished:\n            if f:\n                prev += 1\n            else:\n                cum_unfin.append(prev)\n        sents_seen = set()\n        for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n            unfin_idx = idx // beam_size\n            sent = unfin_idx + cum_unfin[unfin_idx]\n            sents_seen.add((sent, unfin_idx))\n            if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n                score = -math.inf\n\n            def get_hypo():\n                if attn_clone is not None:\n                    hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                    (_, alignment) = hypo_attn.max(dim=0)\n                else:\n                    hypo_attn = None\n                    alignment = None\n                return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n            if len(finalized[sent]) < beam_size:\n                finalized[sent].append(get_hypo())\n        newly_finished = []\n        for (sent, unfin_idx) in sents_seen:\n            if not finished[sent] and is_finished(sent, step, unfin_idx):\n                finished[sent] = True\n                newly_finished.append(unfin_idx)\n        return newly_finished\n\n    def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n        \"\"\"Rescore the top k hypothesis from each beam using noisy channel modeling\n            Returns:\n                new_fw_lprobs: the direct model probabilities after pruning the top k\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\n                new_lm_lprobs: the language model probabilities after pruning the top k\n            \"\"\"\n        with torch.no_grad():\n            lprobs_size = lprobs.size()\n            if prefix_tokens is not None and step < prefix_tokens.size(1):\n                probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n                cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n                cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n                fw_top_k = cand_scores\n                fw_top_k_idx = cand_indices\n                k = 1\n            else:\n                (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n            eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n            ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n            src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n            if self.combine_method != 'lm_only':\n                temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n                not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n                cur_tgt_size = step + 2\n                eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n                eos_tokens[eos_idx] = self.tgt_dict.pad_index\n                if step == 0:\n                    channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                else:\n                    channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n                ch_input_lengths[eos_idx] = cur_tgt_size - 1\n                if self.channel_scoring_type == 'unnormalized':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                    ch_intermed_scores = ch_intermed_scores.float()\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'k2_separate':\n                    for k_idx in range(k):\n                        k_eos_tokens = eos_tokens[k_idx::k, :]\n                        if step == 0:\n                            k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        else:\n                            k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                        k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                        k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                        k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                        k_ch_intermed_scores *= not_padding.float()\n                        ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'src_vocab':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                elif self.channel_scoring_type == 'src_vocab_batched':\n                    ch_bsz_size = temp_src_tokens_full.shape[0]\n                    ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                    for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                        end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                        temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                        channel_input_batch = channel_input[start_idx:end_idx, :]\n                        ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                        ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                        (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                        ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                    ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                else:\n                    ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                    ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                    ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            else:\n                cur_tgt_size = 0\n            ch_scores = ch_scores.view(bsz * beam_size, k)\n            expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n            if self.share_tgt_dict:\n                lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            else:\n                new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n                new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n                lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            lm_scores.add_(expanded_lm_prefix_scores)\n            ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n            new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_fw_lprobs[:, self.pad] = -math.inf\n            new_ch_lm_lprobs[:, self.pad] = -math.inf\n            new_lm_lprobs[:, self.pad] = -math.inf\n            new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n            new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n            new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n            return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)\n\n    def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n        if self.channel_scoring_type == 'unnormalized':\n            ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n        ch_scores = ch_scores * self.ch_weight\n        lm_scores1 = lm_scores1 * self.lm_weight\n        if combine_type == 'lm_only':\n            ch_scores = lm_scores1.view(ch_scores.size())\n        elif combine_type == 'noisy_channel':\n            if self.normalize_lm_scores_by_tgt_len:\n                ch_scores.div_(src_size)\n                lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n                ch_scores.add_(lm_scores_norm)\n            else:\n                ch_scores.add_(lm_scores1.view(ch_scores.size()))\n                ch_scores.div_(src_size)\n        return ch_scores\n    if self.channel_models is not None:\n        channel_model = self.channel_models[0]\n    else:\n        channel_model = None\n    lm = EnsembleModel(self.lm_models)\n    lm_incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(lm.models_size)])\n    reorder_state = None\n    batch_idxs = None\n    for step in range(max_len + 1):\n        if reorder_state is not None:\n            if batch_idxs is not None:\n                corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)\n                reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)\n            model.reorder_incremental_state(incremental_states, reorder_state)\n            encoder_outs = model.reorder_encoder_out(encoder_outs, reorder_state)\n            lm.reorder_incremental_state(lm_incremental_states, reorder_state)\n        (fw_lprobs, avg_attn_scores) = model.forward_decoder(tokens[:, :step + 1], encoder_outs, incremental_states, temperature=self.temperature)\n        fw_lprobs[:, self.pad] = -math.inf\n        fw_lprobs[:, self.unk] -= self.unk_penalty\n        (fw_lprobs, ch_lm_lprobs, lm_lprobs) = noisy_channel_rescoring(fw_lprobs, beam_size, bsz, src_tokens, tokens, self.k2)\n        if step >= max_len:\n            fw_lprobs[:, :self.eos] = -math.inf\n            fw_lprobs[:, self.eos + 1:] = -math.inf\n        elif step < self.min_len:\n            fw_lprobs[:, self.eos] = -math.inf\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)\n            prefix_mask = prefix_toks.ne(self.pad)\n            prefix_fw_lprobs = fw_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            fw_lprobs[prefix_mask] = -math.inf\n            fw_lprobs[prefix_mask] = fw_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_fw_lprobs)\n            prefix_ch_lm_lprobs = ch_lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            ch_lm_lprobs[prefix_mask] = -math.inf\n            ch_lm_lprobs[prefix_mask] = ch_lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_ch_lm_lprobs)\n            prefix_lm_lprobs = lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            lm_lprobs[prefix_mask] = -math.inf\n            lm_lprobs[prefix_mask] = lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lm_lprobs)\n            eos_mask = prefix_toks.eq(self.eos)\n            if eos_mask.any():\n                first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[:, 0, 1:step + 1]\n                eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]\n                target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]\n                assert (first_beam == target_prefix).all()\n\n                def replicate_first_beam(tensor, mask):\n                    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n                    tensor[mask] = tensor[mask][:, :1, :]\n                    return tensor.view(-1, tensor.size(-1))\n                tokens = replicate_first_beam(tokens, eos_mask_batch_dim)\n                scores = replicate_first_beam(scores, eos_mask_batch_dim)\n                fw_lprobs = replicate_first_beam(fw_lprobs, eos_mask_batch_dim)\n                ch_lm_lprobs = replicate_first_beam(ch_lm_lprobs, eos_mask_batch_dim)\n                lm_lprobs = replicate_first_beam(lm_lprobs, eos_mask_batch_dim)\n        if self.no_repeat_ngram_size > 0:\n            gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                gen_tokens = tokens[bbsz_idx].tolist()\n                for ngram in zip(*[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]):\n                    gen_ngrams[bbsz_idx][tuple(ngram[:-1])] = gen_ngrams[bbsz_idx].get(tuple(ngram[:-1]), []) + [ngram[-1]]\n        if avg_attn_scores is not None:\n            if attn is None:\n                attn = scores.new(bsz * beam_size, src_tokens.size(1), max_len + 2)\n                attn_buf = attn.clone()\n                nonpad_idxs = src_tokens.ne(self.pad)\n            attn[:, :, step + 1].copy_(avg_attn_scores)\n        scores = scores.type_as(fw_lprobs)\n        scores_buf = scores_buf.type_as(fw_lprobs)\n        self.search.set_src_lengths(src_lengths_no_eos)\n        if self.no_repeat_ngram_size > 0:\n\n            def calculate_banned_tokens(bbsz_idx):\n                ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n                return gen_ngrams[bbsz_idx].get(ngram_index, [])\n            if step + 2 - self.no_repeat_ngram_size >= 0:\n                banned_tokens = [calculate_banned_tokens(bbsz_idx) for bbsz_idx in range(bsz * beam_size)]\n            else:\n                banned_tokens = [[] for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                fw_lprobs[bbsz_idx, banned_tokens[bbsz_idx]] = -math.inf\n        (combined_noisy_channel_scores, fw_lprobs_top_k, lm_lprobs_top_k, cand_indices, cand_beams) = self.search.step(step, fw_lprobs.view(bsz, -1, self.vocab_size), scores.view(bsz, beam_size, -1)[:, :, :step], ch_lm_lprobs.view(bsz, -1, self.vocab_size), lm_lprobs.view(bsz, -1, self.vocab_size), self.combine_method)\n        cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n        eos_mask = cand_indices.eq(self.eos)\n        eos_mask[:, :beam_size] &= ~cands_to_ignore\n        eos_bbsz_idx = torch.masked_select(cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size])\n        finalized_sents = set()\n        if eos_bbsz_idx.numel() > 0:\n            eos_scores = torch.masked_select(fw_lprobs_top_k[:, :beam_size], mask=eos_mask[:, :beam_size])\n            combined_noisy_channel_eos_scores = torch.masked_select(combined_noisy_channel_scores[:, :beam_size], mask=eos_mask[:, :beam_size])\n            finalized_sents = finalize_hypos(step, eos_bbsz_idx, eos_scores, combined_noisy_channel_eos_scores)\n            num_remaining_sent -= len(finalized_sents)\n        assert num_remaining_sent >= 0\n        if num_remaining_sent == 0:\n            break\n        if len(finalized_sents) > 0:\n            new_bsz = bsz - len(finalized_sents)\n            batch_mask = cand_indices.new_ones(bsz)\n            batch_mask[cand_indices.new(finalized_sents)] = 0\n            batch_idxs = torch.nonzero(batch_mask).squeeze(-1)\n            eos_mask = eos_mask[batch_idxs]\n            cand_beams = cand_beams[batch_idxs]\n            bbsz_offsets.resize_(new_bsz, 1)\n            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n            lm_lprobs_top_k = lm_lprobs_top_k[batch_idxs]\n            fw_lprobs_top_k = fw_lprobs_top_k[batch_idxs]\n            cand_indices = cand_indices[batch_idxs]\n            if prefix_tokens is not None:\n                prefix_tokens = prefix_tokens[batch_idxs]\n            src_lengths_no_eos = src_lengths_no_eos[batch_idxs]\n            cands_to_ignore = cands_to_ignore[batch_idxs]\n            scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            scores_buf.resize_as_(scores)\n            tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            tokens_buf.resize_as_(tokens)\n            src_tokens = src_tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            src_lengths = src_lengths.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            lm_prefix_scores = lm_prefix_scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1).squeeze()\n            if attn is not None:\n                attn = attn.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, attn.size(1), -1)\n                attn_buf.resize_as_(attn)\n            bsz = new_bsz\n        else:\n            batch_idxs = None\n        eos_mask[:, :beam_size] |= cands_to_ignore\n        active_mask = torch.add(eos_mask.type_as(cand_offsets) * cand_size, cand_offsets[:eos_mask.size(1)])\n        (active_hypos, new_cands_to_ignore) = (buffer('active_hypos'), buffer('new_cands_to_ignore'))\n        torch.topk(active_mask, k=beam_size, dim=1, largest=False, out=(new_cands_to_ignore, active_hypos))\n        cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]\n        assert (~cands_to_ignore).any(dim=1).all()\n        active_bbsz_idx = buffer('active_bbsz_idx')\n        torch.gather(cand_bbsz_idx, dim=1, index=active_hypos, out=active_bbsz_idx)\n        active_scores = torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores[:, step].view(bsz, beam_size))\n        active_bbsz_idx = active_bbsz_idx.view(-1)\n        active_scores = active_scores.view(-1)\n        torch.index_select(tokens[:, :step + 1], dim=0, index=active_bbsz_idx, out=tokens_buf[:, :step + 1])\n        torch.gather(cand_indices, dim=1, index=active_hypos, out=tokens_buf.view(bsz, beam_size, -1)[:, :, step + 1])\n        if step > 0:\n            torch.index_select(scores[:, :step], dim=0, index=active_bbsz_idx, out=scores_buf[:, :step])\n        torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores_buf.view(bsz, beam_size, -1)[:, :, step])\n        torch.gather(lm_lprobs_top_k, dim=1, index=active_hypos, out=lm_prefix_scores.view(bsz, beam_size))\n        if attn is not None:\n            torch.index_select(attn[:, :, :step + 2], dim=0, index=active_bbsz_idx, out=attn_buf[:, :, :step + 2])\n        (tokens, tokens_buf) = (tokens_buf, tokens)\n        (scores, scores_buf) = (scores_buf, scores)\n        if attn is not None:\n            (attn, attn_buf) = (attn_buf, attn)\n        reorder_state = active_bbsz_idx\n    for sent in range(len(finalized)):\n        finalized[sent] = sorted(finalized[sent], key=lambda r: r['score'], reverse=True)\n    return finalized",
        "mutated": [
            "@torch.no_grad()\ndef generate(self, models, sample, prefix_tokens=None, bos_token=None, **kwargs):\n    if False:\n        i = 10\n    'Generate a batch of translations.\\n        Args:\\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\\n            sample (dict): batch\\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\\n                with these tokens\\n        '\n    model = EnsembleModel(models)\n    incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(model.models_size)])\n    if not self.retain_dropout:\n        model.eval()\n    encoder_input = {k: v for (k, v) in sample['net_input'].items() if k != 'prev_output_tokens'}\n    src_tokens = encoder_input['src_tokens']\n    src_lengths_no_eos = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n    input_size = src_tokens.size()\n    bsz = input_size[0]\n    src_len = input_size[1]\n    beam_size = self.beam_size\n    if self.match_source_len:\n        max_len = src_lengths_no_eos.max().item()\n    else:\n        max_len = min(int(self.max_len_a * src_len + self.max_len_b), model.max_decoder_positions() - 1)\n    encoder_outs = model.forward_encoder(encoder_input)\n    new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n    new_order = new_order.to(src_tokens.device).long()\n    encoder_outs = model.reorder_encoder_out(encoder_outs, new_order)\n    src_lengths = encoder_input['src_lengths']\n    scores = src_tokens.new(bsz * beam_size, max_len + 1).float().fill_(0)\n    lm_prefix_scores = src_tokens.new(bsz * beam_size).float().fill_(0)\n    scores_buf = scores.clone()\n    tokens = src_tokens.new(bsz * beam_size, max_len + 2).long().fill_(self.pad)\n    tokens_buf = tokens.clone()\n    tokens[:, 0] = self.eos if bos_token is None else bos_token\n    src_tokens = reorder_all_tokens(src_tokens, src_lengths, self.src_dict.eos_index)\n    src_tokens = src_tokens.repeat(1, beam_size).view(-1, src_len)\n    src_lengths = src_lengths.view(bsz, -1).repeat(1, beam_size).view(bsz * beam_size, -1)\n    (attn, attn_buf) = (None, None)\n    nonpad_idxs = None\n    cands_to_ignore = src_tokens.new_zeros(bsz, beam_size).eq(-1)\n    finalized = [[] for i in range(bsz)]\n    finished = [False for i in range(bsz)]\n    num_remaining_sent = bsz\n    cand_size = 2 * beam_size\n    bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n    cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n    buffers = {}\n\n    def buffer(name, type_of=tokens):\n        if name not in buffers:\n            buffers[name] = type_of.new()\n        return buffers[name]\n\n    def is_finished(sent, step, unfin_idx):\n        \"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"\n        assert len(finalized[sent]) <= beam_size\n        if len(finalized[sent]) == beam_size:\n            return True\n        return False\n\n    def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n        \"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    fw scores for each hypothesis\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\n                    combined noisy channel scores for each hypothesis\n            \"\"\"\n        assert bbsz_idx.numel() == eos_scores.numel()\n        tokens_clone = tokens.index_select(0, bbsz_idx)\n        tokens_clone = tokens_clone[:, 1:step + 2]\n        assert not tokens_clone.eq(self.eos).any()\n        tokens_clone[:, step] = self.eos\n        attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n        pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n        pos_scores[:, step] = eos_scores\n        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n        if self.normalize_scores:\n            combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n        cum_unfin = []\n        prev = 0\n        for f in finished:\n            if f:\n                prev += 1\n            else:\n                cum_unfin.append(prev)\n        sents_seen = set()\n        for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n            unfin_idx = idx // beam_size\n            sent = unfin_idx + cum_unfin[unfin_idx]\n            sents_seen.add((sent, unfin_idx))\n            if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n                score = -math.inf\n\n            def get_hypo():\n                if attn_clone is not None:\n                    hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                    (_, alignment) = hypo_attn.max(dim=0)\n                else:\n                    hypo_attn = None\n                    alignment = None\n                return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n            if len(finalized[sent]) < beam_size:\n                finalized[sent].append(get_hypo())\n        newly_finished = []\n        for (sent, unfin_idx) in sents_seen:\n            if not finished[sent] and is_finished(sent, step, unfin_idx):\n                finished[sent] = True\n                newly_finished.append(unfin_idx)\n        return newly_finished\n\n    def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n        \"\"\"Rescore the top k hypothesis from each beam using noisy channel modeling\n            Returns:\n                new_fw_lprobs: the direct model probabilities after pruning the top k\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\n                new_lm_lprobs: the language model probabilities after pruning the top k\n            \"\"\"\n        with torch.no_grad():\n            lprobs_size = lprobs.size()\n            if prefix_tokens is not None and step < prefix_tokens.size(1):\n                probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n                cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n                cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n                fw_top_k = cand_scores\n                fw_top_k_idx = cand_indices\n                k = 1\n            else:\n                (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n            eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n            ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n            src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n            if self.combine_method != 'lm_only':\n                temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n                not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n                cur_tgt_size = step + 2\n                eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n                eos_tokens[eos_idx] = self.tgt_dict.pad_index\n                if step == 0:\n                    channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                else:\n                    channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n                ch_input_lengths[eos_idx] = cur_tgt_size - 1\n                if self.channel_scoring_type == 'unnormalized':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                    ch_intermed_scores = ch_intermed_scores.float()\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'k2_separate':\n                    for k_idx in range(k):\n                        k_eos_tokens = eos_tokens[k_idx::k, :]\n                        if step == 0:\n                            k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        else:\n                            k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                        k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                        k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                        k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                        k_ch_intermed_scores *= not_padding.float()\n                        ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'src_vocab':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                elif self.channel_scoring_type == 'src_vocab_batched':\n                    ch_bsz_size = temp_src_tokens_full.shape[0]\n                    ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                    for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                        end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                        temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                        channel_input_batch = channel_input[start_idx:end_idx, :]\n                        ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                        ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                        (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                        ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                    ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                else:\n                    ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                    ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                    ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            else:\n                cur_tgt_size = 0\n            ch_scores = ch_scores.view(bsz * beam_size, k)\n            expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n            if self.share_tgt_dict:\n                lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            else:\n                new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n                new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n                lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            lm_scores.add_(expanded_lm_prefix_scores)\n            ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n            new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_fw_lprobs[:, self.pad] = -math.inf\n            new_ch_lm_lprobs[:, self.pad] = -math.inf\n            new_lm_lprobs[:, self.pad] = -math.inf\n            new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n            new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n            new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n            return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)\n\n    def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n        if self.channel_scoring_type == 'unnormalized':\n            ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n        ch_scores = ch_scores * self.ch_weight\n        lm_scores1 = lm_scores1 * self.lm_weight\n        if combine_type == 'lm_only':\n            ch_scores = lm_scores1.view(ch_scores.size())\n        elif combine_type == 'noisy_channel':\n            if self.normalize_lm_scores_by_tgt_len:\n                ch_scores.div_(src_size)\n                lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n                ch_scores.add_(lm_scores_norm)\n            else:\n                ch_scores.add_(lm_scores1.view(ch_scores.size()))\n                ch_scores.div_(src_size)\n        return ch_scores\n    if self.channel_models is not None:\n        channel_model = self.channel_models[0]\n    else:\n        channel_model = None\n    lm = EnsembleModel(self.lm_models)\n    lm_incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(lm.models_size)])\n    reorder_state = None\n    batch_idxs = None\n    for step in range(max_len + 1):\n        if reorder_state is not None:\n            if batch_idxs is not None:\n                corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)\n                reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)\n            model.reorder_incremental_state(incremental_states, reorder_state)\n            encoder_outs = model.reorder_encoder_out(encoder_outs, reorder_state)\n            lm.reorder_incremental_state(lm_incremental_states, reorder_state)\n        (fw_lprobs, avg_attn_scores) = model.forward_decoder(tokens[:, :step + 1], encoder_outs, incremental_states, temperature=self.temperature)\n        fw_lprobs[:, self.pad] = -math.inf\n        fw_lprobs[:, self.unk] -= self.unk_penalty\n        (fw_lprobs, ch_lm_lprobs, lm_lprobs) = noisy_channel_rescoring(fw_lprobs, beam_size, bsz, src_tokens, tokens, self.k2)\n        if step >= max_len:\n            fw_lprobs[:, :self.eos] = -math.inf\n            fw_lprobs[:, self.eos + 1:] = -math.inf\n        elif step < self.min_len:\n            fw_lprobs[:, self.eos] = -math.inf\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)\n            prefix_mask = prefix_toks.ne(self.pad)\n            prefix_fw_lprobs = fw_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            fw_lprobs[prefix_mask] = -math.inf\n            fw_lprobs[prefix_mask] = fw_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_fw_lprobs)\n            prefix_ch_lm_lprobs = ch_lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            ch_lm_lprobs[prefix_mask] = -math.inf\n            ch_lm_lprobs[prefix_mask] = ch_lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_ch_lm_lprobs)\n            prefix_lm_lprobs = lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            lm_lprobs[prefix_mask] = -math.inf\n            lm_lprobs[prefix_mask] = lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lm_lprobs)\n            eos_mask = prefix_toks.eq(self.eos)\n            if eos_mask.any():\n                first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[:, 0, 1:step + 1]\n                eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]\n                target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]\n                assert (first_beam == target_prefix).all()\n\n                def replicate_first_beam(tensor, mask):\n                    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n                    tensor[mask] = tensor[mask][:, :1, :]\n                    return tensor.view(-1, tensor.size(-1))\n                tokens = replicate_first_beam(tokens, eos_mask_batch_dim)\n                scores = replicate_first_beam(scores, eos_mask_batch_dim)\n                fw_lprobs = replicate_first_beam(fw_lprobs, eos_mask_batch_dim)\n                ch_lm_lprobs = replicate_first_beam(ch_lm_lprobs, eos_mask_batch_dim)\n                lm_lprobs = replicate_first_beam(lm_lprobs, eos_mask_batch_dim)\n        if self.no_repeat_ngram_size > 0:\n            gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                gen_tokens = tokens[bbsz_idx].tolist()\n                for ngram in zip(*[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]):\n                    gen_ngrams[bbsz_idx][tuple(ngram[:-1])] = gen_ngrams[bbsz_idx].get(tuple(ngram[:-1]), []) + [ngram[-1]]\n        if avg_attn_scores is not None:\n            if attn is None:\n                attn = scores.new(bsz * beam_size, src_tokens.size(1), max_len + 2)\n                attn_buf = attn.clone()\n                nonpad_idxs = src_tokens.ne(self.pad)\n            attn[:, :, step + 1].copy_(avg_attn_scores)\n        scores = scores.type_as(fw_lprobs)\n        scores_buf = scores_buf.type_as(fw_lprobs)\n        self.search.set_src_lengths(src_lengths_no_eos)\n        if self.no_repeat_ngram_size > 0:\n\n            def calculate_banned_tokens(bbsz_idx):\n                ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n                return gen_ngrams[bbsz_idx].get(ngram_index, [])\n            if step + 2 - self.no_repeat_ngram_size >= 0:\n                banned_tokens = [calculate_banned_tokens(bbsz_idx) for bbsz_idx in range(bsz * beam_size)]\n            else:\n                banned_tokens = [[] for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                fw_lprobs[bbsz_idx, banned_tokens[bbsz_idx]] = -math.inf\n        (combined_noisy_channel_scores, fw_lprobs_top_k, lm_lprobs_top_k, cand_indices, cand_beams) = self.search.step(step, fw_lprobs.view(bsz, -1, self.vocab_size), scores.view(bsz, beam_size, -1)[:, :, :step], ch_lm_lprobs.view(bsz, -1, self.vocab_size), lm_lprobs.view(bsz, -1, self.vocab_size), self.combine_method)\n        cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n        eos_mask = cand_indices.eq(self.eos)\n        eos_mask[:, :beam_size] &= ~cands_to_ignore\n        eos_bbsz_idx = torch.masked_select(cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size])\n        finalized_sents = set()\n        if eos_bbsz_idx.numel() > 0:\n            eos_scores = torch.masked_select(fw_lprobs_top_k[:, :beam_size], mask=eos_mask[:, :beam_size])\n            combined_noisy_channel_eos_scores = torch.masked_select(combined_noisy_channel_scores[:, :beam_size], mask=eos_mask[:, :beam_size])\n            finalized_sents = finalize_hypos(step, eos_bbsz_idx, eos_scores, combined_noisy_channel_eos_scores)\n            num_remaining_sent -= len(finalized_sents)\n        assert num_remaining_sent >= 0\n        if num_remaining_sent == 0:\n            break\n        if len(finalized_sents) > 0:\n            new_bsz = bsz - len(finalized_sents)\n            batch_mask = cand_indices.new_ones(bsz)\n            batch_mask[cand_indices.new(finalized_sents)] = 0\n            batch_idxs = torch.nonzero(batch_mask).squeeze(-1)\n            eos_mask = eos_mask[batch_idxs]\n            cand_beams = cand_beams[batch_idxs]\n            bbsz_offsets.resize_(new_bsz, 1)\n            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n            lm_lprobs_top_k = lm_lprobs_top_k[batch_idxs]\n            fw_lprobs_top_k = fw_lprobs_top_k[batch_idxs]\n            cand_indices = cand_indices[batch_idxs]\n            if prefix_tokens is not None:\n                prefix_tokens = prefix_tokens[batch_idxs]\n            src_lengths_no_eos = src_lengths_no_eos[batch_idxs]\n            cands_to_ignore = cands_to_ignore[batch_idxs]\n            scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            scores_buf.resize_as_(scores)\n            tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            tokens_buf.resize_as_(tokens)\n            src_tokens = src_tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            src_lengths = src_lengths.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            lm_prefix_scores = lm_prefix_scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1).squeeze()\n            if attn is not None:\n                attn = attn.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, attn.size(1), -1)\n                attn_buf.resize_as_(attn)\n            bsz = new_bsz\n        else:\n            batch_idxs = None\n        eos_mask[:, :beam_size] |= cands_to_ignore\n        active_mask = torch.add(eos_mask.type_as(cand_offsets) * cand_size, cand_offsets[:eos_mask.size(1)])\n        (active_hypos, new_cands_to_ignore) = (buffer('active_hypos'), buffer('new_cands_to_ignore'))\n        torch.topk(active_mask, k=beam_size, dim=1, largest=False, out=(new_cands_to_ignore, active_hypos))\n        cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]\n        assert (~cands_to_ignore).any(dim=1).all()\n        active_bbsz_idx = buffer('active_bbsz_idx')\n        torch.gather(cand_bbsz_idx, dim=1, index=active_hypos, out=active_bbsz_idx)\n        active_scores = torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores[:, step].view(bsz, beam_size))\n        active_bbsz_idx = active_bbsz_idx.view(-1)\n        active_scores = active_scores.view(-1)\n        torch.index_select(tokens[:, :step + 1], dim=0, index=active_bbsz_idx, out=tokens_buf[:, :step + 1])\n        torch.gather(cand_indices, dim=1, index=active_hypos, out=tokens_buf.view(bsz, beam_size, -1)[:, :, step + 1])\n        if step > 0:\n            torch.index_select(scores[:, :step], dim=0, index=active_bbsz_idx, out=scores_buf[:, :step])\n        torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores_buf.view(bsz, beam_size, -1)[:, :, step])\n        torch.gather(lm_lprobs_top_k, dim=1, index=active_hypos, out=lm_prefix_scores.view(bsz, beam_size))\n        if attn is not None:\n            torch.index_select(attn[:, :, :step + 2], dim=0, index=active_bbsz_idx, out=attn_buf[:, :, :step + 2])\n        (tokens, tokens_buf) = (tokens_buf, tokens)\n        (scores, scores_buf) = (scores_buf, scores)\n        if attn is not None:\n            (attn, attn_buf) = (attn_buf, attn)\n        reorder_state = active_bbsz_idx\n    for sent in range(len(finalized)):\n        finalized[sent] = sorted(finalized[sent], key=lambda r: r['score'], reverse=True)\n    return finalized",
            "@torch.no_grad()\ndef generate(self, models, sample, prefix_tokens=None, bos_token=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generate a batch of translations.\\n        Args:\\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\\n            sample (dict): batch\\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\\n                with these tokens\\n        '\n    model = EnsembleModel(models)\n    incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(model.models_size)])\n    if not self.retain_dropout:\n        model.eval()\n    encoder_input = {k: v for (k, v) in sample['net_input'].items() if k != 'prev_output_tokens'}\n    src_tokens = encoder_input['src_tokens']\n    src_lengths_no_eos = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n    input_size = src_tokens.size()\n    bsz = input_size[0]\n    src_len = input_size[1]\n    beam_size = self.beam_size\n    if self.match_source_len:\n        max_len = src_lengths_no_eos.max().item()\n    else:\n        max_len = min(int(self.max_len_a * src_len + self.max_len_b), model.max_decoder_positions() - 1)\n    encoder_outs = model.forward_encoder(encoder_input)\n    new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n    new_order = new_order.to(src_tokens.device).long()\n    encoder_outs = model.reorder_encoder_out(encoder_outs, new_order)\n    src_lengths = encoder_input['src_lengths']\n    scores = src_tokens.new(bsz * beam_size, max_len + 1).float().fill_(0)\n    lm_prefix_scores = src_tokens.new(bsz * beam_size).float().fill_(0)\n    scores_buf = scores.clone()\n    tokens = src_tokens.new(bsz * beam_size, max_len + 2).long().fill_(self.pad)\n    tokens_buf = tokens.clone()\n    tokens[:, 0] = self.eos if bos_token is None else bos_token\n    src_tokens = reorder_all_tokens(src_tokens, src_lengths, self.src_dict.eos_index)\n    src_tokens = src_tokens.repeat(1, beam_size).view(-1, src_len)\n    src_lengths = src_lengths.view(bsz, -1).repeat(1, beam_size).view(bsz * beam_size, -1)\n    (attn, attn_buf) = (None, None)\n    nonpad_idxs = None\n    cands_to_ignore = src_tokens.new_zeros(bsz, beam_size).eq(-1)\n    finalized = [[] for i in range(bsz)]\n    finished = [False for i in range(bsz)]\n    num_remaining_sent = bsz\n    cand_size = 2 * beam_size\n    bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n    cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n    buffers = {}\n\n    def buffer(name, type_of=tokens):\n        if name not in buffers:\n            buffers[name] = type_of.new()\n        return buffers[name]\n\n    def is_finished(sent, step, unfin_idx):\n        \"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"\n        assert len(finalized[sent]) <= beam_size\n        if len(finalized[sent]) == beam_size:\n            return True\n        return False\n\n    def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n        \"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    fw scores for each hypothesis\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\n                    combined noisy channel scores for each hypothesis\n            \"\"\"\n        assert bbsz_idx.numel() == eos_scores.numel()\n        tokens_clone = tokens.index_select(0, bbsz_idx)\n        tokens_clone = tokens_clone[:, 1:step + 2]\n        assert not tokens_clone.eq(self.eos).any()\n        tokens_clone[:, step] = self.eos\n        attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n        pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n        pos_scores[:, step] = eos_scores\n        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n        if self.normalize_scores:\n            combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n        cum_unfin = []\n        prev = 0\n        for f in finished:\n            if f:\n                prev += 1\n            else:\n                cum_unfin.append(prev)\n        sents_seen = set()\n        for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n            unfin_idx = idx // beam_size\n            sent = unfin_idx + cum_unfin[unfin_idx]\n            sents_seen.add((sent, unfin_idx))\n            if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n                score = -math.inf\n\n            def get_hypo():\n                if attn_clone is not None:\n                    hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                    (_, alignment) = hypo_attn.max(dim=0)\n                else:\n                    hypo_attn = None\n                    alignment = None\n                return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n            if len(finalized[sent]) < beam_size:\n                finalized[sent].append(get_hypo())\n        newly_finished = []\n        for (sent, unfin_idx) in sents_seen:\n            if not finished[sent] and is_finished(sent, step, unfin_idx):\n                finished[sent] = True\n                newly_finished.append(unfin_idx)\n        return newly_finished\n\n    def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n        \"\"\"Rescore the top k hypothesis from each beam using noisy channel modeling\n            Returns:\n                new_fw_lprobs: the direct model probabilities after pruning the top k\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\n                new_lm_lprobs: the language model probabilities after pruning the top k\n            \"\"\"\n        with torch.no_grad():\n            lprobs_size = lprobs.size()\n            if prefix_tokens is not None and step < prefix_tokens.size(1):\n                probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n                cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n                cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n                fw_top_k = cand_scores\n                fw_top_k_idx = cand_indices\n                k = 1\n            else:\n                (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n            eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n            ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n            src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n            if self.combine_method != 'lm_only':\n                temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n                not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n                cur_tgt_size = step + 2\n                eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n                eos_tokens[eos_idx] = self.tgt_dict.pad_index\n                if step == 0:\n                    channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                else:\n                    channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n                ch_input_lengths[eos_idx] = cur_tgt_size - 1\n                if self.channel_scoring_type == 'unnormalized':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                    ch_intermed_scores = ch_intermed_scores.float()\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'k2_separate':\n                    for k_idx in range(k):\n                        k_eos_tokens = eos_tokens[k_idx::k, :]\n                        if step == 0:\n                            k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        else:\n                            k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                        k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                        k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                        k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                        k_ch_intermed_scores *= not_padding.float()\n                        ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'src_vocab':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                elif self.channel_scoring_type == 'src_vocab_batched':\n                    ch_bsz_size = temp_src_tokens_full.shape[0]\n                    ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                    for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                        end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                        temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                        channel_input_batch = channel_input[start_idx:end_idx, :]\n                        ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                        ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                        (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                        ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                    ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                else:\n                    ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                    ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                    ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            else:\n                cur_tgt_size = 0\n            ch_scores = ch_scores.view(bsz * beam_size, k)\n            expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n            if self.share_tgt_dict:\n                lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            else:\n                new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n                new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n                lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            lm_scores.add_(expanded_lm_prefix_scores)\n            ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n            new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_fw_lprobs[:, self.pad] = -math.inf\n            new_ch_lm_lprobs[:, self.pad] = -math.inf\n            new_lm_lprobs[:, self.pad] = -math.inf\n            new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n            new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n            new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n            return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)\n\n    def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n        if self.channel_scoring_type == 'unnormalized':\n            ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n        ch_scores = ch_scores * self.ch_weight\n        lm_scores1 = lm_scores1 * self.lm_weight\n        if combine_type == 'lm_only':\n            ch_scores = lm_scores1.view(ch_scores.size())\n        elif combine_type == 'noisy_channel':\n            if self.normalize_lm_scores_by_tgt_len:\n                ch_scores.div_(src_size)\n                lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n                ch_scores.add_(lm_scores_norm)\n            else:\n                ch_scores.add_(lm_scores1.view(ch_scores.size()))\n                ch_scores.div_(src_size)\n        return ch_scores\n    if self.channel_models is not None:\n        channel_model = self.channel_models[0]\n    else:\n        channel_model = None\n    lm = EnsembleModel(self.lm_models)\n    lm_incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(lm.models_size)])\n    reorder_state = None\n    batch_idxs = None\n    for step in range(max_len + 1):\n        if reorder_state is not None:\n            if batch_idxs is not None:\n                corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)\n                reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)\n            model.reorder_incremental_state(incremental_states, reorder_state)\n            encoder_outs = model.reorder_encoder_out(encoder_outs, reorder_state)\n            lm.reorder_incremental_state(lm_incremental_states, reorder_state)\n        (fw_lprobs, avg_attn_scores) = model.forward_decoder(tokens[:, :step + 1], encoder_outs, incremental_states, temperature=self.temperature)\n        fw_lprobs[:, self.pad] = -math.inf\n        fw_lprobs[:, self.unk] -= self.unk_penalty\n        (fw_lprobs, ch_lm_lprobs, lm_lprobs) = noisy_channel_rescoring(fw_lprobs, beam_size, bsz, src_tokens, tokens, self.k2)\n        if step >= max_len:\n            fw_lprobs[:, :self.eos] = -math.inf\n            fw_lprobs[:, self.eos + 1:] = -math.inf\n        elif step < self.min_len:\n            fw_lprobs[:, self.eos] = -math.inf\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)\n            prefix_mask = prefix_toks.ne(self.pad)\n            prefix_fw_lprobs = fw_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            fw_lprobs[prefix_mask] = -math.inf\n            fw_lprobs[prefix_mask] = fw_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_fw_lprobs)\n            prefix_ch_lm_lprobs = ch_lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            ch_lm_lprobs[prefix_mask] = -math.inf\n            ch_lm_lprobs[prefix_mask] = ch_lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_ch_lm_lprobs)\n            prefix_lm_lprobs = lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            lm_lprobs[prefix_mask] = -math.inf\n            lm_lprobs[prefix_mask] = lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lm_lprobs)\n            eos_mask = prefix_toks.eq(self.eos)\n            if eos_mask.any():\n                first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[:, 0, 1:step + 1]\n                eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]\n                target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]\n                assert (first_beam == target_prefix).all()\n\n                def replicate_first_beam(tensor, mask):\n                    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n                    tensor[mask] = tensor[mask][:, :1, :]\n                    return tensor.view(-1, tensor.size(-1))\n                tokens = replicate_first_beam(tokens, eos_mask_batch_dim)\n                scores = replicate_first_beam(scores, eos_mask_batch_dim)\n                fw_lprobs = replicate_first_beam(fw_lprobs, eos_mask_batch_dim)\n                ch_lm_lprobs = replicate_first_beam(ch_lm_lprobs, eos_mask_batch_dim)\n                lm_lprobs = replicate_first_beam(lm_lprobs, eos_mask_batch_dim)\n        if self.no_repeat_ngram_size > 0:\n            gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                gen_tokens = tokens[bbsz_idx].tolist()\n                for ngram in zip(*[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]):\n                    gen_ngrams[bbsz_idx][tuple(ngram[:-1])] = gen_ngrams[bbsz_idx].get(tuple(ngram[:-1]), []) + [ngram[-1]]\n        if avg_attn_scores is not None:\n            if attn is None:\n                attn = scores.new(bsz * beam_size, src_tokens.size(1), max_len + 2)\n                attn_buf = attn.clone()\n                nonpad_idxs = src_tokens.ne(self.pad)\n            attn[:, :, step + 1].copy_(avg_attn_scores)\n        scores = scores.type_as(fw_lprobs)\n        scores_buf = scores_buf.type_as(fw_lprobs)\n        self.search.set_src_lengths(src_lengths_no_eos)\n        if self.no_repeat_ngram_size > 0:\n\n            def calculate_banned_tokens(bbsz_idx):\n                ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n                return gen_ngrams[bbsz_idx].get(ngram_index, [])\n            if step + 2 - self.no_repeat_ngram_size >= 0:\n                banned_tokens = [calculate_banned_tokens(bbsz_idx) for bbsz_idx in range(bsz * beam_size)]\n            else:\n                banned_tokens = [[] for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                fw_lprobs[bbsz_idx, banned_tokens[bbsz_idx]] = -math.inf\n        (combined_noisy_channel_scores, fw_lprobs_top_k, lm_lprobs_top_k, cand_indices, cand_beams) = self.search.step(step, fw_lprobs.view(bsz, -1, self.vocab_size), scores.view(bsz, beam_size, -1)[:, :, :step], ch_lm_lprobs.view(bsz, -1, self.vocab_size), lm_lprobs.view(bsz, -1, self.vocab_size), self.combine_method)\n        cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n        eos_mask = cand_indices.eq(self.eos)\n        eos_mask[:, :beam_size] &= ~cands_to_ignore\n        eos_bbsz_idx = torch.masked_select(cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size])\n        finalized_sents = set()\n        if eos_bbsz_idx.numel() > 0:\n            eos_scores = torch.masked_select(fw_lprobs_top_k[:, :beam_size], mask=eos_mask[:, :beam_size])\n            combined_noisy_channel_eos_scores = torch.masked_select(combined_noisy_channel_scores[:, :beam_size], mask=eos_mask[:, :beam_size])\n            finalized_sents = finalize_hypos(step, eos_bbsz_idx, eos_scores, combined_noisy_channel_eos_scores)\n            num_remaining_sent -= len(finalized_sents)\n        assert num_remaining_sent >= 0\n        if num_remaining_sent == 0:\n            break\n        if len(finalized_sents) > 0:\n            new_bsz = bsz - len(finalized_sents)\n            batch_mask = cand_indices.new_ones(bsz)\n            batch_mask[cand_indices.new(finalized_sents)] = 0\n            batch_idxs = torch.nonzero(batch_mask).squeeze(-1)\n            eos_mask = eos_mask[batch_idxs]\n            cand_beams = cand_beams[batch_idxs]\n            bbsz_offsets.resize_(new_bsz, 1)\n            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n            lm_lprobs_top_k = lm_lprobs_top_k[batch_idxs]\n            fw_lprobs_top_k = fw_lprobs_top_k[batch_idxs]\n            cand_indices = cand_indices[batch_idxs]\n            if prefix_tokens is not None:\n                prefix_tokens = prefix_tokens[batch_idxs]\n            src_lengths_no_eos = src_lengths_no_eos[batch_idxs]\n            cands_to_ignore = cands_to_ignore[batch_idxs]\n            scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            scores_buf.resize_as_(scores)\n            tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            tokens_buf.resize_as_(tokens)\n            src_tokens = src_tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            src_lengths = src_lengths.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            lm_prefix_scores = lm_prefix_scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1).squeeze()\n            if attn is not None:\n                attn = attn.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, attn.size(1), -1)\n                attn_buf.resize_as_(attn)\n            bsz = new_bsz\n        else:\n            batch_idxs = None\n        eos_mask[:, :beam_size] |= cands_to_ignore\n        active_mask = torch.add(eos_mask.type_as(cand_offsets) * cand_size, cand_offsets[:eos_mask.size(1)])\n        (active_hypos, new_cands_to_ignore) = (buffer('active_hypos'), buffer('new_cands_to_ignore'))\n        torch.topk(active_mask, k=beam_size, dim=1, largest=False, out=(new_cands_to_ignore, active_hypos))\n        cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]\n        assert (~cands_to_ignore).any(dim=1).all()\n        active_bbsz_idx = buffer('active_bbsz_idx')\n        torch.gather(cand_bbsz_idx, dim=1, index=active_hypos, out=active_bbsz_idx)\n        active_scores = torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores[:, step].view(bsz, beam_size))\n        active_bbsz_idx = active_bbsz_idx.view(-1)\n        active_scores = active_scores.view(-1)\n        torch.index_select(tokens[:, :step + 1], dim=0, index=active_bbsz_idx, out=tokens_buf[:, :step + 1])\n        torch.gather(cand_indices, dim=1, index=active_hypos, out=tokens_buf.view(bsz, beam_size, -1)[:, :, step + 1])\n        if step > 0:\n            torch.index_select(scores[:, :step], dim=0, index=active_bbsz_idx, out=scores_buf[:, :step])\n        torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores_buf.view(bsz, beam_size, -1)[:, :, step])\n        torch.gather(lm_lprobs_top_k, dim=1, index=active_hypos, out=lm_prefix_scores.view(bsz, beam_size))\n        if attn is not None:\n            torch.index_select(attn[:, :, :step + 2], dim=0, index=active_bbsz_idx, out=attn_buf[:, :, :step + 2])\n        (tokens, tokens_buf) = (tokens_buf, tokens)\n        (scores, scores_buf) = (scores_buf, scores)\n        if attn is not None:\n            (attn, attn_buf) = (attn_buf, attn)\n        reorder_state = active_bbsz_idx\n    for sent in range(len(finalized)):\n        finalized[sent] = sorted(finalized[sent], key=lambda r: r['score'], reverse=True)\n    return finalized",
            "@torch.no_grad()\ndef generate(self, models, sample, prefix_tokens=None, bos_token=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generate a batch of translations.\\n        Args:\\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\\n            sample (dict): batch\\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\\n                with these tokens\\n        '\n    model = EnsembleModel(models)\n    incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(model.models_size)])\n    if not self.retain_dropout:\n        model.eval()\n    encoder_input = {k: v for (k, v) in sample['net_input'].items() if k != 'prev_output_tokens'}\n    src_tokens = encoder_input['src_tokens']\n    src_lengths_no_eos = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n    input_size = src_tokens.size()\n    bsz = input_size[0]\n    src_len = input_size[1]\n    beam_size = self.beam_size\n    if self.match_source_len:\n        max_len = src_lengths_no_eos.max().item()\n    else:\n        max_len = min(int(self.max_len_a * src_len + self.max_len_b), model.max_decoder_positions() - 1)\n    encoder_outs = model.forward_encoder(encoder_input)\n    new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n    new_order = new_order.to(src_tokens.device).long()\n    encoder_outs = model.reorder_encoder_out(encoder_outs, new_order)\n    src_lengths = encoder_input['src_lengths']\n    scores = src_tokens.new(bsz * beam_size, max_len + 1).float().fill_(0)\n    lm_prefix_scores = src_tokens.new(bsz * beam_size).float().fill_(0)\n    scores_buf = scores.clone()\n    tokens = src_tokens.new(bsz * beam_size, max_len + 2).long().fill_(self.pad)\n    tokens_buf = tokens.clone()\n    tokens[:, 0] = self.eos if bos_token is None else bos_token\n    src_tokens = reorder_all_tokens(src_tokens, src_lengths, self.src_dict.eos_index)\n    src_tokens = src_tokens.repeat(1, beam_size).view(-1, src_len)\n    src_lengths = src_lengths.view(bsz, -1).repeat(1, beam_size).view(bsz * beam_size, -1)\n    (attn, attn_buf) = (None, None)\n    nonpad_idxs = None\n    cands_to_ignore = src_tokens.new_zeros(bsz, beam_size).eq(-1)\n    finalized = [[] for i in range(bsz)]\n    finished = [False for i in range(bsz)]\n    num_remaining_sent = bsz\n    cand_size = 2 * beam_size\n    bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n    cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n    buffers = {}\n\n    def buffer(name, type_of=tokens):\n        if name not in buffers:\n            buffers[name] = type_of.new()\n        return buffers[name]\n\n    def is_finished(sent, step, unfin_idx):\n        \"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"\n        assert len(finalized[sent]) <= beam_size\n        if len(finalized[sent]) == beam_size:\n            return True\n        return False\n\n    def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n        \"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    fw scores for each hypothesis\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\n                    combined noisy channel scores for each hypothesis\n            \"\"\"\n        assert bbsz_idx.numel() == eos_scores.numel()\n        tokens_clone = tokens.index_select(0, bbsz_idx)\n        tokens_clone = tokens_clone[:, 1:step + 2]\n        assert not tokens_clone.eq(self.eos).any()\n        tokens_clone[:, step] = self.eos\n        attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n        pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n        pos_scores[:, step] = eos_scores\n        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n        if self.normalize_scores:\n            combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n        cum_unfin = []\n        prev = 0\n        for f in finished:\n            if f:\n                prev += 1\n            else:\n                cum_unfin.append(prev)\n        sents_seen = set()\n        for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n            unfin_idx = idx // beam_size\n            sent = unfin_idx + cum_unfin[unfin_idx]\n            sents_seen.add((sent, unfin_idx))\n            if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n                score = -math.inf\n\n            def get_hypo():\n                if attn_clone is not None:\n                    hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                    (_, alignment) = hypo_attn.max(dim=0)\n                else:\n                    hypo_attn = None\n                    alignment = None\n                return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n            if len(finalized[sent]) < beam_size:\n                finalized[sent].append(get_hypo())\n        newly_finished = []\n        for (sent, unfin_idx) in sents_seen:\n            if not finished[sent] and is_finished(sent, step, unfin_idx):\n                finished[sent] = True\n                newly_finished.append(unfin_idx)\n        return newly_finished\n\n    def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n        \"\"\"Rescore the top k hypothesis from each beam using noisy channel modeling\n            Returns:\n                new_fw_lprobs: the direct model probabilities after pruning the top k\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\n                new_lm_lprobs: the language model probabilities after pruning the top k\n            \"\"\"\n        with torch.no_grad():\n            lprobs_size = lprobs.size()\n            if prefix_tokens is not None and step < prefix_tokens.size(1):\n                probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n                cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n                cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n                fw_top_k = cand_scores\n                fw_top_k_idx = cand_indices\n                k = 1\n            else:\n                (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n            eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n            ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n            src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n            if self.combine_method != 'lm_only':\n                temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n                not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n                cur_tgt_size = step + 2\n                eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n                eos_tokens[eos_idx] = self.tgt_dict.pad_index\n                if step == 0:\n                    channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                else:\n                    channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n                ch_input_lengths[eos_idx] = cur_tgt_size - 1\n                if self.channel_scoring_type == 'unnormalized':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                    ch_intermed_scores = ch_intermed_scores.float()\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'k2_separate':\n                    for k_idx in range(k):\n                        k_eos_tokens = eos_tokens[k_idx::k, :]\n                        if step == 0:\n                            k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        else:\n                            k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                        k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                        k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                        k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                        k_ch_intermed_scores *= not_padding.float()\n                        ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'src_vocab':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                elif self.channel_scoring_type == 'src_vocab_batched':\n                    ch_bsz_size = temp_src_tokens_full.shape[0]\n                    ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                    for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                        end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                        temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                        channel_input_batch = channel_input[start_idx:end_idx, :]\n                        ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                        ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                        (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                        ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                    ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                else:\n                    ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                    ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                    ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            else:\n                cur_tgt_size = 0\n            ch_scores = ch_scores.view(bsz * beam_size, k)\n            expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n            if self.share_tgt_dict:\n                lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            else:\n                new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n                new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n                lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            lm_scores.add_(expanded_lm_prefix_scores)\n            ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n            new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_fw_lprobs[:, self.pad] = -math.inf\n            new_ch_lm_lprobs[:, self.pad] = -math.inf\n            new_lm_lprobs[:, self.pad] = -math.inf\n            new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n            new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n            new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n            return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)\n\n    def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n        if self.channel_scoring_type == 'unnormalized':\n            ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n        ch_scores = ch_scores * self.ch_weight\n        lm_scores1 = lm_scores1 * self.lm_weight\n        if combine_type == 'lm_only':\n            ch_scores = lm_scores1.view(ch_scores.size())\n        elif combine_type == 'noisy_channel':\n            if self.normalize_lm_scores_by_tgt_len:\n                ch_scores.div_(src_size)\n                lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n                ch_scores.add_(lm_scores_norm)\n            else:\n                ch_scores.add_(lm_scores1.view(ch_scores.size()))\n                ch_scores.div_(src_size)\n        return ch_scores\n    if self.channel_models is not None:\n        channel_model = self.channel_models[0]\n    else:\n        channel_model = None\n    lm = EnsembleModel(self.lm_models)\n    lm_incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(lm.models_size)])\n    reorder_state = None\n    batch_idxs = None\n    for step in range(max_len + 1):\n        if reorder_state is not None:\n            if batch_idxs is not None:\n                corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)\n                reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)\n            model.reorder_incremental_state(incremental_states, reorder_state)\n            encoder_outs = model.reorder_encoder_out(encoder_outs, reorder_state)\n            lm.reorder_incremental_state(lm_incremental_states, reorder_state)\n        (fw_lprobs, avg_attn_scores) = model.forward_decoder(tokens[:, :step + 1], encoder_outs, incremental_states, temperature=self.temperature)\n        fw_lprobs[:, self.pad] = -math.inf\n        fw_lprobs[:, self.unk] -= self.unk_penalty\n        (fw_lprobs, ch_lm_lprobs, lm_lprobs) = noisy_channel_rescoring(fw_lprobs, beam_size, bsz, src_tokens, tokens, self.k2)\n        if step >= max_len:\n            fw_lprobs[:, :self.eos] = -math.inf\n            fw_lprobs[:, self.eos + 1:] = -math.inf\n        elif step < self.min_len:\n            fw_lprobs[:, self.eos] = -math.inf\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)\n            prefix_mask = prefix_toks.ne(self.pad)\n            prefix_fw_lprobs = fw_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            fw_lprobs[prefix_mask] = -math.inf\n            fw_lprobs[prefix_mask] = fw_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_fw_lprobs)\n            prefix_ch_lm_lprobs = ch_lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            ch_lm_lprobs[prefix_mask] = -math.inf\n            ch_lm_lprobs[prefix_mask] = ch_lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_ch_lm_lprobs)\n            prefix_lm_lprobs = lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            lm_lprobs[prefix_mask] = -math.inf\n            lm_lprobs[prefix_mask] = lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lm_lprobs)\n            eos_mask = prefix_toks.eq(self.eos)\n            if eos_mask.any():\n                first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[:, 0, 1:step + 1]\n                eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]\n                target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]\n                assert (first_beam == target_prefix).all()\n\n                def replicate_first_beam(tensor, mask):\n                    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n                    tensor[mask] = tensor[mask][:, :1, :]\n                    return tensor.view(-1, tensor.size(-1))\n                tokens = replicate_first_beam(tokens, eos_mask_batch_dim)\n                scores = replicate_first_beam(scores, eos_mask_batch_dim)\n                fw_lprobs = replicate_first_beam(fw_lprobs, eos_mask_batch_dim)\n                ch_lm_lprobs = replicate_first_beam(ch_lm_lprobs, eos_mask_batch_dim)\n                lm_lprobs = replicate_first_beam(lm_lprobs, eos_mask_batch_dim)\n        if self.no_repeat_ngram_size > 0:\n            gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                gen_tokens = tokens[bbsz_idx].tolist()\n                for ngram in zip(*[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]):\n                    gen_ngrams[bbsz_idx][tuple(ngram[:-1])] = gen_ngrams[bbsz_idx].get(tuple(ngram[:-1]), []) + [ngram[-1]]\n        if avg_attn_scores is not None:\n            if attn is None:\n                attn = scores.new(bsz * beam_size, src_tokens.size(1), max_len + 2)\n                attn_buf = attn.clone()\n                nonpad_idxs = src_tokens.ne(self.pad)\n            attn[:, :, step + 1].copy_(avg_attn_scores)\n        scores = scores.type_as(fw_lprobs)\n        scores_buf = scores_buf.type_as(fw_lprobs)\n        self.search.set_src_lengths(src_lengths_no_eos)\n        if self.no_repeat_ngram_size > 0:\n\n            def calculate_banned_tokens(bbsz_idx):\n                ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n                return gen_ngrams[bbsz_idx].get(ngram_index, [])\n            if step + 2 - self.no_repeat_ngram_size >= 0:\n                banned_tokens = [calculate_banned_tokens(bbsz_idx) for bbsz_idx in range(bsz * beam_size)]\n            else:\n                banned_tokens = [[] for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                fw_lprobs[bbsz_idx, banned_tokens[bbsz_idx]] = -math.inf\n        (combined_noisy_channel_scores, fw_lprobs_top_k, lm_lprobs_top_k, cand_indices, cand_beams) = self.search.step(step, fw_lprobs.view(bsz, -1, self.vocab_size), scores.view(bsz, beam_size, -1)[:, :, :step], ch_lm_lprobs.view(bsz, -1, self.vocab_size), lm_lprobs.view(bsz, -1, self.vocab_size), self.combine_method)\n        cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n        eos_mask = cand_indices.eq(self.eos)\n        eos_mask[:, :beam_size] &= ~cands_to_ignore\n        eos_bbsz_idx = torch.masked_select(cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size])\n        finalized_sents = set()\n        if eos_bbsz_idx.numel() > 0:\n            eos_scores = torch.masked_select(fw_lprobs_top_k[:, :beam_size], mask=eos_mask[:, :beam_size])\n            combined_noisy_channel_eos_scores = torch.masked_select(combined_noisy_channel_scores[:, :beam_size], mask=eos_mask[:, :beam_size])\n            finalized_sents = finalize_hypos(step, eos_bbsz_idx, eos_scores, combined_noisy_channel_eos_scores)\n            num_remaining_sent -= len(finalized_sents)\n        assert num_remaining_sent >= 0\n        if num_remaining_sent == 0:\n            break\n        if len(finalized_sents) > 0:\n            new_bsz = bsz - len(finalized_sents)\n            batch_mask = cand_indices.new_ones(bsz)\n            batch_mask[cand_indices.new(finalized_sents)] = 0\n            batch_idxs = torch.nonzero(batch_mask).squeeze(-1)\n            eos_mask = eos_mask[batch_idxs]\n            cand_beams = cand_beams[batch_idxs]\n            bbsz_offsets.resize_(new_bsz, 1)\n            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n            lm_lprobs_top_k = lm_lprobs_top_k[batch_idxs]\n            fw_lprobs_top_k = fw_lprobs_top_k[batch_idxs]\n            cand_indices = cand_indices[batch_idxs]\n            if prefix_tokens is not None:\n                prefix_tokens = prefix_tokens[batch_idxs]\n            src_lengths_no_eos = src_lengths_no_eos[batch_idxs]\n            cands_to_ignore = cands_to_ignore[batch_idxs]\n            scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            scores_buf.resize_as_(scores)\n            tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            tokens_buf.resize_as_(tokens)\n            src_tokens = src_tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            src_lengths = src_lengths.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            lm_prefix_scores = lm_prefix_scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1).squeeze()\n            if attn is not None:\n                attn = attn.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, attn.size(1), -1)\n                attn_buf.resize_as_(attn)\n            bsz = new_bsz\n        else:\n            batch_idxs = None\n        eos_mask[:, :beam_size] |= cands_to_ignore\n        active_mask = torch.add(eos_mask.type_as(cand_offsets) * cand_size, cand_offsets[:eos_mask.size(1)])\n        (active_hypos, new_cands_to_ignore) = (buffer('active_hypos'), buffer('new_cands_to_ignore'))\n        torch.topk(active_mask, k=beam_size, dim=1, largest=False, out=(new_cands_to_ignore, active_hypos))\n        cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]\n        assert (~cands_to_ignore).any(dim=1).all()\n        active_bbsz_idx = buffer('active_bbsz_idx')\n        torch.gather(cand_bbsz_idx, dim=1, index=active_hypos, out=active_bbsz_idx)\n        active_scores = torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores[:, step].view(bsz, beam_size))\n        active_bbsz_idx = active_bbsz_idx.view(-1)\n        active_scores = active_scores.view(-1)\n        torch.index_select(tokens[:, :step + 1], dim=0, index=active_bbsz_idx, out=tokens_buf[:, :step + 1])\n        torch.gather(cand_indices, dim=1, index=active_hypos, out=tokens_buf.view(bsz, beam_size, -1)[:, :, step + 1])\n        if step > 0:\n            torch.index_select(scores[:, :step], dim=0, index=active_bbsz_idx, out=scores_buf[:, :step])\n        torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores_buf.view(bsz, beam_size, -1)[:, :, step])\n        torch.gather(lm_lprobs_top_k, dim=1, index=active_hypos, out=lm_prefix_scores.view(bsz, beam_size))\n        if attn is not None:\n            torch.index_select(attn[:, :, :step + 2], dim=0, index=active_bbsz_idx, out=attn_buf[:, :, :step + 2])\n        (tokens, tokens_buf) = (tokens_buf, tokens)\n        (scores, scores_buf) = (scores_buf, scores)\n        if attn is not None:\n            (attn, attn_buf) = (attn_buf, attn)\n        reorder_state = active_bbsz_idx\n    for sent in range(len(finalized)):\n        finalized[sent] = sorted(finalized[sent], key=lambda r: r['score'], reverse=True)\n    return finalized",
            "@torch.no_grad()\ndef generate(self, models, sample, prefix_tokens=None, bos_token=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generate a batch of translations.\\n        Args:\\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\\n            sample (dict): batch\\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\\n                with these tokens\\n        '\n    model = EnsembleModel(models)\n    incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(model.models_size)])\n    if not self.retain_dropout:\n        model.eval()\n    encoder_input = {k: v for (k, v) in sample['net_input'].items() if k != 'prev_output_tokens'}\n    src_tokens = encoder_input['src_tokens']\n    src_lengths_no_eos = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n    input_size = src_tokens.size()\n    bsz = input_size[0]\n    src_len = input_size[1]\n    beam_size = self.beam_size\n    if self.match_source_len:\n        max_len = src_lengths_no_eos.max().item()\n    else:\n        max_len = min(int(self.max_len_a * src_len + self.max_len_b), model.max_decoder_positions() - 1)\n    encoder_outs = model.forward_encoder(encoder_input)\n    new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n    new_order = new_order.to(src_tokens.device).long()\n    encoder_outs = model.reorder_encoder_out(encoder_outs, new_order)\n    src_lengths = encoder_input['src_lengths']\n    scores = src_tokens.new(bsz * beam_size, max_len + 1).float().fill_(0)\n    lm_prefix_scores = src_tokens.new(bsz * beam_size).float().fill_(0)\n    scores_buf = scores.clone()\n    tokens = src_tokens.new(bsz * beam_size, max_len + 2).long().fill_(self.pad)\n    tokens_buf = tokens.clone()\n    tokens[:, 0] = self.eos if bos_token is None else bos_token\n    src_tokens = reorder_all_tokens(src_tokens, src_lengths, self.src_dict.eos_index)\n    src_tokens = src_tokens.repeat(1, beam_size).view(-1, src_len)\n    src_lengths = src_lengths.view(bsz, -1).repeat(1, beam_size).view(bsz * beam_size, -1)\n    (attn, attn_buf) = (None, None)\n    nonpad_idxs = None\n    cands_to_ignore = src_tokens.new_zeros(bsz, beam_size).eq(-1)\n    finalized = [[] for i in range(bsz)]\n    finished = [False for i in range(bsz)]\n    num_remaining_sent = bsz\n    cand_size = 2 * beam_size\n    bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n    cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n    buffers = {}\n\n    def buffer(name, type_of=tokens):\n        if name not in buffers:\n            buffers[name] = type_of.new()\n        return buffers[name]\n\n    def is_finished(sent, step, unfin_idx):\n        \"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"\n        assert len(finalized[sent]) <= beam_size\n        if len(finalized[sent]) == beam_size:\n            return True\n        return False\n\n    def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n        \"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    fw scores for each hypothesis\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\n                    combined noisy channel scores for each hypothesis\n            \"\"\"\n        assert bbsz_idx.numel() == eos_scores.numel()\n        tokens_clone = tokens.index_select(0, bbsz_idx)\n        tokens_clone = tokens_clone[:, 1:step + 2]\n        assert not tokens_clone.eq(self.eos).any()\n        tokens_clone[:, step] = self.eos\n        attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n        pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n        pos_scores[:, step] = eos_scores\n        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n        if self.normalize_scores:\n            combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n        cum_unfin = []\n        prev = 0\n        for f in finished:\n            if f:\n                prev += 1\n            else:\n                cum_unfin.append(prev)\n        sents_seen = set()\n        for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n            unfin_idx = idx // beam_size\n            sent = unfin_idx + cum_unfin[unfin_idx]\n            sents_seen.add((sent, unfin_idx))\n            if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n                score = -math.inf\n\n            def get_hypo():\n                if attn_clone is not None:\n                    hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                    (_, alignment) = hypo_attn.max(dim=0)\n                else:\n                    hypo_attn = None\n                    alignment = None\n                return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n            if len(finalized[sent]) < beam_size:\n                finalized[sent].append(get_hypo())\n        newly_finished = []\n        for (sent, unfin_idx) in sents_seen:\n            if not finished[sent] and is_finished(sent, step, unfin_idx):\n                finished[sent] = True\n                newly_finished.append(unfin_idx)\n        return newly_finished\n\n    def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n        \"\"\"Rescore the top k hypothesis from each beam using noisy channel modeling\n            Returns:\n                new_fw_lprobs: the direct model probabilities after pruning the top k\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\n                new_lm_lprobs: the language model probabilities after pruning the top k\n            \"\"\"\n        with torch.no_grad():\n            lprobs_size = lprobs.size()\n            if prefix_tokens is not None and step < prefix_tokens.size(1):\n                probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n                cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n                cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n                fw_top_k = cand_scores\n                fw_top_k_idx = cand_indices\n                k = 1\n            else:\n                (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n            eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n            ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n            src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n            if self.combine_method != 'lm_only':\n                temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n                not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n                cur_tgt_size = step + 2\n                eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n                eos_tokens[eos_idx] = self.tgt_dict.pad_index\n                if step == 0:\n                    channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                else:\n                    channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n                ch_input_lengths[eos_idx] = cur_tgt_size - 1\n                if self.channel_scoring_type == 'unnormalized':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                    ch_intermed_scores = ch_intermed_scores.float()\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'k2_separate':\n                    for k_idx in range(k):\n                        k_eos_tokens = eos_tokens[k_idx::k, :]\n                        if step == 0:\n                            k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        else:\n                            k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                        k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                        k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                        k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                        k_ch_intermed_scores *= not_padding.float()\n                        ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'src_vocab':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                elif self.channel_scoring_type == 'src_vocab_batched':\n                    ch_bsz_size = temp_src_tokens_full.shape[0]\n                    ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                    for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                        end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                        temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                        channel_input_batch = channel_input[start_idx:end_idx, :]\n                        ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                        ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                        (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                        ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                    ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                else:\n                    ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                    ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                    ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            else:\n                cur_tgt_size = 0\n            ch_scores = ch_scores.view(bsz * beam_size, k)\n            expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n            if self.share_tgt_dict:\n                lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            else:\n                new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n                new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n                lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            lm_scores.add_(expanded_lm_prefix_scores)\n            ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n            new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_fw_lprobs[:, self.pad] = -math.inf\n            new_ch_lm_lprobs[:, self.pad] = -math.inf\n            new_lm_lprobs[:, self.pad] = -math.inf\n            new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n            new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n            new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n            return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)\n\n    def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n        if self.channel_scoring_type == 'unnormalized':\n            ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n        ch_scores = ch_scores * self.ch_weight\n        lm_scores1 = lm_scores1 * self.lm_weight\n        if combine_type == 'lm_only':\n            ch_scores = lm_scores1.view(ch_scores.size())\n        elif combine_type == 'noisy_channel':\n            if self.normalize_lm_scores_by_tgt_len:\n                ch_scores.div_(src_size)\n                lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n                ch_scores.add_(lm_scores_norm)\n            else:\n                ch_scores.add_(lm_scores1.view(ch_scores.size()))\n                ch_scores.div_(src_size)\n        return ch_scores\n    if self.channel_models is not None:\n        channel_model = self.channel_models[0]\n    else:\n        channel_model = None\n    lm = EnsembleModel(self.lm_models)\n    lm_incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(lm.models_size)])\n    reorder_state = None\n    batch_idxs = None\n    for step in range(max_len + 1):\n        if reorder_state is not None:\n            if batch_idxs is not None:\n                corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)\n                reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)\n            model.reorder_incremental_state(incremental_states, reorder_state)\n            encoder_outs = model.reorder_encoder_out(encoder_outs, reorder_state)\n            lm.reorder_incremental_state(lm_incremental_states, reorder_state)\n        (fw_lprobs, avg_attn_scores) = model.forward_decoder(tokens[:, :step + 1], encoder_outs, incremental_states, temperature=self.temperature)\n        fw_lprobs[:, self.pad] = -math.inf\n        fw_lprobs[:, self.unk] -= self.unk_penalty\n        (fw_lprobs, ch_lm_lprobs, lm_lprobs) = noisy_channel_rescoring(fw_lprobs, beam_size, bsz, src_tokens, tokens, self.k2)\n        if step >= max_len:\n            fw_lprobs[:, :self.eos] = -math.inf\n            fw_lprobs[:, self.eos + 1:] = -math.inf\n        elif step < self.min_len:\n            fw_lprobs[:, self.eos] = -math.inf\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)\n            prefix_mask = prefix_toks.ne(self.pad)\n            prefix_fw_lprobs = fw_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            fw_lprobs[prefix_mask] = -math.inf\n            fw_lprobs[prefix_mask] = fw_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_fw_lprobs)\n            prefix_ch_lm_lprobs = ch_lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            ch_lm_lprobs[prefix_mask] = -math.inf\n            ch_lm_lprobs[prefix_mask] = ch_lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_ch_lm_lprobs)\n            prefix_lm_lprobs = lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            lm_lprobs[prefix_mask] = -math.inf\n            lm_lprobs[prefix_mask] = lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lm_lprobs)\n            eos_mask = prefix_toks.eq(self.eos)\n            if eos_mask.any():\n                first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[:, 0, 1:step + 1]\n                eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]\n                target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]\n                assert (first_beam == target_prefix).all()\n\n                def replicate_first_beam(tensor, mask):\n                    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n                    tensor[mask] = tensor[mask][:, :1, :]\n                    return tensor.view(-1, tensor.size(-1))\n                tokens = replicate_first_beam(tokens, eos_mask_batch_dim)\n                scores = replicate_first_beam(scores, eos_mask_batch_dim)\n                fw_lprobs = replicate_first_beam(fw_lprobs, eos_mask_batch_dim)\n                ch_lm_lprobs = replicate_first_beam(ch_lm_lprobs, eos_mask_batch_dim)\n                lm_lprobs = replicate_first_beam(lm_lprobs, eos_mask_batch_dim)\n        if self.no_repeat_ngram_size > 0:\n            gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                gen_tokens = tokens[bbsz_idx].tolist()\n                for ngram in zip(*[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]):\n                    gen_ngrams[bbsz_idx][tuple(ngram[:-1])] = gen_ngrams[bbsz_idx].get(tuple(ngram[:-1]), []) + [ngram[-1]]\n        if avg_attn_scores is not None:\n            if attn is None:\n                attn = scores.new(bsz * beam_size, src_tokens.size(1), max_len + 2)\n                attn_buf = attn.clone()\n                nonpad_idxs = src_tokens.ne(self.pad)\n            attn[:, :, step + 1].copy_(avg_attn_scores)\n        scores = scores.type_as(fw_lprobs)\n        scores_buf = scores_buf.type_as(fw_lprobs)\n        self.search.set_src_lengths(src_lengths_no_eos)\n        if self.no_repeat_ngram_size > 0:\n\n            def calculate_banned_tokens(bbsz_idx):\n                ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n                return gen_ngrams[bbsz_idx].get(ngram_index, [])\n            if step + 2 - self.no_repeat_ngram_size >= 0:\n                banned_tokens = [calculate_banned_tokens(bbsz_idx) for bbsz_idx in range(bsz * beam_size)]\n            else:\n                banned_tokens = [[] for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                fw_lprobs[bbsz_idx, banned_tokens[bbsz_idx]] = -math.inf\n        (combined_noisy_channel_scores, fw_lprobs_top_k, lm_lprobs_top_k, cand_indices, cand_beams) = self.search.step(step, fw_lprobs.view(bsz, -1, self.vocab_size), scores.view(bsz, beam_size, -1)[:, :, :step], ch_lm_lprobs.view(bsz, -1, self.vocab_size), lm_lprobs.view(bsz, -1, self.vocab_size), self.combine_method)\n        cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n        eos_mask = cand_indices.eq(self.eos)\n        eos_mask[:, :beam_size] &= ~cands_to_ignore\n        eos_bbsz_idx = torch.masked_select(cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size])\n        finalized_sents = set()\n        if eos_bbsz_idx.numel() > 0:\n            eos_scores = torch.masked_select(fw_lprobs_top_k[:, :beam_size], mask=eos_mask[:, :beam_size])\n            combined_noisy_channel_eos_scores = torch.masked_select(combined_noisy_channel_scores[:, :beam_size], mask=eos_mask[:, :beam_size])\n            finalized_sents = finalize_hypos(step, eos_bbsz_idx, eos_scores, combined_noisy_channel_eos_scores)\n            num_remaining_sent -= len(finalized_sents)\n        assert num_remaining_sent >= 0\n        if num_remaining_sent == 0:\n            break\n        if len(finalized_sents) > 0:\n            new_bsz = bsz - len(finalized_sents)\n            batch_mask = cand_indices.new_ones(bsz)\n            batch_mask[cand_indices.new(finalized_sents)] = 0\n            batch_idxs = torch.nonzero(batch_mask).squeeze(-1)\n            eos_mask = eos_mask[batch_idxs]\n            cand_beams = cand_beams[batch_idxs]\n            bbsz_offsets.resize_(new_bsz, 1)\n            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n            lm_lprobs_top_k = lm_lprobs_top_k[batch_idxs]\n            fw_lprobs_top_k = fw_lprobs_top_k[batch_idxs]\n            cand_indices = cand_indices[batch_idxs]\n            if prefix_tokens is not None:\n                prefix_tokens = prefix_tokens[batch_idxs]\n            src_lengths_no_eos = src_lengths_no_eos[batch_idxs]\n            cands_to_ignore = cands_to_ignore[batch_idxs]\n            scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            scores_buf.resize_as_(scores)\n            tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            tokens_buf.resize_as_(tokens)\n            src_tokens = src_tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            src_lengths = src_lengths.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            lm_prefix_scores = lm_prefix_scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1).squeeze()\n            if attn is not None:\n                attn = attn.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, attn.size(1), -1)\n                attn_buf.resize_as_(attn)\n            bsz = new_bsz\n        else:\n            batch_idxs = None\n        eos_mask[:, :beam_size] |= cands_to_ignore\n        active_mask = torch.add(eos_mask.type_as(cand_offsets) * cand_size, cand_offsets[:eos_mask.size(1)])\n        (active_hypos, new_cands_to_ignore) = (buffer('active_hypos'), buffer('new_cands_to_ignore'))\n        torch.topk(active_mask, k=beam_size, dim=1, largest=False, out=(new_cands_to_ignore, active_hypos))\n        cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]\n        assert (~cands_to_ignore).any(dim=1).all()\n        active_bbsz_idx = buffer('active_bbsz_idx')\n        torch.gather(cand_bbsz_idx, dim=1, index=active_hypos, out=active_bbsz_idx)\n        active_scores = torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores[:, step].view(bsz, beam_size))\n        active_bbsz_idx = active_bbsz_idx.view(-1)\n        active_scores = active_scores.view(-1)\n        torch.index_select(tokens[:, :step + 1], dim=0, index=active_bbsz_idx, out=tokens_buf[:, :step + 1])\n        torch.gather(cand_indices, dim=1, index=active_hypos, out=tokens_buf.view(bsz, beam_size, -1)[:, :, step + 1])\n        if step > 0:\n            torch.index_select(scores[:, :step], dim=0, index=active_bbsz_idx, out=scores_buf[:, :step])\n        torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores_buf.view(bsz, beam_size, -1)[:, :, step])\n        torch.gather(lm_lprobs_top_k, dim=1, index=active_hypos, out=lm_prefix_scores.view(bsz, beam_size))\n        if attn is not None:\n            torch.index_select(attn[:, :, :step + 2], dim=0, index=active_bbsz_idx, out=attn_buf[:, :, :step + 2])\n        (tokens, tokens_buf) = (tokens_buf, tokens)\n        (scores, scores_buf) = (scores_buf, scores)\n        if attn is not None:\n            (attn, attn_buf) = (attn_buf, attn)\n        reorder_state = active_bbsz_idx\n    for sent in range(len(finalized)):\n        finalized[sent] = sorted(finalized[sent], key=lambda r: r['score'], reverse=True)\n    return finalized",
            "@torch.no_grad()\ndef generate(self, models, sample, prefix_tokens=None, bos_token=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generate a batch of translations.\\n        Args:\\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\\n            sample (dict): batch\\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\\n                with these tokens\\n        '\n    model = EnsembleModel(models)\n    incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(model.models_size)])\n    if not self.retain_dropout:\n        model.eval()\n    encoder_input = {k: v for (k, v) in sample['net_input'].items() if k != 'prev_output_tokens'}\n    src_tokens = encoder_input['src_tokens']\n    src_lengths_no_eos = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n    input_size = src_tokens.size()\n    bsz = input_size[0]\n    src_len = input_size[1]\n    beam_size = self.beam_size\n    if self.match_source_len:\n        max_len = src_lengths_no_eos.max().item()\n    else:\n        max_len = min(int(self.max_len_a * src_len + self.max_len_b), model.max_decoder_positions() - 1)\n    encoder_outs = model.forward_encoder(encoder_input)\n    new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n    new_order = new_order.to(src_tokens.device).long()\n    encoder_outs = model.reorder_encoder_out(encoder_outs, new_order)\n    src_lengths = encoder_input['src_lengths']\n    scores = src_tokens.new(bsz * beam_size, max_len + 1).float().fill_(0)\n    lm_prefix_scores = src_tokens.new(bsz * beam_size).float().fill_(0)\n    scores_buf = scores.clone()\n    tokens = src_tokens.new(bsz * beam_size, max_len + 2).long().fill_(self.pad)\n    tokens_buf = tokens.clone()\n    tokens[:, 0] = self.eos if bos_token is None else bos_token\n    src_tokens = reorder_all_tokens(src_tokens, src_lengths, self.src_dict.eos_index)\n    src_tokens = src_tokens.repeat(1, beam_size).view(-1, src_len)\n    src_lengths = src_lengths.view(bsz, -1).repeat(1, beam_size).view(bsz * beam_size, -1)\n    (attn, attn_buf) = (None, None)\n    nonpad_idxs = None\n    cands_to_ignore = src_tokens.new_zeros(bsz, beam_size).eq(-1)\n    finalized = [[] for i in range(bsz)]\n    finished = [False for i in range(bsz)]\n    num_remaining_sent = bsz\n    cand_size = 2 * beam_size\n    bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n    cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n    buffers = {}\n\n    def buffer(name, type_of=tokens):\n        if name not in buffers:\n            buffers[name] = type_of.new()\n        return buffers[name]\n\n    def is_finished(sent, step, unfin_idx):\n        \"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"\n        assert len(finalized[sent]) <= beam_size\n        if len(finalized[sent]) == beam_size:\n            return True\n        return False\n\n    def finalize_hypos(step, bbsz_idx, eos_scores, combined_noisy_channel_eos_scores):\n        \"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    fw scores for each hypothesis\n                combined_noisy_channel_eos_scores: A vector of the same size as bbsz_idx containing\n                    combined noisy channel scores for each hypothesis\n            \"\"\"\n        assert bbsz_idx.numel() == eos_scores.numel()\n        tokens_clone = tokens.index_select(0, bbsz_idx)\n        tokens_clone = tokens_clone[:, 1:step + 2]\n        assert not tokens_clone.eq(self.eos).any()\n        tokens_clone[:, step] = self.eos\n        attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None\n        pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]\n        pos_scores[:, step] = eos_scores\n        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n        if self.normalize_scores:\n            combined_noisy_channel_eos_scores /= (step + 1) ** self.len_penalty\n        cum_unfin = []\n        prev = 0\n        for f in finished:\n            if f:\n                prev += 1\n            else:\n                cum_unfin.append(prev)\n        sents_seen = set()\n        for (i, (idx, score)) in enumerate(zip(bbsz_idx.tolist(), combined_noisy_channel_eos_scores.tolist())):\n            unfin_idx = idx // beam_size\n            sent = unfin_idx + cum_unfin[unfin_idx]\n            sents_seen.add((sent, unfin_idx))\n            if self.match_source_len and step > src_lengths_no_eos[unfin_idx]:\n                score = -math.inf\n\n            def get_hypo():\n                if attn_clone is not None:\n                    hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                    (_, alignment) = hypo_attn.max(dim=0)\n                else:\n                    hypo_attn = None\n                    alignment = None\n                return {'tokens': tokens_clone[i], 'score': score, 'attention': hypo_attn, 'alignment': alignment, 'positional_scores': pos_scores[i]}\n            if len(finalized[sent]) < beam_size:\n                finalized[sent].append(get_hypo())\n        newly_finished = []\n        for (sent, unfin_idx) in sents_seen:\n            if not finished[sent] and is_finished(sent, step, unfin_idx):\n                finished[sent] = True\n                newly_finished.append(unfin_idx)\n        return newly_finished\n\n    def noisy_channel_rescoring(lprobs, beam_size, bsz, src_tokens, tokens, k):\n        \"\"\"Rescore the top k hypothesis from each beam using noisy channel modeling\n            Returns:\n                new_fw_lprobs: the direct model probabilities after pruning the top k\n                new_ch_lm_lprobs:  the combined channel and language model probabilities\n                new_lm_lprobs: the language model probabilities after pruning the top k\n            \"\"\"\n        with torch.no_grad():\n            lprobs_size = lprobs.size()\n            if prefix_tokens is not None and step < prefix_tokens.size(1):\n                probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n                cand_scores = torch.gather(probs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1).data).expand(-1, beam_size).contiguous().view(bsz * beam_size, 1)\n                cand_indices = prefix_tokens[:, step].view(-1, 1).expand(bsz, beam_size).data.contiguous().view(bsz * beam_size, 1)\n                fw_top_k = cand_scores\n                fw_top_k_idx = cand_indices\n                k = 1\n            else:\n                (fw_top_k, fw_top_k_idx) = torch.topk(lprobs.view(beam_size * bsz, -1), k=k)\n            eos_idx = torch.nonzero(fw_top_k_idx.view(bsz * beam_size * k, -1) == self.eos)[:, 0]\n            ch_scores = fw_top_k.new_full((beam_size * bsz * k,), 0)\n            src_size = torch.sum(src_tokens[:, :] != self.src_dict.pad_index, dim=1, keepdim=True, dtype=fw_top_k.dtype)\n            if self.combine_method != 'lm_only':\n                temp_src_tokens_full = src_tokens[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n                not_padding = temp_src_tokens_full[:, 1:] != self.src_dict.pad_index\n                cur_tgt_size = step + 2\n                eos_tokens = tokens[:, 0].repeat(1, k).view(-1, 1)\n                eos_tokens[eos_idx] = self.tgt_dict.pad_index\n                if step == 0:\n                    channel_input = torch.cat((fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                else:\n                    channel_input = torch.cat((tokens[:, 1:step + 1].repeat(1, k).view(-1, step), fw_top_k_idx.view(-1, 1), eos_tokens), 1)\n                ch_input_lengths = torch.tensor(np.full(channel_input.size(0), cur_tgt_size))\n                ch_input_lengths[eos_idx] = cur_tgt_size - 1\n                if self.channel_scoring_type == 'unnormalized':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_intermed_scores = channel_model.decoder.unnormalized_scores_given_target(ch_decoder_output, target_ids=temp_src_tokens_full[:, 1:])\n                    ch_intermed_scores = ch_intermed_scores.float()\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'k2_separate':\n                    for k_idx in range(k):\n                        k_eos_tokens = eos_tokens[k_idx::k, :]\n                        if step == 0:\n                            k_ch_input = torch.cat((fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        else:\n                            k_ch_input = torch.cat((tokens[:, 1:step + 1], fw_top_k_idx[:, k_idx:k_idx + 1], k_eos_tokens), 1)\n                        k_ch_input_lengths = ch_input_lengths[k_idx::k]\n                        k_ch_output = channel_model(k_ch_input, k_ch_input_lengths, src_tokens)\n                        k_ch_lprobs = channel_model.get_normalized_probs(k_ch_output, log_probs=True)\n                        k_ch_intermed_scores = torch.gather(k_ch_lprobs[:, :-1, :], 2, src_tokens[:, 1:].unsqueeze(2)).squeeze(2)\n                        k_ch_intermed_scores *= not_padding.float()\n                        ch_scores[k_idx::k] = torch.sum(k_ch_intermed_scores, dim=1)\n                elif self.channel_scoring_type == 'src_vocab':\n                    ch_encoder_output = channel_model.encoder(channel_input, src_lengths=ch_input_lengths)\n                    (ch_decoder_output, _) = channel_model.decoder(temp_src_tokens_full, encoder_out=ch_encoder_output, features_only=True)\n                    del ch_encoder_output\n                    ch_lprobs = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                elif self.channel_scoring_type == 'src_vocab_batched':\n                    ch_bsz_size = temp_src_tokens_full.shape[0]\n                    ch_lprobs_list = [None] * len(range(0, ch_bsz_size, self.ch_scoring_bsz))\n                    for (i, start_idx) in enumerate(range(0, ch_bsz_size, self.ch_scoring_bsz)):\n                        end_idx = min(start_idx + self.ch_scoring_bsz, ch_bsz_size)\n                        temp_src_tokens_full_batch = temp_src_tokens_full[start_idx:end_idx, :]\n                        channel_input_batch = channel_input[start_idx:end_idx, :]\n                        ch_input_lengths_batch = ch_input_lengths[start_idx:end_idx]\n                        ch_encoder_output_batch = channel_model.encoder(channel_input_batch, src_lengths=ch_input_lengths_batch)\n                        (ch_decoder_output_batch, _) = channel_model.decoder(temp_src_tokens_full_batch, encoder_out=ch_encoder_output_batch, features_only=True)\n                        ch_lprobs_list[i] = normalized_scores_with_batch_vocab(channel_model.decoder, ch_decoder_output_batch, src_tokens, k, bsz, beam_size, self.src_dict.pad_index, top_k=self.top_k_vocab, start_idx=start_idx, end_idx=end_idx)\n                    ch_lprobs = torch.cat(ch_lprobs_list, dim=0)\n                    ch_scores = torch.sum(ch_lprobs, dim=1)\n                else:\n                    ch_output = channel_model(channel_input, ch_input_lengths, temp_src_tokens_full)\n                    ch_lprobs = channel_model.get_normalized_probs(ch_output, log_probs=True)\n                    ch_intermed_scores = torch.gather(ch_lprobs[:, :-1, :], 2, temp_src_tokens_full[:, 1:].unsqueeze(2)).squeeze().view(bsz * beam_size * k, -1)\n                    ch_intermed_scores *= not_padding.float()\n                    ch_scores = torch.sum(ch_intermed_scores, dim=1)\n            else:\n                cur_tgt_size = 0\n            ch_scores = ch_scores.view(bsz * beam_size, k)\n            expanded_lm_prefix_scores = lm_prefix_scores.unsqueeze(1).expand(-1, k).flatten()\n            if self.share_tgt_dict:\n                lm_scores = get_lm_scores(lm, tokens[:, :step + 1].view(-1, step + 1), lm_incremental_states, fw_top_k_idx.view(-1, 1), torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            else:\n                new_lm_input = dict2dict(tokens[:, :step + 1].view(-1, step + 1), self.tgt_to_lm)\n                new_cands = dict2dict(fw_top_k_idx.view(-1, 1), self.tgt_to_lm)\n                lm_scores = get_lm_scores(lm, new_lm_input, lm_incremental_states, new_cands, torch.tensor(np.full(tokens.size(0), step + 1)), k)\n            lm_scores.add_(expanded_lm_prefix_scores)\n            ch_lm_scores = combine_ch_lm(self.combine_method, ch_scores, lm_scores, src_size, cur_tgt_size)\n            new_fw_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_ch_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_lm_lprobs = ch_scores.new(lprobs_size).fill_(-1e+17).view(bsz * beam_size, -1)\n            new_fw_lprobs[:, self.pad] = -math.inf\n            new_ch_lm_lprobs[:, self.pad] = -math.inf\n            new_lm_lprobs[:, self.pad] = -math.inf\n            new_fw_lprobs.scatter_(1, fw_top_k_idx, fw_top_k)\n            new_ch_lm_lprobs.scatter_(1, fw_top_k_idx, ch_lm_scores)\n            new_lm_lprobs.scatter_(1, fw_top_k_idx, lm_scores.view(-1, k))\n            return (new_fw_lprobs, new_ch_lm_lprobs, new_lm_lprobs)\n\n    def combine_ch_lm(combine_type, ch_scores, lm_scores1, src_size, tgt_size):\n        if self.channel_scoring_type == 'unnormalized':\n            ch_scores = self.log_softmax_fn(ch_scores.view(-1, self.beam_size * self.k2)).view(ch_scores.shape)\n        ch_scores = ch_scores * self.ch_weight\n        lm_scores1 = lm_scores1 * self.lm_weight\n        if combine_type == 'lm_only':\n            ch_scores = lm_scores1.view(ch_scores.size())\n        elif combine_type == 'noisy_channel':\n            if self.normalize_lm_scores_by_tgt_len:\n                ch_scores.div_(src_size)\n                lm_scores_norm = lm_scores1.view(ch_scores.size()).div(tgt_size)\n                ch_scores.add_(lm_scores_norm)\n            else:\n                ch_scores.add_(lm_scores1.view(ch_scores.size()))\n                ch_scores.div_(src_size)\n        return ch_scores\n    if self.channel_models is not None:\n        channel_model = self.channel_models[0]\n    else:\n        channel_model = None\n    lm = EnsembleModel(self.lm_models)\n    lm_incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(lm.models_size)])\n    reorder_state = None\n    batch_idxs = None\n    for step in range(max_len + 1):\n        if reorder_state is not None:\n            if batch_idxs is not None:\n                corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)\n                reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)\n            model.reorder_incremental_state(incremental_states, reorder_state)\n            encoder_outs = model.reorder_encoder_out(encoder_outs, reorder_state)\n            lm.reorder_incremental_state(lm_incremental_states, reorder_state)\n        (fw_lprobs, avg_attn_scores) = model.forward_decoder(tokens[:, :step + 1], encoder_outs, incremental_states, temperature=self.temperature)\n        fw_lprobs[:, self.pad] = -math.inf\n        fw_lprobs[:, self.unk] -= self.unk_penalty\n        (fw_lprobs, ch_lm_lprobs, lm_lprobs) = noisy_channel_rescoring(fw_lprobs, beam_size, bsz, src_tokens, tokens, self.k2)\n        if step >= max_len:\n            fw_lprobs[:, :self.eos] = -math.inf\n            fw_lprobs[:, self.eos + 1:] = -math.inf\n        elif step < self.min_len:\n            fw_lprobs[:, self.eos] = -math.inf\n        if prefix_tokens is not None and step < prefix_tokens.size(1):\n            prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)\n            prefix_mask = prefix_toks.ne(self.pad)\n            prefix_fw_lprobs = fw_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            fw_lprobs[prefix_mask] = -math.inf\n            fw_lprobs[prefix_mask] = fw_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_fw_lprobs)\n            prefix_ch_lm_lprobs = ch_lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            ch_lm_lprobs[prefix_mask] = -math.inf\n            ch_lm_lprobs[prefix_mask] = ch_lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_ch_lm_lprobs)\n            prefix_lm_lprobs = lm_lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n            lm_lprobs[prefix_mask] = -math.inf\n            lm_lprobs[prefix_mask] = lm_lprobs[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lm_lprobs)\n            eos_mask = prefix_toks.eq(self.eos)\n            if eos_mask.any():\n                first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[:, 0, 1:step + 1]\n                eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]\n                target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]\n                assert (first_beam == target_prefix).all()\n\n                def replicate_first_beam(tensor, mask):\n                    tensor = tensor.view(-1, beam_size, tensor.size(-1))\n                    tensor[mask] = tensor[mask][:, :1, :]\n                    return tensor.view(-1, tensor.size(-1))\n                tokens = replicate_first_beam(tokens, eos_mask_batch_dim)\n                scores = replicate_first_beam(scores, eos_mask_batch_dim)\n                fw_lprobs = replicate_first_beam(fw_lprobs, eos_mask_batch_dim)\n                ch_lm_lprobs = replicate_first_beam(ch_lm_lprobs, eos_mask_batch_dim)\n                lm_lprobs = replicate_first_beam(lm_lprobs, eos_mask_batch_dim)\n        if self.no_repeat_ngram_size > 0:\n            gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                gen_tokens = tokens[bbsz_idx].tolist()\n                for ngram in zip(*[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]):\n                    gen_ngrams[bbsz_idx][tuple(ngram[:-1])] = gen_ngrams[bbsz_idx].get(tuple(ngram[:-1]), []) + [ngram[-1]]\n        if avg_attn_scores is not None:\n            if attn is None:\n                attn = scores.new(bsz * beam_size, src_tokens.size(1), max_len + 2)\n                attn_buf = attn.clone()\n                nonpad_idxs = src_tokens.ne(self.pad)\n            attn[:, :, step + 1].copy_(avg_attn_scores)\n        scores = scores.type_as(fw_lprobs)\n        scores_buf = scores_buf.type_as(fw_lprobs)\n        self.search.set_src_lengths(src_lengths_no_eos)\n        if self.no_repeat_ngram_size > 0:\n\n            def calculate_banned_tokens(bbsz_idx):\n                ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n                return gen_ngrams[bbsz_idx].get(ngram_index, [])\n            if step + 2 - self.no_repeat_ngram_size >= 0:\n                banned_tokens = [calculate_banned_tokens(bbsz_idx) for bbsz_idx in range(bsz * beam_size)]\n            else:\n                banned_tokens = [[] for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                fw_lprobs[bbsz_idx, banned_tokens[bbsz_idx]] = -math.inf\n        (combined_noisy_channel_scores, fw_lprobs_top_k, lm_lprobs_top_k, cand_indices, cand_beams) = self.search.step(step, fw_lprobs.view(bsz, -1, self.vocab_size), scores.view(bsz, beam_size, -1)[:, :, :step], ch_lm_lprobs.view(bsz, -1, self.vocab_size), lm_lprobs.view(bsz, -1, self.vocab_size), self.combine_method)\n        cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n        eos_mask = cand_indices.eq(self.eos)\n        eos_mask[:, :beam_size] &= ~cands_to_ignore\n        eos_bbsz_idx = torch.masked_select(cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size])\n        finalized_sents = set()\n        if eos_bbsz_idx.numel() > 0:\n            eos_scores = torch.masked_select(fw_lprobs_top_k[:, :beam_size], mask=eos_mask[:, :beam_size])\n            combined_noisy_channel_eos_scores = torch.masked_select(combined_noisy_channel_scores[:, :beam_size], mask=eos_mask[:, :beam_size])\n            finalized_sents = finalize_hypos(step, eos_bbsz_idx, eos_scores, combined_noisy_channel_eos_scores)\n            num_remaining_sent -= len(finalized_sents)\n        assert num_remaining_sent >= 0\n        if num_remaining_sent == 0:\n            break\n        if len(finalized_sents) > 0:\n            new_bsz = bsz - len(finalized_sents)\n            batch_mask = cand_indices.new_ones(bsz)\n            batch_mask[cand_indices.new(finalized_sents)] = 0\n            batch_idxs = torch.nonzero(batch_mask).squeeze(-1)\n            eos_mask = eos_mask[batch_idxs]\n            cand_beams = cand_beams[batch_idxs]\n            bbsz_offsets.resize_(new_bsz, 1)\n            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n            lm_lprobs_top_k = lm_lprobs_top_k[batch_idxs]\n            fw_lprobs_top_k = fw_lprobs_top_k[batch_idxs]\n            cand_indices = cand_indices[batch_idxs]\n            if prefix_tokens is not None:\n                prefix_tokens = prefix_tokens[batch_idxs]\n            src_lengths_no_eos = src_lengths_no_eos[batch_idxs]\n            cands_to_ignore = cands_to_ignore[batch_idxs]\n            scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            scores_buf.resize_as_(scores)\n            tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            tokens_buf.resize_as_(tokens)\n            src_tokens = src_tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            src_lengths = src_lengths.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            lm_prefix_scores = lm_prefix_scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1).squeeze()\n            if attn is not None:\n                attn = attn.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, attn.size(1), -1)\n                attn_buf.resize_as_(attn)\n            bsz = new_bsz\n        else:\n            batch_idxs = None\n        eos_mask[:, :beam_size] |= cands_to_ignore\n        active_mask = torch.add(eos_mask.type_as(cand_offsets) * cand_size, cand_offsets[:eos_mask.size(1)])\n        (active_hypos, new_cands_to_ignore) = (buffer('active_hypos'), buffer('new_cands_to_ignore'))\n        torch.topk(active_mask, k=beam_size, dim=1, largest=False, out=(new_cands_to_ignore, active_hypos))\n        cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]\n        assert (~cands_to_ignore).any(dim=1).all()\n        active_bbsz_idx = buffer('active_bbsz_idx')\n        torch.gather(cand_bbsz_idx, dim=1, index=active_hypos, out=active_bbsz_idx)\n        active_scores = torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores[:, step].view(bsz, beam_size))\n        active_bbsz_idx = active_bbsz_idx.view(-1)\n        active_scores = active_scores.view(-1)\n        torch.index_select(tokens[:, :step + 1], dim=0, index=active_bbsz_idx, out=tokens_buf[:, :step + 1])\n        torch.gather(cand_indices, dim=1, index=active_hypos, out=tokens_buf.view(bsz, beam_size, -1)[:, :, step + 1])\n        if step > 0:\n            torch.index_select(scores[:, :step], dim=0, index=active_bbsz_idx, out=scores_buf[:, :step])\n        torch.gather(fw_lprobs_top_k, dim=1, index=active_hypos, out=scores_buf.view(bsz, beam_size, -1)[:, :, step])\n        torch.gather(lm_lprobs_top_k, dim=1, index=active_hypos, out=lm_prefix_scores.view(bsz, beam_size))\n        if attn is not None:\n            torch.index_select(attn[:, :, :step + 2], dim=0, index=active_bbsz_idx, out=attn_buf[:, :, :step + 2])\n        (tokens, tokens_buf) = (tokens_buf, tokens)\n        (scores, scores_buf) = (scores_buf, scores)\n        if attn is not None:\n            (attn, attn_buf) = (attn_buf, attn)\n        reorder_state = active_bbsz_idx\n    for sent in range(len(finalized)):\n        finalized[sent] = sorted(finalized[sent], key=lambda r: r['score'], reverse=True)\n    return finalized"
        ]
    },
    {
        "func_name": "get_lm_scores",
        "original": "def get_lm_scores(model, input_tokens, incremental_states, cand_tokens, input_len, k):\n    with torch.no_grad():\n        (lm_lprobs, avg_attn_scores) = model.forward_decoder(input_tokens, encoder_outs=None, incremental_states=incremental_states)\n        lm_lprobs_size = lm_lprobs.size(0)\n        probs_next_wrd = torch.gather(lm_lprobs.repeat(1, k).view(lm_lprobs_size * k, -1), 1, cand_tokens).squeeze().view(-1)\n        return probs_next_wrd",
        "mutated": [
            "def get_lm_scores(model, input_tokens, incremental_states, cand_tokens, input_len, k):\n    if False:\n        i = 10\n    with torch.no_grad():\n        (lm_lprobs, avg_attn_scores) = model.forward_decoder(input_tokens, encoder_outs=None, incremental_states=incremental_states)\n        lm_lprobs_size = lm_lprobs.size(0)\n        probs_next_wrd = torch.gather(lm_lprobs.repeat(1, k).view(lm_lprobs_size * k, -1), 1, cand_tokens).squeeze().view(-1)\n        return probs_next_wrd",
            "def get_lm_scores(model, input_tokens, incremental_states, cand_tokens, input_len, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with torch.no_grad():\n        (lm_lprobs, avg_attn_scores) = model.forward_decoder(input_tokens, encoder_outs=None, incremental_states=incremental_states)\n        lm_lprobs_size = lm_lprobs.size(0)\n        probs_next_wrd = torch.gather(lm_lprobs.repeat(1, k).view(lm_lprobs_size * k, -1), 1, cand_tokens).squeeze().view(-1)\n        return probs_next_wrd",
            "def get_lm_scores(model, input_tokens, incremental_states, cand_tokens, input_len, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with torch.no_grad():\n        (lm_lprobs, avg_attn_scores) = model.forward_decoder(input_tokens, encoder_outs=None, incremental_states=incremental_states)\n        lm_lprobs_size = lm_lprobs.size(0)\n        probs_next_wrd = torch.gather(lm_lprobs.repeat(1, k).view(lm_lprobs_size * k, -1), 1, cand_tokens).squeeze().view(-1)\n        return probs_next_wrd",
            "def get_lm_scores(model, input_tokens, incremental_states, cand_tokens, input_len, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with torch.no_grad():\n        (lm_lprobs, avg_attn_scores) = model.forward_decoder(input_tokens, encoder_outs=None, incremental_states=incremental_states)\n        lm_lprobs_size = lm_lprobs.size(0)\n        probs_next_wrd = torch.gather(lm_lprobs.repeat(1, k).view(lm_lprobs_size * k, -1), 1, cand_tokens).squeeze().view(-1)\n        return probs_next_wrd",
            "def get_lm_scores(model, input_tokens, incremental_states, cand_tokens, input_len, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with torch.no_grad():\n        (lm_lprobs, avg_attn_scores) = model.forward_decoder(input_tokens, encoder_outs=None, incremental_states=incremental_states)\n        lm_lprobs_size = lm_lprobs.size(0)\n        probs_next_wrd = torch.gather(lm_lprobs.repeat(1, k).view(lm_lprobs_size * k, -1), 1, cand_tokens).squeeze().view(-1)\n        return probs_next_wrd"
        ]
    },
    {
        "func_name": "make_dict2dict",
        "original": "def make_dict2dict(old_dict, new_dict):\n    dict2dict_map = {}\n    for sym in old_dict.symbols:\n        dict2dict_map[old_dict.index(sym)] = new_dict.index(sym)\n    return dict2dict_map",
        "mutated": [
            "def make_dict2dict(old_dict, new_dict):\n    if False:\n        i = 10\n    dict2dict_map = {}\n    for sym in old_dict.symbols:\n        dict2dict_map[old_dict.index(sym)] = new_dict.index(sym)\n    return dict2dict_map",
            "def make_dict2dict(old_dict, new_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dict2dict_map = {}\n    for sym in old_dict.symbols:\n        dict2dict_map[old_dict.index(sym)] = new_dict.index(sym)\n    return dict2dict_map",
            "def make_dict2dict(old_dict, new_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dict2dict_map = {}\n    for sym in old_dict.symbols:\n        dict2dict_map[old_dict.index(sym)] = new_dict.index(sym)\n    return dict2dict_map",
            "def make_dict2dict(old_dict, new_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dict2dict_map = {}\n    for sym in old_dict.symbols:\n        dict2dict_map[old_dict.index(sym)] = new_dict.index(sym)\n    return dict2dict_map",
            "def make_dict2dict(old_dict, new_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dict2dict_map = {}\n    for sym in old_dict.symbols:\n        dict2dict_map[old_dict.index(sym)] = new_dict.index(sym)\n    return dict2dict_map"
        ]
    },
    {
        "func_name": "dict2dict",
        "original": "def dict2dict(tokens, dict2dict_map):\n    if tokens.device == torch.device('cpu'):\n        tokens_tmp = tokens\n    else:\n        tokens_tmp = tokens.cpu()\n    return tokens_tmp.map_(tokens_tmp, lambda _, val, dict2dict_map=dict2dict_map: dict2dict_map[float(val)]).to(tokens.device)",
        "mutated": [
            "def dict2dict(tokens, dict2dict_map):\n    if False:\n        i = 10\n    if tokens.device == torch.device('cpu'):\n        tokens_tmp = tokens\n    else:\n        tokens_tmp = tokens.cpu()\n    return tokens_tmp.map_(tokens_tmp, lambda _, val, dict2dict_map=dict2dict_map: dict2dict_map[float(val)]).to(tokens.device)",
            "def dict2dict(tokens, dict2dict_map):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if tokens.device == torch.device('cpu'):\n        tokens_tmp = tokens\n    else:\n        tokens_tmp = tokens.cpu()\n    return tokens_tmp.map_(tokens_tmp, lambda _, val, dict2dict_map=dict2dict_map: dict2dict_map[float(val)]).to(tokens.device)",
            "def dict2dict(tokens, dict2dict_map):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if tokens.device == torch.device('cpu'):\n        tokens_tmp = tokens\n    else:\n        tokens_tmp = tokens.cpu()\n    return tokens_tmp.map_(tokens_tmp, lambda _, val, dict2dict_map=dict2dict_map: dict2dict_map[float(val)]).to(tokens.device)",
            "def dict2dict(tokens, dict2dict_map):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if tokens.device == torch.device('cpu'):\n        tokens_tmp = tokens\n    else:\n        tokens_tmp = tokens.cpu()\n    return tokens_tmp.map_(tokens_tmp, lambda _, val, dict2dict_map=dict2dict_map: dict2dict_map[float(val)]).to(tokens.device)",
            "def dict2dict(tokens, dict2dict_map):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if tokens.device == torch.device('cpu'):\n        tokens_tmp = tokens\n    else:\n        tokens_tmp = tokens.cpu()\n    return tokens_tmp.map_(tokens_tmp, lambda _, val, dict2dict_map=dict2dict_map: dict2dict_map[float(val)]).to(tokens.device)"
        ]
    },
    {
        "func_name": "reorder_tokens",
        "original": "def reorder_tokens(tokens, lengths, eos):\n    return torch.cat((tokens.new([eos]), tokens[-lengths:-1], tokens[:-lengths]), 0)",
        "mutated": [
            "def reorder_tokens(tokens, lengths, eos):\n    if False:\n        i = 10\n    return torch.cat((tokens.new([eos]), tokens[-lengths:-1], tokens[:-lengths]), 0)",
            "def reorder_tokens(tokens, lengths, eos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return torch.cat((tokens.new([eos]), tokens[-lengths:-1], tokens[:-lengths]), 0)",
            "def reorder_tokens(tokens, lengths, eos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return torch.cat((tokens.new([eos]), tokens[-lengths:-1], tokens[:-lengths]), 0)",
            "def reorder_tokens(tokens, lengths, eos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return torch.cat((tokens.new([eos]), tokens[-lengths:-1], tokens[:-lengths]), 0)",
            "def reorder_tokens(tokens, lengths, eos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return torch.cat((tokens.new([eos]), tokens[-lengths:-1], tokens[:-lengths]), 0)"
        ]
    },
    {
        "func_name": "reorder_all_tokens",
        "original": "def reorder_all_tokens(tokens, lengths, eos):\n    return torch.stack([reorder_tokens(token, length, eos) for (token, length) in zip(tokens, lengths)])",
        "mutated": [
            "def reorder_all_tokens(tokens, lengths, eos):\n    if False:\n        i = 10\n    return torch.stack([reorder_tokens(token, length, eos) for (token, length) in zip(tokens, lengths)])",
            "def reorder_all_tokens(tokens, lengths, eos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return torch.stack([reorder_tokens(token, length, eos) for (token, length) in zip(tokens, lengths)])",
            "def reorder_all_tokens(tokens, lengths, eos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return torch.stack([reorder_tokens(token, length, eos) for (token, length) in zip(tokens, lengths)])",
            "def reorder_all_tokens(tokens, lengths, eos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return torch.stack([reorder_tokens(token, length, eos) for (token, length) in zip(tokens, lengths)])",
            "def reorder_all_tokens(tokens, lengths, eos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return torch.stack([reorder_tokens(token, length, eos) for (token, length) in zip(tokens, lengths)])"
        ]
    },
    {
        "func_name": "normalized_scores_with_batch_vocab",
        "original": "def normalized_scores_with_batch_vocab(model_decoder, features, target_ids, k, bsz, beam_size, pad_idx, top_k=0, vocab_size_meter=None, start_idx=None, end_idx=None, **kwargs):\n    \"\"\"\n        Get normalized probabilities (or log probs) from a net's output\n        w.r.t. vocab consisting of target IDs in the batch\n    \"\"\"\n    if model_decoder.adaptive_softmax is None:\n        weight = model_decoder.output_projection.weight\n        vocab_ids = torch.unique(torch.cat((torch.unique(target_ids), torch.arange(top_k, device=target_ids.device))))\n        id_map = dict(zip(vocab_ids.tolist(), range(len(vocab_ids))))\n        mapped_target_ids = target_ids.cpu().apply_(lambda x, id_map=id_map: id_map[x]).to(target_ids.device)\n        expanded_target_ids = mapped_target_ids[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n        if start_idx is not None and end_idx is not None:\n            expanded_target_ids = expanded_target_ids[start_idx:end_idx, :]\n        logits = F.linear(features, weight[vocab_ids, :])\n        log_softmax = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n        intermed_scores = torch.gather(log_softmax[:, :-1, :], 2, expanded_target_ids[:, 1:].unsqueeze(2)).squeeze()\n        not_padding = expanded_target_ids[:, 1:] != pad_idx\n        intermed_scores *= not_padding.float()\n        return intermed_scores\n    else:\n        raise ValueError(\"adaptive softmax doesn't work with \" + '`normalized_scores_with_batch_vocab()`')",
        "mutated": [
            "def normalized_scores_with_batch_vocab(model_decoder, features, target_ids, k, bsz, beam_size, pad_idx, top_k=0, vocab_size_meter=None, start_idx=None, end_idx=None, **kwargs):\n    if False:\n        i = 10\n    \"\\n        Get normalized probabilities (or log probs) from a net's output\\n        w.r.t. vocab consisting of target IDs in the batch\\n    \"\n    if model_decoder.adaptive_softmax is None:\n        weight = model_decoder.output_projection.weight\n        vocab_ids = torch.unique(torch.cat((torch.unique(target_ids), torch.arange(top_k, device=target_ids.device))))\n        id_map = dict(zip(vocab_ids.tolist(), range(len(vocab_ids))))\n        mapped_target_ids = target_ids.cpu().apply_(lambda x, id_map=id_map: id_map[x]).to(target_ids.device)\n        expanded_target_ids = mapped_target_ids[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n        if start_idx is not None and end_idx is not None:\n            expanded_target_ids = expanded_target_ids[start_idx:end_idx, :]\n        logits = F.linear(features, weight[vocab_ids, :])\n        log_softmax = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n        intermed_scores = torch.gather(log_softmax[:, :-1, :], 2, expanded_target_ids[:, 1:].unsqueeze(2)).squeeze()\n        not_padding = expanded_target_ids[:, 1:] != pad_idx\n        intermed_scores *= not_padding.float()\n        return intermed_scores\n    else:\n        raise ValueError(\"adaptive softmax doesn't work with \" + '`normalized_scores_with_batch_vocab()`')",
            "def normalized_scores_with_batch_vocab(model_decoder, features, target_ids, k, bsz, beam_size, pad_idx, top_k=0, vocab_size_meter=None, start_idx=None, end_idx=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Get normalized probabilities (or log probs) from a net's output\\n        w.r.t. vocab consisting of target IDs in the batch\\n    \"\n    if model_decoder.adaptive_softmax is None:\n        weight = model_decoder.output_projection.weight\n        vocab_ids = torch.unique(torch.cat((torch.unique(target_ids), torch.arange(top_k, device=target_ids.device))))\n        id_map = dict(zip(vocab_ids.tolist(), range(len(vocab_ids))))\n        mapped_target_ids = target_ids.cpu().apply_(lambda x, id_map=id_map: id_map[x]).to(target_ids.device)\n        expanded_target_ids = mapped_target_ids[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n        if start_idx is not None and end_idx is not None:\n            expanded_target_ids = expanded_target_ids[start_idx:end_idx, :]\n        logits = F.linear(features, weight[vocab_ids, :])\n        log_softmax = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n        intermed_scores = torch.gather(log_softmax[:, :-1, :], 2, expanded_target_ids[:, 1:].unsqueeze(2)).squeeze()\n        not_padding = expanded_target_ids[:, 1:] != pad_idx\n        intermed_scores *= not_padding.float()\n        return intermed_scores\n    else:\n        raise ValueError(\"adaptive softmax doesn't work with \" + '`normalized_scores_with_batch_vocab()`')",
            "def normalized_scores_with_batch_vocab(model_decoder, features, target_ids, k, bsz, beam_size, pad_idx, top_k=0, vocab_size_meter=None, start_idx=None, end_idx=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Get normalized probabilities (or log probs) from a net's output\\n        w.r.t. vocab consisting of target IDs in the batch\\n    \"\n    if model_decoder.adaptive_softmax is None:\n        weight = model_decoder.output_projection.weight\n        vocab_ids = torch.unique(torch.cat((torch.unique(target_ids), torch.arange(top_k, device=target_ids.device))))\n        id_map = dict(zip(vocab_ids.tolist(), range(len(vocab_ids))))\n        mapped_target_ids = target_ids.cpu().apply_(lambda x, id_map=id_map: id_map[x]).to(target_ids.device)\n        expanded_target_ids = mapped_target_ids[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n        if start_idx is not None and end_idx is not None:\n            expanded_target_ids = expanded_target_ids[start_idx:end_idx, :]\n        logits = F.linear(features, weight[vocab_ids, :])\n        log_softmax = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n        intermed_scores = torch.gather(log_softmax[:, :-1, :], 2, expanded_target_ids[:, 1:].unsqueeze(2)).squeeze()\n        not_padding = expanded_target_ids[:, 1:] != pad_idx\n        intermed_scores *= not_padding.float()\n        return intermed_scores\n    else:\n        raise ValueError(\"adaptive softmax doesn't work with \" + '`normalized_scores_with_batch_vocab()`')",
            "def normalized_scores_with_batch_vocab(model_decoder, features, target_ids, k, bsz, beam_size, pad_idx, top_k=0, vocab_size_meter=None, start_idx=None, end_idx=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Get normalized probabilities (or log probs) from a net's output\\n        w.r.t. vocab consisting of target IDs in the batch\\n    \"\n    if model_decoder.adaptive_softmax is None:\n        weight = model_decoder.output_projection.weight\n        vocab_ids = torch.unique(torch.cat((torch.unique(target_ids), torch.arange(top_k, device=target_ids.device))))\n        id_map = dict(zip(vocab_ids.tolist(), range(len(vocab_ids))))\n        mapped_target_ids = target_ids.cpu().apply_(lambda x, id_map=id_map: id_map[x]).to(target_ids.device)\n        expanded_target_ids = mapped_target_ids[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n        if start_idx is not None and end_idx is not None:\n            expanded_target_ids = expanded_target_ids[start_idx:end_idx, :]\n        logits = F.linear(features, weight[vocab_ids, :])\n        log_softmax = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n        intermed_scores = torch.gather(log_softmax[:, :-1, :], 2, expanded_target_ids[:, 1:].unsqueeze(2)).squeeze()\n        not_padding = expanded_target_ids[:, 1:] != pad_idx\n        intermed_scores *= not_padding.float()\n        return intermed_scores\n    else:\n        raise ValueError(\"adaptive softmax doesn't work with \" + '`normalized_scores_with_batch_vocab()`')",
            "def normalized_scores_with_batch_vocab(model_decoder, features, target_ids, k, bsz, beam_size, pad_idx, top_k=0, vocab_size_meter=None, start_idx=None, end_idx=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Get normalized probabilities (or log probs) from a net's output\\n        w.r.t. vocab consisting of target IDs in the batch\\n    \"\n    if model_decoder.adaptive_softmax is None:\n        weight = model_decoder.output_projection.weight\n        vocab_ids = torch.unique(torch.cat((torch.unique(target_ids), torch.arange(top_k, device=target_ids.device))))\n        id_map = dict(zip(vocab_ids.tolist(), range(len(vocab_ids))))\n        mapped_target_ids = target_ids.cpu().apply_(lambda x, id_map=id_map: id_map[x]).to(target_ids.device)\n        expanded_target_ids = mapped_target_ids[:, :].repeat(1, k).view(bsz * beam_size * k, -1)\n        if start_idx is not None and end_idx is not None:\n            expanded_target_ids = expanded_target_ids[start_idx:end_idx, :]\n        logits = F.linear(features, weight[vocab_ids, :])\n        log_softmax = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n        intermed_scores = torch.gather(log_softmax[:, :-1, :], 2, expanded_target_ids[:, 1:].unsqueeze(2)).squeeze()\n        not_padding = expanded_target_ids[:, 1:] != pad_idx\n        intermed_scores *= not_padding.float()\n        return intermed_scores\n    else:\n        raise ValueError(\"adaptive softmax doesn't work with \" + '`normalized_scores_with_batch_vocab()`')"
        ]
    }
]