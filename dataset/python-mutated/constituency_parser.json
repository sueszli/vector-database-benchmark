[
    {
        "func_name": "build_argparse",
        "original": "def build_argparse():\n    \"\"\"\n    Adds the arguments for building the con parser\n\n    For the most part, defaults are set to cross-validated values, at least for WSJ\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='data/constituency', help='Directory of constituency data.')\n    parser.add_argument('--wordvec_dir', type=str, default='extern_data/wordvec', help='Directory of word vectors')\n    parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')\n    parser.add_argument('--wordvec_pretrain_file', type=str, default=None, help='Exact name of the pretrain file to read')\n    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)\n    parser.add_argument('--charlm_forward_file', type=str, default=None, help='Exact path to use for forward charlm')\n    parser.add_argument('--charlm_backward_file', type=str, default=None, help='Exact path to use for backward charlm')\n    parser.add_argument('--bert_model', type=str, default=None, help='Use an external bert model (requires the transformers package)')\n    parser.add_argument('--no_bert_model', dest='bert_model', action='store_const', const=None, help=\"Don't use bert\")\n    parser.add_argument('--bert_hidden_layers', type=int, default=4, help='How many layers of hidden state to use from the transformer')\n    parser.add_argument('--bert_hidden_layers_original', action='store_const', const=None, dest='bert_hidden_layers', help='Use layers 2,3,4 of the Bert embedding')\n    parser.add_argument('--bert_finetune', default=False, action='store_true', help='Finetune the bert (or other transformer)')\n    parser.add_argument('--no_bert_finetune', dest='bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer)\")\n    parser.add_argument('--bert_finetune_layers', default=None, type=int, help='Only finetune this many layers from the transformer')\n    parser.add_argument('--bert_finetune_begin_epoch', default=None, type=int, help='Which epoch to start finetuning the transformer')\n    parser.add_argument('--bert_finetune_end_epoch', default=None, type=int, help='Which epoch to stop finetuning the transformer')\n    parser.add_argument('--bert_learning_rate', default=0.009, type=float, help='Scale the learning rate for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_learning_rate', default=None, type=float, help='Scale the learning rate for transformer finetuning by this much only during an AdaDelta warmup')\n    parser.add_argument('--bert_weight_decay', default=0.0001, type=float, help='Scale the weight decay for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_finetune', default=None, action='store_true', help=\"Finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--no_stage1_bert_finetune', dest='stage1_bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--tag_embedding_dim', type=int, default=20, help='Embedding size for a tag.  0 turns off the feature')\n    parser.add_argument('--delta_embedding_dim', type=int, default=100, help='Embedding size for a delta embedding')\n    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--silver_file', type=str, default=None, help='Secondary training file.')\n    parser.add_argument('--silver_remove_duplicates', default=False, action='store_true', help=\"Do/don't remove duplicates from the silver training file.  Could be useful for intentionally reweighting some trees\")\n    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--tokenized_file', type=str, default=None, help='Input file of tokenized text for parsing with parse_text.')\n    parser.add_argument('--tokenized_dir', type=str, default=None, help='Input directory of tokenized text for parsing with parse_text.')\n    parser.add_argument('--mode', default='train', choices=['train', 'parse_text', 'predict', 'remove_optimizer'])\n    parser.add_argument('--num_generate', type=int, default=0, help='When running a dev set, how many sentences to generate beyond the greedy one')\n    add_predict_output_args(parser)\n    parser.add_argument('--lang', type=str, help='Language')\n    parser.add_argument('--shorthand', type=str, help='Treebank shorthand')\n    parser.add_argument('--transition_embedding_dim', type=int, default=20, help='Embedding size for a transition')\n    parser.add_argument('--transition_hidden_size', type=int, default=20, help='Embedding size for transition stack')\n    parser.add_argument('--transition_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--transition_heads', default=4, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--constituent_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--constituent_heads', default=8, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--hidden_size', type=int, default=512, help='Size of the output layers for constituency stack and word queue')\n    parser.add_argument('--epochs', type=int, default=400)\n    parser.add_argument('--epoch_size', type=int, default=5000, help=\"Runs this many trees in an 'epoch' instead of going through the training dataset exactly once.  Set to 0 to do the whole training set\")\n    parser.add_argument('--silver_epoch_size', type=int, default=None, help=\"Runs this many trees in a silver 'epoch'.  If not set, will match --epoch_size\")\n    parser.add_argument('--multistage', default=True, action='store_true', help='1/2 epochs with adadelta no pattn or lattn, 1/4 with chosen optim and no lattn, 1/4 full model')\n    parser.add_argument('--no_multistage', dest='multistage', action='store_false', help=\"don't do the multistage learning\")\n    parser.add_argument('--oracle_initial_epoch', type=int, default=1, help='Epoch where we start using the dynamic oracle to let the parser keep going with wrong decisions')\n    parser.add_argument('--oracle_frequency', type=float, default=0.8, help='How often to use the oracle vs how often to force the correct transition')\n    parser.add_argument('--oracle_forced_errors', type=float, default=0.001, help='Occasionally have the model randomly walk through the state space to try to learn how to recover')\n    parser.add_argument('--oracle_level', type=int, default=None, help='Restrict oracle transitions to this level or lower.  0 means off.  None means use all oracle transitions.')\n    parser.add_argument('--train_batch_size', type=int, default=30, help='How many trees to train before taking an optimizer step')\n    parser.add_argument('--eval_batch_size', type=int, default=50, help='How many trees to batch when running eval')\n    parser.add_argument('--save_dir', type=str, default='saved_models/constituency', help='Root dir for saving models.')\n    parser.add_argument('--save_name', type=str, default='{shorthand}_{embedding}_{finetune}_constituency.pt', help='File name to save the model')\n    parser.add_argument('--save_each_name', type=str, default=None, help='Save each model in sequence to this pattern.  Mostly for testing')\n    parser.add_argument('--seed', type=int, default=1234)\n    utils.add_device_args(parser)\n    parser.add_argument('--learning_rate', default=None, type=float, help='Learning rate for the optimizer.  Reasonable values are 1.0 for adadelta or 0.001 for SGD.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_RATES))\n    parser.add_argument('--learning_eps', default=None, type=float, help='eps value to use in the optimizer.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_EPS))\n    parser.add_argument('--learning_momentum', default=None, type=float, help='Momentum.  None uses a default for the given optimizer: {}'.format(DEFAULT_MOMENTUM))\n    parser.add_argument('--learning_weight_decay', default=None, type=float, help='Weight decay (eg, l2 reg) to use in the optimizer')\n    parser.add_argument('--learning_rho', default=DEFAULT_LEARNING_RHO, type=float, help='Rho parameter in Adadelta')\n    parser.add_argument('--learning_beta2', default=0.999, type=float, help='Beta2 argument for AdamW')\n    parser.add_argument('--optim', default=None, help='Optimizer type: SGD, AdamW, Adadelta, AdaBelief, Madgrad')\n    parser.add_argument('--stage1_learning_rate', default=None, type=float, help='Learning rate to use in the first stage of --multistage.  None means use default: {}'.format(DEFAULT_LEARNING_RATES['adadelta']))\n    parser.add_argument('--learning_rate_warmup', default=0, type=int, help=\"Number of epochs to ramp up learning rate from 0 to full.  Set to 0 to always use the chosen learning rate.  Currently not functional, as it didn't do anything\")\n    parser.add_argument('--learning_rate_factor', default=0.6, type=float, help='Plateau learning rate decreate when plateaued')\n    parser.add_argument('--learning_rate_patience', default=5, type=int, help='Plateau learning rate patience')\n    parser.add_argument('--learning_rate_cooldown', default=10, type=int, help='Plateau learning rate cooldown')\n    parser.add_argument('--learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum')\n    parser.add_argument('--stage1_learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum (stage 1)')\n    parser.add_argument('--grad_clipping', default=None, type=float, help='Clip abs(grad) to this amount.  Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--no_grad_clipping', action='store_const', const=None, dest='grad_clipping', help='Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--loss', default='cross', help='cross, large_margin, or focal.  Focal requires `pip install focal_loss_torch`')\n    parser.add_argument('--loss_focal_gamma', default=2, type=float, help='gamma value for a focal loss')\n    parser.add_argument('--word_dropout', default=0.2, type=float, help='Dropout on the word embedding')\n    parser.add_argument('--predict_dropout', default=0.2, type=float, help='Dropout on the final prediction layer')\n    parser.add_argument('--lstm_layer_dropout', default=0.0, type=float, help='Dropout in the LSTM layers')\n    parser.add_argument('--lstm_input_dropout', default=0.2, type=float, help='Dropout on the input to an LSTM')\n    parser.add_argument('--transition_scheme', default=TransitionScheme.IN_ORDER, type=lambda x: TransitionScheme[x.upper()], help='Transition scheme to use.  {}'.format(', '.join((x.name for x in TransitionScheme))))\n    parser.add_argument('--reversed', default=False, action='store_true', help='Do the transition sequence reversed')\n    parser.add_argument('--combined_dummy_embedding', default=True, action='store_true', help='Use the same embedding for dummy nodes and the vectors used when combining constituents')\n    parser.add_argument('--no_combined_dummy_embedding', dest='combined_dummy_embedding', action='store_false', help=\"Don't use the same embedding for dummy nodes and the vectors used when combining constituents\")\n    parser.add_argument('--nonlinearity', default='relu', choices=NONLINEARITY.keys(), help='Nonlinearity to use in the model.  relu is a noticeable improvement over tanh')\n    parser.add_argument('--maxout_k', default=None, type=int, help='Use maxout layers instead of a nonlinearity for the output layers')\n    parser.add_argument('--use_silver_words', default=True, dest='use_silver_words', action='store_true', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--no_use_silver_words', default=True, dest='use_silver_words', action='store_false', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--rare_word_unknown_frequency', default=0.02, type=float, help='How often to replace a rare word with UNK when training')\n    parser.add_argument('--rare_word_threshold', default=0.02, type=float, help='How many words to consider as rare words as a fraction of the dataset')\n    parser.add_argument('--tag_unknown_frequency', default=0.001, type=float, help='How often to replace a tag with UNK when training')\n    parser.add_argument('--num_lstm_layers', default=2, type=int, help='How many layers to use in the LSTMs')\n    parser.add_argument('--num_tree_lstm_layers', default=None, type=int, help='How many layers to use in the TREE_LSTMs, if used.  This also increases the width of the word outputs to match the tree lstm inputs.  Default 2 if TREE_LSTM or TREE_LSTM_CX, 1 otherwise')\n    parser.add_argument('--num_output_layers', default=3, type=int, help='How many layers to use at the prediction level')\n    parser.add_argument('--sentence_boundary_vectors', default=SentenceBoundary.EVERYTHING, type=lambda x: SentenceBoundary[x.upper()], help='Vectors to learn at the start & end of sentences.  {}'.format(', '.join((x.name for x in SentenceBoundary))))\n    parser.add_argument('--constituency_composition', default=ConstituencyComposition.MAX, type=lambda x: ConstituencyComposition[x.upper()], help='How to build a new composition from its children.  {}'.format(', '.join((x.name for x in ConstituencyComposition))))\n    parser.add_argument('--reduce_heads', default=8, type=int, help='Number of attn heads to use when reducing children into a parent tree (constituency_composition == attn)')\n    parser.add_argument('--reduce_position', default=None, type=int, help=\"Dimension of position vector to use when reducing children.  None means 1/4 hidden_size, 0 means don't use (constituency_composition == key | untied_key)\")\n    parser.add_argument('--relearn_structure', action='store_true', help='Starting from an existing checkpoint, add or remove pattn / lattn.  One thing that works well is to train an initial model using adadelta with no pattn, then add pattn with adamw')\n    parser.add_argument('--finetune', action='store_true', help='Load existing model during `train` mode from `load_name` path')\n    parser.add_argument('--checkpoint_save_name', type=str, default=None, help='File name to save the most recent checkpoint')\n    parser.add_argument('--no_checkpoint', dest='checkpoint', action='store_false', help=\"Don't save checkpoints\")\n    parser.add_argument('--load_name', type=str, default=None, help='Model to load when finetuning, evaluating, or manipulating an existing file')\n    parser.add_argument('--load_package', type=str, default=None, help='Download an existing stanza package & use this for tests, finetuning, etc')\n    retagging.add_retag_args(parser)\n    parser.add_argument('--pattn_d_model', default=1024, type=int, help='Partitioned attention model dimensionality')\n    parser.add_argument('--pattn_morpho_emb_dropout', default=0.2, type=float, help='Dropout rate for morphological features obtained from pretrained model')\n    parser.add_argument('--pattn_encoder_max_len', default=512, type=int, help='Max length that can be put into the transformer attention layer')\n    parser.add_argument('--pattn_num_heads', default=8, type=int, help='Partitioned attention model number of attention heads')\n    parser.add_argument('--pattn_d_kv', default=64, type=int, help='Size of the query and key vector')\n    parser.add_argument('--pattn_d_ff', default=2048, type=int, help='Size of the intermediate vectors in the feed-forward sublayer')\n    parser.add_argument('--pattn_relu_dropout', default=0.1, type=float, help='ReLU dropout probability in feed-forward sublayer')\n    parser.add_argument('--pattn_residual_dropout', default=0.2, type=float, help='Residual dropout probability for all residual connections')\n    parser.add_argument('--pattn_attention_dropout', default=0.2, type=float, help='Attention dropout probability')\n    parser.add_argument('--pattn_num_layers', default=0, type=int, help='Number of layers for the Partitioned Attention.  Currently turned off')\n    parser.add_argument('--pattn_bias', default=False, action='store_true', help='Whether or not to learn an additive bias')\n    parser.add_argument('--pattn_timing', default='sin', choices=['learned', 'sin'], help='Use a learned embedding or a sin embedding')\n    parser.add_argument('--lattn_d_input_proj', default=None, type=int, help='If set, project the non-positional inputs down to this size before proceeding.')\n    parser.add_argument('--lattn_d_kv', default=64, type=int, help='Dimension of the key/query vector')\n    parser.add_argument('--lattn_d_proj', default=64, type=int, help='Dimension of the output vector from each label attention head')\n    parser.add_argument('--lattn_resdrop', default=True, action='store_true', help='Whether or not to use Residual Dropout')\n    parser.add_argument('--lattn_pwff', default=True, action='store_true', help='Whether or not to use a Position-wise Feed-forward Layer')\n    parser.add_argument('--lattn_q_as_matrix', default=False, action='store_true', help='Whether or not Label Attention uses learned query vectors. False means it does')\n    parser.add_argument('--lattn_partitioned', default=True, action='store_true', help='Whether or not it is partitioned')\n    parser.add_argument('--no_lattn_partitioned', default=True, action='store_false', dest='lattn_partitioned', help='Whether or not it is partitioned')\n    parser.add_argument('--lattn_combine_as_self', default=False, action='store_true', help='Whether or not the layer uses concatenation. False means it does')\n    parser.add_argument('--lattn_d_l', default=32, type=int, help='Number of labels')\n    parser.add_argument('--lattn_attention_dropout', default=0.2, type=float, help='Dropout for attention layer')\n    parser.add_argument('--lattn_d_ff', default=2048, type=int, help='Dimension of the Feed-forward layer')\n    parser.add_argument('--lattn_relu_dropout', default=0.2, type=float, help='Relu dropout for the label attention')\n    parser.add_argument('--lattn_residual_dropout', default=0.2, type=float, help='Residual dropout for the label attention')\n    parser.add_argument('--lattn_combined_input', default=True, action='store_true', help='Combine all inputs for the lattn, not just the pattn')\n    parser.add_argument('--use_lattn', default=False, action='store_true', help='Use the lattn layers - currently turned off')\n    parser.add_argument('--no_lattn_combined_input', dest='lattn_combined_input', action='store_false', help=\"Don't combine all inputs for the lattn, not just the pattn\")\n    parser.add_argument('--log_norms', default=False, action='store_true', help='Log the parameters norms while training.  A very noisy option')\n    parser.add_argument('--log_shapes', default=False, action='store_true', help='Log the parameters shapes at the beginning')\n    parser.add_argument('--watch_regex', default=None, help='regex to describe which weights and biases to output, if any')\n    parser.add_argument('--wandb', action='store_true', help='Start a wandb session and write the results of training.  Only applies to training.  Use --wandb_name instead to specify a name')\n    parser.add_argument('--wandb_name', default=None, help='Name of a wandb session to start when training.  Will default to the dataset short name')\n    parser.add_argument('--wandb_norm_regex', default=None, help='Log on wandb any tensor whose norm matches this matrix.  Might get cluttered?')\n    return parser",
        "mutated": [
            "def build_argparse():\n    if False:\n        i = 10\n    '\\n    Adds the arguments for building the con parser\\n\\n    For the most part, defaults are set to cross-validated values, at least for WSJ\\n    '\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='data/constituency', help='Directory of constituency data.')\n    parser.add_argument('--wordvec_dir', type=str, default='extern_data/wordvec', help='Directory of word vectors')\n    parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')\n    parser.add_argument('--wordvec_pretrain_file', type=str, default=None, help='Exact name of the pretrain file to read')\n    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)\n    parser.add_argument('--charlm_forward_file', type=str, default=None, help='Exact path to use for forward charlm')\n    parser.add_argument('--charlm_backward_file', type=str, default=None, help='Exact path to use for backward charlm')\n    parser.add_argument('--bert_model', type=str, default=None, help='Use an external bert model (requires the transformers package)')\n    parser.add_argument('--no_bert_model', dest='bert_model', action='store_const', const=None, help=\"Don't use bert\")\n    parser.add_argument('--bert_hidden_layers', type=int, default=4, help='How many layers of hidden state to use from the transformer')\n    parser.add_argument('--bert_hidden_layers_original', action='store_const', const=None, dest='bert_hidden_layers', help='Use layers 2,3,4 of the Bert embedding')\n    parser.add_argument('--bert_finetune', default=False, action='store_true', help='Finetune the bert (or other transformer)')\n    parser.add_argument('--no_bert_finetune', dest='bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer)\")\n    parser.add_argument('--bert_finetune_layers', default=None, type=int, help='Only finetune this many layers from the transformer')\n    parser.add_argument('--bert_finetune_begin_epoch', default=None, type=int, help='Which epoch to start finetuning the transformer')\n    parser.add_argument('--bert_finetune_end_epoch', default=None, type=int, help='Which epoch to stop finetuning the transformer')\n    parser.add_argument('--bert_learning_rate', default=0.009, type=float, help='Scale the learning rate for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_learning_rate', default=None, type=float, help='Scale the learning rate for transformer finetuning by this much only during an AdaDelta warmup')\n    parser.add_argument('--bert_weight_decay', default=0.0001, type=float, help='Scale the weight decay for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_finetune', default=None, action='store_true', help=\"Finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--no_stage1_bert_finetune', dest='stage1_bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--tag_embedding_dim', type=int, default=20, help='Embedding size for a tag.  0 turns off the feature')\n    parser.add_argument('--delta_embedding_dim', type=int, default=100, help='Embedding size for a delta embedding')\n    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--silver_file', type=str, default=None, help='Secondary training file.')\n    parser.add_argument('--silver_remove_duplicates', default=False, action='store_true', help=\"Do/don't remove duplicates from the silver training file.  Could be useful for intentionally reweighting some trees\")\n    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--tokenized_file', type=str, default=None, help='Input file of tokenized text for parsing with parse_text.')\n    parser.add_argument('--tokenized_dir', type=str, default=None, help='Input directory of tokenized text for parsing with parse_text.')\n    parser.add_argument('--mode', default='train', choices=['train', 'parse_text', 'predict', 'remove_optimizer'])\n    parser.add_argument('--num_generate', type=int, default=0, help='When running a dev set, how many sentences to generate beyond the greedy one')\n    add_predict_output_args(parser)\n    parser.add_argument('--lang', type=str, help='Language')\n    parser.add_argument('--shorthand', type=str, help='Treebank shorthand')\n    parser.add_argument('--transition_embedding_dim', type=int, default=20, help='Embedding size for a transition')\n    parser.add_argument('--transition_hidden_size', type=int, default=20, help='Embedding size for transition stack')\n    parser.add_argument('--transition_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--transition_heads', default=4, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--constituent_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--constituent_heads', default=8, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--hidden_size', type=int, default=512, help='Size of the output layers for constituency stack and word queue')\n    parser.add_argument('--epochs', type=int, default=400)\n    parser.add_argument('--epoch_size', type=int, default=5000, help=\"Runs this many trees in an 'epoch' instead of going through the training dataset exactly once.  Set to 0 to do the whole training set\")\n    parser.add_argument('--silver_epoch_size', type=int, default=None, help=\"Runs this many trees in a silver 'epoch'.  If not set, will match --epoch_size\")\n    parser.add_argument('--multistage', default=True, action='store_true', help='1/2 epochs with adadelta no pattn or lattn, 1/4 with chosen optim and no lattn, 1/4 full model')\n    parser.add_argument('--no_multistage', dest='multistage', action='store_false', help=\"don't do the multistage learning\")\n    parser.add_argument('--oracle_initial_epoch', type=int, default=1, help='Epoch where we start using the dynamic oracle to let the parser keep going with wrong decisions')\n    parser.add_argument('--oracle_frequency', type=float, default=0.8, help='How often to use the oracle vs how often to force the correct transition')\n    parser.add_argument('--oracle_forced_errors', type=float, default=0.001, help='Occasionally have the model randomly walk through the state space to try to learn how to recover')\n    parser.add_argument('--oracle_level', type=int, default=None, help='Restrict oracle transitions to this level or lower.  0 means off.  None means use all oracle transitions.')\n    parser.add_argument('--train_batch_size', type=int, default=30, help='How many trees to train before taking an optimizer step')\n    parser.add_argument('--eval_batch_size', type=int, default=50, help='How many trees to batch when running eval')\n    parser.add_argument('--save_dir', type=str, default='saved_models/constituency', help='Root dir for saving models.')\n    parser.add_argument('--save_name', type=str, default='{shorthand}_{embedding}_{finetune}_constituency.pt', help='File name to save the model')\n    parser.add_argument('--save_each_name', type=str, default=None, help='Save each model in sequence to this pattern.  Mostly for testing')\n    parser.add_argument('--seed', type=int, default=1234)\n    utils.add_device_args(parser)\n    parser.add_argument('--learning_rate', default=None, type=float, help='Learning rate for the optimizer.  Reasonable values are 1.0 for adadelta or 0.001 for SGD.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_RATES))\n    parser.add_argument('--learning_eps', default=None, type=float, help='eps value to use in the optimizer.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_EPS))\n    parser.add_argument('--learning_momentum', default=None, type=float, help='Momentum.  None uses a default for the given optimizer: {}'.format(DEFAULT_MOMENTUM))\n    parser.add_argument('--learning_weight_decay', default=None, type=float, help='Weight decay (eg, l2 reg) to use in the optimizer')\n    parser.add_argument('--learning_rho', default=DEFAULT_LEARNING_RHO, type=float, help='Rho parameter in Adadelta')\n    parser.add_argument('--learning_beta2', default=0.999, type=float, help='Beta2 argument for AdamW')\n    parser.add_argument('--optim', default=None, help='Optimizer type: SGD, AdamW, Adadelta, AdaBelief, Madgrad')\n    parser.add_argument('--stage1_learning_rate', default=None, type=float, help='Learning rate to use in the first stage of --multistage.  None means use default: {}'.format(DEFAULT_LEARNING_RATES['adadelta']))\n    parser.add_argument('--learning_rate_warmup', default=0, type=int, help=\"Number of epochs to ramp up learning rate from 0 to full.  Set to 0 to always use the chosen learning rate.  Currently not functional, as it didn't do anything\")\n    parser.add_argument('--learning_rate_factor', default=0.6, type=float, help='Plateau learning rate decreate when plateaued')\n    parser.add_argument('--learning_rate_patience', default=5, type=int, help='Plateau learning rate patience')\n    parser.add_argument('--learning_rate_cooldown', default=10, type=int, help='Plateau learning rate cooldown')\n    parser.add_argument('--learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum')\n    parser.add_argument('--stage1_learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum (stage 1)')\n    parser.add_argument('--grad_clipping', default=None, type=float, help='Clip abs(grad) to this amount.  Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--no_grad_clipping', action='store_const', const=None, dest='grad_clipping', help='Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--loss', default='cross', help='cross, large_margin, or focal.  Focal requires `pip install focal_loss_torch`')\n    parser.add_argument('--loss_focal_gamma', default=2, type=float, help='gamma value for a focal loss')\n    parser.add_argument('--word_dropout', default=0.2, type=float, help='Dropout on the word embedding')\n    parser.add_argument('--predict_dropout', default=0.2, type=float, help='Dropout on the final prediction layer')\n    parser.add_argument('--lstm_layer_dropout', default=0.0, type=float, help='Dropout in the LSTM layers')\n    parser.add_argument('--lstm_input_dropout', default=0.2, type=float, help='Dropout on the input to an LSTM')\n    parser.add_argument('--transition_scheme', default=TransitionScheme.IN_ORDER, type=lambda x: TransitionScheme[x.upper()], help='Transition scheme to use.  {}'.format(', '.join((x.name for x in TransitionScheme))))\n    parser.add_argument('--reversed', default=False, action='store_true', help='Do the transition sequence reversed')\n    parser.add_argument('--combined_dummy_embedding', default=True, action='store_true', help='Use the same embedding for dummy nodes and the vectors used when combining constituents')\n    parser.add_argument('--no_combined_dummy_embedding', dest='combined_dummy_embedding', action='store_false', help=\"Don't use the same embedding for dummy nodes and the vectors used when combining constituents\")\n    parser.add_argument('--nonlinearity', default='relu', choices=NONLINEARITY.keys(), help='Nonlinearity to use in the model.  relu is a noticeable improvement over tanh')\n    parser.add_argument('--maxout_k', default=None, type=int, help='Use maxout layers instead of a nonlinearity for the output layers')\n    parser.add_argument('--use_silver_words', default=True, dest='use_silver_words', action='store_true', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--no_use_silver_words', default=True, dest='use_silver_words', action='store_false', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--rare_word_unknown_frequency', default=0.02, type=float, help='How often to replace a rare word with UNK when training')\n    parser.add_argument('--rare_word_threshold', default=0.02, type=float, help='How many words to consider as rare words as a fraction of the dataset')\n    parser.add_argument('--tag_unknown_frequency', default=0.001, type=float, help='How often to replace a tag with UNK when training')\n    parser.add_argument('--num_lstm_layers', default=2, type=int, help='How many layers to use in the LSTMs')\n    parser.add_argument('--num_tree_lstm_layers', default=None, type=int, help='How many layers to use in the TREE_LSTMs, if used.  This also increases the width of the word outputs to match the tree lstm inputs.  Default 2 if TREE_LSTM or TREE_LSTM_CX, 1 otherwise')\n    parser.add_argument('--num_output_layers', default=3, type=int, help='How many layers to use at the prediction level')\n    parser.add_argument('--sentence_boundary_vectors', default=SentenceBoundary.EVERYTHING, type=lambda x: SentenceBoundary[x.upper()], help='Vectors to learn at the start & end of sentences.  {}'.format(', '.join((x.name for x in SentenceBoundary))))\n    parser.add_argument('--constituency_composition', default=ConstituencyComposition.MAX, type=lambda x: ConstituencyComposition[x.upper()], help='How to build a new composition from its children.  {}'.format(', '.join((x.name for x in ConstituencyComposition))))\n    parser.add_argument('--reduce_heads', default=8, type=int, help='Number of attn heads to use when reducing children into a parent tree (constituency_composition == attn)')\n    parser.add_argument('--reduce_position', default=None, type=int, help=\"Dimension of position vector to use when reducing children.  None means 1/4 hidden_size, 0 means don't use (constituency_composition == key | untied_key)\")\n    parser.add_argument('--relearn_structure', action='store_true', help='Starting from an existing checkpoint, add or remove pattn / lattn.  One thing that works well is to train an initial model using adadelta with no pattn, then add pattn with adamw')\n    parser.add_argument('--finetune', action='store_true', help='Load existing model during `train` mode from `load_name` path')\n    parser.add_argument('--checkpoint_save_name', type=str, default=None, help='File name to save the most recent checkpoint')\n    parser.add_argument('--no_checkpoint', dest='checkpoint', action='store_false', help=\"Don't save checkpoints\")\n    parser.add_argument('--load_name', type=str, default=None, help='Model to load when finetuning, evaluating, or manipulating an existing file')\n    parser.add_argument('--load_package', type=str, default=None, help='Download an existing stanza package & use this for tests, finetuning, etc')\n    retagging.add_retag_args(parser)\n    parser.add_argument('--pattn_d_model', default=1024, type=int, help='Partitioned attention model dimensionality')\n    parser.add_argument('--pattn_morpho_emb_dropout', default=0.2, type=float, help='Dropout rate for morphological features obtained from pretrained model')\n    parser.add_argument('--pattn_encoder_max_len', default=512, type=int, help='Max length that can be put into the transformer attention layer')\n    parser.add_argument('--pattn_num_heads', default=8, type=int, help='Partitioned attention model number of attention heads')\n    parser.add_argument('--pattn_d_kv', default=64, type=int, help='Size of the query and key vector')\n    parser.add_argument('--pattn_d_ff', default=2048, type=int, help='Size of the intermediate vectors in the feed-forward sublayer')\n    parser.add_argument('--pattn_relu_dropout', default=0.1, type=float, help='ReLU dropout probability in feed-forward sublayer')\n    parser.add_argument('--pattn_residual_dropout', default=0.2, type=float, help='Residual dropout probability for all residual connections')\n    parser.add_argument('--pattn_attention_dropout', default=0.2, type=float, help='Attention dropout probability')\n    parser.add_argument('--pattn_num_layers', default=0, type=int, help='Number of layers for the Partitioned Attention.  Currently turned off')\n    parser.add_argument('--pattn_bias', default=False, action='store_true', help='Whether or not to learn an additive bias')\n    parser.add_argument('--pattn_timing', default='sin', choices=['learned', 'sin'], help='Use a learned embedding or a sin embedding')\n    parser.add_argument('--lattn_d_input_proj', default=None, type=int, help='If set, project the non-positional inputs down to this size before proceeding.')\n    parser.add_argument('--lattn_d_kv', default=64, type=int, help='Dimension of the key/query vector')\n    parser.add_argument('--lattn_d_proj', default=64, type=int, help='Dimension of the output vector from each label attention head')\n    parser.add_argument('--lattn_resdrop', default=True, action='store_true', help='Whether or not to use Residual Dropout')\n    parser.add_argument('--lattn_pwff', default=True, action='store_true', help='Whether or not to use a Position-wise Feed-forward Layer')\n    parser.add_argument('--lattn_q_as_matrix', default=False, action='store_true', help='Whether or not Label Attention uses learned query vectors. False means it does')\n    parser.add_argument('--lattn_partitioned', default=True, action='store_true', help='Whether or not it is partitioned')\n    parser.add_argument('--no_lattn_partitioned', default=True, action='store_false', dest='lattn_partitioned', help='Whether or not it is partitioned')\n    parser.add_argument('--lattn_combine_as_self', default=False, action='store_true', help='Whether or not the layer uses concatenation. False means it does')\n    parser.add_argument('--lattn_d_l', default=32, type=int, help='Number of labels')\n    parser.add_argument('--lattn_attention_dropout', default=0.2, type=float, help='Dropout for attention layer')\n    parser.add_argument('--lattn_d_ff', default=2048, type=int, help='Dimension of the Feed-forward layer')\n    parser.add_argument('--lattn_relu_dropout', default=0.2, type=float, help='Relu dropout for the label attention')\n    parser.add_argument('--lattn_residual_dropout', default=0.2, type=float, help='Residual dropout for the label attention')\n    parser.add_argument('--lattn_combined_input', default=True, action='store_true', help='Combine all inputs for the lattn, not just the pattn')\n    parser.add_argument('--use_lattn', default=False, action='store_true', help='Use the lattn layers - currently turned off')\n    parser.add_argument('--no_lattn_combined_input', dest='lattn_combined_input', action='store_false', help=\"Don't combine all inputs for the lattn, not just the pattn\")\n    parser.add_argument('--log_norms', default=False, action='store_true', help='Log the parameters norms while training.  A very noisy option')\n    parser.add_argument('--log_shapes', default=False, action='store_true', help='Log the parameters shapes at the beginning')\n    parser.add_argument('--watch_regex', default=None, help='regex to describe which weights and biases to output, if any')\n    parser.add_argument('--wandb', action='store_true', help='Start a wandb session and write the results of training.  Only applies to training.  Use --wandb_name instead to specify a name')\n    parser.add_argument('--wandb_name', default=None, help='Name of a wandb session to start when training.  Will default to the dataset short name')\n    parser.add_argument('--wandb_norm_regex', default=None, help='Log on wandb any tensor whose norm matches this matrix.  Might get cluttered?')\n    return parser",
            "def build_argparse():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Adds the arguments for building the con parser\\n\\n    For the most part, defaults are set to cross-validated values, at least for WSJ\\n    '\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='data/constituency', help='Directory of constituency data.')\n    parser.add_argument('--wordvec_dir', type=str, default='extern_data/wordvec', help='Directory of word vectors')\n    parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')\n    parser.add_argument('--wordvec_pretrain_file', type=str, default=None, help='Exact name of the pretrain file to read')\n    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)\n    parser.add_argument('--charlm_forward_file', type=str, default=None, help='Exact path to use for forward charlm')\n    parser.add_argument('--charlm_backward_file', type=str, default=None, help='Exact path to use for backward charlm')\n    parser.add_argument('--bert_model', type=str, default=None, help='Use an external bert model (requires the transformers package)')\n    parser.add_argument('--no_bert_model', dest='bert_model', action='store_const', const=None, help=\"Don't use bert\")\n    parser.add_argument('--bert_hidden_layers', type=int, default=4, help='How many layers of hidden state to use from the transformer')\n    parser.add_argument('--bert_hidden_layers_original', action='store_const', const=None, dest='bert_hidden_layers', help='Use layers 2,3,4 of the Bert embedding')\n    parser.add_argument('--bert_finetune', default=False, action='store_true', help='Finetune the bert (or other transformer)')\n    parser.add_argument('--no_bert_finetune', dest='bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer)\")\n    parser.add_argument('--bert_finetune_layers', default=None, type=int, help='Only finetune this many layers from the transformer')\n    parser.add_argument('--bert_finetune_begin_epoch', default=None, type=int, help='Which epoch to start finetuning the transformer')\n    parser.add_argument('--bert_finetune_end_epoch', default=None, type=int, help='Which epoch to stop finetuning the transformer')\n    parser.add_argument('--bert_learning_rate', default=0.009, type=float, help='Scale the learning rate for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_learning_rate', default=None, type=float, help='Scale the learning rate for transformer finetuning by this much only during an AdaDelta warmup')\n    parser.add_argument('--bert_weight_decay', default=0.0001, type=float, help='Scale the weight decay for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_finetune', default=None, action='store_true', help=\"Finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--no_stage1_bert_finetune', dest='stage1_bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--tag_embedding_dim', type=int, default=20, help='Embedding size for a tag.  0 turns off the feature')\n    parser.add_argument('--delta_embedding_dim', type=int, default=100, help='Embedding size for a delta embedding')\n    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--silver_file', type=str, default=None, help='Secondary training file.')\n    parser.add_argument('--silver_remove_duplicates', default=False, action='store_true', help=\"Do/don't remove duplicates from the silver training file.  Could be useful for intentionally reweighting some trees\")\n    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--tokenized_file', type=str, default=None, help='Input file of tokenized text for parsing with parse_text.')\n    parser.add_argument('--tokenized_dir', type=str, default=None, help='Input directory of tokenized text for parsing with parse_text.')\n    parser.add_argument('--mode', default='train', choices=['train', 'parse_text', 'predict', 'remove_optimizer'])\n    parser.add_argument('--num_generate', type=int, default=0, help='When running a dev set, how many sentences to generate beyond the greedy one')\n    add_predict_output_args(parser)\n    parser.add_argument('--lang', type=str, help='Language')\n    parser.add_argument('--shorthand', type=str, help='Treebank shorthand')\n    parser.add_argument('--transition_embedding_dim', type=int, default=20, help='Embedding size for a transition')\n    parser.add_argument('--transition_hidden_size', type=int, default=20, help='Embedding size for transition stack')\n    parser.add_argument('--transition_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--transition_heads', default=4, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--constituent_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--constituent_heads', default=8, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--hidden_size', type=int, default=512, help='Size of the output layers for constituency stack and word queue')\n    parser.add_argument('--epochs', type=int, default=400)\n    parser.add_argument('--epoch_size', type=int, default=5000, help=\"Runs this many trees in an 'epoch' instead of going through the training dataset exactly once.  Set to 0 to do the whole training set\")\n    parser.add_argument('--silver_epoch_size', type=int, default=None, help=\"Runs this many trees in a silver 'epoch'.  If not set, will match --epoch_size\")\n    parser.add_argument('--multistage', default=True, action='store_true', help='1/2 epochs with adadelta no pattn or lattn, 1/4 with chosen optim and no lattn, 1/4 full model')\n    parser.add_argument('--no_multistage', dest='multistage', action='store_false', help=\"don't do the multistage learning\")\n    parser.add_argument('--oracle_initial_epoch', type=int, default=1, help='Epoch where we start using the dynamic oracle to let the parser keep going with wrong decisions')\n    parser.add_argument('--oracle_frequency', type=float, default=0.8, help='How often to use the oracle vs how often to force the correct transition')\n    parser.add_argument('--oracle_forced_errors', type=float, default=0.001, help='Occasionally have the model randomly walk through the state space to try to learn how to recover')\n    parser.add_argument('--oracle_level', type=int, default=None, help='Restrict oracle transitions to this level or lower.  0 means off.  None means use all oracle transitions.')\n    parser.add_argument('--train_batch_size', type=int, default=30, help='How many trees to train before taking an optimizer step')\n    parser.add_argument('--eval_batch_size', type=int, default=50, help='How many trees to batch when running eval')\n    parser.add_argument('--save_dir', type=str, default='saved_models/constituency', help='Root dir for saving models.')\n    parser.add_argument('--save_name', type=str, default='{shorthand}_{embedding}_{finetune}_constituency.pt', help='File name to save the model')\n    parser.add_argument('--save_each_name', type=str, default=None, help='Save each model in sequence to this pattern.  Mostly for testing')\n    parser.add_argument('--seed', type=int, default=1234)\n    utils.add_device_args(parser)\n    parser.add_argument('--learning_rate', default=None, type=float, help='Learning rate for the optimizer.  Reasonable values are 1.0 for adadelta or 0.001 for SGD.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_RATES))\n    parser.add_argument('--learning_eps', default=None, type=float, help='eps value to use in the optimizer.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_EPS))\n    parser.add_argument('--learning_momentum', default=None, type=float, help='Momentum.  None uses a default for the given optimizer: {}'.format(DEFAULT_MOMENTUM))\n    parser.add_argument('--learning_weight_decay', default=None, type=float, help='Weight decay (eg, l2 reg) to use in the optimizer')\n    parser.add_argument('--learning_rho', default=DEFAULT_LEARNING_RHO, type=float, help='Rho parameter in Adadelta')\n    parser.add_argument('--learning_beta2', default=0.999, type=float, help='Beta2 argument for AdamW')\n    parser.add_argument('--optim', default=None, help='Optimizer type: SGD, AdamW, Adadelta, AdaBelief, Madgrad')\n    parser.add_argument('--stage1_learning_rate', default=None, type=float, help='Learning rate to use in the first stage of --multistage.  None means use default: {}'.format(DEFAULT_LEARNING_RATES['adadelta']))\n    parser.add_argument('--learning_rate_warmup', default=0, type=int, help=\"Number of epochs to ramp up learning rate from 0 to full.  Set to 0 to always use the chosen learning rate.  Currently not functional, as it didn't do anything\")\n    parser.add_argument('--learning_rate_factor', default=0.6, type=float, help='Plateau learning rate decreate when plateaued')\n    parser.add_argument('--learning_rate_patience', default=5, type=int, help='Plateau learning rate patience')\n    parser.add_argument('--learning_rate_cooldown', default=10, type=int, help='Plateau learning rate cooldown')\n    parser.add_argument('--learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum')\n    parser.add_argument('--stage1_learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum (stage 1)')\n    parser.add_argument('--grad_clipping', default=None, type=float, help='Clip abs(grad) to this amount.  Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--no_grad_clipping', action='store_const', const=None, dest='grad_clipping', help='Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--loss', default='cross', help='cross, large_margin, or focal.  Focal requires `pip install focal_loss_torch`')\n    parser.add_argument('--loss_focal_gamma', default=2, type=float, help='gamma value for a focal loss')\n    parser.add_argument('--word_dropout', default=0.2, type=float, help='Dropout on the word embedding')\n    parser.add_argument('--predict_dropout', default=0.2, type=float, help='Dropout on the final prediction layer')\n    parser.add_argument('--lstm_layer_dropout', default=0.0, type=float, help='Dropout in the LSTM layers')\n    parser.add_argument('--lstm_input_dropout', default=0.2, type=float, help='Dropout on the input to an LSTM')\n    parser.add_argument('--transition_scheme', default=TransitionScheme.IN_ORDER, type=lambda x: TransitionScheme[x.upper()], help='Transition scheme to use.  {}'.format(', '.join((x.name for x in TransitionScheme))))\n    parser.add_argument('--reversed', default=False, action='store_true', help='Do the transition sequence reversed')\n    parser.add_argument('--combined_dummy_embedding', default=True, action='store_true', help='Use the same embedding for dummy nodes and the vectors used when combining constituents')\n    parser.add_argument('--no_combined_dummy_embedding', dest='combined_dummy_embedding', action='store_false', help=\"Don't use the same embedding for dummy nodes and the vectors used when combining constituents\")\n    parser.add_argument('--nonlinearity', default='relu', choices=NONLINEARITY.keys(), help='Nonlinearity to use in the model.  relu is a noticeable improvement over tanh')\n    parser.add_argument('--maxout_k', default=None, type=int, help='Use maxout layers instead of a nonlinearity for the output layers')\n    parser.add_argument('--use_silver_words', default=True, dest='use_silver_words', action='store_true', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--no_use_silver_words', default=True, dest='use_silver_words', action='store_false', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--rare_word_unknown_frequency', default=0.02, type=float, help='How often to replace a rare word with UNK when training')\n    parser.add_argument('--rare_word_threshold', default=0.02, type=float, help='How many words to consider as rare words as a fraction of the dataset')\n    parser.add_argument('--tag_unknown_frequency', default=0.001, type=float, help='How often to replace a tag with UNK when training')\n    parser.add_argument('--num_lstm_layers', default=2, type=int, help='How many layers to use in the LSTMs')\n    parser.add_argument('--num_tree_lstm_layers', default=None, type=int, help='How many layers to use in the TREE_LSTMs, if used.  This also increases the width of the word outputs to match the tree lstm inputs.  Default 2 if TREE_LSTM or TREE_LSTM_CX, 1 otherwise')\n    parser.add_argument('--num_output_layers', default=3, type=int, help='How many layers to use at the prediction level')\n    parser.add_argument('--sentence_boundary_vectors', default=SentenceBoundary.EVERYTHING, type=lambda x: SentenceBoundary[x.upper()], help='Vectors to learn at the start & end of sentences.  {}'.format(', '.join((x.name for x in SentenceBoundary))))\n    parser.add_argument('--constituency_composition', default=ConstituencyComposition.MAX, type=lambda x: ConstituencyComposition[x.upper()], help='How to build a new composition from its children.  {}'.format(', '.join((x.name for x in ConstituencyComposition))))\n    parser.add_argument('--reduce_heads', default=8, type=int, help='Number of attn heads to use when reducing children into a parent tree (constituency_composition == attn)')\n    parser.add_argument('--reduce_position', default=None, type=int, help=\"Dimension of position vector to use when reducing children.  None means 1/4 hidden_size, 0 means don't use (constituency_composition == key | untied_key)\")\n    parser.add_argument('--relearn_structure', action='store_true', help='Starting from an existing checkpoint, add or remove pattn / lattn.  One thing that works well is to train an initial model using adadelta with no pattn, then add pattn with adamw')\n    parser.add_argument('--finetune', action='store_true', help='Load existing model during `train` mode from `load_name` path')\n    parser.add_argument('--checkpoint_save_name', type=str, default=None, help='File name to save the most recent checkpoint')\n    parser.add_argument('--no_checkpoint', dest='checkpoint', action='store_false', help=\"Don't save checkpoints\")\n    parser.add_argument('--load_name', type=str, default=None, help='Model to load when finetuning, evaluating, or manipulating an existing file')\n    parser.add_argument('--load_package', type=str, default=None, help='Download an existing stanza package & use this for tests, finetuning, etc')\n    retagging.add_retag_args(parser)\n    parser.add_argument('--pattn_d_model', default=1024, type=int, help='Partitioned attention model dimensionality')\n    parser.add_argument('--pattn_morpho_emb_dropout', default=0.2, type=float, help='Dropout rate for morphological features obtained from pretrained model')\n    parser.add_argument('--pattn_encoder_max_len', default=512, type=int, help='Max length that can be put into the transformer attention layer')\n    parser.add_argument('--pattn_num_heads', default=8, type=int, help='Partitioned attention model number of attention heads')\n    parser.add_argument('--pattn_d_kv', default=64, type=int, help='Size of the query and key vector')\n    parser.add_argument('--pattn_d_ff', default=2048, type=int, help='Size of the intermediate vectors in the feed-forward sublayer')\n    parser.add_argument('--pattn_relu_dropout', default=0.1, type=float, help='ReLU dropout probability in feed-forward sublayer')\n    parser.add_argument('--pattn_residual_dropout', default=0.2, type=float, help='Residual dropout probability for all residual connections')\n    parser.add_argument('--pattn_attention_dropout', default=0.2, type=float, help='Attention dropout probability')\n    parser.add_argument('--pattn_num_layers', default=0, type=int, help='Number of layers for the Partitioned Attention.  Currently turned off')\n    parser.add_argument('--pattn_bias', default=False, action='store_true', help='Whether or not to learn an additive bias')\n    parser.add_argument('--pattn_timing', default='sin', choices=['learned', 'sin'], help='Use a learned embedding or a sin embedding')\n    parser.add_argument('--lattn_d_input_proj', default=None, type=int, help='If set, project the non-positional inputs down to this size before proceeding.')\n    parser.add_argument('--lattn_d_kv', default=64, type=int, help='Dimension of the key/query vector')\n    parser.add_argument('--lattn_d_proj', default=64, type=int, help='Dimension of the output vector from each label attention head')\n    parser.add_argument('--lattn_resdrop', default=True, action='store_true', help='Whether or not to use Residual Dropout')\n    parser.add_argument('--lattn_pwff', default=True, action='store_true', help='Whether or not to use a Position-wise Feed-forward Layer')\n    parser.add_argument('--lattn_q_as_matrix', default=False, action='store_true', help='Whether or not Label Attention uses learned query vectors. False means it does')\n    parser.add_argument('--lattn_partitioned', default=True, action='store_true', help='Whether or not it is partitioned')\n    parser.add_argument('--no_lattn_partitioned', default=True, action='store_false', dest='lattn_partitioned', help='Whether or not it is partitioned')\n    parser.add_argument('--lattn_combine_as_self', default=False, action='store_true', help='Whether or not the layer uses concatenation. False means it does')\n    parser.add_argument('--lattn_d_l', default=32, type=int, help='Number of labels')\n    parser.add_argument('--lattn_attention_dropout', default=0.2, type=float, help='Dropout for attention layer')\n    parser.add_argument('--lattn_d_ff', default=2048, type=int, help='Dimension of the Feed-forward layer')\n    parser.add_argument('--lattn_relu_dropout', default=0.2, type=float, help='Relu dropout for the label attention')\n    parser.add_argument('--lattn_residual_dropout', default=0.2, type=float, help='Residual dropout for the label attention')\n    parser.add_argument('--lattn_combined_input', default=True, action='store_true', help='Combine all inputs for the lattn, not just the pattn')\n    parser.add_argument('--use_lattn', default=False, action='store_true', help='Use the lattn layers - currently turned off')\n    parser.add_argument('--no_lattn_combined_input', dest='lattn_combined_input', action='store_false', help=\"Don't combine all inputs for the lattn, not just the pattn\")\n    parser.add_argument('--log_norms', default=False, action='store_true', help='Log the parameters norms while training.  A very noisy option')\n    parser.add_argument('--log_shapes', default=False, action='store_true', help='Log the parameters shapes at the beginning')\n    parser.add_argument('--watch_regex', default=None, help='regex to describe which weights and biases to output, if any')\n    parser.add_argument('--wandb', action='store_true', help='Start a wandb session and write the results of training.  Only applies to training.  Use --wandb_name instead to specify a name')\n    parser.add_argument('--wandb_name', default=None, help='Name of a wandb session to start when training.  Will default to the dataset short name')\n    parser.add_argument('--wandb_norm_regex', default=None, help='Log on wandb any tensor whose norm matches this matrix.  Might get cluttered?')\n    return parser",
            "def build_argparse():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Adds the arguments for building the con parser\\n\\n    For the most part, defaults are set to cross-validated values, at least for WSJ\\n    '\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='data/constituency', help='Directory of constituency data.')\n    parser.add_argument('--wordvec_dir', type=str, default='extern_data/wordvec', help='Directory of word vectors')\n    parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')\n    parser.add_argument('--wordvec_pretrain_file', type=str, default=None, help='Exact name of the pretrain file to read')\n    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)\n    parser.add_argument('--charlm_forward_file', type=str, default=None, help='Exact path to use for forward charlm')\n    parser.add_argument('--charlm_backward_file', type=str, default=None, help='Exact path to use for backward charlm')\n    parser.add_argument('--bert_model', type=str, default=None, help='Use an external bert model (requires the transformers package)')\n    parser.add_argument('--no_bert_model', dest='bert_model', action='store_const', const=None, help=\"Don't use bert\")\n    parser.add_argument('--bert_hidden_layers', type=int, default=4, help='How many layers of hidden state to use from the transformer')\n    parser.add_argument('--bert_hidden_layers_original', action='store_const', const=None, dest='bert_hidden_layers', help='Use layers 2,3,4 of the Bert embedding')\n    parser.add_argument('--bert_finetune', default=False, action='store_true', help='Finetune the bert (or other transformer)')\n    parser.add_argument('--no_bert_finetune', dest='bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer)\")\n    parser.add_argument('--bert_finetune_layers', default=None, type=int, help='Only finetune this many layers from the transformer')\n    parser.add_argument('--bert_finetune_begin_epoch', default=None, type=int, help='Which epoch to start finetuning the transformer')\n    parser.add_argument('--bert_finetune_end_epoch', default=None, type=int, help='Which epoch to stop finetuning the transformer')\n    parser.add_argument('--bert_learning_rate', default=0.009, type=float, help='Scale the learning rate for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_learning_rate', default=None, type=float, help='Scale the learning rate for transformer finetuning by this much only during an AdaDelta warmup')\n    parser.add_argument('--bert_weight_decay', default=0.0001, type=float, help='Scale the weight decay for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_finetune', default=None, action='store_true', help=\"Finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--no_stage1_bert_finetune', dest='stage1_bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--tag_embedding_dim', type=int, default=20, help='Embedding size for a tag.  0 turns off the feature')\n    parser.add_argument('--delta_embedding_dim', type=int, default=100, help='Embedding size for a delta embedding')\n    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--silver_file', type=str, default=None, help='Secondary training file.')\n    parser.add_argument('--silver_remove_duplicates', default=False, action='store_true', help=\"Do/don't remove duplicates from the silver training file.  Could be useful for intentionally reweighting some trees\")\n    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--tokenized_file', type=str, default=None, help='Input file of tokenized text for parsing with parse_text.')\n    parser.add_argument('--tokenized_dir', type=str, default=None, help='Input directory of tokenized text for parsing with parse_text.')\n    parser.add_argument('--mode', default='train', choices=['train', 'parse_text', 'predict', 'remove_optimizer'])\n    parser.add_argument('--num_generate', type=int, default=0, help='When running a dev set, how many sentences to generate beyond the greedy one')\n    add_predict_output_args(parser)\n    parser.add_argument('--lang', type=str, help='Language')\n    parser.add_argument('--shorthand', type=str, help='Treebank shorthand')\n    parser.add_argument('--transition_embedding_dim', type=int, default=20, help='Embedding size for a transition')\n    parser.add_argument('--transition_hidden_size', type=int, default=20, help='Embedding size for transition stack')\n    parser.add_argument('--transition_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--transition_heads', default=4, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--constituent_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--constituent_heads', default=8, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--hidden_size', type=int, default=512, help='Size of the output layers for constituency stack and word queue')\n    parser.add_argument('--epochs', type=int, default=400)\n    parser.add_argument('--epoch_size', type=int, default=5000, help=\"Runs this many trees in an 'epoch' instead of going through the training dataset exactly once.  Set to 0 to do the whole training set\")\n    parser.add_argument('--silver_epoch_size', type=int, default=None, help=\"Runs this many trees in a silver 'epoch'.  If not set, will match --epoch_size\")\n    parser.add_argument('--multistage', default=True, action='store_true', help='1/2 epochs with adadelta no pattn or lattn, 1/4 with chosen optim and no lattn, 1/4 full model')\n    parser.add_argument('--no_multistage', dest='multistage', action='store_false', help=\"don't do the multistage learning\")\n    parser.add_argument('--oracle_initial_epoch', type=int, default=1, help='Epoch where we start using the dynamic oracle to let the parser keep going with wrong decisions')\n    parser.add_argument('--oracle_frequency', type=float, default=0.8, help='How often to use the oracle vs how often to force the correct transition')\n    parser.add_argument('--oracle_forced_errors', type=float, default=0.001, help='Occasionally have the model randomly walk through the state space to try to learn how to recover')\n    parser.add_argument('--oracle_level', type=int, default=None, help='Restrict oracle transitions to this level or lower.  0 means off.  None means use all oracle transitions.')\n    parser.add_argument('--train_batch_size', type=int, default=30, help='How many trees to train before taking an optimizer step')\n    parser.add_argument('--eval_batch_size', type=int, default=50, help='How many trees to batch when running eval')\n    parser.add_argument('--save_dir', type=str, default='saved_models/constituency', help='Root dir for saving models.')\n    parser.add_argument('--save_name', type=str, default='{shorthand}_{embedding}_{finetune}_constituency.pt', help='File name to save the model')\n    parser.add_argument('--save_each_name', type=str, default=None, help='Save each model in sequence to this pattern.  Mostly for testing')\n    parser.add_argument('--seed', type=int, default=1234)\n    utils.add_device_args(parser)\n    parser.add_argument('--learning_rate', default=None, type=float, help='Learning rate for the optimizer.  Reasonable values are 1.0 for adadelta or 0.001 for SGD.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_RATES))\n    parser.add_argument('--learning_eps', default=None, type=float, help='eps value to use in the optimizer.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_EPS))\n    parser.add_argument('--learning_momentum', default=None, type=float, help='Momentum.  None uses a default for the given optimizer: {}'.format(DEFAULT_MOMENTUM))\n    parser.add_argument('--learning_weight_decay', default=None, type=float, help='Weight decay (eg, l2 reg) to use in the optimizer')\n    parser.add_argument('--learning_rho', default=DEFAULT_LEARNING_RHO, type=float, help='Rho parameter in Adadelta')\n    parser.add_argument('--learning_beta2', default=0.999, type=float, help='Beta2 argument for AdamW')\n    parser.add_argument('--optim', default=None, help='Optimizer type: SGD, AdamW, Adadelta, AdaBelief, Madgrad')\n    parser.add_argument('--stage1_learning_rate', default=None, type=float, help='Learning rate to use in the first stage of --multistage.  None means use default: {}'.format(DEFAULT_LEARNING_RATES['adadelta']))\n    parser.add_argument('--learning_rate_warmup', default=0, type=int, help=\"Number of epochs to ramp up learning rate from 0 to full.  Set to 0 to always use the chosen learning rate.  Currently not functional, as it didn't do anything\")\n    parser.add_argument('--learning_rate_factor', default=0.6, type=float, help='Plateau learning rate decreate when plateaued')\n    parser.add_argument('--learning_rate_patience', default=5, type=int, help='Plateau learning rate patience')\n    parser.add_argument('--learning_rate_cooldown', default=10, type=int, help='Plateau learning rate cooldown')\n    parser.add_argument('--learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum')\n    parser.add_argument('--stage1_learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum (stage 1)')\n    parser.add_argument('--grad_clipping', default=None, type=float, help='Clip abs(grad) to this amount.  Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--no_grad_clipping', action='store_const', const=None, dest='grad_clipping', help='Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--loss', default='cross', help='cross, large_margin, or focal.  Focal requires `pip install focal_loss_torch`')\n    parser.add_argument('--loss_focal_gamma', default=2, type=float, help='gamma value for a focal loss')\n    parser.add_argument('--word_dropout', default=0.2, type=float, help='Dropout on the word embedding')\n    parser.add_argument('--predict_dropout', default=0.2, type=float, help='Dropout on the final prediction layer')\n    parser.add_argument('--lstm_layer_dropout', default=0.0, type=float, help='Dropout in the LSTM layers')\n    parser.add_argument('--lstm_input_dropout', default=0.2, type=float, help='Dropout on the input to an LSTM')\n    parser.add_argument('--transition_scheme', default=TransitionScheme.IN_ORDER, type=lambda x: TransitionScheme[x.upper()], help='Transition scheme to use.  {}'.format(', '.join((x.name for x in TransitionScheme))))\n    parser.add_argument('--reversed', default=False, action='store_true', help='Do the transition sequence reversed')\n    parser.add_argument('--combined_dummy_embedding', default=True, action='store_true', help='Use the same embedding for dummy nodes and the vectors used when combining constituents')\n    parser.add_argument('--no_combined_dummy_embedding', dest='combined_dummy_embedding', action='store_false', help=\"Don't use the same embedding for dummy nodes and the vectors used when combining constituents\")\n    parser.add_argument('--nonlinearity', default='relu', choices=NONLINEARITY.keys(), help='Nonlinearity to use in the model.  relu is a noticeable improvement over tanh')\n    parser.add_argument('--maxout_k', default=None, type=int, help='Use maxout layers instead of a nonlinearity for the output layers')\n    parser.add_argument('--use_silver_words', default=True, dest='use_silver_words', action='store_true', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--no_use_silver_words', default=True, dest='use_silver_words', action='store_false', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--rare_word_unknown_frequency', default=0.02, type=float, help='How often to replace a rare word with UNK when training')\n    parser.add_argument('--rare_word_threshold', default=0.02, type=float, help='How many words to consider as rare words as a fraction of the dataset')\n    parser.add_argument('--tag_unknown_frequency', default=0.001, type=float, help='How often to replace a tag with UNK when training')\n    parser.add_argument('--num_lstm_layers', default=2, type=int, help='How many layers to use in the LSTMs')\n    parser.add_argument('--num_tree_lstm_layers', default=None, type=int, help='How many layers to use in the TREE_LSTMs, if used.  This also increases the width of the word outputs to match the tree lstm inputs.  Default 2 if TREE_LSTM or TREE_LSTM_CX, 1 otherwise')\n    parser.add_argument('--num_output_layers', default=3, type=int, help='How many layers to use at the prediction level')\n    parser.add_argument('--sentence_boundary_vectors', default=SentenceBoundary.EVERYTHING, type=lambda x: SentenceBoundary[x.upper()], help='Vectors to learn at the start & end of sentences.  {}'.format(', '.join((x.name for x in SentenceBoundary))))\n    parser.add_argument('--constituency_composition', default=ConstituencyComposition.MAX, type=lambda x: ConstituencyComposition[x.upper()], help='How to build a new composition from its children.  {}'.format(', '.join((x.name for x in ConstituencyComposition))))\n    parser.add_argument('--reduce_heads', default=8, type=int, help='Number of attn heads to use when reducing children into a parent tree (constituency_composition == attn)')\n    parser.add_argument('--reduce_position', default=None, type=int, help=\"Dimension of position vector to use when reducing children.  None means 1/4 hidden_size, 0 means don't use (constituency_composition == key | untied_key)\")\n    parser.add_argument('--relearn_structure', action='store_true', help='Starting from an existing checkpoint, add or remove pattn / lattn.  One thing that works well is to train an initial model using adadelta with no pattn, then add pattn with adamw')\n    parser.add_argument('--finetune', action='store_true', help='Load existing model during `train` mode from `load_name` path')\n    parser.add_argument('--checkpoint_save_name', type=str, default=None, help='File name to save the most recent checkpoint')\n    parser.add_argument('--no_checkpoint', dest='checkpoint', action='store_false', help=\"Don't save checkpoints\")\n    parser.add_argument('--load_name', type=str, default=None, help='Model to load when finetuning, evaluating, or manipulating an existing file')\n    parser.add_argument('--load_package', type=str, default=None, help='Download an existing stanza package & use this for tests, finetuning, etc')\n    retagging.add_retag_args(parser)\n    parser.add_argument('--pattn_d_model', default=1024, type=int, help='Partitioned attention model dimensionality')\n    parser.add_argument('--pattn_morpho_emb_dropout', default=0.2, type=float, help='Dropout rate for morphological features obtained from pretrained model')\n    parser.add_argument('--pattn_encoder_max_len', default=512, type=int, help='Max length that can be put into the transformer attention layer')\n    parser.add_argument('--pattn_num_heads', default=8, type=int, help='Partitioned attention model number of attention heads')\n    parser.add_argument('--pattn_d_kv', default=64, type=int, help='Size of the query and key vector')\n    parser.add_argument('--pattn_d_ff', default=2048, type=int, help='Size of the intermediate vectors in the feed-forward sublayer')\n    parser.add_argument('--pattn_relu_dropout', default=0.1, type=float, help='ReLU dropout probability in feed-forward sublayer')\n    parser.add_argument('--pattn_residual_dropout', default=0.2, type=float, help='Residual dropout probability for all residual connections')\n    parser.add_argument('--pattn_attention_dropout', default=0.2, type=float, help='Attention dropout probability')\n    parser.add_argument('--pattn_num_layers', default=0, type=int, help='Number of layers for the Partitioned Attention.  Currently turned off')\n    parser.add_argument('--pattn_bias', default=False, action='store_true', help='Whether or not to learn an additive bias')\n    parser.add_argument('--pattn_timing', default='sin', choices=['learned', 'sin'], help='Use a learned embedding or a sin embedding')\n    parser.add_argument('--lattn_d_input_proj', default=None, type=int, help='If set, project the non-positional inputs down to this size before proceeding.')\n    parser.add_argument('--lattn_d_kv', default=64, type=int, help='Dimension of the key/query vector')\n    parser.add_argument('--lattn_d_proj', default=64, type=int, help='Dimension of the output vector from each label attention head')\n    parser.add_argument('--lattn_resdrop', default=True, action='store_true', help='Whether or not to use Residual Dropout')\n    parser.add_argument('--lattn_pwff', default=True, action='store_true', help='Whether or not to use a Position-wise Feed-forward Layer')\n    parser.add_argument('--lattn_q_as_matrix', default=False, action='store_true', help='Whether or not Label Attention uses learned query vectors. False means it does')\n    parser.add_argument('--lattn_partitioned', default=True, action='store_true', help='Whether or not it is partitioned')\n    parser.add_argument('--no_lattn_partitioned', default=True, action='store_false', dest='lattn_partitioned', help='Whether or not it is partitioned')\n    parser.add_argument('--lattn_combine_as_self', default=False, action='store_true', help='Whether or not the layer uses concatenation. False means it does')\n    parser.add_argument('--lattn_d_l', default=32, type=int, help='Number of labels')\n    parser.add_argument('--lattn_attention_dropout', default=0.2, type=float, help='Dropout for attention layer')\n    parser.add_argument('--lattn_d_ff', default=2048, type=int, help='Dimension of the Feed-forward layer')\n    parser.add_argument('--lattn_relu_dropout', default=0.2, type=float, help='Relu dropout for the label attention')\n    parser.add_argument('--lattn_residual_dropout', default=0.2, type=float, help='Residual dropout for the label attention')\n    parser.add_argument('--lattn_combined_input', default=True, action='store_true', help='Combine all inputs for the lattn, not just the pattn')\n    parser.add_argument('--use_lattn', default=False, action='store_true', help='Use the lattn layers - currently turned off')\n    parser.add_argument('--no_lattn_combined_input', dest='lattn_combined_input', action='store_false', help=\"Don't combine all inputs for the lattn, not just the pattn\")\n    parser.add_argument('--log_norms', default=False, action='store_true', help='Log the parameters norms while training.  A very noisy option')\n    parser.add_argument('--log_shapes', default=False, action='store_true', help='Log the parameters shapes at the beginning')\n    parser.add_argument('--watch_regex', default=None, help='regex to describe which weights and biases to output, if any')\n    parser.add_argument('--wandb', action='store_true', help='Start a wandb session and write the results of training.  Only applies to training.  Use --wandb_name instead to specify a name')\n    parser.add_argument('--wandb_name', default=None, help='Name of a wandb session to start when training.  Will default to the dataset short name')\n    parser.add_argument('--wandb_norm_regex', default=None, help='Log on wandb any tensor whose norm matches this matrix.  Might get cluttered?')\n    return parser",
            "def build_argparse():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Adds the arguments for building the con parser\\n\\n    For the most part, defaults are set to cross-validated values, at least for WSJ\\n    '\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='data/constituency', help='Directory of constituency data.')\n    parser.add_argument('--wordvec_dir', type=str, default='extern_data/wordvec', help='Directory of word vectors')\n    parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')\n    parser.add_argument('--wordvec_pretrain_file', type=str, default=None, help='Exact name of the pretrain file to read')\n    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)\n    parser.add_argument('--charlm_forward_file', type=str, default=None, help='Exact path to use for forward charlm')\n    parser.add_argument('--charlm_backward_file', type=str, default=None, help='Exact path to use for backward charlm')\n    parser.add_argument('--bert_model', type=str, default=None, help='Use an external bert model (requires the transformers package)')\n    parser.add_argument('--no_bert_model', dest='bert_model', action='store_const', const=None, help=\"Don't use bert\")\n    parser.add_argument('--bert_hidden_layers', type=int, default=4, help='How many layers of hidden state to use from the transformer')\n    parser.add_argument('--bert_hidden_layers_original', action='store_const', const=None, dest='bert_hidden_layers', help='Use layers 2,3,4 of the Bert embedding')\n    parser.add_argument('--bert_finetune', default=False, action='store_true', help='Finetune the bert (or other transformer)')\n    parser.add_argument('--no_bert_finetune', dest='bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer)\")\n    parser.add_argument('--bert_finetune_layers', default=None, type=int, help='Only finetune this many layers from the transformer')\n    parser.add_argument('--bert_finetune_begin_epoch', default=None, type=int, help='Which epoch to start finetuning the transformer')\n    parser.add_argument('--bert_finetune_end_epoch', default=None, type=int, help='Which epoch to stop finetuning the transformer')\n    parser.add_argument('--bert_learning_rate', default=0.009, type=float, help='Scale the learning rate for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_learning_rate', default=None, type=float, help='Scale the learning rate for transformer finetuning by this much only during an AdaDelta warmup')\n    parser.add_argument('--bert_weight_decay', default=0.0001, type=float, help='Scale the weight decay for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_finetune', default=None, action='store_true', help=\"Finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--no_stage1_bert_finetune', dest='stage1_bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--tag_embedding_dim', type=int, default=20, help='Embedding size for a tag.  0 turns off the feature')\n    parser.add_argument('--delta_embedding_dim', type=int, default=100, help='Embedding size for a delta embedding')\n    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--silver_file', type=str, default=None, help='Secondary training file.')\n    parser.add_argument('--silver_remove_duplicates', default=False, action='store_true', help=\"Do/don't remove duplicates from the silver training file.  Could be useful for intentionally reweighting some trees\")\n    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--tokenized_file', type=str, default=None, help='Input file of tokenized text for parsing with parse_text.')\n    parser.add_argument('--tokenized_dir', type=str, default=None, help='Input directory of tokenized text for parsing with parse_text.')\n    parser.add_argument('--mode', default='train', choices=['train', 'parse_text', 'predict', 'remove_optimizer'])\n    parser.add_argument('--num_generate', type=int, default=0, help='When running a dev set, how many sentences to generate beyond the greedy one')\n    add_predict_output_args(parser)\n    parser.add_argument('--lang', type=str, help='Language')\n    parser.add_argument('--shorthand', type=str, help='Treebank shorthand')\n    parser.add_argument('--transition_embedding_dim', type=int, default=20, help='Embedding size for a transition')\n    parser.add_argument('--transition_hidden_size', type=int, default=20, help='Embedding size for transition stack')\n    parser.add_argument('--transition_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--transition_heads', default=4, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--constituent_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--constituent_heads', default=8, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--hidden_size', type=int, default=512, help='Size of the output layers for constituency stack and word queue')\n    parser.add_argument('--epochs', type=int, default=400)\n    parser.add_argument('--epoch_size', type=int, default=5000, help=\"Runs this many trees in an 'epoch' instead of going through the training dataset exactly once.  Set to 0 to do the whole training set\")\n    parser.add_argument('--silver_epoch_size', type=int, default=None, help=\"Runs this many trees in a silver 'epoch'.  If not set, will match --epoch_size\")\n    parser.add_argument('--multistage', default=True, action='store_true', help='1/2 epochs with adadelta no pattn or lattn, 1/4 with chosen optim and no lattn, 1/4 full model')\n    parser.add_argument('--no_multistage', dest='multistage', action='store_false', help=\"don't do the multistage learning\")\n    parser.add_argument('--oracle_initial_epoch', type=int, default=1, help='Epoch where we start using the dynamic oracle to let the parser keep going with wrong decisions')\n    parser.add_argument('--oracle_frequency', type=float, default=0.8, help='How often to use the oracle vs how often to force the correct transition')\n    parser.add_argument('--oracle_forced_errors', type=float, default=0.001, help='Occasionally have the model randomly walk through the state space to try to learn how to recover')\n    parser.add_argument('--oracle_level', type=int, default=None, help='Restrict oracle transitions to this level or lower.  0 means off.  None means use all oracle transitions.')\n    parser.add_argument('--train_batch_size', type=int, default=30, help='How many trees to train before taking an optimizer step')\n    parser.add_argument('--eval_batch_size', type=int, default=50, help='How many trees to batch when running eval')\n    parser.add_argument('--save_dir', type=str, default='saved_models/constituency', help='Root dir for saving models.')\n    parser.add_argument('--save_name', type=str, default='{shorthand}_{embedding}_{finetune}_constituency.pt', help='File name to save the model')\n    parser.add_argument('--save_each_name', type=str, default=None, help='Save each model in sequence to this pattern.  Mostly for testing')\n    parser.add_argument('--seed', type=int, default=1234)\n    utils.add_device_args(parser)\n    parser.add_argument('--learning_rate', default=None, type=float, help='Learning rate for the optimizer.  Reasonable values are 1.0 for adadelta or 0.001 for SGD.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_RATES))\n    parser.add_argument('--learning_eps', default=None, type=float, help='eps value to use in the optimizer.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_EPS))\n    parser.add_argument('--learning_momentum', default=None, type=float, help='Momentum.  None uses a default for the given optimizer: {}'.format(DEFAULT_MOMENTUM))\n    parser.add_argument('--learning_weight_decay', default=None, type=float, help='Weight decay (eg, l2 reg) to use in the optimizer')\n    parser.add_argument('--learning_rho', default=DEFAULT_LEARNING_RHO, type=float, help='Rho parameter in Adadelta')\n    parser.add_argument('--learning_beta2', default=0.999, type=float, help='Beta2 argument for AdamW')\n    parser.add_argument('--optim', default=None, help='Optimizer type: SGD, AdamW, Adadelta, AdaBelief, Madgrad')\n    parser.add_argument('--stage1_learning_rate', default=None, type=float, help='Learning rate to use in the first stage of --multistage.  None means use default: {}'.format(DEFAULT_LEARNING_RATES['adadelta']))\n    parser.add_argument('--learning_rate_warmup', default=0, type=int, help=\"Number of epochs to ramp up learning rate from 0 to full.  Set to 0 to always use the chosen learning rate.  Currently not functional, as it didn't do anything\")\n    parser.add_argument('--learning_rate_factor', default=0.6, type=float, help='Plateau learning rate decreate when plateaued')\n    parser.add_argument('--learning_rate_patience', default=5, type=int, help='Plateau learning rate patience')\n    parser.add_argument('--learning_rate_cooldown', default=10, type=int, help='Plateau learning rate cooldown')\n    parser.add_argument('--learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum')\n    parser.add_argument('--stage1_learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum (stage 1)')\n    parser.add_argument('--grad_clipping', default=None, type=float, help='Clip abs(grad) to this amount.  Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--no_grad_clipping', action='store_const', const=None, dest='grad_clipping', help='Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--loss', default='cross', help='cross, large_margin, or focal.  Focal requires `pip install focal_loss_torch`')\n    parser.add_argument('--loss_focal_gamma', default=2, type=float, help='gamma value for a focal loss')\n    parser.add_argument('--word_dropout', default=0.2, type=float, help='Dropout on the word embedding')\n    parser.add_argument('--predict_dropout', default=0.2, type=float, help='Dropout on the final prediction layer')\n    parser.add_argument('--lstm_layer_dropout', default=0.0, type=float, help='Dropout in the LSTM layers')\n    parser.add_argument('--lstm_input_dropout', default=0.2, type=float, help='Dropout on the input to an LSTM')\n    parser.add_argument('--transition_scheme', default=TransitionScheme.IN_ORDER, type=lambda x: TransitionScheme[x.upper()], help='Transition scheme to use.  {}'.format(', '.join((x.name for x in TransitionScheme))))\n    parser.add_argument('--reversed', default=False, action='store_true', help='Do the transition sequence reversed')\n    parser.add_argument('--combined_dummy_embedding', default=True, action='store_true', help='Use the same embedding for dummy nodes and the vectors used when combining constituents')\n    parser.add_argument('--no_combined_dummy_embedding', dest='combined_dummy_embedding', action='store_false', help=\"Don't use the same embedding for dummy nodes and the vectors used when combining constituents\")\n    parser.add_argument('--nonlinearity', default='relu', choices=NONLINEARITY.keys(), help='Nonlinearity to use in the model.  relu is a noticeable improvement over tanh')\n    parser.add_argument('--maxout_k', default=None, type=int, help='Use maxout layers instead of a nonlinearity for the output layers')\n    parser.add_argument('--use_silver_words', default=True, dest='use_silver_words', action='store_true', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--no_use_silver_words', default=True, dest='use_silver_words', action='store_false', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--rare_word_unknown_frequency', default=0.02, type=float, help='How often to replace a rare word with UNK when training')\n    parser.add_argument('--rare_word_threshold', default=0.02, type=float, help='How many words to consider as rare words as a fraction of the dataset')\n    parser.add_argument('--tag_unknown_frequency', default=0.001, type=float, help='How often to replace a tag with UNK when training')\n    parser.add_argument('--num_lstm_layers', default=2, type=int, help='How many layers to use in the LSTMs')\n    parser.add_argument('--num_tree_lstm_layers', default=None, type=int, help='How many layers to use in the TREE_LSTMs, if used.  This also increases the width of the word outputs to match the tree lstm inputs.  Default 2 if TREE_LSTM or TREE_LSTM_CX, 1 otherwise')\n    parser.add_argument('--num_output_layers', default=3, type=int, help='How many layers to use at the prediction level')\n    parser.add_argument('--sentence_boundary_vectors', default=SentenceBoundary.EVERYTHING, type=lambda x: SentenceBoundary[x.upper()], help='Vectors to learn at the start & end of sentences.  {}'.format(', '.join((x.name for x in SentenceBoundary))))\n    parser.add_argument('--constituency_composition', default=ConstituencyComposition.MAX, type=lambda x: ConstituencyComposition[x.upper()], help='How to build a new composition from its children.  {}'.format(', '.join((x.name for x in ConstituencyComposition))))\n    parser.add_argument('--reduce_heads', default=8, type=int, help='Number of attn heads to use when reducing children into a parent tree (constituency_composition == attn)')\n    parser.add_argument('--reduce_position', default=None, type=int, help=\"Dimension of position vector to use when reducing children.  None means 1/4 hidden_size, 0 means don't use (constituency_composition == key | untied_key)\")\n    parser.add_argument('--relearn_structure', action='store_true', help='Starting from an existing checkpoint, add or remove pattn / lattn.  One thing that works well is to train an initial model using adadelta with no pattn, then add pattn with adamw')\n    parser.add_argument('--finetune', action='store_true', help='Load existing model during `train` mode from `load_name` path')\n    parser.add_argument('--checkpoint_save_name', type=str, default=None, help='File name to save the most recent checkpoint')\n    parser.add_argument('--no_checkpoint', dest='checkpoint', action='store_false', help=\"Don't save checkpoints\")\n    parser.add_argument('--load_name', type=str, default=None, help='Model to load when finetuning, evaluating, or manipulating an existing file')\n    parser.add_argument('--load_package', type=str, default=None, help='Download an existing stanza package & use this for tests, finetuning, etc')\n    retagging.add_retag_args(parser)\n    parser.add_argument('--pattn_d_model', default=1024, type=int, help='Partitioned attention model dimensionality')\n    parser.add_argument('--pattn_morpho_emb_dropout', default=0.2, type=float, help='Dropout rate for morphological features obtained from pretrained model')\n    parser.add_argument('--pattn_encoder_max_len', default=512, type=int, help='Max length that can be put into the transformer attention layer')\n    parser.add_argument('--pattn_num_heads', default=8, type=int, help='Partitioned attention model number of attention heads')\n    parser.add_argument('--pattn_d_kv', default=64, type=int, help='Size of the query and key vector')\n    parser.add_argument('--pattn_d_ff', default=2048, type=int, help='Size of the intermediate vectors in the feed-forward sublayer')\n    parser.add_argument('--pattn_relu_dropout', default=0.1, type=float, help='ReLU dropout probability in feed-forward sublayer')\n    parser.add_argument('--pattn_residual_dropout', default=0.2, type=float, help='Residual dropout probability for all residual connections')\n    parser.add_argument('--pattn_attention_dropout', default=0.2, type=float, help='Attention dropout probability')\n    parser.add_argument('--pattn_num_layers', default=0, type=int, help='Number of layers for the Partitioned Attention.  Currently turned off')\n    parser.add_argument('--pattn_bias', default=False, action='store_true', help='Whether or not to learn an additive bias')\n    parser.add_argument('--pattn_timing', default='sin', choices=['learned', 'sin'], help='Use a learned embedding or a sin embedding')\n    parser.add_argument('--lattn_d_input_proj', default=None, type=int, help='If set, project the non-positional inputs down to this size before proceeding.')\n    parser.add_argument('--lattn_d_kv', default=64, type=int, help='Dimension of the key/query vector')\n    parser.add_argument('--lattn_d_proj', default=64, type=int, help='Dimension of the output vector from each label attention head')\n    parser.add_argument('--lattn_resdrop', default=True, action='store_true', help='Whether or not to use Residual Dropout')\n    parser.add_argument('--lattn_pwff', default=True, action='store_true', help='Whether or not to use a Position-wise Feed-forward Layer')\n    parser.add_argument('--lattn_q_as_matrix', default=False, action='store_true', help='Whether or not Label Attention uses learned query vectors. False means it does')\n    parser.add_argument('--lattn_partitioned', default=True, action='store_true', help='Whether or not it is partitioned')\n    parser.add_argument('--no_lattn_partitioned', default=True, action='store_false', dest='lattn_partitioned', help='Whether or not it is partitioned')\n    parser.add_argument('--lattn_combine_as_self', default=False, action='store_true', help='Whether or not the layer uses concatenation. False means it does')\n    parser.add_argument('--lattn_d_l', default=32, type=int, help='Number of labels')\n    parser.add_argument('--lattn_attention_dropout', default=0.2, type=float, help='Dropout for attention layer')\n    parser.add_argument('--lattn_d_ff', default=2048, type=int, help='Dimension of the Feed-forward layer')\n    parser.add_argument('--lattn_relu_dropout', default=0.2, type=float, help='Relu dropout for the label attention')\n    parser.add_argument('--lattn_residual_dropout', default=0.2, type=float, help='Residual dropout for the label attention')\n    parser.add_argument('--lattn_combined_input', default=True, action='store_true', help='Combine all inputs for the lattn, not just the pattn')\n    parser.add_argument('--use_lattn', default=False, action='store_true', help='Use the lattn layers - currently turned off')\n    parser.add_argument('--no_lattn_combined_input', dest='lattn_combined_input', action='store_false', help=\"Don't combine all inputs for the lattn, not just the pattn\")\n    parser.add_argument('--log_norms', default=False, action='store_true', help='Log the parameters norms while training.  A very noisy option')\n    parser.add_argument('--log_shapes', default=False, action='store_true', help='Log the parameters shapes at the beginning')\n    parser.add_argument('--watch_regex', default=None, help='regex to describe which weights and biases to output, if any')\n    parser.add_argument('--wandb', action='store_true', help='Start a wandb session and write the results of training.  Only applies to training.  Use --wandb_name instead to specify a name')\n    parser.add_argument('--wandb_name', default=None, help='Name of a wandb session to start when training.  Will default to the dataset short name')\n    parser.add_argument('--wandb_norm_regex', default=None, help='Log on wandb any tensor whose norm matches this matrix.  Might get cluttered?')\n    return parser",
            "def build_argparse():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Adds the arguments for building the con parser\\n\\n    For the most part, defaults are set to cross-validated values, at least for WSJ\\n    '\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='data/constituency', help='Directory of constituency data.')\n    parser.add_argument('--wordvec_dir', type=str, default='extern_data/wordvec', help='Directory of word vectors')\n    parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')\n    parser.add_argument('--wordvec_pretrain_file', type=str, default=None, help='Exact name of the pretrain file to read')\n    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)\n    parser.add_argument('--charlm_forward_file', type=str, default=None, help='Exact path to use for forward charlm')\n    parser.add_argument('--charlm_backward_file', type=str, default=None, help='Exact path to use for backward charlm')\n    parser.add_argument('--bert_model', type=str, default=None, help='Use an external bert model (requires the transformers package)')\n    parser.add_argument('--no_bert_model', dest='bert_model', action='store_const', const=None, help=\"Don't use bert\")\n    parser.add_argument('--bert_hidden_layers', type=int, default=4, help='How many layers of hidden state to use from the transformer')\n    parser.add_argument('--bert_hidden_layers_original', action='store_const', const=None, dest='bert_hidden_layers', help='Use layers 2,3,4 of the Bert embedding')\n    parser.add_argument('--bert_finetune', default=False, action='store_true', help='Finetune the bert (or other transformer)')\n    parser.add_argument('--no_bert_finetune', dest='bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer)\")\n    parser.add_argument('--bert_finetune_layers', default=None, type=int, help='Only finetune this many layers from the transformer')\n    parser.add_argument('--bert_finetune_begin_epoch', default=None, type=int, help='Which epoch to start finetuning the transformer')\n    parser.add_argument('--bert_finetune_end_epoch', default=None, type=int, help='Which epoch to stop finetuning the transformer')\n    parser.add_argument('--bert_learning_rate', default=0.009, type=float, help='Scale the learning rate for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_learning_rate', default=None, type=float, help='Scale the learning rate for transformer finetuning by this much only during an AdaDelta warmup')\n    parser.add_argument('--bert_weight_decay', default=0.0001, type=float, help='Scale the weight decay for transformer finetuning by this much')\n    parser.add_argument('--stage1_bert_finetune', default=None, action='store_true', help=\"Finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--no_stage1_bert_finetune', dest='stage1_bert_finetune', action='store_false', help=\"Don't finetune the bert (or other transformer) during an AdaDelta warmup, even if the second half doesn't use bert_finetune\")\n    parser.add_argument('--tag_embedding_dim', type=int, default=20, help='Embedding size for a tag.  0 turns off the feature')\n    parser.add_argument('--delta_embedding_dim', type=int, default=100, help='Embedding size for a delta embedding')\n    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--silver_file', type=str, default=None, help='Secondary training file.')\n    parser.add_argument('--silver_remove_duplicates', default=False, action='store_true', help=\"Do/don't remove duplicates from the silver training file.  Could be useful for intentionally reweighting some trees\")\n    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')\n    parser.add_argument('--tokenized_file', type=str, default=None, help='Input file of tokenized text for parsing with parse_text.')\n    parser.add_argument('--tokenized_dir', type=str, default=None, help='Input directory of tokenized text for parsing with parse_text.')\n    parser.add_argument('--mode', default='train', choices=['train', 'parse_text', 'predict', 'remove_optimizer'])\n    parser.add_argument('--num_generate', type=int, default=0, help='When running a dev set, how many sentences to generate beyond the greedy one')\n    add_predict_output_args(parser)\n    parser.add_argument('--lang', type=str, help='Language')\n    parser.add_argument('--shorthand', type=str, help='Treebank shorthand')\n    parser.add_argument('--transition_embedding_dim', type=int, default=20, help='Embedding size for a transition')\n    parser.add_argument('--transition_hidden_size', type=int, default=20, help='Embedding size for transition stack')\n    parser.add_argument('--transition_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--transition_heads', default=4, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--constituent_stack', default=StackHistory.LSTM, type=lambda x: StackHistory[x.upper()], help='How to track transitions over a parse.  {}'.format(', '.join((x.name for x in StackHistory))))\n    parser.add_argument('--constituent_heads', default=8, type=int, help='How many heads to use in MHA *if* the transition_stack is Attention')\n    parser.add_argument('--hidden_size', type=int, default=512, help='Size of the output layers for constituency stack and word queue')\n    parser.add_argument('--epochs', type=int, default=400)\n    parser.add_argument('--epoch_size', type=int, default=5000, help=\"Runs this many trees in an 'epoch' instead of going through the training dataset exactly once.  Set to 0 to do the whole training set\")\n    parser.add_argument('--silver_epoch_size', type=int, default=None, help=\"Runs this many trees in a silver 'epoch'.  If not set, will match --epoch_size\")\n    parser.add_argument('--multistage', default=True, action='store_true', help='1/2 epochs with adadelta no pattn or lattn, 1/4 with chosen optim and no lattn, 1/4 full model')\n    parser.add_argument('--no_multistage', dest='multistage', action='store_false', help=\"don't do the multistage learning\")\n    parser.add_argument('--oracle_initial_epoch', type=int, default=1, help='Epoch where we start using the dynamic oracle to let the parser keep going with wrong decisions')\n    parser.add_argument('--oracle_frequency', type=float, default=0.8, help='How often to use the oracle vs how often to force the correct transition')\n    parser.add_argument('--oracle_forced_errors', type=float, default=0.001, help='Occasionally have the model randomly walk through the state space to try to learn how to recover')\n    parser.add_argument('--oracle_level', type=int, default=None, help='Restrict oracle transitions to this level or lower.  0 means off.  None means use all oracle transitions.')\n    parser.add_argument('--train_batch_size', type=int, default=30, help='How many trees to train before taking an optimizer step')\n    parser.add_argument('--eval_batch_size', type=int, default=50, help='How many trees to batch when running eval')\n    parser.add_argument('--save_dir', type=str, default='saved_models/constituency', help='Root dir for saving models.')\n    parser.add_argument('--save_name', type=str, default='{shorthand}_{embedding}_{finetune}_constituency.pt', help='File name to save the model')\n    parser.add_argument('--save_each_name', type=str, default=None, help='Save each model in sequence to this pattern.  Mostly for testing')\n    parser.add_argument('--seed', type=int, default=1234)\n    utils.add_device_args(parser)\n    parser.add_argument('--learning_rate', default=None, type=float, help='Learning rate for the optimizer.  Reasonable values are 1.0 for adadelta or 0.001 for SGD.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_RATES))\n    parser.add_argument('--learning_eps', default=None, type=float, help='eps value to use in the optimizer.  None uses a default for the given optimizer: {}'.format(DEFAULT_LEARNING_EPS))\n    parser.add_argument('--learning_momentum', default=None, type=float, help='Momentum.  None uses a default for the given optimizer: {}'.format(DEFAULT_MOMENTUM))\n    parser.add_argument('--learning_weight_decay', default=None, type=float, help='Weight decay (eg, l2 reg) to use in the optimizer')\n    parser.add_argument('--learning_rho', default=DEFAULT_LEARNING_RHO, type=float, help='Rho parameter in Adadelta')\n    parser.add_argument('--learning_beta2', default=0.999, type=float, help='Beta2 argument for AdamW')\n    parser.add_argument('--optim', default=None, help='Optimizer type: SGD, AdamW, Adadelta, AdaBelief, Madgrad')\n    parser.add_argument('--stage1_learning_rate', default=None, type=float, help='Learning rate to use in the first stage of --multistage.  None means use default: {}'.format(DEFAULT_LEARNING_RATES['adadelta']))\n    parser.add_argument('--learning_rate_warmup', default=0, type=int, help=\"Number of epochs to ramp up learning rate from 0 to full.  Set to 0 to always use the chosen learning rate.  Currently not functional, as it didn't do anything\")\n    parser.add_argument('--learning_rate_factor', default=0.6, type=float, help='Plateau learning rate decreate when plateaued')\n    parser.add_argument('--learning_rate_patience', default=5, type=int, help='Plateau learning rate patience')\n    parser.add_argument('--learning_rate_cooldown', default=10, type=int, help='Plateau learning rate cooldown')\n    parser.add_argument('--learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum')\n    parser.add_argument('--stage1_learning_rate_min_lr', default=None, type=float, help='Plateau learning rate minimum (stage 1)')\n    parser.add_argument('--grad_clipping', default=None, type=float, help='Clip abs(grad) to this amount.  Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--no_grad_clipping', action='store_const', const=None, dest='grad_clipping', help='Use --no_grad_clipping to turn off grad clipping')\n    parser.add_argument('--loss', default='cross', help='cross, large_margin, or focal.  Focal requires `pip install focal_loss_torch`')\n    parser.add_argument('--loss_focal_gamma', default=2, type=float, help='gamma value for a focal loss')\n    parser.add_argument('--word_dropout', default=0.2, type=float, help='Dropout on the word embedding')\n    parser.add_argument('--predict_dropout', default=0.2, type=float, help='Dropout on the final prediction layer')\n    parser.add_argument('--lstm_layer_dropout', default=0.0, type=float, help='Dropout in the LSTM layers')\n    parser.add_argument('--lstm_input_dropout', default=0.2, type=float, help='Dropout on the input to an LSTM')\n    parser.add_argument('--transition_scheme', default=TransitionScheme.IN_ORDER, type=lambda x: TransitionScheme[x.upper()], help='Transition scheme to use.  {}'.format(', '.join((x.name for x in TransitionScheme))))\n    parser.add_argument('--reversed', default=False, action='store_true', help='Do the transition sequence reversed')\n    parser.add_argument('--combined_dummy_embedding', default=True, action='store_true', help='Use the same embedding for dummy nodes and the vectors used when combining constituents')\n    parser.add_argument('--no_combined_dummy_embedding', dest='combined_dummy_embedding', action='store_false', help=\"Don't use the same embedding for dummy nodes and the vectors used when combining constituents\")\n    parser.add_argument('--nonlinearity', default='relu', choices=NONLINEARITY.keys(), help='Nonlinearity to use in the model.  relu is a noticeable improvement over tanh')\n    parser.add_argument('--maxout_k', default=None, type=int, help='Use maxout layers instead of a nonlinearity for the output layers')\n    parser.add_argument('--use_silver_words', default=True, dest='use_silver_words', action='store_true', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--no_use_silver_words', default=True, dest='use_silver_words', action='store_false', help=\"Train/don't train word vectors for words only in the silver dataset\")\n    parser.add_argument('--rare_word_unknown_frequency', default=0.02, type=float, help='How often to replace a rare word with UNK when training')\n    parser.add_argument('--rare_word_threshold', default=0.02, type=float, help='How many words to consider as rare words as a fraction of the dataset')\n    parser.add_argument('--tag_unknown_frequency', default=0.001, type=float, help='How often to replace a tag with UNK when training')\n    parser.add_argument('--num_lstm_layers', default=2, type=int, help='How many layers to use in the LSTMs')\n    parser.add_argument('--num_tree_lstm_layers', default=None, type=int, help='How many layers to use in the TREE_LSTMs, if used.  This also increases the width of the word outputs to match the tree lstm inputs.  Default 2 if TREE_LSTM or TREE_LSTM_CX, 1 otherwise')\n    parser.add_argument('--num_output_layers', default=3, type=int, help='How many layers to use at the prediction level')\n    parser.add_argument('--sentence_boundary_vectors', default=SentenceBoundary.EVERYTHING, type=lambda x: SentenceBoundary[x.upper()], help='Vectors to learn at the start & end of sentences.  {}'.format(', '.join((x.name for x in SentenceBoundary))))\n    parser.add_argument('--constituency_composition', default=ConstituencyComposition.MAX, type=lambda x: ConstituencyComposition[x.upper()], help='How to build a new composition from its children.  {}'.format(', '.join((x.name for x in ConstituencyComposition))))\n    parser.add_argument('--reduce_heads', default=8, type=int, help='Number of attn heads to use when reducing children into a parent tree (constituency_composition == attn)')\n    parser.add_argument('--reduce_position', default=None, type=int, help=\"Dimension of position vector to use when reducing children.  None means 1/4 hidden_size, 0 means don't use (constituency_composition == key | untied_key)\")\n    parser.add_argument('--relearn_structure', action='store_true', help='Starting from an existing checkpoint, add or remove pattn / lattn.  One thing that works well is to train an initial model using adadelta with no pattn, then add pattn with adamw')\n    parser.add_argument('--finetune', action='store_true', help='Load existing model during `train` mode from `load_name` path')\n    parser.add_argument('--checkpoint_save_name', type=str, default=None, help='File name to save the most recent checkpoint')\n    parser.add_argument('--no_checkpoint', dest='checkpoint', action='store_false', help=\"Don't save checkpoints\")\n    parser.add_argument('--load_name', type=str, default=None, help='Model to load when finetuning, evaluating, or manipulating an existing file')\n    parser.add_argument('--load_package', type=str, default=None, help='Download an existing stanza package & use this for tests, finetuning, etc')\n    retagging.add_retag_args(parser)\n    parser.add_argument('--pattn_d_model', default=1024, type=int, help='Partitioned attention model dimensionality')\n    parser.add_argument('--pattn_morpho_emb_dropout', default=0.2, type=float, help='Dropout rate for morphological features obtained from pretrained model')\n    parser.add_argument('--pattn_encoder_max_len', default=512, type=int, help='Max length that can be put into the transformer attention layer')\n    parser.add_argument('--pattn_num_heads', default=8, type=int, help='Partitioned attention model number of attention heads')\n    parser.add_argument('--pattn_d_kv', default=64, type=int, help='Size of the query and key vector')\n    parser.add_argument('--pattn_d_ff', default=2048, type=int, help='Size of the intermediate vectors in the feed-forward sublayer')\n    parser.add_argument('--pattn_relu_dropout', default=0.1, type=float, help='ReLU dropout probability in feed-forward sublayer')\n    parser.add_argument('--pattn_residual_dropout', default=0.2, type=float, help='Residual dropout probability for all residual connections')\n    parser.add_argument('--pattn_attention_dropout', default=0.2, type=float, help='Attention dropout probability')\n    parser.add_argument('--pattn_num_layers', default=0, type=int, help='Number of layers for the Partitioned Attention.  Currently turned off')\n    parser.add_argument('--pattn_bias', default=False, action='store_true', help='Whether or not to learn an additive bias')\n    parser.add_argument('--pattn_timing', default='sin', choices=['learned', 'sin'], help='Use a learned embedding or a sin embedding')\n    parser.add_argument('--lattn_d_input_proj', default=None, type=int, help='If set, project the non-positional inputs down to this size before proceeding.')\n    parser.add_argument('--lattn_d_kv', default=64, type=int, help='Dimension of the key/query vector')\n    parser.add_argument('--lattn_d_proj', default=64, type=int, help='Dimension of the output vector from each label attention head')\n    parser.add_argument('--lattn_resdrop', default=True, action='store_true', help='Whether or not to use Residual Dropout')\n    parser.add_argument('--lattn_pwff', default=True, action='store_true', help='Whether or not to use a Position-wise Feed-forward Layer')\n    parser.add_argument('--lattn_q_as_matrix', default=False, action='store_true', help='Whether or not Label Attention uses learned query vectors. False means it does')\n    parser.add_argument('--lattn_partitioned', default=True, action='store_true', help='Whether or not it is partitioned')\n    parser.add_argument('--no_lattn_partitioned', default=True, action='store_false', dest='lattn_partitioned', help='Whether or not it is partitioned')\n    parser.add_argument('--lattn_combine_as_self', default=False, action='store_true', help='Whether or not the layer uses concatenation. False means it does')\n    parser.add_argument('--lattn_d_l', default=32, type=int, help='Number of labels')\n    parser.add_argument('--lattn_attention_dropout', default=0.2, type=float, help='Dropout for attention layer')\n    parser.add_argument('--lattn_d_ff', default=2048, type=int, help='Dimension of the Feed-forward layer')\n    parser.add_argument('--lattn_relu_dropout', default=0.2, type=float, help='Relu dropout for the label attention')\n    parser.add_argument('--lattn_residual_dropout', default=0.2, type=float, help='Residual dropout for the label attention')\n    parser.add_argument('--lattn_combined_input', default=True, action='store_true', help='Combine all inputs for the lattn, not just the pattn')\n    parser.add_argument('--use_lattn', default=False, action='store_true', help='Use the lattn layers - currently turned off')\n    parser.add_argument('--no_lattn_combined_input', dest='lattn_combined_input', action='store_false', help=\"Don't combine all inputs for the lattn, not just the pattn\")\n    parser.add_argument('--log_norms', default=False, action='store_true', help='Log the parameters norms while training.  A very noisy option')\n    parser.add_argument('--log_shapes', default=False, action='store_true', help='Log the parameters shapes at the beginning')\n    parser.add_argument('--watch_regex', default=None, help='regex to describe which weights and biases to output, if any')\n    parser.add_argument('--wandb', action='store_true', help='Start a wandb session and write the results of training.  Only applies to training.  Use --wandb_name instead to specify a name')\n    parser.add_argument('--wandb_name', default=None, help='Name of a wandb session to start when training.  Will default to the dataset short name')\n    parser.add_argument('--wandb_norm_regex', default=None, help='Log on wandb any tensor whose norm matches this matrix.  Might get cluttered?')\n    return parser"
        ]
    },
    {
        "func_name": "build_model_filename",
        "original": "def build_model_filename(args):\n    embedding = utils.embedding_name(args)\n    maybe_finetune = 'finetuned' if args['bert_finetune'] or args['stage1_bert_finetune'] else ''\n    transformer_finetune_begin = '%d' % args['bert_finetune_begin_epoch'] if args['bert_finetune_begin_epoch'] is not None else ''\n    model_save_file = args['save_name'].format(shorthand=args['shorthand'], embedding=embedding, finetune=maybe_finetune, transformer_finetune_begin=transformer_finetune_begin, transition_scheme=args['transition_scheme'].name.lower().replace('_', ''), trans_layers=args['bert_hidden_layers'], seed=args['seed'])\n    model_save_file = re.sub('_+', '_', model_save_file)\n    logger.info('Expanded save_name: %s', model_save_file)\n    model_dir = os.path.split(model_save_file)[0]\n    if model_dir != args['save_dir']:\n        model_save_file = os.path.join(args['save_dir'], model_save_file)\n    return model_save_file",
        "mutated": [
            "def build_model_filename(args):\n    if False:\n        i = 10\n    embedding = utils.embedding_name(args)\n    maybe_finetune = 'finetuned' if args['bert_finetune'] or args['stage1_bert_finetune'] else ''\n    transformer_finetune_begin = '%d' % args['bert_finetune_begin_epoch'] if args['bert_finetune_begin_epoch'] is not None else ''\n    model_save_file = args['save_name'].format(shorthand=args['shorthand'], embedding=embedding, finetune=maybe_finetune, transformer_finetune_begin=transformer_finetune_begin, transition_scheme=args['transition_scheme'].name.lower().replace('_', ''), trans_layers=args['bert_hidden_layers'], seed=args['seed'])\n    model_save_file = re.sub('_+', '_', model_save_file)\n    logger.info('Expanded save_name: %s', model_save_file)\n    model_dir = os.path.split(model_save_file)[0]\n    if model_dir != args['save_dir']:\n        model_save_file = os.path.join(args['save_dir'], model_save_file)\n    return model_save_file",
            "def build_model_filename(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    embedding = utils.embedding_name(args)\n    maybe_finetune = 'finetuned' if args['bert_finetune'] or args['stage1_bert_finetune'] else ''\n    transformer_finetune_begin = '%d' % args['bert_finetune_begin_epoch'] if args['bert_finetune_begin_epoch'] is not None else ''\n    model_save_file = args['save_name'].format(shorthand=args['shorthand'], embedding=embedding, finetune=maybe_finetune, transformer_finetune_begin=transformer_finetune_begin, transition_scheme=args['transition_scheme'].name.lower().replace('_', ''), trans_layers=args['bert_hidden_layers'], seed=args['seed'])\n    model_save_file = re.sub('_+', '_', model_save_file)\n    logger.info('Expanded save_name: %s', model_save_file)\n    model_dir = os.path.split(model_save_file)[0]\n    if model_dir != args['save_dir']:\n        model_save_file = os.path.join(args['save_dir'], model_save_file)\n    return model_save_file",
            "def build_model_filename(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    embedding = utils.embedding_name(args)\n    maybe_finetune = 'finetuned' if args['bert_finetune'] or args['stage1_bert_finetune'] else ''\n    transformer_finetune_begin = '%d' % args['bert_finetune_begin_epoch'] if args['bert_finetune_begin_epoch'] is not None else ''\n    model_save_file = args['save_name'].format(shorthand=args['shorthand'], embedding=embedding, finetune=maybe_finetune, transformer_finetune_begin=transformer_finetune_begin, transition_scheme=args['transition_scheme'].name.lower().replace('_', ''), trans_layers=args['bert_hidden_layers'], seed=args['seed'])\n    model_save_file = re.sub('_+', '_', model_save_file)\n    logger.info('Expanded save_name: %s', model_save_file)\n    model_dir = os.path.split(model_save_file)[0]\n    if model_dir != args['save_dir']:\n        model_save_file = os.path.join(args['save_dir'], model_save_file)\n    return model_save_file",
            "def build_model_filename(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    embedding = utils.embedding_name(args)\n    maybe_finetune = 'finetuned' if args['bert_finetune'] or args['stage1_bert_finetune'] else ''\n    transformer_finetune_begin = '%d' % args['bert_finetune_begin_epoch'] if args['bert_finetune_begin_epoch'] is not None else ''\n    model_save_file = args['save_name'].format(shorthand=args['shorthand'], embedding=embedding, finetune=maybe_finetune, transformer_finetune_begin=transformer_finetune_begin, transition_scheme=args['transition_scheme'].name.lower().replace('_', ''), trans_layers=args['bert_hidden_layers'], seed=args['seed'])\n    model_save_file = re.sub('_+', '_', model_save_file)\n    logger.info('Expanded save_name: %s', model_save_file)\n    model_dir = os.path.split(model_save_file)[0]\n    if model_dir != args['save_dir']:\n        model_save_file = os.path.join(args['save_dir'], model_save_file)\n    return model_save_file",
            "def build_model_filename(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    embedding = utils.embedding_name(args)\n    maybe_finetune = 'finetuned' if args['bert_finetune'] or args['stage1_bert_finetune'] else ''\n    transformer_finetune_begin = '%d' % args['bert_finetune_begin_epoch'] if args['bert_finetune_begin_epoch'] is not None else ''\n    model_save_file = args['save_name'].format(shorthand=args['shorthand'], embedding=embedding, finetune=maybe_finetune, transformer_finetune_begin=transformer_finetune_begin, transition_scheme=args['transition_scheme'].name.lower().replace('_', ''), trans_layers=args['bert_hidden_layers'], seed=args['seed'])\n    model_save_file = re.sub('_+', '_', model_save_file)\n    logger.info('Expanded save_name: %s', model_save_file)\n    model_dir = os.path.split(model_save_file)[0]\n    if model_dir != args['save_dir']:\n        model_save_file = os.path.join(args['save_dir'], model_save_file)\n    return model_save_file"
        ]
    },
    {
        "func_name": "parse_args",
        "original": "def parse_args(args=None):\n    parser = build_argparse()\n    args = parser.parse_args(args=args)\n    if not args.lang and args.shorthand and (len(args.shorthand.split('_', maxsplit=1)) == 2):\n        args.lang = args.shorthand.split('_')[0]\n    if args.stage1_bert_learning_rate is None:\n        args.stage1_bert_learning_rate = args.bert_learning_rate\n    if args.optim is None and args.mode == 'train':\n        if not args.multistage:\n            args.optim = 'adadelta'\n        elif args.bert_finetune or args.stage1_bert_finetune:\n            logger.info('Multistage training is set, optimizer is not chosen, and bert finetuning is active.  Will use AdamW as the second stage optimizer.')\n            args.optim = 'adamw'\n        else:\n            try:\n                import madgrad\n                args.optim = 'madgrad'\n                logger.info('Multistage training is set, optimizer is not chosen, and MADGRAD is available.  Will use MADGRAD as the second stage optimizer.')\n            except ModuleNotFoundError as e:\n                logger.warning('Multistage training is set.  Best models are with MADGRAD, but it is not installed.  Will use AdamW for the second stage optimizer.  Consider installing MADGRAD')\n                args.optim = 'adamw'\n    if args.mode == 'train':\n        if args.learning_rate is None:\n            args.learning_rate = DEFAULT_LEARNING_RATES.get(args.optim.lower(), None)\n        if args.learning_eps is None:\n            args.learning_eps = DEFAULT_LEARNING_EPS.get(args.optim.lower(), None)\n        if args.learning_momentum is None:\n            args.learning_momentum = DEFAULT_MOMENTUM.get(args.optim.lower(), None)\n        if args.learning_weight_decay is None:\n            args.learning_weight_decay = DEFAULT_WEIGHT_DECAY.get(args.optim.lower(), None)\n        if args.stage1_learning_rate is None:\n            args.stage1_learning_rate = DEFAULT_LEARNING_RATES['adadelta']\n        if args.stage1_bert_finetune is None:\n            args.stage1_bert_finetune = args.bert_finetune\n        if args.learning_rate_min_lr is None:\n            args.learning_rate_min_lr = args.learning_rate * 0.02\n        if args.stage1_learning_rate_min_lr is None:\n            args.stage1_learning_rate_min_lr = args.stage1_learning_rate * 0.02\n    if args.reduce_position is None:\n        args.reduce_position = args.hidden_size // 4\n    if args.num_tree_lstm_layers is None:\n        if args.constituency_composition in (ConstituencyComposition.TREE_LSTM, ConstituencyComposition.TREE_LSTM_CX):\n            args.num_tree_lstm_layers = 2\n        else:\n            args.num_tree_lstm_layers = 1\n    if args.wandb_name or args.wandb_norm_regex:\n        args.wandb = True\n    args = vars(args)\n    retagging.postprocess_args(args)\n    postprocess_predict_output_args(args)\n    model_save_file = build_model_filename(args)\n    args['save_name'] = model_save_file\n    if args['checkpoint']:\n        args['checkpoint_save_name'] = utils.checkpoint_name(args['save_dir'], model_save_file, args['checkpoint_save_name'])\n    return args",
        "mutated": [
            "def parse_args(args=None):\n    if False:\n        i = 10\n    parser = build_argparse()\n    args = parser.parse_args(args=args)\n    if not args.lang and args.shorthand and (len(args.shorthand.split('_', maxsplit=1)) == 2):\n        args.lang = args.shorthand.split('_')[0]\n    if args.stage1_bert_learning_rate is None:\n        args.stage1_bert_learning_rate = args.bert_learning_rate\n    if args.optim is None and args.mode == 'train':\n        if not args.multistage:\n            args.optim = 'adadelta'\n        elif args.bert_finetune or args.stage1_bert_finetune:\n            logger.info('Multistage training is set, optimizer is not chosen, and bert finetuning is active.  Will use AdamW as the second stage optimizer.')\n            args.optim = 'adamw'\n        else:\n            try:\n                import madgrad\n                args.optim = 'madgrad'\n                logger.info('Multistage training is set, optimizer is not chosen, and MADGRAD is available.  Will use MADGRAD as the second stage optimizer.')\n            except ModuleNotFoundError as e:\n                logger.warning('Multistage training is set.  Best models are with MADGRAD, but it is not installed.  Will use AdamW for the second stage optimizer.  Consider installing MADGRAD')\n                args.optim = 'adamw'\n    if args.mode == 'train':\n        if args.learning_rate is None:\n            args.learning_rate = DEFAULT_LEARNING_RATES.get(args.optim.lower(), None)\n        if args.learning_eps is None:\n            args.learning_eps = DEFAULT_LEARNING_EPS.get(args.optim.lower(), None)\n        if args.learning_momentum is None:\n            args.learning_momentum = DEFAULT_MOMENTUM.get(args.optim.lower(), None)\n        if args.learning_weight_decay is None:\n            args.learning_weight_decay = DEFAULT_WEIGHT_DECAY.get(args.optim.lower(), None)\n        if args.stage1_learning_rate is None:\n            args.stage1_learning_rate = DEFAULT_LEARNING_RATES['adadelta']\n        if args.stage1_bert_finetune is None:\n            args.stage1_bert_finetune = args.bert_finetune\n        if args.learning_rate_min_lr is None:\n            args.learning_rate_min_lr = args.learning_rate * 0.02\n        if args.stage1_learning_rate_min_lr is None:\n            args.stage1_learning_rate_min_lr = args.stage1_learning_rate * 0.02\n    if args.reduce_position is None:\n        args.reduce_position = args.hidden_size // 4\n    if args.num_tree_lstm_layers is None:\n        if args.constituency_composition in (ConstituencyComposition.TREE_LSTM, ConstituencyComposition.TREE_LSTM_CX):\n            args.num_tree_lstm_layers = 2\n        else:\n            args.num_tree_lstm_layers = 1\n    if args.wandb_name or args.wandb_norm_regex:\n        args.wandb = True\n    args = vars(args)\n    retagging.postprocess_args(args)\n    postprocess_predict_output_args(args)\n    model_save_file = build_model_filename(args)\n    args['save_name'] = model_save_file\n    if args['checkpoint']:\n        args['checkpoint_save_name'] = utils.checkpoint_name(args['save_dir'], model_save_file, args['checkpoint_save_name'])\n    return args",
            "def parse_args(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = build_argparse()\n    args = parser.parse_args(args=args)\n    if not args.lang and args.shorthand and (len(args.shorthand.split('_', maxsplit=1)) == 2):\n        args.lang = args.shorthand.split('_')[0]\n    if args.stage1_bert_learning_rate is None:\n        args.stage1_bert_learning_rate = args.bert_learning_rate\n    if args.optim is None and args.mode == 'train':\n        if not args.multistage:\n            args.optim = 'adadelta'\n        elif args.bert_finetune or args.stage1_bert_finetune:\n            logger.info('Multistage training is set, optimizer is not chosen, and bert finetuning is active.  Will use AdamW as the second stage optimizer.')\n            args.optim = 'adamw'\n        else:\n            try:\n                import madgrad\n                args.optim = 'madgrad'\n                logger.info('Multistage training is set, optimizer is not chosen, and MADGRAD is available.  Will use MADGRAD as the second stage optimizer.')\n            except ModuleNotFoundError as e:\n                logger.warning('Multistage training is set.  Best models are with MADGRAD, but it is not installed.  Will use AdamW for the second stage optimizer.  Consider installing MADGRAD')\n                args.optim = 'adamw'\n    if args.mode == 'train':\n        if args.learning_rate is None:\n            args.learning_rate = DEFAULT_LEARNING_RATES.get(args.optim.lower(), None)\n        if args.learning_eps is None:\n            args.learning_eps = DEFAULT_LEARNING_EPS.get(args.optim.lower(), None)\n        if args.learning_momentum is None:\n            args.learning_momentum = DEFAULT_MOMENTUM.get(args.optim.lower(), None)\n        if args.learning_weight_decay is None:\n            args.learning_weight_decay = DEFAULT_WEIGHT_DECAY.get(args.optim.lower(), None)\n        if args.stage1_learning_rate is None:\n            args.stage1_learning_rate = DEFAULT_LEARNING_RATES['adadelta']\n        if args.stage1_bert_finetune is None:\n            args.stage1_bert_finetune = args.bert_finetune\n        if args.learning_rate_min_lr is None:\n            args.learning_rate_min_lr = args.learning_rate * 0.02\n        if args.stage1_learning_rate_min_lr is None:\n            args.stage1_learning_rate_min_lr = args.stage1_learning_rate * 0.02\n    if args.reduce_position is None:\n        args.reduce_position = args.hidden_size // 4\n    if args.num_tree_lstm_layers is None:\n        if args.constituency_composition in (ConstituencyComposition.TREE_LSTM, ConstituencyComposition.TREE_LSTM_CX):\n            args.num_tree_lstm_layers = 2\n        else:\n            args.num_tree_lstm_layers = 1\n    if args.wandb_name or args.wandb_norm_regex:\n        args.wandb = True\n    args = vars(args)\n    retagging.postprocess_args(args)\n    postprocess_predict_output_args(args)\n    model_save_file = build_model_filename(args)\n    args['save_name'] = model_save_file\n    if args['checkpoint']:\n        args['checkpoint_save_name'] = utils.checkpoint_name(args['save_dir'], model_save_file, args['checkpoint_save_name'])\n    return args",
            "def parse_args(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = build_argparse()\n    args = parser.parse_args(args=args)\n    if not args.lang and args.shorthand and (len(args.shorthand.split('_', maxsplit=1)) == 2):\n        args.lang = args.shorthand.split('_')[0]\n    if args.stage1_bert_learning_rate is None:\n        args.stage1_bert_learning_rate = args.bert_learning_rate\n    if args.optim is None and args.mode == 'train':\n        if not args.multistage:\n            args.optim = 'adadelta'\n        elif args.bert_finetune or args.stage1_bert_finetune:\n            logger.info('Multistage training is set, optimizer is not chosen, and bert finetuning is active.  Will use AdamW as the second stage optimizer.')\n            args.optim = 'adamw'\n        else:\n            try:\n                import madgrad\n                args.optim = 'madgrad'\n                logger.info('Multistage training is set, optimizer is not chosen, and MADGRAD is available.  Will use MADGRAD as the second stage optimizer.')\n            except ModuleNotFoundError as e:\n                logger.warning('Multistage training is set.  Best models are with MADGRAD, but it is not installed.  Will use AdamW for the second stage optimizer.  Consider installing MADGRAD')\n                args.optim = 'adamw'\n    if args.mode == 'train':\n        if args.learning_rate is None:\n            args.learning_rate = DEFAULT_LEARNING_RATES.get(args.optim.lower(), None)\n        if args.learning_eps is None:\n            args.learning_eps = DEFAULT_LEARNING_EPS.get(args.optim.lower(), None)\n        if args.learning_momentum is None:\n            args.learning_momentum = DEFAULT_MOMENTUM.get(args.optim.lower(), None)\n        if args.learning_weight_decay is None:\n            args.learning_weight_decay = DEFAULT_WEIGHT_DECAY.get(args.optim.lower(), None)\n        if args.stage1_learning_rate is None:\n            args.stage1_learning_rate = DEFAULT_LEARNING_RATES['adadelta']\n        if args.stage1_bert_finetune is None:\n            args.stage1_bert_finetune = args.bert_finetune\n        if args.learning_rate_min_lr is None:\n            args.learning_rate_min_lr = args.learning_rate * 0.02\n        if args.stage1_learning_rate_min_lr is None:\n            args.stage1_learning_rate_min_lr = args.stage1_learning_rate * 0.02\n    if args.reduce_position is None:\n        args.reduce_position = args.hidden_size // 4\n    if args.num_tree_lstm_layers is None:\n        if args.constituency_composition in (ConstituencyComposition.TREE_LSTM, ConstituencyComposition.TREE_LSTM_CX):\n            args.num_tree_lstm_layers = 2\n        else:\n            args.num_tree_lstm_layers = 1\n    if args.wandb_name or args.wandb_norm_regex:\n        args.wandb = True\n    args = vars(args)\n    retagging.postprocess_args(args)\n    postprocess_predict_output_args(args)\n    model_save_file = build_model_filename(args)\n    args['save_name'] = model_save_file\n    if args['checkpoint']:\n        args['checkpoint_save_name'] = utils.checkpoint_name(args['save_dir'], model_save_file, args['checkpoint_save_name'])\n    return args",
            "def parse_args(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = build_argparse()\n    args = parser.parse_args(args=args)\n    if not args.lang and args.shorthand and (len(args.shorthand.split('_', maxsplit=1)) == 2):\n        args.lang = args.shorthand.split('_')[0]\n    if args.stage1_bert_learning_rate is None:\n        args.stage1_bert_learning_rate = args.bert_learning_rate\n    if args.optim is None and args.mode == 'train':\n        if not args.multistage:\n            args.optim = 'adadelta'\n        elif args.bert_finetune or args.stage1_bert_finetune:\n            logger.info('Multistage training is set, optimizer is not chosen, and bert finetuning is active.  Will use AdamW as the second stage optimizer.')\n            args.optim = 'adamw'\n        else:\n            try:\n                import madgrad\n                args.optim = 'madgrad'\n                logger.info('Multistage training is set, optimizer is not chosen, and MADGRAD is available.  Will use MADGRAD as the second stage optimizer.')\n            except ModuleNotFoundError as e:\n                logger.warning('Multistage training is set.  Best models are with MADGRAD, but it is not installed.  Will use AdamW for the second stage optimizer.  Consider installing MADGRAD')\n                args.optim = 'adamw'\n    if args.mode == 'train':\n        if args.learning_rate is None:\n            args.learning_rate = DEFAULT_LEARNING_RATES.get(args.optim.lower(), None)\n        if args.learning_eps is None:\n            args.learning_eps = DEFAULT_LEARNING_EPS.get(args.optim.lower(), None)\n        if args.learning_momentum is None:\n            args.learning_momentum = DEFAULT_MOMENTUM.get(args.optim.lower(), None)\n        if args.learning_weight_decay is None:\n            args.learning_weight_decay = DEFAULT_WEIGHT_DECAY.get(args.optim.lower(), None)\n        if args.stage1_learning_rate is None:\n            args.stage1_learning_rate = DEFAULT_LEARNING_RATES['adadelta']\n        if args.stage1_bert_finetune is None:\n            args.stage1_bert_finetune = args.bert_finetune\n        if args.learning_rate_min_lr is None:\n            args.learning_rate_min_lr = args.learning_rate * 0.02\n        if args.stage1_learning_rate_min_lr is None:\n            args.stage1_learning_rate_min_lr = args.stage1_learning_rate * 0.02\n    if args.reduce_position is None:\n        args.reduce_position = args.hidden_size // 4\n    if args.num_tree_lstm_layers is None:\n        if args.constituency_composition in (ConstituencyComposition.TREE_LSTM, ConstituencyComposition.TREE_LSTM_CX):\n            args.num_tree_lstm_layers = 2\n        else:\n            args.num_tree_lstm_layers = 1\n    if args.wandb_name or args.wandb_norm_regex:\n        args.wandb = True\n    args = vars(args)\n    retagging.postprocess_args(args)\n    postprocess_predict_output_args(args)\n    model_save_file = build_model_filename(args)\n    args['save_name'] = model_save_file\n    if args['checkpoint']:\n        args['checkpoint_save_name'] = utils.checkpoint_name(args['save_dir'], model_save_file, args['checkpoint_save_name'])\n    return args",
            "def parse_args(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = build_argparse()\n    args = parser.parse_args(args=args)\n    if not args.lang and args.shorthand and (len(args.shorthand.split('_', maxsplit=1)) == 2):\n        args.lang = args.shorthand.split('_')[0]\n    if args.stage1_bert_learning_rate is None:\n        args.stage1_bert_learning_rate = args.bert_learning_rate\n    if args.optim is None and args.mode == 'train':\n        if not args.multistage:\n            args.optim = 'adadelta'\n        elif args.bert_finetune or args.stage1_bert_finetune:\n            logger.info('Multistage training is set, optimizer is not chosen, and bert finetuning is active.  Will use AdamW as the second stage optimizer.')\n            args.optim = 'adamw'\n        else:\n            try:\n                import madgrad\n                args.optim = 'madgrad'\n                logger.info('Multistage training is set, optimizer is not chosen, and MADGRAD is available.  Will use MADGRAD as the second stage optimizer.')\n            except ModuleNotFoundError as e:\n                logger.warning('Multistage training is set.  Best models are with MADGRAD, but it is not installed.  Will use AdamW for the second stage optimizer.  Consider installing MADGRAD')\n                args.optim = 'adamw'\n    if args.mode == 'train':\n        if args.learning_rate is None:\n            args.learning_rate = DEFAULT_LEARNING_RATES.get(args.optim.lower(), None)\n        if args.learning_eps is None:\n            args.learning_eps = DEFAULT_LEARNING_EPS.get(args.optim.lower(), None)\n        if args.learning_momentum is None:\n            args.learning_momentum = DEFAULT_MOMENTUM.get(args.optim.lower(), None)\n        if args.learning_weight_decay is None:\n            args.learning_weight_decay = DEFAULT_WEIGHT_DECAY.get(args.optim.lower(), None)\n        if args.stage1_learning_rate is None:\n            args.stage1_learning_rate = DEFAULT_LEARNING_RATES['adadelta']\n        if args.stage1_bert_finetune is None:\n            args.stage1_bert_finetune = args.bert_finetune\n        if args.learning_rate_min_lr is None:\n            args.learning_rate_min_lr = args.learning_rate * 0.02\n        if args.stage1_learning_rate_min_lr is None:\n            args.stage1_learning_rate_min_lr = args.stage1_learning_rate * 0.02\n    if args.reduce_position is None:\n        args.reduce_position = args.hidden_size // 4\n    if args.num_tree_lstm_layers is None:\n        if args.constituency_composition in (ConstituencyComposition.TREE_LSTM, ConstituencyComposition.TREE_LSTM_CX):\n            args.num_tree_lstm_layers = 2\n        else:\n            args.num_tree_lstm_layers = 1\n    if args.wandb_name or args.wandb_norm_regex:\n        args.wandb = True\n    args = vars(args)\n    retagging.postprocess_args(args)\n    postprocess_predict_output_args(args)\n    model_save_file = build_model_filename(args)\n    args['save_name'] = model_save_file\n    if args['checkpoint']:\n        args['checkpoint_save_name'] = utils.checkpoint_name(args['save_dir'], model_save_file, args['checkpoint_save_name'])\n    return args"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(args=None):\n    \"\"\"\n    Main function for building con parser\n\n    Processes args, calls the appropriate function for the chosen --mode\n    \"\"\"\n    args = parse_args(args=args)\n    utils.set_random_seed(args['seed'])\n    logger.info('Running constituency parser in %s mode', args['mode'])\n    logger.debug('Using device: %s', args['device'])\n    model_save_each_file = None\n    if args['save_each_name']:\n        model_save_each_file = os.path.join(args['save_dir'], args['save_each_name'])\n        try:\n            model_save_each_file % 1\n        except TypeError:\n            pieces = os.path.splitext(model_save_each_file)\n            model_save_each_file = pieces[0] + '_%04d' + pieces[1]\n    model_load_file = args['save_name']\n    if args['load_name']:\n        if os.path.exists(args['load_name']):\n            model_load_file = args['load_name']\n        else:\n            model_load_file = os.path.join(args['save_dir'], args['load_name'])\n    elif args['load_package']:\n        if args['lang'] is None:\n            lang_pieces = args['load_package'].split('_', maxsplit=1)\n            try:\n                lang = constant.lang_to_langcode(lang_pieces[0])\n            except ValueError as e:\n                raise ValueError('--lang not specified, and the start of the --load_package name, %s, is not a known language.  Please check the values of those parameters' % args['load_package']) from e\n            args['lang'] = lang\n            args['load_package'] = lang_pieces[1]\n        stanza.download(args['lang'], processors='constituency', package={'constituency': args['load_package']})\n        model_load_file = os.path.join(DEFAULT_MODEL_DIR, args['lang'], 'constituency', args['load_package'] + '.pt')\n        if not os.path.exists(model_load_file):\n            raise FileNotFoundError(\"Expected the downloaded model file for language %s package %s to be in %s, but there is nothing there.  Perhaps the package name doesn't exist?\" % (args['lang'], args['load_package'], model_load_file))\n        else:\n            logger.info('Model for language %s package %s is in %s', args['lang'], args['load_package'], model_load_file)\n    retag_pipeline = retagging.build_retag_pipeline(args)\n    if args['mode'] == 'train':\n        trainer.train(args, model_load_file, model_save_each_file, retag_pipeline)\n    elif args['mode'] == 'predict':\n        trainer.evaluate(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'parse_text':\n        trainer.load_model_parse_text(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'remove_optimizer':\n        trainer.remove_optimizer(args, args['save_name'], model_load_file)",
        "mutated": [
            "def main(args=None):\n    if False:\n        i = 10\n    '\\n    Main function for building con parser\\n\\n    Processes args, calls the appropriate function for the chosen --mode\\n    '\n    args = parse_args(args=args)\n    utils.set_random_seed(args['seed'])\n    logger.info('Running constituency parser in %s mode', args['mode'])\n    logger.debug('Using device: %s', args['device'])\n    model_save_each_file = None\n    if args['save_each_name']:\n        model_save_each_file = os.path.join(args['save_dir'], args['save_each_name'])\n        try:\n            model_save_each_file % 1\n        except TypeError:\n            pieces = os.path.splitext(model_save_each_file)\n            model_save_each_file = pieces[0] + '_%04d' + pieces[1]\n    model_load_file = args['save_name']\n    if args['load_name']:\n        if os.path.exists(args['load_name']):\n            model_load_file = args['load_name']\n        else:\n            model_load_file = os.path.join(args['save_dir'], args['load_name'])\n    elif args['load_package']:\n        if args['lang'] is None:\n            lang_pieces = args['load_package'].split('_', maxsplit=1)\n            try:\n                lang = constant.lang_to_langcode(lang_pieces[0])\n            except ValueError as e:\n                raise ValueError('--lang not specified, and the start of the --load_package name, %s, is not a known language.  Please check the values of those parameters' % args['load_package']) from e\n            args['lang'] = lang\n            args['load_package'] = lang_pieces[1]\n        stanza.download(args['lang'], processors='constituency', package={'constituency': args['load_package']})\n        model_load_file = os.path.join(DEFAULT_MODEL_DIR, args['lang'], 'constituency', args['load_package'] + '.pt')\n        if not os.path.exists(model_load_file):\n            raise FileNotFoundError(\"Expected the downloaded model file for language %s package %s to be in %s, but there is nothing there.  Perhaps the package name doesn't exist?\" % (args['lang'], args['load_package'], model_load_file))\n        else:\n            logger.info('Model for language %s package %s is in %s', args['lang'], args['load_package'], model_load_file)\n    retag_pipeline = retagging.build_retag_pipeline(args)\n    if args['mode'] == 'train':\n        trainer.train(args, model_load_file, model_save_each_file, retag_pipeline)\n    elif args['mode'] == 'predict':\n        trainer.evaluate(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'parse_text':\n        trainer.load_model_parse_text(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'remove_optimizer':\n        trainer.remove_optimizer(args, args['save_name'], model_load_file)",
            "def main(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Main function for building con parser\\n\\n    Processes args, calls the appropriate function for the chosen --mode\\n    '\n    args = parse_args(args=args)\n    utils.set_random_seed(args['seed'])\n    logger.info('Running constituency parser in %s mode', args['mode'])\n    logger.debug('Using device: %s', args['device'])\n    model_save_each_file = None\n    if args['save_each_name']:\n        model_save_each_file = os.path.join(args['save_dir'], args['save_each_name'])\n        try:\n            model_save_each_file % 1\n        except TypeError:\n            pieces = os.path.splitext(model_save_each_file)\n            model_save_each_file = pieces[0] + '_%04d' + pieces[1]\n    model_load_file = args['save_name']\n    if args['load_name']:\n        if os.path.exists(args['load_name']):\n            model_load_file = args['load_name']\n        else:\n            model_load_file = os.path.join(args['save_dir'], args['load_name'])\n    elif args['load_package']:\n        if args['lang'] is None:\n            lang_pieces = args['load_package'].split('_', maxsplit=1)\n            try:\n                lang = constant.lang_to_langcode(lang_pieces[0])\n            except ValueError as e:\n                raise ValueError('--lang not specified, and the start of the --load_package name, %s, is not a known language.  Please check the values of those parameters' % args['load_package']) from e\n            args['lang'] = lang\n            args['load_package'] = lang_pieces[1]\n        stanza.download(args['lang'], processors='constituency', package={'constituency': args['load_package']})\n        model_load_file = os.path.join(DEFAULT_MODEL_DIR, args['lang'], 'constituency', args['load_package'] + '.pt')\n        if not os.path.exists(model_load_file):\n            raise FileNotFoundError(\"Expected the downloaded model file for language %s package %s to be in %s, but there is nothing there.  Perhaps the package name doesn't exist?\" % (args['lang'], args['load_package'], model_load_file))\n        else:\n            logger.info('Model for language %s package %s is in %s', args['lang'], args['load_package'], model_load_file)\n    retag_pipeline = retagging.build_retag_pipeline(args)\n    if args['mode'] == 'train':\n        trainer.train(args, model_load_file, model_save_each_file, retag_pipeline)\n    elif args['mode'] == 'predict':\n        trainer.evaluate(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'parse_text':\n        trainer.load_model_parse_text(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'remove_optimizer':\n        trainer.remove_optimizer(args, args['save_name'], model_load_file)",
            "def main(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Main function for building con parser\\n\\n    Processes args, calls the appropriate function for the chosen --mode\\n    '\n    args = parse_args(args=args)\n    utils.set_random_seed(args['seed'])\n    logger.info('Running constituency parser in %s mode', args['mode'])\n    logger.debug('Using device: %s', args['device'])\n    model_save_each_file = None\n    if args['save_each_name']:\n        model_save_each_file = os.path.join(args['save_dir'], args['save_each_name'])\n        try:\n            model_save_each_file % 1\n        except TypeError:\n            pieces = os.path.splitext(model_save_each_file)\n            model_save_each_file = pieces[0] + '_%04d' + pieces[1]\n    model_load_file = args['save_name']\n    if args['load_name']:\n        if os.path.exists(args['load_name']):\n            model_load_file = args['load_name']\n        else:\n            model_load_file = os.path.join(args['save_dir'], args['load_name'])\n    elif args['load_package']:\n        if args['lang'] is None:\n            lang_pieces = args['load_package'].split('_', maxsplit=1)\n            try:\n                lang = constant.lang_to_langcode(lang_pieces[0])\n            except ValueError as e:\n                raise ValueError('--lang not specified, and the start of the --load_package name, %s, is not a known language.  Please check the values of those parameters' % args['load_package']) from e\n            args['lang'] = lang\n            args['load_package'] = lang_pieces[1]\n        stanza.download(args['lang'], processors='constituency', package={'constituency': args['load_package']})\n        model_load_file = os.path.join(DEFAULT_MODEL_DIR, args['lang'], 'constituency', args['load_package'] + '.pt')\n        if not os.path.exists(model_load_file):\n            raise FileNotFoundError(\"Expected the downloaded model file for language %s package %s to be in %s, but there is nothing there.  Perhaps the package name doesn't exist?\" % (args['lang'], args['load_package'], model_load_file))\n        else:\n            logger.info('Model for language %s package %s is in %s', args['lang'], args['load_package'], model_load_file)\n    retag_pipeline = retagging.build_retag_pipeline(args)\n    if args['mode'] == 'train':\n        trainer.train(args, model_load_file, model_save_each_file, retag_pipeline)\n    elif args['mode'] == 'predict':\n        trainer.evaluate(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'parse_text':\n        trainer.load_model_parse_text(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'remove_optimizer':\n        trainer.remove_optimizer(args, args['save_name'], model_load_file)",
            "def main(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Main function for building con parser\\n\\n    Processes args, calls the appropriate function for the chosen --mode\\n    '\n    args = parse_args(args=args)\n    utils.set_random_seed(args['seed'])\n    logger.info('Running constituency parser in %s mode', args['mode'])\n    logger.debug('Using device: %s', args['device'])\n    model_save_each_file = None\n    if args['save_each_name']:\n        model_save_each_file = os.path.join(args['save_dir'], args['save_each_name'])\n        try:\n            model_save_each_file % 1\n        except TypeError:\n            pieces = os.path.splitext(model_save_each_file)\n            model_save_each_file = pieces[0] + '_%04d' + pieces[1]\n    model_load_file = args['save_name']\n    if args['load_name']:\n        if os.path.exists(args['load_name']):\n            model_load_file = args['load_name']\n        else:\n            model_load_file = os.path.join(args['save_dir'], args['load_name'])\n    elif args['load_package']:\n        if args['lang'] is None:\n            lang_pieces = args['load_package'].split('_', maxsplit=1)\n            try:\n                lang = constant.lang_to_langcode(lang_pieces[0])\n            except ValueError as e:\n                raise ValueError('--lang not specified, and the start of the --load_package name, %s, is not a known language.  Please check the values of those parameters' % args['load_package']) from e\n            args['lang'] = lang\n            args['load_package'] = lang_pieces[1]\n        stanza.download(args['lang'], processors='constituency', package={'constituency': args['load_package']})\n        model_load_file = os.path.join(DEFAULT_MODEL_DIR, args['lang'], 'constituency', args['load_package'] + '.pt')\n        if not os.path.exists(model_load_file):\n            raise FileNotFoundError(\"Expected the downloaded model file for language %s package %s to be in %s, but there is nothing there.  Perhaps the package name doesn't exist?\" % (args['lang'], args['load_package'], model_load_file))\n        else:\n            logger.info('Model for language %s package %s is in %s', args['lang'], args['load_package'], model_load_file)\n    retag_pipeline = retagging.build_retag_pipeline(args)\n    if args['mode'] == 'train':\n        trainer.train(args, model_load_file, model_save_each_file, retag_pipeline)\n    elif args['mode'] == 'predict':\n        trainer.evaluate(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'parse_text':\n        trainer.load_model_parse_text(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'remove_optimizer':\n        trainer.remove_optimizer(args, args['save_name'], model_load_file)",
            "def main(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Main function for building con parser\\n\\n    Processes args, calls the appropriate function for the chosen --mode\\n    '\n    args = parse_args(args=args)\n    utils.set_random_seed(args['seed'])\n    logger.info('Running constituency parser in %s mode', args['mode'])\n    logger.debug('Using device: %s', args['device'])\n    model_save_each_file = None\n    if args['save_each_name']:\n        model_save_each_file = os.path.join(args['save_dir'], args['save_each_name'])\n        try:\n            model_save_each_file % 1\n        except TypeError:\n            pieces = os.path.splitext(model_save_each_file)\n            model_save_each_file = pieces[0] + '_%04d' + pieces[1]\n    model_load_file = args['save_name']\n    if args['load_name']:\n        if os.path.exists(args['load_name']):\n            model_load_file = args['load_name']\n        else:\n            model_load_file = os.path.join(args['save_dir'], args['load_name'])\n    elif args['load_package']:\n        if args['lang'] is None:\n            lang_pieces = args['load_package'].split('_', maxsplit=1)\n            try:\n                lang = constant.lang_to_langcode(lang_pieces[0])\n            except ValueError as e:\n                raise ValueError('--lang not specified, and the start of the --load_package name, %s, is not a known language.  Please check the values of those parameters' % args['load_package']) from e\n            args['lang'] = lang\n            args['load_package'] = lang_pieces[1]\n        stanza.download(args['lang'], processors='constituency', package={'constituency': args['load_package']})\n        model_load_file = os.path.join(DEFAULT_MODEL_DIR, args['lang'], 'constituency', args['load_package'] + '.pt')\n        if not os.path.exists(model_load_file):\n            raise FileNotFoundError(\"Expected the downloaded model file for language %s package %s to be in %s, but there is nothing there.  Perhaps the package name doesn't exist?\" % (args['lang'], args['load_package'], model_load_file))\n        else:\n            logger.info('Model for language %s package %s is in %s', args['lang'], args['load_package'], model_load_file)\n    retag_pipeline = retagging.build_retag_pipeline(args)\n    if args['mode'] == 'train':\n        trainer.train(args, model_load_file, model_save_each_file, retag_pipeline)\n    elif args['mode'] == 'predict':\n        trainer.evaluate(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'parse_text':\n        trainer.load_model_parse_text(args, model_load_file, retag_pipeline)\n    elif args['mode'] == 'remove_optimizer':\n        trainer.remove_optimizer(args, args['save_name'], model_load_file)"
        ]
    }
]