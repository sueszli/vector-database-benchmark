import os
import torch
import torch.nn as nn
import transformers
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
'Finetune-opt-bnb-peft.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o\n\n## Fine-tune large models using ðŸ¤— `peft` adapters, `transformers` & `bitsandbytes`\n\nIn this tutorial we will cover how we can fine-tune large language models using the very recent `peft` library and `bitsandbytes` for loading large models in 8-bit.\nThe fine-tuning method will rely on a recent method called "Low Rank Adapters" (LoRA), instead of fine-tuning the entire model you just have to fine-tune these adapters and load them properly inside the model.\nAfter fine-tuning the model you can also share your adapters on the ðŸ¤— Hub and load them very easily. Let\'s get started!\n\n### Install requirements\n\nFirst, run the cells below to install the requirements:\n'
"### Model loading\n\nHere let's load the `opt-6.7b` model, its weights in half-precision (float16) are about 13GB on the Hub! If we load them in 8-bit we would require around 7GB of memory instead.\n"
free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024 ** 3)
max_memory = f'{free_in_GB - 2}GB'
n_gpus = torch.cuda.device_count()
max_memory = {i: max_memory for i in range(n_gpus)}
model = AutoModelForCausalLM.from_pretrained('facebook/opt-350m', max_memory=max_memory, quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_threshold=6.0, llm_int8_has_fp16_weight=False, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type='nf4'), torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')
"### Post-processing on the model\n\nFinally, we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in `float32` for stability. We also cast the output of the last layer in `float32` for the same reasons.\n"
print(model)
for param in model.parameters():
    param.requires_grad = False
    if param.ndim == 1:
        param.data = param.data.to(torch.float32)

class CastOutputToFloat(nn.Sequential):

    def forward(self, x):
        if False:
            for i in range(10):
                print('nop')
        return super().forward(x).to(torch.float32)
model.lm_head = CastOutputToFloat(model.lm_head)
"### Apply LoRA\n\nHere comes the magic with `peft`! Let's load a `PeftModel` and specify that we are going to use low-rank adapters (LoRA) using `get_peft_model` utility function from `peft`.\n"

def print_trainable_parameters(model):
    if False:
        for i in range(10):
            print('nop')
    '\n    Prints the number of trainable parameters in the model.\n    '
    trainable_params = 0
    all_param = 0
    for (_, param) in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(f'trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}')
config = LoraConfig(r=64, lora_alpha=32, target_modules=['q_proj', 'v_proj', 'out_proj', 'fc1', 'fc2'], lora_dropout=0.01, bias='none', task_type='CAUSAL_LM')
model = get_peft_model(model, config)
print_trainable_parameters(model)
dtypes = {}
for (_, p) in model.named_parameters():
    dtype = p.dtype
    if dtype not in dtypes:
        dtypes[dtype] = 0
    dtypes[dtype] += p.numel()
total = 0
for (k, v) in dtypes.items():
    total += v
for (k, v) in dtypes.items():
    print(k, v, v / total)
'### Training'
data = load_dataset('Abirate/english_quotes')
data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)
data = load_dataset('Abirate/english_quotes')
data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)
trainer = transformers.Trainer(model=model, train_dataset=data['train'], args=transformers.TrainingArguments(per_device_train_batch_size=4, gradient_accumulation_steps=4, warmup_steps=10, max_steps=20, learning_rate=0.0003, fp16=True, logging_steps=1, output_dir='outputs'), data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False))
model.config.use_cache = False
trainer.train()
'## Load adapters from the Hub\n\nYou can also directly load adapters from the Hub using the commands below:\n'
batch = tokenizer('Two things are infinite: ', return_tensors='pt')
model.config.use_cache = False
model.eval()
with torch.cuda.amp.autocast():
    output_tokens = model.generate(**batch, max_new_tokens=50)
print('\n\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))