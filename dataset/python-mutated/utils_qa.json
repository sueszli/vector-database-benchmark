[
    {
        "func_name": "postprocess_qa_predictions",
        "original": "def postprocess_qa_predictions(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, null_score_diff_threshold: float=0.0, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    \"\"\"\n    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\n    original contexts. This is the base postprocessing functions for models that only return start and end logits.\n\n    Args:\n        examples: The non-preprocessed dataset (see the main script for more information).\n        features: The processed dataset (see the main script for more information).\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n            first dimension must match the number of elements of :obj:`features`.\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n            Whether or not the underlying dataset contains examples with no answers.\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\n            The total number of n-best predictions to generate when looking for an answer.\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n            are not conditioned on one another.\n        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\n            The threshold used to select the null answer: if the best answer has a score that is less than the score of\n            the null answer minus this threshold, the null answer is selected for this example (note that the score of\n            the null answer for an example giving several features is the minimum of the scores for the null answer on\n            each feature: all features must be aligned on the fact they `want` to predict a null answer).\n\n            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\n        output_dir (:obj:`str`, `optional`):\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\n            answers, are saved in `output_dir`.\n        prefix (:obj:`str`, `optional`):\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\n            ``logging`` log level (e.g., ``logging.WARNING``)\n    \"\"\"\n    if len(predictions) != 2:\n        raise ValueError('`predictions` should be a tuple with two elements (start_logits, end_logits).')\n    (all_start_logits, all_end_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    if version_2_with_negative:\n        scores_diff_json = collections.OrderedDict()\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_prediction = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            feature_null_score = start_logits[0] + end_logits[0]\n            if min_null_prediction is None or min_null_prediction['score'] > feature_null_score:\n                min_null_prediction = {'offsets': (0, 0), 'score': feature_null_score, 'start_logit': start_logits[0], 'end_logit': end_logits[0]}\n            start_indexes = np.argsort(start_logits)[-1:-n_best_size - 1:-1].tolist()\n            end_indexes = np.argsort(end_logits)[-1:-n_best_size - 1:-1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_logits[start_index] + end_logits[end_index], 'start_logit': start_logits[start_index], 'end_logit': end_logits[end_index]})\n        if version_2_with_negative and min_null_prediction is not None:\n            prelim_predictions.append(min_null_prediction)\n            null_score = min_null_prediction['score']\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        if version_2_with_negative and min_null_prediction is not None and (not any((p['offsets'] == (0, 0) for p in predictions))):\n            predictions.append(min_null_prediction)\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0]['text'] == ''):\n            predictions.insert(0, {'text': 'empty', 'start_logit': 0.0, 'end_logit': 0.0, 'score': 0.0})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        if not version_2_with_negative:\n            all_predictions[example['id']] = predictions[0]['text']\n        else:\n            i = 0\n            while predictions[i]['text'] == '':\n                i += 1\n            best_non_null_pred = predictions[i]\n            score_diff = null_score - best_non_null_pred['start_logit'] - best_non_null_pred['end_logit']\n            scores_diff_json[example['id']] = float(score_diff)\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example['id']] = ''\n            else:\n                all_predictions[example['id']] = best_non_null_pred['text']\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return all_predictions",
        "mutated": [
            "def postprocess_qa_predictions(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, null_score_diff_threshold: float=0.0, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    if False:\n        i = 10\n    '\\n    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\\n    original contexts. This is the base postprocessing functions for models that only return start and end logits.\\n\\n    Args:\\n        examples: The non-preprocessed dataset (see the main script for more information).\\n        features: The processed dataset (see the main script for more information).\\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\\n            first dimension must match the number of elements of :obj:`features`.\\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\\n            Whether or not the underlying dataset contains examples with no answers.\\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\\n            The total number of n-best predictions to generate when looking for an answer.\\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\\n            are not conditioned on one another.\\n        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\\n            The threshold used to select the null answer: if the best answer has a score that is less than the score of\\n            the null answer minus this threshold, the null answer is selected for this example (note that the score of\\n            the null answer for an example giving several features is the minimum of the scores for the null answer on\\n            each feature: all features must be aligned on the fact they `want` to predict a null answer).\\n\\n            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\\n        output_dir (:obj:`str`, `optional`):\\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\\n            answers, are saved in `output_dir`.\\n        prefix (:obj:`str`, `optional`):\\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\\n            ``logging`` log level (e.g., ``logging.WARNING``)\\n    '\n    if len(predictions) != 2:\n        raise ValueError('`predictions` should be a tuple with two elements (start_logits, end_logits).')\n    (all_start_logits, all_end_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    if version_2_with_negative:\n        scores_diff_json = collections.OrderedDict()\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_prediction = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            feature_null_score = start_logits[0] + end_logits[0]\n            if min_null_prediction is None or min_null_prediction['score'] > feature_null_score:\n                min_null_prediction = {'offsets': (0, 0), 'score': feature_null_score, 'start_logit': start_logits[0], 'end_logit': end_logits[0]}\n            start_indexes = np.argsort(start_logits)[-1:-n_best_size - 1:-1].tolist()\n            end_indexes = np.argsort(end_logits)[-1:-n_best_size - 1:-1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_logits[start_index] + end_logits[end_index], 'start_logit': start_logits[start_index], 'end_logit': end_logits[end_index]})\n        if version_2_with_negative and min_null_prediction is not None:\n            prelim_predictions.append(min_null_prediction)\n            null_score = min_null_prediction['score']\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        if version_2_with_negative and min_null_prediction is not None and (not any((p['offsets'] == (0, 0) for p in predictions))):\n            predictions.append(min_null_prediction)\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0]['text'] == ''):\n            predictions.insert(0, {'text': 'empty', 'start_logit': 0.0, 'end_logit': 0.0, 'score': 0.0})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        if not version_2_with_negative:\n            all_predictions[example['id']] = predictions[0]['text']\n        else:\n            i = 0\n            while predictions[i]['text'] == '':\n                i += 1\n            best_non_null_pred = predictions[i]\n            score_diff = null_score - best_non_null_pred['start_logit'] - best_non_null_pred['end_logit']\n            scores_diff_json[example['id']] = float(score_diff)\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example['id']] = ''\n            else:\n                all_predictions[example['id']] = best_non_null_pred['text']\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return all_predictions",
            "def postprocess_qa_predictions(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, null_score_diff_threshold: float=0.0, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\\n    original contexts. This is the base postprocessing functions for models that only return start and end logits.\\n\\n    Args:\\n        examples: The non-preprocessed dataset (see the main script for more information).\\n        features: The processed dataset (see the main script for more information).\\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\\n            first dimension must match the number of elements of :obj:`features`.\\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\\n            Whether or not the underlying dataset contains examples with no answers.\\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\\n            The total number of n-best predictions to generate when looking for an answer.\\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\\n            are not conditioned on one another.\\n        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\\n            The threshold used to select the null answer: if the best answer has a score that is less than the score of\\n            the null answer minus this threshold, the null answer is selected for this example (note that the score of\\n            the null answer for an example giving several features is the minimum of the scores for the null answer on\\n            each feature: all features must be aligned on the fact they `want` to predict a null answer).\\n\\n            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\\n        output_dir (:obj:`str`, `optional`):\\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\\n            answers, are saved in `output_dir`.\\n        prefix (:obj:`str`, `optional`):\\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\\n            ``logging`` log level (e.g., ``logging.WARNING``)\\n    '\n    if len(predictions) != 2:\n        raise ValueError('`predictions` should be a tuple with two elements (start_logits, end_logits).')\n    (all_start_logits, all_end_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    if version_2_with_negative:\n        scores_diff_json = collections.OrderedDict()\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_prediction = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            feature_null_score = start_logits[0] + end_logits[0]\n            if min_null_prediction is None or min_null_prediction['score'] > feature_null_score:\n                min_null_prediction = {'offsets': (0, 0), 'score': feature_null_score, 'start_logit': start_logits[0], 'end_logit': end_logits[0]}\n            start_indexes = np.argsort(start_logits)[-1:-n_best_size - 1:-1].tolist()\n            end_indexes = np.argsort(end_logits)[-1:-n_best_size - 1:-1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_logits[start_index] + end_logits[end_index], 'start_logit': start_logits[start_index], 'end_logit': end_logits[end_index]})\n        if version_2_with_negative and min_null_prediction is not None:\n            prelim_predictions.append(min_null_prediction)\n            null_score = min_null_prediction['score']\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        if version_2_with_negative and min_null_prediction is not None and (not any((p['offsets'] == (0, 0) for p in predictions))):\n            predictions.append(min_null_prediction)\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0]['text'] == ''):\n            predictions.insert(0, {'text': 'empty', 'start_logit': 0.0, 'end_logit': 0.0, 'score': 0.0})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        if not version_2_with_negative:\n            all_predictions[example['id']] = predictions[0]['text']\n        else:\n            i = 0\n            while predictions[i]['text'] == '':\n                i += 1\n            best_non_null_pred = predictions[i]\n            score_diff = null_score - best_non_null_pred['start_logit'] - best_non_null_pred['end_logit']\n            scores_diff_json[example['id']] = float(score_diff)\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example['id']] = ''\n            else:\n                all_predictions[example['id']] = best_non_null_pred['text']\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return all_predictions",
            "def postprocess_qa_predictions(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, null_score_diff_threshold: float=0.0, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\\n    original contexts. This is the base postprocessing functions for models that only return start and end logits.\\n\\n    Args:\\n        examples: The non-preprocessed dataset (see the main script for more information).\\n        features: The processed dataset (see the main script for more information).\\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\\n            first dimension must match the number of elements of :obj:`features`.\\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\\n            Whether or not the underlying dataset contains examples with no answers.\\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\\n            The total number of n-best predictions to generate when looking for an answer.\\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\\n            are not conditioned on one another.\\n        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\\n            The threshold used to select the null answer: if the best answer has a score that is less than the score of\\n            the null answer minus this threshold, the null answer is selected for this example (note that the score of\\n            the null answer for an example giving several features is the minimum of the scores for the null answer on\\n            each feature: all features must be aligned on the fact they `want` to predict a null answer).\\n\\n            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\\n        output_dir (:obj:`str`, `optional`):\\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\\n            answers, are saved in `output_dir`.\\n        prefix (:obj:`str`, `optional`):\\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\\n            ``logging`` log level (e.g., ``logging.WARNING``)\\n    '\n    if len(predictions) != 2:\n        raise ValueError('`predictions` should be a tuple with two elements (start_logits, end_logits).')\n    (all_start_logits, all_end_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    if version_2_with_negative:\n        scores_diff_json = collections.OrderedDict()\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_prediction = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            feature_null_score = start_logits[0] + end_logits[0]\n            if min_null_prediction is None or min_null_prediction['score'] > feature_null_score:\n                min_null_prediction = {'offsets': (0, 0), 'score': feature_null_score, 'start_logit': start_logits[0], 'end_logit': end_logits[0]}\n            start_indexes = np.argsort(start_logits)[-1:-n_best_size - 1:-1].tolist()\n            end_indexes = np.argsort(end_logits)[-1:-n_best_size - 1:-1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_logits[start_index] + end_logits[end_index], 'start_logit': start_logits[start_index], 'end_logit': end_logits[end_index]})\n        if version_2_with_negative and min_null_prediction is not None:\n            prelim_predictions.append(min_null_prediction)\n            null_score = min_null_prediction['score']\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        if version_2_with_negative and min_null_prediction is not None and (not any((p['offsets'] == (0, 0) for p in predictions))):\n            predictions.append(min_null_prediction)\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0]['text'] == ''):\n            predictions.insert(0, {'text': 'empty', 'start_logit': 0.0, 'end_logit': 0.0, 'score': 0.0})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        if not version_2_with_negative:\n            all_predictions[example['id']] = predictions[0]['text']\n        else:\n            i = 0\n            while predictions[i]['text'] == '':\n                i += 1\n            best_non_null_pred = predictions[i]\n            score_diff = null_score - best_non_null_pred['start_logit'] - best_non_null_pred['end_logit']\n            scores_diff_json[example['id']] = float(score_diff)\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example['id']] = ''\n            else:\n                all_predictions[example['id']] = best_non_null_pred['text']\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return all_predictions",
            "def postprocess_qa_predictions(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, null_score_diff_threshold: float=0.0, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\\n    original contexts. This is the base postprocessing functions for models that only return start and end logits.\\n\\n    Args:\\n        examples: The non-preprocessed dataset (see the main script for more information).\\n        features: The processed dataset (see the main script for more information).\\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\\n            first dimension must match the number of elements of :obj:`features`.\\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\\n            Whether or not the underlying dataset contains examples with no answers.\\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\\n            The total number of n-best predictions to generate when looking for an answer.\\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\\n            are not conditioned on one another.\\n        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\\n            The threshold used to select the null answer: if the best answer has a score that is less than the score of\\n            the null answer minus this threshold, the null answer is selected for this example (note that the score of\\n            the null answer for an example giving several features is the minimum of the scores for the null answer on\\n            each feature: all features must be aligned on the fact they `want` to predict a null answer).\\n\\n            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\\n        output_dir (:obj:`str`, `optional`):\\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\\n            answers, are saved in `output_dir`.\\n        prefix (:obj:`str`, `optional`):\\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\\n            ``logging`` log level (e.g., ``logging.WARNING``)\\n    '\n    if len(predictions) != 2:\n        raise ValueError('`predictions` should be a tuple with two elements (start_logits, end_logits).')\n    (all_start_logits, all_end_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    if version_2_with_negative:\n        scores_diff_json = collections.OrderedDict()\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_prediction = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            feature_null_score = start_logits[0] + end_logits[0]\n            if min_null_prediction is None or min_null_prediction['score'] > feature_null_score:\n                min_null_prediction = {'offsets': (0, 0), 'score': feature_null_score, 'start_logit': start_logits[0], 'end_logit': end_logits[0]}\n            start_indexes = np.argsort(start_logits)[-1:-n_best_size - 1:-1].tolist()\n            end_indexes = np.argsort(end_logits)[-1:-n_best_size - 1:-1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_logits[start_index] + end_logits[end_index], 'start_logit': start_logits[start_index], 'end_logit': end_logits[end_index]})\n        if version_2_with_negative and min_null_prediction is not None:\n            prelim_predictions.append(min_null_prediction)\n            null_score = min_null_prediction['score']\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        if version_2_with_negative and min_null_prediction is not None and (not any((p['offsets'] == (0, 0) for p in predictions))):\n            predictions.append(min_null_prediction)\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0]['text'] == ''):\n            predictions.insert(0, {'text': 'empty', 'start_logit': 0.0, 'end_logit': 0.0, 'score': 0.0})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        if not version_2_with_negative:\n            all_predictions[example['id']] = predictions[0]['text']\n        else:\n            i = 0\n            while predictions[i]['text'] == '':\n                i += 1\n            best_non_null_pred = predictions[i]\n            score_diff = null_score - best_non_null_pred['start_logit'] - best_non_null_pred['end_logit']\n            scores_diff_json[example['id']] = float(score_diff)\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example['id']] = ''\n            else:\n                all_predictions[example['id']] = best_non_null_pred['text']\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return all_predictions",
            "def postprocess_qa_predictions(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, null_score_diff_threshold: float=0.0, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\\n    original contexts. This is the base postprocessing functions for models that only return start and end logits.\\n\\n    Args:\\n        examples: The non-preprocessed dataset (see the main script for more information).\\n        features: The processed dataset (see the main script for more information).\\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\\n            first dimension must match the number of elements of :obj:`features`.\\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\\n            Whether or not the underlying dataset contains examples with no answers.\\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\\n            The total number of n-best predictions to generate when looking for an answer.\\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\\n            are not conditioned on one another.\\n        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\\n            The threshold used to select the null answer: if the best answer has a score that is less than the score of\\n            the null answer minus this threshold, the null answer is selected for this example (note that the score of\\n            the null answer for an example giving several features is the minimum of the scores for the null answer on\\n            each feature: all features must be aligned on the fact they `want` to predict a null answer).\\n\\n            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\\n        output_dir (:obj:`str`, `optional`):\\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\\n            answers, are saved in `output_dir`.\\n        prefix (:obj:`str`, `optional`):\\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\\n            ``logging`` log level (e.g., ``logging.WARNING``)\\n    '\n    if len(predictions) != 2:\n        raise ValueError('`predictions` should be a tuple with two elements (start_logits, end_logits).')\n    (all_start_logits, all_end_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    if version_2_with_negative:\n        scores_diff_json = collections.OrderedDict()\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_prediction = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            feature_null_score = start_logits[0] + end_logits[0]\n            if min_null_prediction is None or min_null_prediction['score'] > feature_null_score:\n                min_null_prediction = {'offsets': (0, 0), 'score': feature_null_score, 'start_logit': start_logits[0], 'end_logit': end_logits[0]}\n            start_indexes = np.argsort(start_logits)[-1:-n_best_size - 1:-1].tolist()\n            end_indexes = np.argsort(end_logits)[-1:-n_best_size - 1:-1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_logits[start_index] + end_logits[end_index], 'start_logit': start_logits[start_index], 'end_logit': end_logits[end_index]})\n        if version_2_with_negative and min_null_prediction is not None:\n            prelim_predictions.append(min_null_prediction)\n            null_score = min_null_prediction['score']\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        if version_2_with_negative and min_null_prediction is not None and (not any((p['offsets'] == (0, 0) for p in predictions))):\n            predictions.append(min_null_prediction)\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0]['text'] == ''):\n            predictions.insert(0, {'text': 'empty', 'start_logit': 0.0, 'end_logit': 0.0, 'score': 0.0})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        if not version_2_with_negative:\n            all_predictions[example['id']] = predictions[0]['text']\n        else:\n            i = 0\n            while predictions[i]['text'] == '':\n                i += 1\n            best_non_null_pred = predictions[i]\n            score_diff = null_score - best_non_null_pred['start_logit'] - best_non_null_pred['end_logit']\n            scores_diff_json[example['id']] = float(score_diff)\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example['id']] = ''\n            else:\n                all_predictions[example['id']] = best_non_null_pred['text']\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return all_predictions"
        ]
    },
    {
        "func_name": "postprocess_qa_predictions_with_beam_search",
        "original": "def postprocess_qa_predictions_with_beam_search(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, start_n_top: int=5, end_n_top: int=5, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    \"\"\"\n    Post-processes the predictions of a question-answering model with beam search to convert them to answers that are substrings of the\n    original contexts. This is the postprocessing functions for models that return start and end logits, indices, as well as\n    cls token predictions.\n\n    Args:\n        examples: The non-preprocessed dataset (see the main script for more information).\n        features: The processed dataset (see the main script for more information).\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n            first dimension must match the number of elements of :obj:`features`.\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n            Whether or not the underlying dataset contains examples with no answers.\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\n            The total number of n-best predictions to generate when looking for an answer.\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n            are not conditioned on one another.\n        start_n_top (:obj:`int`, `optional`, defaults to 5):\n            The number of top start logits too keep when searching for the :obj:`n_best_size` predictions.\n        end_n_top (:obj:`int`, `optional`, defaults to 5):\n            The number of top end logits too keep when searching for the :obj:`n_best_size` predictions.\n        output_dir (:obj:`str`, `optional`):\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\n            answers, are saved in `output_dir`.\n        prefix (:obj:`str`, `optional`):\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\n            ``logging`` log level (e.g., ``logging.WARNING``)\n    \"\"\"\n    if len(predictions) != 5:\n        raise ValueError('`predictions` should be a tuple with five elements.')\n    (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict() if version_2_with_negative else None\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_score = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_log_prob = start_top_log_probs[feature_index]\n            start_indexes = start_top_index[feature_index]\n            end_log_prob = end_top_log_probs[feature_index]\n            end_indexes = end_top_index[feature_index]\n            feature_null_score = cls_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            if min_null_score is None or feature_null_score < min_null_score:\n                min_null_score = feature_null_score\n            for i in range(start_n_top):\n                for j in range(end_n_top):\n                    start_index = int(start_indexes[i])\n                    j_index = i * end_n_top + j\n                    end_index = int(end_indexes[j_index])\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_log_prob[i] + end_log_prob[j_index], 'start_log_prob': start_log_prob[i], 'end_log_prob': end_log_prob[j_index]})\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0:\n            min_null_score = -2e-06\n            predictions.insert(0, {'text': '', 'start_logit': -1e-06, 'end_logit': -1e-06, 'score': min_null_score})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        all_predictions[example['id']] = predictions[0]['text']\n        if version_2_with_negative:\n            scores_diff_json[example['id']] = float(min_null_score)\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return (all_predictions, scores_diff_json)",
        "mutated": [
            "def postprocess_qa_predictions_with_beam_search(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, start_n_top: int=5, end_n_top: int=5, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    if False:\n        i = 10\n    '\\n    Post-processes the predictions of a question-answering model with beam search to convert them to answers that are substrings of the\\n    original contexts. This is the postprocessing functions for models that return start and end logits, indices, as well as\\n    cls token predictions.\\n\\n    Args:\\n        examples: The non-preprocessed dataset (see the main script for more information).\\n        features: The processed dataset (see the main script for more information).\\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\\n            first dimension must match the number of elements of :obj:`features`.\\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\\n            Whether or not the underlying dataset contains examples with no answers.\\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\\n            The total number of n-best predictions to generate when looking for an answer.\\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\\n            are not conditioned on one another.\\n        start_n_top (:obj:`int`, `optional`, defaults to 5):\\n            The number of top start logits too keep when searching for the :obj:`n_best_size` predictions.\\n        end_n_top (:obj:`int`, `optional`, defaults to 5):\\n            The number of top end logits too keep when searching for the :obj:`n_best_size` predictions.\\n        output_dir (:obj:`str`, `optional`):\\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\\n            answers, are saved in `output_dir`.\\n        prefix (:obj:`str`, `optional`):\\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\\n            ``logging`` log level (e.g., ``logging.WARNING``)\\n    '\n    if len(predictions) != 5:\n        raise ValueError('`predictions` should be a tuple with five elements.')\n    (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict() if version_2_with_negative else None\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_score = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_log_prob = start_top_log_probs[feature_index]\n            start_indexes = start_top_index[feature_index]\n            end_log_prob = end_top_log_probs[feature_index]\n            end_indexes = end_top_index[feature_index]\n            feature_null_score = cls_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            if min_null_score is None or feature_null_score < min_null_score:\n                min_null_score = feature_null_score\n            for i in range(start_n_top):\n                for j in range(end_n_top):\n                    start_index = int(start_indexes[i])\n                    j_index = i * end_n_top + j\n                    end_index = int(end_indexes[j_index])\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_log_prob[i] + end_log_prob[j_index], 'start_log_prob': start_log_prob[i], 'end_log_prob': end_log_prob[j_index]})\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0:\n            min_null_score = -2e-06\n            predictions.insert(0, {'text': '', 'start_logit': -1e-06, 'end_logit': -1e-06, 'score': min_null_score})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        all_predictions[example['id']] = predictions[0]['text']\n        if version_2_with_negative:\n            scores_diff_json[example['id']] = float(min_null_score)\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return (all_predictions, scores_diff_json)",
            "def postprocess_qa_predictions_with_beam_search(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, start_n_top: int=5, end_n_top: int=5, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Post-processes the predictions of a question-answering model with beam search to convert them to answers that are substrings of the\\n    original contexts. This is the postprocessing functions for models that return start and end logits, indices, as well as\\n    cls token predictions.\\n\\n    Args:\\n        examples: The non-preprocessed dataset (see the main script for more information).\\n        features: The processed dataset (see the main script for more information).\\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\\n            first dimension must match the number of elements of :obj:`features`.\\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\\n            Whether or not the underlying dataset contains examples with no answers.\\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\\n            The total number of n-best predictions to generate when looking for an answer.\\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\\n            are not conditioned on one another.\\n        start_n_top (:obj:`int`, `optional`, defaults to 5):\\n            The number of top start logits too keep when searching for the :obj:`n_best_size` predictions.\\n        end_n_top (:obj:`int`, `optional`, defaults to 5):\\n            The number of top end logits too keep when searching for the :obj:`n_best_size` predictions.\\n        output_dir (:obj:`str`, `optional`):\\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\\n            answers, are saved in `output_dir`.\\n        prefix (:obj:`str`, `optional`):\\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\\n            ``logging`` log level (e.g., ``logging.WARNING``)\\n    '\n    if len(predictions) != 5:\n        raise ValueError('`predictions` should be a tuple with five elements.')\n    (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict() if version_2_with_negative else None\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_score = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_log_prob = start_top_log_probs[feature_index]\n            start_indexes = start_top_index[feature_index]\n            end_log_prob = end_top_log_probs[feature_index]\n            end_indexes = end_top_index[feature_index]\n            feature_null_score = cls_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            if min_null_score is None or feature_null_score < min_null_score:\n                min_null_score = feature_null_score\n            for i in range(start_n_top):\n                for j in range(end_n_top):\n                    start_index = int(start_indexes[i])\n                    j_index = i * end_n_top + j\n                    end_index = int(end_indexes[j_index])\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_log_prob[i] + end_log_prob[j_index], 'start_log_prob': start_log_prob[i], 'end_log_prob': end_log_prob[j_index]})\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0:\n            min_null_score = -2e-06\n            predictions.insert(0, {'text': '', 'start_logit': -1e-06, 'end_logit': -1e-06, 'score': min_null_score})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        all_predictions[example['id']] = predictions[0]['text']\n        if version_2_with_negative:\n            scores_diff_json[example['id']] = float(min_null_score)\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return (all_predictions, scores_diff_json)",
            "def postprocess_qa_predictions_with_beam_search(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, start_n_top: int=5, end_n_top: int=5, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Post-processes the predictions of a question-answering model with beam search to convert them to answers that are substrings of the\\n    original contexts. This is the postprocessing functions for models that return start and end logits, indices, as well as\\n    cls token predictions.\\n\\n    Args:\\n        examples: The non-preprocessed dataset (see the main script for more information).\\n        features: The processed dataset (see the main script for more information).\\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\\n            first dimension must match the number of elements of :obj:`features`.\\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\\n            Whether or not the underlying dataset contains examples with no answers.\\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\\n            The total number of n-best predictions to generate when looking for an answer.\\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\\n            are not conditioned on one another.\\n        start_n_top (:obj:`int`, `optional`, defaults to 5):\\n            The number of top start logits too keep when searching for the :obj:`n_best_size` predictions.\\n        end_n_top (:obj:`int`, `optional`, defaults to 5):\\n            The number of top end logits too keep when searching for the :obj:`n_best_size` predictions.\\n        output_dir (:obj:`str`, `optional`):\\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\\n            answers, are saved in `output_dir`.\\n        prefix (:obj:`str`, `optional`):\\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\\n            ``logging`` log level (e.g., ``logging.WARNING``)\\n    '\n    if len(predictions) != 5:\n        raise ValueError('`predictions` should be a tuple with five elements.')\n    (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict() if version_2_with_negative else None\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_score = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_log_prob = start_top_log_probs[feature_index]\n            start_indexes = start_top_index[feature_index]\n            end_log_prob = end_top_log_probs[feature_index]\n            end_indexes = end_top_index[feature_index]\n            feature_null_score = cls_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            if min_null_score is None or feature_null_score < min_null_score:\n                min_null_score = feature_null_score\n            for i in range(start_n_top):\n                for j in range(end_n_top):\n                    start_index = int(start_indexes[i])\n                    j_index = i * end_n_top + j\n                    end_index = int(end_indexes[j_index])\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_log_prob[i] + end_log_prob[j_index], 'start_log_prob': start_log_prob[i], 'end_log_prob': end_log_prob[j_index]})\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0:\n            min_null_score = -2e-06\n            predictions.insert(0, {'text': '', 'start_logit': -1e-06, 'end_logit': -1e-06, 'score': min_null_score})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        all_predictions[example['id']] = predictions[0]['text']\n        if version_2_with_negative:\n            scores_diff_json[example['id']] = float(min_null_score)\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return (all_predictions, scores_diff_json)",
            "def postprocess_qa_predictions_with_beam_search(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, start_n_top: int=5, end_n_top: int=5, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Post-processes the predictions of a question-answering model with beam search to convert them to answers that are substrings of the\\n    original contexts. This is the postprocessing functions for models that return start and end logits, indices, as well as\\n    cls token predictions.\\n\\n    Args:\\n        examples: The non-preprocessed dataset (see the main script for more information).\\n        features: The processed dataset (see the main script for more information).\\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\\n            first dimension must match the number of elements of :obj:`features`.\\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\\n            Whether or not the underlying dataset contains examples with no answers.\\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\\n            The total number of n-best predictions to generate when looking for an answer.\\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\\n            are not conditioned on one another.\\n        start_n_top (:obj:`int`, `optional`, defaults to 5):\\n            The number of top start logits too keep when searching for the :obj:`n_best_size` predictions.\\n        end_n_top (:obj:`int`, `optional`, defaults to 5):\\n            The number of top end logits too keep when searching for the :obj:`n_best_size` predictions.\\n        output_dir (:obj:`str`, `optional`):\\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\\n            answers, are saved in `output_dir`.\\n        prefix (:obj:`str`, `optional`):\\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\\n            ``logging`` log level (e.g., ``logging.WARNING``)\\n    '\n    if len(predictions) != 5:\n        raise ValueError('`predictions` should be a tuple with five elements.')\n    (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict() if version_2_with_negative else None\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_score = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_log_prob = start_top_log_probs[feature_index]\n            start_indexes = start_top_index[feature_index]\n            end_log_prob = end_top_log_probs[feature_index]\n            end_indexes = end_top_index[feature_index]\n            feature_null_score = cls_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            if min_null_score is None or feature_null_score < min_null_score:\n                min_null_score = feature_null_score\n            for i in range(start_n_top):\n                for j in range(end_n_top):\n                    start_index = int(start_indexes[i])\n                    j_index = i * end_n_top + j\n                    end_index = int(end_indexes[j_index])\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_log_prob[i] + end_log_prob[j_index], 'start_log_prob': start_log_prob[i], 'end_log_prob': end_log_prob[j_index]})\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0:\n            min_null_score = -2e-06\n            predictions.insert(0, {'text': '', 'start_logit': -1e-06, 'end_logit': -1e-06, 'score': min_null_score})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        all_predictions[example['id']] = predictions[0]['text']\n        if version_2_with_negative:\n            scores_diff_json[example['id']] = float(min_null_score)\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return (all_predictions, scores_diff_json)",
            "def postprocess_qa_predictions_with_beam_search(examples, features, predictions: Tuple[np.ndarray, np.ndarray], version_2_with_negative: bool=False, n_best_size: int=20, max_answer_length: int=30, start_n_top: int=5, end_n_top: int=5, output_dir: Optional[str]=None, prefix: Optional[str]=None, log_level: Optional[int]=logging.WARNING):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Post-processes the predictions of a question-answering model with beam search to convert them to answers that are substrings of the\\n    original contexts. This is the postprocessing functions for models that return start and end logits, indices, as well as\\n    cls token predictions.\\n\\n    Args:\\n        examples: The non-preprocessed dataset (see the main script for more information).\\n        features: The processed dataset (see the main script for more information).\\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\\n            first dimension must match the number of elements of :obj:`features`.\\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\\n            Whether or not the underlying dataset contains examples with no answers.\\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\\n            The total number of n-best predictions to generate when looking for an answer.\\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\\n            are not conditioned on one another.\\n        start_n_top (:obj:`int`, `optional`, defaults to 5):\\n            The number of top start logits too keep when searching for the :obj:`n_best_size` predictions.\\n        end_n_top (:obj:`int`, `optional`, defaults to 5):\\n            The number of top end logits too keep when searching for the :obj:`n_best_size` predictions.\\n        output_dir (:obj:`str`, `optional`):\\n            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\\n            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\\n            answers, are saved in `output_dir`.\\n        prefix (:obj:`str`, `optional`):\\n            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\\n        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\\n            ``logging`` log level (e.g., ``logging.WARNING``)\\n    '\n    if len(predictions) != 5:\n        raise ValueError('`predictions` should be a tuple with five elements.')\n    (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) = predictions\n    if len(predictions[0]) != len(features):\n        raise ValueError(f'Got {len(predictions[0])} predictions and {len(features)} features.')\n    example_id_to_index = {k: i for (i, k) in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for (i, feature) in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict() if version_2_with_negative else None\n    logger.setLevel(log_level)\n    logger.info(f'Post-processing {len(examples)} example predictions split into {len(features)} features.')\n    for (example_index, example) in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        min_null_score = None\n        prelim_predictions = []\n        for feature_index in feature_indices:\n            start_log_prob = start_top_log_probs[feature_index]\n            start_indexes = start_top_index[feature_index]\n            end_log_prob = end_top_log_probs[feature_index]\n            end_indexes = end_top_index[feature_index]\n            feature_null_score = cls_logits[feature_index]\n            offset_mapping = features[feature_index]['offset_mapping']\n            token_is_max_context = features[feature_index].get('token_is_max_context', None)\n            if min_null_score is None or feature_null_score < min_null_score:\n                min_null_score = feature_null_score\n            for i in range(start_n_top):\n                for j in range(end_n_top):\n                    start_index = int(start_indexes[i])\n                    j_index = i * end_n_top + j\n                    end_index = int(end_indexes[j_index])\n                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or (len(offset_mapping[start_index]) < 2) or (offset_mapping[end_index] is None) or (len(offset_mapping[end_index]) < 2):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    if token_is_max_context is not None and (not token_is_max_context.get(str(start_index), False)):\n                        continue\n                    prelim_predictions.append({'offsets': (offset_mapping[start_index][0], offset_mapping[end_index][1]), 'score': start_log_prob[i] + end_log_prob[j_index], 'start_log_prob': start_log_prob[i], 'end_log_prob': end_log_prob[j_index]})\n        predictions = sorted(prelim_predictions, key=lambda x: x['score'], reverse=True)[:n_best_size]\n        context = example['context']\n        for pred in predictions:\n            offsets = pred.pop('offsets')\n            pred['text'] = context[offsets[0]:offsets[1]]\n        if len(predictions) == 0:\n            min_null_score = -2e-06\n            predictions.insert(0, {'text': '', 'start_logit': -1e-06, 'end_logit': -1e-06, 'score': min_null_score})\n        scores = np.array([pred.pop('score') for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n        for (prob, pred) in zip(probs, predictions):\n            pred['probability'] = prob\n        all_predictions[example['id']] = predictions[0]['text']\n        if version_2_with_negative:\n            scores_diff_json[example['id']] = float(min_null_score)\n        all_nbest_json[example['id']] = [{k: float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v for (k, v) in pred.items()} for pred in predictions]\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f'{output_dir} is not a directory.')\n        prediction_file = os.path.join(output_dir, 'predictions.json' if prefix is None else f'{prefix}_predictions.json')\n        nbest_file = os.path.join(output_dir, 'nbest_predictions.json' if prefix is None else f'{prefix}_nbest_predictions.json')\n        if version_2_with_negative:\n            null_odds_file = os.path.join(output_dir, 'null_odds.json' if prefix is None else f'{prefix}_null_odds.json')\n        logger.info(f'Saving predictions to {prediction_file}.')\n        with open(prediction_file, 'w') as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + '\\n')\n        logger.info(f'Saving nbest_preds to {nbest_file}.')\n        with open(nbest_file, 'w') as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + '\\n')\n        if version_2_with_negative:\n            logger.info(f'Saving null_odds to {null_odds_file}.')\n            with open(null_odds_file, 'w') as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + '\\n')\n    return (all_predictions, scores_diff_json)"
        ]
    }
]