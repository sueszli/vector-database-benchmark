[
    {
        "func_name": "_draw_bootstrap_sample",
        "original": "def _draw_bootstrap_sample(rng, X, y):\n    sample_indices = np.arange(X.shape[0])\n    bootstrap_indices = rng.choice(sample_indices, size=sample_indices.shape[0], replace=True)\n    return (X[bootstrap_indices], y[bootstrap_indices])",
        "mutated": [
            "def _draw_bootstrap_sample(rng, X, y):\n    if False:\n        i = 10\n    sample_indices = np.arange(X.shape[0])\n    bootstrap_indices = rng.choice(sample_indices, size=sample_indices.shape[0], replace=True)\n    return (X[bootstrap_indices], y[bootstrap_indices])",
            "def _draw_bootstrap_sample(rng, X, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    sample_indices = np.arange(X.shape[0])\n    bootstrap_indices = rng.choice(sample_indices, size=sample_indices.shape[0], replace=True)\n    return (X[bootstrap_indices], y[bootstrap_indices])",
            "def _draw_bootstrap_sample(rng, X, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    sample_indices = np.arange(X.shape[0])\n    bootstrap_indices = rng.choice(sample_indices, size=sample_indices.shape[0], replace=True)\n    return (X[bootstrap_indices], y[bootstrap_indices])",
            "def _draw_bootstrap_sample(rng, X, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    sample_indices = np.arange(X.shape[0])\n    bootstrap_indices = rng.choice(sample_indices, size=sample_indices.shape[0], replace=True)\n    return (X[bootstrap_indices], y[bootstrap_indices])",
            "def _draw_bootstrap_sample(rng, X, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    sample_indices = np.arange(X.shape[0])\n    bootstrap_indices = rng.choice(sample_indices, size=sample_indices.shape[0], replace=True)\n    return (X[bootstrap_indices], y[bootstrap_indices])"
        ]
    },
    {
        "func_name": "bias_variance_decomp",
        "original": "def bias_variance_decomp(estimator, X_train, y_train, X_test, y_test, loss='0-1_loss', num_rounds=200, random_seed=None, **fit_params):\n    \"\"\"\n    estimator : object\n        A classifier or regressor object or class implementing both a\n        `fit` and `predict` method similar to the scikit-learn API.\n\n    X_train : array-like, shape=(num_examples, num_features)\n        A training dataset for drawing the bootstrap samples to carry\n        out the bias-variance decomposition.\n\n    y_train : array-like, shape=(num_examples)\n        Targets (class labels, continuous values in case of regression)\n        associated with the `X_train` examples.\n\n    X_test : array-like, shape=(num_examples, num_features)\n        The test dataset for computing the average loss, bias,\n        and variance.\n\n    y_test : array-like, shape=(num_examples)\n        Targets (class labels, continuous values in case of regression)\n        associated with the `X_test` examples.\n\n    loss : str (default='0-1_loss')\n        Loss function for performing the bias-variance decomposition.\n        Currently allowed values are '0-1_loss' and 'mse'.\n\n    num_rounds : int (default=200)\n        Number of bootstrap rounds (sampling from the training set)\n        for performing the bias-variance decomposition. Each bootstrap\n        sample has the same size as the original training set.\n\n    random_seed : int (default=None)\n        Random seed for the bootstrap sampling used for the\n        bias-variance decomposition.\n\n    fit_params : additional parameters\n        Additional parameters to be passed to the .fit() function of the\n        estimator when it is fit to the bootstrap samples.\n\n    Returns\n    ----------\n    avg_expected_loss, avg_bias, avg_var : returns the average expected\n        average bias, and average bias (all floats), where the average\n        is computed over the data points in the test set.\n\n    Examples\n    -----------\n    For usage examples, please see\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/\n\n    \"\"\"\n    supported = ['0-1_loss', 'mse']\n    if loss not in supported:\n        raise NotImplementedError('loss must be one of the following: %s' % supported)\n    for ary in (X_train, y_train, X_test, y_test):\n        if hasattr(ary, 'loc'):\n            raise ValueError('The bias_variance_decomp does not support pandas DataFrames yet. Please check the inputs to X_train, y_train, X_test, y_test. If e.g., X_train is a pandas DataFrame, try passing it as NumPy array via X_train=X_train.values.')\n    rng = np.random.RandomState(random_seed)\n    if loss == '0-1_loss':\n        dtype = np.int64\n    elif loss == 'mse':\n        dtype = np.float64\n    all_pred = np.zeros((num_rounds, y_test.shape[0]), dtype=dtype)\n    for i in range(num_rounds):\n        (X_boot, y_boot) = _draw_bootstrap_sample(rng, X_train, y_train)\n        if estimator.__class__.__name__ in ['Sequential', 'Functional']:\n            for (ix, layer) in enumerate(estimator.layers):\n                if hasattr(estimator.layers[ix], 'kernel_initializer') and hasattr(estimator.layers[ix], 'bias_initializer'):\n                    weight_initializer = estimator.layers[ix].kernel_initializer\n                    bias_initializer = estimator.layers[ix].bias_initializer\n                    (old_weights, old_biases) = estimator.layers[ix].get_weights()\n                    estimator.layers[ix].set_weights([weight_initializer(shape=old_weights.shape), bias_initializer(shape=len(old_biases))])\n            estimator.fit(X_boot, y_boot, **fit_params)\n            pred = estimator.predict(X_test).reshape(1, -1)\n        else:\n            pred = estimator.fit(X_boot, y_boot, **fit_params).predict(X_test)\n        all_pred[i] = pred\n    if loss == '0-1_loss':\n        main_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=all_pred)\n        avg_expected_loss = np.apply_along_axis(lambda x: (x != y_test).mean(), axis=1, arr=all_pred).mean()\n        avg_bias = np.sum(main_predictions != y_test) / y_test.size\n        var = np.zeros(pred.shape)\n        for pred in all_pred:\n            var += (pred != main_predictions).astype(np.int_)\n        var /= num_rounds\n        avg_var = var.sum() / y_test.shape[0]\n    else:\n        avg_expected_loss = np.apply_along_axis(lambda x: ((x - y_test) ** 2).mean(), axis=1, arr=all_pred).mean()\n        main_predictions = np.mean(all_pred, axis=0)\n        avg_bias = np.sum((main_predictions - y_test) ** 2) / y_test.size\n        avg_var = np.sum((main_predictions - all_pred) ** 2) / all_pred.size\n    return (avg_expected_loss, avg_bias, avg_var)",
        "mutated": [
            "def bias_variance_decomp(estimator, X_train, y_train, X_test, y_test, loss='0-1_loss', num_rounds=200, random_seed=None, **fit_params):\n    if False:\n        i = 10\n    \"\\n    estimator : object\\n        A classifier or regressor object or class implementing both a\\n        `fit` and `predict` method similar to the scikit-learn API.\\n\\n    X_train : array-like, shape=(num_examples, num_features)\\n        A training dataset for drawing the bootstrap samples to carry\\n        out the bias-variance decomposition.\\n\\n    y_train : array-like, shape=(num_examples)\\n        Targets (class labels, continuous values in case of regression)\\n        associated with the `X_train` examples.\\n\\n    X_test : array-like, shape=(num_examples, num_features)\\n        The test dataset for computing the average loss, bias,\\n        and variance.\\n\\n    y_test : array-like, shape=(num_examples)\\n        Targets (class labels, continuous values in case of regression)\\n        associated with the `X_test` examples.\\n\\n    loss : str (default='0-1_loss')\\n        Loss function for performing the bias-variance decomposition.\\n        Currently allowed values are '0-1_loss' and 'mse'.\\n\\n    num_rounds : int (default=200)\\n        Number of bootstrap rounds (sampling from the training set)\\n        for performing the bias-variance decomposition. Each bootstrap\\n        sample has the same size as the original training set.\\n\\n    random_seed : int (default=None)\\n        Random seed for the bootstrap sampling used for the\\n        bias-variance decomposition.\\n\\n    fit_params : additional parameters\\n        Additional parameters to be passed to the .fit() function of the\\n        estimator when it is fit to the bootstrap samples.\\n\\n    Returns\\n    ----------\\n    avg_expected_loss, avg_bias, avg_var : returns the average expected\\n        average bias, and average bias (all floats), where the average\\n        is computed over the data points in the test set.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/\\n\\n    \"\n    supported = ['0-1_loss', 'mse']\n    if loss not in supported:\n        raise NotImplementedError('loss must be one of the following: %s' % supported)\n    for ary in (X_train, y_train, X_test, y_test):\n        if hasattr(ary, 'loc'):\n            raise ValueError('The bias_variance_decomp does not support pandas DataFrames yet. Please check the inputs to X_train, y_train, X_test, y_test. If e.g., X_train is a pandas DataFrame, try passing it as NumPy array via X_train=X_train.values.')\n    rng = np.random.RandomState(random_seed)\n    if loss == '0-1_loss':\n        dtype = np.int64\n    elif loss == 'mse':\n        dtype = np.float64\n    all_pred = np.zeros((num_rounds, y_test.shape[0]), dtype=dtype)\n    for i in range(num_rounds):\n        (X_boot, y_boot) = _draw_bootstrap_sample(rng, X_train, y_train)\n        if estimator.__class__.__name__ in ['Sequential', 'Functional']:\n            for (ix, layer) in enumerate(estimator.layers):\n                if hasattr(estimator.layers[ix], 'kernel_initializer') and hasattr(estimator.layers[ix], 'bias_initializer'):\n                    weight_initializer = estimator.layers[ix].kernel_initializer\n                    bias_initializer = estimator.layers[ix].bias_initializer\n                    (old_weights, old_biases) = estimator.layers[ix].get_weights()\n                    estimator.layers[ix].set_weights([weight_initializer(shape=old_weights.shape), bias_initializer(shape=len(old_biases))])\n            estimator.fit(X_boot, y_boot, **fit_params)\n            pred = estimator.predict(X_test).reshape(1, -1)\n        else:\n            pred = estimator.fit(X_boot, y_boot, **fit_params).predict(X_test)\n        all_pred[i] = pred\n    if loss == '0-1_loss':\n        main_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=all_pred)\n        avg_expected_loss = np.apply_along_axis(lambda x: (x != y_test).mean(), axis=1, arr=all_pred).mean()\n        avg_bias = np.sum(main_predictions != y_test) / y_test.size\n        var = np.zeros(pred.shape)\n        for pred in all_pred:\n            var += (pred != main_predictions).astype(np.int_)\n        var /= num_rounds\n        avg_var = var.sum() / y_test.shape[0]\n    else:\n        avg_expected_loss = np.apply_along_axis(lambda x: ((x - y_test) ** 2).mean(), axis=1, arr=all_pred).mean()\n        main_predictions = np.mean(all_pred, axis=0)\n        avg_bias = np.sum((main_predictions - y_test) ** 2) / y_test.size\n        avg_var = np.sum((main_predictions - all_pred) ** 2) / all_pred.size\n    return (avg_expected_loss, avg_bias, avg_var)",
            "def bias_variance_decomp(estimator, X_train, y_train, X_test, y_test, loss='0-1_loss', num_rounds=200, random_seed=None, **fit_params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    estimator : object\\n        A classifier or regressor object or class implementing both a\\n        `fit` and `predict` method similar to the scikit-learn API.\\n\\n    X_train : array-like, shape=(num_examples, num_features)\\n        A training dataset for drawing the bootstrap samples to carry\\n        out the bias-variance decomposition.\\n\\n    y_train : array-like, shape=(num_examples)\\n        Targets (class labels, continuous values in case of regression)\\n        associated with the `X_train` examples.\\n\\n    X_test : array-like, shape=(num_examples, num_features)\\n        The test dataset for computing the average loss, bias,\\n        and variance.\\n\\n    y_test : array-like, shape=(num_examples)\\n        Targets (class labels, continuous values in case of regression)\\n        associated with the `X_test` examples.\\n\\n    loss : str (default='0-1_loss')\\n        Loss function for performing the bias-variance decomposition.\\n        Currently allowed values are '0-1_loss' and 'mse'.\\n\\n    num_rounds : int (default=200)\\n        Number of bootstrap rounds (sampling from the training set)\\n        for performing the bias-variance decomposition. Each bootstrap\\n        sample has the same size as the original training set.\\n\\n    random_seed : int (default=None)\\n        Random seed for the bootstrap sampling used for the\\n        bias-variance decomposition.\\n\\n    fit_params : additional parameters\\n        Additional parameters to be passed to the .fit() function of the\\n        estimator when it is fit to the bootstrap samples.\\n\\n    Returns\\n    ----------\\n    avg_expected_loss, avg_bias, avg_var : returns the average expected\\n        average bias, and average bias (all floats), where the average\\n        is computed over the data points in the test set.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/\\n\\n    \"\n    supported = ['0-1_loss', 'mse']\n    if loss not in supported:\n        raise NotImplementedError('loss must be one of the following: %s' % supported)\n    for ary in (X_train, y_train, X_test, y_test):\n        if hasattr(ary, 'loc'):\n            raise ValueError('The bias_variance_decomp does not support pandas DataFrames yet. Please check the inputs to X_train, y_train, X_test, y_test. If e.g., X_train is a pandas DataFrame, try passing it as NumPy array via X_train=X_train.values.')\n    rng = np.random.RandomState(random_seed)\n    if loss == '0-1_loss':\n        dtype = np.int64\n    elif loss == 'mse':\n        dtype = np.float64\n    all_pred = np.zeros((num_rounds, y_test.shape[0]), dtype=dtype)\n    for i in range(num_rounds):\n        (X_boot, y_boot) = _draw_bootstrap_sample(rng, X_train, y_train)\n        if estimator.__class__.__name__ in ['Sequential', 'Functional']:\n            for (ix, layer) in enumerate(estimator.layers):\n                if hasattr(estimator.layers[ix], 'kernel_initializer') and hasattr(estimator.layers[ix], 'bias_initializer'):\n                    weight_initializer = estimator.layers[ix].kernel_initializer\n                    bias_initializer = estimator.layers[ix].bias_initializer\n                    (old_weights, old_biases) = estimator.layers[ix].get_weights()\n                    estimator.layers[ix].set_weights([weight_initializer(shape=old_weights.shape), bias_initializer(shape=len(old_biases))])\n            estimator.fit(X_boot, y_boot, **fit_params)\n            pred = estimator.predict(X_test).reshape(1, -1)\n        else:\n            pred = estimator.fit(X_boot, y_boot, **fit_params).predict(X_test)\n        all_pred[i] = pred\n    if loss == '0-1_loss':\n        main_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=all_pred)\n        avg_expected_loss = np.apply_along_axis(lambda x: (x != y_test).mean(), axis=1, arr=all_pred).mean()\n        avg_bias = np.sum(main_predictions != y_test) / y_test.size\n        var = np.zeros(pred.shape)\n        for pred in all_pred:\n            var += (pred != main_predictions).astype(np.int_)\n        var /= num_rounds\n        avg_var = var.sum() / y_test.shape[0]\n    else:\n        avg_expected_loss = np.apply_along_axis(lambda x: ((x - y_test) ** 2).mean(), axis=1, arr=all_pred).mean()\n        main_predictions = np.mean(all_pred, axis=0)\n        avg_bias = np.sum((main_predictions - y_test) ** 2) / y_test.size\n        avg_var = np.sum((main_predictions - all_pred) ** 2) / all_pred.size\n    return (avg_expected_loss, avg_bias, avg_var)",
            "def bias_variance_decomp(estimator, X_train, y_train, X_test, y_test, loss='0-1_loss', num_rounds=200, random_seed=None, **fit_params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    estimator : object\\n        A classifier or regressor object or class implementing both a\\n        `fit` and `predict` method similar to the scikit-learn API.\\n\\n    X_train : array-like, shape=(num_examples, num_features)\\n        A training dataset for drawing the bootstrap samples to carry\\n        out the bias-variance decomposition.\\n\\n    y_train : array-like, shape=(num_examples)\\n        Targets (class labels, continuous values in case of regression)\\n        associated with the `X_train` examples.\\n\\n    X_test : array-like, shape=(num_examples, num_features)\\n        The test dataset for computing the average loss, bias,\\n        and variance.\\n\\n    y_test : array-like, shape=(num_examples)\\n        Targets (class labels, continuous values in case of regression)\\n        associated with the `X_test` examples.\\n\\n    loss : str (default='0-1_loss')\\n        Loss function for performing the bias-variance decomposition.\\n        Currently allowed values are '0-1_loss' and 'mse'.\\n\\n    num_rounds : int (default=200)\\n        Number of bootstrap rounds (sampling from the training set)\\n        for performing the bias-variance decomposition. Each bootstrap\\n        sample has the same size as the original training set.\\n\\n    random_seed : int (default=None)\\n        Random seed for the bootstrap sampling used for the\\n        bias-variance decomposition.\\n\\n    fit_params : additional parameters\\n        Additional parameters to be passed to the .fit() function of the\\n        estimator when it is fit to the bootstrap samples.\\n\\n    Returns\\n    ----------\\n    avg_expected_loss, avg_bias, avg_var : returns the average expected\\n        average bias, and average bias (all floats), where the average\\n        is computed over the data points in the test set.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/\\n\\n    \"\n    supported = ['0-1_loss', 'mse']\n    if loss not in supported:\n        raise NotImplementedError('loss must be one of the following: %s' % supported)\n    for ary in (X_train, y_train, X_test, y_test):\n        if hasattr(ary, 'loc'):\n            raise ValueError('The bias_variance_decomp does not support pandas DataFrames yet. Please check the inputs to X_train, y_train, X_test, y_test. If e.g., X_train is a pandas DataFrame, try passing it as NumPy array via X_train=X_train.values.')\n    rng = np.random.RandomState(random_seed)\n    if loss == '0-1_loss':\n        dtype = np.int64\n    elif loss == 'mse':\n        dtype = np.float64\n    all_pred = np.zeros((num_rounds, y_test.shape[0]), dtype=dtype)\n    for i in range(num_rounds):\n        (X_boot, y_boot) = _draw_bootstrap_sample(rng, X_train, y_train)\n        if estimator.__class__.__name__ in ['Sequential', 'Functional']:\n            for (ix, layer) in enumerate(estimator.layers):\n                if hasattr(estimator.layers[ix], 'kernel_initializer') and hasattr(estimator.layers[ix], 'bias_initializer'):\n                    weight_initializer = estimator.layers[ix].kernel_initializer\n                    bias_initializer = estimator.layers[ix].bias_initializer\n                    (old_weights, old_biases) = estimator.layers[ix].get_weights()\n                    estimator.layers[ix].set_weights([weight_initializer(shape=old_weights.shape), bias_initializer(shape=len(old_biases))])\n            estimator.fit(X_boot, y_boot, **fit_params)\n            pred = estimator.predict(X_test).reshape(1, -1)\n        else:\n            pred = estimator.fit(X_boot, y_boot, **fit_params).predict(X_test)\n        all_pred[i] = pred\n    if loss == '0-1_loss':\n        main_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=all_pred)\n        avg_expected_loss = np.apply_along_axis(lambda x: (x != y_test).mean(), axis=1, arr=all_pred).mean()\n        avg_bias = np.sum(main_predictions != y_test) / y_test.size\n        var = np.zeros(pred.shape)\n        for pred in all_pred:\n            var += (pred != main_predictions).astype(np.int_)\n        var /= num_rounds\n        avg_var = var.sum() / y_test.shape[0]\n    else:\n        avg_expected_loss = np.apply_along_axis(lambda x: ((x - y_test) ** 2).mean(), axis=1, arr=all_pred).mean()\n        main_predictions = np.mean(all_pred, axis=0)\n        avg_bias = np.sum((main_predictions - y_test) ** 2) / y_test.size\n        avg_var = np.sum((main_predictions - all_pred) ** 2) / all_pred.size\n    return (avg_expected_loss, avg_bias, avg_var)",
            "def bias_variance_decomp(estimator, X_train, y_train, X_test, y_test, loss='0-1_loss', num_rounds=200, random_seed=None, **fit_params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    estimator : object\\n        A classifier or regressor object or class implementing both a\\n        `fit` and `predict` method similar to the scikit-learn API.\\n\\n    X_train : array-like, shape=(num_examples, num_features)\\n        A training dataset for drawing the bootstrap samples to carry\\n        out the bias-variance decomposition.\\n\\n    y_train : array-like, shape=(num_examples)\\n        Targets (class labels, continuous values in case of regression)\\n        associated with the `X_train` examples.\\n\\n    X_test : array-like, shape=(num_examples, num_features)\\n        The test dataset for computing the average loss, bias,\\n        and variance.\\n\\n    y_test : array-like, shape=(num_examples)\\n        Targets (class labels, continuous values in case of regression)\\n        associated with the `X_test` examples.\\n\\n    loss : str (default='0-1_loss')\\n        Loss function for performing the bias-variance decomposition.\\n        Currently allowed values are '0-1_loss' and 'mse'.\\n\\n    num_rounds : int (default=200)\\n        Number of bootstrap rounds (sampling from the training set)\\n        for performing the bias-variance decomposition. Each bootstrap\\n        sample has the same size as the original training set.\\n\\n    random_seed : int (default=None)\\n        Random seed for the bootstrap sampling used for the\\n        bias-variance decomposition.\\n\\n    fit_params : additional parameters\\n        Additional parameters to be passed to the .fit() function of the\\n        estimator when it is fit to the bootstrap samples.\\n\\n    Returns\\n    ----------\\n    avg_expected_loss, avg_bias, avg_var : returns the average expected\\n        average bias, and average bias (all floats), where the average\\n        is computed over the data points in the test set.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/\\n\\n    \"\n    supported = ['0-1_loss', 'mse']\n    if loss not in supported:\n        raise NotImplementedError('loss must be one of the following: %s' % supported)\n    for ary in (X_train, y_train, X_test, y_test):\n        if hasattr(ary, 'loc'):\n            raise ValueError('The bias_variance_decomp does not support pandas DataFrames yet. Please check the inputs to X_train, y_train, X_test, y_test. If e.g., X_train is a pandas DataFrame, try passing it as NumPy array via X_train=X_train.values.')\n    rng = np.random.RandomState(random_seed)\n    if loss == '0-1_loss':\n        dtype = np.int64\n    elif loss == 'mse':\n        dtype = np.float64\n    all_pred = np.zeros((num_rounds, y_test.shape[0]), dtype=dtype)\n    for i in range(num_rounds):\n        (X_boot, y_boot) = _draw_bootstrap_sample(rng, X_train, y_train)\n        if estimator.__class__.__name__ in ['Sequential', 'Functional']:\n            for (ix, layer) in enumerate(estimator.layers):\n                if hasattr(estimator.layers[ix], 'kernel_initializer') and hasattr(estimator.layers[ix], 'bias_initializer'):\n                    weight_initializer = estimator.layers[ix].kernel_initializer\n                    bias_initializer = estimator.layers[ix].bias_initializer\n                    (old_weights, old_biases) = estimator.layers[ix].get_weights()\n                    estimator.layers[ix].set_weights([weight_initializer(shape=old_weights.shape), bias_initializer(shape=len(old_biases))])\n            estimator.fit(X_boot, y_boot, **fit_params)\n            pred = estimator.predict(X_test).reshape(1, -1)\n        else:\n            pred = estimator.fit(X_boot, y_boot, **fit_params).predict(X_test)\n        all_pred[i] = pred\n    if loss == '0-1_loss':\n        main_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=all_pred)\n        avg_expected_loss = np.apply_along_axis(lambda x: (x != y_test).mean(), axis=1, arr=all_pred).mean()\n        avg_bias = np.sum(main_predictions != y_test) / y_test.size\n        var = np.zeros(pred.shape)\n        for pred in all_pred:\n            var += (pred != main_predictions).astype(np.int_)\n        var /= num_rounds\n        avg_var = var.sum() / y_test.shape[0]\n    else:\n        avg_expected_loss = np.apply_along_axis(lambda x: ((x - y_test) ** 2).mean(), axis=1, arr=all_pred).mean()\n        main_predictions = np.mean(all_pred, axis=0)\n        avg_bias = np.sum((main_predictions - y_test) ** 2) / y_test.size\n        avg_var = np.sum((main_predictions - all_pred) ** 2) / all_pred.size\n    return (avg_expected_loss, avg_bias, avg_var)",
            "def bias_variance_decomp(estimator, X_train, y_train, X_test, y_test, loss='0-1_loss', num_rounds=200, random_seed=None, **fit_params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    estimator : object\\n        A classifier or regressor object or class implementing both a\\n        `fit` and `predict` method similar to the scikit-learn API.\\n\\n    X_train : array-like, shape=(num_examples, num_features)\\n        A training dataset for drawing the bootstrap samples to carry\\n        out the bias-variance decomposition.\\n\\n    y_train : array-like, shape=(num_examples)\\n        Targets (class labels, continuous values in case of regression)\\n        associated with the `X_train` examples.\\n\\n    X_test : array-like, shape=(num_examples, num_features)\\n        The test dataset for computing the average loss, bias,\\n        and variance.\\n\\n    y_test : array-like, shape=(num_examples)\\n        Targets (class labels, continuous values in case of regression)\\n        associated with the `X_test` examples.\\n\\n    loss : str (default='0-1_loss')\\n        Loss function for performing the bias-variance decomposition.\\n        Currently allowed values are '0-1_loss' and 'mse'.\\n\\n    num_rounds : int (default=200)\\n        Number of bootstrap rounds (sampling from the training set)\\n        for performing the bias-variance decomposition. Each bootstrap\\n        sample has the same size as the original training set.\\n\\n    random_seed : int (default=None)\\n        Random seed for the bootstrap sampling used for the\\n        bias-variance decomposition.\\n\\n    fit_params : additional parameters\\n        Additional parameters to be passed to the .fit() function of the\\n        estimator when it is fit to the bootstrap samples.\\n\\n    Returns\\n    ----------\\n    avg_expected_loss, avg_bias, avg_var : returns the average expected\\n        average bias, and average bias (all floats), where the average\\n        is computed over the data points in the test set.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/\\n\\n    \"\n    supported = ['0-1_loss', 'mse']\n    if loss not in supported:\n        raise NotImplementedError('loss must be one of the following: %s' % supported)\n    for ary in (X_train, y_train, X_test, y_test):\n        if hasattr(ary, 'loc'):\n            raise ValueError('The bias_variance_decomp does not support pandas DataFrames yet. Please check the inputs to X_train, y_train, X_test, y_test. If e.g., X_train is a pandas DataFrame, try passing it as NumPy array via X_train=X_train.values.')\n    rng = np.random.RandomState(random_seed)\n    if loss == '0-1_loss':\n        dtype = np.int64\n    elif loss == 'mse':\n        dtype = np.float64\n    all_pred = np.zeros((num_rounds, y_test.shape[0]), dtype=dtype)\n    for i in range(num_rounds):\n        (X_boot, y_boot) = _draw_bootstrap_sample(rng, X_train, y_train)\n        if estimator.__class__.__name__ in ['Sequential', 'Functional']:\n            for (ix, layer) in enumerate(estimator.layers):\n                if hasattr(estimator.layers[ix], 'kernel_initializer') and hasattr(estimator.layers[ix], 'bias_initializer'):\n                    weight_initializer = estimator.layers[ix].kernel_initializer\n                    bias_initializer = estimator.layers[ix].bias_initializer\n                    (old_weights, old_biases) = estimator.layers[ix].get_weights()\n                    estimator.layers[ix].set_weights([weight_initializer(shape=old_weights.shape), bias_initializer(shape=len(old_biases))])\n            estimator.fit(X_boot, y_boot, **fit_params)\n            pred = estimator.predict(X_test).reshape(1, -1)\n        else:\n            pred = estimator.fit(X_boot, y_boot, **fit_params).predict(X_test)\n        all_pred[i] = pred\n    if loss == '0-1_loss':\n        main_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=all_pred)\n        avg_expected_loss = np.apply_along_axis(lambda x: (x != y_test).mean(), axis=1, arr=all_pred).mean()\n        avg_bias = np.sum(main_predictions != y_test) / y_test.size\n        var = np.zeros(pred.shape)\n        for pred in all_pred:\n            var += (pred != main_predictions).astype(np.int_)\n        var /= num_rounds\n        avg_var = var.sum() / y_test.shape[0]\n    else:\n        avg_expected_loss = np.apply_along_axis(lambda x: ((x - y_test) ** 2).mean(), axis=1, arr=all_pred).mean()\n        main_predictions = np.mean(all_pred, axis=0)\n        avg_bias = np.sum((main_predictions - y_test) ** 2) / y_test.size\n        avg_var = np.sum((main_predictions - all_pred) ** 2) / all_pred.size\n    return (avg_expected_loss, avg_bias, avg_var)"
        ]
    }
]