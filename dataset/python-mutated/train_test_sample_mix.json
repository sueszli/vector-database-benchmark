[
    {
        "func_name": "__init__",
        "original": "def __init__(self, ignore_case: bool=True, remove_punctuation: bool=True, normalize_unicode: bool=True, remove_stopwords: bool=True, ignore_whitespace: bool=False, n_samples: int=10000000, n_to_show: int=10, random_state: int=42, max_text_length_for_display: int=30, **kwargs):\n    super().__init__(**kwargs)\n    self.ignore_case = ignore_case\n    self.remove_punctuation = remove_punctuation\n    self.normalize_unicode = normalize_unicode\n    self.remove_stopwords = remove_stopwords\n    self.ignore_whitespace = ignore_whitespace\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.max_text_length_for_display = max_text_length_for_display",
        "mutated": [
            "def __init__(self, ignore_case: bool=True, remove_punctuation: bool=True, normalize_unicode: bool=True, remove_stopwords: bool=True, ignore_whitespace: bool=False, n_samples: int=10000000, n_to_show: int=10, random_state: int=42, max_text_length_for_display: int=30, **kwargs):\n    if False:\n        i = 10\n    super().__init__(**kwargs)\n    self.ignore_case = ignore_case\n    self.remove_punctuation = remove_punctuation\n    self.normalize_unicode = normalize_unicode\n    self.remove_stopwords = remove_stopwords\n    self.ignore_whitespace = ignore_whitespace\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.max_text_length_for_display = max_text_length_for_display",
            "def __init__(self, ignore_case: bool=True, remove_punctuation: bool=True, normalize_unicode: bool=True, remove_stopwords: bool=True, ignore_whitespace: bool=False, n_samples: int=10000000, n_to_show: int=10, random_state: int=42, max_text_length_for_display: int=30, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(**kwargs)\n    self.ignore_case = ignore_case\n    self.remove_punctuation = remove_punctuation\n    self.normalize_unicode = normalize_unicode\n    self.remove_stopwords = remove_stopwords\n    self.ignore_whitespace = ignore_whitespace\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.max_text_length_for_display = max_text_length_for_display",
            "def __init__(self, ignore_case: bool=True, remove_punctuation: bool=True, normalize_unicode: bool=True, remove_stopwords: bool=True, ignore_whitespace: bool=False, n_samples: int=10000000, n_to_show: int=10, random_state: int=42, max_text_length_for_display: int=30, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(**kwargs)\n    self.ignore_case = ignore_case\n    self.remove_punctuation = remove_punctuation\n    self.normalize_unicode = normalize_unicode\n    self.remove_stopwords = remove_stopwords\n    self.ignore_whitespace = ignore_whitespace\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.max_text_length_for_display = max_text_length_for_display",
            "def __init__(self, ignore_case: bool=True, remove_punctuation: bool=True, normalize_unicode: bool=True, remove_stopwords: bool=True, ignore_whitespace: bool=False, n_samples: int=10000000, n_to_show: int=10, random_state: int=42, max_text_length_for_display: int=30, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(**kwargs)\n    self.ignore_case = ignore_case\n    self.remove_punctuation = remove_punctuation\n    self.normalize_unicode = normalize_unicode\n    self.remove_stopwords = remove_stopwords\n    self.ignore_whitespace = ignore_whitespace\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.max_text_length_for_display = max_text_length_for_display",
            "def __init__(self, ignore_case: bool=True, remove_punctuation: bool=True, normalize_unicode: bool=True, remove_stopwords: bool=True, ignore_whitespace: bool=False, n_samples: int=10000000, n_to_show: int=10, random_state: int=42, max_text_length_for_display: int=30, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(**kwargs)\n    self.ignore_case = ignore_case\n    self.remove_punctuation = remove_punctuation\n    self.normalize_unicode = normalize_unicode\n    self.remove_stopwords = remove_stopwords\n    self.ignore_whitespace = ignore_whitespace\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.max_text_length_for_display = max_text_length_for_display"
        ]
    },
    {
        "func_name": "_text_normalization_kwargs",
        "original": "@property\ndef _text_normalization_kwargs(self):\n    return {'ignore_case': self.ignore_case, 'ignore_whitespace': self.ignore_whitespace, 'normalize_uni': self.normalize_unicode, 'remove_punct': self.remove_punctuation, 'remove_stops': self.remove_stopwords}",
        "mutated": [
            "@property\ndef _text_normalization_kwargs(self):\n    if False:\n        i = 10\n    return {'ignore_case': self.ignore_case, 'ignore_whitespace': self.ignore_whitespace, 'normalize_uni': self.normalize_unicode, 'remove_punct': self.remove_punctuation, 'remove_stops': self.remove_stopwords}",
            "@property\ndef _text_normalization_kwargs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'ignore_case': self.ignore_case, 'ignore_whitespace': self.ignore_whitespace, 'normalize_uni': self.normalize_unicode, 'remove_punct': self.remove_punctuation, 'remove_stops': self.remove_stopwords}",
            "@property\ndef _text_normalization_kwargs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'ignore_case': self.ignore_case, 'ignore_whitespace': self.ignore_whitespace, 'normalize_uni': self.normalize_unicode, 'remove_punct': self.remove_punctuation, 'remove_stops': self.remove_stopwords}",
            "@property\ndef _text_normalization_kwargs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'ignore_case': self.ignore_case, 'ignore_whitespace': self.ignore_whitespace, 'normalize_uni': self.normalize_unicode, 'remove_punct': self.remove_punctuation, 'remove_stops': self.remove_stopwords}",
            "@property\ndef _text_normalization_kwargs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'ignore_case': self.ignore_case, 'ignore_whitespace': self.ignore_whitespace, 'normalize_uni': self.normalize_unicode, 'remove_punct': self.remove_punctuation, 'remove_stops': self.remove_stopwords}"
        ]
    },
    {
        "func_name": "_truncate_text",
        "original": "def _truncate_text(self, x: str) -> str:\n    return truncate_string(x, self.max_text_length_for_display)",
        "mutated": [
            "def _truncate_text(self, x: str) -> str:\n    if False:\n        i = 10\n    return truncate_string(x, self.max_text_length_for_display)",
            "def _truncate_text(self, x: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return truncate_string(x, self.max_text_length_for_display)",
            "def _truncate_text(self, x: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return truncate_string(x, self.max_text_length_for_display)",
            "def _truncate_text(self, x: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return truncate_string(x, self.max_text_length_for_display)",
            "def _truncate_text(self, x: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return truncate_string(x, self.max_text_length_for_display)"
        ]
    },
    {
        "func_name": "_get_duplicate_indices",
        "original": "def _get_duplicate_indices(self, train: TextData, test: TextData, train_samples: t.Sequence[str], test_samples: t.Sequence[str]):\n    normalization_kwargs = self._text_normalization_kwargs\n    train_sample_hashes = hash_samples(normalize_samples(train_samples, **normalization_kwargs))\n    test_sample_hashes = hash_samples(normalize_samples(test_samples, **normalization_kwargs))\n    train_df = pd.DataFrame({'hash': train_sample_hashes, 'Text': train_samples, 'Dataset': ['train' for _ in range(len(train_samples))], 'Sample ID': train.get_original_text_indexes()})\n    test_df = pd.DataFrame({'hash': test_sample_hashes, 'Text': test_samples, 'Dataset': ['test' for _ in range(len(test_samples))], 'Sample ID': test.get_original_text_indexes()})\n    hash_intersection = set(train_sample_hashes).intersection(set(test_sample_hashes))\n    df = pd.concat([test_df, train_df])\n    return (df['hash'].isin(hash_intersection), df)",
        "mutated": [
            "def _get_duplicate_indices(self, train: TextData, test: TextData, train_samples: t.Sequence[str], test_samples: t.Sequence[str]):\n    if False:\n        i = 10\n    normalization_kwargs = self._text_normalization_kwargs\n    train_sample_hashes = hash_samples(normalize_samples(train_samples, **normalization_kwargs))\n    test_sample_hashes = hash_samples(normalize_samples(test_samples, **normalization_kwargs))\n    train_df = pd.DataFrame({'hash': train_sample_hashes, 'Text': train_samples, 'Dataset': ['train' for _ in range(len(train_samples))], 'Sample ID': train.get_original_text_indexes()})\n    test_df = pd.DataFrame({'hash': test_sample_hashes, 'Text': test_samples, 'Dataset': ['test' for _ in range(len(test_samples))], 'Sample ID': test.get_original_text_indexes()})\n    hash_intersection = set(train_sample_hashes).intersection(set(test_sample_hashes))\n    df = pd.concat([test_df, train_df])\n    return (df['hash'].isin(hash_intersection), df)",
            "def _get_duplicate_indices(self, train: TextData, test: TextData, train_samples: t.Sequence[str], test_samples: t.Sequence[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    normalization_kwargs = self._text_normalization_kwargs\n    train_sample_hashes = hash_samples(normalize_samples(train_samples, **normalization_kwargs))\n    test_sample_hashes = hash_samples(normalize_samples(test_samples, **normalization_kwargs))\n    train_df = pd.DataFrame({'hash': train_sample_hashes, 'Text': train_samples, 'Dataset': ['train' for _ in range(len(train_samples))], 'Sample ID': train.get_original_text_indexes()})\n    test_df = pd.DataFrame({'hash': test_sample_hashes, 'Text': test_samples, 'Dataset': ['test' for _ in range(len(test_samples))], 'Sample ID': test.get_original_text_indexes()})\n    hash_intersection = set(train_sample_hashes).intersection(set(test_sample_hashes))\n    df = pd.concat([test_df, train_df])\n    return (df['hash'].isin(hash_intersection), df)",
            "def _get_duplicate_indices(self, train: TextData, test: TextData, train_samples: t.Sequence[str], test_samples: t.Sequence[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    normalization_kwargs = self._text_normalization_kwargs\n    train_sample_hashes = hash_samples(normalize_samples(train_samples, **normalization_kwargs))\n    test_sample_hashes = hash_samples(normalize_samples(test_samples, **normalization_kwargs))\n    train_df = pd.DataFrame({'hash': train_sample_hashes, 'Text': train_samples, 'Dataset': ['train' for _ in range(len(train_samples))], 'Sample ID': train.get_original_text_indexes()})\n    test_df = pd.DataFrame({'hash': test_sample_hashes, 'Text': test_samples, 'Dataset': ['test' for _ in range(len(test_samples))], 'Sample ID': test.get_original_text_indexes()})\n    hash_intersection = set(train_sample_hashes).intersection(set(test_sample_hashes))\n    df = pd.concat([test_df, train_df])\n    return (df['hash'].isin(hash_intersection), df)",
            "def _get_duplicate_indices(self, train: TextData, test: TextData, train_samples: t.Sequence[str], test_samples: t.Sequence[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    normalization_kwargs = self._text_normalization_kwargs\n    train_sample_hashes = hash_samples(normalize_samples(train_samples, **normalization_kwargs))\n    test_sample_hashes = hash_samples(normalize_samples(test_samples, **normalization_kwargs))\n    train_df = pd.DataFrame({'hash': train_sample_hashes, 'Text': train_samples, 'Dataset': ['train' for _ in range(len(train_samples))], 'Sample ID': train.get_original_text_indexes()})\n    test_df = pd.DataFrame({'hash': test_sample_hashes, 'Text': test_samples, 'Dataset': ['test' for _ in range(len(test_samples))], 'Sample ID': test.get_original_text_indexes()})\n    hash_intersection = set(train_sample_hashes).intersection(set(test_sample_hashes))\n    df = pd.concat([test_df, train_df])\n    return (df['hash'].isin(hash_intersection), df)",
            "def _get_duplicate_indices(self, train: TextData, test: TextData, train_samples: t.Sequence[str], test_samples: t.Sequence[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    normalization_kwargs = self._text_normalization_kwargs\n    train_sample_hashes = hash_samples(normalize_samples(train_samples, **normalization_kwargs))\n    test_sample_hashes = hash_samples(normalize_samples(test_samples, **normalization_kwargs))\n    train_df = pd.DataFrame({'hash': train_sample_hashes, 'Text': train_samples, 'Dataset': ['train' for _ in range(len(train_samples))], 'Sample ID': train.get_original_text_indexes()})\n    test_df = pd.DataFrame({'hash': test_sample_hashes, 'Text': test_samples, 'Dataset': ['test' for _ in range(len(test_samples))], 'Sample ID': test.get_original_text_indexes()})\n    hash_intersection = set(train_sample_hashes).intersection(set(test_sample_hashes))\n    df = pd.concat([test_df, train_df])\n    return (df['hash'].isin(hash_intersection), df)"
        ]
    },
    {
        "func_name": "run_logic",
        "original": "def run_logic(self, context: Context) -> CheckResult:\n    \"\"\"Run check.\"\"\"\n    train = context.train.sample(self.n_samples, random_state=self.random_state)\n    test = context.test.sample(self.n_samples, random_state=self.random_state)\n    train = t.cast(TextData, train)\n    test = t.cast(TextData, test)\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    n_of_test_samples = len(test_samples)\n    if len(train_samples) == 0:\n        raise DeepchecksValueError('Train dataset cannot be empty')\n    if len(test_samples) == 0:\n        raise DeepchecksValueError('Test dataset cannot be empty')\n    train_truncated = [cut_string(x) for x in train_samples]\n    test_truncated = [cut_string(x) for x in test_samples]\n    duplicate_bool_df = self._get_duplicate_indices(train, test, train_truncated, test_truncated)[0]\n    train_indices_reinspect = duplicate_bool_df.iloc[len(test_samples):]\n    train_indices_reinspect = np.where(train_indices_reinspect.values)[0]\n    test_indices_reinspect = duplicate_bool_df.iloc[:len(test_samples)]\n    test_indices_reinspect = np.where(test_indices_reinspect.values)[0]\n    train = train.copy(train_indices_reinspect.tolist())\n    test = test.copy(test_indices_reinspect.tolist())\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    if len(train_samples) == 0 or len(test_samples) == 0:\n        result_value = {'ratio': 0, 'duplicates': pd.DataFrame(index=pd.MultiIndex(levels=[[], [], []], codes=[[], [], []], names=['Duplicate', 'Dataset', 'Sample ID']), columns=['Text'])}\n        return CheckResult(value=result_value)\n    (bool_df, df) = self._get_duplicate_indices(train, test, train_samples, test_samples)\n    df = df[bool_df]\n    n_of_test_duplicates = df[df['Dataset'] == 'test']['Text'].count()\n    duplicates_ratio = n_of_test_duplicates / n_of_test_samples\n    result_df = df.rename(columns={'hash': 'Duplicate'})\n    duplicates_enumeration = to_ordional_enumeration(result_df['Duplicate'].to_list())\n    result_df['Duplicate'] = result_df['Duplicate'].apply(lambda x: duplicates_enumeration[x])\n    result_df = result_df.set_index(['Duplicate', 'Dataset', 'Sample ID'])\n    result_df = result_df.sort_index()\n    result_value = {'ratio': duplicates_ratio, 'duplicates': result_df}\n    if context.with_display is False or duplicates_ratio == 0:\n        return CheckResult(value=result_value)\n    train_grouped = df[df['Dataset'] == 'train'].groupby(['hash'], dropna=False)\n    train_instances = train_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    test_grouped = df[df['Dataset'] == 'test'].groupby(['hash'], dropna=False)\n    test_instances = test_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    counted_test_duplicates = test_grouped.size()\n    first_sample_in_group = test_grouped['Text'].first()\n    display_table = pd.DataFrame({'Train Sample IDs': train_instances, 'Test Sample IDs': test_instances, 'Test Text Sample': first_sample_in_group.apply(self._truncate_text), 'Number of Test Duplicates': counted_test_duplicates})\n    display_table = display_table.iloc[:self.n_to_show]\n    display_table = display_table.reset_index(drop=True).set_index(['Train Sample IDs', 'Test Sample IDs'])\n    message = f'{format_percent(duplicates_ratio)} ({n_of_test_duplicates} / {n_of_test_samples}) of test data samples also appear in train data'\n    return CheckResult(value=result_value, display=[message, display_table])",
        "mutated": [
            "def run_logic(self, context: Context) -> CheckResult:\n    if False:\n        i = 10\n    'Run check.'\n    train = context.train.sample(self.n_samples, random_state=self.random_state)\n    test = context.test.sample(self.n_samples, random_state=self.random_state)\n    train = t.cast(TextData, train)\n    test = t.cast(TextData, test)\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    n_of_test_samples = len(test_samples)\n    if len(train_samples) == 0:\n        raise DeepchecksValueError('Train dataset cannot be empty')\n    if len(test_samples) == 0:\n        raise DeepchecksValueError('Test dataset cannot be empty')\n    train_truncated = [cut_string(x) for x in train_samples]\n    test_truncated = [cut_string(x) for x in test_samples]\n    duplicate_bool_df = self._get_duplicate_indices(train, test, train_truncated, test_truncated)[0]\n    train_indices_reinspect = duplicate_bool_df.iloc[len(test_samples):]\n    train_indices_reinspect = np.where(train_indices_reinspect.values)[0]\n    test_indices_reinspect = duplicate_bool_df.iloc[:len(test_samples)]\n    test_indices_reinspect = np.where(test_indices_reinspect.values)[0]\n    train = train.copy(train_indices_reinspect.tolist())\n    test = test.copy(test_indices_reinspect.tolist())\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    if len(train_samples) == 0 or len(test_samples) == 0:\n        result_value = {'ratio': 0, 'duplicates': pd.DataFrame(index=pd.MultiIndex(levels=[[], [], []], codes=[[], [], []], names=['Duplicate', 'Dataset', 'Sample ID']), columns=['Text'])}\n        return CheckResult(value=result_value)\n    (bool_df, df) = self._get_duplicate_indices(train, test, train_samples, test_samples)\n    df = df[bool_df]\n    n_of_test_duplicates = df[df['Dataset'] == 'test']['Text'].count()\n    duplicates_ratio = n_of_test_duplicates / n_of_test_samples\n    result_df = df.rename(columns={'hash': 'Duplicate'})\n    duplicates_enumeration = to_ordional_enumeration(result_df['Duplicate'].to_list())\n    result_df['Duplicate'] = result_df['Duplicate'].apply(lambda x: duplicates_enumeration[x])\n    result_df = result_df.set_index(['Duplicate', 'Dataset', 'Sample ID'])\n    result_df = result_df.sort_index()\n    result_value = {'ratio': duplicates_ratio, 'duplicates': result_df}\n    if context.with_display is False or duplicates_ratio == 0:\n        return CheckResult(value=result_value)\n    train_grouped = df[df['Dataset'] == 'train'].groupby(['hash'], dropna=False)\n    train_instances = train_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    test_grouped = df[df['Dataset'] == 'test'].groupby(['hash'], dropna=False)\n    test_instances = test_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    counted_test_duplicates = test_grouped.size()\n    first_sample_in_group = test_grouped['Text'].first()\n    display_table = pd.DataFrame({'Train Sample IDs': train_instances, 'Test Sample IDs': test_instances, 'Test Text Sample': first_sample_in_group.apply(self._truncate_text), 'Number of Test Duplicates': counted_test_duplicates})\n    display_table = display_table.iloc[:self.n_to_show]\n    display_table = display_table.reset_index(drop=True).set_index(['Train Sample IDs', 'Test Sample IDs'])\n    message = f'{format_percent(duplicates_ratio)} ({n_of_test_duplicates} / {n_of_test_samples}) of test data samples also appear in train data'\n    return CheckResult(value=result_value, display=[message, display_table])",
            "def run_logic(self, context: Context) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Run check.'\n    train = context.train.sample(self.n_samples, random_state=self.random_state)\n    test = context.test.sample(self.n_samples, random_state=self.random_state)\n    train = t.cast(TextData, train)\n    test = t.cast(TextData, test)\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    n_of_test_samples = len(test_samples)\n    if len(train_samples) == 0:\n        raise DeepchecksValueError('Train dataset cannot be empty')\n    if len(test_samples) == 0:\n        raise DeepchecksValueError('Test dataset cannot be empty')\n    train_truncated = [cut_string(x) for x in train_samples]\n    test_truncated = [cut_string(x) for x in test_samples]\n    duplicate_bool_df = self._get_duplicate_indices(train, test, train_truncated, test_truncated)[0]\n    train_indices_reinspect = duplicate_bool_df.iloc[len(test_samples):]\n    train_indices_reinspect = np.where(train_indices_reinspect.values)[0]\n    test_indices_reinspect = duplicate_bool_df.iloc[:len(test_samples)]\n    test_indices_reinspect = np.where(test_indices_reinspect.values)[0]\n    train = train.copy(train_indices_reinspect.tolist())\n    test = test.copy(test_indices_reinspect.tolist())\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    if len(train_samples) == 0 or len(test_samples) == 0:\n        result_value = {'ratio': 0, 'duplicates': pd.DataFrame(index=pd.MultiIndex(levels=[[], [], []], codes=[[], [], []], names=['Duplicate', 'Dataset', 'Sample ID']), columns=['Text'])}\n        return CheckResult(value=result_value)\n    (bool_df, df) = self._get_duplicate_indices(train, test, train_samples, test_samples)\n    df = df[bool_df]\n    n_of_test_duplicates = df[df['Dataset'] == 'test']['Text'].count()\n    duplicates_ratio = n_of_test_duplicates / n_of_test_samples\n    result_df = df.rename(columns={'hash': 'Duplicate'})\n    duplicates_enumeration = to_ordional_enumeration(result_df['Duplicate'].to_list())\n    result_df['Duplicate'] = result_df['Duplicate'].apply(lambda x: duplicates_enumeration[x])\n    result_df = result_df.set_index(['Duplicate', 'Dataset', 'Sample ID'])\n    result_df = result_df.sort_index()\n    result_value = {'ratio': duplicates_ratio, 'duplicates': result_df}\n    if context.with_display is False or duplicates_ratio == 0:\n        return CheckResult(value=result_value)\n    train_grouped = df[df['Dataset'] == 'train'].groupby(['hash'], dropna=False)\n    train_instances = train_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    test_grouped = df[df['Dataset'] == 'test'].groupby(['hash'], dropna=False)\n    test_instances = test_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    counted_test_duplicates = test_grouped.size()\n    first_sample_in_group = test_grouped['Text'].first()\n    display_table = pd.DataFrame({'Train Sample IDs': train_instances, 'Test Sample IDs': test_instances, 'Test Text Sample': first_sample_in_group.apply(self._truncate_text), 'Number of Test Duplicates': counted_test_duplicates})\n    display_table = display_table.iloc[:self.n_to_show]\n    display_table = display_table.reset_index(drop=True).set_index(['Train Sample IDs', 'Test Sample IDs'])\n    message = f'{format_percent(duplicates_ratio)} ({n_of_test_duplicates} / {n_of_test_samples}) of test data samples also appear in train data'\n    return CheckResult(value=result_value, display=[message, display_table])",
            "def run_logic(self, context: Context) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Run check.'\n    train = context.train.sample(self.n_samples, random_state=self.random_state)\n    test = context.test.sample(self.n_samples, random_state=self.random_state)\n    train = t.cast(TextData, train)\n    test = t.cast(TextData, test)\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    n_of_test_samples = len(test_samples)\n    if len(train_samples) == 0:\n        raise DeepchecksValueError('Train dataset cannot be empty')\n    if len(test_samples) == 0:\n        raise DeepchecksValueError('Test dataset cannot be empty')\n    train_truncated = [cut_string(x) for x in train_samples]\n    test_truncated = [cut_string(x) for x in test_samples]\n    duplicate_bool_df = self._get_duplicate_indices(train, test, train_truncated, test_truncated)[0]\n    train_indices_reinspect = duplicate_bool_df.iloc[len(test_samples):]\n    train_indices_reinspect = np.where(train_indices_reinspect.values)[0]\n    test_indices_reinspect = duplicate_bool_df.iloc[:len(test_samples)]\n    test_indices_reinspect = np.where(test_indices_reinspect.values)[0]\n    train = train.copy(train_indices_reinspect.tolist())\n    test = test.copy(test_indices_reinspect.tolist())\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    if len(train_samples) == 0 or len(test_samples) == 0:\n        result_value = {'ratio': 0, 'duplicates': pd.DataFrame(index=pd.MultiIndex(levels=[[], [], []], codes=[[], [], []], names=['Duplicate', 'Dataset', 'Sample ID']), columns=['Text'])}\n        return CheckResult(value=result_value)\n    (bool_df, df) = self._get_duplicate_indices(train, test, train_samples, test_samples)\n    df = df[bool_df]\n    n_of_test_duplicates = df[df['Dataset'] == 'test']['Text'].count()\n    duplicates_ratio = n_of_test_duplicates / n_of_test_samples\n    result_df = df.rename(columns={'hash': 'Duplicate'})\n    duplicates_enumeration = to_ordional_enumeration(result_df['Duplicate'].to_list())\n    result_df['Duplicate'] = result_df['Duplicate'].apply(lambda x: duplicates_enumeration[x])\n    result_df = result_df.set_index(['Duplicate', 'Dataset', 'Sample ID'])\n    result_df = result_df.sort_index()\n    result_value = {'ratio': duplicates_ratio, 'duplicates': result_df}\n    if context.with_display is False or duplicates_ratio == 0:\n        return CheckResult(value=result_value)\n    train_grouped = df[df['Dataset'] == 'train'].groupby(['hash'], dropna=False)\n    train_instances = train_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    test_grouped = df[df['Dataset'] == 'test'].groupby(['hash'], dropna=False)\n    test_instances = test_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    counted_test_duplicates = test_grouped.size()\n    first_sample_in_group = test_grouped['Text'].first()\n    display_table = pd.DataFrame({'Train Sample IDs': train_instances, 'Test Sample IDs': test_instances, 'Test Text Sample': first_sample_in_group.apply(self._truncate_text), 'Number of Test Duplicates': counted_test_duplicates})\n    display_table = display_table.iloc[:self.n_to_show]\n    display_table = display_table.reset_index(drop=True).set_index(['Train Sample IDs', 'Test Sample IDs'])\n    message = f'{format_percent(duplicates_ratio)} ({n_of_test_duplicates} / {n_of_test_samples}) of test data samples also appear in train data'\n    return CheckResult(value=result_value, display=[message, display_table])",
            "def run_logic(self, context: Context) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Run check.'\n    train = context.train.sample(self.n_samples, random_state=self.random_state)\n    test = context.test.sample(self.n_samples, random_state=self.random_state)\n    train = t.cast(TextData, train)\n    test = t.cast(TextData, test)\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    n_of_test_samples = len(test_samples)\n    if len(train_samples) == 0:\n        raise DeepchecksValueError('Train dataset cannot be empty')\n    if len(test_samples) == 0:\n        raise DeepchecksValueError('Test dataset cannot be empty')\n    train_truncated = [cut_string(x) for x in train_samples]\n    test_truncated = [cut_string(x) for x in test_samples]\n    duplicate_bool_df = self._get_duplicate_indices(train, test, train_truncated, test_truncated)[0]\n    train_indices_reinspect = duplicate_bool_df.iloc[len(test_samples):]\n    train_indices_reinspect = np.where(train_indices_reinspect.values)[0]\n    test_indices_reinspect = duplicate_bool_df.iloc[:len(test_samples)]\n    test_indices_reinspect = np.where(test_indices_reinspect.values)[0]\n    train = train.copy(train_indices_reinspect.tolist())\n    test = test.copy(test_indices_reinspect.tolist())\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    if len(train_samples) == 0 or len(test_samples) == 0:\n        result_value = {'ratio': 0, 'duplicates': pd.DataFrame(index=pd.MultiIndex(levels=[[], [], []], codes=[[], [], []], names=['Duplicate', 'Dataset', 'Sample ID']), columns=['Text'])}\n        return CheckResult(value=result_value)\n    (bool_df, df) = self._get_duplicate_indices(train, test, train_samples, test_samples)\n    df = df[bool_df]\n    n_of_test_duplicates = df[df['Dataset'] == 'test']['Text'].count()\n    duplicates_ratio = n_of_test_duplicates / n_of_test_samples\n    result_df = df.rename(columns={'hash': 'Duplicate'})\n    duplicates_enumeration = to_ordional_enumeration(result_df['Duplicate'].to_list())\n    result_df['Duplicate'] = result_df['Duplicate'].apply(lambda x: duplicates_enumeration[x])\n    result_df = result_df.set_index(['Duplicate', 'Dataset', 'Sample ID'])\n    result_df = result_df.sort_index()\n    result_value = {'ratio': duplicates_ratio, 'duplicates': result_df}\n    if context.with_display is False or duplicates_ratio == 0:\n        return CheckResult(value=result_value)\n    train_grouped = df[df['Dataset'] == 'train'].groupby(['hash'], dropna=False)\n    train_instances = train_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    test_grouped = df[df['Dataset'] == 'test'].groupby(['hash'], dropna=False)\n    test_instances = test_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    counted_test_duplicates = test_grouped.size()\n    first_sample_in_group = test_grouped['Text'].first()\n    display_table = pd.DataFrame({'Train Sample IDs': train_instances, 'Test Sample IDs': test_instances, 'Test Text Sample': first_sample_in_group.apply(self._truncate_text), 'Number of Test Duplicates': counted_test_duplicates})\n    display_table = display_table.iloc[:self.n_to_show]\n    display_table = display_table.reset_index(drop=True).set_index(['Train Sample IDs', 'Test Sample IDs'])\n    message = f'{format_percent(duplicates_ratio)} ({n_of_test_duplicates} / {n_of_test_samples}) of test data samples also appear in train data'\n    return CheckResult(value=result_value, display=[message, display_table])",
            "def run_logic(self, context: Context) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Run check.'\n    train = context.train.sample(self.n_samples, random_state=self.random_state)\n    test = context.test.sample(self.n_samples, random_state=self.random_state)\n    train = t.cast(TextData, train)\n    test = t.cast(TextData, test)\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    n_of_test_samples = len(test_samples)\n    if len(train_samples) == 0:\n        raise DeepchecksValueError('Train dataset cannot be empty')\n    if len(test_samples) == 0:\n        raise DeepchecksValueError('Test dataset cannot be empty')\n    train_truncated = [cut_string(x) for x in train_samples]\n    test_truncated = [cut_string(x) for x in test_samples]\n    duplicate_bool_df = self._get_duplicate_indices(train, test, train_truncated, test_truncated)[0]\n    train_indices_reinspect = duplicate_bool_df.iloc[len(test_samples):]\n    train_indices_reinspect = np.where(train_indices_reinspect.values)[0]\n    test_indices_reinspect = duplicate_bool_df.iloc[:len(test_samples)]\n    test_indices_reinspect = np.where(test_indices_reinspect.values)[0]\n    train = train.copy(train_indices_reinspect.tolist())\n    test = test.copy(test_indices_reinspect.tolist())\n    train_samples = t.cast(t.Sequence[str], train.text)\n    test_samples = t.cast(t.Sequence[str], test.text)\n    if len(train_samples) == 0 or len(test_samples) == 0:\n        result_value = {'ratio': 0, 'duplicates': pd.DataFrame(index=pd.MultiIndex(levels=[[], [], []], codes=[[], [], []], names=['Duplicate', 'Dataset', 'Sample ID']), columns=['Text'])}\n        return CheckResult(value=result_value)\n    (bool_df, df) = self._get_duplicate_indices(train, test, train_samples, test_samples)\n    df = df[bool_df]\n    n_of_test_duplicates = df[df['Dataset'] == 'test']['Text'].count()\n    duplicates_ratio = n_of_test_duplicates / n_of_test_samples\n    result_df = df.rename(columns={'hash': 'Duplicate'})\n    duplicates_enumeration = to_ordional_enumeration(result_df['Duplicate'].to_list())\n    result_df['Duplicate'] = result_df['Duplicate'].apply(lambda x: duplicates_enumeration[x])\n    result_df = result_df.set_index(['Duplicate', 'Dataset', 'Sample ID'])\n    result_df = result_df.sort_index()\n    result_value = {'ratio': duplicates_ratio, 'duplicates': result_df}\n    if context.with_display is False or duplicates_ratio == 0:\n        return CheckResult(value=result_value)\n    train_grouped = df[df['Dataset'] == 'train'].groupby(['hash'], dropna=False)\n    train_instances = train_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    test_grouped = df[df['Dataset'] == 'test'].groupby(['hash'], dropna=False)\n    test_instances = test_grouped['Sample ID'].aggregate(lambda x: format_list(x.to_list()))\n    counted_test_duplicates = test_grouped.size()\n    first_sample_in_group = test_grouped['Text'].first()\n    display_table = pd.DataFrame({'Train Sample IDs': train_instances, 'Test Sample IDs': test_instances, 'Test Text Sample': first_sample_in_group.apply(self._truncate_text), 'Number of Test Duplicates': counted_test_duplicates})\n    display_table = display_table.iloc[:self.n_to_show]\n    display_table = display_table.reset_index(drop=True).set_index(['Train Sample IDs', 'Test Sample IDs'])\n    message = f'{format_percent(duplicates_ratio)} ({n_of_test_duplicates} / {n_of_test_samples}) of test data samples also appear in train data'\n    return CheckResult(value=result_value, display=[message, display_table])"
        ]
    }
]