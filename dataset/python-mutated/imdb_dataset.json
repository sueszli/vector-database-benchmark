[
    {
        "func_name": "load_imdb_dataset",
        "original": "def load_imdb_dataset(path='data', nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2, index_from=3):\n    \"\"\"Load IMDB dataset.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/imdb/``.\n    nb_words : int\n        Number of words to get.\n    skip_top : int\n        Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\n    maxlen : int\n        Maximum sequence length. Any longer sequence will be truncated.\n    seed : int\n        Seed for reproducible data shuffling.\n    start_char : int\n        The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\n    oov_char : int\n        Words that were cut out because of the num_words or skip_top limit will be replaced with this character.\n    index_from : int\n        Index actual words with this index and higher.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(\n    ...                                 nb_words=20000, test_split=0.2)\n    >>> print('X_train.shape', X_train.shape)\n    (20000,)  [[1, 62, 74, ... 1033, 507, 27],[1, 60, 33, ... 13, 1053, 7]..]\n    >>> print('y_train.shape', y_train.shape)\n    (20000,)  [1 0 0 ..., 1 0 1]\n\n    References\n    -----------\n    - `Modified from keras. <https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py>`__\n\n    \"\"\"\n    path = os.path.join(path, 'imdb')\n    filename = 'imdb.pkl'\n    url = 'https://s3.amazonaws.com/text-datasets/'\n    maybe_download_and_extract(filename, path, url)\n    if filename.endswith('.gz'):\n        f = gzip.open(os.path.join(path, filename), 'rb')\n    else:\n        f = open(os.path.join(path, filename), 'rb')\n    (X, labels) = pickle.load(f)\n    f.close()\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(labels)\n    if start_char is not None:\n        X = [[start_char] + [w + index_from for w in x] for x in X]\n    elif index_from:\n        X = [[w + index_from for w in x] for x in X]\n    if maxlen:\n        new_X = []\n        new_labels = []\n        for (x, y) in zip(X, labels):\n            if len(x) < maxlen:\n                new_X.append(x)\n                new_labels.append(y)\n        X = new_X\n        labels = new_labels\n    if not X:\n        raise Exception('After filtering for sequences shorter than maxlen=' + str(maxlen) + ', no sequence was kept. Increase maxlen.')\n    if not nb_words:\n        nb_words = max([max(x) for x in X])\n    if oov_char is not None:\n        X = [[oov_char if w >= nb_words or w < skip_top else w for w in x] for x in X]\n    else:\n        nX = []\n        for x in X:\n            nx = []\n            for w in x:\n                if w >= nb_words or w < skip_top:\n                    nx.append(w)\n            nX.append(nx)\n        X = nX\n    X_train = np.array(X[:int(len(X) * (1 - test_split))])\n    y_train = np.array(labels[:int(len(X) * (1 - test_split))])\n    X_test = np.array(X[int(len(X) * (1 - test_split)):])\n    y_test = np.array(labels[int(len(X) * (1 - test_split)):])\n    return (X_train, y_train, X_test, y_test)",
        "mutated": [
            "def load_imdb_dataset(path='data', nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2, index_from=3):\n    if False:\n        i = 10\n    \"Load IMDB dataset.\\n\\n    Parameters\\n    ----------\\n    path : str\\n        The path that the data is downloaded to, defaults is ``data/imdb/``.\\n    nb_words : int\\n        Number of words to get.\\n    skip_top : int\\n        Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\\n    maxlen : int\\n        Maximum sequence length. Any longer sequence will be truncated.\\n    seed : int\\n        Seed for reproducible data shuffling.\\n    start_char : int\\n        The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\\n    oov_char : int\\n        Words that were cut out because of the num_words or skip_top limit will be replaced with this character.\\n    index_from : int\\n        Index actual words with this index and higher.\\n\\n    Examples\\n    --------\\n    >>> X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(\\n    ...                                 nb_words=20000, test_split=0.2)\\n    >>> print('X_train.shape', X_train.shape)\\n    (20000,)  [[1, 62, 74, ... 1033, 507, 27],[1, 60, 33, ... 13, 1053, 7]..]\\n    >>> print('y_train.shape', y_train.shape)\\n    (20000,)  [1 0 0 ..., 1 0 1]\\n\\n    References\\n    -----------\\n    - `Modified from keras. <https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py>`__\\n\\n    \"\n    path = os.path.join(path, 'imdb')\n    filename = 'imdb.pkl'\n    url = 'https://s3.amazonaws.com/text-datasets/'\n    maybe_download_and_extract(filename, path, url)\n    if filename.endswith('.gz'):\n        f = gzip.open(os.path.join(path, filename), 'rb')\n    else:\n        f = open(os.path.join(path, filename), 'rb')\n    (X, labels) = pickle.load(f)\n    f.close()\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(labels)\n    if start_char is not None:\n        X = [[start_char] + [w + index_from for w in x] for x in X]\n    elif index_from:\n        X = [[w + index_from for w in x] for x in X]\n    if maxlen:\n        new_X = []\n        new_labels = []\n        for (x, y) in zip(X, labels):\n            if len(x) < maxlen:\n                new_X.append(x)\n                new_labels.append(y)\n        X = new_X\n        labels = new_labels\n    if not X:\n        raise Exception('After filtering for sequences shorter than maxlen=' + str(maxlen) + ', no sequence was kept. Increase maxlen.')\n    if not nb_words:\n        nb_words = max([max(x) for x in X])\n    if oov_char is not None:\n        X = [[oov_char if w >= nb_words or w < skip_top else w for w in x] for x in X]\n    else:\n        nX = []\n        for x in X:\n            nx = []\n            for w in x:\n                if w >= nb_words or w < skip_top:\n                    nx.append(w)\n            nX.append(nx)\n        X = nX\n    X_train = np.array(X[:int(len(X) * (1 - test_split))])\n    y_train = np.array(labels[:int(len(X) * (1 - test_split))])\n    X_test = np.array(X[int(len(X) * (1 - test_split)):])\n    y_test = np.array(labels[int(len(X) * (1 - test_split)):])\n    return (X_train, y_train, X_test, y_test)",
            "def load_imdb_dataset(path='data', nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2, index_from=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Load IMDB dataset.\\n\\n    Parameters\\n    ----------\\n    path : str\\n        The path that the data is downloaded to, defaults is ``data/imdb/``.\\n    nb_words : int\\n        Number of words to get.\\n    skip_top : int\\n        Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\\n    maxlen : int\\n        Maximum sequence length. Any longer sequence will be truncated.\\n    seed : int\\n        Seed for reproducible data shuffling.\\n    start_char : int\\n        The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\\n    oov_char : int\\n        Words that were cut out because of the num_words or skip_top limit will be replaced with this character.\\n    index_from : int\\n        Index actual words with this index and higher.\\n\\n    Examples\\n    --------\\n    >>> X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(\\n    ...                                 nb_words=20000, test_split=0.2)\\n    >>> print('X_train.shape', X_train.shape)\\n    (20000,)  [[1, 62, 74, ... 1033, 507, 27],[1, 60, 33, ... 13, 1053, 7]..]\\n    >>> print('y_train.shape', y_train.shape)\\n    (20000,)  [1 0 0 ..., 1 0 1]\\n\\n    References\\n    -----------\\n    - `Modified from keras. <https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py>`__\\n\\n    \"\n    path = os.path.join(path, 'imdb')\n    filename = 'imdb.pkl'\n    url = 'https://s3.amazonaws.com/text-datasets/'\n    maybe_download_and_extract(filename, path, url)\n    if filename.endswith('.gz'):\n        f = gzip.open(os.path.join(path, filename), 'rb')\n    else:\n        f = open(os.path.join(path, filename), 'rb')\n    (X, labels) = pickle.load(f)\n    f.close()\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(labels)\n    if start_char is not None:\n        X = [[start_char] + [w + index_from for w in x] for x in X]\n    elif index_from:\n        X = [[w + index_from for w in x] for x in X]\n    if maxlen:\n        new_X = []\n        new_labels = []\n        for (x, y) in zip(X, labels):\n            if len(x) < maxlen:\n                new_X.append(x)\n                new_labels.append(y)\n        X = new_X\n        labels = new_labels\n    if not X:\n        raise Exception('After filtering for sequences shorter than maxlen=' + str(maxlen) + ', no sequence was kept. Increase maxlen.')\n    if not nb_words:\n        nb_words = max([max(x) for x in X])\n    if oov_char is not None:\n        X = [[oov_char if w >= nb_words or w < skip_top else w for w in x] for x in X]\n    else:\n        nX = []\n        for x in X:\n            nx = []\n            for w in x:\n                if w >= nb_words or w < skip_top:\n                    nx.append(w)\n            nX.append(nx)\n        X = nX\n    X_train = np.array(X[:int(len(X) * (1 - test_split))])\n    y_train = np.array(labels[:int(len(X) * (1 - test_split))])\n    X_test = np.array(X[int(len(X) * (1 - test_split)):])\n    y_test = np.array(labels[int(len(X) * (1 - test_split)):])\n    return (X_train, y_train, X_test, y_test)",
            "def load_imdb_dataset(path='data', nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2, index_from=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Load IMDB dataset.\\n\\n    Parameters\\n    ----------\\n    path : str\\n        The path that the data is downloaded to, defaults is ``data/imdb/``.\\n    nb_words : int\\n        Number of words to get.\\n    skip_top : int\\n        Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\\n    maxlen : int\\n        Maximum sequence length. Any longer sequence will be truncated.\\n    seed : int\\n        Seed for reproducible data shuffling.\\n    start_char : int\\n        The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\\n    oov_char : int\\n        Words that were cut out because of the num_words or skip_top limit will be replaced with this character.\\n    index_from : int\\n        Index actual words with this index and higher.\\n\\n    Examples\\n    --------\\n    >>> X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(\\n    ...                                 nb_words=20000, test_split=0.2)\\n    >>> print('X_train.shape', X_train.shape)\\n    (20000,)  [[1, 62, 74, ... 1033, 507, 27],[1, 60, 33, ... 13, 1053, 7]..]\\n    >>> print('y_train.shape', y_train.shape)\\n    (20000,)  [1 0 0 ..., 1 0 1]\\n\\n    References\\n    -----------\\n    - `Modified from keras. <https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py>`__\\n\\n    \"\n    path = os.path.join(path, 'imdb')\n    filename = 'imdb.pkl'\n    url = 'https://s3.amazonaws.com/text-datasets/'\n    maybe_download_and_extract(filename, path, url)\n    if filename.endswith('.gz'):\n        f = gzip.open(os.path.join(path, filename), 'rb')\n    else:\n        f = open(os.path.join(path, filename), 'rb')\n    (X, labels) = pickle.load(f)\n    f.close()\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(labels)\n    if start_char is not None:\n        X = [[start_char] + [w + index_from for w in x] for x in X]\n    elif index_from:\n        X = [[w + index_from for w in x] for x in X]\n    if maxlen:\n        new_X = []\n        new_labels = []\n        for (x, y) in zip(X, labels):\n            if len(x) < maxlen:\n                new_X.append(x)\n                new_labels.append(y)\n        X = new_X\n        labels = new_labels\n    if not X:\n        raise Exception('After filtering for sequences shorter than maxlen=' + str(maxlen) + ', no sequence was kept. Increase maxlen.')\n    if not nb_words:\n        nb_words = max([max(x) for x in X])\n    if oov_char is not None:\n        X = [[oov_char if w >= nb_words or w < skip_top else w for w in x] for x in X]\n    else:\n        nX = []\n        for x in X:\n            nx = []\n            for w in x:\n                if w >= nb_words or w < skip_top:\n                    nx.append(w)\n            nX.append(nx)\n        X = nX\n    X_train = np.array(X[:int(len(X) * (1 - test_split))])\n    y_train = np.array(labels[:int(len(X) * (1 - test_split))])\n    X_test = np.array(X[int(len(X) * (1 - test_split)):])\n    y_test = np.array(labels[int(len(X) * (1 - test_split)):])\n    return (X_train, y_train, X_test, y_test)",
            "def load_imdb_dataset(path='data', nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2, index_from=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Load IMDB dataset.\\n\\n    Parameters\\n    ----------\\n    path : str\\n        The path that the data is downloaded to, defaults is ``data/imdb/``.\\n    nb_words : int\\n        Number of words to get.\\n    skip_top : int\\n        Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\\n    maxlen : int\\n        Maximum sequence length. Any longer sequence will be truncated.\\n    seed : int\\n        Seed for reproducible data shuffling.\\n    start_char : int\\n        The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\\n    oov_char : int\\n        Words that were cut out because of the num_words or skip_top limit will be replaced with this character.\\n    index_from : int\\n        Index actual words with this index and higher.\\n\\n    Examples\\n    --------\\n    >>> X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(\\n    ...                                 nb_words=20000, test_split=0.2)\\n    >>> print('X_train.shape', X_train.shape)\\n    (20000,)  [[1, 62, 74, ... 1033, 507, 27],[1, 60, 33, ... 13, 1053, 7]..]\\n    >>> print('y_train.shape', y_train.shape)\\n    (20000,)  [1 0 0 ..., 1 0 1]\\n\\n    References\\n    -----------\\n    - `Modified from keras. <https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py>`__\\n\\n    \"\n    path = os.path.join(path, 'imdb')\n    filename = 'imdb.pkl'\n    url = 'https://s3.amazonaws.com/text-datasets/'\n    maybe_download_and_extract(filename, path, url)\n    if filename.endswith('.gz'):\n        f = gzip.open(os.path.join(path, filename), 'rb')\n    else:\n        f = open(os.path.join(path, filename), 'rb')\n    (X, labels) = pickle.load(f)\n    f.close()\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(labels)\n    if start_char is not None:\n        X = [[start_char] + [w + index_from for w in x] for x in X]\n    elif index_from:\n        X = [[w + index_from for w in x] for x in X]\n    if maxlen:\n        new_X = []\n        new_labels = []\n        for (x, y) in zip(X, labels):\n            if len(x) < maxlen:\n                new_X.append(x)\n                new_labels.append(y)\n        X = new_X\n        labels = new_labels\n    if not X:\n        raise Exception('After filtering for sequences shorter than maxlen=' + str(maxlen) + ', no sequence was kept. Increase maxlen.')\n    if not nb_words:\n        nb_words = max([max(x) for x in X])\n    if oov_char is not None:\n        X = [[oov_char if w >= nb_words or w < skip_top else w for w in x] for x in X]\n    else:\n        nX = []\n        for x in X:\n            nx = []\n            for w in x:\n                if w >= nb_words or w < skip_top:\n                    nx.append(w)\n            nX.append(nx)\n        X = nX\n    X_train = np.array(X[:int(len(X) * (1 - test_split))])\n    y_train = np.array(labels[:int(len(X) * (1 - test_split))])\n    X_test = np.array(X[int(len(X) * (1 - test_split)):])\n    y_test = np.array(labels[int(len(X) * (1 - test_split)):])\n    return (X_train, y_train, X_test, y_test)",
            "def load_imdb_dataset(path='data', nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2, index_from=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Load IMDB dataset.\\n\\n    Parameters\\n    ----------\\n    path : str\\n        The path that the data is downloaded to, defaults is ``data/imdb/``.\\n    nb_words : int\\n        Number of words to get.\\n    skip_top : int\\n        Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\\n    maxlen : int\\n        Maximum sequence length. Any longer sequence will be truncated.\\n    seed : int\\n        Seed for reproducible data shuffling.\\n    start_char : int\\n        The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\\n    oov_char : int\\n        Words that were cut out because of the num_words or skip_top limit will be replaced with this character.\\n    index_from : int\\n        Index actual words with this index and higher.\\n\\n    Examples\\n    --------\\n    >>> X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(\\n    ...                                 nb_words=20000, test_split=0.2)\\n    >>> print('X_train.shape', X_train.shape)\\n    (20000,)  [[1, 62, 74, ... 1033, 507, 27],[1, 60, 33, ... 13, 1053, 7]..]\\n    >>> print('y_train.shape', y_train.shape)\\n    (20000,)  [1 0 0 ..., 1 0 1]\\n\\n    References\\n    -----------\\n    - `Modified from keras. <https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py>`__\\n\\n    \"\n    path = os.path.join(path, 'imdb')\n    filename = 'imdb.pkl'\n    url = 'https://s3.amazonaws.com/text-datasets/'\n    maybe_download_and_extract(filename, path, url)\n    if filename.endswith('.gz'):\n        f = gzip.open(os.path.join(path, filename), 'rb')\n    else:\n        f = open(os.path.join(path, filename), 'rb')\n    (X, labels) = pickle.load(f)\n    f.close()\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(labels)\n    if start_char is not None:\n        X = [[start_char] + [w + index_from for w in x] for x in X]\n    elif index_from:\n        X = [[w + index_from for w in x] for x in X]\n    if maxlen:\n        new_X = []\n        new_labels = []\n        for (x, y) in zip(X, labels):\n            if len(x) < maxlen:\n                new_X.append(x)\n                new_labels.append(y)\n        X = new_X\n        labels = new_labels\n    if not X:\n        raise Exception('After filtering for sequences shorter than maxlen=' + str(maxlen) + ', no sequence was kept. Increase maxlen.')\n    if not nb_words:\n        nb_words = max([max(x) for x in X])\n    if oov_char is not None:\n        X = [[oov_char if w >= nb_words or w < skip_top else w for w in x] for x in X]\n    else:\n        nX = []\n        for x in X:\n            nx = []\n            for w in x:\n                if w >= nb_words or w < skip_top:\n                    nx.append(w)\n            nX.append(nx)\n        X = nX\n    X_train = np.array(X[:int(len(X) * (1 - test_split))])\n    y_train = np.array(labels[:int(len(X) * (1 - test_split))])\n    X_test = np.array(X[int(len(X) * (1 - test_split)):])\n    y_test = np.array(labels[int(len(X) * (1 - test_split)):])\n    return (X_train, y_train, X_test, y_test)"
        ]
    }
]