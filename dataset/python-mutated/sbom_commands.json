[
    {
        "func_name": "sbom",
        "original": "@click.group(cls=BreezeGroup, name='sbom', help='Tools that release managers can use to prepare sbom information')\ndef sbom():\n    pass",
        "mutated": [
            "@click.group(cls=BreezeGroup, name='sbom', help='Tools that release managers can use to prepare sbom information')\ndef sbom():\n    if False:\n        i = 10\n    pass",
            "@click.group(cls=BreezeGroup, name='sbom', help='Tools that release managers can use to prepare sbom information')\ndef sbom():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "@click.group(cls=BreezeGroup, name='sbom', help='Tools that release managers can use to prepare sbom information')\ndef sbom():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "@click.group(cls=BreezeGroup, name='sbom', help='Tools that release managers can use to prepare sbom information')\ndef sbom():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "@click.group(cls=BreezeGroup, name='sbom', help='Tools that release managers can use to prepare sbom information')\ndef sbom():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "_dir_exists_warn_and_should_skip",
        "original": "def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n    if dir.exists():\n        if not force:\n            get_console().print(f'[warning]The {dir} already exists. Skipping')\n            return True\n        else:\n            get_console().print(f'[warning]The {dir} already exists. Forcing update')\n            return False\n    return False",
        "mutated": [
            "def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n    if False:\n        i = 10\n    if dir.exists():\n        if not force:\n            get_console().print(f'[warning]The {dir} already exists. Skipping')\n            return True\n        else:\n            get_console().print(f'[warning]The {dir} already exists. Forcing update')\n            return False\n    return False",
            "def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if dir.exists():\n        if not force:\n            get_console().print(f'[warning]The {dir} already exists. Skipping')\n            return True\n        else:\n            get_console().print(f'[warning]The {dir} already exists. Forcing update')\n            return False\n    return False",
            "def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if dir.exists():\n        if not force:\n            get_console().print(f'[warning]The {dir} already exists. Skipping')\n            return True\n        else:\n            get_console().print(f'[warning]The {dir} already exists. Forcing update')\n            return False\n    return False",
            "def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if dir.exists():\n        if not force:\n            get_console().print(f'[warning]The {dir} already exists. Skipping')\n            return True\n        else:\n            get_console().print(f'[warning]The {dir} already exists. Forcing update')\n            return False\n    return False",
            "def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if dir.exists():\n        if not force:\n            get_console().print(f'[warning]The {dir} already exists. Skipping')\n            return True\n        else:\n            get_console().print(f'[warning]The {dir} already exists. Forcing update')\n            return False\n    return False"
        ]
    },
    {
        "func_name": "_generate_index",
        "original": "def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n    destination_index_path = destination_dir / 'index.html'\n    get_console().print(f'[info]Generating index for {destination_dir}')\n    sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n    if not get_dry_run():\n        destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))",
        "mutated": [
            "def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n    if False:\n        i = 10\n    destination_index_path = destination_dir / 'index.html'\n    get_console().print(f'[info]Generating index for {destination_dir}')\n    sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n    if not get_dry_run():\n        destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))",
            "def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    destination_index_path = destination_dir / 'index.html'\n    get_console().print(f'[info]Generating index for {destination_dir}')\n    sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n    if not get_dry_run():\n        destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))",
            "def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    destination_index_path = destination_dir / 'index.html'\n    get_console().print(f'[info]Generating index for {destination_dir}')\n    sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n    if not get_dry_run():\n        destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))",
            "def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    destination_index_path = destination_dir / 'index.html'\n    get_console().print(f'[info]Generating index for {destination_dir}')\n    sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n    if not get_dry_run():\n        destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))",
            "def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    destination_index_path = destination_dir / 'index.html'\n    get_console().print(f'[info]Generating index for {destination_dir}')\n    sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n    if not get_dry_run():\n        destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))"
        ]
    },
    {
        "func_name": "update_sbom_information",
        "original": "@sbom.command(name='update-sbom-information', help='Update SBOM information in airflow-site project.')\n@click.option('--airflow-site-directory', type=click.Path(file_okay=False, dir_okay=True, path_type=Path, exists=True), required=True, envvar='AIRFLOW_SITE_DIRECTORY', help='Directory where airflow-site directory is located.')\n@click.option('--airflow-version', type=str, required=False, envvar='AIRFLOW_VERSION', help='Version of airflow to update sbom from. (defaulted to all active airflow versions)')\n@option_historical_python_version\n@click.option('--include-provider-dependencies', is_flag=True, help='Whether to include provider dependencies in SBOM generation.')\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update of sbom even if it already exists.')\n@option_verbose\n@option_dry_run\n@option_answer\n@click.option('--package-filter', help='List of packages to consider. You can use `apache-airflow` for core or `apache-airflow-providers` to consider all the providers.', type=BetterChoice(['apache-airflow-providers', 'apache-airflow']), required=False, default='apache-airflow')\ndef update_sbom_information(airflow_site_directory: Path, airflow_version: str | None, python: str | None, include_provider_dependencies: bool, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool, package_filter: tuple[str, ...]):\n    import jinja2\n    from jinja2 import StrictUndefined\n    from airflow_breeze.utils.cdxgen import produce_sbom_for_application_via_cdxgen_server, start_cdxgen_server\n    from airflow_breeze.utils.github import get_active_airflow_versions\n    if airflow_version is None:\n        airflow_versions = get_active_airflow_versions()\n    else:\n        airflow_versions = [airflow_version]\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    application_root_path = FILES_SBOM_DIR\n    start_cdxgen_server(application_root_path, run_in_parallel, parallelism)\n    jobs_to_run: list[SbomApplicationJob] = []\n    airflow_site_archive_directory = airflow_site_directory / 'docs-archive'\n\n    def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n        if dir.exists():\n            if not force:\n                get_console().print(f'[warning]The {dir} already exists. Skipping')\n                return True\n            else:\n                get_console().print(f'[warning]The {dir} already exists. Forcing update')\n                return False\n        return False\n    if package_filter == 'apache-airflow':\n        apache_airflow_documentation_directory = airflow_site_archive_directory / 'apache-airflow'\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            if not airflow_version_dir.exists():\n                get_console().print(f'[warning]The {airflow_version_dir} does not exist. Skipping')\n                continue\n            destination_dir = airflow_version_dir / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {airflow_v}.')\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{airflow_v}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomCoreJob(airflow_version=airflow_v, python_version=python_version, application_root_path=application_root_path, include_provider_dependencies=include_provider_dependencies, target_path=target_sbom_path))\n    elif package_filter == 'apache-airflow-providers':\n        user_confirm('You are about to update sbom information for providers, did you refresh the providers requirements with the command `breeze sbom generate-providers-requirements`?', quit_allowed=False, default_answer=Answer.YES)\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {provider_id} version {provider_version}.')\n            python_versions = set((dir_name.replace('python', '') for dir_name in os.listdir(PROVIDER_REQUIREMENTS_DIR_PATH / node_name)))\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{provider_id}-{provider_version}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomProviderJob(provider_id=provider_id, provider_version=provider_version, python_version=python_version, target_path=target_sbom_path, folder_name=node_name))\n    if len(jobs_to_run) == 0:\n        get_console().print('[info]Nothing to do, there is no job to process')\n        return\n    if run_in_parallel:\n        parallelism = min(parallelism, len(jobs_to_run))\n        get_console().print(f'[info]Running {len(jobs_to_run)} jobs in parallel')\n        with ci_group(f'Generating SBOMs for {jobs_to_run}'):\n            all_params = [f'Generate SBOMs for {job.get_job_name()}' for job in jobs_to_run]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                port_map = get_cdxgen_port_mapping(parallelism, pool)\n                results = [pool.apply_async(produce_sbom_for_application_via_cdxgen_server, kwds={'job': job, 'output': outputs[index], 'port_map': port_map}) for (index, job) in enumerate(jobs_to_run)]\n        check_async_run_results(results=results, success='All SBOMs were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for job in jobs_to_run:\n            produce_sbom_for_application_via_cdxgen_server(job, output=None)\n    html_template = SBOM_INDEX_TEMPLATE\n\n    def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n        destination_index_path = destination_dir / 'index.html'\n        get_console().print(f'[info]Generating index for {destination_dir}')\n        sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n        if not get_dry_run():\n            destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))\n    if package_filter == 'apache-airflow':\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            destination_dir = airflow_version_dir / 'sbom'\n            _generate_index(destination_dir, None, airflow_v)\n    elif package_filter == 'apache-airflow-providers':\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            _generate_index(destination_dir, provider_id, provider_version)",
        "mutated": [
            "@sbom.command(name='update-sbom-information', help='Update SBOM information in airflow-site project.')\n@click.option('--airflow-site-directory', type=click.Path(file_okay=False, dir_okay=True, path_type=Path, exists=True), required=True, envvar='AIRFLOW_SITE_DIRECTORY', help='Directory where airflow-site directory is located.')\n@click.option('--airflow-version', type=str, required=False, envvar='AIRFLOW_VERSION', help='Version of airflow to update sbom from. (defaulted to all active airflow versions)')\n@option_historical_python_version\n@click.option('--include-provider-dependencies', is_flag=True, help='Whether to include provider dependencies in SBOM generation.')\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update of sbom even if it already exists.')\n@option_verbose\n@option_dry_run\n@option_answer\n@click.option('--package-filter', help='List of packages to consider. You can use `apache-airflow` for core or `apache-airflow-providers` to consider all the providers.', type=BetterChoice(['apache-airflow-providers', 'apache-airflow']), required=False, default='apache-airflow')\ndef update_sbom_information(airflow_site_directory: Path, airflow_version: str | None, python: str | None, include_provider_dependencies: bool, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool, package_filter: tuple[str, ...]):\n    if False:\n        i = 10\n    import jinja2\n    from jinja2 import StrictUndefined\n    from airflow_breeze.utils.cdxgen import produce_sbom_for_application_via_cdxgen_server, start_cdxgen_server\n    from airflow_breeze.utils.github import get_active_airflow_versions\n    if airflow_version is None:\n        airflow_versions = get_active_airflow_versions()\n    else:\n        airflow_versions = [airflow_version]\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    application_root_path = FILES_SBOM_DIR\n    start_cdxgen_server(application_root_path, run_in_parallel, parallelism)\n    jobs_to_run: list[SbomApplicationJob] = []\n    airflow_site_archive_directory = airflow_site_directory / 'docs-archive'\n\n    def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n        if dir.exists():\n            if not force:\n                get_console().print(f'[warning]The {dir} already exists. Skipping')\n                return True\n            else:\n                get_console().print(f'[warning]The {dir} already exists. Forcing update')\n                return False\n        return False\n    if package_filter == 'apache-airflow':\n        apache_airflow_documentation_directory = airflow_site_archive_directory / 'apache-airflow'\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            if not airflow_version_dir.exists():\n                get_console().print(f'[warning]The {airflow_version_dir} does not exist. Skipping')\n                continue\n            destination_dir = airflow_version_dir / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {airflow_v}.')\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{airflow_v}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomCoreJob(airflow_version=airflow_v, python_version=python_version, application_root_path=application_root_path, include_provider_dependencies=include_provider_dependencies, target_path=target_sbom_path))\n    elif package_filter == 'apache-airflow-providers':\n        user_confirm('You are about to update sbom information for providers, did you refresh the providers requirements with the command `breeze sbom generate-providers-requirements`?', quit_allowed=False, default_answer=Answer.YES)\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {provider_id} version {provider_version}.')\n            python_versions = set((dir_name.replace('python', '') for dir_name in os.listdir(PROVIDER_REQUIREMENTS_DIR_PATH / node_name)))\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{provider_id}-{provider_version}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomProviderJob(provider_id=provider_id, provider_version=provider_version, python_version=python_version, target_path=target_sbom_path, folder_name=node_name))\n    if len(jobs_to_run) == 0:\n        get_console().print('[info]Nothing to do, there is no job to process')\n        return\n    if run_in_parallel:\n        parallelism = min(parallelism, len(jobs_to_run))\n        get_console().print(f'[info]Running {len(jobs_to_run)} jobs in parallel')\n        with ci_group(f'Generating SBOMs for {jobs_to_run}'):\n            all_params = [f'Generate SBOMs for {job.get_job_name()}' for job in jobs_to_run]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                port_map = get_cdxgen_port_mapping(parallelism, pool)\n                results = [pool.apply_async(produce_sbom_for_application_via_cdxgen_server, kwds={'job': job, 'output': outputs[index], 'port_map': port_map}) for (index, job) in enumerate(jobs_to_run)]\n        check_async_run_results(results=results, success='All SBOMs were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for job in jobs_to_run:\n            produce_sbom_for_application_via_cdxgen_server(job, output=None)\n    html_template = SBOM_INDEX_TEMPLATE\n\n    def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n        destination_index_path = destination_dir / 'index.html'\n        get_console().print(f'[info]Generating index for {destination_dir}')\n        sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n        if not get_dry_run():\n            destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))\n    if package_filter == 'apache-airflow':\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            destination_dir = airflow_version_dir / 'sbom'\n            _generate_index(destination_dir, None, airflow_v)\n    elif package_filter == 'apache-airflow-providers':\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            _generate_index(destination_dir, provider_id, provider_version)",
            "@sbom.command(name='update-sbom-information', help='Update SBOM information in airflow-site project.')\n@click.option('--airflow-site-directory', type=click.Path(file_okay=False, dir_okay=True, path_type=Path, exists=True), required=True, envvar='AIRFLOW_SITE_DIRECTORY', help='Directory where airflow-site directory is located.')\n@click.option('--airflow-version', type=str, required=False, envvar='AIRFLOW_VERSION', help='Version of airflow to update sbom from. (defaulted to all active airflow versions)')\n@option_historical_python_version\n@click.option('--include-provider-dependencies', is_flag=True, help='Whether to include provider dependencies in SBOM generation.')\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update of sbom even if it already exists.')\n@option_verbose\n@option_dry_run\n@option_answer\n@click.option('--package-filter', help='List of packages to consider. You can use `apache-airflow` for core or `apache-airflow-providers` to consider all the providers.', type=BetterChoice(['apache-airflow-providers', 'apache-airflow']), required=False, default='apache-airflow')\ndef update_sbom_information(airflow_site_directory: Path, airflow_version: str | None, python: str | None, include_provider_dependencies: bool, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool, package_filter: tuple[str, ...]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    import jinja2\n    from jinja2 import StrictUndefined\n    from airflow_breeze.utils.cdxgen import produce_sbom_for_application_via_cdxgen_server, start_cdxgen_server\n    from airflow_breeze.utils.github import get_active_airflow_versions\n    if airflow_version is None:\n        airflow_versions = get_active_airflow_versions()\n    else:\n        airflow_versions = [airflow_version]\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    application_root_path = FILES_SBOM_DIR\n    start_cdxgen_server(application_root_path, run_in_parallel, parallelism)\n    jobs_to_run: list[SbomApplicationJob] = []\n    airflow_site_archive_directory = airflow_site_directory / 'docs-archive'\n\n    def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n        if dir.exists():\n            if not force:\n                get_console().print(f'[warning]The {dir} already exists. Skipping')\n                return True\n            else:\n                get_console().print(f'[warning]The {dir} already exists. Forcing update')\n                return False\n        return False\n    if package_filter == 'apache-airflow':\n        apache_airflow_documentation_directory = airflow_site_archive_directory / 'apache-airflow'\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            if not airflow_version_dir.exists():\n                get_console().print(f'[warning]The {airflow_version_dir} does not exist. Skipping')\n                continue\n            destination_dir = airflow_version_dir / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {airflow_v}.')\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{airflow_v}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomCoreJob(airflow_version=airflow_v, python_version=python_version, application_root_path=application_root_path, include_provider_dependencies=include_provider_dependencies, target_path=target_sbom_path))\n    elif package_filter == 'apache-airflow-providers':\n        user_confirm('You are about to update sbom information for providers, did you refresh the providers requirements with the command `breeze sbom generate-providers-requirements`?', quit_allowed=False, default_answer=Answer.YES)\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {provider_id} version {provider_version}.')\n            python_versions = set((dir_name.replace('python', '') for dir_name in os.listdir(PROVIDER_REQUIREMENTS_DIR_PATH / node_name)))\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{provider_id}-{provider_version}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomProviderJob(provider_id=provider_id, provider_version=provider_version, python_version=python_version, target_path=target_sbom_path, folder_name=node_name))\n    if len(jobs_to_run) == 0:\n        get_console().print('[info]Nothing to do, there is no job to process')\n        return\n    if run_in_parallel:\n        parallelism = min(parallelism, len(jobs_to_run))\n        get_console().print(f'[info]Running {len(jobs_to_run)} jobs in parallel')\n        with ci_group(f'Generating SBOMs for {jobs_to_run}'):\n            all_params = [f'Generate SBOMs for {job.get_job_name()}' for job in jobs_to_run]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                port_map = get_cdxgen_port_mapping(parallelism, pool)\n                results = [pool.apply_async(produce_sbom_for_application_via_cdxgen_server, kwds={'job': job, 'output': outputs[index], 'port_map': port_map}) for (index, job) in enumerate(jobs_to_run)]\n        check_async_run_results(results=results, success='All SBOMs were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for job in jobs_to_run:\n            produce_sbom_for_application_via_cdxgen_server(job, output=None)\n    html_template = SBOM_INDEX_TEMPLATE\n\n    def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n        destination_index_path = destination_dir / 'index.html'\n        get_console().print(f'[info]Generating index for {destination_dir}')\n        sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n        if not get_dry_run():\n            destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))\n    if package_filter == 'apache-airflow':\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            destination_dir = airflow_version_dir / 'sbom'\n            _generate_index(destination_dir, None, airflow_v)\n    elif package_filter == 'apache-airflow-providers':\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            _generate_index(destination_dir, provider_id, provider_version)",
            "@sbom.command(name='update-sbom-information', help='Update SBOM information in airflow-site project.')\n@click.option('--airflow-site-directory', type=click.Path(file_okay=False, dir_okay=True, path_type=Path, exists=True), required=True, envvar='AIRFLOW_SITE_DIRECTORY', help='Directory where airflow-site directory is located.')\n@click.option('--airflow-version', type=str, required=False, envvar='AIRFLOW_VERSION', help='Version of airflow to update sbom from. (defaulted to all active airflow versions)')\n@option_historical_python_version\n@click.option('--include-provider-dependencies', is_flag=True, help='Whether to include provider dependencies in SBOM generation.')\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update of sbom even if it already exists.')\n@option_verbose\n@option_dry_run\n@option_answer\n@click.option('--package-filter', help='List of packages to consider. You can use `apache-airflow` for core or `apache-airflow-providers` to consider all the providers.', type=BetterChoice(['apache-airflow-providers', 'apache-airflow']), required=False, default='apache-airflow')\ndef update_sbom_information(airflow_site_directory: Path, airflow_version: str | None, python: str | None, include_provider_dependencies: bool, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool, package_filter: tuple[str, ...]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    import jinja2\n    from jinja2 import StrictUndefined\n    from airflow_breeze.utils.cdxgen import produce_sbom_for_application_via_cdxgen_server, start_cdxgen_server\n    from airflow_breeze.utils.github import get_active_airflow_versions\n    if airflow_version is None:\n        airflow_versions = get_active_airflow_versions()\n    else:\n        airflow_versions = [airflow_version]\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    application_root_path = FILES_SBOM_DIR\n    start_cdxgen_server(application_root_path, run_in_parallel, parallelism)\n    jobs_to_run: list[SbomApplicationJob] = []\n    airflow_site_archive_directory = airflow_site_directory / 'docs-archive'\n\n    def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n        if dir.exists():\n            if not force:\n                get_console().print(f'[warning]The {dir} already exists. Skipping')\n                return True\n            else:\n                get_console().print(f'[warning]The {dir} already exists. Forcing update')\n                return False\n        return False\n    if package_filter == 'apache-airflow':\n        apache_airflow_documentation_directory = airflow_site_archive_directory / 'apache-airflow'\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            if not airflow_version_dir.exists():\n                get_console().print(f'[warning]The {airflow_version_dir} does not exist. Skipping')\n                continue\n            destination_dir = airflow_version_dir / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {airflow_v}.')\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{airflow_v}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomCoreJob(airflow_version=airflow_v, python_version=python_version, application_root_path=application_root_path, include_provider_dependencies=include_provider_dependencies, target_path=target_sbom_path))\n    elif package_filter == 'apache-airflow-providers':\n        user_confirm('You are about to update sbom information for providers, did you refresh the providers requirements with the command `breeze sbom generate-providers-requirements`?', quit_allowed=False, default_answer=Answer.YES)\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {provider_id} version {provider_version}.')\n            python_versions = set((dir_name.replace('python', '') for dir_name in os.listdir(PROVIDER_REQUIREMENTS_DIR_PATH / node_name)))\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{provider_id}-{provider_version}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomProviderJob(provider_id=provider_id, provider_version=provider_version, python_version=python_version, target_path=target_sbom_path, folder_name=node_name))\n    if len(jobs_to_run) == 0:\n        get_console().print('[info]Nothing to do, there is no job to process')\n        return\n    if run_in_parallel:\n        parallelism = min(parallelism, len(jobs_to_run))\n        get_console().print(f'[info]Running {len(jobs_to_run)} jobs in parallel')\n        with ci_group(f'Generating SBOMs for {jobs_to_run}'):\n            all_params = [f'Generate SBOMs for {job.get_job_name()}' for job in jobs_to_run]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                port_map = get_cdxgen_port_mapping(parallelism, pool)\n                results = [pool.apply_async(produce_sbom_for_application_via_cdxgen_server, kwds={'job': job, 'output': outputs[index], 'port_map': port_map}) for (index, job) in enumerate(jobs_to_run)]\n        check_async_run_results(results=results, success='All SBOMs were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for job in jobs_to_run:\n            produce_sbom_for_application_via_cdxgen_server(job, output=None)\n    html_template = SBOM_INDEX_TEMPLATE\n\n    def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n        destination_index_path = destination_dir / 'index.html'\n        get_console().print(f'[info]Generating index for {destination_dir}')\n        sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n        if not get_dry_run():\n            destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))\n    if package_filter == 'apache-airflow':\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            destination_dir = airflow_version_dir / 'sbom'\n            _generate_index(destination_dir, None, airflow_v)\n    elif package_filter == 'apache-airflow-providers':\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            _generate_index(destination_dir, provider_id, provider_version)",
            "@sbom.command(name='update-sbom-information', help='Update SBOM information in airflow-site project.')\n@click.option('--airflow-site-directory', type=click.Path(file_okay=False, dir_okay=True, path_type=Path, exists=True), required=True, envvar='AIRFLOW_SITE_DIRECTORY', help='Directory where airflow-site directory is located.')\n@click.option('--airflow-version', type=str, required=False, envvar='AIRFLOW_VERSION', help='Version of airflow to update sbom from. (defaulted to all active airflow versions)')\n@option_historical_python_version\n@click.option('--include-provider-dependencies', is_flag=True, help='Whether to include provider dependencies in SBOM generation.')\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update of sbom even if it already exists.')\n@option_verbose\n@option_dry_run\n@option_answer\n@click.option('--package-filter', help='List of packages to consider. You can use `apache-airflow` for core or `apache-airflow-providers` to consider all the providers.', type=BetterChoice(['apache-airflow-providers', 'apache-airflow']), required=False, default='apache-airflow')\ndef update_sbom_information(airflow_site_directory: Path, airflow_version: str | None, python: str | None, include_provider_dependencies: bool, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool, package_filter: tuple[str, ...]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    import jinja2\n    from jinja2 import StrictUndefined\n    from airflow_breeze.utils.cdxgen import produce_sbom_for_application_via_cdxgen_server, start_cdxgen_server\n    from airflow_breeze.utils.github import get_active_airflow_versions\n    if airflow_version is None:\n        airflow_versions = get_active_airflow_versions()\n    else:\n        airflow_versions = [airflow_version]\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    application_root_path = FILES_SBOM_DIR\n    start_cdxgen_server(application_root_path, run_in_parallel, parallelism)\n    jobs_to_run: list[SbomApplicationJob] = []\n    airflow_site_archive_directory = airflow_site_directory / 'docs-archive'\n\n    def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n        if dir.exists():\n            if not force:\n                get_console().print(f'[warning]The {dir} already exists. Skipping')\n                return True\n            else:\n                get_console().print(f'[warning]The {dir} already exists. Forcing update')\n                return False\n        return False\n    if package_filter == 'apache-airflow':\n        apache_airflow_documentation_directory = airflow_site_archive_directory / 'apache-airflow'\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            if not airflow_version_dir.exists():\n                get_console().print(f'[warning]The {airflow_version_dir} does not exist. Skipping')\n                continue\n            destination_dir = airflow_version_dir / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {airflow_v}.')\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{airflow_v}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomCoreJob(airflow_version=airflow_v, python_version=python_version, application_root_path=application_root_path, include_provider_dependencies=include_provider_dependencies, target_path=target_sbom_path))\n    elif package_filter == 'apache-airflow-providers':\n        user_confirm('You are about to update sbom information for providers, did you refresh the providers requirements with the command `breeze sbom generate-providers-requirements`?', quit_allowed=False, default_answer=Answer.YES)\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {provider_id} version {provider_version}.')\n            python_versions = set((dir_name.replace('python', '') for dir_name in os.listdir(PROVIDER_REQUIREMENTS_DIR_PATH / node_name)))\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{provider_id}-{provider_version}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomProviderJob(provider_id=provider_id, provider_version=provider_version, python_version=python_version, target_path=target_sbom_path, folder_name=node_name))\n    if len(jobs_to_run) == 0:\n        get_console().print('[info]Nothing to do, there is no job to process')\n        return\n    if run_in_parallel:\n        parallelism = min(parallelism, len(jobs_to_run))\n        get_console().print(f'[info]Running {len(jobs_to_run)} jobs in parallel')\n        with ci_group(f'Generating SBOMs for {jobs_to_run}'):\n            all_params = [f'Generate SBOMs for {job.get_job_name()}' for job in jobs_to_run]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                port_map = get_cdxgen_port_mapping(parallelism, pool)\n                results = [pool.apply_async(produce_sbom_for_application_via_cdxgen_server, kwds={'job': job, 'output': outputs[index], 'port_map': port_map}) for (index, job) in enumerate(jobs_to_run)]\n        check_async_run_results(results=results, success='All SBOMs were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for job in jobs_to_run:\n            produce_sbom_for_application_via_cdxgen_server(job, output=None)\n    html_template = SBOM_INDEX_TEMPLATE\n\n    def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n        destination_index_path = destination_dir / 'index.html'\n        get_console().print(f'[info]Generating index for {destination_dir}')\n        sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n        if not get_dry_run():\n            destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))\n    if package_filter == 'apache-airflow':\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            destination_dir = airflow_version_dir / 'sbom'\n            _generate_index(destination_dir, None, airflow_v)\n    elif package_filter == 'apache-airflow-providers':\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            _generate_index(destination_dir, provider_id, provider_version)",
            "@sbom.command(name='update-sbom-information', help='Update SBOM information in airflow-site project.')\n@click.option('--airflow-site-directory', type=click.Path(file_okay=False, dir_okay=True, path_type=Path, exists=True), required=True, envvar='AIRFLOW_SITE_DIRECTORY', help='Directory where airflow-site directory is located.')\n@click.option('--airflow-version', type=str, required=False, envvar='AIRFLOW_VERSION', help='Version of airflow to update sbom from. (defaulted to all active airflow versions)')\n@option_historical_python_version\n@click.option('--include-provider-dependencies', is_flag=True, help='Whether to include provider dependencies in SBOM generation.')\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update of sbom even if it already exists.')\n@option_verbose\n@option_dry_run\n@option_answer\n@click.option('--package-filter', help='List of packages to consider. You can use `apache-airflow` for core or `apache-airflow-providers` to consider all the providers.', type=BetterChoice(['apache-airflow-providers', 'apache-airflow']), required=False, default='apache-airflow')\ndef update_sbom_information(airflow_site_directory: Path, airflow_version: str | None, python: str | None, include_provider_dependencies: bool, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool, package_filter: tuple[str, ...]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    import jinja2\n    from jinja2 import StrictUndefined\n    from airflow_breeze.utils.cdxgen import produce_sbom_for_application_via_cdxgen_server, start_cdxgen_server\n    from airflow_breeze.utils.github import get_active_airflow_versions\n    if airflow_version is None:\n        airflow_versions = get_active_airflow_versions()\n    else:\n        airflow_versions = [airflow_version]\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    application_root_path = FILES_SBOM_DIR\n    start_cdxgen_server(application_root_path, run_in_parallel, parallelism)\n    jobs_to_run: list[SbomApplicationJob] = []\n    airflow_site_archive_directory = airflow_site_directory / 'docs-archive'\n\n    def _dir_exists_warn_and_should_skip(dir: Path, force: bool) -> bool:\n        if dir.exists():\n            if not force:\n                get_console().print(f'[warning]The {dir} already exists. Skipping')\n                return True\n            else:\n                get_console().print(f'[warning]The {dir} already exists. Forcing update')\n                return False\n        return False\n    if package_filter == 'apache-airflow':\n        apache_airflow_documentation_directory = airflow_site_archive_directory / 'apache-airflow'\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            if not airflow_version_dir.exists():\n                get_console().print(f'[warning]The {airflow_version_dir} does not exist. Skipping')\n                continue\n            destination_dir = airflow_version_dir / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {airflow_v}.')\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{airflow_v}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomCoreJob(airflow_version=airflow_v, python_version=python_version, application_root_path=application_root_path, include_provider_dependencies=include_provider_dependencies, target_path=target_sbom_path))\n    elif package_filter == 'apache-airflow-providers':\n        user_confirm('You are about to update sbom information for providers, did you refresh the providers requirements with the command `breeze sbom generate-providers-requirements`?', quit_allowed=False, default_answer=Answer.YES)\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            if _dir_exists_warn_and_should_skip(destination_dir, force):\n                continue\n            destination_dir.mkdir(parents=True, exist_ok=True)\n            get_console().print(f'[info]Attempting to update sbom for {provider_id} version {provider_version}.')\n            python_versions = set((dir_name.replace('python', '') for dir_name in os.listdir(PROVIDER_REQUIREMENTS_DIR_PATH / node_name)))\n            for python_version in python_versions:\n                target_sbom_file_name = f'apache-airflow-sbom-{provider_id}-{provider_version}-python{python_version}.json'\n                target_sbom_path = destination_dir / target_sbom_file_name\n                if _dir_exists_warn_and_should_skip(target_sbom_path, force):\n                    continue\n                jobs_to_run.append(SbomProviderJob(provider_id=provider_id, provider_version=provider_version, python_version=python_version, target_path=target_sbom_path, folder_name=node_name))\n    if len(jobs_to_run) == 0:\n        get_console().print('[info]Nothing to do, there is no job to process')\n        return\n    if run_in_parallel:\n        parallelism = min(parallelism, len(jobs_to_run))\n        get_console().print(f'[info]Running {len(jobs_to_run)} jobs in parallel')\n        with ci_group(f'Generating SBOMs for {jobs_to_run}'):\n            all_params = [f'Generate SBOMs for {job.get_job_name()}' for job in jobs_to_run]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                port_map = get_cdxgen_port_mapping(parallelism, pool)\n                results = [pool.apply_async(produce_sbom_for_application_via_cdxgen_server, kwds={'job': job, 'output': outputs[index], 'port_map': port_map}) for (index, job) in enumerate(jobs_to_run)]\n        check_async_run_results(results=results, success='All SBOMs were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for job in jobs_to_run:\n            produce_sbom_for_application_via_cdxgen_server(job, output=None)\n    html_template = SBOM_INDEX_TEMPLATE\n\n    def _generate_index(destination_dir: Path, provider_id: str | None, version: str) -> None:\n        destination_index_path = destination_dir / 'index.html'\n        get_console().print(f'[info]Generating index for {destination_dir}')\n        sbom_files = sorted(destination_dir.glob('apache-airflow-sbom-*'))\n        if not get_dry_run():\n            destination_index_path.write_text(jinja2.Template(html_template, autoescape=True, undefined=StrictUndefined).render(provider_id=provider_id, version=version, sbom_files=sbom_files))\n    if package_filter == 'apache-airflow':\n        for airflow_v in airflow_versions:\n            airflow_version_dir = apache_airflow_documentation_directory / airflow_v\n            destination_dir = airflow_version_dir / 'sbom'\n            _generate_index(destination_dir, None, airflow_v)\n    elif package_filter == 'apache-airflow-providers':\n        for (node_name, provider_id, provider_version, provider_version_documentation_directory) in list_providers_from_providers_requirements(airflow_site_archive_directory):\n            destination_dir = provider_version_documentation_directory / 'sbom'\n            _generate_index(destination_dir, provider_id, provider_version)"
        ]
    },
    {
        "func_name": "build_all_airflow_images",
        "original": "@sbom.command(name='build-all-airflow-images', help='Generate images with airflow versions pre-installed')\n@option_historical_python_version\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\ndef build_all_airflow_images(python: str, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool):\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(python_versions))\n        get_console().print(f'[info]Running {len(python_versions)} jobs in parallel')\n        with ci_group(f'Building all airflow base images for python: {python_versions}'):\n            all_params = [f'Building all airflow base image for python: {python_version}' for python_version in python_versions]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=DockerBuildxProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(build_all_airflow_versions_base_image, kwds={'python_version': python_version, 'output': outputs[index]}) for (index, python_version) in enumerate(python_versions)]\n        check_async_run_results(results=results, success='All airflow base images were built successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for python_version in python_versions:\n            build_all_airflow_versions_base_image(python_version=python_version, output=None)",
        "mutated": [
            "@sbom.command(name='build-all-airflow-images', help='Generate images with airflow versions pre-installed')\n@option_historical_python_version\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\ndef build_all_airflow_images(python: str, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool):\n    if False:\n        i = 10\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(python_versions))\n        get_console().print(f'[info]Running {len(python_versions)} jobs in parallel')\n        with ci_group(f'Building all airflow base images for python: {python_versions}'):\n            all_params = [f'Building all airflow base image for python: {python_version}' for python_version in python_versions]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=DockerBuildxProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(build_all_airflow_versions_base_image, kwds={'python_version': python_version, 'output': outputs[index]}) for (index, python_version) in enumerate(python_versions)]\n        check_async_run_results(results=results, success='All airflow base images were built successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for python_version in python_versions:\n            build_all_airflow_versions_base_image(python_version=python_version, output=None)",
            "@sbom.command(name='build-all-airflow-images', help='Generate images with airflow versions pre-installed')\n@option_historical_python_version\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\ndef build_all_airflow_images(python: str, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(python_versions))\n        get_console().print(f'[info]Running {len(python_versions)} jobs in parallel')\n        with ci_group(f'Building all airflow base images for python: {python_versions}'):\n            all_params = [f'Building all airflow base image for python: {python_version}' for python_version in python_versions]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=DockerBuildxProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(build_all_airflow_versions_base_image, kwds={'python_version': python_version, 'output': outputs[index]}) for (index, python_version) in enumerate(python_versions)]\n        check_async_run_results(results=results, success='All airflow base images were built successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for python_version in python_versions:\n            build_all_airflow_versions_base_image(python_version=python_version, output=None)",
            "@sbom.command(name='build-all-airflow-images', help='Generate images with airflow versions pre-installed')\n@option_historical_python_version\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\ndef build_all_airflow_images(python: str, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(python_versions))\n        get_console().print(f'[info]Running {len(python_versions)} jobs in parallel')\n        with ci_group(f'Building all airflow base images for python: {python_versions}'):\n            all_params = [f'Building all airflow base image for python: {python_version}' for python_version in python_versions]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=DockerBuildxProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(build_all_airflow_versions_base_image, kwds={'python_version': python_version, 'output': outputs[index]}) for (index, python_version) in enumerate(python_versions)]\n        check_async_run_results(results=results, success='All airflow base images were built successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for python_version in python_versions:\n            build_all_airflow_versions_base_image(python_version=python_version, output=None)",
            "@sbom.command(name='build-all-airflow-images', help='Generate images with airflow versions pre-installed')\n@option_historical_python_version\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\ndef build_all_airflow_images(python: str, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(python_versions))\n        get_console().print(f'[info]Running {len(python_versions)} jobs in parallel')\n        with ci_group(f'Building all airflow base images for python: {python_versions}'):\n            all_params = [f'Building all airflow base image for python: {python_version}' for python_version in python_versions]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=DockerBuildxProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(build_all_airflow_versions_base_image, kwds={'python_version': python_version, 'output': outputs[index]}) for (index, python_version) in enumerate(python_versions)]\n        check_async_run_results(results=results, success='All airflow base images were built successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for python_version in python_versions:\n            build_all_airflow_versions_base_image(python_version=python_version, output=None)",
            "@sbom.command(name='build-all-airflow-images', help='Generate images with airflow versions pre-installed')\n@option_historical_python_version\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\ndef build_all_airflow_images(python: str, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(python_versions))\n        get_console().print(f'[info]Running {len(python_versions)} jobs in parallel')\n        with ci_group(f'Building all airflow base images for python: {python_versions}'):\n            all_params = [f'Building all airflow base image for python: {python_version}' for python_version in python_versions]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=DockerBuildxProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(build_all_airflow_versions_base_image, kwds={'python_version': python_version, 'output': outputs[index]}) for (index, python_version) in enumerate(python_versions)]\n        check_async_run_results(results=results, success='All airflow base images were built successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for python_version in python_versions:\n            build_all_airflow_versions_base_image(python_version=python_version, output=None)"
        ]
    },
    {
        "func_name": "generate_providers_requirements",
        "original": "@sbom.command(name='generate-providers-requirements', help='Generate requirements for selected provider.')\n@option_historical_python_version\n@click.option('--provider-id', type=BetterChoice(list(PROVIDER_DEPENDENCIES.keys())), required=False, help='Provider id to generate the requirements for')\n@click.option('--provider-version', type=str, required=False, help='Provider version to generate the requirements for i.e `2.1.0`. `latest` is also a supported value to account for the most recent version of the provider')\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update providers requirements even if they already exist.')\ndef generate_providers_requirements(python: str, provider_id: str | None, provider_version: str | None, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool):\n    perform_environment_checks()\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    with open(PROVIDER_METADATA_JSON_FILE_PATH) as f:\n        provider_metadata = json.load(f)\n    if provider_id is None:\n        if provider_version is not None and provider_version != 'latest':\n            get_console().print('[error] You cannot pin the version of the providers if you generate the requirements for all historical or latest versions. --provider-version needs to be unset when you pass None or latest to --provider-id')\n            sys.exit(1)\n        provider_ids = provider_metadata.keys()\n    else:\n        provider_ids = [provider_id]\n    if provider_version is None:\n        user_confirm(f'You are about to generate providers requirements for all historical versions for {len(provider_ids)} provider(s) based on `provider_metadata.json` file. Do you want to proceed?', quit_allowed=False, default_answer=Answer.YES)\n    providers_info = []\n    for provider_id in provider_ids:\n        if provider_version is not None:\n            if provider_version == 'latest':\n                (p_version, info) = list(provider_metadata[provider_id].items())[-1]\n            else:\n                info = provider_metadata[provider_id][provider_version]\n                p_version = provider_version\n            airflow_version = info['associated_airflow_version']\n            providers_info += [(provider_id, p_version, python_version, airflow_version) for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[airflow_version] if python_version in python_versions]\n        else:\n            providers_info += [(provider_id, p_version, python_version, info['associated_airflow_version']) for (p_version, info) in provider_metadata[provider_id].items() for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[info['associated_airflow_version']] if python_version in python_versions]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(providers_info))\n        get_console().print(f'[info]Running {len(providers_info)} jobs in parallel')\n        with ci_group(f'Generating provider requirements for {providers_info}'):\n            all_params = [f'Generate provider requirements for {provider_id} version {provider_version} python {python_version}' for (provider_id, provider_version, python_version, _) in providers_info]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(get_requirements_for_provider, kwds={'provider_id': provider_id, 'airflow_version': airflow_version, 'provider_version': provider_version, 'python_version': python_version, 'force': force, 'output': outputs[index]}) for (index, (provider_id, provider_version, python_version, airflow_version)) in enumerate(providers_info)]\n        check_async_run_results(results=results, success='Providers requirements were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for (provider_id, provider_version, python_version, airflow_version) in providers_info:\n            get_requirements_for_provider(provider_id=provider_id, provider_version=provider_version, airflow_version=airflow_version, python_version=python_version, force=force, output=None)",
        "mutated": [
            "@sbom.command(name='generate-providers-requirements', help='Generate requirements for selected provider.')\n@option_historical_python_version\n@click.option('--provider-id', type=BetterChoice(list(PROVIDER_DEPENDENCIES.keys())), required=False, help='Provider id to generate the requirements for')\n@click.option('--provider-version', type=str, required=False, help='Provider version to generate the requirements for i.e `2.1.0`. `latest` is also a supported value to account for the most recent version of the provider')\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update providers requirements even if they already exist.')\ndef generate_providers_requirements(python: str, provider_id: str | None, provider_version: str | None, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool):\n    if False:\n        i = 10\n    perform_environment_checks()\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    with open(PROVIDER_METADATA_JSON_FILE_PATH) as f:\n        provider_metadata = json.load(f)\n    if provider_id is None:\n        if provider_version is not None and provider_version != 'latest':\n            get_console().print('[error] You cannot pin the version of the providers if you generate the requirements for all historical or latest versions. --provider-version needs to be unset when you pass None or latest to --provider-id')\n            sys.exit(1)\n        provider_ids = provider_metadata.keys()\n    else:\n        provider_ids = [provider_id]\n    if provider_version is None:\n        user_confirm(f'You are about to generate providers requirements for all historical versions for {len(provider_ids)} provider(s) based on `provider_metadata.json` file. Do you want to proceed?', quit_allowed=False, default_answer=Answer.YES)\n    providers_info = []\n    for provider_id in provider_ids:\n        if provider_version is not None:\n            if provider_version == 'latest':\n                (p_version, info) = list(provider_metadata[provider_id].items())[-1]\n            else:\n                info = provider_metadata[provider_id][provider_version]\n                p_version = provider_version\n            airflow_version = info['associated_airflow_version']\n            providers_info += [(provider_id, p_version, python_version, airflow_version) for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[airflow_version] if python_version in python_versions]\n        else:\n            providers_info += [(provider_id, p_version, python_version, info['associated_airflow_version']) for (p_version, info) in provider_metadata[provider_id].items() for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[info['associated_airflow_version']] if python_version in python_versions]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(providers_info))\n        get_console().print(f'[info]Running {len(providers_info)} jobs in parallel')\n        with ci_group(f'Generating provider requirements for {providers_info}'):\n            all_params = [f'Generate provider requirements for {provider_id} version {provider_version} python {python_version}' for (provider_id, provider_version, python_version, _) in providers_info]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(get_requirements_for_provider, kwds={'provider_id': provider_id, 'airflow_version': airflow_version, 'provider_version': provider_version, 'python_version': python_version, 'force': force, 'output': outputs[index]}) for (index, (provider_id, provider_version, python_version, airflow_version)) in enumerate(providers_info)]\n        check_async_run_results(results=results, success='Providers requirements were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for (provider_id, provider_version, python_version, airflow_version) in providers_info:\n            get_requirements_for_provider(provider_id=provider_id, provider_version=provider_version, airflow_version=airflow_version, python_version=python_version, force=force, output=None)",
            "@sbom.command(name='generate-providers-requirements', help='Generate requirements for selected provider.')\n@option_historical_python_version\n@click.option('--provider-id', type=BetterChoice(list(PROVIDER_DEPENDENCIES.keys())), required=False, help='Provider id to generate the requirements for')\n@click.option('--provider-version', type=str, required=False, help='Provider version to generate the requirements for i.e `2.1.0`. `latest` is also a supported value to account for the most recent version of the provider')\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update providers requirements even if they already exist.')\ndef generate_providers_requirements(python: str, provider_id: str | None, provider_version: str | None, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    perform_environment_checks()\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    with open(PROVIDER_METADATA_JSON_FILE_PATH) as f:\n        provider_metadata = json.load(f)\n    if provider_id is None:\n        if provider_version is not None and provider_version != 'latest':\n            get_console().print('[error] You cannot pin the version of the providers if you generate the requirements for all historical or latest versions. --provider-version needs to be unset when you pass None or latest to --provider-id')\n            sys.exit(1)\n        provider_ids = provider_metadata.keys()\n    else:\n        provider_ids = [provider_id]\n    if provider_version is None:\n        user_confirm(f'You are about to generate providers requirements for all historical versions for {len(provider_ids)} provider(s) based on `provider_metadata.json` file. Do you want to proceed?', quit_allowed=False, default_answer=Answer.YES)\n    providers_info = []\n    for provider_id in provider_ids:\n        if provider_version is not None:\n            if provider_version == 'latest':\n                (p_version, info) = list(provider_metadata[provider_id].items())[-1]\n            else:\n                info = provider_metadata[provider_id][provider_version]\n                p_version = provider_version\n            airflow_version = info['associated_airflow_version']\n            providers_info += [(provider_id, p_version, python_version, airflow_version) for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[airflow_version] if python_version in python_versions]\n        else:\n            providers_info += [(provider_id, p_version, python_version, info['associated_airflow_version']) for (p_version, info) in provider_metadata[provider_id].items() for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[info['associated_airflow_version']] if python_version in python_versions]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(providers_info))\n        get_console().print(f'[info]Running {len(providers_info)} jobs in parallel')\n        with ci_group(f'Generating provider requirements for {providers_info}'):\n            all_params = [f'Generate provider requirements for {provider_id} version {provider_version} python {python_version}' for (provider_id, provider_version, python_version, _) in providers_info]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(get_requirements_for_provider, kwds={'provider_id': provider_id, 'airflow_version': airflow_version, 'provider_version': provider_version, 'python_version': python_version, 'force': force, 'output': outputs[index]}) for (index, (provider_id, provider_version, python_version, airflow_version)) in enumerate(providers_info)]\n        check_async_run_results(results=results, success='Providers requirements were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for (provider_id, provider_version, python_version, airflow_version) in providers_info:\n            get_requirements_for_provider(provider_id=provider_id, provider_version=provider_version, airflow_version=airflow_version, python_version=python_version, force=force, output=None)",
            "@sbom.command(name='generate-providers-requirements', help='Generate requirements for selected provider.')\n@option_historical_python_version\n@click.option('--provider-id', type=BetterChoice(list(PROVIDER_DEPENDENCIES.keys())), required=False, help='Provider id to generate the requirements for')\n@click.option('--provider-version', type=str, required=False, help='Provider version to generate the requirements for i.e `2.1.0`. `latest` is also a supported value to account for the most recent version of the provider')\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update providers requirements even if they already exist.')\ndef generate_providers_requirements(python: str, provider_id: str | None, provider_version: str | None, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    perform_environment_checks()\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    with open(PROVIDER_METADATA_JSON_FILE_PATH) as f:\n        provider_metadata = json.load(f)\n    if provider_id is None:\n        if provider_version is not None and provider_version != 'latest':\n            get_console().print('[error] You cannot pin the version of the providers if you generate the requirements for all historical or latest versions. --provider-version needs to be unset when you pass None or latest to --provider-id')\n            sys.exit(1)\n        provider_ids = provider_metadata.keys()\n    else:\n        provider_ids = [provider_id]\n    if provider_version is None:\n        user_confirm(f'You are about to generate providers requirements for all historical versions for {len(provider_ids)} provider(s) based on `provider_metadata.json` file. Do you want to proceed?', quit_allowed=False, default_answer=Answer.YES)\n    providers_info = []\n    for provider_id in provider_ids:\n        if provider_version is not None:\n            if provider_version == 'latest':\n                (p_version, info) = list(provider_metadata[provider_id].items())[-1]\n            else:\n                info = provider_metadata[provider_id][provider_version]\n                p_version = provider_version\n            airflow_version = info['associated_airflow_version']\n            providers_info += [(provider_id, p_version, python_version, airflow_version) for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[airflow_version] if python_version in python_versions]\n        else:\n            providers_info += [(provider_id, p_version, python_version, info['associated_airflow_version']) for (p_version, info) in provider_metadata[provider_id].items() for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[info['associated_airflow_version']] if python_version in python_versions]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(providers_info))\n        get_console().print(f'[info]Running {len(providers_info)} jobs in parallel')\n        with ci_group(f'Generating provider requirements for {providers_info}'):\n            all_params = [f'Generate provider requirements for {provider_id} version {provider_version} python {python_version}' for (provider_id, provider_version, python_version, _) in providers_info]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(get_requirements_for_provider, kwds={'provider_id': provider_id, 'airflow_version': airflow_version, 'provider_version': provider_version, 'python_version': python_version, 'force': force, 'output': outputs[index]}) for (index, (provider_id, provider_version, python_version, airflow_version)) in enumerate(providers_info)]\n        check_async_run_results(results=results, success='Providers requirements were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for (provider_id, provider_version, python_version, airflow_version) in providers_info:\n            get_requirements_for_provider(provider_id=provider_id, provider_version=provider_version, airflow_version=airflow_version, python_version=python_version, force=force, output=None)",
            "@sbom.command(name='generate-providers-requirements', help='Generate requirements for selected provider.')\n@option_historical_python_version\n@click.option('--provider-id', type=BetterChoice(list(PROVIDER_DEPENDENCIES.keys())), required=False, help='Provider id to generate the requirements for')\n@click.option('--provider-version', type=str, required=False, help='Provider version to generate the requirements for i.e `2.1.0`. `latest` is also a supported value to account for the most recent version of the provider')\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update providers requirements even if they already exist.')\ndef generate_providers_requirements(python: str, provider_id: str | None, provider_version: str | None, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    perform_environment_checks()\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    with open(PROVIDER_METADATA_JSON_FILE_PATH) as f:\n        provider_metadata = json.load(f)\n    if provider_id is None:\n        if provider_version is not None and provider_version != 'latest':\n            get_console().print('[error] You cannot pin the version of the providers if you generate the requirements for all historical or latest versions. --provider-version needs to be unset when you pass None or latest to --provider-id')\n            sys.exit(1)\n        provider_ids = provider_metadata.keys()\n    else:\n        provider_ids = [provider_id]\n    if provider_version is None:\n        user_confirm(f'You are about to generate providers requirements for all historical versions for {len(provider_ids)} provider(s) based on `provider_metadata.json` file. Do you want to proceed?', quit_allowed=False, default_answer=Answer.YES)\n    providers_info = []\n    for provider_id in provider_ids:\n        if provider_version is not None:\n            if provider_version == 'latest':\n                (p_version, info) = list(provider_metadata[provider_id].items())[-1]\n            else:\n                info = provider_metadata[provider_id][provider_version]\n                p_version = provider_version\n            airflow_version = info['associated_airflow_version']\n            providers_info += [(provider_id, p_version, python_version, airflow_version) for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[airflow_version] if python_version in python_versions]\n        else:\n            providers_info += [(provider_id, p_version, python_version, info['associated_airflow_version']) for (p_version, info) in provider_metadata[provider_id].items() for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[info['associated_airflow_version']] if python_version in python_versions]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(providers_info))\n        get_console().print(f'[info]Running {len(providers_info)} jobs in parallel')\n        with ci_group(f'Generating provider requirements for {providers_info}'):\n            all_params = [f'Generate provider requirements for {provider_id} version {provider_version} python {python_version}' for (provider_id, provider_version, python_version, _) in providers_info]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(get_requirements_for_provider, kwds={'provider_id': provider_id, 'airflow_version': airflow_version, 'provider_version': provider_version, 'python_version': python_version, 'force': force, 'output': outputs[index]}) for (index, (provider_id, provider_version, python_version, airflow_version)) in enumerate(providers_info)]\n        check_async_run_results(results=results, success='Providers requirements were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for (provider_id, provider_version, python_version, airflow_version) in providers_info:\n            get_requirements_for_provider(provider_id=provider_id, provider_version=provider_version, airflow_version=airflow_version, python_version=python_version, force=force, output=None)",
            "@sbom.command(name='generate-providers-requirements', help='Generate requirements for selected provider.')\n@option_historical_python_version\n@click.option('--provider-id', type=BetterChoice(list(PROVIDER_DEPENDENCIES.keys())), required=False, help='Provider id to generate the requirements for')\n@click.option('--provider-version', type=str, required=False, help='Provider version to generate the requirements for i.e `2.1.0`. `latest` is also a supported value to account for the most recent version of the provider')\n@option_verbose\n@option_dry_run\n@option_answer\n@option_run_in_parallel\n@option_parallelism\n@option_debug_resources\n@option_include_success_outputs\n@option_skip_cleanup\n@click.option('--force', is_flag=True, help='Force update providers requirements even if they already exist.')\ndef generate_providers_requirements(python: str, provider_id: str | None, provider_version: str | None, run_in_parallel: bool, parallelism: int, debug_resources: bool, include_success_outputs: bool, skip_cleanup: bool, force: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    perform_environment_checks()\n    if python is None:\n        python_versions = ALL_HISTORICAL_PYTHON_VERSIONS\n    else:\n        python_versions = [python]\n    with open(PROVIDER_METADATA_JSON_FILE_PATH) as f:\n        provider_metadata = json.load(f)\n    if provider_id is None:\n        if provider_version is not None and provider_version != 'latest':\n            get_console().print('[error] You cannot pin the version of the providers if you generate the requirements for all historical or latest versions. --provider-version needs to be unset when you pass None or latest to --provider-id')\n            sys.exit(1)\n        provider_ids = provider_metadata.keys()\n    else:\n        provider_ids = [provider_id]\n    if provider_version is None:\n        user_confirm(f'You are about to generate providers requirements for all historical versions for {len(provider_ids)} provider(s) based on `provider_metadata.json` file. Do you want to proceed?', quit_allowed=False, default_answer=Answer.YES)\n    providers_info = []\n    for provider_id in provider_ids:\n        if provider_version is not None:\n            if provider_version == 'latest':\n                (p_version, info) = list(provider_metadata[provider_id].items())[-1]\n            else:\n                info = provider_metadata[provider_id][provider_version]\n                p_version = provider_version\n            airflow_version = info['associated_airflow_version']\n            providers_info += [(provider_id, p_version, python_version, airflow_version) for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[airflow_version] if python_version in python_versions]\n        else:\n            providers_info += [(provider_id, p_version, python_version, info['associated_airflow_version']) for (p_version, info) in provider_metadata[provider_id].items() for python_version in AIRFLOW_PYTHON_COMPATIBILITY_MATRIX[info['associated_airflow_version']] if python_version in python_versions]\n    if run_in_parallel:\n        parallelism = min(parallelism, len(providers_info))\n        get_console().print(f'[info]Running {len(providers_info)} jobs in parallel')\n        with ci_group(f'Generating provider requirements for {providers_info}'):\n            all_params = [f'Generate provider requirements for {provider_id} version {provider_version} python {python_version}' for (provider_id, provider_version, python_version, _) in providers_info]\n            with run_with_pool(parallelism=parallelism, all_params=all_params, debug_resources=debug_resources, progress_matcher=ShowLastLineProgressMatcher()) as (pool, outputs):\n                results = [pool.apply_async(get_requirements_for_provider, kwds={'provider_id': provider_id, 'airflow_version': airflow_version, 'provider_version': provider_version, 'python_version': python_version, 'force': force, 'output': outputs[index]}) for (index, (provider_id, provider_version, python_version, airflow_version)) in enumerate(providers_info)]\n        check_async_run_results(results=results, success='Providers requirements were generated successfully', outputs=outputs, include_success_outputs=include_success_outputs, skip_cleanup=skip_cleanup)\n    else:\n        for (provider_id, provider_version, python_version, airflow_version) in providers_info:\n            get_requirements_for_provider(provider_id=provider_id, provider_version=provider_version, airflow_version=airflow_version, python_version=python_version, force=force, output=None)"
        ]
    }
]