[
    {
        "func_name": "__init__",
        "original": "def __init__(self, channels: int, reduction_ratio: float=0.25, gate_layer: Optional[Callable[..., nn.Module]]=None, activation_layer: Optional[Callable[..., nn.Module]]=None):\n    super().__init__()\n    rd_channels = make_divisible(channels * reduction_ratio, 8)\n    gate_layer = gate_layer or nn.Hardsigmoid\n    activation_layer = activation_layer or nn.ReLU\n    self.conv_reduce = MutableConv2d(channels, rd_channels, 1, bias=True)\n    self.act1 = activation_layer(inplace=True)\n    self.conv_expand = MutableConv2d(rd_channels, channels, 1, bias=True)\n    self.gate = gate_layer()",
        "mutated": [
            "def __init__(self, channels: int, reduction_ratio: float=0.25, gate_layer: Optional[Callable[..., nn.Module]]=None, activation_layer: Optional[Callable[..., nn.Module]]=None):\n    if False:\n        i = 10\n    super().__init__()\n    rd_channels = make_divisible(channels * reduction_ratio, 8)\n    gate_layer = gate_layer or nn.Hardsigmoid\n    activation_layer = activation_layer or nn.ReLU\n    self.conv_reduce = MutableConv2d(channels, rd_channels, 1, bias=True)\n    self.act1 = activation_layer(inplace=True)\n    self.conv_expand = MutableConv2d(rd_channels, channels, 1, bias=True)\n    self.gate = gate_layer()",
            "def __init__(self, channels: int, reduction_ratio: float=0.25, gate_layer: Optional[Callable[..., nn.Module]]=None, activation_layer: Optional[Callable[..., nn.Module]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    rd_channels = make_divisible(channels * reduction_ratio, 8)\n    gate_layer = gate_layer or nn.Hardsigmoid\n    activation_layer = activation_layer or nn.ReLU\n    self.conv_reduce = MutableConv2d(channels, rd_channels, 1, bias=True)\n    self.act1 = activation_layer(inplace=True)\n    self.conv_expand = MutableConv2d(rd_channels, channels, 1, bias=True)\n    self.gate = gate_layer()",
            "def __init__(self, channels: int, reduction_ratio: float=0.25, gate_layer: Optional[Callable[..., nn.Module]]=None, activation_layer: Optional[Callable[..., nn.Module]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    rd_channels = make_divisible(channels * reduction_ratio, 8)\n    gate_layer = gate_layer or nn.Hardsigmoid\n    activation_layer = activation_layer or nn.ReLU\n    self.conv_reduce = MutableConv2d(channels, rd_channels, 1, bias=True)\n    self.act1 = activation_layer(inplace=True)\n    self.conv_expand = MutableConv2d(rd_channels, channels, 1, bias=True)\n    self.gate = gate_layer()",
            "def __init__(self, channels: int, reduction_ratio: float=0.25, gate_layer: Optional[Callable[..., nn.Module]]=None, activation_layer: Optional[Callable[..., nn.Module]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    rd_channels = make_divisible(channels * reduction_ratio, 8)\n    gate_layer = gate_layer or nn.Hardsigmoid\n    activation_layer = activation_layer or nn.ReLU\n    self.conv_reduce = MutableConv2d(channels, rd_channels, 1, bias=True)\n    self.act1 = activation_layer(inplace=True)\n    self.conv_expand = MutableConv2d(rd_channels, channels, 1, bias=True)\n    self.gate = gate_layer()",
            "def __init__(self, channels: int, reduction_ratio: float=0.25, gate_layer: Optional[Callable[..., nn.Module]]=None, activation_layer: Optional[Callable[..., nn.Module]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    rd_channels = make_divisible(channels * reduction_ratio, 8)\n    gate_layer = gate_layer or nn.Hardsigmoid\n    activation_layer = activation_layer or nn.ReLU\n    self.conv_reduce = MutableConv2d(channels, rd_channels, 1, bias=True)\n    self.act1 = activation_layer(inplace=True)\n    self.conv_expand = MutableConv2d(rd_channels, channels, 1, bias=True)\n    self.gate = gate_layer()"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    x_se = x.mean((2, 3), keepdim=True)\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    x_se = x.mean((2, 3), keepdim=True)\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x_se = x.mean((2, 3), keepdim=True)\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x_se = x.mean((2, 3), keepdim=True)\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x_se = x.mean((2, 3), keepdim=True)\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x_se = x.mean((2, 3), keepdim=True)\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)"
        ]
    },
    {
        "func_name": "_se_or_skip",
        "original": "def _se_or_skip(hidden_ch: int, input_ch: int, optional: bool, se_from_exp: bool, label: str) -> nn.Module:\n    ch = hidden_ch if se_from_exp else input_ch\n    if optional:\n        return LayerChoice({'identity': nn.Identity(), 'se': SqueezeExcite(ch)}, label=label)\n    else:\n        return SqueezeExcite(ch)",
        "mutated": [
            "def _se_or_skip(hidden_ch: int, input_ch: int, optional: bool, se_from_exp: bool, label: str) -> nn.Module:\n    if False:\n        i = 10\n    ch = hidden_ch if se_from_exp else input_ch\n    if optional:\n        return LayerChoice({'identity': nn.Identity(), 'se': SqueezeExcite(ch)}, label=label)\n    else:\n        return SqueezeExcite(ch)",
            "def _se_or_skip(hidden_ch: int, input_ch: int, optional: bool, se_from_exp: bool, label: str) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ch = hidden_ch if se_from_exp else input_ch\n    if optional:\n        return LayerChoice({'identity': nn.Identity(), 'se': SqueezeExcite(ch)}, label=label)\n    else:\n        return SqueezeExcite(ch)",
            "def _se_or_skip(hidden_ch: int, input_ch: int, optional: bool, se_from_exp: bool, label: str) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ch = hidden_ch if se_from_exp else input_ch\n    if optional:\n        return LayerChoice({'identity': nn.Identity(), 'se': SqueezeExcite(ch)}, label=label)\n    else:\n        return SqueezeExcite(ch)",
            "def _se_or_skip(hidden_ch: int, input_ch: int, optional: bool, se_from_exp: bool, label: str) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ch = hidden_ch if se_from_exp else input_ch\n    if optional:\n        return LayerChoice({'identity': nn.Identity(), 'se': SqueezeExcite(ch)}, label=label)\n    else:\n        return SqueezeExcite(ch)",
            "def _se_or_skip(hidden_ch: int, input_ch: int, optional: bool, se_from_exp: bool, label: str) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ch = hidden_ch if se_from_exp else input_ch\n    if optional:\n        return LayerChoice({'identity': nn.Identity(), 'se': SqueezeExcite(ch)}, label=label)\n    else:\n        return SqueezeExcite(ch)"
        ]
    },
    {
        "func_name": "_act_fn",
        "original": "def _act_fn(act_alias: Literal['hswish', 'swish', 'relu']) -> Type[nn.Module]:\n    if act_alias == 'hswish':\n        return nn.Hardswish\n    elif act_alias == 'swish':\n        return nn.SiLU\n    elif act_alias == 'relu':\n        return nn.ReLU\n    else:\n        raise ValueError(f'Unsupported act alias: {act_alias}')",
        "mutated": [
            "def _act_fn(act_alias: Literal['hswish', 'swish', 'relu']) -> Type[nn.Module]:\n    if False:\n        i = 10\n    if act_alias == 'hswish':\n        return nn.Hardswish\n    elif act_alias == 'swish':\n        return nn.SiLU\n    elif act_alias == 'relu':\n        return nn.ReLU\n    else:\n        raise ValueError(f'Unsupported act alias: {act_alias}')",
            "def _act_fn(act_alias: Literal['hswish', 'swish', 'relu']) -> Type[nn.Module]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if act_alias == 'hswish':\n        return nn.Hardswish\n    elif act_alias == 'swish':\n        return nn.SiLU\n    elif act_alias == 'relu':\n        return nn.ReLU\n    else:\n        raise ValueError(f'Unsupported act alias: {act_alias}')",
            "def _act_fn(act_alias: Literal['hswish', 'swish', 'relu']) -> Type[nn.Module]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if act_alias == 'hswish':\n        return nn.Hardswish\n    elif act_alias == 'swish':\n        return nn.SiLU\n    elif act_alias == 'relu':\n        return nn.ReLU\n    else:\n        raise ValueError(f'Unsupported act alias: {act_alias}')",
            "def _act_fn(act_alias: Literal['hswish', 'swish', 'relu']) -> Type[nn.Module]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if act_alias == 'hswish':\n        return nn.Hardswish\n    elif act_alias == 'swish':\n        return nn.SiLU\n    elif act_alias == 'relu':\n        return nn.ReLU\n    else:\n        raise ValueError(f'Unsupported act alias: {act_alias}')",
            "def _act_fn(act_alias: Literal['hswish', 'swish', 'relu']) -> Type[nn.Module]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if act_alias == 'hswish':\n        return nn.Hardswish\n    elif act_alias == 'swish':\n        return nn.SiLU\n    elif act_alias == 'relu':\n        return nn.ReLU\n    else:\n        raise ValueError(f'Unsupported act alias: {act_alias}')"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, num_labels: int=1000, base_widths: Tuple[int, ...]=(16, 16, 16, 32, 64, 128, 256, 512, 1024), width_multipliers: Union[Tuple[float, ...], float]=(0.5, 0.625, 0.75, 1.0, 1.25, 1.5, 2.0), expand_ratios: Tuple[float, ...]=(1.0, 2.0, 3.0, 4.0, 5.0, 6.0), squeeze_excite: Tuple[Literal['force', 'optional', 'none'], ...]=('none', 'none', 'optional', 'none', 'optional', 'optional'), depth_range: Union[List[Tuple[int, int]], Tuple[int, int]]=(1, 4), stride: Tuple[int, ...]=(2, 1, 2, 2, 2, 1, 2, 1, 1), activation: Tuple[Literal['hswish', 'swish', 'relu'], ...]=('hswish', 'relu', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'), se_from_exp: bool=True, dropout_rate: float=0.2, bn_eps: float=0.001, bn_momentum: float=0.1):\n    super().__init__()\n    self.num_blocks = len(base_widths) - 1\n    assert self.num_blocks >= 4\n    assert len(base_widths) == len(stride) == len(activation) == self.num_blocks + 1\n    assert len(squeeze_excite) == self.num_blocks - 2 and all((se in ['force', 'optional', 'none'] for se in squeeze_excite))\n    if isinstance(depth_range[0], int):\n        depth_range = cast(Tuple[int, int], depth_range)\n        assert len(depth_range) == 2 and depth_range[1] >= depth_range[0] >= 1\n        self.depth_range = [depth_range] * (self.num_blocks - 3)\n    else:\n        assert len(depth_range) == self.num_blocks - 3\n        self.depth_range = cast(List[Tuple[int, int]], depth_range)\n        for d in self.depth_range:\n            d = cast(Tuple[int, int], d)\n            assert len(d) == 2 and d[1] >= d[0] >= 1, f'{d} does not satisfy depth constraints'\n    self.widths = []\n    for (i, base_width) in enumerate(base_widths):\n        if isinstance(width_multipliers, float):\n            self.widths.append(make_divisible(base_width * width_multipliers, 8))\n        else:\n            self.widths.append(make_divisible(nni.choice(f's{max(i - 1, 0)}_width_mult', list(width_multipliers)) * base_width, 8))\n    self.expand_ratios = expand_ratios\n    self.se_from_exp = se_from_exp\n    self.stem = ConvBNReLU(3, self.widths[0], nni.choice(f'stem_ks', [3, 5]), stride=stride[0], activation_layer=_act_fn(activation[0]))\n    blocks: List[nn.Module] = [DepthwiseSeparableConv(self.widths[0], self.widths[1], nni.choice(f's0_i0_ks', [3, 5, 7]), stride=stride[1], squeeze_excite=cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=squeeze_excite[0] == 'optional', se_from_exp=self.se_from_exp, label=f's0_i0_se')) if squeeze_excite[0] != 'none' else None, activation_layer=_act_fn(activation[1]))]\n    blocks += [self._make_stage(i, self.widths[i], self.widths[i + 1], squeeze_excite[i], stride[i + 1], _act_fn(activation[i + 1])) for i in range(1, self.num_blocks - 2)]\n    blocks += [ConvBNReLU(self.widths[self.num_blocks - 2], self.widths[self.num_blocks - 1], kernel_size=1, stride=stride[self.num_blocks - 1], activation_layer=_act_fn(activation[self.num_blocks - 1])), nn.AdaptiveAvgPool2d(1), ConvBNReLU(self.widths[self.num_blocks - 1], self.widths[self.num_blocks], kernel_size=1, stride=stride[self.num_blocks], norm_layer=nn.Identity, activation_layer=_act_fn(activation[self.num_blocks]))]\n    self.blocks = nn.Sequential(*blocks)\n    self.classifier = nn.Sequential(nn.Dropout(dropout_rate), MutableLinear(cast(int, self.widths[self.num_blocks]), num_labels))\n    reset_parameters(self, bn_momentum=bn_momentum, bn_eps=bn_eps)",
        "mutated": [
            "def __init__(self, num_labels: int=1000, base_widths: Tuple[int, ...]=(16, 16, 16, 32, 64, 128, 256, 512, 1024), width_multipliers: Union[Tuple[float, ...], float]=(0.5, 0.625, 0.75, 1.0, 1.25, 1.5, 2.0), expand_ratios: Tuple[float, ...]=(1.0, 2.0, 3.0, 4.0, 5.0, 6.0), squeeze_excite: Tuple[Literal['force', 'optional', 'none'], ...]=('none', 'none', 'optional', 'none', 'optional', 'optional'), depth_range: Union[List[Tuple[int, int]], Tuple[int, int]]=(1, 4), stride: Tuple[int, ...]=(2, 1, 2, 2, 2, 1, 2, 1, 1), activation: Tuple[Literal['hswish', 'swish', 'relu'], ...]=('hswish', 'relu', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'), se_from_exp: bool=True, dropout_rate: float=0.2, bn_eps: float=0.001, bn_momentum: float=0.1):\n    if False:\n        i = 10\n    super().__init__()\n    self.num_blocks = len(base_widths) - 1\n    assert self.num_blocks >= 4\n    assert len(base_widths) == len(stride) == len(activation) == self.num_blocks + 1\n    assert len(squeeze_excite) == self.num_blocks - 2 and all((se in ['force', 'optional', 'none'] for se in squeeze_excite))\n    if isinstance(depth_range[0], int):\n        depth_range = cast(Tuple[int, int], depth_range)\n        assert len(depth_range) == 2 and depth_range[1] >= depth_range[0] >= 1\n        self.depth_range = [depth_range] * (self.num_blocks - 3)\n    else:\n        assert len(depth_range) == self.num_blocks - 3\n        self.depth_range = cast(List[Tuple[int, int]], depth_range)\n        for d in self.depth_range:\n            d = cast(Tuple[int, int], d)\n            assert len(d) == 2 and d[1] >= d[0] >= 1, f'{d} does not satisfy depth constraints'\n    self.widths = []\n    for (i, base_width) in enumerate(base_widths):\n        if isinstance(width_multipliers, float):\n            self.widths.append(make_divisible(base_width * width_multipliers, 8))\n        else:\n            self.widths.append(make_divisible(nni.choice(f's{max(i - 1, 0)}_width_mult', list(width_multipliers)) * base_width, 8))\n    self.expand_ratios = expand_ratios\n    self.se_from_exp = se_from_exp\n    self.stem = ConvBNReLU(3, self.widths[0], nni.choice(f'stem_ks', [3, 5]), stride=stride[0], activation_layer=_act_fn(activation[0]))\n    blocks: List[nn.Module] = [DepthwiseSeparableConv(self.widths[0], self.widths[1], nni.choice(f's0_i0_ks', [3, 5, 7]), stride=stride[1], squeeze_excite=cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=squeeze_excite[0] == 'optional', se_from_exp=self.se_from_exp, label=f's0_i0_se')) if squeeze_excite[0] != 'none' else None, activation_layer=_act_fn(activation[1]))]\n    blocks += [self._make_stage(i, self.widths[i], self.widths[i + 1], squeeze_excite[i], stride[i + 1], _act_fn(activation[i + 1])) for i in range(1, self.num_blocks - 2)]\n    blocks += [ConvBNReLU(self.widths[self.num_blocks - 2], self.widths[self.num_blocks - 1], kernel_size=1, stride=stride[self.num_blocks - 1], activation_layer=_act_fn(activation[self.num_blocks - 1])), nn.AdaptiveAvgPool2d(1), ConvBNReLU(self.widths[self.num_blocks - 1], self.widths[self.num_blocks], kernel_size=1, stride=stride[self.num_blocks], norm_layer=nn.Identity, activation_layer=_act_fn(activation[self.num_blocks]))]\n    self.blocks = nn.Sequential(*blocks)\n    self.classifier = nn.Sequential(nn.Dropout(dropout_rate), MutableLinear(cast(int, self.widths[self.num_blocks]), num_labels))\n    reset_parameters(self, bn_momentum=bn_momentum, bn_eps=bn_eps)",
            "def __init__(self, num_labels: int=1000, base_widths: Tuple[int, ...]=(16, 16, 16, 32, 64, 128, 256, 512, 1024), width_multipliers: Union[Tuple[float, ...], float]=(0.5, 0.625, 0.75, 1.0, 1.25, 1.5, 2.0), expand_ratios: Tuple[float, ...]=(1.0, 2.0, 3.0, 4.0, 5.0, 6.0), squeeze_excite: Tuple[Literal['force', 'optional', 'none'], ...]=('none', 'none', 'optional', 'none', 'optional', 'optional'), depth_range: Union[List[Tuple[int, int]], Tuple[int, int]]=(1, 4), stride: Tuple[int, ...]=(2, 1, 2, 2, 2, 1, 2, 1, 1), activation: Tuple[Literal['hswish', 'swish', 'relu'], ...]=('hswish', 'relu', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'), se_from_exp: bool=True, dropout_rate: float=0.2, bn_eps: float=0.001, bn_momentum: float=0.1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.num_blocks = len(base_widths) - 1\n    assert self.num_blocks >= 4\n    assert len(base_widths) == len(stride) == len(activation) == self.num_blocks + 1\n    assert len(squeeze_excite) == self.num_blocks - 2 and all((se in ['force', 'optional', 'none'] for se in squeeze_excite))\n    if isinstance(depth_range[0], int):\n        depth_range = cast(Tuple[int, int], depth_range)\n        assert len(depth_range) == 2 and depth_range[1] >= depth_range[0] >= 1\n        self.depth_range = [depth_range] * (self.num_blocks - 3)\n    else:\n        assert len(depth_range) == self.num_blocks - 3\n        self.depth_range = cast(List[Tuple[int, int]], depth_range)\n        for d in self.depth_range:\n            d = cast(Tuple[int, int], d)\n            assert len(d) == 2 and d[1] >= d[0] >= 1, f'{d} does not satisfy depth constraints'\n    self.widths = []\n    for (i, base_width) in enumerate(base_widths):\n        if isinstance(width_multipliers, float):\n            self.widths.append(make_divisible(base_width * width_multipliers, 8))\n        else:\n            self.widths.append(make_divisible(nni.choice(f's{max(i - 1, 0)}_width_mult', list(width_multipliers)) * base_width, 8))\n    self.expand_ratios = expand_ratios\n    self.se_from_exp = se_from_exp\n    self.stem = ConvBNReLU(3, self.widths[0], nni.choice(f'stem_ks', [3, 5]), stride=stride[0], activation_layer=_act_fn(activation[0]))\n    blocks: List[nn.Module] = [DepthwiseSeparableConv(self.widths[0], self.widths[1], nni.choice(f's0_i0_ks', [3, 5, 7]), stride=stride[1], squeeze_excite=cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=squeeze_excite[0] == 'optional', se_from_exp=self.se_from_exp, label=f's0_i0_se')) if squeeze_excite[0] != 'none' else None, activation_layer=_act_fn(activation[1]))]\n    blocks += [self._make_stage(i, self.widths[i], self.widths[i + 1], squeeze_excite[i], stride[i + 1], _act_fn(activation[i + 1])) for i in range(1, self.num_blocks - 2)]\n    blocks += [ConvBNReLU(self.widths[self.num_blocks - 2], self.widths[self.num_blocks - 1], kernel_size=1, stride=stride[self.num_blocks - 1], activation_layer=_act_fn(activation[self.num_blocks - 1])), nn.AdaptiveAvgPool2d(1), ConvBNReLU(self.widths[self.num_blocks - 1], self.widths[self.num_blocks], kernel_size=1, stride=stride[self.num_blocks], norm_layer=nn.Identity, activation_layer=_act_fn(activation[self.num_blocks]))]\n    self.blocks = nn.Sequential(*blocks)\n    self.classifier = nn.Sequential(nn.Dropout(dropout_rate), MutableLinear(cast(int, self.widths[self.num_blocks]), num_labels))\n    reset_parameters(self, bn_momentum=bn_momentum, bn_eps=bn_eps)",
            "def __init__(self, num_labels: int=1000, base_widths: Tuple[int, ...]=(16, 16, 16, 32, 64, 128, 256, 512, 1024), width_multipliers: Union[Tuple[float, ...], float]=(0.5, 0.625, 0.75, 1.0, 1.25, 1.5, 2.0), expand_ratios: Tuple[float, ...]=(1.0, 2.0, 3.0, 4.0, 5.0, 6.0), squeeze_excite: Tuple[Literal['force', 'optional', 'none'], ...]=('none', 'none', 'optional', 'none', 'optional', 'optional'), depth_range: Union[List[Tuple[int, int]], Tuple[int, int]]=(1, 4), stride: Tuple[int, ...]=(2, 1, 2, 2, 2, 1, 2, 1, 1), activation: Tuple[Literal['hswish', 'swish', 'relu'], ...]=('hswish', 'relu', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'), se_from_exp: bool=True, dropout_rate: float=0.2, bn_eps: float=0.001, bn_momentum: float=0.1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.num_blocks = len(base_widths) - 1\n    assert self.num_blocks >= 4\n    assert len(base_widths) == len(stride) == len(activation) == self.num_blocks + 1\n    assert len(squeeze_excite) == self.num_blocks - 2 and all((se in ['force', 'optional', 'none'] for se in squeeze_excite))\n    if isinstance(depth_range[0], int):\n        depth_range = cast(Tuple[int, int], depth_range)\n        assert len(depth_range) == 2 and depth_range[1] >= depth_range[0] >= 1\n        self.depth_range = [depth_range] * (self.num_blocks - 3)\n    else:\n        assert len(depth_range) == self.num_blocks - 3\n        self.depth_range = cast(List[Tuple[int, int]], depth_range)\n        for d in self.depth_range:\n            d = cast(Tuple[int, int], d)\n            assert len(d) == 2 and d[1] >= d[0] >= 1, f'{d} does not satisfy depth constraints'\n    self.widths = []\n    for (i, base_width) in enumerate(base_widths):\n        if isinstance(width_multipliers, float):\n            self.widths.append(make_divisible(base_width * width_multipliers, 8))\n        else:\n            self.widths.append(make_divisible(nni.choice(f's{max(i - 1, 0)}_width_mult', list(width_multipliers)) * base_width, 8))\n    self.expand_ratios = expand_ratios\n    self.se_from_exp = se_from_exp\n    self.stem = ConvBNReLU(3, self.widths[0], nni.choice(f'stem_ks', [3, 5]), stride=stride[0], activation_layer=_act_fn(activation[0]))\n    blocks: List[nn.Module] = [DepthwiseSeparableConv(self.widths[0], self.widths[1], nni.choice(f's0_i0_ks', [3, 5, 7]), stride=stride[1], squeeze_excite=cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=squeeze_excite[0] == 'optional', se_from_exp=self.se_from_exp, label=f's0_i0_se')) if squeeze_excite[0] != 'none' else None, activation_layer=_act_fn(activation[1]))]\n    blocks += [self._make_stage(i, self.widths[i], self.widths[i + 1], squeeze_excite[i], stride[i + 1], _act_fn(activation[i + 1])) for i in range(1, self.num_blocks - 2)]\n    blocks += [ConvBNReLU(self.widths[self.num_blocks - 2], self.widths[self.num_blocks - 1], kernel_size=1, stride=stride[self.num_blocks - 1], activation_layer=_act_fn(activation[self.num_blocks - 1])), nn.AdaptiveAvgPool2d(1), ConvBNReLU(self.widths[self.num_blocks - 1], self.widths[self.num_blocks], kernel_size=1, stride=stride[self.num_blocks], norm_layer=nn.Identity, activation_layer=_act_fn(activation[self.num_blocks]))]\n    self.blocks = nn.Sequential(*blocks)\n    self.classifier = nn.Sequential(nn.Dropout(dropout_rate), MutableLinear(cast(int, self.widths[self.num_blocks]), num_labels))\n    reset_parameters(self, bn_momentum=bn_momentum, bn_eps=bn_eps)",
            "def __init__(self, num_labels: int=1000, base_widths: Tuple[int, ...]=(16, 16, 16, 32, 64, 128, 256, 512, 1024), width_multipliers: Union[Tuple[float, ...], float]=(0.5, 0.625, 0.75, 1.0, 1.25, 1.5, 2.0), expand_ratios: Tuple[float, ...]=(1.0, 2.0, 3.0, 4.0, 5.0, 6.0), squeeze_excite: Tuple[Literal['force', 'optional', 'none'], ...]=('none', 'none', 'optional', 'none', 'optional', 'optional'), depth_range: Union[List[Tuple[int, int]], Tuple[int, int]]=(1, 4), stride: Tuple[int, ...]=(2, 1, 2, 2, 2, 1, 2, 1, 1), activation: Tuple[Literal['hswish', 'swish', 'relu'], ...]=('hswish', 'relu', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'), se_from_exp: bool=True, dropout_rate: float=0.2, bn_eps: float=0.001, bn_momentum: float=0.1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.num_blocks = len(base_widths) - 1\n    assert self.num_blocks >= 4\n    assert len(base_widths) == len(stride) == len(activation) == self.num_blocks + 1\n    assert len(squeeze_excite) == self.num_blocks - 2 and all((se in ['force', 'optional', 'none'] for se in squeeze_excite))\n    if isinstance(depth_range[0], int):\n        depth_range = cast(Tuple[int, int], depth_range)\n        assert len(depth_range) == 2 and depth_range[1] >= depth_range[0] >= 1\n        self.depth_range = [depth_range] * (self.num_blocks - 3)\n    else:\n        assert len(depth_range) == self.num_blocks - 3\n        self.depth_range = cast(List[Tuple[int, int]], depth_range)\n        for d in self.depth_range:\n            d = cast(Tuple[int, int], d)\n            assert len(d) == 2 and d[1] >= d[0] >= 1, f'{d} does not satisfy depth constraints'\n    self.widths = []\n    for (i, base_width) in enumerate(base_widths):\n        if isinstance(width_multipliers, float):\n            self.widths.append(make_divisible(base_width * width_multipliers, 8))\n        else:\n            self.widths.append(make_divisible(nni.choice(f's{max(i - 1, 0)}_width_mult', list(width_multipliers)) * base_width, 8))\n    self.expand_ratios = expand_ratios\n    self.se_from_exp = se_from_exp\n    self.stem = ConvBNReLU(3, self.widths[0], nni.choice(f'stem_ks', [3, 5]), stride=stride[0], activation_layer=_act_fn(activation[0]))\n    blocks: List[nn.Module] = [DepthwiseSeparableConv(self.widths[0], self.widths[1], nni.choice(f's0_i0_ks', [3, 5, 7]), stride=stride[1], squeeze_excite=cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=squeeze_excite[0] == 'optional', se_from_exp=self.se_from_exp, label=f's0_i0_se')) if squeeze_excite[0] != 'none' else None, activation_layer=_act_fn(activation[1]))]\n    blocks += [self._make_stage(i, self.widths[i], self.widths[i + 1], squeeze_excite[i], stride[i + 1], _act_fn(activation[i + 1])) for i in range(1, self.num_blocks - 2)]\n    blocks += [ConvBNReLU(self.widths[self.num_blocks - 2], self.widths[self.num_blocks - 1], kernel_size=1, stride=stride[self.num_blocks - 1], activation_layer=_act_fn(activation[self.num_blocks - 1])), nn.AdaptiveAvgPool2d(1), ConvBNReLU(self.widths[self.num_blocks - 1], self.widths[self.num_blocks], kernel_size=1, stride=stride[self.num_blocks], norm_layer=nn.Identity, activation_layer=_act_fn(activation[self.num_blocks]))]\n    self.blocks = nn.Sequential(*blocks)\n    self.classifier = nn.Sequential(nn.Dropout(dropout_rate), MutableLinear(cast(int, self.widths[self.num_blocks]), num_labels))\n    reset_parameters(self, bn_momentum=bn_momentum, bn_eps=bn_eps)",
            "def __init__(self, num_labels: int=1000, base_widths: Tuple[int, ...]=(16, 16, 16, 32, 64, 128, 256, 512, 1024), width_multipliers: Union[Tuple[float, ...], float]=(0.5, 0.625, 0.75, 1.0, 1.25, 1.5, 2.0), expand_ratios: Tuple[float, ...]=(1.0, 2.0, 3.0, 4.0, 5.0, 6.0), squeeze_excite: Tuple[Literal['force', 'optional', 'none'], ...]=('none', 'none', 'optional', 'none', 'optional', 'optional'), depth_range: Union[List[Tuple[int, int]], Tuple[int, int]]=(1, 4), stride: Tuple[int, ...]=(2, 1, 2, 2, 2, 1, 2, 1, 1), activation: Tuple[Literal['hswish', 'swish', 'relu'], ...]=('hswish', 'relu', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'), se_from_exp: bool=True, dropout_rate: float=0.2, bn_eps: float=0.001, bn_momentum: float=0.1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.num_blocks = len(base_widths) - 1\n    assert self.num_blocks >= 4\n    assert len(base_widths) == len(stride) == len(activation) == self.num_blocks + 1\n    assert len(squeeze_excite) == self.num_blocks - 2 and all((se in ['force', 'optional', 'none'] for se in squeeze_excite))\n    if isinstance(depth_range[0], int):\n        depth_range = cast(Tuple[int, int], depth_range)\n        assert len(depth_range) == 2 and depth_range[1] >= depth_range[0] >= 1\n        self.depth_range = [depth_range] * (self.num_blocks - 3)\n    else:\n        assert len(depth_range) == self.num_blocks - 3\n        self.depth_range = cast(List[Tuple[int, int]], depth_range)\n        for d in self.depth_range:\n            d = cast(Tuple[int, int], d)\n            assert len(d) == 2 and d[1] >= d[0] >= 1, f'{d} does not satisfy depth constraints'\n    self.widths = []\n    for (i, base_width) in enumerate(base_widths):\n        if isinstance(width_multipliers, float):\n            self.widths.append(make_divisible(base_width * width_multipliers, 8))\n        else:\n            self.widths.append(make_divisible(nni.choice(f's{max(i - 1, 0)}_width_mult', list(width_multipliers)) * base_width, 8))\n    self.expand_ratios = expand_ratios\n    self.se_from_exp = se_from_exp\n    self.stem = ConvBNReLU(3, self.widths[0], nni.choice(f'stem_ks', [3, 5]), stride=stride[0], activation_layer=_act_fn(activation[0]))\n    blocks: List[nn.Module] = [DepthwiseSeparableConv(self.widths[0], self.widths[1], nni.choice(f's0_i0_ks', [3, 5, 7]), stride=stride[1], squeeze_excite=cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=squeeze_excite[0] == 'optional', se_from_exp=self.se_from_exp, label=f's0_i0_se')) if squeeze_excite[0] != 'none' else None, activation_layer=_act_fn(activation[1]))]\n    blocks += [self._make_stage(i, self.widths[i], self.widths[i + 1], squeeze_excite[i], stride[i + 1], _act_fn(activation[i + 1])) for i in range(1, self.num_blocks - 2)]\n    blocks += [ConvBNReLU(self.widths[self.num_blocks - 2], self.widths[self.num_blocks - 1], kernel_size=1, stride=stride[self.num_blocks - 1], activation_layer=_act_fn(activation[self.num_blocks - 1])), nn.AdaptiveAvgPool2d(1), ConvBNReLU(self.widths[self.num_blocks - 1], self.widths[self.num_blocks], kernel_size=1, stride=stride[self.num_blocks], norm_layer=nn.Identity, activation_layer=_act_fn(activation[self.num_blocks]))]\n    self.blocks = nn.Sequential(*blocks)\n    self.classifier = nn.Sequential(nn.Dropout(dropout_rate), MutableLinear(cast(int, self.widths[self.num_blocks]), num_labels))\n    reset_parameters(self, bn_momentum=bn_momentum, bn_eps=bn_eps)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    x = self.stem(x)\n    x = self.blocks(x)\n    x = x.view(x.size(0), -1)\n    x = self.classifier(x)\n    return x",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    x = self.stem(x)\n    x = self.blocks(x)\n    x = x.view(x.size(0), -1)\n    x = self.classifier(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x = self.stem(x)\n    x = self.blocks(x)\n    x = x.view(x.size(0), -1)\n    x = self.classifier(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x = self.stem(x)\n    x = self.blocks(x)\n    x = x.view(x.size(0), -1)\n    x = self.classifier(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x = self.stem(x)\n    x = self.blocks(x)\n    x = x.view(x.size(0), -1)\n    x = self.classifier(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x = self.stem(x)\n    x = self.blocks(x)\n    x = x.view(x.size(0), -1)\n    x = self.classifier(x)\n    return x"
        ]
    },
    {
        "func_name": "layer_builder",
        "original": "def layer_builder(idx):\n    exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n    ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n    se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n    return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)",
        "mutated": [
            "def layer_builder(idx):\n    if False:\n        i = 10\n    exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n    ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n    se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n    return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)",
            "def layer_builder(idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n    ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n    se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n    return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)",
            "def layer_builder(idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n    ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n    se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n    return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)",
            "def layer_builder(idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n    ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n    se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n    return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)",
            "def layer_builder(idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n    ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n    se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n    return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)"
        ]
    },
    {
        "func_name": "_make_stage",
        "original": "def _make_stage(self, stage_idx, inp, oup, se, stride, act):\n\n    def layer_builder(idx):\n        exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n        ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n        se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n        return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)\n    (min_depth, max_depth) = self.depth_range[stage_idx - 1]\n    if stride != 1:\n        min_depth = max(min_depth, 1)\n    return Repeat(layer_builder, depth=(min_depth, max_depth), label=f's{stage_idx}_depth')",
        "mutated": [
            "def _make_stage(self, stage_idx, inp, oup, se, stride, act):\n    if False:\n        i = 10\n\n    def layer_builder(idx):\n        exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n        ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n        se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n        return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)\n    (min_depth, max_depth) = self.depth_range[stage_idx - 1]\n    if stride != 1:\n        min_depth = max(min_depth, 1)\n    return Repeat(layer_builder, depth=(min_depth, max_depth), label=f's{stage_idx}_depth')",
            "def _make_stage(self, stage_idx, inp, oup, se, stride, act):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def layer_builder(idx):\n        exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n        ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n        se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n        return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)\n    (min_depth, max_depth) = self.depth_range[stage_idx - 1]\n    if stride != 1:\n        min_depth = max(min_depth, 1)\n    return Repeat(layer_builder, depth=(min_depth, max_depth), label=f's{stage_idx}_depth')",
            "def _make_stage(self, stage_idx, inp, oup, se, stride, act):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def layer_builder(idx):\n        exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n        ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n        se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n        return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)\n    (min_depth, max_depth) = self.depth_range[stage_idx - 1]\n    if stride != 1:\n        min_depth = max(min_depth, 1)\n    return Repeat(layer_builder, depth=(min_depth, max_depth), label=f's{stage_idx}_depth')",
            "def _make_stage(self, stage_idx, inp, oup, se, stride, act):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def layer_builder(idx):\n        exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n        ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n        se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n        return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)\n    (min_depth, max_depth) = self.depth_range[stage_idx - 1]\n    if stride != 1:\n        min_depth = max(min_depth, 1)\n    return Repeat(layer_builder, depth=(min_depth, max_depth), label=f's{stage_idx}_depth')",
            "def _make_stage(self, stage_idx, inp, oup, se, stride, act):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def layer_builder(idx):\n        exp = nni.choice(f's{stage_idx}_i{idx}_exp', list(self.expand_ratios))\n        ks = nni.choice(f's{stage_idx}_i{idx}_ks', [3, 5, 7])\n        se_or_skip = cast(Callable[[MaybeIntChoice, MaybeIntChoice], nn.Module], partial(_se_or_skip, optional=se == 'optional', se_from_exp=self.se_from_exp, label=f's{stage_idx}_i{idx}_se')) if se != 'none' else None\n        return InvertedResidual(inp if idx == 0 else oup, oup, exp, ks, stride=stride if idx == 0 else 1, squeeze_excite=se_or_skip, activation_layer=act)\n    (min_depth, max_depth) = self.depth_range[stage_idx - 1]\n    if stride != 1:\n        min_depth = max(min_depth, 1)\n    return Repeat(layer_builder, depth=(min_depth, max_depth), label=f's{stage_idx}_depth')"
        ]
    },
    {
        "func_name": "load_searched_model",
        "original": "@classmethod\ndef load_searched_model(cls, name: str, pretrained: bool=False, download: bool=False, progress: bool=True) -> nn.Module:\n    init_kwargs = {}\n    if name == 'mobilenetv3-large-100':\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4, 's1_i0_ks': 3, 's1_i1_exp': 3, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 3, 's2_i0_ks': 5, 's2_i1_exp': 3, 's2_i1_ks': 5, 's2_i2_exp': 3, 's2_i2_ks': 5, 's3_depth': 4, 's3_i0_exp': 6, 's3_i0_ks': 3, 's3_i1_exp': 2.5, 's3_i1_ks': 3, 's3_i2_exp': 2.3, 's3_i2_ks': 3, 's3_i3_exp': 2.3, 's3_i3_ks': 3, 's4_depth': 2, 's4_i0_exp': 6, 's4_i0_ks': 3, 's4_i1_exp': 6, 's4_i1_ks': 3, 's5_depth': 3, 's5_i0_exp': 6, 's5_i0_ks': 5, 's5_i1_exp': 6, 's5_i1_ks': 5, 's5_i2_exp': 6, 's5_i2_ks': 5}\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 112, 160, 960, 1280], expand_ratios=[1.0, 2.0, 2.3, 2.5, 3.0, 4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, width_multipliers=1.0, squeeze_excite=['none', 'none', 'force', 'none', 'force', 'force'])\n    elif name.startswith('mobilenetv3-small-'):\n        multiplier = int(name.split('-')[-1]) / 100\n        widths = [16, 16, 24, 40, 48, 96, 576, 1024]\n        for i in range(7):\n            if i > 0 or multiplier >= 0.75:\n                widths[i] = make_divisible(widths[i] * multiplier, 8)\n        init_kwargs.update(base_widths=widths, width_multipliers=1.0, expand_ratios=[3.0, 3.67, 4.0, 4.5, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force', 'none', 'force', 'force', 'force'], activation=['hswish', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'], stride=[2, 2, 2, 2, 1, 2, 1, 1], depth_range=(1, 2))\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4.5, 's1_i0_ks': 3, 's1_i1_exp': 3.67, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 4.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's3_depth': 2, 's3_i0_exp': 3.0, 's3_i0_ks': 5, 's3_i1_exp': 3.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5}\n    elif name.startswith('cream'):\n        level = name.split('-')[-1]\n        if level == '014':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 1, 's4_i0_exp': 6.0, 's4_i0_ks': 3, 's5_depth': 1, 's5_i0_exp': 6.0, 's5_i0_ks': 5}\n        elif level == '043':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 3, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '114':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '287':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 3, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 3, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5}\n        elif level == '481':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 4, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 4.0, 's1_i1_ks': 7, 's1_i2_exp': 6.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 3, 's2_depth': 4, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 3, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 6.0, 's3_i3_ks': 3, 's3_i4_exp': 6.0, 's3_i4_ks': 3, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 4, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5}\n        elif level == '604':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 5, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 6.0, 's1_i1_ks': 5, 's1_i2_exp': 4.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 5, 's1_i4_exp': 6.0, 's1_i4_ks': 5, 's2_depth': 5, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 5, 's2_i4_exp': 6.0, 's2_i4_ks': 5, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 4.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 4.0, 's3_i3_ks': 5, 's3_i4_exp': 6.0, 's3_i4_ks': 5, 's4_depth': 6, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 4.0, 's4_i2_ks': 5, 's4_i3_exp': 4.0, 's4_i3_ks': 5, 's4_i4_exp': 6.0, 's4_i4_ks': 5, 's4_i5_exp': 6.0, 's4_i5_ks': 5, 's5_depth': 6, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 4.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5, 's5_i4_exp': 6.0, 's5_i4_ks': 5, 's5_i5_exp': 6.0, 's5_i5_ks': 5}\n        else:\n            raise ValueError(f'Unsupported cream model level: {level}')\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 96, 192, 320, 1280], width_multipliers=1.0, expand_ratios=[4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force'] * 6, activation=['swish'] * 9)\n    else:\n        raise ValueError(f'Unsupported architecture with name: {name}')\n    model_factory = cls.frozen_factory(arch)\n    model = model_factory(**init_kwargs)\n    if pretrained:\n        weight_file = load_pretrained_weight(name, download=download, progress=progress)\n        pretrained_weights = torch.load(weight_file)\n        model.load_state_dict(pretrained_weights)\n    return model",
        "mutated": [
            "@classmethod\ndef load_searched_model(cls, name: str, pretrained: bool=False, download: bool=False, progress: bool=True) -> nn.Module:\n    if False:\n        i = 10\n    init_kwargs = {}\n    if name == 'mobilenetv3-large-100':\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4, 's1_i0_ks': 3, 's1_i1_exp': 3, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 3, 's2_i0_ks': 5, 's2_i1_exp': 3, 's2_i1_ks': 5, 's2_i2_exp': 3, 's2_i2_ks': 5, 's3_depth': 4, 's3_i0_exp': 6, 's3_i0_ks': 3, 's3_i1_exp': 2.5, 's3_i1_ks': 3, 's3_i2_exp': 2.3, 's3_i2_ks': 3, 's3_i3_exp': 2.3, 's3_i3_ks': 3, 's4_depth': 2, 's4_i0_exp': 6, 's4_i0_ks': 3, 's4_i1_exp': 6, 's4_i1_ks': 3, 's5_depth': 3, 's5_i0_exp': 6, 's5_i0_ks': 5, 's5_i1_exp': 6, 's5_i1_ks': 5, 's5_i2_exp': 6, 's5_i2_ks': 5}\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 112, 160, 960, 1280], expand_ratios=[1.0, 2.0, 2.3, 2.5, 3.0, 4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, width_multipliers=1.0, squeeze_excite=['none', 'none', 'force', 'none', 'force', 'force'])\n    elif name.startswith('mobilenetv3-small-'):\n        multiplier = int(name.split('-')[-1]) / 100\n        widths = [16, 16, 24, 40, 48, 96, 576, 1024]\n        for i in range(7):\n            if i > 0 or multiplier >= 0.75:\n                widths[i] = make_divisible(widths[i] * multiplier, 8)\n        init_kwargs.update(base_widths=widths, width_multipliers=1.0, expand_ratios=[3.0, 3.67, 4.0, 4.5, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force', 'none', 'force', 'force', 'force'], activation=['hswish', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'], stride=[2, 2, 2, 2, 1, 2, 1, 1], depth_range=(1, 2))\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4.5, 's1_i0_ks': 3, 's1_i1_exp': 3.67, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 4.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's3_depth': 2, 's3_i0_exp': 3.0, 's3_i0_ks': 5, 's3_i1_exp': 3.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5}\n    elif name.startswith('cream'):\n        level = name.split('-')[-1]\n        if level == '014':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 1, 's4_i0_exp': 6.0, 's4_i0_ks': 3, 's5_depth': 1, 's5_i0_exp': 6.0, 's5_i0_ks': 5}\n        elif level == '043':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 3, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '114':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '287':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 3, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 3, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5}\n        elif level == '481':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 4, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 4.0, 's1_i1_ks': 7, 's1_i2_exp': 6.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 3, 's2_depth': 4, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 3, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 6.0, 's3_i3_ks': 3, 's3_i4_exp': 6.0, 's3_i4_ks': 3, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 4, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5}\n        elif level == '604':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 5, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 6.0, 's1_i1_ks': 5, 's1_i2_exp': 4.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 5, 's1_i4_exp': 6.0, 's1_i4_ks': 5, 's2_depth': 5, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 5, 's2_i4_exp': 6.0, 's2_i4_ks': 5, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 4.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 4.0, 's3_i3_ks': 5, 's3_i4_exp': 6.0, 's3_i4_ks': 5, 's4_depth': 6, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 4.0, 's4_i2_ks': 5, 's4_i3_exp': 4.0, 's4_i3_ks': 5, 's4_i4_exp': 6.0, 's4_i4_ks': 5, 's4_i5_exp': 6.0, 's4_i5_ks': 5, 's5_depth': 6, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 4.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5, 's5_i4_exp': 6.0, 's5_i4_ks': 5, 's5_i5_exp': 6.0, 's5_i5_ks': 5}\n        else:\n            raise ValueError(f'Unsupported cream model level: {level}')\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 96, 192, 320, 1280], width_multipliers=1.0, expand_ratios=[4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force'] * 6, activation=['swish'] * 9)\n    else:\n        raise ValueError(f'Unsupported architecture with name: {name}')\n    model_factory = cls.frozen_factory(arch)\n    model = model_factory(**init_kwargs)\n    if pretrained:\n        weight_file = load_pretrained_weight(name, download=download, progress=progress)\n        pretrained_weights = torch.load(weight_file)\n        model.load_state_dict(pretrained_weights)\n    return model",
            "@classmethod\ndef load_searched_model(cls, name: str, pretrained: bool=False, download: bool=False, progress: bool=True) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    init_kwargs = {}\n    if name == 'mobilenetv3-large-100':\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4, 's1_i0_ks': 3, 's1_i1_exp': 3, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 3, 's2_i0_ks': 5, 's2_i1_exp': 3, 's2_i1_ks': 5, 's2_i2_exp': 3, 's2_i2_ks': 5, 's3_depth': 4, 's3_i0_exp': 6, 's3_i0_ks': 3, 's3_i1_exp': 2.5, 's3_i1_ks': 3, 's3_i2_exp': 2.3, 's3_i2_ks': 3, 's3_i3_exp': 2.3, 's3_i3_ks': 3, 's4_depth': 2, 's4_i0_exp': 6, 's4_i0_ks': 3, 's4_i1_exp': 6, 's4_i1_ks': 3, 's5_depth': 3, 's5_i0_exp': 6, 's5_i0_ks': 5, 's5_i1_exp': 6, 's5_i1_ks': 5, 's5_i2_exp': 6, 's5_i2_ks': 5}\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 112, 160, 960, 1280], expand_ratios=[1.0, 2.0, 2.3, 2.5, 3.0, 4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, width_multipliers=1.0, squeeze_excite=['none', 'none', 'force', 'none', 'force', 'force'])\n    elif name.startswith('mobilenetv3-small-'):\n        multiplier = int(name.split('-')[-1]) / 100\n        widths = [16, 16, 24, 40, 48, 96, 576, 1024]\n        for i in range(7):\n            if i > 0 or multiplier >= 0.75:\n                widths[i] = make_divisible(widths[i] * multiplier, 8)\n        init_kwargs.update(base_widths=widths, width_multipliers=1.0, expand_ratios=[3.0, 3.67, 4.0, 4.5, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force', 'none', 'force', 'force', 'force'], activation=['hswish', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'], stride=[2, 2, 2, 2, 1, 2, 1, 1], depth_range=(1, 2))\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4.5, 's1_i0_ks': 3, 's1_i1_exp': 3.67, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 4.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's3_depth': 2, 's3_i0_exp': 3.0, 's3_i0_ks': 5, 's3_i1_exp': 3.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5}\n    elif name.startswith('cream'):\n        level = name.split('-')[-1]\n        if level == '014':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 1, 's4_i0_exp': 6.0, 's4_i0_ks': 3, 's5_depth': 1, 's5_i0_exp': 6.0, 's5_i0_ks': 5}\n        elif level == '043':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 3, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '114':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '287':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 3, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 3, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5}\n        elif level == '481':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 4, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 4.0, 's1_i1_ks': 7, 's1_i2_exp': 6.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 3, 's2_depth': 4, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 3, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 6.0, 's3_i3_ks': 3, 's3_i4_exp': 6.0, 's3_i4_ks': 3, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 4, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5}\n        elif level == '604':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 5, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 6.0, 's1_i1_ks': 5, 's1_i2_exp': 4.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 5, 's1_i4_exp': 6.0, 's1_i4_ks': 5, 's2_depth': 5, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 5, 's2_i4_exp': 6.0, 's2_i4_ks': 5, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 4.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 4.0, 's3_i3_ks': 5, 's3_i4_exp': 6.0, 's3_i4_ks': 5, 's4_depth': 6, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 4.0, 's4_i2_ks': 5, 's4_i3_exp': 4.0, 's4_i3_ks': 5, 's4_i4_exp': 6.0, 's4_i4_ks': 5, 's4_i5_exp': 6.0, 's4_i5_ks': 5, 's5_depth': 6, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 4.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5, 's5_i4_exp': 6.0, 's5_i4_ks': 5, 's5_i5_exp': 6.0, 's5_i5_ks': 5}\n        else:\n            raise ValueError(f'Unsupported cream model level: {level}')\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 96, 192, 320, 1280], width_multipliers=1.0, expand_ratios=[4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force'] * 6, activation=['swish'] * 9)\n    else:\n        raise ValueError(f'Unsupported architecture with name: {name}')\n    model_factory = cls.frozen_factory(arch)\n    model = model_factory(**init_kwargs)\n    if pretrained:\n        weight_file = load_pretrained_weight(name, download=download, progress=progress)\n        pretrained_weights = torch.load(weight_file)\n        model.load_state_dict(pretrained_weights)\n    return model",
            "@classmethod\ndef load_searched_model(cls, name: str, pretrained: bool=False, download: bool=False, progress: bool=True) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    init_kwargs = {}\n    if name == 'mobilenetv3-large-100':\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4, 's1_i0_ks': 3, 's1_i1_exp': 3, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 3, 's2_i0_ks': 5, 's2_i1_exp': 3, 's2_i1_ks': 5, 's2_i2_exp': 3, 's2_i2_ks': 5, 's3_depth': 4, 's3_i0_exp': 6, 's3_i0_ks': 3, 's3_i1_exp': 2.5, 's3_i1_ks': 3, 's3_i2_exp': 2.3, 's3_i2_ks': 3, 's3_i3_exp': 2.3, 's3_i3_ks': 3, 's4_depth': 2, 's4_i0_exp': 6, 's4_i0_ks': 3, 's4_i1_exp': 6, 's4_i1_ks': 3, 's5_depth': 3, 's5_i0_exp': 6, 's5_i0_ks': 5, 's5_i1_exp': 6, 's5_i1_ks': 5, 's5_i2_exp': 6, 's5_i2_ks': 5}\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 112, 160, 960, 1280], expand_ratios=[1.0, 2.0, 2.3, 2.5, 3.0, 4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, width_multipliers=1.0, squeeze_excite=['none', 'none', 'force', 'none', 'force', 'force'])\n    elif name.startswith('mobilenetv3-small-'):\n        multiplier = int(name.split('-')[-1]) / 100\n        widths = [16, 16, 24, 40, 48, 96, 576, 1024]\n        for i in range(7):\n            if i > 0 or multiplier >= 0.75:\n                widths[i] = make_divisible(widths[i] * multiplier, 8)\n        init_kwargs.update(base_widths=widths, width_multipliers=1.0, expand_ratios=[3.0, 3.67, 4.0, 4.5, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force', 'none', 'force', 'force', 'force'], activation=['hswish', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'], stride=[2, 2, 2, 2, 1, 2, 1, 1], depth_range=(1, 2))\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4.5, 's1_i0_ks': 3, 's1_i1_exp': 3.67, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 4.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's3_depth': 2, 's3_i0_exp': 3.0, 's3_i0_ks': 5, 's3_i1_exp': 3.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5}\n    elif name.startswith('cream'):\n        level = name.split('-')[-1]\n        if level == '014':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 1, 's4_i0_exp': 6.0, 's4_i0_ks': 3, 's5_depth': 1, 's5_i0_exp': 6.0, 's5_i0_ks': 5}\n        elif level == '043':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 3, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '114':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '287':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 3, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 3, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5}\n        elif level == '481':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 4, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 4.0, 's1_i1_ks': 7, 's1_i2_exp': 6.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 3, 's2_depth': 4, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 3, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 6.0, 's3_i3_ks': 3, 's3_i4_exp': 6.0, 's3_i4_ks': 3, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 4, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5}\n        elif level == '604':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 5, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 6.0, 's1_i1_ks': 5, 's1_i2_exp': 4.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 5, 's1_i4_exp': 6.0, 's1_i4_ks': 5, 's2_depth': 5, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 5, 's2_i4_exp': 6.0, 's2_i4_ks': 5, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 4.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 4.0, 's3_i3_ks': 5, 's3_i4_exp': 6.0, 's3_i4_ks': 5, 's4_depth': 6, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 4.0, 's4_i2_ks': 5, 's4_i3_exp': 4.0, 's4_i3_ks': 5, 's4_i4_exp': 6.0, 's4_i4_ks': 5, 's4_i5_exp': 6.0, 's4_i5_ks': 5, 's5_depth': 6, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 4.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5, 's5_i4_exp': 6.0, 's5_i4_ks': 5, 's5_i5_exp': 6.0, 's5_i5_ks': 5}\n        else:\n            raise ValueError(f'Unsupported cream model level: {level}')\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 96, 192, 320, 1280], width_multipliers=1.0, expand_ratios=[4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force'] * 6, activation=['swish'] * 9)\n    else:\n        raise ValueError(f'Unsupported architecture with name: {name}')\n    model_factory = cls.frozen_factory(arch)\n    model = model_factory(**init_kwargs)\n    if pretrained:\n        weight_file = load_pretrained_weight(name, download=download, progress=progress)\n        pretrained_weights = torch.load(weight_file)\n        model.load_state_dict(pretrained_weights)\n    return model",
            "@classmethod\ndef load_searched_model(cls, name: str, pretrained: bool=False, download: bool=False, progress: bool=True) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    init_kwargs = {}\n    if name == 'mobilenetv3-large-100':\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4, 's1_i0_ks': 3, 's1_i1_exp': 3, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 3, 's2_i0_ks': 5, 's2_i1_exp': 3, 's2_i1_ks': 5, 's2_i2_exp': 3, 's2_i2_ks': 5, 's3_depth': 4, 's3_i0_exp': 6, 's3_i0_ks': 3, 's3_i1_exp': 2.5, 's3_i1_ks': 3, 's3_i2_exp': 2.3, 's3_i2_ks': 3, 's3_i3_exp': 2.3, 's3_i3_ks': 3, 's4_depth': 2, 's4_i0_exp': 6, 's4_i0_ks': 3, 's4_i1_exp': 6, 's4_i1_ks': 3, 's5_depth': 3, 's5_i0_exp': 6, 's5_i0_ks': 5, 's5_i1_exp': 6, 's5_i1_ks': 5, 's5_i2_exp': 6, 's5_i2_ks': 5}\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 112, 160, 960, 1280], expand_ratios=[1.0, 2.0, 2.3, 2.5, 3.0, 4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, width_multipliers=1.0, squeeze_excite=['none', 'none', 'force', 'none', 'force', 'force'])\n    elif name.startswith('mobilenetv3-small-'):\n        multiplier = int(name.split('-')[-1]) / 100\n        widths = [16, 16, 24, 40, 48, 96, 576, 1024]\n        for i in range(7):\n            if i > 0 or multiplier >= 0.75:\n                widths[i] = make_divisible(widths[i] * multiplier, 8)\n        init_kwargs.update(base_widths=widths, width_multipliers=1.0, expand_ratios=[3.0, 3.67, 4.0, 4.5, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force', 'none', 'force', 'force', 'force'], activation=['hswish', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'], stride=[2, 2, 2, 2, 1, 2, 1, 1], depth_range=(1, 2))\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4.5, 's1_i0_ks': 3, 's1_i1_exp': 3.67, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 4.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's3_depth': 2, 's3_i0_exp': 3.0, 's3_i0_ks': 5, 's3_i1_exp': 3.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5}\n    elif name.startswith('cream'):\n        level = name.split('-')[-1]\n        if level == '014':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 1, 's4_i0_exp': 6.0, 's4_i0_ks': 3, 's5_depth': 1, 's5_i0_exp': 6.0, 's5_i0_ks': 5}\n        elif level == '043':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 3, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '114':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '287':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 3, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 3, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5}\n        elif level == '481':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 4, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 4.0, 's1_i1_ks': 7, 's1_i2_exp': 6.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 3, 's2_depth': 4, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 3, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 6.0, 's3_i3_ks': 3, 's3_i4_exp': 6.0, 's3_i4_ks': 3, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 4, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5}\n        elif level == '604':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 5, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 6.0, 's1_i1_ks': 5, 's1_i2_exp': 4.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 5, 's1_i4_exp': 6.0, 's1_i4_ks': 5, 's2_depth': 5, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 5, 's2_i4_exp': 6.0, 's2_i4_ks': 5, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 4.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 4.0, 's3_i3_ks': 5, 's3_i4_exp': 6.0, 's3_i4_ks': 5, 's4_depth': 6, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 4.0, 's4_i2_ks': 5, 's4_i3_exp': 4.0, 's4_i3_ks': 5, 's4_i4_exp': 6.0, 's4_i4_ks': 5, 's4_i5_exp': 6.0, 's4_i5_ks': 5, 's5_depth': 6, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 4.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5, 's5_i4_exp': 6.0, 's5_i4_ks': 5, 's5_i5_exp': 6.0, 's5_i5_ks': 5}\n        else:\n            raise ValueError(f'Unsupported cream model level: {level}')\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 96, 192, 320, 1280], width_multipliers=1.0, expand_ratios=[4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force'] * 6, activation=['swish'] * 9)\n    else:\n        raise ValueError(f'Unsupported architecture with name: {name}')\n    model_factory = cls.frozen_factory(arch)\n    model = model_factory(**init_kwargs)\n    if pretrained:\n        weight_file = load_pretrained_weight(name, download=download, progress=progress)\n        pretrained_weights = torch.load(weight_file)\n        model.load_state_dict(pretrained_weights)\n    return model",
            "@classmethod\ndef load_searched_model(cls, name: str, pretrained: bool=False, download: bool=False, progress: bool=True) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    init_kwargs = {}\n    if name == 'mobilenetv3-large-100':\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4, 's1_i0_ks': 3, 's1_i1_exp': 3, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 3, 's2_i0_ks': 5, 's2_i1_exp': 3, 's2_i1_ks': 5, 's2_i2_exp': 3, 's2_i2_ks': 5, 's3_depth': 4, 's3_i0_exp': 6, 's3_i0_ks': 3, 's3_i1_exp': 2.5, 's3_i1_ks': 3, 's3_i2_exp': 2.3, 's3_i2_ks': 3, 's3_i3_exp': 2.3, 's3_i3_ks': 3, 's4_depth': 2, 's4_i0_exp': 6, 's4_i0_ks': 3, 's4_i1_exp': 6, 's4_i1_ks': 3, 's5_depth': 3, 's5_i0_exp': 6, 's5_i0_ks': 5, 's5_i1_exp': 6, 's5_i1_ks': 5, 's5_i2_exp': 6, 's5_i2_ks': 5}\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 112, 160, 960, 1280], expand_ratios=[1.0, 2.0, 2.3, 2.5, 3.0, 4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, width_multipliers=1.0, squeeze_excite=['none', 'none', 'force', 'none', 'force', 'force'])\n    elif name.startswith('mobilenetv3-small-'):\n        multiplier = int(name.split('-')[-1]) / 100\n        widths = [16, 16, 24, 40, 48, 96, 576, 1024]\n        for i in range(7):\n            if i > 0 or multiplier >= 0.75:\n                widths[i] = make_divisible(widths[i] * multiplier, 8)\n        init_kwargs.update(base_widths=widths, width_multipliers=1.0, expand_ratios=[3.0, 3.67, 4.0, 4.5, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force', 'none', 'force', 'force', 'force'], activation=['hswish', 'relu', 'relu', 'hswish', 'hswish', 'hswish', 'hswish', 'hswish'], stride=[2, 2, 2, 2, 1, 2, 1, 1], depth_range=(1, 2))\n        arch = {'stem_ks': 3, 's0_i0_ks': 3, 's1_depth': 2, 's1_i0_exp': 4.5, 's1_i0_ks': 3, 's1_i1_exp': 3.67, 's1_i1_ks': 3, 's2_depth': 3, 's2_i0_exp': 4.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's3_depth': 2, 's3_i0_exp': 3.0, 's3_i0_ks': 5, 's3_i1_exp': 3.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5}\n    elif name.startswith('cream'):\n        level = name.split('-')[-1]\n        if level == '014':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 1, 's4_i0_exp': 6.0, 's4_i0_ks': 3, 's5_depth': 1, 's5_i0_exp': 6.0, 's5_i0_ks': 5}\n        elif level == '043':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 3, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '114':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 2, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's4_depth': 3, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's5_depth': 2, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5}\n        elif level == '287':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 1, 's1_i0_exp': 4.0, 's1_i0_ks': 3, 's2_depth': 2, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 6.0, 's2_i1_ks': 5, 's3_depth': 3, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 3, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 3, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5}\n        elif level == '481':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 4, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 4.0, 's1_i1_ks': 7, 's1_i2_exp': 6.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 3, 's2_depth': 4, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 3, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 6.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 6.0, 's3_i3_ks': 3, 's3_i4_exp': 6.0, 's3_i4_ks': 3, 's4_depth': 4, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 6.0, 's4_i2_ks': 5, 's4_i3_exp': 6.0, 's4_i3_ks': 5, 's5_depth': 4, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 6.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5}\n        elif level == '604':\n            arch = {'stem_ks': 3, 's0_depth': 1, 's0_i0_ks': 3, 's1_depth': 5, 's1_i0_exp': 6.0, 's1_i0_ks': 5, 's1_i1_exp': 6.0, 's1_i1_ks': 5, 's1_i2_exp': 4.0, 's1_i2_ks': 5, 's1_i3_exp': 6.0, 's1_i3_ks': 5, 's1_i4_exp': 6.0, 's1_i4_ks': 5, 's2_depth': 5, 's2_i0_exp': 6.0, 's2_i0_ks': 5, 's2_i1_exp': 4.0, 's2_i1_ks': 5, 's2_i2_exp': 6.0, 's2_i2_ks': 5, 's2_i3_exp': 4.0, 's2_i3_ks': 5, 's2_i4_exp': 6.0, 's2_i4_ks': 5, 's3_depth': 5, 's3_i0_exp': 6.0, 's3_i0_ks': 5, 's3_i1_exp': 4.0, 's3_i1_ks': 5, 's3_i2_exp': 6.0, 's3_i2_ks': 5, 's3_i3_exp': 4.0, 's3_i3_ks': 5, 's3_i4_exp': 6.0, 's3_i4_ks': 5, 's4_depth': 6, 's4_i0_exp': 6.0, 's4_i0_ks': 5, 's4_i1_exp': 6.0, 's4_i1_ks': 5, 's4_i2_exp': 4.0, 's4_i2_ks': 5, 's4_i3_exp': 4.0, 's4_i3_ks': 5, 's4_i4_exp': 6.0, 's4_i4_ks': 5, 's4_i5_exp': 6.0, 's4_i5_ks': 5, 's5_depth': 6, 's5_i0_exp': 6.0, 's5_i0_ks': 5, 's5_i1_exp': 6.0, 's5_i1_ks': 5, 's5_i2_exp': 4.0, 's5_i2_ks': 5, 's5_i3_exp': 6.0, 's5_i3_ks': 5, 's5_i4_exp': 6.0, 's5_i4_ks': 5, 's5_i5_exp': 6.0, 's5_i5_ks': 5}\n        else:\n            raise ValueError(f'Unsupported cream model level: {level}')\n        init_kwargs.update(base_widths=[16, 16, 24, 40, 80, 96, 192, 320, 1280], width_multipliers=1.0, expand_ratios=[4.0, 6.0], bn_eps=1e-05, bn_momentum=0.1, squeeze_excite=['force'] * 6, activation=['swish'] * 9)\n    else:\n        raise ValueError(f'Unsupported architecture with name: {name}')\n    model_factory = cls.frozen_factory(arch)\n    model = model_factory(**init_kwargs)\n    if pretrained:\n        weight_file = load_pretrained_weight(name, download=download, progress=progress)\n        pretrained_weights = torch.load(weight_file)\n        model.load_state_dict(pretrained_weights)\n    return model"
        ]
    }
]