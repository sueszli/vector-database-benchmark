[
    {
        "func_name": "__init__",
        "original": "def __init__(self, num_units, forget_bias=1.0, weight_scale=1.0, clip_value=np.inf, collections=None):\n    \"\"\"Create a GRU object.\n\n    Args:\n      num_units: Number of units in the GRU\n      forget_bias (optional): Hack to help learning.\n      weight_scale (optional): weights are scaled by ws/sqrt(#inputs), with\n       ws being the weight scale.\n      clip_value (optional): if the recurrent values grow above this value,\n        clip them.\n      collections (optional): List of additonal collections variables should\n        belong to.\n    \"\"\"\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._weight_scale = weight_scale\n    self._clip_value = clip_value\n    self._collections = collections",
        "mutated": [
            "def __init__(self, num_units, forget_bias=1.0, weight_scale=1.0, clip_value=np.inf, collections=None):\n    if False:\n        i = 10\n    'Create a GRU object.\\n\\n    Args:\\n      num_units: Number of units in the GRU\\n      forget_bias (optional): Hack to help learning.\\n      weight_scale (optional): weights are scaled by ws/sqrt(#inputs), with\\n       ws being the weight scale.\\n      clip_value (optional): if the recurrent values grow above this value,\\n        clip them.\\n      collections (optional): List of additonal collections variables should\\n        belong to.\\n    '\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._weight_scale = weight_scale\n    self._clip_value = clip_value\n    self._collections = collections",
            "def __init__(self, num_units, forget_bias=1.0, weight_scale=1.0, clip_value=np.inf, collections=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a GRU object.\\n\\n    Args:\\n      num_units: Number of units in the GRU\\n      forget_bias (optional): Hack to help learning.\\n      weight_scale (optional): weights are scaled by ws/sqrt(#inputs), with\\n       ws being the weight scale.\\n      clip_value (optional): if the recurrent values grow above this value,\\n        clip them.\\n      collections (optional): List of additonal collections variables should\\n        belong to.\\n    '\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._weight_scale = weight_scale\n    self._clip_value = clip_value\n    self._collections = collections",
            "def __init__(self, num_units, forget_bias=1.0, weight_scale=1.0, clip_value=np.inf, collections=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a GRU object.\\n\\n    Args:\\n      num_units: Number of units in the GRU\\n      forget_bias (optional): Hack to help learning.\\n      weight_scale (optional): weights are scaled by ws/sqrt(#inputs), with\\n       ws being the weight scale.\\n      clip_value (optional): if the recurrent values grow above this value,\\n        clip them.\\n      collections (optional): List of additonal collections variables should\\n        belong to.\\n    '\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._weight_scale = weight_scale\n    self._clip_value = clip_value\n    self._collections = collections",
            "def __init__(self, num_units, forget_bias=1.0, weight_scale=1.0, clip_value=np.inf, collections=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a GRU object.\\n\\n    Args:\\n      num_units: Number of units in the GRU\\n      forget_bias (optional): Hack to help learning.\\n      weight_scale (optional): weights are scaled by ws/sqrt(#inputs), with\\n       ws being the weight scale.\\n      clip_value (optional): if the recurrent values grow above this value,\\n        clip them.\\n      collections (optional): List of additonal collections variables should\\n        belong to.\\n    '\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._weight_scale = weight_scale\n    self._clip_value = clip_value\n    self._collections = collections",
            "def __init__(self, num_units, forget_bias=1.0, weight_scale=1.0, clip_value=np.inf, collections=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a GRU object.\\n\\n    Args:\\n      num_units: Number of units in the GRU\\n      forget_bias (optional): Hack to help learning.\\n      weight_scale (optional): weights are scaled by ws/sqrt(#inputs), with\\n       ws being the weight scale.\\n      clip_value (optional): if the recurrent values grow above this value,\\n        clip them.\\n      collections (optional): List of additonal collections variables should\\n        belong to.\\n    '\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._weight_scale = weight_scale\n    self._clip_value = clip_value\n    self._collections = collections"
        ]
    },
    {
        "func_name": "state_size",
        "original": "@property\ndef state_size(self):\n    return self._num_units",
        "mutated": [
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n    return self._num_units",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self._num_units",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self._num_units",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self._num_units",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self._num_units"
        ]
    },
    {
        "func_name": "output_size",
        "original": "@property\ndef output_size(self):\n    return self._num_units",
        "mutated": [
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n    return self._num_units",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self._num_units",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self._num_units",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self._num_units",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self._num_units"
        ]
    },
    {
        "func_name": "state_multiplier",
        "original": "@property\ndef state_multiplier(self):\n    return 1",
        "mutated": [
            "@property\ndef state_multiplier(self):\n    if False:\n        i = 10\n    return 1",
            "@property\ndef state_multiplier(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 1",
            "@property\ndef state_multiplier(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 1",
            "@property\ndef state_multiplier(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 1",
            "@property\ndef state_multiplier(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 1"
        ]
    },
    {
        "func_name": "output_from_state",
        "original": "def output_from_state(self, state):\n    \"\"\"Return the output portion of the state.\"\"\"\n    return state",
        "mutated": [
            "def output_from_state(self, state):\n    if False:\n        i = 10\n    'Return the output portion of the state.'\n    return state",
            "def output_from_state(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return the output portion of the state.'\n    return state",
            "def output_from_state(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return the output portion of the state.'\n    return state",
            "def output_from_state(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return the output portion of the state.'\n    return state",
            "def output_from_state(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return the output portion of the state.'\n    return state"
        ]
    },
    {
        "func_name": "__call__",
        "original": "def __call__(self, inputs, state, scope=None):\n    \"\"\"Gated recurrent unit (GRU) function.\n\n    Args:\n      inputs: A 2D batch x input_dim tensor of inputs.\n      state: The previous state from the last time step.\n      scope (optional): TF variable scope for defined GRU variables.\n\n    Returns:\n      A tuple (state, state), where state is the newly computed state at time t.\n      It is returned twice to respect an interface that works for LSTMs.\n    \"\"\"\n    x = inputs\n    h = state\n    if inputs is not None:\n        xh = tf.concat(axis=1, values=[x, h])\n    else:\n        xh = h\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            (r, u) = tf.split(axis=1, num_or_size_splits=2, value=linear(xh, 2 * self._num_units, alpha=self._weight_scale, name='xh_2_ru', collections=self._collections))\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            xrh = tf.concat(axis=1, values=[x, r * h])\n            c = tf.tanh(linear(xrh, self._num_units, name='xrh_2_c', collections=self._collections))\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)",
        "mutated": [
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n    'Gated recurrent unit (GRU) function.\\n\\n    Args:\\n      inputs: A 2D batch x input_dim tensor of inputs.\\n      state: The previous state from the last time step.\\n      scope (optional): TF variable scope for defined GRU variables.\\n\\n    Returns:\\n      A tuple (state, state), where state is the newly computed state at time t.\\n      It is returned twice to respect an interface that works for LSTMs.\\n    '\n    x = inputs\n    h = state\n    if inputs is not None:\n        xh = tf.concat(axis=1, values=[x, h])\n    else:\n        xh = h\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            (r, u) = tf.split(axis=1, num_or_size_splits=2, value=linear(xh, 2 * self._num_units, alpha=self._weight_scale, name='xh_2_ru', collections=self._collections))\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            xrh = tf.concat(axis=1, values=[x, r * h])\n            c = tf.tanh(linear(xrh, self._num_units, name='xrh_2_c', collections=self._collections))\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Gated recurrent unit (GRU) function.\\n\\n    Args:\\n      inputs: A 2D batch x input_dim tensor of inputs.\\n      state: The previous state from the last time step.\\n      scope (optional): TF variable scope for defined GRU variables.\\n\\n    Returns:\\n      A tuple (state, state), where state is the newly computed state at time t.\\n      It is returned twice to respect an interface that works for LSTMs.\\n    '\n    x = inputs\n    h = state\n    if inputs is not None:\n        xh = tf.concat(axis=1, values=[x, h])\n    else:\n        xh = h\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            (r, u) = tf.split(axis=1, num_or_size_splits=2, value=linear(xh, 2 * self._num_units, alpha=self._weight_scale, name='xh_2_ru', collections=self._collections))\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            xrh = tf.concat(axis=1, values=[x, r * h])\n            c = tf.tanh(linear(xrh, self._num_units, name='xrh_2_c', collections=self._collections))\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Gated recurrent unit (GRU) function.\\n\\n    Args:\\n      inputs: A 2D batch x input_dim tensor of inputs.\\n      state: The previous state from the last time step.\\n      scope (optional): TF variable scope for defined GRU variables.\\n\\n    Returns:\\n      A tuple (state, state), where state is the newly computed state at time t.\\n      It is returned twice to respect an interface that works for LSTMs.\\n    '\n    x = inputs\n    h = state\n    if inputs is not None:\n        xh = tf.concat(axis=1, values=[x, h])\n    else:\n        xh = h\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            (r, u) = tf.split(axis=1, num_or_size_splits=2, value=linear(xh, 2 * self._num_units, alpha=self._weight_scale, name='xh_2_ru', collections=self._collections))\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            xrh = tf.concat(axis=1, values=[x, r * h])\n            c = tf.tanh(linear(xrh, self._num_units, name='xrh_2_c', collections=self._collections))\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Gated recurrent unit (GRU) function.\\n\\n    Args:\\n      inputs: A 2D batch x input_dim tensor of inputs.\\n      state: The previous state from the last time step.\\n      scope (optional): TF variable scope for defined GRU variables.\\n\\n    Returns:\\n      A tuple (state, state), where state is the newly computed state at time t.\\n      It is returned twice to respect an interface that works for LSTMs.\\n    '\n    x = inputs\n    h = state\n    if inputs is not None:\n        xh = tf.concat(axis=1, values=[x, h])\n    else:\n        xh = h\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            (r, u) = tf.split(axis=1, num_or_size_splits=2, value=linear(xh, 2 * self._num_units, alpha=self._weight_scale, name='xh_2_ru', collections=self._collections))\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            xrh = tf.concat(axis=1, values=[x, r * h])\n            c = tf.tanh(linear(xrh, self._num_units, name='xrh_2_c', collections=self._collections))\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Gated recurrent unit (GRU) function.\\n\\n    Args:\\n      inputs: A 2D batch x input_dim tensor of inputs.\\n      state: The previous state from the last time step.\\n      scope (optional): TF variable scope for defined GRU variables.\\n\\n    Returns:\\n      A tuple (state, state), where state is the newly computed state at time t.\\n      It is returned twice to respect an interface that works for LSTMs.\\n    '\n    x = inputs\n    h = state\n    if inputs is not None:\n        xh = tf.concat(axis=1, values=[x, h])\n    else:\n        xh = h\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            (r, u) = tf.split(axis=1, num_or_size_splits=2, value=linear(xh, 2 * self._num_units, alpha=self._weight_scale, name='xh_2_ru', collections=self._collections))\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            xrh = tf.concat(axis=1, values=[x, r * h])\n            c = tf.tanh(linear(xrh, self._num_units, name='xrh_2_c', collections=self._collections))\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, num_units, forget_bias=1.0, input_weight_scale=1.0, rec_weight_scale=1.0, clip_value=np.inf, input_collections=None, recurrent_collections=None):\n    \"\"\"Create a GRU object.\n\n    Args:\n      num_units: Number of units in the GRU\n      forget_bias (optional): Hack to help learning.\n      input_weight_scale (optional): weights are scaled ws/sqrt(#inputs), with\n        ws being the weight scale.\n      rec_weight_scale (optional): weights are scaled ws/sqrt(#inputs),\n        with ws being the weight scale.\n      clip_value (optional): if the recurrent values grow above this value,\n        clip them.\n      input_collections (optional): List of additonal collections variables\n        that input->rec weights should belong to.\n      recurrent_collections (optional): List of additonal collections variables\n        that rec->rec weights should belong to.\n    \"\"\"\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._input_weight_scale = input_weight_scale\n    self._rec_weight_scale = rec_weight_scale\n    self._clip_value = clip_value\n    self._input_collections = input_collections\n    self._rec_collections = recurrent_collections",
        "mutated": [
            "def __init__(self, num_units, forget_bias=1.0, input_weight_scale=1.0, rec_weight_scale=1.0, clip_value=np.inf, input_collections=None, recurrent_collections=None):\n    if False:\n        i = 10\n    'Create a GRU object.\\n\\n    Args:\\n      num_units: Number of units in the GRU\\n      forget_bias (optional): Hack to help learning.\\n      input_weight_scale (optional): weights are scaled ws/sqrt(#inputs), with\\n        ws being the weight scale.\\n      rec_weight_scale (optional): weights are scaled ws/sqrt(#inputs),\\n        with ws being the weight scale.\\n      clip_value (optional): if the recurrent values grow above this value,\\n        clip them.\\n      input_collections (optional): List of additonal collections variables\\n        that input->rec weights should belong to.\\n      recurrent_collections (optional): List of additonal collections variables\\n        that rec->rec weights should belong to.\\n    '\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._input_weight_scale = input_weight_scale\n    self._rec_weight_scale = rec_weight_scale\n    self._clip_value = clip_value\n    self._input_collections = input_collections\n    self._rec_collections = recurrent_collections",
            "def __init__(self, num_units, forget_bias=1.0, input_weight_scale=1.0, rec_weight_scale=1.0, clip_value=np.inf, input_collections=None, recurrent_collections=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a GRU object.\\n\\n    Args:\\n      num_units: Number of units in the GRU\\n      forget_bias (optional): Hack to help learning.\\n      input_weight_scale (optional): weights are scaled ws/sqrt(#inputs), with\\n        ws being the weight scale.\\n      rec_weight_scale (optional): weights are scaled ws/sqrt(#inputs),\\n        with ws being the weight scale.\\n      clip_value (optional): if the recurrent values grow above this value,\\n        clip them.\\n      input_collections (optional): List of additonal collections variables\\n        that input->rec weights should belong to.\\n      recurrent_collections (optional): List of additonal collections variables\\n        that rec->rec weights should belong to.\\n    '\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._input_weight_scale = input_weight_scale\n    self._rec_weight_scale = rec_weight_scale\n    self._clip_value = clip_value\n    self._input_collections = input_collections\n    self._rec_collections = recurrent_collections",
            "def __init__(self, num_units, forget_bias=1.0, input_weight_scale=1.0, rec_weight_scale=1.0, clip_value=np.inf, input_collections=None, recurrent_collections=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a GRU object.\\n\\n    Args:\\n      num_units: Number of units in the GRU\\n      forget_bias (optional): Hack to help learning.\\n      input_weight_scale (optional): weights are scaled ws/sqrt(#inputs), with\\n        ws being the weight scale.\\n      rec_weight_scale (optional): weights are scaled ws/sqrt(#inputs),\\n        with ws being the weight scale.\\n      clip_value (optional): if the recurrent values grow above this value,\\n        clip them.\\n      input_collections (optional): List of additonal collections variables\\n        that input->rec weights should belong to.\\n      recurrent_collections (optional): List of additonal collections variables\\n        that rec->rec weights should belong to.\\n    '\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._input_weight_scale = input_weight_scale\n    self._rec_weight_scale = rec_weight_scale\n    self._clip_value = clip_value\n    self._input_collections = input_collections\n    self._rec_collections = recurrent_collections",
            "def __init__(self, num_units, forget_bias=1.0, input_weight_scale=1.0, rec_weight_scale=1.0, clip_value=np.inf, input_collections=None, recurrent_collections=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a GRU object.\\n\\n    Args:\\n      num_units: Number of units in the GRU\\n      forget_bias (optional): Hack to help learning.\\n      input_weight_scale (optional): weights are scaled ws/sqrt(#inputs), with\\n        ws being the weight scale.\\n      rec_weight_scale (optional): weights are scaled ws/sqrt(#inputs),\\n        with ws being the weight scale.\\n      clip_value (optional): if the recurrent values grow above this value,\\n        clip them.\\n      input_collections (optional): List of additonal collections variables\\n        that input->rec weights should belong to.\\n      recurrent_collections (optional): List of additonal collections variables\\n        that rec->rec weights should belong to.\\n    '\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._input_weight_scale = input_weight_scale\n    self._rec_weight_scale = rec_weight_scale\n    self._clip_value = clip_value\n    self._input_collections = input_collections\n    self._rec_collections = recurrent_collections",
            "def __init__(self, num_units, forget_bias=1.0, input_weight_scale=1.0, rec_weight_scale=1.0, clip_value=np.inf, input_collections=None, recurrent_collections=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a GRU object.\\n\\n    Args:\\n      num_units: Number of units in the GRU\\n      forget_bias (optional): Hack to help learning.\\n      input_weight_scale (optional): weights are scaled ws/sqrt(#inputs), with\\n        ws being the weight scale.\\n      rec_weight_scale (optional): weights are scaled ws/sqrt(#inputs),\\n        with ws being the weight scale.\\n      clip_value (optional): if the recurrent values grow above this value,\\n        clip them.\\n      input_collections (optional): List of additonal collections variables\\n        that input->rec weights should belong to.\\n      recurrent_collections (optional): List of additonal collections variables\\n        that rec->rec weights should belong to.\\n    '\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._input_weight_scale = input_weight_scale\n    self._rec_weight_scale = rec_weight_scale\n    self._clip_value = clip_value\n    self._input_collections = input_collections\n    self._rec_collections = recurrent_collections"
        ]
    },
    {
        "func_name": "state_size",
        "original": "@property\ndef state_size(self):\n    return self._num_units",
        "mutated": [
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n    return self._num_units",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self._num_units",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self._num_units",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self._num_units",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self._num_units"
        ]
    },
    {
        "func_name": "output_size",
        "original": "@property\ndef output_size(self):\n    return self._num_units",
        "mutated": [
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n    return self._num_units",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self._num_units",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self._num_units",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self._num_units",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self._num_units"
        ]
    },
    {
        "func_name": "state_multiplier",
        "original": "@property\ndef state_multiplier(self):\n    return 1",
        "mutated": [
            "@property\ndef state_multiplier(self):\n    if False:\n        i = 10\n    return 1",
            "@property\ndef state_multiplier(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 1",
            "@property\ndef state_multiplier(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 1",
            "@property\ndef state_multiplier(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 1",
            "@property\ndef state_multiplier(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 1"
        ]
    },
    {
        "func_name": "output_from_state",
        "original": "def output_from_state(self, state):\n    \"\"\"Return the output portion of the state.\"\"\"\n    return state",
        "mutated": [
            "def output_from_state(self, state):\n    if False:\n        i = 10\n    'Return the output portion of the state.'\n    return state",
            "def output_from_state(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return the output portion of the state.'\n    return state",
            "def output_from_state(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return the output portion of the state.'\n    return state",
            "def output_from_state(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return the output portion of the state.'\n    return state",
            "def output_from_state(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return the output portion of the state.'\n    return state"
        ]
    },
    {
        "func_name": "__call__",
        "original": "def __call__(self, inputs, state, scope=None):\n    \"\"\"Gated recurrent unit (GRU) function.\n\n    Args:\n      inputs: A 2D batch x input_dim tensor of inputs.\n      state: The previous state from the last time step.\n      scope (optional): TF variable scope for defined GRU variables.\n\n    Returns:\n      A tuple (state, state), where state is the newly computed state at time t.\n      It is returned twice to respect an interface that works for LSTMs.\n    \"\"\"\n    x = inputs\n    h = state\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            r_x = u_x = 0.0\n            if x is not None:\n                (r_x, u_x) = tf.split(axis=1, num_or_size_splits=2, value=linear(x, 2 * self._num_units, alpha=self._input_weight_scale, do_bias=False, name='x_2_ru', normalized=False, collections=self._input_collections))\n            (r_h, u_h) = tf.split(axis=1, num_or_size_splits=2, value=linear(h, 2 * self._num_units, do_bias=True, alpha=self._rec_weight_scale, name='h_2_ru', collections=self._rec_collections))\n            r = r_x + r_h\n            u = u_x + u_h\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            c_x = 0.0\n            if x is not None:\n                c_x = linear(x, self._num_units, name='x_2_c', do_bias=False, alpha=self._input_weight_scale, normalized=False, collections=self._input_collections)\n            c_rh = linear(r * h, self._num_units, name='rh_2_c', do_bias=True, alpha=self._rec_weight_scale, collections=self._rec_collections)\n            c = tf.tanh(c_x + c_rh)\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)",
        "mutated": [
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n    'Gated recurrent unit (GRU) function.\\n\\n    Args:\\n      inputs: A 2D batch x input_dim tensor of inputs.\\n      state: The previous state from the last time step.\\n      scope (optional): TF variable scope for defined GRU variables.\\n\\n    Returns:\\n      A tuple (state, state), where state is the newly computed state at time t.\\n      It is returned twice to respect an interface that works for LSTMs.\\n    '\n    x = inputs\n    h = state\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            r_x = u_x = 0.0\n            if x is not None:\n                (r_x, u_x) = tf.split(axis=1, num_or_size_splits=2, value=linear(x, 2 * self._num_units, alpha=self._input_weight_scale, do_bias=False, name='x_2_ru', normalized=False, collections=self._input_collections))\n            (r_h, u_h) = tf.split(axis=1, num_or_size_splits=2, value=linear(h, 2 * self._num_units, do_bias=True, alpha=self._rec_weight_scale, name='h_2_ru', collections=self._rec_collections))\n            r = r_x + r_h\n            u = u_x + u_h\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            c_x = 0.0\n            if x is not None:\n                c_x = linear(x, self._num_units, name='x_2_c', do_bias=False, alpha=self._input_weight_scale, normalized=False, collections=self._input_collections)\n            c_rh = linear(r * h, self._num_units, name='rh_2_c', do_bias=True, alpha=self._rec_weight_scale, collections=self._rec_collections)\n            c = tf.tanh(c_x + c_rh)\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Gated recurrent unit (GRU) function.\\n\\n    Args:\\n      inputs: A 2D batch x input_dim tensor of inputs.\\n      state: The previous state from the last time step.\\n      scope (optional): TF variable scope for defined GRU variables.\\n\\n    Returns:\\n      A tuple (state, state), where state is the newly computed state at time t.\\n      It is returned twice to respect an interface that works for LSTMs.\\n    '\n    x = inputs\n    h = state\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            r_x = u_x = 0.0\n            if x is not None:\n                (r_x, u_x) = tf.split(axis=1, num_or_size_splits=2, value=linear(x, 2 * self._num_units, alpha=self._input_weight_scale, do_bias=False, name='x_2_ru', normalized=False, collections=self._input_collections))\n            (r_h, u_h) = tf.split(axis=1, num_or_size_splits=2, value=linear(h, 2 * self._num_units, do_bias=True, alpha=self._rec_weight_scale, name='h_2_ru', collections=self._rec_collections))\n            r = r_x + r_h\n            u = u_x + u_h\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            c_x = 0.0\n            if x is not None:\n                c_x = linear(x, self._num_units, name='x_2_c', do_bias=False, alpha=self._input_weight_scale, normalized=False, collections=self._input_collections)\n            c_rh = linear(r * h, self._num_units, name='rh_2_c', do_bias=True, alpha=self._rec_weight_scale, collections=self._rec_collections)\n            c = tf.tanh(c_x + c_rh)\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Gated recurrent unit (GRU) function.\\n\\n    Args:\\n      inputs: A 2D batch x input_dim tensor of inputs.\\n      state: The previous state from the last time step.\\n      scope (optional): TF variable scope for defined GRU variables.\\n\\n    Returns:\\n      A tuple (state, state), where state is the newly computed state at time t.\\n      It is returned twice to respect an interface that works for LSTMs.\\n    '\n    x = inputs\n    h = state\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            r_x = u_x = 0.0\n            if x is not None:\n                (r_x, u_x) = tf.split(axis=1, num_or_size_splits=2, value=linear(x, 2 * self._num_units, alpha=self._input_weight_scale, do_bias=False, name='x_2_ru', normalized=False, collections=self._input_collections))\n            (r_h, u_h) = tf.split(axis=1, num_or_size_splits=2, value=linear(h, 2 * self._num_units, do_bias=True, alpha=self._rec_weight_scale, name='h_2_ru', collections=self._rec_collections))\n            r = r_x + r_h\n            u = u_x + u_h\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            c_x = 0.0\n            if x is not None:\n                c_x = linear(x, self._num_units, name='x_2_c', do_bias=False, alpha=self._input_weight_scale, normalized=False, collections=self._input_collections)\n            c_rh = linear(r * h, self._num_units, name='rh_2_c', do_bias=True, alpha=self._rec_weight_scale, collections=self._rec_collections)\n            c = tf.tanh(c_x + c_rh)\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Gated recurrent unit (GRU) function.\\n\\n    Args:\\n      inputs: A 2D batch x input_dim tensor of inputs.\\n      state: The previous state from the last time step.\\n      scope (optional): TF variable scope for defined GRU variables.\\n\\n    Returns:\\n      A tuple (state, state), where state is the newly computed state at time t.\\n      It is returned twice to respect an interface that works for LSTMs.\\n    '\n    x = inputs\n    h = state\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            r_x = u_x = 0.0\n            if x is not None:\n                (r_x, u_x) = tf.split(axis=1, num_or_size_splits=2, value=linear(x, 2 * self._num_units, alpha=self._input_weight_scale, do_bias=False, name='x_2_ru', normalized=False, collections=self._input_collections))\n            (r_h, u_h) = tf.split(axis=1, num_or_size_splits=2, value=linear(h, 2 * self._num_units, do_bias=True, alpha=self._rec_weight_scale, name='h_2_ru', collections=self._rec_collections))\n            r = r_x + r_h\n            u = u_x + u_h\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            c_x = 0.0\n            if x is not None:\n                c_x = linear(x, self._num_units, name='x_2_c', do_bias=False, alpha=self._input_weight_scale, normalized=False, collections=self._input_collections)\n            c_rh = linear(r * h, self._num_units, name='rh_2_c', do_bias=True, alpha=self._rec_weight_scale, collections=self._rec_collections)\n            c = tf.tanh(c_x + c_rh)\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Gated recurrent unit (GRU) function.\\n\\n    Args:\\n      inputs: A 2D batch x input_dim tensor of inputs.\\n      state: The previous state from the last time step.\\n      scope (optional): TF variable scope for defined GRU variables.\\n\\n    Returns:\\n      A tuple (state, state), where state is the newly computed state at time t.\\n      It is returned twice to respect an interface that works for LSTMs.\\n    '\n    x = inputs\n    h = state\n    with tf.variable_scope(scope or type(self).__name__):\n        with tf.variable_scope('Gates'):\n            r_x = u_x = 0.0\n            if x is not None:\n                (r_x, u_x) = tf.split(axis=1, num_or_size_splits=2, value=linear(x, 2 * self._num_units, alpha=self._input_weight_scale, do_bias=False, name='x_2_ru', normalized=False, collections=self._input_collections))\n            (r_h, u_h) = tf.split(axis=1, num_or_size_splits=2, value=linear(h, 2 * self._num_units, do_bias=True, alpha=self._rec_weight_scale, name='h_2_ru', collections=self._rec_collections))\n            r = r_x + r_h\n            u = u_x + u_h\n            (r, u) = (tf.sigmoid(r), tf.sigmoid(u + self._forget_bias))\n        with tf.variable_scope('Candidate'):\n            c_x = 0.0\n            if x is not None:\n                c_x = linear(x, self._num_units, name='x_2_c', do_bias=False, alpha=self._input_weight_scale, normalized=False, collections=self._input_collections)\n            c_rh = linear(r * h, self._num_units, name='rh_2_c', do_bias=True, alpha=self._rec_weight_scale, collections=self._rec_collections)\n            c = tf.tanh(c_x + c_rh)\n        new_h = u * h + (1 - u) * c\n        new_h = tf.clip_by_value(new_h, -self._clip_value, self._clip_value)\n    return (new_h, new_h)"
        ]
    },
    {
        "func_name": "makelambda",
        "original": "def makelambda(v):\n    return lambda : v",
        "mutated": [
            "def makelambda(v):\n    if False:\n        i = 10\n    return lambda : v",
            "def makelambda(v):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return lambda : v",
            "def makelambda(v):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return lambda : v",
            "def makelambda(v):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return lambda : v",
            "def makelambda(v):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return lambda : v"
        ]
    },
    {
        "func_name": "encode_data",
        "original": "def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n    \"\"\"Encode data for LFADS\n      Args:\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\n          time x batch x data dims.\n        enc_cell: encoder cell\n        name: name of encoder\n        forward_or_reverse: string, encode in forward or reverse direction\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\n      Returns:\n        encoded data as a list with num_steps_to_encode items, in order\n      \"\"\"\n    if forward_or_reverse == 'forward':\n        dstr = '_fwd'\n        time_fwd_or_rev = range(num_steps_to_encode)\n    else:\n        dstr = '_rev'\n        time_fwd_or_rev = reversed(range(num_steps_to_encode))\n    with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n        enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n        enc_state.set_shape([None, enc_cell.state_size])\n    enc_outs = [None] * num_steps_to_encode\n    for (i, t) in enumerate(time_fwd_or_rev):\n        with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n            dataset_t_bxd = dataset_bxtxd[:, t, :]\n            in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n            in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n            if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n            else:\n                enc_input_t_bxfpe = in_fac_t_bxf\n            (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n            enc_outs[t] = enc_out\n    return enc_outs",
        "mutated": [
            "def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n    if False:\n        i = 10\n    'Encode data for LFADS\\n      Args:\\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\\n          time x batch x data dims.\\n        enc_cell: encoder cell\\n        name: name of encoder\\n        forward_or_reverse: string, encode in forward or reverse direction\\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\\n      Returns:\\n        encoded data as a list with num_steps_to_encode items, in order\\n      '\n    if forward_or_reverse == 'forward':\n        dstr = '_fwd'\n        time_fwd_or_rev = range(num_steps_to_encode)\n    else:\n        dstr = '_rev'\n        time_fwd_or_rev = reversed(range(num_steps_to_encode))\n    with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n        enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n        enc_state.set_shape([None, enc_cell.state_size])\n    enc_outs = [None] * num_steps_to_encode\n    for (i, t) in enumerate(time_fwd_or_rev):\n        with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n            dataset_t_bxd = dataset_bxtxd[:, t, :]\n            in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n            in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n            if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n            else:\n                enc_input_t_bxfpe = in_fac_t_bxf\n            (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n            enc_outs[t] = enc_out\n    return enc_outs",
            "def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Encode data for LFADS\\n      Args:\\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\\n          time x batch x data dims.\\n        enc_cell: encoder cell\\n        name: name of encoder\\n        forward_or_reverse: string, encode in forward or reverse direction\\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\\n      Returns:\\n        encoded data as a list with num_steps_to_encode items, in order\\n      '\n    if forward_or_reverse == 'forward':\n        dstr = '_fwd'\n        time_fwd_or_rev = range(num_steps_to_encode)\n    else:\n        dstr = '_rev'\n        time_fwd_or_rev = reversed(range(num_steps_to_encode))\n    with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n        enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n        enc_state.set_shape([None, enc_cell.state_size])\n    enc_outs = [None] * num_steps_to_encode\n    for (i, t) in enumerate(time_fwd_or_rev):\n        with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n            dataset_t_bxd = dataset_bxtxd[:, t, :]\n            in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n            in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n            if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n            else:\n                enc_input_t_bxfpe = in_fac_t_bxf\n            (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n            enc_outs[t] = enc_out\n    return enc_outs",
            "def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Encode data for LFADS\\n      Args:\\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\\n          time x batch x data dims.\\n        enc_cell: encoder cell\\n        name: name of encoder\\n        forward_or_reverse: string, encode in forward or reverse direction\\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\\n      Returns:\\n        encoded data as a list with num_steps_to_encode items, in order\\n      '\n    if forward_or_reverse == 'forward':\n        dstr = '_fwd'\n        time_fwd_or_rev = range(num_steps_to_encode)\n    else:\n        dstr = '_rev'\n        time_fwd_or_rev = reversed(range(num_steps_to_encode))\n    with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n        enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n        enc_state.set_shape([None, enc_cell.state_size])\n    enc_outs = [None] * num_steps_to_encode\n    for (i, t) in enumerate(time_fwd_or_rev):\n        with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n            dataset_t_bxd = dataset_bxtxd[:, t, :]\n            in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n            in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n            if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n            else:\n                enc_input_t_bxfpe = in_fac_t_bxf\n            (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n            enc_outs[t] = enc_out\n    return enc_outs",
            "def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Encode data for LFADS\\n      Args:\\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\\n          time x batch x data dims.\\n        enc_cell: encoder cell\\n        name: name of encoder\\n        forward_or_reverse: string, encode in forward or reverse direction\\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\\n      Returns:\\n        encoded data as a list with num_steps_to_encode items, in order\\n      '\n    if forward_or_reverse == 'forward':\n        dstr = '_fwd'\n        time_fwd_or_rev = range(num_steps_to_encode)\n    else:\n        dstr = '_rev'\n        time_fwd_or_rev = reversed(range(num_steps_to_encode))\n    with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n        enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n        enc_state.set_shape([None, enc_cell.state_size])\n    enc_outs = [None] * num_steps_to_encode\n    for (i, t) in enumerate(time_fwd_or_rev):\n        with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n            dataset_t_bxd = dataset_bxtxd[:, t, :]\n            in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n            in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n            if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n            else:\n                enc_input_t_bxfpe = in_fac_t_bxf\n            (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n            enc_outs[t] = enc_out\n    return enc_outs",
            "def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Encode data for LFADS\\n      Args:\\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\\n          time x batch x data dims.\\n        enc_cell: encoder cell\\n        name: name of encoder\\n        forward_or_reverse: string, encode in forward or reverse direction\\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\\n      Returns:\\n        encoded data as a list with num_steps_to_encode items, in order\\n      '\n    if forward_or_reverse == 'forward':\n        dstr = '_fwd'\n        time_fwd_or_rev = range(num_steps_to_encode)\n    else:\n        dstr = '_rev'\n        time_fwd_or_rev = reversed(range(num_steps_to_encode))\n    with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n        enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n        enc_state.set_shape([None, enc_cell.state_size])\n    enc_outs = [None] * num_steps_to_encode\n    for (i, t) in enumerate(time_fwd_or_rev):\n        with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n            dataset_t_bxd = dataset_bxtxd[:, t, :]\n            in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n            in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n            if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n            else:\n                enc_input_t_bxfpe = in_fac_t_bxf\n            (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n            enc_outs[t] = enc_out\n    return enc_outs"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, hps, kind='train', datasets=None):\n    \"\"\"Create an LFADS model.\n\n       train - a model for training, sampling of posteriors is used\n       posterior_sample_and_average - sample from the posterior, this is used\n         for evaluating the expected value of the outputs of LFADS, given a\n         specific input, by averaging over multiple samples from the approx\n         posterior.  Also used for the lower bound on the negative\n         log-likelihood using IWAE error (Importance Weighed Auto-encoder).\n         This is the denoising operation.\n       prior_sample - a model for generation - sampling from priors is used\n\n    Args:\n      hps: The dictionary of hyper parameters.\n      kind: the type of model to build (see above).\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\n    \"\"\"\n    print('Building graph...')\n    all_kinds = ['train', 'posterior_sample_and_average', 'posterior_push_mean', 'prior_sample']\n    assert kind in all_kinds, 'Wrong kind'\n    if hps.feedback_factors_or_rates == 'rates':\n        assert len(hps.dataset_names) == 1, 'Multiple datasets not supported for rate feedback.'\n    num_steps = hps.num_steps\n    ic_dim = hps.ic_dim\n    co_dim = hps.co_dim\n    ext_input_dim = hps.ext_input_dim\n    cell_class = GRU\n    gen_cell_class = GenGRU\n\n    def makelambda(v):\n        return lambda : v\n    self.dataName = tf.placeholder(tf.string, shape=())\n    if hps.output_dist == 'poisson':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, int), 'Data dtype must be int for poisson output distribution'\n        data_dtype = tf.int32\n    elif hps.output_dist == 'gaussian':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, float), 'Data dtype must be float for gaussian output dsitribution'\n        data_dtype = tf.float32\n    else:\n        assert False, 'NIY'\n    self.dataset_ph = dataset_ph = tf.placeholder(data_dtype, [None, num_steps, None], name='data')\n    self.train_step = tf.get_variable('global_step', [], tf.int64, tf.zeros_initializer(), trainable=False)\n    self.hps = hps\n    ndatasets = hps.ndatasets\n    factors_dim = hps.factors_dim\n    self.preds = preds = [None] * ndatasets\n    self.fns_in_fac_Ws = fns_in_fac_Ws = [None] * ndatasets\n    self.fns_in_fatcor_bs = fns_in_fac_bs = [None] * ndatasets\n    self.fns_out_fac_Ws = fns_out_fac_Ws = [None] * ndatasets\n    self.fns_out_fac_bs = fns_out_fac_bs = [None] * ndatasets\n    self.datasetNames = dataset_names = hps.dataset_names\n    self.ext_inputs = ext_inputs = None\n    if len(dataset_names) == 1:\n        if 'alignment_matrix_cxf' in datasets[dataset_names[0]].keys():\n            used_in_factors_dim = factors_dim\n            in_identity_if_poss = False\n        else:\n            used_in_factors_dim = hps.dataset_dims[dataset_names[0]]\n            in_identity_if_poss = True\n    else:\n        used_in_factors_dim = factors_dim\n        in_identity_if_poss = False\n    for (d, name) in enumerate(dataset_names):\n        data_dim = hps.dataset_dims[name]\n        in_mat_cxf = None\n        in_bias_1xf = None\n        align_bias_1xc = None\n        if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin matrix with alignment matrix provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin matrix to alignment matrix provided for dataset:', name)\n            in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if in_mat_cxf.shape != (data_dim, factors_dim):\n                raise ValueError('Alignment matrix must have dimensions %d x %d\\n          (data_dim x factors_dim), but currently has %d x %d.' % (data_dim, factors_dim, in_mat_cxf.shape[0], in_mat_cxf.shape[1]))\n        if datasets and 'alignment_bias_c' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin bias with alignment bias provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin bias to alignment bias provided for dataset:', name)\n            align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n            align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            if align_bias_1xc.shape[1] != data_dim:\n                raise ValueError('Alignment bias must have dimensions %d\\n          (data_dim), but currently has %d.' % (data_dim, in_mat_cxf.shape[0]))\n            if in_mat_cxf is not None and align_bias_1xc is not None:\n                in_bias_1xf = -np.dot(align_bias_1xc, in_mat_cxf)\n        if hps.do_train_readin:\n            collections_readin = ['IO_transformations']\n        else:\n            collections_readin = None\n        in_fac_lin = init_linear(data_dim, used_in_factors_dim, do_bias=True, mat_init_value=in_mat_cxf, bias_init_value=in_bias_1xf, identity_if_possible=in_identity_if_poss, normalized=False, name='x_2_infac_' + name, collections=collections_readin, trainable=hps.do_train_readin)\n        (in_fac_W, in_fac_b) = in_fac_lin\n        fns_in_fac_Ws[d] = makelambda(in_fac_W)\n        fns_in_fac_bs[d] = makelambda(in_fac_b)\n    with tf.variable_scope('glm'):\n        out_identity_if_poss = False\n        if len(dataset_names) == 1 and factors_dim == hps.dataset_dims[dataset_names[0]]:\n            out_identity_if_poss = True\n        for (d, name) in enumerate(dataset_names):\n            data_dim = hps.dataset_dims[name]\n            in_mat_cxf = None\n            if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n                dataset = datasets[name]\n                in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if datasets and 'alignment_bias_c' in datasets[name].keys():\n                dataset = datasets[name]\n                align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n                align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            out_mat_fxc = None\n            out_bias_1xc = None\n            if in_mat_cxf is not None:\n                out_mat_fxc = in_mat_cxf.T\n            if align_bias_1xc is not None:\n                out_bias_1xc = align_bias_1xc\n            if hps.output_dist == 'poisson':\n                out_fac_lin = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, identity_if_possible=out_identity_if_poss, normalized=False, name='fac_2_logrates_' + name, collections=['IO_transformations'])\n                (out_fac_W, out_fac_b) = out_fac_lin\n            elif hps.output_dist == 'gaussian':\n                out_fac_lin_mean = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, normalized=False, name='fac_2_means_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                mat_init_value = np.zeros([factors_dim, data_dim]).astype(np.float32)\n                bias_init_value = np.ones([1, data_dim]).astype(np.float32)\n                out_fac_lin_logvar = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=mat_init_value, bias_init_value=bias_init_value, normalized=False, name='fac_2_logvars_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                (out_fac_W_logvar, out_fac_b_logvar) = out_fac_lin_logvar\n                out_fac_W = tf.concat(axis=1, values=[out_fac_W_mean, out_fac_W_logvar])\n                out_fac_b = tf.concat(axis=1, values=[out_fac_b_mean, out_fac_b_logvar])\n            else:\n                assert False, 'NIY'\n            preds[d] = tf.equal(tf.constant(name), self.dataName)\n            data_dim = hps.dataset_dims[name]\n            fns_out_fac_Ws[d] = makelambda(out_fac_W)\n            fns_out_fac_bs[d] = makelambda(out_fac_b)\n    pf_pairs_in_fac_Ws = zip(preds, fns_in_fac_Ws)\n    pf_pairs_in_fac_bs = zip(preds, fns_in_fac_bs)\n    pf_pairs_out_fac_Ws = zip(preds, fns_out_fac_Ws)\n    pf_pairs_out_fac_bs = zip(preds, fns_out_fac_bs)\n    this_in_fac_W = tf.case(pf_pairs_in_fac_Ws, exclusive=True)\n    this_in_fac_b = tf.case(pf_pairs_in_fac_bs, exclusive=True)\n    this_out_fac_W = tf.case(pf_pairs_out_fac_Ws, exclusive=True)\n    this_out_fac_b = tf.case(pf_pairs_out_fac_bs, exclusive=True)\n    if hps.ext_input_dim > 0:\n        self.ext_input = tf.placeholder(tf.float32, [None, num_steps, ext_input_dim], name='ext_input')\n    else:\n        self.ext_input = None\n    ext_input_bxtxi = self.ext_input\n    self.keep_prob = keep_prob = tf.placeholder(tf.float32, [], 'keep_prob')\n    self.batch_size = batch_size = int(hps.batch_size)\n    self.learning_rate = tf.Variable(float(hps.learning_rate_init), trainable=False, name='learning_rate')\n    self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * hps.learning_rate_decay_factor)\n    dataset_do_bxtxd = tf.nn.dropout(tf.to_float(dataset_ph), keep_prob)\n    if hps.ext_input_dim > 0:\n        ext_input_do_bxtxi = tf.nn.dropout(ext_input_bxtxi, keep_prob)\n    else:\n        ext_input_do_bxtxi = None\n\n    def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n        \"\"\"Encode data for LFADS\n      Args:\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\n          time x batch x data dims.\n        enc_cell: encoder cell\n        name: name of encoder\n        forward_or_reverse: string, encode in forward or reverse direction\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\n      Returns:\n        encoded data as a list with num_steps_to_encode items, in order\n      \"\"\"\n        if forward_or_reverse == 'forward':\n            dstr = '_fwd'\n            time_fwd_or_rev = range(num_steps_to_encode)\n        else:\n            dstr = '_rev'\n            time_fwd_or_rev = reversed(range(num_steps_to_encode))\n        with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n            enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n            enc_state.set_shape([None, enc_cell.state_size])\n        enc_outs = [None] * num_steps_to_encode\n        for (i, t) in enumerate(time_fwd_or_rev):\n            with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n                dataset_t_bxd = dataset_bxtxd[:, t, :]\n                in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n                in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n                if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                    ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                    enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n                else:\n                    enc_input_t_bxfpe = in_fac_t_bxf\n                (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n                enc_outs[t] = enc_out\n        return enc_outs\n    self.ic_enc_fwd = [None] * num_steps\n    self.ic_enc_rev = [None] * num_steps\n    if ic_dim > 0:\n        enc_ic_cell = cell_class(hps.ic_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ic_enc_fwd = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'forward', hps.num_steps_for_gen_ic)\n        ic_enc_rev = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'reverse', hps.num_steps_for_gen_ic)\n        self.ic_enc_fwd = ic_enc_fwd\n        self.ic_enc_rev = ic_enc_rev\n    self.ci_enc_fwd = [None] * num_steps\n    self.ci_enc_rev = [None] * num_steps\n    if co_dim > 0:\n        enc_ci_cell = cell_class(hps.ci_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ci_enc_fwd = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'forward', hps.num_steps)\n        if hps.do_causal_controller:\n            ci_enc_rev = None\n        else:\n            ci_enc_rev = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'reverse', hps.num_steps)\n        self.ci_enc_fwd = ci_enc_fwd\n        self.ci_enc_rev = ci_enc_rev\n    with tf.variable_scope('z', reuse=False):\n        self.prior_zs_g0 = None\n        self.posterior_zs_g0 = None\n        self.g0s_val = None\n        if ic_dim > 0:\n            self.prior_zs_g0 = LearnableDiagonalGaussian(batch_size, ic_dim, name='prior_g0', mean_init=0.0, var_min=hps.ic_prior_var_min, var_init=hps.ic_prior_var_scale, var_max=hps.ic_prior_var_max)\n            ic_enc = tf.concat(axis=1, values=[ic_enc_fwd[-1], ic_enc_rev[0]])\n            ic_enc = tf.nn.dropout(ic_enc, keep_prob)\n            self.posterior_zs_g0 = DiagonalGaussianFromInput(ic_enc, ic_dim, 'ic_enc_2_post_g0', var_min=hps.ic_post_var_min)\n            if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n                zs_g0 = self.posterior_zs_g0\n            else:\n                zs_g0 = self.prior_zs_g0\n            if kind in ['train', 'posterior_sample_and_average', 'prior_sample']:\n                self.g0s_val = zs_g0.sample\n            else:\n                self.g0s_val = zs_g0.mean\n        self.prior_zs_co = prior_zs_co = [None] * num_steps\n        self.posterior_zs_co = posterior_zs_co = [None] * num_steps\n        self.zs_co = zs_co = [None] * num_steps\n        self.prior_zs_ar_con = None\n        if co_dim > 0:\n            autocorrelation_taus = [hps.prior_ar_atau for x in range(hps.co_dim)]\n            noise_variances = [hps.prior_ar_nvar for x in range(hps.co_dim)]\n            self.prior_zs_ar_con = prior_zs_ar_con = LearnableAutoRegressive1Prior(batch_size, hps.co_dim, autocorrelation_taus, noise_variances, hps.do_train_prior_ar_atau, hps.do_train_prior_ar_nvar, num_steps, 'u_prior_ar1')\n    self.controller_outputs = u_t = [None] * num_steps\n    self.con_ics = con_state = None\n    self.con_states = con_states = [None] * num_steps\n    self.con_outs = con_outs = [None] * num_steps\n    self.gen_inputs = gen_inputs = [None] * num_steps\n    if co_dim > 0:\n        con_cell = gen_cell_class(hps.con_dim, input_weight_scale=hps.cell_weight_scale, rec_weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_con_reg'])\n        with tf.variable_scope('con', reuse=False):\n            self.con_ics = tf.tile(tf.Variable(tf.zeros([1, hps.con_dim * con_cell.state_multiplier]), name='c0'), tf.stack([batch_size, 1]))\n            self.con_ics.set_shape([None, con_cell.state_size])\n            con_states[-1] = self.con_ics\n    gen_cell = gen_cell_class(hps.gen_dim, input_weight_scale=hps.gen_cell_input_weight_scale, rec_weight_scale=hps.gen_cell_rec_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_gen_reg'])\n    with tf.variable_scope('gen', reuse=False):\n        if ic_dim == 0:\n            self.gen_ics = tf.tile(tf.Variable(tf.zeros([1, gen_cell.state_size]), name='g0'), tf.stack([batch_size, 1]))\n        else:\n            self.gen_ics = linear(self.g0s_val, gen_cell.state_size, identity_if_possible=True, name='g0_2_gen_ic')\n        self.gen_states = gen_states = [None] * num_steps\n        self.gen_outs = gen_outs = [None] * num_steps\n        gen_states[-1] = self.gen_ics\n        gen_outs[-1] = gen_cell.output_from_state(gen_states[-1])\n        self.factors = factors = [None] * num_steps\n        factors[-1] = linear(gen_outs[-1], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n    self.rates = rates = [None] * num_steps\n    with tf.variable_scope('glm', reuse=False):\n        if hps.output_dist == 'poisson':\n            log_rates_t0 = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            log_rates_t0.set_shape([None, None])\n            rates[-1] = tf.exp(log_rates_t0)\n            rates[-1].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n        elif hps.output_dist == 'gaussian':\n            mean_n_logvars = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            mean_n_logvars.set_shape([None, None])\n            (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n            rates[-1] = means_t_bxd\n        else:\n            assert False, 'NIY'\n    self.output_dist_params = dist_params = [None] * num_steps\n    self.log_p_xgz_b = log_p_xgz_b = 0.0\n    for t in range(num_steps):\n        if co_dim > 0:\n            tlag = t - hps.controller_input_lag\n            if tlag < 0:\n                con_in_f_t = tf.zeros_like(ci_enc_fwd[0])\n            else:\n                con_in_f_t = ci_enc_fwd[tlag]\n            if hps.do_causal_controller:\n                con_in_list_t = [con_in_f_t]\n            else:\n                tlag_rev = t + hps.controller_input_lag\n                if tlag_rev >= num_steps:\n                    con_in_r_t = tf.zeros_like(ci_enc_rev[0])\n                else:\n                    con_in_r_t = ci_enc_rev[tlag_rev]\n                con_in_list_t = [con_in_f_t, con_in_r_t]\n            if hps.do_feed_factors_to_controller:\n                if hps.feedback_factors_or_rates == 'factors':\n                    con_in_list_t.append(factors[t - 1])\n                elif hps.feedback_factors_or_rates == 'rates':\n                    con_in_list_t.append(rates[t - 1])\n                else:\n                    assert False, 'NIY'\n            con_in_t = tf.concat(axis=1, values=con_in_list_t)\n            con_in_t = tf.nn.dropout(con_in_t, keep_prob)\n            with tf.variable_scope('con', reuse=True if t > 0 else None):\n                (con_outs[t], con_states[t]) = con_cell(con_in_t, con_states[t - 1])\n                posterior_zs_co[t] = DiagonalGaussianFromInput(con_outs[t], co_dim, name='con_to_post_co')\n            if kind == 'train':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_sample_and_average':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_push_mean':\n                u_t[t] = posterior_zs_co[t].mean\n            else:\n                u_t[t] = prior_zs_ar_con.samples_t[t]\n        if ext_input_dim > 0 and hps.inject_ext_input_to_gen:\n            ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n            if co_dim > 0:\n                gen_inputs[t] = tf.concat(axis=1, values=[u_t[t], ext_input_t_bxi])\n            else:\n                gen_inputs[t] = ext_input_t_bxi\n        else:\n            gen_inputs[t] = u_t[t]\n        data_t_bxd = dataset_ph[:, t, :]\n        with tf.variable_scope('gen', reuse=True if t > 0 else None):\n            (gen_outs[t], gen_states[t]) = gen_cell(gen_inputs[t], gen_states[t - 1])\n            gen_outs[t] = tf.nn.dropout(gen_outs[t], keep_prob)\n        with tf.variable_scope('gen', reuse=True):\n            factors[t] = linear(gen_outs[t], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n        with tf.variable_scope('glm', reuse=True if t > 0 else None):\n            if hps.output_dist == 'poisson':\n                log_rates_t = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                log_rates_t.set_shape([None, None])\n                rates[t] = dist_params[t] = tf.exp(log_rates_t)\n                rates[t].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n                loglikelihood_t = Poisson(log_rates_t).logp(data_t_bxd)\n            elif hps.output_dist == 'gaussian':\n                mean_n_logvars = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                mean_n_logvars.set_shape([None, None])\n                (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n                rates[t] = means_t_bxd\n                dist_params[t] = tf.concat(axis=1, values=[means_t_bxd, tf.exp(logvars_t_bxd)])\n                loglikelihood_t = diag_gaussian_log_likelihood(data_t_bxd, means_t_bxd, logvars_t_bxd)\n            else:\n                assert False, 'NIY'\n            log_p_xgz_b += tf.reduce_sum(loglikelihood_t, [1])\n    self.corr_cost = tf.constant(0.0)\n    if hps.co_mean_corr_scale > 0.0:\n        all_sum_corr = []\n        for i in range(hps.co_dim):\n            for j in range(i + 1, hps.co_dim):\n                sum_corr_ij = tf.constant(0.0)\n                for t in range(num_steps):\n                    u_mean_t = posterior_zs_co[t].mean\n                    sum_corr_ij += u_mean_t[:, i] * u_mean_t[:, j]\n                all_sum_corr.append(0.5 * tf.square(sum_corr_ij))\n        self.corr_cost = tf.reduce_mean(all_sum_corr)\n    kl_cost_g0_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    kl_cost_co_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    self.kl_cost = tf.constant(0.0)\n    self.recon_cost = tf.constant(0.0)\n    self.nll_bound_vae = tf.constant(0.0)\n    self.nll_bound_iwae = tf.constant(0.0)\n    if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n        kl_cost_g0_b = 0.0\n        kl_cost_co_b = 0.0\n        if ic_dim > 0:\n            g0_priors = [self.prior_zs_g0]\n            g0_posts = [self.posterior_zs_g0]\n            kl_cost_g0_b = KLCost_GaussianGaussian(g0_posts, g0_priors).kl_cost_b\n            kl_cost_g0_b = hps.kl_ic_weight * kl_cost_g0_b\n        if co_dim > 0:\n            kl_cost_co_b = KLCost_GaussianGaussianProcessSampled(posterior_zs_co, prior_zs_ar_con).kl_cost_b\n            kl_cost_co_b = hps.kl_co_weight * kl_cost_co_b\n        self.recon_cost = -tf.reduce_mean(log_p_xgz_b)\n        self.kl_cost = tf.reduce_mean(kl_cost_g0_b + kl_cost_co_b)\n        lb_on_ll_b = log_p_xgz_b - kl_cost_g0_b - kl_cost_co_b\n        self.nll_bound_vae = -tf.reduce_mean(lb_on_ll_b)\n        k = tf.cast(tf.shape(log_p_xgz_b)[0], tf.float32)\n        iwae_lb_on_ll = -tf.log(k) + log_sum_exp(lb_on_ll_b)\n        self.nll_bound_iwae = -iwae_lb_on_ll\n    self.l2_cost = tf.constant(0.0)\n    if self.hps.l2_gen_scale > 0.0 or self.hps.l2_con_scale > 0.0:\n        l2_costs = []\n        l2_numels = []\n        l2_reg_var_lists = [tf.get_collection('l2_gen_reg'), tf.get_collection('l2_con_reg')]\n        l2_reg_scales = [self.hps.l2_gen_scale, self.hps.l2_con_scale]\n        for (l2_reg_vars, l2_scale) in zip(l2_reg_var_lists, l2_reg_scales):\n            for v in l2_reg_vars:\n                numel = tf.reduce_prod(tf.concat(axis=0, values=tf.shape(v)))\n                numel_f = tf.cast(numel, tf.float32)\n                l2_numels.append(numel_f)\n                v_l2 = tf.reduce_sum(v * v)\n                l2_costs.append(0.5 * l2_scale * v_l2)\n        self.l2_cost = tf.add_n(l2_costs) / tf.add_n(l2_numels)\n    self.kl_decay_step = tf.maximum(self.train_step - hps.kl_start_step, 0)\n    self.l2_decay_step = tf.maximum(self.train_step - hps.l2_start_step, 0)\n    kl_decay_step_f = tf.cast(self.kl_decay_step, tf.float32)\n    l2_decay_step_f = tf.cast(self.l2_decay_step, tf.float32)\n    kl_increase_steps_f = tf.cast(hps.kl_increase_steps, tf.float32)\n    l2_increase_steps_f = tf.cast(hps.l2_increase_steps, tf.float32)\n    self.kl_weight = kl_weight = tf.minimum(kl_decay_step_f / kl_increase_steps_f, 1.0)\n    self.l2_weight = l2_weight = tf.minimum(l2_decay_step_f / l2_increase_steps_f, 1.0)\n    self.timed_kl_cost = kl_weight * self.kl_cost\n    self.timed_l2_cost = l2_weight * self.l2_cost\n    self.weight_corr_cost = hps.co_mean_corr_scale * self.corr_cost\n    self.cost = self.recon_cost + self.timed_kl_cost + self.timed_l2_cost + self.weight_corr_cost\n    if kind != 'train':\n        self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n        self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep_lve)\n        return\n    if self.hps.do_train_io_only:\n        self.train_vars = tvars = tf.get_collection('IO_transformations', scope=tf.get_variable_scope().name)\n    elif self.hps.do_train_encoder_only:\n        tvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/ic_enc_*')\n        tvars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/z/ic_enc_*')\n        self.train_vars = tvars = tvars1 + tvars2\n    else:\n        self.train_vars = tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n    print('done.')\n    print('Model Variables (to be optimized): ')\n    total_params = 0\n    for i in range(len(tvars)):\n        shape = tvars[i].get_shape().as_list()\n        print('    ', i, tvars[i].name, shape)\n        total_params += np.prod(shape)\n    print('Total model parameters: ', total_params)\n    grads = tf.gradients(self.cost, tvars)\n    (grads, grad_global_norm) = tf.clip_by_global_norm(grads, hps.max_grad_norm)\n    opt = tf.train.AdamOptimizer(self.learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n    self.grads = grads\n    self.grad_global_norm = grad_global_norm\n    self.train_op = opt.apply_gradients(zip(grads, tvars), global_step=self.train_step)\n    self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.example_image = tf.placeholder(tf.float32, shape=[1, None, None, 3], name='image_tensor')\n    self.example_summ = tf.summary.image('LFADS example', self.example_image, collections=['example_summaries'])\n    self.lr_summ = tf.summary.scalar('Learning rate', self.learning_rate)\n    self.kl_weight_summ = tf.summary.scalar('KL weight', self.kl_weight)\n    self.l2_weight_summ = tf.summary.scalar('L2 weight', self.l2_weight)\n    self.corr_cost_summ = tf.summary.scalar('Corr cost', self.weight_corr_cost)\n    self.grad_global_norm_summ = tf.summary.scalar('Gradient global norm', self.grad_global_norm)\n    if hps.co_dim > 0:\n        self.atau_summ = [None] * hps.co_dim\n        self.pvar_summ = [None] * hps.co_dim\n        for c in range(hps.co_dim):\n            self.atau_summ[c] = tf.summary.scalar('AR Autocorrelation taus ' + str(c), tf.exp(self.prior_zs_ar_con.logataus_1xu[0, c]))\n            self.pvar_summ[c] = tf.summary.scalar('AR Variances ' + str(c), tf.exp(self.prior_zs_ar_con.logpvars_1xu[0, c]))\n    kl_cost_ph = tf.placeholder(tf.float32, shape=[], name='kl_cost_ph')\n    self.kl_t_cost_summ = tf.summary.scalar('KL cost (train)', kl_cost_ph, collections=['train_summaries'])\n    self.kl_v_cost_summ = tf.summary.scalar('KL cost (valid)', kl_cost_ph, collections=['valid_summaries'])\n    l2_cost_ph = tf.placeholder(tf.float32, shape=[], name='l2_cost_ph')\n    self.l2_cost_summ = tf.summary.scalar('L2 cost', l2_cost_ph, collections=['train_summaries'])\n    recon_cost_ph = tf.placeholder(tf.float32, shape=[], name='recon_cost_ph')\n    self.recon_t_cost_summ = tf.summary.scalar('Reconstruction cost (train)', recon_cost_ph, collections=['train_summaries'])\n    self.recon_v_cost_summ = tf.summary.scalar('Reconstruction cost (valid)', recon_cost_ph, collections=['valid_summaries'])\n    total_cost_ph = tf.placeholder(tf.float32, shape=[], name='total_cost_ph')\n    self.cost_t_summ = tf.summary.scalar('Total cost (train)', total_cost_ph, collections=['train_summaries'])\n    self.cost_v_summ = tf.summary.scalar('Total cost (valid)', total_cost_ph, collections=['valid_summaries'])\n    self.kl_cost_ph = kl_cost_ph\n    self.l2_cost_ph = l2_cost_ph\n    self.recon_cost_ph = recon_cost_ph\n    self.total_cost_ph = total_cost_ph\n    self.merged_examples = tf.summary.merge_all(key='example_summaries')\n    self.merged_generic = tf.summary.merge_all()\n    self.merged_train = tf.summary.merge_all(key='train_summaries')\n    self.merged_valid = tf.summary.merge_all(key='valid_summaries')\n    session = tf.get_default_session()\n    self.logfile = os.path.join(hps.lfads_save_dir, 'lfads_log')\n    self.writer = tf.summary.FileWriter(self.logfile)",
        "mutated": [
            "def __init__(self, hps, kind='train', datasets=None):\n    if False:\n        i = 10\n    'Create an LFADS model.\\n\\n       train - a model for training, sampling of posteriors is used\\n       posterior_sample_and_average - sample from the posterior, this is used\\n         for evaluating the expected value of the outputs of LFADS, given a\\n         specific input, by averaging over multiple samples from the approx\\n         posterior.  Also used for the lower bound on the negative\\n         log-likelihood using IWAE error (Importance Weighed Auto-encoder).\\n         This is the denoising operation.\\n       prior_sample - a model for generation - sampling from priors is used\\n\\n    Args:\\n      hps: The dictionary of hyper parameters.\\n      kind: the type of model to build (see above).\\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\\n    '\n    print('Building graph...')\n    all_kinds = ['train', 'posterior_sample_and_average', 'posterior_push_mean', 'prior_sample']\n    assert kind in all_kinds, 'Wrong kind'\n    if hps.feedback_factors_or_rates == 'rates':\n        assert len(hps.dataset_names) == 1, 'Multiple datasets not supported for rate feedback.'\n    num_steps = hps.num_steps\n    ic_dim = hps.ic_dim\n    co_dim = hps.co_dim\n    ext_input_dim = hps.ext_input_dim\n    cell_class = GRU\n    gen_cell_class = GenGRU\n\n    def makelambda(v):\n        return lambda : v\n    self.dataName = tf.placeholder(tf.string, shape=())\n    if hps.output_dist == 'poisson':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, int), 'Data dtype must be int for poisson output distribution'\n        data_dtype = tf.int32\n    elif hps.output_dist == 'gaussian':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, float), 'Data dtype must be float for gaussian output dsitribution'\n        data_dtype = tf.float32\n    else:\n        assert False, 'NIY'\n    self.dataset_ph = dataset_ph = tf.placeholder(data_dtype, [None, num_steps, None], name='data')\n    self.train_step = tf.get_variable('global_step', [], tf.int64, tf.zeros_initializer(), trainable=False)\n    self.hps = hps\n    ndatasets = hps.ndatasets\n    factors_dim = hps.factors_dim\n    self.preds = preds = [None] * ndatasets\n    self.fns_in_fac_Ws = fns_in_fac_Ws = [None] * ndatasets\n    self.fns_in_fatcor_bs = fns_in_fac_bs = [None] * ndatasets\n    self.fns_out_fac_Ws = fns_out_fac_Ws = [None] * ndatasets\n    self.fns_out_fac_bs = fns_out_fac_bs = [None] * ndatasets\n    self.datasetNames = dataset_names = hps.dataset_names\n    self.ext_inputs = ext_inputs = None\n    if len(dataset_names) == 1:\n        if 'alignment_matrix_cxf' in datasets[dataset_names[0]].keys():\n            used_in_factors_dim = factors_dim\n            in_identity_if_poss = False\n        else:\n            used_in_factors_dim = hps.dataset_dims[dataset_names[0]]\n            in_identity_if_poss = True\n    else:\n        used_in_factors_dim = factors_dim\n        in_identity_if_poss = False\n    for (d, name) in enumerate(dataset_names):\n        data_dim = hps.dataset_dims[name]\n        in_mat_cxf = None\n        in_bias_1xf = None\n        align_bias_1xc = None\n        if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin matrix with alignment matrix provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin matrix to alignment matrix provided for dataset:', name)\n            in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if in_mat_cxf.shape != (data_dim, factors_dim):\n                raise ValueError('Alignment matrix must have dimensions %d x %d\\n          (data_dim x factors_dim), but currently has %d x %d.' % (data_dim, factors_dim, in_mat_cxf.shape[0], in_mat_cxf.shape[1]))\n        if datasets and 'alignment_bias_c' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin bias with alignment bias provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin bias to alignment bias provided for dataset:', name)\n            align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n            align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            if align_bias_1xc.shape[1] != data_dim:\n                raise ValueError('Alignment bias must have dimensions %d\\n          (data_dim), but currently has %d.' % (data_dim, in_mat_cxf.shape[0]))\n            if in_mat_cxf is not None and align_bias_1xc is not None:\n                in_bias_1xf = -np.dot(align_bias_1xc, in_mat_cxf)\n        if hps.do_train_readin:\n            collections_readin = ['IO_transformations']\n        else:\n            collections_readin = None\n        in_fac_lin = init_linear(data_dim, used_in_factors_dim, do_bias=True, mat_init_value=in_mat_cxf, bias_init_value=in_bias_1xf, identity_if_possible=in_identity_if_poss, normalized=False, name='x_2_infac_' + name, collections=collections_readin, trainable=hps.do_train_readin)\n        (in_fac_W, in_fac_b) = in_fac_lin\n        fns_in_fac_Ws[d] = makelambda(in_fac_W)\n        fns_in_fac_bs[d] = makelambda(in_fac_b)\n    with tf.variable_scope('glm'):\n        out_identity_if_poss = False\n        if len(dataset_names) == 1 and factors_dim == hps.dataset_dims[dataset_names[0]]:\n            out_identity_if_poss = True\n        for (d, name) in enumerate(dataset_names):\n            data_dim = hps.dataset_dims[name]\n            in_mat_cxf = None\n            if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n                dataset = datasets[name]\n                in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if datasets and 'alignment_bias_c' in datasets[name].keys():\n                dataset = datasets[name]\n                align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n                align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            out_mat_fxc = None\n            out_bias_1xc = None\n            if in_mat_cxf is not None:\n                out_mat_fxc = in_mat_cxf.T\n            if align_bias_1xc is not None:\n                out_bias_1xc = align_bias_1xc\n            if hps.output_dist == 'poisson':\n                out_fac_lin = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, identity_if_possible=out_identity_if_poss, normalized=False, name='fac_2_logrates_' + name, collections=['IO_transformations'])\n                (out_fac_W, out_fac_b) = out_fac_lin\n            elif hps.output_dist == 'gaussian':\n                out_fac_lin_mean = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, normalized=False, name='fac_2_means_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                mat_init_value = np.zeros([factors_dim, data_dim]).astype(np.float32)\n                bias_init_value = np.ones([1, data_dim]).astype(np.float32)\n                out_fac_lin_logvar = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=mat_init_value, bias_init_value=bias_init_value, normalized=False, name='fac_2_logvars_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                (out_fac_W_logvar, out_fac_b_logvar) = out_fac_lin_logvar\n                out_fac_W = tf.concat(axis=1, values=[out_fac_W_mean, out_fac_W_logvar])\n                out_fac_b = tf.concat(axis=1, values=[out_fac_b_mean, out_fac_b_logvar])\n            else:\n                assert False, 'NIY'\n            preds[d] = tf.equal(tf.constant(name), self.dataName)\n            data_dim = hps.dataset_dims[name]\n            fns_out_fac_Ws[d] = makelambda(out_fac_W)\n            fns_out_fac_bs[d] = makelambda(out_fac_b)\n    pf_pairs_in_fac_Ws = zip(preds, fns_in_fac_Ws)\n    pf_pairs_in_fac_bs = zip(preds, fns_in_fac_bs)\n    pf_pairs_out_fac_Ws = zip(preds, fns_out_fac_Ws)\n    pf_pairs_out_fac_bs = zip(preds, fns_out_fac_bs)\n    this_in_fac_W = tf.case(pf_pairs_in_fac_Ws, exclusive=True)\n    this_in_fac_b = tf.case(pf_pairs_in_fac_bs, exclusive=True)\n    this_out_fac_W = tf.case(pf_pairs_out_fac_Ws, exclusive=True)\n    this_out_fac_b = tf.case(pf_pairs_out_fac_bs, exclusive=True)\n    if hps.ext_input_dim > 0:\n        self.ext_input = tf.placeholder(tf.float32, [None, num_steps, ext_input_dim], name='ext_input')\n    else:\n        self.ext_input = None\n    ext_input_bxtxi = self.ext_input\n    self.keep_prob = keep_prob = tf.placeholder(tf.float32, [], 'keep_prob')\n    self.batch_size = batch_size = int(hps.batch_size)\n    self.learning_rate = tf.Variable(float(hps.learning_rate_init), trainable=False, name='learning_rate')\n    self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * hps.learning_rate_decay_factor)\n    dataset_do_bxtxd = tf.nn.dropout(tf.to_float(dataset_ph), keep_prob)\n    if hps.ext_input_dim > 0:\n        ext_input_do_bxtxi = tf.nn.dropout(ext_input_bxtxi, keep_prob)\n    else:\n        ext_input_do_bxtxi = None\n\n    def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n        \"\"\"Encode data for LFADS\n      Args:\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\n          time x batch x data dims.\n        enc_cell: encoder cell\n        name: name of encoder\n        forward_or_reverse: string, encode in forward or reverse direction\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\n      Returns:\n        encoded data as a list with num_steps_to_encode items, in order\n      \"\"\"\n        if forward_or_reverse == 'forward':\n            dstr = '_fwd'\n            time_fwd_or_rev = range(num_steps_to_encode)\n        else:\n            dstr = '_rev'\n            time_fwd_or_rev = reversed(range(num_steps_to_encode))\n        with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n            enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n            enc_state.set_shape([None, enc_cell.state_size])\n        enc_outs = [None] * num_steps_to_encode\n        for (i, t) in enumerate(time_fwd_or_rev):\n            with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n                dataset_t_bxd = dataset_bxtxd[:, t, :]\n                in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n                in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n                if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                    ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                    enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n                else:\n                    enc_input_t_bxfpe = in_fac_t_bxf\n                (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n                enc_outs[t] = enc_out\n        return enc_outs\n    self.ic_enc_fwd = [None] * num_steps\n    self.ic_enc_rev = [None] * num_steps\n    if ic_dim > 0:\n        enc_ic_cell = cell_class(hps.ic_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ic_enc_fwd = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'forward', hps.num_steps_for_gen_ic)\n        ic_enc_rev = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'reverse', hps.num_steps_for_gen_ic)\n        self.ic_enc_fwd = ic_enc_fwd\n        self.ic_enc_rev = ic_enc_rev\n    self.ci_enc_fwd = [None] * num_steps\n    self.ci_enc_rev = [None] * num_steps\n    if co_dim > 0:\n        enc_ci_cell = cell_class(hps.ci_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ci_enc_fwd = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'forward', hps.num_steps)\n        if hps.do_causal_controller:\n            ci_enc_rev = None\n        else:\n            ci_enc_rev = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'reverse', hps.num_steps)\n        self.ci_enc_fwd = ci_enc_fwd\n        self.ci_enc_rev = ci_enc_rev\n    with tf.variable_scope('z', reuse=False):\n        self.prior_zs_g0 = None\n        self.posterior_zs_g0 = None\n        self.g0s_val = None\n        if ic_dim > 0:\n            self.prior_zs_g0 = LearnableDiagonalGaussian(batch_size, ic_dim, name='prior_g0', mean_init=0.0, var_min=hps.ic_prior_var_min, var_init=hps.ic_prior_var_scale, var_max=hps.ic_prior_var_max)\n            ic_enc = tf.concat(axis=1, values=[ic_enc_fwd[-1], ic_enc_rev[0]])\n            ic_enc = tf.nn.dropout(ic_enc, keep_prob)\n            self.posterior_zs_g0 = DiagonalGaussianFromInput(ic_enc, ic_dim, 'ic_enc_2_post_g0', var_min=hps.ic_post_var_min)\n            if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n                zs_g0 = self.posterior_zs_g0\n            else:\n                zs_g0 = self.prior_zs_g0\n            if kind in ['train', 'posterior_sample_and_average', 'prior_sample']:\n                self.g0s_val = zs_g0.sample\n            else:\n                self.g0s_val = zs_g0.mean\n        self.prior_zs_co = prior_zs_co = [None] * num_steps\n        self.posterior_zs_co = posterior_zs_co = [None] * num_steps\n        self.zs_co = zs_co = [None] * num_steps\n        self.prior_zs_ar_con = None\n        if co_dim > 0:\n            autocorrelation_taus = [hps.prior_ar_atau for x in range(hps.co_dim)]\n            noise_variances = [hps.prior_ar_nvar for x in range(hps.co_dim)]\n            self.prior_zs_ar_con = prior_zs_ar_con = LearnableAutoRegressive1Prior(batch_size, hps.co_dim, autocorrelation_taus, noise_variances, hps.do_train_prior_ar_atau, hps.do_train_prior_ar_nvar, num_steps, 'u_prior_ar1')\n    self.controller_outputs = u_t = [None] * num_steps\n    self.con_ics = con_state = None\n    self.con_states = con_states = [None] * num_steps\n    self.con_outs = con_outs = [None] * num_steps\n    self.gen_inputs = gen_inputs = [None] * num_steps\n    if co_dim > 0:\n        con_cell = gen_cell_class(hps.con_dim, input_weight_scale=hps.cell_weight_scale, rec_weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_con_reg'])\n        with tf.variable_scope('con', reuse=False):\n            self.con_ics = tf.tile(tf.Variable(tf.zeros([1, hps.con_dim * con_cell.state_multiplier]), name='c0'), tf.stack([batch_size, 1]))\n            self.con_ics.set_shape([None, con_cell.state_size])\n            con_states[-1] = self.con_ics\n    gen_cell = gen_cell_class(hps.gen_dim, input_weight_scale=hps.gen_cell_input_weight_scale, rec_weight_scale=hps.gen_cell_rec_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_gen_reg'])\n    with tf.variable_scope('gen', reuse=False):\n        if ic_dim == 0:\n            self.gen_ics = tf.tile(tf.Variable(tf.zeros([1, gen_cell.state_size]), name='g0'), tf.stack([batch_size, 1]))\n        else:\n            self.gen_ics = linear(self.g0s_val, gen_cell.state_size, identity_if_possible=True, name='g0_2_gen_ic')\n        self.gen_states = gen_states = [None] * num_steps\n        self.gen_outs = gen_outs = [None] * num_steps\n        gen_states[-1] = self.gen_ics\n        gen_outs[-1] = gen_cell.output_from_state(gen_states[-1])\n        self.factors = factors = [None] * num_steps\n        factors[-1] = linear(gen_outs[-1], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n    self.rates = rates = [None] * num_steps\n    with tf.variable_scope('glm', reuse=False):\n        if hps.output_dist == 'poisson':\n            log_rates_t0 = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            log_rates_t0.set_shape([None, None])\n            rates[-1] = tf.exp(log_rates_t0)\n            rates[-1].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n        elif hps.output_dist == 'gaussian':\n            mean_n_logvars = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            mean_n_logvars.set_shape([None, None])\n            (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n            rates[-1] = means_t_bxd\n        else:\n            assert False, 'NIY'\n    self.output_dist_params = dist_params = [None] * num_steps\n    self.log_p_xgz_b = log_p_xgz_b = 0.0\n    for t in range(num_steps):\n        if co_dim > 0:\n            tlag = t - hps.controller_input_lag\n            if tlag < 0:\n                con_in_f_t = tf.zeros_like(ci_enc_fwd[0])\n            else:\n                con_in_f_t = ci_enc_fwd[tlag]\n            if hps.do_causal_controller:\n                con_in_list_t = [con_in_f_t]\n            else:\n                tlag_rev = t + hps.controller_input_lag\n                if tlag_rev >= num_steps:\n                    con_in_r_t = tf.zeros_like(ci_enc_rev[0])\n                else:\n                    con_in_r_t = ci_enc_rev[tlag_rev]\n                con_in_list_t = [con_in_f_t, con_in_r_t]\n            if hps.do_feed_factors_to_controller:\n                if hps.feedback_factors_or_rates == 'factors':\n                    con_in_list_t.append(factors[t - 1])\n                elif hps.feedback_factors_or_rates == 'rates':\n                    con_in_list_t.append(rates[t - 1])\n                else:\n                    assert False, 'NIY'\n            con_in_t = tf.concat(axis=1, values=con_in_list_t)\n            con_in_t = tf.nn.dropout(con_in_t, keep_prob)\n            with tf.variable_scope('con', reuse=True if t > 0 else None):\n                (con_outs[t], con_states[t]) = con_cell(con_in_t, con_states[t - 1])\n                posterior_zs_co[t] = DiagonalGaussianFromInput(con_outs[t], co_dim, name='con_to_post_co')\n            if kind == 'train':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_sample_and_average':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_push_mean':\n                u_t[t] = posterior_zs_co[t].mean\n            else:\n                u_t[t] = prior_zs_ar_con.samples_t[t]\n        if ext_input_dim > 0 and hps.inject_ext_input_to_gen:\n            ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n            if co_dim > 0:\n                gen_inputs[t] = tf.concat(axis=1, values=[u_t[t], ext_input_t_bxi])\n            else:\n                gen_inputs[t] = ext_input_t_bxi\n        else:\n            gen_inputs[t] = u_t[t]\n        data_t_bxd = dataset_ph[:, t, :]\n        with tf.variable_scope('gen', reuse=True if t > 0 else None):\n            (gen_outs[t], gen_states[t]) = gen_cell(gen_inputs[t], gen_states[t - 1])\n            gen_outs[t] = tf.nn.dropout(gen_outs[t], keep_prob)\n        with tf.variable_scope('gen', reuse=True):\n            factors[t] = linear(gen_outs[t], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n        with tf.variable_scope('glm', reuse=True if t > 0 else None):\n            if hps.output_dist == 'poisson':\n                log_rates_t = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                log_rates_t.set_shape([None, None])\n                rates[t] = dist_params[t] = tf.exp(log_rates_t)\n                rates[t].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n                loglikelihood_t = Poisson(log_rates_t).logp(data_t_bxd)\n            elif hps.output_dist == 'gaussian':\n                mean_n_logvars = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                mean_n_logvars.set_shape([None, None])\n                (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n                rates[t] = means_t_bxd\n                dist_params[t] = tf.concat(axis=1, values=[means_t_bxd, tf.exp(logvars_t_bxd)])\n                loglikelihood_t = diag_gaussian_log_likelihood(data_t_bxd, means_t_bxd, logvars_t_bxd)\n            else:\n                assert False, 'NIY'\n            log_p_xgz_b += tf.reduce_sum(loglikelihood_t, [1])\n    self.corr_cost = tf.constant(0.0)\n    if hps.co_mean_corr_scale > 0.0:\n        all_sum_corr = []\n        for i in range(hps.co_dim):\n            for j in range(i + 1, hps.co_dim):\n                sum_corr_ij = tf.constant(0.0)\n                for t in range(num_steps):\n                    u_mean_t = posterior_zs_co[t].mean\n                    sum_corr_ij += u_mean_t[:, i] * u_mean_t[:, j]\n                all_sum_corr.append(0.5 * tf.square(sum_corr_ij))\n        self.corr_cost = tf.reduce_mean(all_sum_corr)\n    kl_cost_g0_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    kl_cost_co_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    self.kl_cost = tf.constant(0.0)\n    self.recon_cost = tf.constant(0.0)\n    self.nll_bound_vae = tf.constant(0.0)\n    self.nll_bound_iwae = tf.constant(0.0)\n    if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n        kl_cost_g0_b = 0.0\n        kl_cost_co_b = 0.0\n        if ic_dim > 0:\n            g0_priors = [self.prior_zs_g0]\n            g0_posts = [self.posterior_zs_g0]\n            kl_cost_g0_b = KLCost_GaussianGaussian(g0_posts, g0_priors).kl_cost_b\n            kl_cost_g0_b = hps.kl_ic_weight * kl_cost_g0_b\n        if co_dim > 0:\n            kl_cost_co_b = KLCost_GaussianGaussianProcessSampled(posterior_zs_co, prior_zs_ar_con).kl_cost_b\n            kl_cost_co_b = hps.kl_co_weight * kl_cost_co_b\n        self.recon_cost = -tf.reduce_mean(log_p_xgz_b)\n        self.kl_cost = tf.reduce_mean(kl_cost_g0_b + kl_cost_co_b)\n        lb_on_ll_b = log_p_xgz_b - kl_cost_g0_b - kl_cost_co_b\n        self.nll_bound_vae = -tf.reduce_mean(lb_on_ll_b)\n        k = tf.cast(tf.shape(log_p_xgz_b)[0], tf.float32)\n        iwae_lb_on_ll = -tf.log(k) + log_sum_exp(lb_on_ll_b)\n        self.nll_bound_iwae = -iwae_lb_on_ll\n    self.l2_cost = tf.constant(0.0)\n    if self.hps.l2_gen_scale > 0.0 or self.hps.l2_con_scale > 0.0:\n        l2_costs = []\n        l2_numels = []\n        l2_reg_var_lists = [tf.get_collection('l2_gen_reg'), tf.get_collection('l2_con_reg')]\n        l2_reg_scales = [self.hps.l2_gen_scale, self.hps.l2_con_scale]\n        for (l2_reg_vars, l2_scale) in zip(l2_reg_var_lists, l2_reg_scales):\n            for v in l2_reg_vars:\n                numel = tf.reduce_prod(tf.concat(axis=0, values=tf.shape(v)))\n                numel_f = tf.cast(numel, tf.float32)\n                l2_numels.append(numel_f)\n                v_l2 = tf.reduce_sum(v * v)\n                l2_costs.append(0.5 * l2_scale * v_l2)\n        self.l2_cost = tf.add_n(l2_costs) / tf.add_n(l2_numels)\n    self.kl_decay_step = tf.maximum(self.train_step - hps.kl_start_step, 0)\n    self.l2_decay_step = tf.maximum(self.train_step - hps.l2_start_step, 0)\n    kl_decay_step_f = tf.cast(self.kl_decay_step, tf.float32)\n    l2_decay_step_f = tf.cast(self.l2_decay_step, tf.float32)\n    kl_increase_steps_f = tf.cast(hps.kl_increase_steps, tf.float32)\n    l2_increase_steps_f = tf.cast(hps.l2_increase_steps, tf.float32)\n    self.kl_weight = kl_weight = tf.minimum(kl_decay_step_f / kl_increase_steps_f, 1.0)\n    self.l2_weight = l2_weight = tf.minimum(l2_decay_step_f / l2_increase_steps_f, 1.0)\n    self.timed_kl_cost = kl_weight * self.kl_cost\n    self.timed_l2_cost = l2_weight * self.l2_cost\n    self.weight_corr_cost = hps.co_mean_corr_scale * self.corr_cost\n    self.cost = self.recon_cost + self.timed_kl_cost + self.timed_l2_cost + self.weight_corr_cost\n    if kind != 'train':\n        self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n        self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep_lve)\n        return\n    if self.hps.do_train_io_only:\n        self.train_vars = tvars = tf.get_collection('IO_transformations', scope=tf.get_variable_scope().name)\n    elif self.hps.do_train_encoder_only:\n        tvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/ic_enc_*')\n        tvars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/z/ic_enc_*')\n        self.train_vars = tvars = tvars1 + tvars2\n    else:\n        self.train_vars = tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n    print('done.')\n    print('Model Variables (to be optimized): ')\n    total_params = 0\n    for i in range(len(tvars)):\n        shape = tvars[i].get_shape().as_list()\n        print('    ', i, tvars[i].name, shape)\n        total_params += np.prod(shape)\n    print('Total model parameters: ', total_params)\n    grads = tf.gradients(self.cost, tvars)\n    (grads, grad_global_norm) = tf.clip_by_global_norm(grads, hps.max_grad_norm)\n    opt = tf.train.AdamOptimizer(self.learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n    self.grads = grads\n    self.grad_global_norm = grad_global_norm\n    self.train_op = opt.apply_gradients(zip(grads, tvars), global_step=self.train_step)\n    self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.example_image = tf.placeholder(tf.float32, shape=[1, None, None, 3], name='image_tensor')\n    self.example_summ = tf.summary.image('LFADS example', self.example_image, collections=['example_summaries'])\n    self.lr_summ = tf.summary.scalar('Learning rate', self.learning_rate)\n    self.kl_weight_summ = tf.summary.scalar('KL weight', self.kl_weight)\n    self.l2_weight_summ = tf.summary.scalar('L2 weight', self.l2_weight)\n    self.corr_cost_summ = tf.summary.scalar('Corr cost', self.weight_corr_cost)\n    self.grad_global_norm_summ = tf.summary.scalar('Gradient global norm', self.grad_global_norm)\n    if hps.co_dim > 0:\n        self.atau_summ = [None] * hps.co_dim\n        self.pvar_summ = [None] * hps.co_dim\n        for c in range(hps.co_dim):\n            self.atau_summ[c] = tf.summary.scalar('AR Autocorrelation taus ' + str(c), tf.exp(self.prior_zs_ar_con.logataus_1xu[0, c]))\n            self.pvar_summ[c] = tf.summary.scalar('AR Variances ' + str(c), tf.exp(self.prior_zs_ar_con.logpvars_1xu[0, c]))\n    kl_cost_ph = tf.placeholder(tf.float32, shape=[], name='kl_cost_ph')\n    self.kl_t_cost_summ = tf.summary.scalar('KL cost (train)', kl_cost_ph, collections=['train_summaries'])\n    self.kl_v_cost_summ = tf.summary.scalar('KL cost (valid)', kl_cost_ph, collections=['valid_summaries'])\n    l2_cost_ph = tf.placeholder(tf.float32, shape=[], name='l2_cost_ph')\n    self.l2_cost_summ = tf.summary.scalar('L2 cost', l2_cost_ph, collections=['train_summaries'])\n    recon_cost_ph = tf.placeholder(tf.float32, shape=[], name='recon_cost_ph')\n    self.recon_t_cost_summ = tf.summary.scalar('Reconstruction cost (train)', recon_cost_ph, collections=['train_summaries'])\n    self.recon_v_cost_summ = tf.summary.scalar('Reconstruction cost (valid)', recon_cost_ph, collections=['valid_summaries'])\n    total_cost_ph = tf.placeholder(tf.float32, shape=[], name='total_cost_ph')\n    self.cost_t_summ = tf.summary.scalar('Total cost (train)', total_cost_ph, collections=['train_summaries'])\n    self.cost_v_summ = tf.summary.scalar('Total cost (valid)', total_cost_ph, collections=['valid_summaries'])\n    self.kl_cost_ph = kl_cost_ph\n    self.l2_cost_ph = l2_cost_ph\n    self.recon_cost_ph = recon_cost_ph\n    self.total_cost_ph = total_cost_ph\n    self.merged_examples = tf.summary.merge_all(key='example_summaries')\n    self.merged_generic = tf.summary.merge_all()\n    self.merged_train = tf.summary.merge_all(key='train_summaries')\n    self.merged_valid = tf.summary.merge_all(key='valid_summaries')\n    session = tf.get_default_session()\n    self.logfile = os.path.join(hps.lfads_save_dir, 'lfads_log')\n    self.writer = tf.summary.FileWriter(self.logfile)",
            "def __init__(self, hps, kind='train', datasets=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create an LFADS model.\\n\\n       train - a model for training, sampling of posteriors is used\\n       posterior_sample_and_average - sample from the posterior, this is used\\n         for evaluating the expected value of the outputs of LFADS, given a\\n         specific input, by averaging over multiple samples from the approx\\n         posterior.  Also used for the lower bound on the negative\\n         log-likelihood using IWAE error (Importance Weighed Auto-encoder).\\n         This is the denoising operation.\\n       prior_sample - a model for generation - sampling from priors is used\\n\\n    Args:\\n      hps: The dictionary of hyper parameters.\\n      kind: the type of model to build (see above).\\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\\n    '\n    print('Building graph...')\n    all_kinds = ['train', 'posterior_sample_and_average', 'posterior_push_mean', 'prior_sample']\n    assert kind in all_kinds, 'Wrong kind'\n    if hps.feedback_factors_or_rates == 'rates':\n        assert len(hps.dataset_names) == 1, 'Multiple datasets not supported for rate feedback.'\n    num_steps = hps.num_steps\n    ic_dim = hps.ic_dim\n    co_dim = hps.co_dim\n    ext_input_dim = hps.ext_input_dim\n    cell_class = GRU\n    gen_cell_class = GenGRU\n\n    def makelambda(v):\n        return lambda : v\n    self.dataName = tf.placeholder(tf.string, shape=())\n    if hps.output_dist == 'poisson':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, int), 'Data dtype must be int for poisson output distribution'\n        data_dtype = tf.int32\n    elif hps.output_dist == 'gaussian':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, float), 'Data dtype must be float for gaussian output dsitribution'\n        data_dtype = tf.float32\n    else:\n        assert False, 'NIY'\n    self.dataset_ph = dataset_ph = tf.placeholder(data_dtype, [None, num_steps, None], name='data')\n    self.train_step = tf.get_variable('global_step', [], tf.int64, tf.zeros_initializer(), trainable=False)\n    self.hps = hps\n    ndatasets = hps.ndatasets\n    factors_dim = hps.factors_dim\n    self.preds = preds = [None] * ndatasets\n    self.fns_in_fac_Ws = fns_in_fac_Ws = [None] * ndatasets\n    self.fns_in_fatcor_bs = fns_in_fac_bs = [None] * ndatasets\n    self.fns_out_fac_Ws = fns_out_fac_Ws = [None] * ndatasets\n    self.fns_out_fac_bs = fns_out_fac_bs = [None] * ndatasets\n    self.datasetNames = dataset_names = hps.dataset_names\n    self.ext_inputs = ext_inputs = None\n    if len(dataset_names) == 1:\n        if 'alignment_matrix_cxf' in datasets[dataset_names[0]].keys():\n            used_in_factors_dim = factors_dim\n            in_identity_if_poss = False\n        else:\n            used_in_factors_dim = hps.dataset_dims[dataset_names[0]]\n            in_identity_if_poss = True\n    else:\n        used_in_factors_dim = factors_dim\n        in_identity_if_poss = False\n    for (d, name) in enumerate(dataset_names):\n        data_dim = hps.dataset_dims[name]\n        in_mat_cxf = None\n        in_bias_1xf = None\n        align_bias_1xc = None\n        if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin matrix with alignment matrix provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin matrix to alignment matrix provided for dataset:', name)\n            in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if in_mat_cxf.shape != (data_dim, factors_dim):\n                raise ValueError('Alignment matrix must have dimensions %d x %d\\n          (data_dim x factors_dim), but currently has %d x %d.' % (data_dim, factors_dim, in_mat_cxf.shape[0], in_mat_cxf.shape[1]))\n        if datasets and 'alignment_bias_c' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin bias with alignment bias provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin bias to alignment bias provided for dataset:', name)\n            align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n            align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            if align_bias_1xc.shape[1] != data_dim:\n                raise ValueError('Alignment bias must have dimensions %d\\n          (data_dim), but currently has %d.' % (data_dim, in_mat_cxf.shape[0]))\n            if in_mat_cxf is not None and align_bias_1xc is not None:\n                in_bias_1xf = -np.dot(align_bias_1xc, in_mat_cxf)\n        if hps.do_train_readin:\n            collections_readin = ['IO_transformations']\n        else:\n            collections_readin = None\n        in_fac_lin = init_linear(data_dim, used_in_factors_dim, do_bias=True, mat_init_value=in_mat_cxf, bias_init_value=in_bias_1xf, identity_if_possible=in_identity_if_poss, normalized=False, name='x_2_infac_' + name, collections=collections_readin, trainable=hps.do_train_readin)\n        (in_fac_W, in_fac_b) = in_fac_lin\n        fns_in_fac_Ws[d] = makelambda(in_fac_W)\n        fns_in_fac_bs[d] = makelambda(in_fac_b)\n    with tf.variable_scope('glm'):\n        out_identity_if_poss = False\n        if len(dataset_names) == 1 and factors_dim == hps.dataset_dims[dataset_names[0]]:\n            out_identity_if_poss = True\n        for (d, name) in enumerate(dataset_names):\n            data_dim = hps.dataset_dims[name]\n            in_mat_cxf = None\n            if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n                dataset = datasets[name]\n                in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if datasets and 'alignment_bias_c' in datasets[name].keys():\n                dataset = datasets[name]\n                align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n                align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            out_mat_fxc = None\n            out_bias_1xc = None\n            if in_mat_cxf is not None:\n                out_mat_fxc = in_mat_cxf.T\n            if align_bias_1xc is not None:\n                out_bias_1xc = align_bias_1xc\n            if hps.output_dist == 'poisson':\n                out_fac_lin = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, identity_if_possible=out_identity_if_poss, normalized=False, name='fac_2_logrates_' + name, collections=['IO_transformations'])\n                (out_fac_W, out_fac_b) = out_fac_lin\n            elif hps.output_dist == 'gaussian':\n                out_fac_lin_mean = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, normalized=False, name='fac_2_means_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                mat_init_value = np.zeros([factors_dim, data_dim]).astype(np.float32)\n                bias_init_value = np.ones([1, data_dim]).astype(np.float32)\n                out_fac_lin_logvar = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=mat_init_value, bias_init_value=bias_init_value, normalized=False, name='fac_2_logvars_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                (out_fac_W_logvar, out_fac_b_logvar) = out_fac_lin_logvar\n                out_fac_W = tf.concat(axis=1, values=[out_fac_W_mean, out_fac_W_logvar])\n                out_fac_b = tf.concat(axis=1, values=[out_fac_b_mean, out_fac_b_logvar])\n            else:\n                assert False, 'NIY'\n            preds[d] = tf.equal(tf.constant(name), self.dataName)\n            data_dim = hps.dataset_dims[name]\n            fns_out_fac_Ws[d] = makelambda(out_fac_W)\n            fns_out_fac_bs[d] = makelambda(out_fac_b)\n    pf_pairs_in_fac_Ws = zip(preds, fns_in_fac_Ws)\n    pf_pairs_in_fac_bs = zip(preds, fns_in_fac_bs)\n    pf_pairs_out_fac_Ws = zip(preds, fns_out_fac_Ws)\n    pf_pairs_out_fac_bs = zip(preds, fns_out_fac_bs)\n    this_in_fac_W = tf.case(pf_pairs_in_fac_Ws, exclusive=True)\n    this_in_fac_b = tf.case(pf_pairs_in_fac_bs, exclusive=True)\n    this_out_fac_W = tf.case(pf_pairs_out_fac_Ws, exclusive=True)\n    this_out_fac_b = tf.case(pf_pairs_out_fac_bs, exclusive=True)\n    if hps.ext_input_dim > 0:\n        self.ext_input = tf.placeholder(tf.float32, [None, num_steps, ext_input_dim], name='ext_input')\n    else:\n        self.ext_input = None\n    ext_input_bxtxi = self.ext_input\n    self.keep_prob = keep_prob = tf.placeholder(tf.float32, [], 'keep_prob')\n    self.batch_size = batch_size = int(hps.batch_size)\n    self.learning_rate = tf.Variable(float(hps.learning_rate_init), trainable=False, name='learning_rate')\n    self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * hps.learning_rate_decay_factor)\n    dataset_do_bxtxd = tf.nn.dropout(tf.to_float(dataset_ph), keep_prob)\n    if hps.ext_input_dim > 0:\n        ext_input_do_bxtxi = tf.nn.dropout(ext_input_bxtxi, keep_prob)\n    else:\n        ext_input_do_bxtxi = None\n\n    def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n        \"\"\"Encode data for LFADS\n      Args:\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\n          time x batch x data dims.\n        enc_cell: encoder cell\n        name: name of encoder\n        forward_or_reverse: string, encode in forward or reverse direction\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\n      Returns:\n        encoded data as a list with num_steps_to_encode items, in order\n      \"\"\"\n        if forward_or_reverse == 'forward':\n            dstr = '_fwd'\n            time_fwd_or_rev = range(num_steps_to_encode)\n        else:\n            dstr = '_rev'\n            time_fwd_or_rev = reversed(range(num_steps_to_encode))\n        with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n            enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n            enc_state.set_shape([None, enc_cell.state_size])\n        enc_outs = [None] * num_steps_to_encode\n        for (i, t) in enumerate(time_fwd_or_rev):\n            with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n                dataset_t_bxd = dataset_bxtxd[:, t, :]\n                in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n                in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n                if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                    ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                    enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n                else:\n                    enc_input_t_bxfpe = in_fac_t_bxf\n                (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n                enc_outs[t] = enc_out\n        return enc_outs\n    self.ic_enc_fwd = [None] * num_steps\n    self.ic_enc_rev = [None] * num_steps\n    if ic_dim > 0:\n        enc_ic_cell = cell_class(hps.ic_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ic_enc_fwd = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'forward', hps.num_steps_for_gen_ic)\n        ic_enc_rev = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'reverse', hps.num_steps_for_gen_ic)\n        self.ic_enc_fwd = ic_enc_fwd\n        self.ic_enc_rev = ic_enc_rev\n    self.ci_enc_fwd = [None] * num_steps\n    self.ci_enc_rev = [None] * num_steps\n    if co_dim > 0:\n        enc_ci_cell = cell_class(hps.ci_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ci_enc_fwd = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'forward', hps.num_steps)\n        if hps.do_causal_controller:\n            ci_enc_rev = None\n        else:\n            ci_enc_rev = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'reverse', hps.num_steps)\n        self.ci_enc_fwd = ci_enc_fwd\n        self.ci_enc_rev = ci_enc_rev\n    with tf.variable_scope('z', reuse=False):\n        self.prior_zs_g0 = None\n        self.posterior_zs_g0 = None\n        self.g0s_val = None\n        if ic_dim > 0:\n            self.prior_zs_g0 = LearnableDiagonalGaussian(batch_size, ic_dim, name='prior_g0', mean_init=0.0, var_min=hps.ic_prior_var_min, var_init=hps.ic_prior_var_scale, var_max=hps.ic_prior_var_max)\n            ic_enc = tf.concat(axis=1, values=[ic_enc_fwd[-1], ic_enc_rev[0]])\n            ic_enc = tf.nn.dropout(ic_enc, keep_prob)\n            self.posterior_zs_g0 = DiagonalGaussianFromInput(ic_enc, ic_dim, 'ic_enc_2_post_g0', var_min=hps.ic_post_var_min)\n            if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n                zs_g0 = self.posterior_zs_g0\n            else:\n                zs_g0 = self.prior_zs_g0\n            if kind in ['train', 'posterior_sample_and_average', 'prior_sample']:\n                self.g0s_val = zs_g0.sample\n            else:\n                self.g0s_val = zs_g0.mean\n        self.prior_zs_co = prior_zs_co = [None] * num_steps\n        self.posterior_zs_co = posterior_zs_co = [None] * num_steps\n        self.zs_co = zs_co = [None] * num_steps\n        self.prior_zs_ar_con = None\n        if co_dim > 0:\n            autocorrelation_taus = [hps.prior_ar_atau for x in range(hps.co_dim)]\n            noise_variances = [hps.prior_ar_nvar for x in range(hps.co_dim)]\n            self.prior_zs_ar_con = prior_zs_ar_con = LearnableAutoRegressive1Prior(batch_size, hps.co_dim, autocorrelation_taus, noise_variances, hps.do_train_prior_ar_atau, hps.do_train_prior_ar_nvar, num_steps, 'u_prior_ar1')\n    self.controller_outputs = u_t = [None] * num_steps\n    self.con_ics = con_state = None\n    self.con_states = con_states = [None] * num_steps\n    self.con_outs = con_outs = [None] * num_steps\n    self.gen_inputs = gen_inputs = [None] * num_steps\n    if co_dim > 0:\n        con_cell = gen_cell_class(hps.con_dim, input_weight_scale=hps.cell_weight_scale, rec_weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_con_reg'])\n        with tf.variable_scope('con', reuse=False):\n            self.con_ics = tf.tile(tf.Variable(tf.zeros([1, hps.con_dim * con_cell.state_multiplier]), name='c0'), tf.stack([batch_size, 1]))\n            self.con_ics.set_shape([None, con_cell.state_size])\n            con_states[-1] = self.con_ics\n    gen_cell = gen_cell_class(hps.gen_dim, input_weight_scale=hps.gen_cell_input_weight_scale, rec_weight_scale=hps.gen_cell_rec_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_gen_reg'])\n    with tf.variable_scope('gen', reuse=False):\n        if ic_dim == 0:\n            self.gen_ics = tf.tile(tf.Variable(tf.zeros([1, gen_cell.state_size]), name='g0'), tf.stack([batch_size, 1]))\n        else:\n            self.gen_ics = linear(self.g0s_val, gen_cell.state_size, identity_if_possible=True, name='g0_2_gen_ic')\n        self.gen_states = gen_states = [None] * num_steps\n        self.gen_outs = gen_outs = [None] * num_steps\n        gen_states[-1] = self.gen_ics\n        gen_outs[-1] = gen_cell.output_from_state(gen_states[-1])\n        self.factors = factors = [None] * num_steps\n        factors[-1] = linear(gen_outs[-1], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n    self.rates = rates = [None] * num_steps\n    with tf.variable_scope('glm', reuse=False):\n        if hps.output_dist == 'poisson':\n            log_rates_t0 = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            log_rates_t0.set_shape([None, None])\n            rates[-1] = tf.exp(log_rates_t0)\n            rates[-1].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n        elif hps.output_dist == 'gaussian':\n            mean_n_logvars = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            mean_n_logvars.set_shape([None, None])\n            (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n            rates[-1] = means_t_bxd\n        else:\n            assert False, 'NIY'\n    self.output_dist_params = dist_params = [None] * num_steps\n    self.log_p_xgz_b = log_p_xgz_b = 0.0\n    for t in range(num_steps):\n        if co_dim > 0:\n            tlag = t - hps.controller_input_lag\n            if tlag < 0:\n                con_in_f_t = tf.zeros_like(ci_enc_fwd[0])\n            else:\n                con_in_f_t = ci_enc_fwd[tlag]\n            if hps.do_causal_controller:\n                con_in_list_t = [con_in_f_t]\n            else:\n                tlag_rev = t + hps.controller_input_lag\n                if tlag_rev >= num_steps:\n                    con_in_r_t = tf.zeros_like(ci_enc_rev[0])\n                else:\n                    con_in_r_t = ci_enc_rev[tlag_rev]\n                con_in_list_t = [con_in_f_t, con_in_r_t]\n            if hps.do_feed_factors_to_controller:\n                if hps.feedback_factors_or_rates == 'factors':\n                    con_in_list_t.append(factors[t - 1])\n                elif hps.feedback_factors_or_rates == 'rates':\n                    con_in_list_t.append(rates[t - 1])\n                else:\n                    assert False, 'NIY'\n            con_in_t = tf.concat(axis=1, values=con_in_list_t)\n            con_in_t = tf.nn.dropout(con_in_t, keep_prob)\n            with tf.variable_scope('con', reuse=True if t > 0 else None):\n                (con_outs[t], con_states[t]) = con_cell(con_in_t, con_states[t - 1])\n                posterior_zs_co[t] = DiagonalGaussianFromInput(con_outs[t], co_dim, name='con_to_post_co')\n            if kind == 'train':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_sample_and_average':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_push_mean':\n                u_t[t] = posterior_zs_co[t].mean\n            else:\n                u_t[t] = prior_zs_ar_con.samples_t[t]\n        if ext_input_dim > 0 and hps.inject_ext_input_to_gen:\n            ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n            if co_dim > 0:\n                gen_inputs[t] = tf.concat(axis=1, values=[u_t[t], ext_input_t_bxi])\n            else:\n                gen_inputs[t] = ext_input_t_bxi\n        else:\n            gen_inputs[t] = u_t[t]\n        data_t_bxd = dataset_ph[:, t, :]\n        with tf.variable_scope('gen', reuse=True if t > 0 else None):\n            (gen_outs[t], gen_states[t]) = gen_cell(gen_inputs[t], gen_states[t - 1])\n            gen_outs[t] = tf.nn.dropout(gen_outs[t], keep_prob)\n        with tf.variable_scope('gen', reuse=True):\n            factors[t] = linear(gen_outs[t], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n        with tf.variable_scope('glm', reuse=True if t > 0 else None):\n            if hps.output_dist == 'poisson':\n                log_rates_t = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                log_rates_t.set_shape([None, None])\n                rates[t] = dist_params[t] = tf.exp(log_rates_t)\n                rates[t].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n                loglikelihood_t = Poisson(log_rates_t).logp(data_t_bxd)\n            elif hps.output_dist == 'gaussian':\n                mean_n_logvars = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                mean_n_logvars.set_shape([None, None])\n                (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n                rates[t] = means_t_bxd\n                dist_params[t] = tf.concat(axis=1, values=[means_t_bxd, tf.exp(logvars_t_bxd)])\n                loglikelihood_t = diag_gaussian_log_likelihood(data_t_bxd, means_t_bxd, logvars_t_bxd)\n            else:\n                assert False, 'NIY'\n            log_p_xgz_b += tf.reduce_sum(loglikelihood_t, [1])\n    self.corr_cost = tf.constant(0.0)\n    if hps.co_mean_corr_scale > 0.0:\n        all_sum_corr = []\n        for i in range(hps.co_dim):\n            for j in range(i + 1, hps.co_dim):\n                sum_corr_ij = tf.constant(0.0)\n                for t in range(num_steps):\n                    u_mean_t = posterior_zs_co[t].mean\n                    sum_corr_ij += u_mean_t[:, i] * u_mean_t[:, j]\n                all_sum_corr.append(0.5 * tf.square(sum_corr_ij))\n        self.corr_cost = tf.reduce_mean(all_sum_corr)\n    kl_cost_g0_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    kl_cost_co_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    self.kl_cost = tf.constant(0.0)\n    self.recon_cost = tf.constant(0.0)\n    self.nll_bound_vae = tf.constant(0.0)\n    self.nll_bound_iwae = tf.constant(0.0)\n    if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n        kl_cost_g0_b = 0.0\n        kl_cost_co_b = 0.0\n        if ic_dim > 0:\n            g0_priors = [self.prior_zs_g0]\n            g0_posts = [self.posterior_zs_g0]\n            kl_cost_g0_b = KLCost_GaussianGaussian(g0_posts, g0_priors).kl_cost_b\n            kl_cost_g0_b = hps.kl_ic_weight * kl_cost_g0_b\n        if co_dim > 0:\n            kl_cost_co_b = KLCost_GaussianGaussianProcessSampled(posterior_zs_co, prior_zs_ar_con).kl_cost_b\n            kl_cost_co_b = hps.kl_co_weight * kl_cost_co_b\n        self.recon_cost = -tf.reduce_mean(log_p_xgz_b)\n        self.kl_cost = tf.reduce_mean(kl_cost_g0_b + kl_cost_co_b)\n        lb_on_ll_b = log_p_xgz_b - kl_cost_g0_b - kl_cost_co_b\n        self.nll_bound_vae = -tf.reduce_mean(lb_on_ll_b)\n        k = tf.cast(tf.shape(log_p_xgz_b)[0], tf.float32)\n        iwae_lb_on_ll = -tf.log(k) + log_sum_exp(lb_on_ll_b)\n        self.nll_bound_iwae = -iwae_lb_on_ll\n    self.l2_cost = tf.constant(0.0)\n    if self.hps.l2_gen_scale > 0.0 or self.hps.l2_con_scale > 0.0:\n        l2_costs = []\n        l2_numels = []\n        l2_reg_var_lists = [tf.get_collection('l2_gen_reg'), tf.get_collection('l2_con_reg')]\n        l2_reg_scales = [self.hps.l2_gen_scale, self.hps.l2_con_scale]\n        for (l2_reg_vars, l2_scale) in zip(l2_reg_var_lists, l2_reg_scales):\n            for v in l2_reg_vars:\n                numel = tf.reduce_prod(tf.concat(axis=0, values=tf.shape(v)))\n                numel_f = tf.cast(numel, tf.float32)\n                l2_numels.append(numel_f)\n                v_l2 = tf.reduce_sum(v * v)\n                l2_costs.append(0.5 * l2_scale * v_l2)\n        self.l2_cost = tf.add_n(l2_costs) / tf.add_n(l2_numels)\n    self.kl_decay_step = tf.maximum(self.train_step - hps.kl_start_step, 0)\n    self.l2_decay_step = tf.maximum(self.train_step - hps.l2_start_step, 0)\n    kl_decay_step_f = tf.cast(self.kl_decay_step, tf.float32)\n    l2_decay_step_f = tf.cast(self.l2_decay_step, tf.float32)\n    kl_increase_steps_f = tf.cast(hps.kl_increase_steps, tf.float32)\n    l2_increase_steps_f = tf.cast(hps.l2_increase_steps, tf.float32)\n    self.kl_weight = kl_weight = tf.minimum(kl_decay_step_f / kl_increase_steps_f, 1.0)\n    self.l2_weight = l2_weight = tf.minimum(l2_decay_step_f / l2_increase_steps_f, 1.0)\n    self.timed_kl_cost = kl_weight * self.kl_cost\n    self.timed_l2_cost = l2_weight * self.l2_cost\n    self.weight_corr_cost = hps.co_mean_corr_scale * self.corr_cost\n    self.cost = self.recon_cost + self.timed_kl_cost + self.timed_l2_cost + self.weight_corr_cost\n    if kind != 'train':\n        self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n        self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep_lve)\n        return\n    if self.hps.do_train_io_only:\n        self.train_vars = tvars = tf.get_collection('IO_transformations', scope=tf.get_variable_scope().name)\n    elif self.hps.do_train_encoder_only:\n        tvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/ic_enc_*')\n        tvars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/z/ic_enc_*')\n        self.train_vars = tvars = tvars1 + tvars2\n    else:\n        self.train_vars = tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n    print('done.')\n    print('Model Variables (to be optimized): ')\n    total_params = 0\n    for i in range(len(tvars)):\n        shape = tvars[i].get_shape().as_list()\n        print('    ', i, tvars[i].name, shape)\n        total_params += np.prod(shape)\n    print('Total model parameters: ', total_params)\n    grads = tf.gradients(self.cost, tvars)\n    (grads, grad_global_norm) = tf.clip_by_global_norm(grads, hps.max_grad_norm)\n    opt = tf.train.AdamOptimizer(self.learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n    self.grads = grads\n    self.grad_global_norm = grad_global_norm\n    self.train_op = opt.apply_gradients(zip(grads, tvars), global_step=self.train_step)\n    self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.example_image = tf.placeholder(tf.float32, shape=[1, None, None, 3], name='image_tensor')\n    self.example_summ = tf.summary.image('LFADS example', self.example_image, collections=['example_summaries'])\n    self.lr_summ = tf.summary.scalar('Learning rate', self.learning_rate)\n    self.kl_weight_summ = tf.summary.scalar('KL weight', self.kl_weight)\n    self.l2_weight_summ = tf.summary.scalar('L2 weight', self.l2_weight)\n    self.corr_cost_summ = tf.summary.scalar('Corr cost', self.weight_corr_cost)\n    self.grad_global_norm_summ = tf.summary.scalar('Gradient global norm', self.grad_global_norm)\n    if hps.co_dim > 0:\n        self.atau_summ = [None] * hps.co_dim\n        self.pvar_summ = [None] * hps.co_dim\n        for c in range(hps.co_dim):\n            self.atau_summ[c] = tf.summary.scalar('AR Autocorrelation taus ' + str(c), tf.exp(self.prior_zs_ar_con.logataus_1xu[0, c]))\n            self.pvar_summ[c] = tf.summary.scalar('AR Variances ' + str(c), tf.exp(self.prior_zs_ar_con.logpvars_1xu[0, c]))\n    kl_cost_ph = tf.placeholder(tf.float32, shape=[], name='kl_cost_ph')\n    self.kl_t_cost_summ = tf.summary.scalar('KL cost (train)', kl_cost_ph, collections=['train_summaries'])\n    self.kl_v_cost_summ = tf.summary.scalar('KL cost (valid)', kl_cost_ph, collections=['valid_summaries'])\n    l2_cost_ph = tf.placeholder(tf.float32, shape=[], name='l2_cost_ph')\n    self.l2_cost_summ = tf.summary.scalar('L2 cost', l2_cost_ph, collections=['train_summaries'])\n    recon_cost_ph = tf.placeholder(tf.float32, shape=[], name='recon_cost_ph')\n    self.recon_t_cost_summ = tf.summary.scalar('Reconstruction cost (train)', recon_cost_ph, collections=['train_summaries'])\n    self.recon_v_cost_summ = tf.summary.scalar('Reconstruction cost (valid)', recon_cost_ph, collections=['valid_summaries'])\n    total_cost_ph = tf.placeholder(tf.float32, shape=[], name='total_cost_ph')\n    self.cost_t_summ = tf.summary.scalar('Total cost (train)', total_cost_ph, collections=['train_summaries'])\n    self.cost_v_summ = tf.summary.scalar('Total cost (valid)', total_cost_ph, collections=['valid_summaries'])\n    self.kl_cost_ph = kl_cost_ph\n    self.l2_cost_ph = l2_cost_ph\n    self.recon_cost_ph = recon_cost_ph\n    self.total_cost_ph = total_cost_ph\n    self.merged_examples = tf.summary.merge_all(key='example_summaries')\n    self.merged_generic = tf.summary.merge_all()\n    self.merged_train = tf.summary.merge_all(key='train_summaries')\n    self.merged_valid = tf.summary.merge_all(key='valid_summaries')\n    session = tf.get_default_session()\n    self.logfile = os.path.join(hps.lfads_save_dir, 'lfads_log')\n    self.writer = tf.summary.FileWriter(self.logfile)",
            "def __init__(self, hps, kind='train', datasets=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create an LFADS model.\\n\\n       train - a model for training, sampling of posteriors is used\\n       posterior_sample_and_average - sample from the posterior, this is used\\n         for evaluating the expected value of the outputs of LFADS, given a\\n         specific input, by averaging over multiple samples from the approx\\n         posterior.  Also used for the lower bound on the negative\\n         log-likelihood using IWAE error (Importance Weighed Auto-encoder).\\n         This is the denoising operation.\\n       prior_sample - a model for generation - sampling from priors is used\\n\\n    Args:\\n      hps: The dictionary of hyper parameters.\\n      kind: the type of model to build (see above).\\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\\n    '\n    print('Building graph...')\n    all_kinds = ['train', 'posterior_sample_and_average', 'posterior_push_mean', 'prior_sample']\n    assert kind in all_kinds, 'Wrong kind'\n    if hps.feedback_factors_or_rates == 'rates':\n        assert len(hps.dataset_names) == 1, 'Multiple datasets not supported for rate feedback.'\n    num_steps = hps.num_steps\n    ic_dim = hps.ic_dim\n    co_dim = hps.co_dim\n    ext_input_dim = hps.ext_input_dim\n    cell_class = GRU\n    gen_cell_class = GenGRU\n\n    def makelambda(v):\n        return lambda : v\n    self.dataName = tf.placeholder(tf.string, shape=())\n    if hps.output_dist == 'poisson':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, int), 'Data dtype must be int for poisson output distribution'\n        data_dtype = tf.int32\n    elif hps.output_dist == 'gaussian':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, float), 'Data dtype must be float for gaussian output dsitribution'\n        data_dtype = tf.float32\n    else:\n        assert False, 'NIY'\n    self.dataset_ph = dataset_ph = tf.placeholder(data_dtype, [None, num_steps, None], name='data')\n    self.train_step = tf.get_variable('global_step', [], tf.int64, tf.zeros_initializer(), trainable=False)\n    self.hps = hps\n    ndatasets = hps.ndatasets\n    factors_dim = hps.factors_dim\n    self.preds = preds = [None] * ndatasets\n    self.fns_in_fac_Ws = fns_in_fac_Ws = [None] * ndatasets\n    self.fns_in_fatcor_bs = fns_in_fac_bs = [None] * ndatasets\n    self.fns_out_fac_Ws = fns_out_fac_Ws = [None] * ndatasets\n    self.fns_out_fac_bs = fns_out_fac_bs = [None] * ndatasets\n    self.datasetNames = dataset_names = hps.dataset_names\n    self.ext_inputs = ext_inputs = None\n    if len(dataset_names) == 1:\n        if 'alignment_matrix_cxf' in datasets[dataset_names[0]].keys():\n            used_in_factors_dim = factors_dim\n            in_identity_if_poss = False\n        else:\n            used_in_factors_dim = hps.dataset_dims[dataset_names[0]]\n            in_identity_if_poss = True\n    else:\n        used_in_factors_dim = factors_dim\n        in_identity_if_poss = False\n    for (d, name) in enumerate(dataset_names):\n        data_dim = hps.dataset_dims[name]\n        in_mat_cxf = None\n        in_bias_1xf = None\n        align_bias_1xc = None\n        if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin matrix with alignment matrix provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin matrix to alignment matrix provided for dataset:', name)\n            in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if in_mat_cxf.shape != (data_dim, factors_dim):\n                raise ValueError('Alignment matrix must have dimensions %d x %d\\n          (data_dim x factors_dim), but currently has %d x %d.' % (data_dim, factors_dim, in_mat_cxf.shape[0], in_mat_cxf.shape[1]))\n        if datasets and 'alignment_bias_c' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin bias with alignment bias provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin bias to alignment bias provided for dataset:', name)\n            align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n            align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            if align_bias_1xc.shape[1] != data_dim:\n                raise ValueError('Alignment bias must have dimensions %d\\n          (data_dim), but currently has %d.' % (data_dim, in_mat_cxf.shape[0]))\n            if in_mat_cxf is not None and align_bias_1xc is not None:\n                in_bias_1xf = -np.dot(align_bias_1xc, in_mat_cxf)\n        if hps.do_train_readin:\n            collections_readin = ['IO_transformations']\n        else:\n            collections_readin = None\n        in_fac_lin = init_linear(data_dim, used_in_factors_dim, do_bias=True, mat_init_value=in_mat_cxf, bias_init_value=in_bias_1xf, identity_if_possible=in_identity_if_poss, normalized=False, name='x_2_infac_' + name, collections=collections_readin, trainable=hps.do_train_readin)\n        (in_fac_W, in_fac_b) = in_fac_lin\n        fns_in_fac_Ws[d] = makelambda(in_fac_W)\n        fns_in_fac_bs[d] = makelambda(in_fac_b)\n    with tf.variable_scope('glm'):\n        out_identity_if_poss = False\n        if len(dataset_names) == 1 and factors_dim == hps.dataset_dims[dataset_names[0]]:\n            out_identity_if_poss = True\n        for (d, name) in enumerate(dataset_names):\n            data_dim = hps.dataset_dims[name]\n            in_mat_cxf = None\n            if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n                dataset = datasets[name]\n                in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if datasets and 'alignment_bias_c' in datasets[name].keys():\n                dataset = datasets[name]\n                align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n                align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            out_mat_fxc = None\n            out_bias_1xc = None\n            if in_mat_cxf is not None:\n                out_mat_fxc = in_mat_cxf.T\n            if align_bias_1xc is not None:\n                out_bias_1xc = align_bias_1xc\n            if hps.output_dist == 'poisson':\n                out_fac_lin = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, identity_if_possible=out_identity_if_poss, normalized=False, name='fac_2_logrates_' + name, collections=['IO_transformations'])\n                (out_fac_W, out_fac_b) = out_fac_lin\n            elif hps.output_dist == 'gaussian':\n                out_fac_lin_mean = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, normalized=False, name='fac_2_means_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                mat_init_value = np.zeros([factors_dim, data_dim]).astype(np.float32)\n                bias_init_value = np.ones([1, data_dim]).astype(np.float32)\n                out_fac_lin_logvar = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=mat_init_value, bias_init_value=bias_init_value, normalized=False, name='fac_2_logvars_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                (out_fac_W_logvar, out_fac_b_logvar) = out_fac_lin_logvar\n                out_fac_W = tf.concat(axis=1, values=[out_fac_W_mean, out_fac_W_logvar])\n                out_fac_b = tf.concat(axis=1, values=[out_fac_b_mean, out_fac_b_logvar])\n            else:\n                assert False, 'NIY'\n            preds[d] = tf.equal(tf.constant(name), self.dataName)\n            data_dim = hps.dataset_dims[name]\n            fns_out_fac_Ws[d] = makelambda(out_fac_W)\n            fns_out_fac_bs[d] = makelambda(out_fac_b)\n    pf_pairs_in_fac_Ws = zip(preds, fns_in_fac_Ws)\n    pf_pairs_in_fac_bs = zip(preds, fns_in_fac_bs)\n    pf_pairs_out_fac_Ws = zip(preds, fns_out_fac_Ws)\n    pf_pairs_out_fac_bs = zip(preds, fns_out_fac_bs)\n    this_in_fac_W = tf.case(pf_pairs_in_fac_Ws, exclusive=True)\n    this_in_fac_b = tf.case(pf_pairs_in_fac_bs, exclusive=True)\n    this_out_fac_W = tf.case(pf_pairs_out_fac_Ws, exclusive=True)\n    this_out_fac_b = tf.case(pf_pairs_out_fac_bs, exclusive=True)\n    if hps.ext_input_dim > 0:\n        self.ext_input = tf.placeholder(tf.float32, [None, num_steps, ext_input_dim], name='ext_input')\n    else:\n        self.ext_input = None\n    ext_input_bxtxi = self.ext_input\n    self.keep_prob = keep_prob = tf.placeholder(tf.float32, [], 'keep_prob')\n    self.batch_size = batch_size = int(hps.batch_size)\n    self.learning_rate = tf.Variable(float(hps.learning_rate_init), trainable=False, name='learning_rate')\n    self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * hps.learning_rate_decay_factor)\n    dataset_do_bxtxd = tf.nn.dropout(tf.to_float(dataset_ph), keep_prob)\n    if hps.ext_input_dim > 0:\n        ext_input_do_bxtxi = tf.nn.dropout(ext_input_bxtxi, keep_prob)\n    else:\n        ext_input_do_bxtxi = None\n\n    def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n        \"\"\"Encode data for LFADS\n      Args:\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\n          time x batch x data dims.\n        enc_cell: encoder cell\n        name: name of encoder\n        forward_or_reverse: string, encode in forward or reverse direction\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\n      Returns:\n        encoded data as a list with num_steps_to_encode items, in order\n      \"\"\"\n        if forward_or_reverse == 'forward':\n            dstr = '_fwd'\n            time_fwd_or_rev = range(num_steps_to_encode)\n        else:\n            dstr = '_rev'\n            time_fwd_or_rev = reversed(range(num_steps_to_encode))\n        with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n            enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n            enc_state.set_shape([None, enc_cell.state_size])\n        enc_outs = [None] * num_steps_to_encode\n        for (i, t) in enumerate(time_fwd_or_rev):\n            with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n                dataset_t_bxd = dataset_bxtxd[:, t, :]\n                in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n                in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n                if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                    ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                    enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n                else:\n                    enc_input_t_bxfpe = in_fac_t_bxf\n                (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n                enc_outs[t] = enc_out\n        return enc_outs\n    self.ic_enc_fwd = [None] * num_steps\n    self.ic_enc_rev = [None] * num_steps\n    if ic_dim > 0:\n        enc_ic_cell = cell_class(hps.ic_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ic_enc_fwd = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'forward', hps.num_steps_for_gen_ic)\n        ic_enc_rev = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'reverse', hps.num_steps_for_gen_ic)\n        self.ic_enc_fwd = ic_enc_fwd\n        self.ic_enc_rev = ic_enc_rev\n    self.ci_enc_fwd = [None] * num_steps\n    self.ci_enc_rev = [None] * num_steps\n    if co_dim > 0:\n        enc_ci_cell = cell_class(hps.ci_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ci_enc_fwd = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'forward', hps.num_steps)\n        if hps.do_causal_controller:\n            ci_enc_rev = None\n        else:\n            ci_enc_rev = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'reverse', hps.num_steps)\n        self.ci_enc_fwd = ci_enc_fwd\n        self.ci_enc_rev = ci_enc_rev\n    with tf.variable_scope('z', reuse=False):\n        self.prior_zs_g0 = None\n        self.posterior_zs_g0 = None\n        self.g0s_val = None\n        if ic_dim > 0:\n            self.prior_zs_g0 = LearnableDiagonalGaussian(batch_size, ic_dim, name='prior_g0', mean_init=0.0, var_min=hps.ic_prior_var_min, var_init=hps.ic_prior_var_scale, var_max=hps.ic_prior_var_max)\n            ic_enc = tf.concat(axis=1, values=[ic_enc_fwd[-1], ic_enc_rev[0]])\n            ic_enc = tf.nn.dropout(ic_enc, keep_prob)\n            self.posterior_zs_g0 = DiagonalGaussianFromInput(ic_enc, ic_dim, 'ic_enc_2_post_g0', var_min=hps.ic_post_var_min)\n            if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n                zs_g0 = self.posterior_zs_g0\n            else:\n                zs_g0 = self.prior_zs_g0\n            if kind in ['train', 'posterior_sample_and_average', 'prior_sample']:\n                self.g0s_val = zs_g0.sample\n            else:\n                self.g0s_val = zs_g0.mean\n        self.prior_zs_co = prior_zs_co = [None] * num_steps\n        self.posterior_zs_co = posterior_zs_co = [None] * num_steps\n        self.zs_co = zs_co = [None] * num_steps\n        self.prior_zs_ar_con = None\n        if co_dim > 0:\n            autocorrelation_taus = [hps.prior_ar_atau for x in range(hps.co_dim)]\n            noise_variances = [hps.prior_ar_nvar for x in range(hps.co_dim)]\n            self.prior_zs_ar_con = prior_zs_ar_con = LearnableAutoRegressive1Prior(batch_size, hps.co_dim, autocorrelation_taus, noise_variances, hps.do_train_prior_ar_atau, hps.do_train_prior_ar_nvar, num_steps, 'u_prior_ar1')\n    self.controller_outputs = u_t = [None] * num_steps\n    self.con_ics = con_state = None\n    self.con_states = con_states = [None] * num_steps\n    self.con_outs = con_outs = [None] * num_steps\n    self.gen_inputs = gen_inputs = [None] * num_steps\n    if co_dim > 0:\n        con_cell = gen_cell_class(hps.con_dim, input_weight_scale=hps.cell_weight_scale, rec_weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_con_reg'])\n        with tf.variable_scope('con', reuse=False):\n            self.con_ics = tf.tile(tf.Variable(tf.zeros([1, hps.con_dim * con_cell.state_multiplier]), name='c0'), tf.stack([batch_size, 1]))\n            self.con_ics.set_shape([None, con_cell.state_size])\n            con_states[-1] = self.con_ics\n    gen_cell = gen_cell_class(hps.gen_dim, input_weight_scale=hps.gen_cell_input_weight_scale, rec_weight_scale=hps.gen_cell_rec_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_gen_reg'])\n    with tf.variable_scope('gen', reuse=False):\n        if ic_dim == 0:\n            self.gen_ics = tf.tile(tf.Variable(tf.zeros([1, gen_cell.state_size]), name='g0'), tf.stack([batch_size, 1]))\n        else:\n            self.gen_ics = linear(self.g0s_val, gen_cell.state_size, identity_if_possible=True, name='g0_2_gen_ic')\n        self.gen_states = gen_states = [None] * num_steps\n        self.gen_outs = gen_outs = [None] * num_steps\n        gen_states[-1] = self.gen_ics\n        gen_outs[-1] = gen_cell.output_from_state(gen_states[-1])\n        self.factors = factors = [None] * num_steps\n        factors[-1] = linear(gen_outs[-1], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n    self.rates = rates = [None] * num_steps\n    with tf.variable_scope('glm', reuse=False):\n        if hps.output_dist == 'poisson':\n            log_rates_t0 = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            log_rates_t0.set_shape([None, None])\n            rates[-1] = tf.exp(log_rates_t0)\n            rates[-1].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n        elif hps.output_dist == 'gaussian':\n            mean_n_logvars = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            mean_n_logvars.set_shape([None, None])\n            (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n            rates[-1] = means_t_bxd\n        else:\n            assert False, 'NIY'\n    self.output_dist_params = dist_params = [None] * num_steps\n    self.log_p_xgz_b = log_p_xgz_b = 0.0\n    for t in range(num_steps):\n        if co_dim > 0:\n            tlag = t - hps.controller_input_lag\n            if tlag < 0:\n                con_in_f_t = tf.zeros_like(ci_enc_fwd[0])\n            else:\n                con_in_f_t = ci_enc_fwd[tlag]\n            if hps.do_causal_controller:\n                con_in_list_t = [con_in_f_t]\n            else:\n                tlag_rev = t + hps.controller_input_lag\n                if tlag_rev >= num_steps:\n                    con_in_r_t = tf.zeros_like(ci_enc_rev[0])\n                else:\n                    con_in_r_t = ci_enc_rev[tlag_rev]\n                con_in_list_t = [con_in_f_t, con_in_r_t]\n            if hps.do_feed_factors_to_controller:\n                if hps.feedback_factors_or_rates == 'factors':\n                    con_in_list_t.append(factors[t - 1])\n                elif hps.feedback_factors_or_rates == 'rates':\n                    con_in_list_t.append(rates[t - 1])\n                else:\n                    assert False, 'NIY'\n            con_in_t = tf.concat(axis=1, values=con_in_list_t)\n            con_in_t = tf.nn.dropout(con_in_t, keep_prob)\n            with tf.variable_scope('con', reuse=True if t > 0 else None):\n                (con_outs[t], con_states[t]) = con_cell(con_in_t, con_states[t - 1])\n                posterior_zs_co[t] = DiagonalGaussianFromInput(con_outs[t], co_dim, name='con_to_post_co')\n            if kind == 'train':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_sample_and_average':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_push_mean':\n                u_t[t] = posterior_zs_co[t].mean\n            else:\n                u_t[t] = prior_zs_ar_con.samples_t[t]\n        if ext_input_dim > 0 and hps.inject_ext_input_to_gen:\n            ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n            if co_dim > 0:\n                gen_inputs[t] = tf.concat(axis=1, values=[u_t[t], ext_input_t_bxi])\n            else:\n                gen_inputs[t] = ext_input_t_bxi\n        else:\n            gen_inputs[t] = u_t[t]\n        data_t_bxd = dataset_ph[:, t, :]\n        with tf.variable_scope('gen', reuse=True if t > 0 else None):\n            (gen_outs[t], gen_states[t]) = gen_cell(gen_inputs[t], gen_states[t - 1])\n            gen_outs[t] = tf.nn.dropout(gen_outs[t], keep_prob)\n        with tf.variable_scope('gen', reuse=True):\n            factors[t] = linear(gen_outs[t], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n        with tf.variable_scope('glm', reuse=True if t > 0 else None):\n            if hps.output_dist == 'poisson':\n                log_rates_t = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                log_rates_t.set_shape([None, None])\n                rates[t] = dist_params[t] = tf.exp(log_rates_t)\n                rates[t].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n                loglikelihood_t = Poisson(log_rates_t).logp(data_t_bxd)\n            elif hps.output_dist == 'gaussian':\n                mean_n_logvars = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                mean_n_logvars.set_shape([None, None])\n                (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n                rates[t] = means_t_bxd\n                dist_params[t] = tf.concat(axis=1, values=[means_t_bxd, tf.exp(logvars_t_bxd)])\n                loglikelihood_t = diag_gaussian_log_likelihood(data_t_bxd, means_t_bxd, logvars_t_bxd)\n            else:\n                assert False, 'NIY'\n            log_p_xgz_b += tf.reduce_sum(loglikelihood_t, [1])\n    self.corr_cost = tf.constant(0.0)\n    if hps.co_mean_corr_scale > 0.0:\n        all_sum_corr = []\n        for i in range(hps.co_dim):\n            for j in range(i + 1, hps.co_dim):\n                sum_corr_ij = tf.constant(0.0)\n                for t in range(num_steps):\n                    u_mean_t = posterior_zs_co[t].mean\n                    sum_corr_ij += u_mean_t[:, i] * u_mean_t[:, j]\n                all_sum_corr.append(0.5 * tf.square(sum_corr_ij))\n        self.corr_cost = tf.reduce_mean(all_sum_corr)\n    kl_cost_g0_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    kl_cost_co_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    self.kl_cost = tf.constant(0.0)\n    self.recon_cost = tf.constant(0.0)\n    self.nll_bound_vae = tf.constant(0.0)\n    self.nll_bound_iwae = tf.constant(0.0)\n    if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n        kl_cost_g0_b = 0.0\n        kl_cost_co_b = 0.0\n        if ic_dim > 0:\n            g0_priors = [self.prior_zs_g0]\n            g0_posts = [self.posterior_zs_g0]\n            kl_cost_g0_b = KLCost_GaussianGaussian(g0_posts, g0_priors).kl_cost_b\n            kl_cost_g0_b = hps.kl_ic_weight * kl_cost_g0_b\n        if co_dim > 0:\n            kl_cost_co_b = KLCost_GaussianGaussianProcessSampled(posterior_zs_co, prior_zs_ar_con).kl_cost_b\n            kl_cost_co_b = hps.kl_co_weight * kl_cost_co_b\n        self.recon_cost = -tf.reduce_mean(log_p_xgz_b)\n        self.kl_cost = tf.reduce_mean(kl_cost_g0_b + kl_cost_co_b)\n        lb_on_ll_b = log_p_xgz_b - kl_cost_g0_b - kl_cost_co_b\n        self.nll_bound_vae = -tf.reduce_mean(lb_on_ll_b)\n        k = tf.cast(tf.shape(log_p_xgz_b)[0], tf.float32)\n        iwae_lb_on_ll = -tf.log(k) + log_sum_exp(lb_on_ll_b)\n        self.nll_bound_iwae = -iwae_lb_on_ll\n    self.l2_cost = tf.constant(0.0)\n    if self.hps.l2_gen_scale > 0.0 or self.hps.l2_con_scale > 0.0:\n        l2_costs = []\n        l2_numels = []\n        l2_reg_var_lists = [tf.get_collection('l2_gen_reg'), tf.get_collection('l2_con_reg')]\n        l2_reg_scales = [self.hps.l2_gen_scale, self.hps.l2_con_scale]\n        for (l2_reg_vars, l2_scale) in zip(l2_reg_var_lists, l2_reg_scales):\n            for v in l2_reg_vars:\n                numel = tf.reduce_prod(tf.concat(axis=0, values=tf.shape(v)))\n                numel_f = tf.cast(numel, tf.float32)\n                l2_numels.append(numel_f)\n                v_l2 = tf.reduce_sum(v * v)\n                l2_costs.append(0.5 * l2_scale * v_l2)\n        self.l2_cost = tf.add_n(l2_costs) / tf.add_n(l2_numels)\n    self.kl_decay_step = tf.maximum(self.train_step - hps.kl_start_step, 0)\n    self.l2_decay_step = tf.maximum(self.train_step - hps.l2_start_step, 0)\n    kl_decay_step_f = tf.cast(self.kl_decay_step, tf.float32)\n    l2_decay_step_f = tf.cast(self.l2_decay_step, tf.float32)\n    kl_increase_steps_f = tf.cast(hps.kl_increase_steps, tf.float32)\n    l2_increase_steps_f = tf.cast(hps.l2_increase_steps, tf.float32)\n    self.kl_weight = kl_weight = tf.minimum(kl_decay_step_f / kl_increase_steps_f, 1.0)\n    self.l2_weight = l2_weight = tf.minimum(l2_decay_step_f / l2_increase_steps_f, 1.0)\n    self.timed_kl_cost = kl_weight * self.kl_cost\n    self.timed_l2_cost = l2_weight * self.l2_cost\n    self.weight_corr_cost = hps.co_mean_corr_scale * self.corr_cost\n    self.cost = self.recon_cost + self.timed_kl_cost + self.timed_l2_cost + self.weight_corr_cost\n    if kind != 'train':\n        self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n        self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep_lve)\n        return\n    if self.hps.do_train_io_only:\n        self.train_vars = tvars = tf.get_collection('IO_transformations', scope=tf.get_variable_scope().name)\n    elif self.hps.do_train_encoder_only:\n        tvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/ic_enc_*')\n        tvars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/z/ic_enc_*')\n        self.train_vars = tvars = tvars1 + tvars2\n    else:\n        self.train_vars = tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n    print('done.')\n    print('Model Variables (to be optimized): ')\n    total_params = 0\n    for i in range(len(tvars)):\n        shape = tvars[i].get_shape().as_list()\n        print('    ', i, tvars[i].name, shape)\n        total_params += np.prod(shape)\n    print('Total model parameters: ', total_params)\n    grads = tf.gradients(self.cost, tvars)\n    (grads, grad_global_norm) = tf.clip_by_global_norm(grads, hps.max_grad_norm)\n    opt = tf.train.AdamOptimizer(self.learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n    self.grads = grads\n    self.grad_global_norm = grad_global_norm\n    self.train_op = opt.apply_gradients(zip(grads, tvars), global_step=self.train_step)\n    self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.example_image = tf.placeholder(tf.float32, shape=[1, None, None, 3], name='image_tensor')\n    self.example_summ = tf.summary.image('LFADS example', self.example_image, collections=['example_summaries'])\n    self.lr_summ = tf.summary.scalar('Learning rate', self.learning_rate)\n    self.kl_weight_summ = tf.summary.scalar('KL weight', self.kl_weight)\n    self.l2_weight_summ = tf.summary.scalar('L2 weight', self.l2_weight)\n    self.corr_cost_summ = tf.summary.scalar('Corr cost', self.weight_corr_cost)\n    self.grad_global_norm_summ = tf.summary.scalar('Gradient global norm', self.grad_global_norm)\n    if hps.co_dim > 0:\n        self.atau_summ = [None] * hps.co_dim\n        self.pvar_summ = [None] * hps.co_dim\n        for c in range(hps.co_dim):\n            self.atau_summ[c] = tf.summary.scalar('AR Autocorrelation taus ' + str(c), tf.exp(self.prior_zs_ar_con.logataus_1xu[0, c]))\n            self.pvar_summ[c] = tf.summary.scalar('AR Variances ' + str(c), tf.exp(self.prior_zs_ar_con.logpvars_1xu[0, c]))\n    kl_cost_ph = tf.placeholder(tf.float32, shape=[], name='kl_cost_ph')\n    self.kl_t_cost_summ = tf.summary.scalar('KL cost (train)', kl_cost_ph, collections=['train_summaries'])\n    self.kl_v_cost_summ = tf.summary.scalar('KL cost (valid)', kl_cost_ph, collections=['valid_summaries'])\n    l2_cost_ph = tf.placeholder(tf.float32, shape=[], name='l2_cost_ph')\n    self.l2_cost_summ = tf.summary.scalar('L2 cost', l2_cost_ph, collections=['train_summaries'])\n    recon_cost_ph = tf.placeholder(tf.float32, shape=[], name='recon_cost_ph')\n    self.recon_t_cost_summ = tf.summary.scalar('Reconstruction cost (train)', recon_cost_ph, collections=['train_summaries'])\n    self.recon_v_cost_summ = tf.summary.scalar('Reconstruction cost (valid)', recon_cost_ph, collections=['valid_summaries'])\n    total_cost_ph = tf.placeholder(tf.float32, shape=[], name='total_cost_ph')\n    self.cost_t_summ = tf.summary.scalar('Total cost (train)', total_cost_ph, collections=['train_summaries'])\n    self.cost_v_summ = tf.summary.scalar('Total cost (valid)', total_cost_ph, collections=['valid_summaries'])\n    self.kl_cost_ph = kl_cost_ph\n    self.l2_cost_ph = l2_cost_ph\n    self.recon_cost_ph = recon_cost_ph\n    self.total_cost_ph = total_cost_ph\n    self.merged_examples = tf.summary.merge_all(key='example_summaries')\n    self.merged_generic = tf.summary.merge_all()\n    self.merged_train = tf.summary.merge_all(key='train_summaries')\n    self.merged_valid = tf.summary.merge_all(key='valid_summaries')\n    session = tf.get_default_session()\n    self.logfile = os.path.join(hps.lfads_save_dir, 'lfads_log')\n    self.writer = tf.summary.FileWriter(self.logfile)",
            "def __init__(self, hps, kind='train', datasets=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create an LFADS model.\\n\\n       train - a model for training, sampling of posteriors is used\\n       posterior_sample_and_average - sample from the posterior, this is used\\n         for evaluating the expected value of the outputs of LFADS, given a\\n         specific input, by averaging over multiple samples from the approx\\n         posterior.  Also used for the lower bound on the negative\\n         log-likelihood using IWAE error (Importance Weighed Auto-encoder).\\n         This is the denoising operation.\\n       prior_sample - a model for generation - sampling from priors is used\\n\\n    Args:\\n      hps: The dictionary of hyper parameters.\\n      kind: the type of model to build (see above).\\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\\n    '\n    print('Building graph...')\n    all_kinds = ['train', 'posterior_sample_and_average', 'posterior_push_mean', 'prior_sample']\n    assert kind in all_kinds, 'Wrong kind'\n    if hps.feedback_factors_or_rates == 'rates':\n        assert len(hps.dataset_names) == 1, 'Multiple datasets not supported for rate feedback.'\n    num_steps = hps.num_steps\n    ic_dim = hps.ic_dim\n    co_dim = hps.co_dim\n    ext_input_dim = hps.ext_input_dim\n    cell_class = GRU\n    gen_cell_class = GenGRU\n\n    def makelambda(v):\n        return lambda : v\n    self.dataName = tf.placeholder(tf.string, shape=())\n    if hps.output_dist == 'poisson':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, int), 'Data dtype must be int for poisson output distribution'\n        data_dtype = tf.int32\n    elif hps.output_dist == 'gaussian':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, float), 'Data dtype must be float for gaussian output dsitribution'\n        data_dtype = tf.float32\n    else:\n        assert False, 'NIY'\n    self.dataset_ph = dataset_ph = tf.placeholder(data_dtype, [None, num_steps, None], name='data')\n    self.train_step = tf.get_variable('global_step', [], tf.int64, tf.zeros_initializer(), trainable=False)\n    self.hps = hps\n    ndatasets = hps.ndatasets\n    factors_dim = hps.factors_dim\n    self.preds = preds = [None] * ndatasets\n    self.fns_in_fac_Ws = fns_in_fac_Ws = [None] * ndatasets\n    self.fns_in_fatcor_bs = fns_in_fac_bs = [None] * ndatasets\n    self.fns_out_fac_Ws = fns_out_fac_Ws = [None] * ndatasets\n    self.fns_out_fac_bs = fns_out_fac_bs = [None] * ndatasets\n    self.datasetNames = dataset_names = hps.dataset_names\n    self.ext_inputs = ext_inputs = None\n    if len(dataset_names) == 1:\n        if 'alignment_matrix_cxf' in datasets[dataset_names[0]].keys():\n            used_in_factors_dim = factors_dim\n            in_identity_if_poss = False\n        else:\n            used_in_factors_dim = hps.dataset_dims[dataset_names[0]]\n            in_identity_if_poss = True\n    else:\n        used_in_factors_dim = factors_dim\n        in_identity_if_poss = False\n    for (d, name) in enumerate(dataset_names):\n        data_dim = hps.dataset_dims[name]\n        in_mat_cxf = None\n        in_bias_1xf = None\n        align_bias_1xc = None\n        if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin matrix with alignment matrix provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin matrix to alignment matrix provided for dataset:', name)\n            in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if in_mat_cxf.shape != (data_dim, factors_dim):\n                raise ValueError('Alignment matrix must have dimensions %d x %d\\n          (data_dim x factors_dim), but currently has %d x %d.' % (data_dim, factors_dim, in_mat_cxf.shape[0], in_mat_cxf.shape[1]))\n        if datasets and 'alignment_bias_c' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin bias with alignment bias provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin bias to alignment bias provided for dataset:', name)\n            align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n            align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            if align_bias_1xc.shape[1] != data_dim:\n                raise ValueError('Alignment bias must have dimensions %d\\n          (data_dim), but currently has %d.' % (data_dim, in_mat_cxf.shape[0]))\n            if in_mat_cxf is not None and align_bias_1xc is not None:\n                in_bias_1xf = -np.dot(align_bias_1xc, in_mat_cxf)\n        if hps.do_train_readin:\n            collections_readin = ['IO_transformations']\n        else:\n            collections_readin = None\n        in_fac_lin = init_linear(data_dim, used_in_factors_dim, do_bias=True, mat_init_value=in_mat_cxf, bias_init_value=in_bias_1xf, identity_if_possible=in_identity_if_poss, normalized=False, name='x_2_infac_' + name, collections=collections_readin, trainable=hps.do_train_readin)\n        (in_fac_W, in_fac_b) = in_fac_lin\n        fns_in_fac_Ws[d] = makelambda(in_fac_W)\n        fns_in_fac_bs[d] = makelambda(in_fac_b)\n    with tf.variable_scope('glm'):\n        out_identity_if_poss = False\n        if len(dataset_names) == 1 and factors_dim == hps.dataset_dims[dataset_names[0]]:\n            out_identity_if_poss = True\n        for (d, name) in enumerate(dataset_names):\n            data_dim = hps.dataset_dims[name]\n            in_mat_cxf = None\n            if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n                dataset = datasets[name]\n                in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if datasets and 'alignment_bias_c' in datasets[name].keys():\n                dataset = datasets[name]\n                align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n                align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            out_mat_fxc = None\n            out_bias_1xc = None\n            if in_mat_cxf is not None:\n                out_mat_fxc = in_mat_cxf.T\n            if align_bias_1xc is not None:\n                out_bias_1xc = align_bias_1xc\n            if hps.output_dist == 'poisson':\n                out_fac_lin = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, identity_if_possible=out_identity_if_poss, normalized=False, name='fac_2_logrates_' + name, collections=['IO_transformations'])\n                (out_fac_W, out_fac_b) = out_fac_lin\n            elif hps.output_dist == 'gaussian':\n                out_fac_lin_mean = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, normalized=False, name='fac_2_means_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                mat_init_value = np.zeros([factors_dim, data_dim]).astype(np.float32)\n                bias_init_value = np.ones([1, data_dim]).astype(np.float32)\n                out_fac_lin_logvar = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=mat_init_value, bias_init_value=bias_init_value, normalized=False, name='fac_2_logvars_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                (out_fac_W_logvar, out_fac_b_logvar) = out_fac_lin_logvar\n                out_fac_W = tf.concat(axis=1, values=[out_fac_W_mean, out_fac_W_logvar])\n                out_fac_b = tf.concat(axis=1, values=[out_fac_b_mean, out_fac_b_logvar])\n            else:\n                assert False, 'NIY'\n            preds[d] = tf.equal(tf.constant(name), self.dataName)\n            data_dim = hps.dataset_dims[name]\n            fns_out_fac_Ws[d] = makelambda(out_fac_W)\n            fns_out_fac_bs[d] = makelambda(out_fac_b)\n    pf_pairs_in_fac_Ws = zip(preds, fns_in_fac_Ws)\n    pf_pairs_in_fac_bs = zip(preds, fns_in_fac_bs)\n    pf_pairs_out_fac_Ws = zip(preds, fns_out_fac_Ws)\n    pf_pairs_out_fac_bs = zip(preds, fns_out_fac_bs)\n    this_in_fac_W = tf.case(pf_pairs_in_fac_Ws, exclusive=True)\n    this_in_fac_b = tf.case(pf_pairs_in_fac_bs, exclusive=True)\n    this_out_fac_W = tf.case(pf_pairs_out_fac_Ws, exclusive=True)\n    this_out_fac_b = tf.case(pf_pairs_out_fac_bs, exclusive=True)\n    if hps.ext_input_dim > 0:\n        self.ext_input = tf.placeholder(tf.float32, [None, num_steps, ext_input_dim], name='ext_input')\n    else:\n        self.ext_input = None\n    ext_input_bxtxi = self.ext_input\n    self.keep_prob = keep_prob = tf.placeholder(tf.float32, [], 'keep_prob')\n    self.batch_size = batch_size = int(hps.batch_size)\n    self.learning_rate = tf.Variable(float(hps.learning_rate_init), trainable=False, name='learning_rate')\n    self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * hps.learning_rate_decay_factor)\n    dataset_do_bxtxd = tf.nn.dropout(tf.to_float(dataset_ph), keep_prob)\n    if hps.ext_input_dim > 0:\n        ext_input_do_bxtxi = tf.nn.dropout(ext_input_bxtxi, keep_prob)\n    else:\n        ext_input_do_bxtxi = None\n\n    def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n        \"\"\"Encode data for LFADS\n      Args:\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\n          time x batch x data dims.\n        enc_cell: encoder cell\n        name: name of encoder\n        forward_or_reverse: string, encode in forward or reverse direction\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\n      Returns:\n        encoded data as a list with num_steps_to_encode items, in order\n      \"\"\"\n        if forward_or_reverse == 'forward':\n            dstr = '_fwd'\n            time_fwd_or_rev = range(num_steps_to_encode)\n        else:\n            dstr = '_rev'\n            time_fwd_or_rev = reversed(range(num_steps_to_encode))\n        with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n            enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n            enc_state.set_shape([None, enc_cell.state_size])\n        enc_outs = [None] * num_steps_to_encode\n        for (i, t) in enumerate(time_fwd_or_rev):\n            with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n                dataset_t_bxd = dataset_bxtxd[:, t, :]\n                in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n                in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n                if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                    ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                    enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n                else:\n                    enc_input_t_bxfpe = in_fac_t_bxf\n                (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n                enc_outs[t] = enc_out\n        return enc_outs\n    self.ic_enc_fwd = [None] * num_steps\n    self.ic_enc_rev = [None] * num_steps\n    if ic_dim > 0:\n        enc_ic_cell = cell_class(hps.ic_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ic_enc_fwd = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'forward', hps.num_steps_for_gen_ic)\n        ic_enc_rev = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'reverse', hps.num_steps_for_gen_ic)\n        self.ic_enc_fwd = ic_enc_fwd\n        self.ic_enc_rev = ic_enc_rev\n    self.ci_enc_fwd = [None] * num_steps\n    self.ci_enc_rev = [None] * num_steps\n    if co_dim > 0:\n        enc_ci_cell = cell_class(hps.ci_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ci_enc_fwd = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'forward', hps.num_steps)\n        if hps.do_causal_controller:\n            ci_enc_rev = None\n        else:\n            ci_enc_rev = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'reverse', hps.num_steps)\n        self.ci_enc_fwd = ci_enc_fwd\n        self.ci_enc_rev = ci_enc_rev\n    with tf.variable_scope('z', reuse=False):\n        self.prior_zs_g0 = None\n        self.posterior_zs_g0 = None\n        self.g0s_val = None\n        if ic_dim > 0:\n            self.prior_zs_g0 = LearnableDiagonalGaussian(batch_size, ic_dim, name='prior_g0', mean_init=0.0, var_min=hps.ic_prior_var_min, var_init=hps.ic_prior_var_scale, var_max=hps.ic_prior_var_max)\n            ic_enc = tf.concat(axis=1, values=[ic_enc_fwd[-1], ic_enc_rev[0]])\n            ic_enc = tf.nn.dropout(ic_enc, keep_prob)\n            self.posterior_zs_g0 = DiagonalGaussianFromInput(ic_enc, ic_dim, 'ic_enc_2_post_g0', var_min=hps.ic_post_var_min)\n            if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n                zs_g0 = self.posterior_zs_g0\n            else:\n                zs_g0 = self.prior_zs_g0\n            if kind in ['train', 'posterior_sample_and_average', 'prior_sample']:\n                self.g0s_val = zs_g0.sample\n            else:\n                self.g0s_val = zs_g0.mean\n        self.prior_zs_co = prior_zs_co = [None] * num_steps\n        self.posterior_zs_co = posterior_zs_co = [None] * num_steps\n        self.zs_co = zs_co = [None] * num_steps\n        self.prior_zs_ar_con = None\n        if co_dim > 0:\n            autocorrelation_taus = [hps.prior_ar_atau for x in range(hps.co_dim)]\n            noise_variances = [hps.prior_ar_nvar for x in range(hps.co_dim)]\n            self.prior_zs_ar_con = prior_zs_ar_con = LearnableAutoRegressive1Prior(batch_size, hps.co_dim, autocorrelation_taus, noise_variances, hps.do_train_prior_ar_atau, hps.do_train_prior_ar_nvar, num_steps, 'u_prior_ar1')\n    self.controller_outputs = u_t = [None] * num_steps\n    self.con_ics = con_state = None\n    self.con_states = con_states = [None] * num_steps\n    self.con_outs = con_outs = [None] * num_steps\n    self.gen_inputs = gen_inputs = [None] * num_steps\n    if co_dim > 0:\n        con_cell = gen_cell_class(hps.con_dim, input_weight_scale=hps.cell_weight_scale, rec_weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_con_reg'])\n        with tf.variable_scope('con', reuse=False):\n            self.con_ics = tf.tile(tf.Variable(tf.zeros([1, hps.con_dim * con_cell.state_multiplier]), name='c0'), tf.stack([batch_size, 1]))\n            self.con_ics.set_shape([None, con_cell.state_size])\n            con_states[-1] = self.con_ics\n    gen_cell = gen_cell_class(hps.gen_dim, input_weight_scale=hps.gen_cell_input_weight_scale, rec_weight_scale=hps.gen_cell_rec_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_gen_reg'])\n    with tf.variable_scope('gen', reuse=False):\n        if ic_dim == 0:\n            self.gen_ics = tf.tile(tf.Variable(tf.zeros([1, gen_cell.state_size]), name='g0'), tf.stack([batch_size, 1]))\n        else:\n            self.gen_ics = linear(self.g0s_val, gen_cell.state_size, identity_if_possible=True, name='g0_2_gen_ic')\n        self.gen_states = gen_states = [None] * num_steps\n        self.gen_outs = gen_outs = [None] * num_steps\n        gen_states[-1] = self.gen_ics\n        gen_outs[-1] = gen_cell.output_from_state(gen_states[-1])\n        self.factors = factors = [None] * num_steps\n        factors[-1] = linear(gen_outs[-1], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n    self.rates = rates = [None] * num_steps\n    with tf.variable_scope('glm', reuse=False):\n        if hps.output_dist == 'poisson':\n            log_rates_t0 = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            log_rates_t0.set_shape([None, None])\n            rates[-1] = tf.exp(log_rates_t0)\n            rates[-1].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n        elif hps.output_dist == 'gaussian':\n            mean_n_logvars = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            mean_n_logvars.set_shape([None, None])\n            (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n            rates[-1] = means_t_bxd\n        else:\n            assert False, 'NIY'\n    self.output_dist_params = dist_params = [None] * num_steps\n    self.log_p_xgz_b = log_p_xgz_b = 0.0\n    for t in range(num_steps):\n        if co_dim > 0:\n            tlag = t - hps.controller_input_lag\n            if tlag < 0:\n                con_in_f_t = tf.zeros_like(ci_enc_fwd[0])\n            else:\n                con_in_f_t = ci_enc_fwd[tlag]\n            if hps.do_causal_controller:\n                con_in_list_t = [con_in_f_t]\n            else:\n                tlag_rev = t + hps.controller_input_lag\n                if tlag_rev >= num_steps:\n                    con_in_r_t = tf.zeros_like(ci_enc_rev[0])\n                else:\n                    con_in_r_t = ci_enc_rev[tlag_rev]\n                con_in_list_t = [con_in_f_t, con_in_r_t]\n            if hps.do_feed_factors_to_controller:\n                if hps.feedback_factors_or_rates == 'factors':\n                    con_in_list_t.append(factors[t - 1])\n                elif hps.feedback_factors_or_rates == 'rates':\n                    con_in_list_t.append(rates[t - 1])\n                else:\n                    assert False, 'NIY'\n            con_in_t = tf.concat(axis=1, values=con_in_list_t)\n            con_in_t = tf.nn.dropout(con_in_t, keep_prob)\n            with tf.variable_scope('con', reuse=True if t > 0 else None):\n                (con_outs[t], con_states[t]) = con_cell(con_in_t, con_states[t - 1])\n                posterior_zs_co[t] = DiagonalGaussianFromInput(con_outs[t], co_dim, name='con_to_post_co')\n            if kind == 'train':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_sample_and_average':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_push_mean':\n                u_t[t] = posterior_zs_co[t].mean\n            else:\n                u_t[t] = prior_zs_ar_con.samples_t[t]\n        if ext_input_dim > 0 and hps.inject_ext_input_to_gen:\n            ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n            if co_dim > 0:\n                gen_inputs[t] = tf.concat(axis=1, values=[u_t[t], ext_input_t_bxi])\n            else:\n                gen_inputs[t] = ext_input_t_bxi\n        else:\n            gen_inputs[t] = u_t[t]\n        data_t_bxd = dataset_ph[:, t, :]\n        with tf.variable_scope('gen', reuse=True if t > 0 else None):\n            (gen_outs[t], gen_states[t]) = gen_cell(gen_inputs[t], gen_states[t - 1])\n            gen_outs[t] = tf.nn.dropout(gen_outs[t], keep_prob)\n        with tf.variable_scope('gen', reuse=True):\n            factors[t] = linear(gen_outs[t], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n        with tf.variable_scope('glm', reuse=True if t > 0 else None):\n            if hps.output_dist == 'poisson':\n                log_rates_t = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                log_rates_t.set_shape([None, None])\n                rates[t] = dist_params[t] = tf.exp(log_rates_t)\n                rates[t].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n                loglikelihood_t = Poisson(log_rates_t).logp(data_t_bxd)\n            elif hps.output_dist == 'gaussian':\n                mean_n_logvars = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                mean_n_logvars.set_shape([None, None])\n                (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n                rates[t] = means_t_bxd\n                dist_params[t] = tf.concat(axis=1, values=[means_t_bxd, tf.exp(logvars_t_bxd)])\n                loglikelihood_t = diag_gaussian_log_likelihood(data_t_bxd, means_t_bxd, logvars_t_bxd)\n            else:\n                assert False, 'NIY'\n            log_p_xgz_b += tf.reduce_sum(loglikelihood_t, [1])\n    self.corr_cost = tf.constant(0.0)\n    if hps.co_mean_corr_scale > 0.0:\n        all_sum_corr = []\n        for i in range(hps.co_dim):\n            for j in range(i + 1, hps.co_dim):\n                sum_corr_ij = tf.constant(0.0)\n                for t in range(num_steps):\n                    u_mean_t = posterior_zs_co[t].mean\n                    sum_corr_ij += u_mean_t[:, i] * u_mean_t[:, j]\n                all_sum_corr.append(0.5 * tf.square(sum_corr_ij))\n        self.corr_cost = tf.reduce_mean(all_sum_corr)\n    kl_cost_g0_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    kl_cost_co_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    self.kl_cost = tf.constant(0.0)\n    self.recon_cost = tf.constant(0.0)\n    self.nll_bound_vae = tf.constant(0.0)\n    self.nll_bound_iwae = tf.constant(0.0)\n    if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n        kl_cost_g0_b = 0.0\n        kl_cost_co_b = 0.0\n        if ic_dim > 0:\n            g0_priors = [self.prior_zs_g0]\n            g0_posts = [self.posterior_zs_g0]\n            kl_cost_g0_b = KLCost_GaussianGaussian(g0_posts, g0_priors).kl_cost_b\n            kl_cost_g0_b = hps.kl_ic_weight * kl_cost_g0_b\n        if co_dim > 0:\n            kl_cost_co_b = KLCost_GaussianGaussianProcessSampled(posterior_zs_co, prior_zs_ar_con).kl_cost_b\n            kl_cost_co_b = hps.kl_co_weight * kl_cost_co_b\n        self.recon_cost = -tf.reduce_mean(log_p_xgz_b)\n        self.kl_cost = tf.reduce_mean(kl_cost_g0_b + kl_cost_co_b)\n        lb_on_ll_b = log_p_xgz_b - kl_cost_g0_b - kl_cost_co_b\n        self.nll_bound_vae = -tf.reduce_mean(lb_on_ll_b)\n        k = tf.cast(tf.shape(log_p_xgz_b)[0], tf.float32)\n        iwae_lb_on_ll = -tf.log(k) + log_sum_exp(lb_on_ll_b)\n        self.nll_bound_iwae = -iwae_lb_on_ll\n    self.l2_cost = tf.constant(0.0)\n    if self.hps.l2_gen_scale > 0.0 or self.hps.l2_con_scale > 0.0:\n        l2_costs = []\n        l2_numels = []\n        l2_reg_var_lists = [tf.get_collection('l2_gen_reg'), tf.get_collection('l2_con_reg')]\n        l2_reg_scales = [self.hps.l2_gen_scale, self.hps.l2_con_scale]\n        for (l2_reg_vars, l2_scale) in zip(l2_reg_var_lists, l2_reg_scales):\n            for v in l2_reg_vars:\n                numel = tf.reduce_prod(tf.concat(axis=0, values=tf.shape(v)))\n                numel_f = tf.cast(numel, tf.float32)\n                l2_numels.append(numel_f)\n                v_l2 = tf.reduce_sum(v * v)\n                l2_costs.append(0.5 * l2_scale * v_l2)\n        self.l2_cost = tf.add_n(l2_costs) / tf.add_n(l2_numels)\n    self.kl_decay_step = tf.maximum(self.train_step - hps.kl_start_step, 0)\n    self.l2_decay_step = tf.maximum(self.train_step - hps.l2_start_step, 0)\n    kl_decay_step_f = tf.cast(self.kl_decay_step, tf.float32)\n    l2_decay_step_f = tf.cast(self.l2_decay_step, tf.float32)\n    kl_increase_steps_f = tf.cast(hps.kl_increase_steps, tf.float32)\n    l2_increase_steps_f = tf.cast(hps.l2_increase_steps, tf.float32)\n    self.kl_weight = kl_weight = tf.minimum(kl_decay_step_f / kl_increase_steps_f, 1.0)\n    self.l2_weight = l2_weight = tf.minimum(l2_decay_step_f / l2_increase_steps_f, 1.0)\n    self.timed_kl_cost = kl_weight * self.kl_cost\n    self.timed_l2_cost = l2_weight * self.l2_cost\n    self.weight_corr_cost = hps.co_mean_corr_scale * self.corr_cost\n    self.cost = self.recon_cost + self.timed_kl_cost + self.timed_l2_cost + self.weight_corr_cost\n    if kind != 'train':\n        self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n        self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep_lve)\n        return\n    if self.hps.do_train_io_only:\n        self.train_vars = tvars = tf.get_collection('IO_transformations', scope=tf.get_variable_scope().name)\n    elif self.hps.do_train_encoder_only:\n        tvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/ic_enc_*')\n        tvars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/z/ic_enc_*')\n        self.train_vars = tvars = tvars1 + tvars2\n    else:\n        self.train_vars = tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n    print('done.')\n    print('Model Variables (to be optimized): ')\n    total_params = 0\n    for i in range(len(tvars)):\n        shape = tvars[i].get_shape().as_list()\n        print('    ', i, tvars[i].name, shape)\n        total_params += np.prod(shape)\n    print('Total model parameters: ', total_params)\n    grads = tf.gradients(self.cost, tvars)\n    (grads, grad_global_norm) = tf.clip_by_global_norm(grads, hps.max_grad_norm)\n    opt = tf.train.AdamOptimizer(self.learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n    self.grads = grads\n    self.grad_global_norm = grad_global_norm\n    self.train_op = opt.apply_gradients(zip(grads, tvars), global_step=self.train_step)\n    self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.example_image = tf.placeholder(tf.float32, shape=[1, None, None, 3], name='image_tensor')\n    self.example_summ = tf.summary.image('LFADS example', self.example_image, collections=['example_summaries'])\n    self.lr_summ = tf.summary.scalar('Learning rate', self.learning_rate)\n    self.kl_weight_summ = tf.summary.scalar('KL weight', self.kl_weight)\n    self.l2_weight_summ = tf.summary.scalar('L2 weight', self.l2_weight)\n    self.corr_cost_summ = tf.summary.scalar('Corr cost', self.weight_corr_cost)\n    self.grad_global_norm_summ = tf.summary.scalar('Gradient global norm', self.grad_global_norm)\n    if hps.co_dim > 0:\n        self.atau_summ = [None] * hps.co_dim\n        self.pvar_summ = [None] * hps.co_dim\n        for c in range(hps.co_dim):\n            self.atau_summ[c] = tf.summary.scalar('AR Autocorrelation taus ' + str(c), tf.exp(self.prior_zs_ar_con.logataus_1xu[0, c]))\n            self.pvar_summ[c] = tf.summary.scalar('AR Variances ' + str(c), tf.exp(self.prior_zs_ar_con.logpvars_1xu[0, c]))\n    kl_cost_ph = tf.placeholder(tf.float32, shape=[], name='kl_cost_ph')\n    self.kl_t_cost_summ = tf.summary.scalar('KL cost (train)', kl_cost_ph, collections=['train_summaries'])\n    self.kl_v_cost_summ = tf.summary.scalar('KL cost (valid)', kl_cost_ph, collections=['valid_summaries'])\n    l2_cost_ph = tf.placeholder(tf.float32, shape=[], name='l2_cost_ph')\n    self.l2_cost_summ = tf.summary.scalar('L2 cost', l2_cost_ph, collections=['train_summaries'])\n    recon_cost_ph = tf.placeholder(tf.float32, shape=[], name='recon_cost_ph')\n    self.recon_t_cost_summ = tf.summary.scalar('Reconstruction cost (train)', recon_cost_ph, collections=['train_summaries'])\n    self.recon_v_cost_summ = tf.summary.scalar('Reconstruction cost (valid)', recon_cost_ph, collections=['valid_summaries'])\n    total_cost_ph = tf.placeholder(tf.float32, shape=[], name='total_cost_ph')\n    self.cost_t_summ = tf.summary.scalar('Total cost (train)', total_cost_ph, collections=['train_summaries'])\n    self.cost_v_summ = tf.summary.scalar('Total cost (valid)', total_cost_ph, collections=['valid_summaries'])\n    self.kl_cost_ph = kl_cost_ph\n    self.l2_cost_ph = l2_cost_ph\n    self.recon_cost_ph = recon_cost_ph\n    self.total_cost_ph = total_cost_ph\n    self.merged_examples = tf.summary.merge_all(key='example_summaries')\n    self.merged_generic = tf.summary.merge_all()\n    self.merged_train = tf.summary.merge_all(key='train_summaries')\n    self.merged_valid = tf.summary.merge_all(key='valid_summaries')\n    session = tf.get_default_session()\n    self.logfile = os.path.join(hps.lfads_save_dir, 'lfads_log')\n    self.writer = tf.summary.FileWriter(self.logfile)",
            "def __init__(self, hps, kind='train', datasets=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create an LFADS model.\\n\\n       train - a model for training, sampling of posteriors is used\\n       posterior_sample_and_average - sample from the posterior, this is used\\n         for evaluating the expected value of the outputs of LFADS, given a\\n         specific input, by averaging over multiple samples from the approx\\n         posterior.  Also used for the lower bound on the negative\\n         log-likelihood using IWAE error (Importance Weighed Auto-encoder).\\n         This is the denoising operation.\\n       prior_sample - a model for generation - sampling from priors is used\\n\\n    Args:\\n      hps: The dictionary of hyper parameters.\\n      kind: the type of model to build (see above).\\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\\n    '\n    print('Building graph...')\n    all_kinds = ['train', 'posterior_sample_and_average', 'posterior_push_mean', 'prior_sample']\n    assert kind in all_kinds, 'Wrong kind'\n    if hps.feedback_factors_or_rates == 'rates':\n        assert len(hps.dataset_names) == 1, 'Multiple datasets not supported for rate feedback.'\n    num_steps = hps.num_steps\n    ic_dim = hps.ic_dim\n    co_dim = hps.co_dim\n    ext_input_dim = hps.ext_input_dim\n    cell_class = GRU\n    gen_cell_class = GenGRU\n\n    def makelambda(v):\n        return lambda : v\n    self.dataName = tf.placeholder(tf.string, shape=())\n    if hps.output_dist == 'poisson':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, int), 'Data dtype must be int for poisson output distribution'\n        data_dtype = tf.int32\n    elif hps.output_dist == 'gaussian':\n        assert np.issubdtype(datasets[hps.dataset_names[0]]['train_data'].dtype, float), 'Data dtype must be float for gaussian output dsitribution'\n        data_dtype = tf.float32\n    else:\n        assert False, 'NIY'\n    self.dataset_ph = dataset_ph = tf.placeholder(data_dtype, [None, num_steps, None], name='data')\n    self.train_step = tf.get_variable('global_step', [], tf.int64, tf.zeros_initializer(), trainable=False)\n    self.hps = hps\n    ndatasets = hps.ndatasets\n    factors_dim = hps.factors_dim\n    self.preds = preds = [None] * ndatasets\n    self.fns_in_fac_Ws = fns_in_fac_Ws = [None] * ndatasets\n    self.fns_in_fatcor_bs = fns_in_fac_bs = [None] * ndatasets\n    self.fns_out_fac_Ws = fns_out_fac_Ws = [None] * ndatasets\n    self.fns_out_fac_bs = fns_out_fac_bs = [None] * ndatasets\n    self.datasetNames = dataset_names = hps.dataset_names\n    self.ext_inputs = ext_inputs = None\n    if len(dataset_names) == 1:\n        if 'alignment_matrix_cxf' in datasets[dataset_names[0]].keys():\n            used_in_factors_dim = factors_dim\n            in_identity_if_poss = False\n        else:\n            used_in_factors_dim = hps.dataset_dims[dataset_names[0]]\n            in_identity_if_poss = True\n    else:\n        used_in_factors_dim = factors_dim\n        in_identity_if_poss = False\n    for (d, name) in enumerate(dataset_names):\n        data_dim = hps.dataset_dims[name]\n        in_mat_cxf = None\n        in_bias_1xf = None\n        align_bias_1xc = None\n        if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin matrix with alignment matrix provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin matrix to alignment matrix provided for dataset:', name)\n            in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if in_mat_cxf.shape != (data_dim, factors_dim):\n                raise ValueError('Alignment matrix must have dimensions %d x %d\\n          (data_dim x factors_dim), but currently has %d x %d.' % (data_dim, factors_dim, in_mat_cxf.shape[0], in_mat_cxf.shape[1]))\n        if datasets and 'alignment_bias_c' in datasets[name].keys():\n            dataset = datasets[name]\n            if hps.do_train_readin:\n                print('Initializing trainable readin bias with alignment bias provided for dataset:', name)\n            else:\n                print('Setting non-trainable readin bias to alignment bias provided for dataset:', name)\n            align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n            align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            if align_bias_1xc.shape[1] != data_dim:\n                raise ValueError('Alignment bias must have dimensions %d\\n          (data_dim), but currently has %d.' % (data_dim, in_mat_cxf.shape[0]))\n            if in_mat_cxf is not None and align_bias_1xc is not None:\n                in_bias_1xf = -np.dot(align_bias_1xc, in_mat_cxf)\n        if hps.do_train_readin:\n            collections_readin = ['IO_transformations']\n        else:\n            collections_readin = None\n        in_fac_lin = init_linear(data_dim, used_in_factors_dim, do_bias=True, mat_init_value=in_mat_cxf, bias_init_value=in_bias_1xf, identity_if_possible=in_identity_if_poss, normalized=False, name='x_2_infac_' + name, collections=collections_readin, trainable=hps.do_train_readin)\n        (in_fac_W, in_fac_b) = in_fac_lin\n        fns_in_fac_Ws[d] = makelambda(in_fac_W)\n        fns_in_fac_bs[d] = makelambda(in_fac_b)\n    with tf.variable_scope('glm'):\n        out_identity_if_poss = False\n        if len(dataset_names) == 1 and factors_dim == hps.dataset_dims[dataset_names[0]]:\n            out_identity_if_poss = True\n        for (d, name) in enumerate(dataset_names):\n            data_dim = hps.dataset_dims[name]\n            in_mat_cxf = None\n            if datasets and 'alignment_matrix_cxf' in datasets[name].keys():\n                dataset = datasets[name]\n                in_mat_cxf = dataset['alignment_matrix_cxf'].astype(np.float32)\n            if datasets and 'alignment_bias_c' in datasets[name].keys():\n                dataset = datasets[name]\n                align_bias_c = dataset['alignment_bias_c'].astype(np.float32)\n                align_bias_1xc = np.expand_dims(align_bias_c, axis=0)\n            out_mat_fxc = None\n            out_bias_1xc = None\n            if in_mat_cxf is not None:\n                out_mat_fxc = in_mat_cxf.T\n            if align_bias_1xc is not None:\n                out_bias_1xc = align_bias_1xc\n            if hps.output_dist == 'poisson':\n                out_fac_lin = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, identity_if_possible=out_identity_if_poss, normalized=False, name='fac_2_logrates_' + name, collections=['IO_transformations'])\n                (out_fac_W, out_fac_b) = out_fac_lin\n            elif hps.output_dist == 'gaussian':\n                out_fac_lin_mean = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=out_mat_fxc, bias_init_value=out_bias_1xc, normalized=False, name='fac_2_means_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                mat_init_value = np.zeros([factors_dim, data_dim]).astype(np.float32)\n                bias_init_value = np.ones([1, data_dim]).astype(np.float32)\n                out_fac_lin_logvar = init_linear(factors_dim, data_dim, do_bias=True, mat_init_value=mat_init_value, bias_init_value=bias_init_value, normalized=False, name='fac_2_logvars_' + name, collections=['IO_transformations'])\n                (out_fac_W_mean, out_fac_b_mean) = out_fac_lin_mean\n                (out_fac_W_logvar, out_fac_b_logvar) = out_fac_lin_logvar\n                out_fac_W = tf.concat(axis=1, values=[out_fac_W_mean, out_fac_W_logvar])\n                out_fac_b = tf.concat(axis=1, values=[out_fac_b_mean, out_fac_b_logvar])\n            else:\n                assert False, 'NIY'\n            preds[d] = tf.equal(tf.constant(name), self.dataName)\n            data_dim = hps.dataset_dims[name]\n            fns_out_fac_Ws[d] = makelambda(out_fac_W)\n            fns_out_fac_bs[d] = makelambda(out_fac_b)\n    pf_pairs_in_fac_Ws = zip(preds, fns_in_fac_Ws)\n    pf_pairs_in_fac_bs = zip(preds, fns_in_fac_bs)\n    pf_pairs_out_fac_Ws = zip(preds, fns_out_fac_Ws)\n    pf_pairs_out_fac_bs = zip(preds, fns_out_fac_bs)\n    this_in_fac_W = tf.case(pf_pairs_in_fac_Ws, exclusive=True)\n    this_in_fac_b = tf.case(pf_pairs_in_fac_bs, exclusive=True)\n    this_out_fac_W = tf.case(pf_pairs_out_fac_Ws, exclusive=True)\n    this_out_fac_b = tf.case(pf_pairs_out_fac_bs, exclusive=True)\n    if hps.ext_input_dim > 0:\n        self.ext_input = tf.placeholder(tf.float32, [None, num_steps, ext_input_dim], name='ext_input')\n    else:\n        self.ext_input = None\n    ext_input_bxtxi = self.ext_input\n    self.keep_prob = keep_prob = tf.placeholder(tf.float32, [], 'keep_prob')\n    self.batch_size = batch_size = int(hps.batch_size)\n    self.learning_rate = tf.Variable(float(hps.learning_rate_init), trainable=False, name='learning_rate')\n    self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * hps.learning_rate_decay_factor)\n    dataset_do_bxtxd = tf.nn.dropout(tf.to_float(dataset_ph), keep_prob)\n    if hps.ext_input_dim > 0:\n        ext_input_do_bxtxi = tf.nn.dropout(ext_input_bxtxi, keep_prob)\n    else:\n        ext_input_do_bxtxi = None\n\n    def encode_data(dataset_bxtxd, enc_cell, name, forward_or_reverse, num_steps_to_encode):\n        \"\"\"Encode data for LFADS\n      Args:\n        dataset_bxtxd - the data to encode, as a 3 tensor, with dims\n          time x batch x data dims.\n        enc_cell: encoder cell\n        name: name of encoder\n        forward_or_reverse: string, encode in forward or reverse direction\n        num_steps_to_encode: number of steps to  encode, 0:num_steps_to_encode\n      Returns:\n        encoded data as a list with num_steps_to_encode items, in order\n      \"\"\"\n        if forward_or_reverse == 'forward':\n            dstr = '_fwd'\n            time_fwd_or_rev = range(num_steps_to_encode)\n        else:\n            dstr = '_rev'\n            time_fwd_or_rev = reversed(range(num_steps_to_encode))\n        with tf.variable_scope(name + '_enc' + dstr, reuse=False):\n            enc_state = tf.tile(tf.Variable(tf.zeros([1, enc_cell.state_size]), name=name + '_enc_t0' + dstr), tf.stack([batch_size, 1]))\n            enc_state.set_shape([None, enc_cell.state_size])\n        enc_outs = [None] * num_steps_to_encode\n        for (i, t) in enumerate(time_fwd_or_rev):\n            with tf.variable_scope(name + '_enc' + dstr, reuse=True if i > 0 else None):\n                dataset_t_bxd = dataset_bxtxd[:, t, :]\n                in_fac_t_bxf = tf.matmul(dataset_t_bxd, this_in_fac_W) + this_in_fac_b\n                in_fac_t_bxf.set_shape([None, used_in_factors_dim])\n                if ext_input_dim > 0 and (not hps.inject_ext_input_to_gen):\n                    ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n                    enc_input_t_bxfpe = tf.concat(axis=1, values=[in_fac_t_bxf, ext_input_t_bxi])\n                else:\n                    enc_input_t_bxfpe = in_fac_t_bxf\n                (enc_out, enc_state) = enc_cell(enc_input_t_bxfpe, enc_state)\n                enc_outs[t] = enc_out\n        return enc_outs\n    self.ic_enc_fwd = [None] * num_steps\n    self.ic_enc_rev = [None] * num_steps\n    if ic_dim > 0:\n        enc_ic_cell = cell_class(hps.ic_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ic_enc_fwd = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'forward', hps.num_steps_for_gen_ic)\n        ic_enc_rev = encode_data(dataset_do_bxtxd, enc_ic_cell, 'ic', 'reverse', hps.num_steps_for_gen_ic)\n        self.ic_enc_fwd = ic_enc_fwd\n        self.ic_enc_rev = ic_enc_rev\n    self.ci_enc_fwd = [None] * num_steps\n    self.ci_enc_rev = [None] * num_steps\n    if co_dim > 0:\n        enc_ci_cell = cell_class(hps.ci_enc_dim, weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value)\n        ci_enc_fwd = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'forward', hps.num_steps)\n        if hps.do_causal_controller:\n            ci_enc_rev = None\n        else:\n            ci_enc_rev = encode_data(dataset_do_bxtxd, enc_ci_cell, 'ci', 'reverse', hps.num_steps)\n        self.ci_enc_fwd = ci_enc_fwd\n        self.ci_enc_rev = ci_enc_rev\n    with tf.variable_scope('z', reuse=False):\n        self.prior_zs_g0 = None\n        self.posterior_zs_g0 = None\n        self.g0s_val = None\n        if ic_dim > 0:\n            self.prior_zs_g0 = LearnableDiagonalGaussian(batch_size, ic_dim, name='prior_g0', mean_init=0.0, var_min=hps.ic_prior_var_min, var_init=hps.ic_prior_var_scale, var_max=hps.ic_prior_var_max)\n            ic_enc = tf.concat(axis=1, values=[ic_enc_fwd[-1], ic_enc_rev[0]])\n            ic_enc = tf.nn.dropout(ic_enc, keep_prob)\n            self.posterior_zs_g0 = DiagonalGaussianFromInput(ic_enc, ic_dim, 'ic_enc_2_post_g0', var_min=hps.ic_post_var_min)\n            if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n                zs_g0 = self.posterior_zs_g0\n            else:\n                zs_g0 = self.prior_zs_g0\n            if kind in ['train', 'posterior_sample_and_average', 'prior_sample']:\n                self.g0s_val = zs_g0.sample\n            else:\n                self.g0s_val = zs_g0.mean\n        self.prior_zs_co = prior_zs_co = [None] * num_steps\n        self.posterior_zs_co = posterior_zs_co = [None] * num_steps\n        self.zs_co = zs_co = [None] * num_steps\n        self.prior_zs_ar_con = None\n        if co_dim > 0:\n            autocorrelation_taus = [hps.prior_ar_atau for x in range(hps.co_dim)]\n            noise_variances = [hps.prior_ar_nvar for x in range(hps.co_dim)]\n            self.prior_zs_ar_con = prior_zs_ar_con = LearnableAutoRegressive1Prior(batch_size, hps.co_dim, autocorrelation_taus, noise_variances, hps.do_train_prior_ar_atau, hps.do_train_prior_ar_nvar, num_steps, 'u_prior_ar1')\n    self.controller_outputs = u_t = [None] * num_steps\n    self.con_ics = con_state = None\n    self.con_states = con_states = [None] * num_steps\n    self.con_outs = con_outs = [None] * num_steps\n    self.gen_inputs = gen_inputs = [None] * num_steps\n    if co_dim > 0:\n        con_cell = gen_cell_class(hps.con_dim, input_weight_scale=hps.cell_weight_scale, rec_weight_scale=hps.cell_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_con_reg'])\n        with tf.variable_scope('con', reuse=False):\n            self.con_ics = tf.tile(tf.Variable(tf.zeros([1, hps.con_dim * con_cell.state_multiplier]), name='c0'), tf.stack([batch_size, 1]))\n            self.con_ics.set_shape([None, con_cell.state_size])\n            con_states[-1] = self.con_ics\n    gen_cell = gen_cell_class(hps.gen_dim, input_weight_scale=hps.gen_cell_input_weight_scale, rec_weight_scale=hps.gen_cell_rec_weight_scale, clip_value=hps.cell_clip_value, recurrent_collections=['l2_gen_reg'])\n    with tf.variable_scope('gen', reuse=False):\n        if ic_dim == 0:\n            self.gen_ics = tf.tile(tf.Variable(tf.zeros([1, gen_cell.state_size]), name='g0'), tf.stack([batch_size, 1]))\n        else:\n            self.gen_ics = linear(self.g0s_val, gen_cell.state_size, identity_if_possible=True, name='g0_2_gen_ic')\n        self.gen_states = gen_states = [None] * num_steps\n        self.gen_outs = gen_outs = [None] * num_steps\n        gen_states[-1] = self.gen_ics\n        gen_outs[-1] = gen_cell.output_from_state(gen_states[-1])\n        self.factors = factors = [None] * num_steps\n        factors[-1] = linear(gen_outs[-1], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n    self.rates = rates = [None] * num_steps\n    with tf.variable_scope('glm', reuse=False):\n        if hps.output_dist == 'poisson':\n            log_rates_t0 = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            log_rates_t0.set_shape([None, None])\n            rates[-1] = tf.exp(log_rates_t0)\n            rates[-1].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n        elif hps.output_dist == 'gaussian':\n            mean_n_logvars = tf.matmul(factors[-1], this_out_fac_W) + this_out_fac_b\n            mean_n_logvars.set_shape([None, None])\n            (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n            rates[-1] = means_t_bxd\n        else:\n            assert False, 'NIY'\n    self.output_dist_params = dist_params = [None] * num_steps\n    self.log_p_xgz_b = log_p_xgz_b = 0.0\n    for t in range(num_steps):\n        if co_dim > 0:\n            tlag = t - hps.controller_input_lag\n            if tlag < 0:\n                con_in_f_t = tf.zeros_like(ci_enc_fwd[0])\n            else:\n                con_in_f_t = ci_enc_fwd[tlag]\n            if hps.do_causal_controller:\n                con_in_list_t = [con_in_f_t]\n            else:\n                tlag_rev = t + hps.controller_input_lag\n                if tlag_rev >= num_steps:\n                    con_in_r_t = tf.zeros_like(ci_enc_rev[0])\n                else:\n                    con_in_r_t = ci_enc_rev[tlag_rev]\n                con_in_list_t = [con_in_f_t, con_in_r_t]\n            if hps.do_feed_factors_to_controller:\n                if hps.feedback_factors_or_rates == 'factors':\n                    con_in_list_t.append(factors[t - 1])\n                elif hps.feedback_factors_or_rates == 'rates':\n                    con_in_list_t.append(rates[t - 1])\n                else:\n                    assert False, 'NIY'\n            con_in_t = tf.concat(axis=1, values=con_in_list_t)\n            con_in_t = tf.nn.dropout(con_in_t, keep_prob)\n            with tf.variable_scope('con', reuse=True if t > 0 else None):\n                (con_outs[t], con_states[t]) = con_cell(con_in_t, con_states[t - 1])\n                posterior_zs_co[t] = DiagonalGaussianFromInput(con_outs[t], co_dim, name='con_to_post_co')\n            if kind == 'train':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_sample_and_average':\n                u_t[t] = posterior_zs_co[t].sample\n            elif kind == 'posterior_push_mean':\n                u_t[t] = posterior_zs_co[t].mean\n            else:\n                u_t[t] = prior_zs_ar_con.samples_t[t]\n        if ext_input_dim > 0 and hps.inject_ext_input_to_gen:\n            ext_input_t_bxi = ext_input_do_bxtxi[:, t, :]\n            if co_dim > 0:\n                gen_inputs[t] = tf.concat(axis=1, values=[u_t[t], ext_input_t_bxi])\n            else:\n                gen_inputs[t] = ext_input_t_bxi\n        else:\n            gen_inputs[t] = u_t[t]\n        data_t_bxd = dataset_ph[:, t, :]\n        with tf.variable_scope('gen', reuse=True if t > 0 else None):\n            (gen_outs[t], gen_states[t]) = gen_cell(gen_inputs[t], gen_states[t - 1])\n            gen_outs[t] = tf.nn.dropout(gen_outs[t], keep_prob)\n        with tf.variable_scope('gen', reuse=True):\n            factors[t] = linear(gen_outs[t], factors_dim, do_bias=False, normalized=True, name='gen_2_fac')\n        with tf.variable_scope('glm', reuse=True if t > 0 else None):\n            if hps.output_dist == 'poisson':\n                log_rates_t = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                log_rates_t.set_shape([None, None])\n                rates[t] = dist_params[t] = tf.exp(log_rates_t)\n                rates[t].set_shape([None, hps.dataset_dims[hps.dataset_names[0]]])\n                loglikelihood_t = Poisson(log_rates_t).logp(data_t_bxd)\n            elif hps.output_dist == 'gaussian':\n                mean_n_logvars = tf.matmul(factors[t], this_out_fac_W) + this_out_fac_b\n                mean_n_logvars.set_shape([None, None])\n                (means_t_bxd, logvars_t_bxd) = tf.split(axis=1, num_or_size_splits=2, value=mean_n_logvars)\n                rates[t] = means_t_bxd\n                dist_params[t] = tf.concat(axis=1, values=[means_t_bxd, tf.exp(logvars_t_bxd)])\n                loglikelihood_t = diag_gaussian_log_likelihood(data_t_bxd, means_t_bxd, logvars_t_bxd)\n            else:\n                assert False, 'NIY'\n            log_p_xgz_b += tf.reduce_sum(loglikelihood_t, [1])\n    self.corr_cost = tf.constant(0.0)\n    if hps.co_mean_corr_scale > 0.0:\n        all_sum_corr = []\n        for i in range(hps.co_dim):\n            for j in range(i + 1, hps.co_dim):\n                sum_corr_ij = tf.constant(0.0)\n                for t in range(num_steps):\n                    u_mean_t = posterior_zs_co[t].mean\n                    sum_corr_ij += u_mean_t[:, i] * u_mean_t[:, j]\n                all_sum_corr.append(0.5 * tf.square(sum_corr_ij))\n        self.corr_cost = tf.reduce_mean(all_sum_corr)\n    kl_cost_g0_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    kl_cost_co_b = tf.zeros_like(batch_size, dtype=tf.float32)\n    self.kl_cost = tf.constant(0.0)\n    self.recon_cost = tf.constant(0.0)\n    self.nll_bound_vae = tf.constant(0.0)\n    self.nll_bound_iwae = tf.constant(0.0)\n    if kind in ['train', 'posterior_sample_and_average', 'posterior_push_mean']:\n        kl_cost_g0_b = 0.0\n        kl_cost_co_b = 0.0\n        if ic_dim > 0:\n            g0_priors = [self.prior_zs_g0]\n            g0_posts = [self.posterior_zs_g0]\n            kl_cost_g0_b = KLCost_GaussianGaussian(g0_posts, g0_priors).kl_cost_b\n            kl_cost_g0_b = hps.kl_ic_weight * kl_cost_g0_b\n        if co_dim > 0:\n            kl_cost_co_b = KLCost_GaussianGaussianProcessSampled(posterior_zs_co, prior_zs_ar_con).kl_cost_b\n            kl_cost_co_b = hps.kl_co_weight * kl_cost_co_b\n        self.recon_cost = -tf.reduce_mean(log_p_xgz_b)\n        self.kl_cost = tf.reduce_mean(kl_cost_g0_b + kl_cost_co_b)\n        lb_on_ll_b = log_p_xgz_b - kl_cost_g0_b - kl_cost_co_b\n        self.nll_bound_vae = -tf.reduce_mean(lb_on_ll_b)\n        k = tf.cast(tf.shape(log_p_xgz_b)[0], tf.float32)\n        iwae_lb_on_ll = -tf.log(k) + log_sum_exp(lb_on_ll_b)\n        self.nll_bound_iwae = -iwae_lb_on_ll\n    self.l2_cost = tf.constant(0.0)\n    if self.hps.l2_gen_scale > 0.0 or self.hps.l2_con_scale > 0.0:\n        l2_costs = []\n        l2_numels = []\n        l2_reg_var_lists = [tf.get_collection('l2_gen_reg'), tf.get_collection('l2_con_reg')]\n        l2_reg_scales = [self.hps.l2_gen_scale, self.hps.l2_con_scale]\n        for (l2_reg_vars, l2_scale) in zip(l2_reg_var_lists, l2_reg_scales):\n            for v in l2_reg_vars:\n                numel = tf.reduce_prod(tf.concat(axis=0, values=tf.shape(v)))\n                numel_f = tf.cast(numel, tf.float32)\n                l2_numels.append(numel_f)\n                v_l2 = tf.reduce_sum(v * v)\n                l2_costs.append(0.5 * l2_scale * v_l2)\n        self.l2_cost = tf.add_n(l2_costs) / tf.add_n(l2_numels)\n    self.kl_decay_step = tf.maximum(self.train_step - hps.kl_start_step, 0)\n    self.l2_decay_step = tf.maximum(self.train_step - hps.l2_start_step, 0)\n    kl_decay_step_f = tf.cast(self.kl_decay_step, tf.float32)\n    l2_decay_step_f = tf.cast(self.l2_decay_step, tf.float32)\n    kl_increase_steps_f = tf.cast(hps.kl_increase_steps, tf.float32)\n    l2_increase_steps_f = tf.cast(hps.l2_increase_steps, tf.float32)\n    self.kl_weight = kl_weight = tf.minimum(kl_decay_step_f / kl_increase_steps_f, 1.0)\n    self.l2_weight = l2_weight = tf.minimum(l2_decay_step_f / l2_increase_steps_f, 1.0)\n    self.timed_kl_cost = kl_weight * self.kl_cost\n    self.timed_l2_cost = l2_weight * self.l2_cost\n    self.weight_corr_cost = hps.co_mean_corr_scale * self.corr_cost\n    self.cost = self.recon_cost + self.timed_kl_cost + self.timed_l2_cost + self.weight_corr_cost\n    if kind != 'train':\n        self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n        self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep_lve)\n        return\n    if self.hps.do_train_io_only:\n        self.train_vars = tvars = tf.get_collection('IO_transformations', scope=tf.get_variable_scope().name)\n    elif self.hps.do_train_encoder_only:\n        tvars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/ic_enc_*')\n        tvars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='LFADS/z/ic_enc_*')\n        self.train_vars = tvars = tvars1 + tvars2\n    else:\n        self.train_vars = tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n    print('done.')\n    print('Model Variables (to be optimized): ')\n    total_params = 0\n    for i in range(len(tvars)):\n        shape = tvars[i].get_shape().as_list()\n        print('    ', i, tvars[i].name, shape)\n        total_params += np.prod(shape)\n    print('Total model parameters: ', total_params)\n    grads = tf.gradients(self.cost, tvars)\n    (grads, grad_global_norm) = tf.clip_by_global_norm(grads, hps.max_grad_norm)\n    opt = tf.train.AdamOptimizer(self.learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n    self.grads = grads\n    self.grad_global_norm = grad_global_norm\n    self.train_op = opt.apply_gradients(zip(grads, tvars), global_step=self.train_step)\n    self.seso_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.lve_saver = tf.train.Saver(tf.global_variables(), max_to_keep=hps.max_ckpt_to_keep)\n    self.example_image = tf.placeholder(tf.float32, shape=[1, None, None, 3], name='image_tensor')\n    self.example_summ = tf.summary.image('LFADS example', self.example_image, collections=['example_summaries'])\n    self.lr_summ = tf.summary.scalar('Learning rate', self.learning_rate)\n    self.kl_weight_summ = tf.summary.scalar('KL weight', self.kl_weight)\n    self.l2_weight_summ = tf.summary.scalar('L2 weight', self.l2_weight)\n    self.corr_cost_summ = tf.summary.scalar('Corr cost', self.weight_corr_cost)\n    self.grad_global_norm_summ = tf.summary.scalar('Gradient global norm', self.grad_global_norm)\n    if hps.co_dim > 0:\n        self.atau_summ = [None] * hps.co_dim\n        self.pvar_summ = [None] * hps.co_dim\n        for c in range(hps.co_dim):\n            self.atau_summ[c] = tf.summary.scalar('AR Autocorrelation taus ' + str(c), tf.exp(self.prior_zs_ar_con.logataus_1xu[0, c]))\n            self.pvar_summ[c] = tf.summary.scalar('AR Variances ' + str(c), tf.exp(self.prior_zs_ar_con.logpvars_1xu[0, c]))\n    kl_cost_ph = tf.placeholder(tf.float32, shape=[], name='kl_cost_ph')\n    self.kl_t_cost_summ = tf.summary.scalar('KL cost (train)', kl_cost_ph, collections=['train_summaries'])\n    self.kl_v_cost_summ = tf.summary.scalar('KL cost (valid)', kl_cost_ph, collections=['valid_summaries'])\n    l2_cost_ph = tf.placeholder(tf.float32, shape=[], name='l2_cost_ph')\n    self.l2_cost_summ = tf.summary.scalar('L2 cost', l2_cost_ph, collections=['train_summaries'])\n    recon_cost_ph = tf.placeholder(tf.float32, shape=[], name='recon_cost_ph')\n    self.recon_t_cost_summ = tf.summary.scalar('Reconstruction cost (train)', recon_cost_ph, collections=['train_summaries'])\n    self.recon_v_cost_summ = tf.summary.scalar('Reconstruction cost (valid)', recon_cost_ph, collections=['valid_summaries'])\n    total_cost_ph = tf.placeholder(tf.float32, shape=[], name='total_cost_ph')\n    self.cost_t_summ = tf.summary.scalar('Total cost (train)', total_cost_ph, collections=['train_summaries'])\n    self.cost_v_summ = tf.summary.scalar('Total cost (valid)', total_cost_ph, collections=['valid_summaries'])\n    self.kl_cost_ph = kl_cost_ph\n    self.l2_cost_ph = l2_cost_ph\n    self.recon_cost_ph = recon_cost_ph\n    self.total_cost_ph = total_cost_ph\n    self.merged_examples = tf.summary.merge_all(key='example_summaries')\n    self.merged_generic = tf.summary.merge_all()\n    self.merged_train = tf.summary.merge_all(key='train_summaries')\n    self.merged_valid = tf.summary.merge_all(key='valid_summaries')\n    session = tf.get_default_session()\n    self.logfile = os.path.join(hps.lfads_save_dir, 'lfads_log')\n    self.writer = tf.summary.FileWriter(self.logfile)"
        ]
    },
    {
        "func_name": "build_feed_dict",
        "original": "def build_feed_dict(self, train_name, data_bxtxd, ext_input_bxtxi=None, keep_prob=None):\n    \"\"\"Build the feed dictionary, handles cases where there is no value defined.\n\n    Args:\n      train_name: The key into the datasets, to set the tf.case statement for\n        the proper readin / readout matrices.\n      data_bxtxd: The data tensor\n      ext_input_bxtxi (optional): The external input tensor\n      keep_prob: The drop out keep probability.\n\n    Returns:\n      The feed dictionary with TF tensors as keys and data as values, for use\n      with tf.Session.run()\n\n    \"\"\"\n    feed_dict = {}\n    (B, T, _) = data_bxtxd.shape\n    feed_dict[self.dataName] = train_name\n    feed_dict[self.dataset_ph] = data_bxtxd\n    if self.ext_input is not None and ext_input_bxtxi is not None:\n        feed_dict[self.ext_input] = ext_input_bxtxi\n    if keep_prob is None:\n        feed_dict[self.keep_prob] = self.hps.keep_prob\n    else:\n        feed_dict[self.keep_prob] = keep_prob\n    return feed_dict",
        "mutated": [
            "def build_feed_dict(self, train_name, data_bxtxd, ext_input_bxtxi=None, keep_prob=None):\n    if False:\n        i = 10\n    'Build the feed dictionary, handles cases where there is no value defined.\\n\\n    Args:\\n      train_name: The key into the datasets, to set the tf.case statement for\\n        the proper readin / readout matrices.\\n      data_bxtxd: The data tensor\\n      ext_input_bxtxi (optional): The external input tensor\\n      keep_prob: The drop out keep probability.\\n\\n    Returns:\\n      The feed dictionary with TF tensors as keys and data as values, for use\\n      with tf.Session.run()\\n\\n    '\n    feed_dict = {}\n    (B, T, _) = data_bxtxd.shape\n    feed_dict[self.dataName] = train_name\n    feed_dict[self.dataset_ph] = data_bxtxd\n    if self.ext_input is not None and ext_input_bxtxi is not None:\n        feed_dict[self.ext_input] = ext_input_bxtxi\n    if keep_prob is None:\n        feed_dict[self.keep_prob] = self.hps.keep_prob\n    else:\n        feed_dict[self.keep_prob] = keep_prob\n    return feed_dict",
            "def build_feed_dict(self, train_name, data_bxtxd, ext_input_bxtxi=None, keep_prob=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Build the feed dictionary, handles cases where there is no value defined.\\n\\n    Args:\\n      train_name: The key into the datasets, to set the tf.case statement for\\n        the proper readin / readout matrices.\\n      data_bxtxd: The data tensor\\n      ext_input_bxtxi (optional): The external input tensor\\n      keep_prob: The drop out keep probability.\\n\\n    Returns:\\n      The feed dictionary with TF tensors as keys and data as values, for use\\n      with tf.Session.run()\\n\\n    '\n    feed_dict = {}\n    (B, T, _) = data_bxtxd.shape\n    feed_dict[self.dataName] = train_name\n    feed_dict[self.dataset_ph] = data_bxtxd\n    if self.ext_input is not None and ext_input_bxtxi is not None:\n        feed_dict[self.ext_input] = ext_input_bxtxi\n    if keep_prob is None:\n        feed_dict[self.keep_prob] = self.hps.keep_prob\n    else:\n        feed_dict[self.keep_prob] = keep_prob\n    return feed_dict",
            "def build_feed_dict(self, train_name, data_bxtxd, ext_input_bxtxi=None, keep_prob=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Build the feed dictionary, handles cases where there is no value defined.\\n\\n    Args:\\n      train_name: The key into the datasets, to set the tf.case statement for\\n        the proper readin / readout matrices.\\n      data_bxtxd: The data tensor\\n      ext_input_bxtxi (optional): The external input tensor\\n      keep_prob: The drop out keep probability.\\n\\n    Returns:\\n      The feed dictionary with TF tensors as keys and data as values, for use\\n      with tf.Session.run()\\n\\n    '\n    feed_dict = {}\n    (B, T, _) = data_bxtxd.shape\n    feed_dict[self.dataName] = train_name\n    feed_dict[self.dataset_ph] = data_bxtxd\n    if self.ext_input is not None and ext_input_bxtxi is not None:\n        feed_dict[self.ext_input] = ext_input_bxtxi\n    if keep_prob is None:\n        feed_dict[self.keep_prob] = self.hps.keep_prob\n    else:\n        feed_dict[self.keep_prob] = keep_prob\n    return feed_dict",
            "def build_feed_dict(self, train_name, data_bxtxd, ext_input_bxtxi=None, keep_prob=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Build the feed dictionary, handles cases where there is no value defined.\\n\\n    Args:\\n      train_name: The key into the datasets, to set the tf.case statement for\\n        the proper readin / readout matrices.\\n      data_bxtxd: The data tensor\\n      ext_input_bxtxi (optional): The external input tensor\\n      keep_prob: The drop out keep probability.\\n\\n    Returns:\\n      The feed dictionary with TF tensors as keys and data as values, for use\\n      with tf.Session.run()\\n\\n    '\n    feed_dict = {}\n    (B, T, _) = data_bxtxd.shape\n    feed_dict[self.dataName] = train_name\n    feed_dict[self.dataset_ph] = data_bxtxd\n    if self.ext_input is not None and ext_input_bxtxi is not None:\n        feed_dict[self.ext_input] = ext_input_bxtxi\n    if keep_prob is None:\n        feed_dict[self.keep_prob] = self.hps.keep_prob\n    else:\n        feed_dict[self.keep_prob] = keep_prob\n    return feed_dict",
            "def build_feed_dict(self, train_name, data_bxtxd, ext_input_bxtxi=None, keep_prob=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Build the feed dictionary, handles cases where there is no value defined.\\n\\n    Args:\\n      train_name: The key into the datasets, to set the tf.case statement for\\n        the proper readin / readout matrices.\\n      data_bxtxd: The data tensor\\n      ext_input_bxtxi (optional): The external input tensor\\n      keep_prob: The drop out keep probability.\\n\\n    Returns:\\n      The feed dictionary with TF tensors as keys and data as values, for use\\n      with tf.Session.run()\\n\\n    '\n    feed_dict = {}\n    (B, T, _) = data_bxtxd.shape\n    feed_dict[self.dataName] = train_name\n    feed_dict[self.dataset_ph] = data_bxtxd\n    if self.ext_input is not None and ext_input_bxtxi is not None:\n        feed_dict[self.ext_input] = ext_input_bxtxi\n    if keep_prob is None:\n        feed_dict[self.keep_prob] = self.hps.keep_prob\n    else:\n        feed_dict[self.keep_prob] = keep_prob\n    return feed_dict"
        ]
    },
    {
        "func_name": "get_batch",
        "original": "@staticmethod\ndef get_batch(data_extxd, ext_input_extxi=None, batch_size=None, example_idxs=None):\n    \"\"\"Get a batch of data, either randomly chosen, or specified directly.\n\n    Args:\n      data_extxd: The data to model, numpy tensors with shape:\n        # examples x # time steps x # dimensions\n      ext_input_extxi (optional): The external inputs, numpy tensor with shape:\n        # examples x # time steps x # external input dimensions\n      batch_size:  The size of the batch to return\n      example_idxs (optional): The example indices used to select examples.\n\n    Returns:\n      A tuple with two parts:\n        1. Batched data numpy tensor with shape:\n        batch_size x # time steps x # dimensions\n        2. Batched external input numpy tensor with shape:\n        batch_size x # time steps x # external input dims\n    \"\"\"\n    assert batch_size is not None or example_idxs is not None, 'Problems'\n    (E, T, D) = data_extxd.shape\n    if example_idxs is None:\n        example_idxs = np.random.choice(E, batch_size)\n    ext_input_bxtxi = None\n    if ext_input_extxi is not None:\n        ext_input_bxtxi = ext_input_extxi[example_idxs, :, :]\n    return (data_extxd[example_idxs, :, :], ext_input_bxtxi)",
        "mutated": [
            "@staticmethod\ndef get_batch(data_extxd, ext_input_extxi=None, batch_size=None, example_idxs=None):\n    if False:\n        i = 10\n    'Get a batch of data, either randomly chosen, or specified directly.\\n\\n    Args:\\n      data_extxd: The data to model, numpy tensors with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): The external inputs, numpy tensor with shape:\\n        # examples x # time steps x # external input dimensions\\n      batch_size:  The size of the batch to return\\n      example_idxs (optional): The example indices used to select examples.\\n\\n    Returns:\\n      A tuple with two parts:\\n        1. Batched data numpy tensor with shape:\\n        batch_size x # time steps x # dimensions\\n        2. Batched external input numpy tensor with shape:\\n        batch_size x # time steps x # external input dims\\n    '\n    assert batch_size is not None or example_idxs is not None, 'Problems'\n    (E, T, D) = data_extxd.shape\n    if example_idxs is None:\n        example_idxs = np.random.choice(E, batch_size)\n    ext_input_bxtxi = None\n    if ext_input_extxi is not None:\n        ext_input_bxtxi = ext_input_extxi[example_idxs, :, :]\n    return (data_extxd[example_idxs, :, :], ext_input_bxtxi)",
            "@staticmethod\ndef get_batch(data_extxd, ext_input_extxi=None, batch_size=None, example_idxs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get a batch of data, either randomly chosen, or specified directly.\\n\\n    Args:\\n      data_extxd: The data to model, numpy tensors with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): The external inputs, numpy tensor with shape:\\n        # examples x # time steps x # external input dimensions\\n      batch_size:  The size of the batch to return\\n      example_idxs (optional): The example indices used to select examples.\\n\\n    Returns:\\n      A tuple with two parts:\\n        1. Batched data numpy tensor with shape:\\n        batch_size x # time steps x # dimensions\\n        2. Batched external input numpy tensor with shape:\\n        batch_size x # time steps x # external input dims\\n    '\n    assert batch_size is not None or example_idxs is not None, 'Problems'\n    (E, T, D) = data_extxd.shape\n    if example_idxs is None:\n        example_idxs = np.random.choice(E, batch_size)\n    ext_input_bxtxi = None\n    if ext_input_extxi is not None:\n        ext_input_bxtxi = ext_input_extxi[example_idxs, :, :]\n    return (data_extxd[example_idxs, :, :], ext_input_bxtxi)",
            "@staticmethod\ndef get_batch(data_extxd, ext_input_extxi=None, batch_size=None, example_idxs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get a batch of data, either randomly chosen, or specified directly.\\n\\n    Args:\\n      data_extxd: The data to model, numpy tensors with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): The external inputs, numpy tensor with shape:\\n        # examples x # time steps x # external input dimensions\\n      batch_size:  The size of the batch to return\\n      example_idxs (optional): The example indices used to select examples.\\n\\n    Returns:\\n      A tuple with two parts:\\n        1. Batched data numpy tensor with shape:\\n        batch_size x # time steps x # dimensions\\n        2. Batched external input numpy tensor with shape:\\n        batch_size x # time steps x # external input dims\\n    '\n    assert batch_size is not None or example_idxs is not None, 'Problems'\n    (E, T, D) = data_extxd.shape\n    if example_idxs is None:\n        example_idxs = np.random.choice(E, batch_size)\n    ext_input_bxtxi = None\n    if ext_input_extxi is not None:\n        ext_input_bxtxi = ext_input_extxi[example_idxs, :, :]\n    return (data_extxd[example_idxs, :, :], ext_input_bxtxi)",
            "@staticmethod\ndef get_batch(data_extxd, ext_input_extxi=None, batch_size=None, example_idxs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get a batch of data, either randomly chosen, or specified directly.\\n\\n    Args:\\n      data_extxd: The data to model, numpy tensors with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): The external inputs, numpy tensor with shape:\\n        # examples x # time steps x # external input dimensions\\n      batch_size:  The size of the batch to return\\n      example_idxs (optional): The example indices used to select examples.\\n\\n    Returns:\\n      A tuple with two parts:\\n        1. Batched data numpy tensor with shape:\\n        batch_size x # time steps x # dimensions\\n        2. Batched external input numpy tensor with shape:\\n        batch_size x # time steps x # external input dims\\n    '\n    assert batch_size is not None or example_idxs is not None, 'Problems'\n    (E, T, D) = data_extxd.shape\n    if example_idxs is None:\n        example_idxs = np.random.choice(E, batch_size)\n    ext_input_bxtxi = None\n    if ext_input_extxi is not None:\n        ext_input_bxtxi = ext_input_extxi[example_idxs, :, :]\n    return (data_extxd[example_idxs, :, :], ext_input_bxtxi)",
            "@staticmethod\ndef get_batch(data_extxd, ext_input_extxi=None, batch_size=None, example_idxs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get a batch of data, either randomly chosen, or specified directly.\\n\\n    Args:\\n      data_extxd: The data to model, numpy tensors with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): The external inputs, numpy tensor with shape:\\n        # examples x # time steps x # external input dimensions\\n      batch_size:  The size of the batch to return\\n      example_idxs (optional): The example indices used to select examples.\\n\\n    Returns:\\n      A tuple with two parts:\\n        1. Batched data numpy tensor with shape:\\n        batch_size x # time steps x # dimensions\\n        2. Batched external input numpy tensor with shape:\\n        batch_size x # time steps x # external input dims\\n    '\n    assert batch_size is not None or example_idxs is not None, 'Problems'\n    (E, T, D) = data_extxd.shape\n    if example_idxs is None:\n        example_idxs = np.random.choice(E, batch_size)\n    ext_input_bxtxi = None\n    if ext_input_extxi is not None:\n        ext_input_bxtxi = ext_input_extxi[example_idxs, :, :]\n    return (data_extxd[example_idxs, :, :], ext_input_bxtxi)"
        ]
    },
    {
        "func_name": "example_idxs_mod_batch_size",
        "original": "@staticmethod\ndef example_idxs_mod_batch_size(nexamples, batch_size):\n    \"\"\"Given a number of examples, E, and a batch_size, B, generate indices\n    [0, 1, 2, ... B-1;\n    [B, B+1, ... 2*B-1;\n    ...\n    ]\n    returning those indices as a 2-dim tensor shaped like E/B x B.  Note that\n    shape is only correct if E % B == 0.  If not, then an extra row is generated\n    so that the remainder of examples is included. The extra examples are\n    explicitly to to the zero index (see randomize_example_idxs_mod_batch_size)\n    for randomized behavior.\n\n    Args:\n      nexamples: The number of examples to batch up.\n      batch_size: The size of the batch.\n    Returns:\n      2-dim tensor as described above.\n    \"\"\"\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        ridxs = np.random.permutation(nexamples)[0:bmrem].astype(np.int32)\n        bmrem_examples = np.sort(ridxs)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    example_idxs_e_x_edivb = np.reshape(example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)",
        "mutated": [
            "@staticmethod\ndef example_idxs_mod_batch_size(nexamples, batch_size):\n    if False:\n        i = 10\n    'Given a number of examples, E, and a batch_size, B, generate indices\\n    [0, 1, 2, ... B-1;\\n    [B, B+1, ... 2*B-1;\\n    ...\\n    ]\\n    returning those indices as a 2-dim tensor shaped like E/B x B.  Note that\\n    shape is only correct if E % B == 0.  If not, then an extra row is generated\\n    so that the remainder of examples is included. The extra examples are\\n    explicitly to to the zero index (see randomize_example_idxs_mod_batch_size)\\n    for randomized behavior.\\n\\n    Args:\\n      nexamples: The number of examples to batch up.\\n      batch_size: The size of the batch.\\n    Returns:\\n      2-dim tensor as described above.\\n    '\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        ridxs = np.random.permutation(nexamples)[0:bmrem].astype(np.int32)\n        bmrem_examples = np.sort(ridxs)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    example_idxs_e_x_edivb = np.reshape(example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)",
            "@staticmethod\ndef example_idxs_mod_batch_size(nexamples, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Given a number of examples, E, and a batch_size, B, generate indices\\n    [0, 1, 2, ... B-1;\\n    [B, B+1, ... 2*B-1;\\n    ...\\n    ]\\n    returning those indices as a 2-dim tensor shaped like E/B x B.  Note that\\n    shape is only correct if E % B == 0.  If not, then an extra row is generated\\n    so that the remainder of examples is included. The extra examples are\\n    explicitly to to the zero index (see randomize_example_idxs_mod_batch_size)\\n    for randomized behavior.\\n\\n    Args:\\n      nexamples: The number of examples to batch up.\\n      batch_size: The size of the batch.\\n    Returns:\\n      2-dim tensor as described above.\\n    '\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        ridxs = np.random.permutation(nexamples)[0:bmrem].astype(np.int32)\n        bmrem_examples = np.sort(ridxs)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    example_idxs_e_x_edivb = np.reshape(example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)",
            "@staticmethod\ndef example_idxs_mod_batch_size(nexamples, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Given a number of examples, E, and a batch_size, B, generate indices\\n    [0, 1, 2, ... B-1;\\n    [B, B+1, ... 2*B-1;\\n    ...\\n    ]\\n    returning those indices as a 2-dim tensor shaped like E/B x B.  Note that\\n    shape is only correct if E % B == 0.  If not, then an extra row is generated\\n    so that the remainder of examples is included. The extra examples are\\n    explicitly to to the zero index (see randomize_example_idxs_mod_batch_size)\\n    for randomized behavior.\\n\\n    Args:\\n      nexamples: The number of examples to batch up.\\n      batch_size: The size of the batch.\\n    Returns:\\n      2-dim tensor as described above.\\n    '\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        ridxs = np.random.permutation(nexamples)[0:bmrem].astype(np.int32)\n        bmrem_examples = np.sort(ridxs)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    example_idxs_e_x_edivb = np.reshape(example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)",
            "@staticmethod\ndef example_idxs_mod_batch_size(nexamples, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Given a number of examples, E, and a batch_size, B, generate indices\\n    [0, 1, 2, ... B-1;\\n    [B, B+1, ... 2*B-1;\\n    ...\\n    ]\\n    returning those indices as a 2-dim tensor shaped like E/B x B.  Note that\\n    shape is only correct if E % B == 0.  If not, then an extra row is generated\\n    so that the remainder of examples is included. The extra examples are\\n    explicitly to to the zero index (see randomize_example_idxs_mod_batch_size)\\n    for randomized behavior.\\n\\n    Args:\\n      nexamples: The number of examples to batch up.\\n      batch_size: The size of the batch.\\n    Returns:\\n      2-dim tensor as described above.\\n    '\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        ridxs = np.random.permutation(nexamples)[0:bmrem].astype(np.int32)\n        bmrem_examples = np.sort(ridxs)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    example_idxs_e_x_edivb = np.reshape(example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)",
            "@staticmethod\ndef example_idxs_mod_batch_size(nexamples, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Given a number of examples, E, and a batch_size, B, generate indices\\n    [0, 1, 2, ... B-1;\\n    [B, B+1, ... 2*B-1;\\n    ...\\n    ]\\n    returning those indices as a 2-dim tensor shaped like E/B x B.  Note that\\n    shape is only correct if E % B == 0.  If not, then an extra row is generated\\n    so that the remainder of examples is included. The extra examples are\\n    explicitly to to the zero index (see randomize_example_idxs_mod_batch_size)\\n    for randomized behavior.\\n\\n    Args:\\n      nexamples: The number of examples to batch up.\\n      batch_size: The size of the batch.\\n    Returns:\\n      2-dim tensor as described above.\\n    '\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        ridxs = np.random.permutation(nexamples)[0:bmrem].astype(np.int32)\n        bmrem_examples = np.sort(ridxs)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    example_idxs_e_x_edivb = np.reshape(example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)"
        ]
    },
    {
        "func_name": "randomize_example_idxs_mod_batch_size",
        "original": "@staticmethod\ndef randomize_example_idxs_mod_batch_size(nexamples, batch_size):\n    \"\"\"Indices 1:nexamples, randomized, in 2D form of\n    shape = (nexamples / batch_size) x batch_size.  The remainder\n    is managed by drawing randomly from 1:nexamples.\n\n    Args:\n      nexamples: number of examples to randomize\n      batch_size: number of elements in batch\n\n    Returns:\n      The randomized, properly shaped indicies.\n    \"\"\"\n    assert nexamples > batch_size, 'Problems'\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        bmrem_examples = np.random.choice(range(nexamples), size=bmrem, replace=False)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    mixed_example_idxs = np.random.permutation(example_idxs)\n    example_idxs_e_x_edivb = np.reshape(mixed_example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)",
        "mutated": [
            "@staticmethod\ndef randomize_example_idxs_mod_batch_size(nexamples, batch_size):\n    if False:\n        i = 10\n    'Indices 1:nexamples, randomized, in 2D form of\\n    shape = (nexamples / batch_size) x batch_size.  The remainder\\n    is managed by drawing randomly from 1:nexamples.\\n\\n    Args:\\n      nexamples: number of examples to randomize\\n      batch_size: number of elements in batch\\n\\n    Returns:\\n      The randomized, properly shaped indicies.\\n    '\n    assert nexamples > batch_size, 'Problems'\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        bmrem_examples = np.random.choice(range(nexamples), size=bmrem, replace=False)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    mixed_example_idxs = np.random.permutation(example_idxs)\n    example_idxs_e_x_edivb = np.reshape(mixed_example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)",
            "@staticmethod\ndef randomize_example_idxs_mod_batch_size(nexamples, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Indices 1:nexamples, randomized, in 2D form of\\n    shape = (nexamples / batch_size) x batch_size.  The remainder\\n    is managed by drawing randomly from 1:nexamples.\\n\\n    Args:\\n      nexamples: number of examples to randomize\\n      batch_size: number of elements in batch\\n\\n    Returns:\\n      The randomized, properly shaped indicies.\\n    '\n    assert nexamples > batch_size, 'Problems'\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        bmrem_examples = np.random.choice(range(nexamples), size=bmrem, replace=False)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    mixed_example_idxs = np.random.permutation(example_idxs)\n    example_idxs_e_x_edivb = np.reshape(mixed_example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)",
            "@staticmethod\ndef randomize_example_idxs_mod_batch_size(nexamples, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Indices 1:nexamples, randomized, in 2D form of\\n    shape = (nexamples / batch_size) x batch_size.  The remainder\\n    is managed by drawing randomly from 1:nexamples.\\n\\n    Args:\\n      nexamples: number of examples to randomize\\n      batch_size: number of elements in batch\\n\\n    Returns:\\n      The randomized, properly shaped indicies.\\n    '\n    assert nexamples > batch_size, 'Problems'\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        bmrem_examples = np.random.choice(range(nexamples), size=bmrem, replace=False)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    mixed_example_idxs = np.random.permutation(example_idxs)\n    example_idxs_e_x_edivb = np.reshape(mixed_example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)",
            "@staticmethod\ndef randomize_example_idxs_mod_batch_size(nexamples, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Indices 1:nexamples, randomized, in 2D form of\\n    shape = (nexamples / batch_size) x batch_size.  The remainder\\n    is managed by drawing randomly from 1:nexamples.\\n\\n    Args:\\n      nexamples: number of examples to randomize\\n      batch_size: number of elements in batch\\n\\n    Returns:\\n      The randomized, properly shaped indicies.\\n    '\n    assert nexamples > batch_size, 'Problems'\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        bmrem_examples = np.random.choice(range(nexamples), size=bmrem, replace=False)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    mixed_example_idxs = np.random.permutation(example_idxs)\n    example_idxs_e_x_edivb = np.reshape(mixed_example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)",
            "@staticmethod\ndef randomize_example_idxs_mod_batch_size(nexamples, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Indices 1:nexamples, randomized, in 2D form of\\n    shape = (nexamples / batch_size) x batch_size.  The remainder\\n    is managed by drawing randomly from 1:nexamples.\\n\\n    Args:\\n      nexamples: number of examples to randomize\\n      batch_size: number of elements in batch\\n\\n    Returns:\\n      The randomized, properly shaped indicies.\\n    '\n    assert nexamples > batch_size, 'Problems'\n    bmrem = batch_size - nexamples % batch_size\n    bmrem_examples = []\n    if bmrem < batch_size:\n        bmrem_examples = np.random.choice(range(nexamples), size=bmrem, replace=False)\n    example_idxs = range(nexamples) + list(bmrem_examples)\n    mixed_example_idxs = np.random.permutation(example_idxs)\n    example_idxs_e_x_edivb = np.reshape(mixed_example_idxs, [-1, batch_size])\n    return (example_idxs_e_x_edivb, bmrem)"
        ]
    },
    {
        "func_name": "shuffle_spikes_in_time",
        "original": "def shuffle_spikes_in_time(self, data_bxtxd):\n    \"\"\"Shuffle the spikes in the temporal dimension.  This is useful to\n    help the LFADS system avoid overfitting to individual spikes or fast\n    oscillations found in the data that are irrelevant to behavior. A\n    pure 'tabula rasa' approach would avoid this, but LFADS is sensitive\n    enough to pick up dynamics that you may not want.\n\n    Args:\n      data_bxtxd: numpy array of spike count data to be shuffled.\n    Returns:\n    S_bxtxd, a numpy array with the same dimensions and contents as\n      data_bxtxd, but shuffled appropriately.\n\n    \"\"\"\n    (B, T, N) = data_bxtxd.shape\n    w = self.hps.temporal_spike_jitter_width\n    if w == 0:\n        return data_bxtxd\n    max_counts = np.max(data_bxtxd)\n    S_bxtxd = np.zeros([B, T, N])\n    for mc in range(1, max_counts + 1):\n        idxs = np.nonzero(data_bxtxd >= mc)\n        data_ones = np.zeros_like(data_bxtxd)\n        data_ones[data_bxtxd >= mc] = 1\n        nfound = len(idxs[0])\n        shuffles_incrs_in_time = np.random.randint(-w, w, size=nfound)\n        shuffle_tidxs = idxs[1].copy()\n        shuffle_tidxs += shuffles_incrs_in_time\n        shuffle_tidxs[shuffle_tidxs < 0] = -shuffle_tidxs[shuffle_tidxs < 0]\n        shuffle_tidxs[shuffle_tidxs > T - 1] = T - 1 - (shuffle_tidxs[shuffle_tidxs > T - 1] - (T - 1))\n        for iii in zip(idxs[0], shuffle_tidxs, idxs[2]):\n            S_bxtxd[iii] += 1\n    return S_bxtxd",
        "mutated": [
            "def shuffle_spikes_in_time(self, data_bxtxd):\n    if False:\n        i = 10\n    \"Shuffle the spikes in the temporal dimension.  This is useful to\\n    help the LFADS system avoid overfitting to individual spikes or fast\\n    oscillations found in the data that are irrelevant to behavior. A\\n    pure 'tabula rasa' approach would avoid this, but LFADS is sensitive\\n    enough to pick up dynamics that you may not want.\\n\\n    Args:\\n      data_bxtxd: numpy array of spike count data to be shuffled.\\n    Returns:\\n    S_bxtxd, a numpy array with the same dimensions and contents as\\n      data_bxtxd, but shuffled appropriately.\\n\\n    \"\n    (B, T, N) = data_bxtxd.shape\n    w = self.hps.temporal_spike_jitter_width\n    if w == 0:\n        return data_bxtxd\n    max_counts = np.max(data_bxtxd)\n    S_bxtxd = np.zeros([B, T, N])\n    for mc in range(1, max_counts + 1):\n        idxs = np.nonzero(data_bxtxd >= mc)\n        data_ones = np.zeros_like(data_bxtxd)\n        data_ones[data_bxtxd >= mc] = 1\n        nfound = len(idxs[0])\n        shuffles_incrs_in_time = np.random.randint(-w, w, size=nfound)\n        shuffle_tidxs = idxs[1].copy()\n        shuffle_tidxs += shuffles_incrs_in_time\n        shuffle_tidxs[shuffle_tidxs < 0] = -shuffle_tidxs[shuffle_tidxs < 0]\n        shuffle_tidxs[shuffle_tidxs > T - 1] = T - 1 - (shuffle_tidxs[shuffle_tidxs > T - 1] - (T - 1))\n        for iii in zip(idxs[0], shuffle_tidxs, idxs[2]):\n            S_bxtxd[iii] += 1\n    return S_bxtxd",
            "def shuffle_spikes_in_time(self, data_bxtxd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Shuffle the spikes in the temporal dimension.  This is useful to\\n    help the LFADS system avoid overfitting to individual spikes or fast\\n    oscillations found in the data that are irrelevant to behavior. A\\n    pure 'tabula rasa' approach would avoid this, but LFADS is sensitive\\n    enough to pick up dynamics that you may not want.\\n\\n    Args:\\n      data_bxtxd: numpy array of spike count data to be shuffled.\\n    Returns:\\n    S_bxtxd, a numpy array with the same dimensions and contents as\\n      data_bxtxd, but shuffled appropriately.\\n\\n    \"\n    (B, T, N) = data_bxtxd.shape\n    w = self.hps.temporal_spike_jitter_width\n    if w == 0:\n        return data_bxtxd\n    max_counts = np.max(data_bxtxd)\n    S_bxtxd = np.zeros([B, T, N])\n    for mc in range(1, max_counts + 1):\n        idxs = np.nonzero(data_bxtxd >= mc)\n        data_ones = np.zeros_like(data_bxtxd)\n        data_ones[data_bxtxd >= mc] = 1\n        nfound = len(idxs[0])\n        shuffles_incrs_in_time = np.random.randint(-w, w, size=nfound)\n        shuffle_tidxs = idxs[1].copy()\n        shuffle_tidxs += shuffles_incrs_in_time\n        shuffle_tidxs[shuffle_tidxs < 0] = -shuffle_tidxs[shuffle_tidxs < 0]\n        shuffle_tidxs[shuffle_tidxs > T - 1] = T - 1 - (shuffle_tidxs[shuffle_tidxs > T - 1] - (T - 1))\n        for iii in zip(idxs[0], shuffle_tidxs, idxs[2]):\n            S_bxtxd[iii] += 1\n    return S_bxtxd",
            "def shuffle_spikes_in_time(self, data_bxtxd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Shuffle the spikes in the temporal dimension.  This is useful to\\n    help the LFADS system avoid overfitting to individual spikes or fast\\n    oscillations found in the data that are irrelevant to behavior. A\\n    pure 'tabula rasa' approach would avoid this, but LFADS is sensitive\\n    enough to pick up dynamics that you may not want.\\n\\n    Args:\\n      data_bxtxd: numpy array of spike count data to be shuffled.\\n    Returns:\\n    S_bxtxd, a numpy array with the same dimensions and contents as\\n      data_bxtxd, but shuffled appropriately.\\n\\n    \"\n    (B, T, N) = data_bxtxd.shape\n    w = self.hps.temporal_spike_jitter_width\n    if w == 0:\n        return data_bxtxd\n    max_counts = np.max(data_bxtxd)\n    S_bxtxd = np.zeros([B, T, N])\n    for mc in range(1, max_counts + 1):\n        idxs = np.nonzero(data_bxtxd >= mc)\n        data_ones = np.zeros_like(data_bxtxd)\n        data_ones[data_bxtxd >= mc] = 1\n        nfound = len(idxs[0])\n        shuffles_incrs_in_time = np.random.randint(-w, w, size=nfound)\n        shuffle_tidxs = idxs[1].copy()\n        shuffle_tidxs += shuffles_incrs_in_time\n        shuffle_tidxs[shuffle_tidxs < 0] = -shuffle_tidxs[shuffle_tidxs < 0]\n        shuffle_tidxs[shuffle_tidxs > T - 1] = T - 1 - (shuffle_tidxs[shuffle_tidxs > T - 1] - (T - 1))\n        for iii in zip(idxs[0], shuffle_tidxs, idxs[2]):\n            S_bxtxd[iii] += 1\n    return S_bxtxd",
            "def shuffle_spikes_in_time(self, data_bxtxd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Shuffle the spikes in the temporal dimension.  This is useful to\\n    help the LFADS system avoid overfitting to individual spikes or fast\\n    oscillations found in the data that are irrelevant to behavior. A\\n    pure 'tabula rasa' approach would avoid this, but LFADS is sensitive\\n    enough to pick up dynamics that you may not want.\\n\\n    Args:\\n      data_bxtxd: numpy array of spike count data to be shuffled.\\n    Returns:\\n    S_bxtxd, a numpy array with the same dimensions and contents as\\n      data_bxtxd, but shuffled appropriately.\\n\\n    \"\n    (B, T, N) = data_bxtxd.shape\n    w = self.hps.temporal_spike_jitter_width\n    if w == 0:\n        return data_bxtxd\n    max_counts = np.max(data_bxtxd)\n    S_bxtxd = np.zeros([B, T, N])\n    for mc in range(1, max_counts + 1):\n        idxs = np.nonzero(data_bxtxd >= mc)\n        data_ones = np.zeros_like(data_bxtxd)\n        data_ones[data_bxtxd >= mc] = 1\n        nfound = len(idxs[0])\n        shuffles_incrs_in_time = np.random.randint(-w, w, size=nfound)\n        shuffle_tidxs = idxs[1].copy()\n        shuffle_tidxs += shuffles_incrs_in_time\n        shuffle_tidxs[shuffle_tidxs < 0] = -shuffle_tidxs[shuffle_tidxs < 0]\n        shuffle_tidxs[shuffle_tidxs > T - 1] = T - 1 - (shuffle_tidxs[shuffle_tidxs > T - 1] - (T - 1))\n        for iii in zip(idxs[0], shuffle_tidxs, idxs[2]):\n            S_bxtxd[iii] += 1\n    return S_bxtxd",
            "def shuffle_spikes_in_time(self, data_bxtxd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Shuffle the spikes in the temporal dimension.  This is useful to\\n    help the LFADS system avoid overfitting to individual spikes or fast\\n    oscillations found in the data that are irrelevant to behavior. A\\n    pure 'tabula rasa' approach would avoid this, but LFADS is sensitive\\n    enough to pick up dynamics that you may not want.\\n\\n    Args:\\n      data_bxtxd: numpy array of spike count data to be shuffled.\\n    Returns:\\n    S_bxtxd, a numpy array with the same dimensions and contents as\\n      data_bxtxd, but shuffled appropriately.\\n\\n    \"\n    (B, T, N) = data_bxtxd.shape\n    w = self.hps.temporal_spike_jitter_width\n    if w == 0:\n        return data_bxtxd\n    max_counts = np.max(data_bxtxd)\n    S_bxtxd = np.zeros([B, T, N])\n    for mc in range(1, max_counts + 1):\n        idxs = np.nonzero(data_bxtxd >= mc)\n        data_ones = np.zeros_like(data_bxtxd)\n        data_ones[data_bxtxd >= mc] = 1\n        nfound = len(idxs[0])\n        shuffles_incrs_in_time = np.random.randint(-w, w, size=nfound)\n        shuffle_tidxs = idxs[1].copy()\n        shuffle_tidxs += shuffles_incrs_in_time\n        shuffle_tidxs[shuffle_tidxs < 0] = -shuffle_tidxs[shuffle_tidxs < 0]\n        shuffle_tidxs[shuffle_tidxs > T - 1] = T - 1 - (shuffle_tidxs[shuffle_tidxs > T - 1] - (T - 1))\n        for iii in zip(idxs[0], shuffle_tidxs, idxs[2]):\n            S_bxtxd[iii] += 1\n    return S_bxtxd"
        ]
    },
    {
        "func_name": "shuffle_and_flatten_datasets",
        "original": "def shuffle_and_flatten_datasets(self, datasets, kind='train'):\n    \"\"\"Since LFADS supports multiple datasets in the same dynamical model,\n    we have to be careful to use all the data in a single training epoch.  But\n    since the datasets my have different data dimensionality, we cannot batch\n    examples from data dictionaries together.  Instead, we generate random\n    batches within each data dictionary, and then randomize these batches\n    while holding onto the dataname, so that when it's time to feed\n    the graph, the correct in/out matrices can be selected, per batch.\n\n    Args:\n      datasets: A dict of data dicts.  The dataset dict is simply a\n        name(string)-> data dictionary mapping (See top of lfads.py).\n      kind: 'train' or 'valid'\n\n    Returns:\n      A flat list, in which each element is a pair ('name', indices).\n    \"\"\"\n    batch_size = self.hps.batch_size\n    ndatasets = len(datasets)\n    random_example_idxs = {}\n    epoch_idxs = {}\n    all_name_example_idx_pairs = []\n    kind_data = kind + '_data'\n    for (name, data_dict) in datasets.items():\n        (nexamples, ntime, data_dim) = data_dict[kind_data].shape\n        epoch_idxs[name] = 0\n        (random_example_idxs, _) = self.randomize_example_idxs_mod_batch_size(nexamples, batch_size)\n        epoch_size = random_example_idxs.shape[0]\n        names = [name] * epoch_size\n        all_name_example_idx_pairs += zip(names, random_example_idxs)\n    np.random.shuffle(all_name_example_idx_pairs)\n    return all_name_example_idx_pairs",
        "mutated": [
            "def shuffle_and_flatten_datasets(self, datasets, kind='train'):\n    if False:\n        i = 10\n    \"Since LFADS supports multiple datasets in the same dynamical model,\\n    we have to be careful to use all the data in a single training epoch.  But\\n    since the datasets my have different data dimensionality, we cannot batch\\n    examples from data dictionaries together.  Instead, we generate random\\n    batches within each data dictionary, and then randomize these batches\\n    while holding onto the dataname, so that when it's time to feed\\n    the graph, the correct in/out matrices can be selected, per batch.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      kind: 'train' or 'valid'\\n\\n    Returns:\\n      A flat list, in which each element is a pair ('name', indices).\\n    \"\n    batch_size = self.hps.batch_size\n    ndatasets = len(datasets)\n    random_example_idxs = {}\n    epoch_idxs = {}\n    all_name_example_idx_pairs = []\n    kind_data = kind + '_data'\n    for (name, data_dict) in datasets.items():\n        (nexamples, ntime, data_dim) = data_dict[kind_data].shape\n        epoch_idxs[name] = 0\n        (random_example_idxs, _) = self.randomize_example_idxs_mod_batch_size(nexamples, batch_size)\n        epoch_size = random_example_idxs.shape[0]\n        names = [name] * epoch_size\n        all_name_example_idx_pairs += zip(names, random_example_idxs)\n    np.random.shuffle(all_name_example_idx_pairs)\n    return all_name_example_idx_pairs",
            "def shuffle_and_flatten_datasets(self, datasets, kind='train'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Since LFADS supports multiple datasets in the same dynamical model,\\n    we have to be careful to use all the data in a single training epoch.  But\\n    since the datasets my have different data dimensionality, we cannot batch\\n    examples from data dictionaries together.  Instead, we generate random\\n    batches within each data dictionary, and then randomize these batches\\n    while holding onto the dataname, so that when it's time to feed\\n    the graph, the correct in/out matrices can be selected, per batch.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      kind: 'train' or 'valid'\\n\\n    Returns:\\n      A flat list, in which each element is a pair ('name', indices).\\n    \"\n    batch_size = self.hps.batch_size\n    ndatasets = len(datasets)\n    random_example_idxs = {}\n    epoch_idxs = {}\n    all_name_example_idx_pairs = []\n    kind_data = kind + '_data'\n    for (name, data_dict) in datasets.items():\n        (nexamples, ntime, data_dim) = data_dict[kind_data].shape\n        epoch_idxs[name] = 0\n        (random_example_idxs, _) = self.randomize_example_idxs_mod_batch_size(nexamples, batch_size)\n        epoch_size = random_example_idxs.shape[0]\n        names = [name] * epoch_size\n        all_name_example_idx_pairs += zip(names, random_example_idxs)\n    np.random.shuffle(all_name_example_idx_pairs)\n    return all_name_example_idx_pairs",
            "def shuffle_and_flatten_datasets(self, datasets, kind='train'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Since LFADS supports multiple datasets in the same dynamical model,\\n    we have to be careful to use all the data in a single training epoch.  But\\n    since the datasets my have different data dimensionality, we cannot batch\\n    examples from data dictionaries together.  Instead, we generate random\\n    batches within each data dictionary, and then randomize these batches\\n    while holding onto the dataname, so that when it's time to feed\\n    the graph, the correct in/out matrices can be selected, per batch.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      kind: 'train' or 'valid'\\n\\n    Returns:\\n      A flat list, in which each element is a pair ('name', indices).\\n    \"\n    batch_size = self.hps.batch_size\n    ndatasets = len(datasets)\n    random_example_idxs = {}\n    epoch_idxs = {}\n    all_name_example_idx_pairs = []\n    kind_data = kind + '_data'\n    for (name, data_dict) in datasets.items():\n        (nexamples, ntime, data_dim) = data_dict[kind_data].shape\n        epoch_idxs[name] = 0\n        (random_example_idxs, _) = self.randomize_example_idxs_mod_batch_size(nexamples, batch_size)\n        epoch_size = random_example_idxs.shape[0]\n        names = [name] * epoch_size\n        all_name_example_idx_pairs += zip(names, random_example_idxs)\n    np.random.shuffle(all_name_example_idx_pairs)\n    return all_name_example_idx_pairs",
            "def shuffle_and_flatten_datasets(self, datasets, kind='train'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Since LFADS supports multiple datasets in the same dynamical model,\\n    we have to be careful to use all the data in a single training epoch.  But\\n    since the datasets my have different data dimensionality, we cannot batch\\n    examples from data dictionaries together.  Instead, we generate random\\n    batches within each data dictionary, and then randomize these batches\\n    while holding onto the dataname, so that when it's time to feed\\n    the graph, the correct in/out matrices can be selected, per batch.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      kind: 'train' or 'valid'\\n\\n    Returns:\\n      A flat list, in which each element is a pair ('name', indices).\\n    \"\n    batch_size = self.hps.batch_size\n    ndatasets = len(datasets)\n    random_example_idxs = {}\n    epoch_idxs = {}\n    all_name_example_idx_pairs = []\n    kind_data = kind + '_data'\n    for (name, data_dict) in datasets.items():\n        (nexamples, ntime, data_dim) = data_dict[kind_data].shape\n        epoch_idxs[name] = 0\n        (random_example_idxs, _) = self.randomize_example_idxs_mod_batch_size(nexamples, batch_size)\n        epoch_size = random_example_idxs.shape[0]\n        names = [name] * epoch_size\n        all_name_example_idx_pairs += zip(names, random_example_idxs)\n    np.random.shuffle(all_name_example_idx_pairs)\n    return all_name_example_idx_pairs",
            "def shuffle_and_flatten_datasets(self, datasets, kind='train'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Since LFADS supports multiple datasets in the same dynamical model,\\n    we have to be careful to use all the data in a single training epoch.  But\\n    since the datasets my have different data dimensionality, we cannot batch\\n    examples from data dictionaries together.  Instead, we generate random\\n    batches within each data dictionary, and then randomize these batches\\n    while holding onto the dataname, so that when it's time to feed\\n    the graph, the correct in/out matrices can be selected, per batch.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      kind: 'train' or 'valid'\\n\\n    Returns:\\n      A flat list, in which each element is a pair ('name', indices).\\n    \"\n    batch_size = self.hps.batch_size\n    ndatasets = len(datasets)\n    random_example_idxs = {}\n    epoch_idxs = {}\n    all_name_example_idx_pairs = []\n    kind_data = kind + '_data'\n    for (name, data_dict) in datasets.items():\n        (nexamples, ntime, data_dim) = data_dict[kind_data].shape\n        epoch_idxs[name] = 0\n        (random_example_idxs, _) = self.randomize_example_idxs_mod_batch_size(nexamples, batch_size)\n        epoch_size = random_example_idxs.shape[0]\n        names = [name] * epoch_size\n        all_name_example_idx_pairs += zip(names, random_example_idxs)\n    np.random.shuffle(all_name_example_idx_pairs)\n    return all_name_example_idx_pairs"
        ]
    },
    {
        "func_name": "train_epoch",
        "original": "def train_epoch(self, datasets, batch_size=None, do_save_ckpt=True):\n    \"\"\"Train the model through the entire dataset once.\n\n    Args:\n      datasets: A dict of data dicts.  The dataset dict is simply a\n        name(string)-> data dictionary mapping (See top of lfads.py).\n      batch_size (optional):  The batch_size to use\n      do_save_ckpt (optional): Should the routine save a checkpoint on this\n        training epoch?\n\n    Returns:\n    A tuple with 6 float values:\n      (total cost of the epoch, epoch reconstruction cost,\n       epoch kl cost, KL weight used this training epoch,\n       total l2 cost on generator, and the corresponding weight).\n    \"\"\"\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost, self.kl_weight, self.l2_cost, self.l2_weight, self.train_op]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind='train')\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    kl_weight = collected_op_values[-1][3]\n    l2_cost = collected_op_values[-1][4]\n    l2_weight = collected_op_values[-1][5]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    if do_save_ckpt:\n        session = tf.get_default_session()\n        checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '.ckpt')\n        self.seso_saver.save(session, checkpoint_path, global_step=self.train_step)\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost, kl_weight, l2_cost, l2_weight)",
        "mutated": [
            "def train_epoch(self, datasets, batch_size=None, do_save_ckpt=True):\n    if False:\n        i = 10\n    'Train the model through the entire dataset once.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      batch_size (optional):  The batch_size to use\\n      do_save_ckpt (optional): Should the routine save a checkpoint on this\\n        training epoch?\\n\\n    Returns:\\n    A tuple with 6 float values:\\n      (total cost of the epoch, epoch reconstruction cost,\\n       epoch kl cost, KL weight used this training epoch,\\n       total l2 cost on generator, and the corresponding weight).\\n    '\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost, self.kl_weight, self.l2_cost, self.l2_weight, self.train_op]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind='train')\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    kl_weight = collected_op_values[-1][3]\n    l2_cost = collected_op_values[-1][4]\n    l2_weight = collected_op_values[-1][5]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    if do_save_ckpt:\n        session = tf.get_default_session()\n        checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '.ckpt')\n        self.seso_saver.save(session, checkpoint_path, global_step=self.train_step)\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost, kl_weight, l2_cost, l2_weight)",
            "def train_epoch(self, datasets, batch_size=None, do_save_ckpt=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Train the model through the entire dataset once.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      batch_size (optional):  The batch_size to use\\n      do_save_ckpt (optional): Should the routine save a checkpoint on this\\n        training epoch?\\n\\n    Returns:\\n    A tuple with 6 float values:\\n      (total cost of the epoch, epoch reconstruction cost,\\n       epoch kl cost, KL weight used this training epoch,\\n       total l2 cost on generator, and the corresponding weight).\\n    '\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost, self.kl_weight, self.l2_cost, self.l2_weight, self.train_op]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind='train')\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    kl_weight = collected_op_values[-1][3]\n    l2_cost = collected_op_values[-1][4]\n    l2_weight = collected_op_values[-1][5]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    if do_save_ckpt:\n        session = tf.get_default_session()\n        checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '.ckpt')\n        self.seso_saver.save(session, checkpoint_path, global_step=self.train_step)\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost, kl_weight, l2_cost, l2_weight)",
            "def train_epoch(self, datasets, batch_size=None, do_save_ckpt=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Train the model through the entire dataset once.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      batch_size (optional):  The batch_size to use\\n      do_save_ckpt (optional): Should the routine save a checkpoint on this\\n        training epoch?\\n\\n    Returns:\\n    A tuple with 6 float values:\\n      (total cost of the epoch, epoch reconstruction cost,\\n       epoch kl cost, KL weight used this training epoch,\\n       total l2 cost on generator, and the corresponding weight).\\n    '\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost, self.kl_weight, self.l2_cost, self.l2_weight, self.train_op]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind='train')\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    kl_weight = collected_op_values[-1][3]\n    l2_cost = collected_op_values[-1][4]\n    l2_weight = collected_op_values[-1][5]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    if do_save_ckpt:\n        session = tf.get_default_session()\n        checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '.ckpt')\n        self.seso_saver.save(session, checkpoint_path, global_step=self.train_step)\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost, kl_weight, l2_cost, l2_weight)",
            "def train_epoch(self, datasets, batch_size=None, do_save_ckpt=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Train the model through the entire dataset once.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      batch_size (optional):  The batch_size to use\\n      do_save_ckpt (optional): Should the routine save a checkpoint on this\\n        training epoch?\\n\\n    Returns:\\n    A tuple with 6 float values:\\n      (total cost of the epoch, epoch reconstruction cost,\\n       epoch kl cost, KL weight used this training epoch,\\n       total l2 cost on generator, and the corresponding weight).\\n    '\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost, self.kl_weight, self.l2_cost, self.l2_weight, self.train_op]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind='train')\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    kl_weight = collected_op_values[-1][3]\n    l2_cost = collected_op_values[-1][4]\n    l2_weight = collected_op_values[-1][5]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    if do_save_ckpt:\n        session = tf.get_default_session()\n        checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '.ckpt')\n        self.seso_saver.save(session, checkpoint_path, global_step=self.train_step)\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost, kl_weight, l2_cost, l2_weight)",
            "def train_epoch(self, datasets, batch_size=None, do_save_ckpt=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Train the model through the entire dataset once.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      batch_size (optional):  The batch_size to use\\n      do_save_ckpt (optional): Should the routine save a checkpoint on this\\n        training epoch?\\n\\n    Returns:\\n    A tuple with 6 float values:\\n      (total cost of the epoch, epoch reconstruction cost,\\n       epoch kl cost, KL weight used this training epoch,\\n       total l2 cost on generator, and the corresponding weight).\\n    '\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost, self.kl_weight, self.l2_cost, self.l2_weight, self.train_op]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind='train')\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    kl_weight = collected_op_values[-1][3]\n    l2_cost = collected_op_values[-1][4]\n    l2_weight = collected_op_values[-1][5]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    if do_save_ckpt:\n        session = tf.get_default_session()\n        checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '.ckpt')\n        self.seso_saver.save(session, checkpoint_path, global_step=self.train_step)\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost, kl_weight, l2_cost, l2_weight)"
        ]
    },
    {
        "func_name": "run_epoch",
        "original": "def run_epoch(self, datasets, ops_to_eval, kind='train', batch_size=None, do_collect=True, keep_prob=None):\n    \"\"\"Run the model through the entire dataset once.\n\n    Args:\n      datasets: A dict of data dicts.  The dataset dict is simply a\n        name(string)-> data dictionary mapping (See top of lfads.py).\n      ops_to_eval: A list of tensorflow operations that will be evaluated in\n        the tf.session.run() call.\n      batch_size (optional):  The batch_size to use\n      do_collect (optional): Should the routine collect all session.run\n        output as a list, and return it?\n      keep_prob (optional): The dropout keep probability.\n\n    Returns:\n      A list of lists, the internal list is the return for the ops for each\n      session.run() call.  The outer list collects over the epoch.\n    \"\"\"\n    hps = self.hps\n    all_name_example_idx_pairs = self.shuffle_and_flatten_datasets(datasets, kind)\n    kind_data = kind + '_data'\n    kind_ext_input = kind + '_ext_input'\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    session = tf.get_default_session()\n    epoch_size = len(all_name_example_idx_pairs)\n    evaled_ops_list = []\n    for (name, example_idxs) in all_name_example_idx_pairs:\n        data_dict = datasets[name]\n        data_extxd = data_dict[kind_data]\n        if hps.output_dist == 'poisson' and hps.temporal_spike_jitter_width > 0:\n            data_extxd = self.shuffle_spikes_in_time(data_extxd)\n        ext_input_extxi = data_dict[kind_ext_input]\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, example_idxs=example_idxs)\n        feed_dict = self.build_feed_dict(name, data_bxtxd, ext_input_bxtxi, keep_prob=keep_prob)\n        evaled_ops_np = session.run(ops_to_eval, feed_dict=feed_dict)\n        if do_collect:\n            evaled_ops_list.append(evaled_ops_np)\n    return evaled_ops_list",
        "mutated": [
            "def run_epoch(self, datasets, ops_to_eval, kind='train', batch_size=None, do_collect=True, keep_prob=None):\n    if False:\n        i = 10\n    'Run the model through the entire dataset once.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      ops_to_eval: A list of tensorflow operations that will be evaluated in\\n        the tf.session.run() call.\\n      batch_size (optional):  The batch_size to use\\n      do_collect (optional): Should the routine collect all session.run\\n        output as a list, and return it?\\n      keep_prob (optional): The dropout keep probability.\\n\\n    Returns:\\n      A list of lists, the internal list is the return for the ops for each\\n      session.run() call.  The outer list collects over the epoch.\\n    '\n    hps = self.hps\n    all_name_example_idx_pairs = self.shuffle_and_flatten_datasets(datasets, kind)\n    kind_data = kind + '_data'\n    kind_ext_input = kind + '_ext_input'\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    session = tf.get_default_session()\n    epoch_size = len(all_name_example_idx_pairs)\n    evaled_ops_list = []\n    for (name, example_idxs) in all_name_example_idx_pairs:\n        data_dict = datasets[name]\n        data_extxd = data_dict[kind_data]\n        if hps.output_dist == 'poisson' and hps.temporal_spike_jitter_width > 0:\n            data_extxd = self.shuffle_spikes_in_time(data_extxd)\n        ext_input_extxi = data_dict[kind_ext_input]\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, example_idxs=example_idxs)\n        feed_dict = self.build_feed_dict(name, data_bxtxd, ext_input_bxtxi, keep_prob=keep_prob)\n        evaled_ops_np = session.run(ops_to_eval, feed_dict=feed_dict)\n        if do_collect:\n            evaled_ops_list.append(evaled_ops_np)\n    return evaled_ops_list",
            "def run_epoch(self, datasets, ops_to_eval, kind='train', batch_size=None, do_collect=True, keep_prob=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Run the model through the entire dataset once.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      ops_to_eval: A list of tensorflow operations that will be evaluated in\\n        the tf.session.run() call.\\n      batch_size (optional):  The batch_size to use\\n      do_collect (optional): Should the routine collect all session.run\\n        output as a list, and return it?\\n      keep_prob (optional): The dropout keep probability.\\n\\n    Returns:\\n      A list of lists, the internal list is the return for the ops for each\\n      session.run() call.  The outer list collects over the epoch.\\n    '\n    hps = self.hps\n    all_name_example_idx_pairs = self.shuffle_and_flatten_datasets(datasets, kind)\n    kind_data = kind + '_data'\n    kind_ext_input = kind + '_ext_input'\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    session = tf.get_default_session()\n    epoch_size = len(all_name_example_idx_pairs)\n    evaled_ops_list = []\n    for (name, example_idxs) in all_name_example_idx_pairs:\n        data_dict = datasets[name]\n        data_extxd = data_dict[kind_data]\n        if hps.output_dist == 'poisson' and hps.temporal_spike_jitter_width > 0:\n            data_extxd = self.shuffle_spikes_in_time(data_extxd)\n        ext_input_extxi = data_dict[kind_ext_input]\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, example_idxs=example_idxs)\n        feed_dict = self.build_feed_dict(name, data_bxtxd, ext_input_bxtxi, keep_prob=keep_prob)\n        evaled_ops_np = session.run(ops_to_eval, feed_dict=feed_dict)\n        if do_collect:\n            evaled_ops_list.append(evaled_ops_np)\n    return evaled_ops_list",
            "def run_epoch(self, datasets, ops_to_eval, kind='train', batch_size=None, do_collect=True, keep_prob=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Run the model through the entire dataset once.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      ops_to_eval: A list of tensorflow operations that will be evaluated in\\n        the tf.session.run() call.\\n      batch_size (optional):  The batch_size to use\\n      do_collect (optional): Should the routine collect all session.run\\n        output as a list, and return it?\\n      keep_prob (optional): The dropout keep probability.\\n\\n    Returns:\\n      A list of lists, the internal list is the return for the ops for each\\n      session.run() call.  The outer list collects over the epoch.\\n    '\n    hps = self.hps\n    all_name_example_idx_pairs = self.shuffle_and_flatten_datasets(datasets, kind)\n    kind_data = kind + '_data'\n    kind_ext_input = kind + '_ext_input'\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    session = tf.get_default_session()\n    epoch_size = len(all_name_example_idx_pairs)\n    evaled_ops_list = []\n    for (name, example_idxs) in all_name_example_idx_pairs:\n        data_dict = datasets[name]\n        data_extxd = data_dict[kind_data]\n        if hps.output_dist == 'poisson' and hps.temporal_spike_jitter_width > 0:\n            data_extxd = self.shuffle_spikes_in_time(data_extxd)\n        ext_input_extxi = data_dict[kind_ext_input]\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, example_idxs=example_idxs)\n        feed_dict = self.build_feed_dict(name, data_bxtxd, ext_input_bxtxi, keep_prob=keep_prob)\n        evaled_ops_np = session.run(ops_to_eval, feed_dict=feed_dict)\n        if do_collect:\n            evaled_ops_list.append(evaled_ops_np)\n    return evaled_ops_list",
            "def run_epoch(self, datasets, ops_to_eval, kind='train', batch_size=None, do_collect=True, keep_prob=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Run the model through the entire dataset once.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      ops_to_eval: A list of tensorflow operations that will be evaluated in\\n        the tf.session.run() call.\\n      batch_size (optional):  The batch_size to use\\n      do_collect (optional): Should the routine collect all session.run\\n        output as a list, and return it?\\n      keep_prob (optional): The dropout keep probability.\\n\\n    Returns:\\n      A list of lists, the internal list is the return for the ops for each\\n      session.run() call.  The outer list collects over the epoch.\\n    '\n    hps = self.hps\n    all_name_example_idx_pairs = self.shuffle_and_flatten_datasets(datasets, kind)\n    kind_data = kind + '_data'\n    kind_ext_input = kind + '_ext_input'\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    session = tf.get_default_session()\n    epoch_size = len(all_name_example_idx_pairs)\n    evaled_ops_list = []\n    for (name, example_idxs) in all_name_example_idx_pairs:\n        data_dict = datasets[name]\n        data_extxd = data_dict[kind_data]\n        if hps.output_dist == 'poisson' and hps.temporal_spike_jitter_width > 0:\n            data_extxd = self.shuffle_spikes_in_time(data_extxd)\n        ext_input_extxi = data_dict[kind_ext_input]\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, example_idxs=example_idxs)\n        feed_dict = self.build_feed_dict(name, data_bxtxd, ext_input_bxtxi, keep_prob=keep_prob)\n        evaled_ops_np = session.run(ops_to_eval, feed_dict=feed_dict)\n        if do_collect:\n            evaled_ops_list.append(evaled_ops_np)\n    return evaled_ops_list",
            "def run_epoch(self, datasets, ops_to_eval, kind='train', batch_size=None, do_collect=True, keep_prob=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Run the model through the entire dataset once.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n      ops_to_eval: A list of tensorflow operations that will be evaluated in\\n        the tf.session.run() call.\\n      batch_size (optional):  The batch_size to use\\n      do_collect (optional): Should the routine collect all session.run\\n        output as a list, and return it?\\n      keep_prob (optional): The dropout keep probability.\\n\\n    Returns:\\n      A list of lists, the internal list is the return for the ops for each\\n      session.run() call.  The outer list collects over the epoch.\\n    '\n    hps = self.hps\n    all_name_example_idx_pairs = self.shuffle_and_flatten_datasets(datasets, kind)\n    kind_data = kind + '_data'\n    kind_ext_input = kind + '_ext_input'\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    session = tf.get_default_session()\n    epoch_size = len(all_name_example_idx_pairs)\n    evaled_ops_list = []\n    for (name, example_idxs) in all_name_example_idx_pairs:\n        data_dict = datasets[name]\n        data_extxd = data_dict[kind_data]\n        if hps.output_dist == 'poisson' and hps.temporal_spike_jitter_width > 0:\n            data_extxd = self.shuffle_spikes_in_time(data_extxd)\n        ext_input_extxi = data_dict[kind_ext_input]\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, example_idxs=example_idxs)\n        feed_dict = self.build_feed_dict(name, data_bxtxd, ext_input_bxtxi, keep_prob=keep_prob)\n        evaled_ops_np = session.run(ops_to_eval, feed_dict=feed_dict)\n        if do_collect:\n            evaled_ops_list.append(evaled_ops_np)\n    return evaled_ops_list"
        ]
    },
    {
        "func_name": "summarize_all",
        "original": "def summarize_all(self, datasets, summary_values):\n    \"\"\"Plot and summarize stuff in tensorboard.\n\n    Note that everything done in the current function is otherwise done on\n    a single, randomly selected dataset (except for summary_values, which are\n    passed in.)\n\n    Args:\n      datasets, the dictionary of datasets used in the study.\n      summary_values:  These summary values are created from the training loop,\n      and so summarize the entire set of datasets.\n    \"\"\"\n    hps = self.hps\n    tr_kl_cost = summary_values['tr_kl_cost']\n    tr_recon_cost = summary_values['tr_recon_cost']\n    tr_total_cost = summary_values['tr_total_cost']\n    kl_weight = summary_values['kl_weight']\n    l2_weight = summary_values['l2_weight']\n    l2_cost = summary_values['l2_cost']\n    has_any_valid_set = summary_values['has_any_valid_set']\n    i = summary_values['nepochs']\n    session = tf.get_default_session()\n    (train_summ, train_step) = session.run([self.merged_train, self.train_step], feed_dict={self.l2_cost_ph: l2_cost, self.kl_cost_ph: tr_kl_cost, self.recon_cost_ph: tr_recon_cost, self.total_cost_ph: tr_total_cost})\n    self.writer.add_summary(train_summ, train_step)\n    if has_any_valid_set:\n        ev_kl_cost = summary_values['ev_kl_cost']\n        ev_recon_cost = summary_values['ev_recon_cost']\n        ev_total_cost = summary_values['ev_total_cost']\n        eval_summ = session.run(self.merged_valid, feed_dict={self.kl_cost_ph: ev_kl_cost, self.recon_cost_ph: ev_recon_cost, self.total_cost_ph: ev_total_cost})\n        self.writer.add_summary(eval_summ, train_step)\n        print('Epoch:%d, step:%d (TRAIN, VALID): total: %.2f, %.2f      recon: %.2f, %.2f,     kl: %.2f, %.2f,     l2: %.5f,      kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f,%.2f,       recon,%.2f,%.2f, kl,%.2f,%.2f, l2,%.5f,       klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight)\n    else:\n        print('Epoch:%d, step:%d TRAIN: total: %.2f     recon: %.2f, kl: %.2f,      l2: %.5f,    kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f, recon,%.2f, kl,%.2f,       l2,%.5f, klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight)\n    if self.hps.csv_log:\n        csv_file = os.path.join(self.hps.lfads_save_dir, self.hps.csv_log + '.csv')\n        with open(csv_file, 'a') as myfile:\n            myfile.write(csv_outstr)",
        "mutated": [
            "def summarize_all(self, datasets, summary_values):\n    if False:\n        i = 10\n    'Plot and summarize stuff in tensorboard.\\n\\n    Note that everything done in the current function is otherwise done on\\n    a single, randomly selected dataset (except for summary_values, which are\\n    passed in.)\\n\\n    Args:\\n      datasets, the dictionary of datasets used in the study.\\n      summary_values:  These summary values are created from the training loop,\\n      and so summarize the entire set of datasets.\\n    '\n    hps = self.hps\n    tr_kl_cost = summary_values['tr_kl_cost']\n    tr_recon_cost = summary_values['tr_recon_cost']\n    tr_total_cost = summary_values['tr_total_cost']\n    kl_weight = summary_values['kl_weight']\n    l2_weight = summary_values['l2_weight']\n    l2_cost = summary_values['l2_cost']\n    has_any_valid_set = summary_values['has_any_valid_set']\n    i = summary_values['nepochs']\n    session = tf.get_default_session()\n    (train_summ, train_step) = session.run([self.merged_train, self.train_step], feed_dict={self.l2_cost_ph: l2_cost, self.kl_cost_ph: tr_kl_cost, self.recon_cost_ph: tr_recon_cost, self.total_cost_ph: tr_total_cost})\n    self.writer.add_summary(train_summ, train_step)\n    if has_any_valid_set:\n        ev_kl_cost = summary_values['ev_kl_cost']\n        ev_recon_cost = summary_values['ev_recon_cost']\n        ev_total_cost = summary_values['ev_total_cost']\n        eval_summ = session.run(self.merged_valid, feed_dict={self.kl_cost_ph: ev_kl_cost, self.recon_cost_ph: ev_recon_cost, self.total_cost_ph: ev_total_cost})\n        self.writer.add_summary(eval_summ, train_step)\n        print('Epoch:%d, step:%d (TRAIN, VALID): total: %.2f, %.2f      recon: %.2f, %.2f,     kl: %.2f, %.2f,     l2: %.5f,      kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f,%.2f,       recon,%.2f,%.2f, kl,%.2f,%.2f, l2,%.5f,       klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight)\n    else:\n        print('Epoch:%d, step:%d TRAIN: total: %.2f     recon: %.2f, kl: %.2f,      l2: %.5f,    kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f, recon,%.2f, kl,%.2f,       l2,%.5f, klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight)\n    if self.hps.csv_log:\n        csv_file = os.path.join(self.hps.lfads_save_dir, self.hps.csv_log + '.csv')\n        with open(csv_file, 'a') as myfile:\n            myfile.write(csv_outstr)",
            "def summarize_all(self, datasets, summary_values):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Plot and summarize stuff in tensorboard.\\n\\n    Note that everything done in the current function is otherwise done on\\n    a single, randomly selected dataset (except for summary_values, which are\\n    passed in.)\\n\\n    Args:\\n      datasets, the dictionary of datasets used in the study.\\n      summary_values:  These summary values are created from the training loop,\\n      and so summarize the entire set of datasets.\\n    '\n    hps = self.hps\n    tr_kl_cost = summary_values['tr_kl_cost']\n    tr_recon_cost = summary_values['tr_recon_cost']\n    tr_total_cost = summary_values['tr_total_cost']\n    kl_weight = summary_values['kl_weight']\n    l2_weight = summary_values['l2_weight']\n    l2_cost = summary_values['l2_cost']\n    has_any_valid_set = summary_values['has_any_valid_set']\n    i = summary_values['nepochs']\n    session = tf.get_default_session()\n    (train_summ, train_step) = session.run([self.merged_train, self.train_step], feed_dict={self.l2_cost_ph: l2_cost, self.kl_cost_ph: tr_kl_cost, self.recon_cost_ph: tr_recon_cost, self.total_cost_ph: tr_total_cost})\n    self.writer.add_summary(train_summ, train_step)\n    if has_any_valid_set:\n        ev_kl_cost = summary_values['ev_kl_cost']\n        ev_recon_cost = summary_values['ev_recon_cost']\n        ev_total_cost = summary_values['ev_total_cost']\n        eval_summ = session.run(self.merged_valid, feed_dict={self.kl_cost_ph: ev_kl_cost, self.recon_cost_ph: ev_recon_cost, self.total_cost_ph: ev_total_cost})\n        self.writer.add_summary(eval_summ, train_step)\n        print('Epoch:%d, step:%d (TRAIN, VALID): total: %.2f, %.2f      recon: %.2f, %.2f,     kl: %.2f, %.2f,     l2: %.5f,      kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f,%.2f,       recon,%.2f,%.2f, kl,%.2f,%.2f, l2,%.5f,       klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight)\n    else:\n        print('Epoch:%d, step:%d TRAIN: total: %.2f     recon: %.2f, kl: %.2f,      l2: %.5f,    kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f, recon,%.2f, kl,%.2f,       l2,%.5f, klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight)\n    if self.hps.csv_log:\n        csv_file = os.path.join(self.hps.lfads_save_dir, self.hps.csv_log + '.csv')\n        with open(csv_file, 'a') as myfile:\n            myfile.write(csv_outstr)",
            "def summarize_all(self, datasets, summary_values):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Plot and summarize stuff in tensorboard.\\n\\n    Note that everything done in the current function is otherwise done on\\n    a single, randomly selected dataset (except for summary_values, which are\\n    passed in.)\\n\\n    Args:\\n      datasets, the dictionary of datasets used in the study.\\n      summary_values:  These summary values are created from the training loop,\\n      and so summarize the entire set of datasets.\\n    '\n    hps = self.hps\n    tr_kl_cost = summary_values['tr_kl_cost']\n    tr_recon_cost = summary_values['tr_recon_cost']\n    tr_total_cost = summary_values['tr_total_cost']\n    kl_weight = summary_values['kl_weight']\n    l2_weight = summary_values['l2_weight']\n    l2_cost = summary_values['l2_cost']\n    has_any_valid_set = summary_values['has_any_valid_set']\n    i = summary_values['nepochs']\n    session = tf.get_default_session()\n    (train_summ, train_step) = session.run([self.merged_train, self.train_step], feed_dict={self.l2_cost_ph: l2_cost, self.kl_cost_ph: tr_kl_cost, self.recon_cost_ph: tr_recon_cost, self.total_cost_ph: tr_total_cost})\n    self.writer.add_summary(train_summ, train_step)\n    if has_any_valid_set:\n        ev_kl_cost = summary_values['ev_kl_cost']\n        ev_recon_cost = summary_values['ev_recon_cost']\n        ev_total_cost = summary_values['ev_total_cost']\n        eval_summ = session.run(self.merged_valid, feed_dict={self.kl_cost_ph: ev_kl_cost, self.recon_cost_ph: ev_recon_cost, self.total_cost_ph: ev_total_cost})\n        self.writer.add_summary(eval_summ, train_step)\n        print('Epoch:%d, step:%d (TRAIN, VALID): total: %.2f, %.2f      recon: %.2f, %.2f,     kl: %.2f, %.2f,     l2: %.5f,      kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f,%.2f,       recon,%.2f,%.2f, kl,%.2f,%.2f, l2,%.5f,       klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight)\n    else:\n        print('Epoch:%d, step:%d TRAIN: total: %.2f     recon: %.2f, kl: %.2f,      l2: %.5f,    kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f, recon,%.2f, kl,%.2f,       l2,%.5f, klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight)\n    if self.hps.csv_log:\n        csv_file = os.path.join(self.hps.lfads_save_dir, self.hps.csv_log + '.csv')\n        with open(csv_file, 'a') as myfile:\n            myfile.write(csv_outstr)",
            "def summarize_all(self, datasets, summary_values):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Plot and summarize stuff in tensorboard.\\n\\n    Note that everything done in the current function is otherwise done on\\n    a single, randomly selected dataset (except for summary_values, which are\\n    passed in.)\\n\\n    Args:\\n      datasets, the dictionary of datasets used in the study.\\n      summary_values:  These summary values are created from the training loop,\\n      and so summarize the entire set of datasets.\\n    '\n    hps = self.hps\n    tr_kl_cost = summary_values['tr_kl_cost']\n    tr_recon_cost = summary_values['tr_recon_cost']\n    tr_total_cost = summary_values['tr_total_cost']\n    kl_weight = summary_values['kl_weight']\n    l2_weight = summary_values['l2_weight']\n    l2_cost = summary_values['l2_cost']\n    has_any_valid_set = summary_values['has_any_valid_set']\n    i = summary_values['nepochs']\n    session = tf.get_default_session()\n    (train_summ, train_step) = session.run([self.merged_train, self.train_step], feed_dict={self.l2_cost_ph: l2_cost, self.kl_cost_ph: tr_kl_cost, self.recon_cost_ph: tr_recon_cost, self.total_cost_ph: tr_total_cost})\n    self.writer.add_summary(train_summ, train_step)\n    if has_any_valid_set:\n        ev_kl_cost = summary_values['ev_kl_cost']\n        ev_recon_cost = summary_values['ev_recon_cost']\n        ev_total_cost = summary_values['ev_total_cost']\n        eval_summ = session.run(self.merged_valid, feed_dict={self.kl_cost_ph: ev_kl_cost, self.recon_cost_ph: ev_recon_cost, self.total_cost_ph: ev_total_cost})\n        self.writer.add_summary(eval_summ, train_step)\n        print('Epoch:%d, step:%d (TRAIN, VALID): total: %.2f, %.2f      recon: %.2f, %.2f,     kl: %.2f, %.2f,     l2: %.5f,      kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f,%.2f,       recon,%.2f,%.2f, kl,%.2f,%.2f, l2,%.5f,       klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight)\n    else:\n        print('Epoch:%d, step:%d TRAIN: total: %.2f     recon: %.2f, kl: %.2f,      l2: %.5f,    kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f, recon,%.2f, kl,%.2f,       l2,%.5f, klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight)\n    if self.hps.csv_log:\n        csv_file = os.path.join(self.hps.lfads_save_dir, self.hps.csv_log + '.csv')\n        with open(csv_file, 'a') as myfile:\n            myfile.write(csv_outstr)",
            "def summarize_all(self, datasets, summary_values):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Plot and summarize stuff in tensorboard.\\n\\n    Note that everything done in the current function is otherwise done on\\n    a single, randomly selected dataset (except for summary_values, which are\\n    passed in.)\\n\\n    Args:\\n      datasets, the dictionary of datasets used in the study.\\n      summary_values:  These summary values are created from the training loop,\\n      and so summarize the entire set of datasets.\\n    '\n    hps = self.hps\n    tr_kl_cost = summary_values['tr_kl_cost']\n    tr_recon_cost = summary_values['tr_recon_cost']\n    tr_total_cost = summary_values['tr_total_cost']\n    kl_weight = summary_values['kl_weight']\n    l2_weight = summary_values['l2_weight']\n    l2_cost = summary_values['l2_cost']\n    has_any_valid_set = summary_values['has_any_valid_set']\n    i = summary_values['nepochs']\n    session = tf.get_default_session()\n    (train_summ, train_step) = session.run([self.merged_train, self.train_step], feed_dict={self.l2_cost_ph: l2_cost, self.kl_cost_ph: tr_kl_cost, self.recon_cost_ph: tr_recon_cost, self.total_cost_ph: tr_total_cost})\n    self.writer.add_summary(train_summ, train_step)\n    if has_any_valid_set:\n        ev_kl_cost = summary_values['ev_kl_cost']\n        ev_recon_cost = summary_values['ev_recon_cost']\n        ev_total_cost = summary_values['ev_total_cost']\n        eval_summ = session.run(self.merged_valid, feed_dict={self.kl_cost_ph: ev_kl_cost, self.recon_cost_ph: ev_recon_cost, self.total_cost_ph: ev_total_cost})\n        self.writer.add_summary(eval_summ, train_step)\n        print('Epoch:%d, step:%d (TRAIN, VALID): total: %.2f, %.2f      recon: %.2f, %.2f,     kl: %.2f, %.2f,     l2: %.5f,      kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f,%.2f,       recon,%.2f,%.2f, kl,%.2f,%.2f, l2,%.5f,       klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, ev_total_cost, tr_recon_cost, ev_recon_cost, tr_kl_cost, ev_kl_cost, l2_cost, kl_weight, l2_weight)\n    else:\n        print('Epoch:%d, step:%d TRAIN: total: %.2f     recon: %.2f, kl: %.2f,      l2: %.5f,    kl weight: %.2f, l2 weight: %.2f' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight))\n        csv_outstr = 'epoch,%d, step,%d, total,%.2f, recon,%.2f, kl,%.2f,       l2,%.5f, klweight,%.2f, l2weight,%.2f\\n' % (i, train_step, tr_total_cost, tr_recon_cost, tr_kl_cost, l2_cost, kl_weight, l2_weight)\n    if self.hps.csv_log:\n        csv_file = os.path.join(self.hps.lfads_save_dir, self.hps.csv_log + '.csv')\n        with open(csv_file, 'a') as myfile:\n            myfile.write(csv_outstr)"
        ]
    },
    {
        "func_name": "plot_single_example",
        "original": "def plot_single_example(self, datasets):\n    \"\"\"Plot an image relating to a randomly chosen, specific example.  We use\n    posterior sample and average by taking one example, and filling a whole\n    batch with that example, sample from the posterior, and then average the\n    quantities.\n\n    \"\"\"\n    hps = self.hps\n    all_data_names = datasets.keys()\n    data_name = np.random.permutation(all_data_names)[0]\n    data_dict = datasets[data_name]\n    has_valid_set = True if data_dict['valid_data'] is not None else False\n    cf = 1.0\n    (E, _, _) = data_dict['train_data'].shape\n    eidx = np.random.choice(E)\n    example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n    (train_data_bxtxd, train_ext_input_bxtxi) = self.get_batch(data_dict['train_data'], data_dict['train_ext_input'], example_idxs=example_idxs)\n    truth_train_data_bxtxd = None\n    if 'train_truth' in data_dict and data_dict['train_truth'] is not None:\n        (truth_train_data_bxtxd, _) = self.get_batch(data_dict['train_truth'], example_idxs=example_idxs)\n        cf = data_dict['conversion_factor']\n    train_model_values = self.eval_model_runs_batch(data_name, train_data_bxtxd, train_ext_input_bxtxi, do_average_batch=False)\n    train_step = train_model_values['train_steps']\n    feed_dict = self.build_feed_dict(data_name, train_data_bxtxd, train_ext_input_bxtxi, keep_prob=1.0)\n    session = tf.get_default_session()\n    generic_summ = session.run(self.merged_generic, feed_dict=feed_dict)\n    self.writer.add_summary(generic_summ, train_step)\n    valid_data_bxtxd = valid_model_values = valid_ext_input_bxtxi = None\n    truth_valid_data_bxtxd = None\n    if has_valid_set:\n        (E, _, _) = data_dict['valid_data'].shape\n        eidx = np.random.choice(E)\n        example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n        (valid_data_bxtxd, valid_ext_input_bxtxi) = self.get_batch(data_dict['valid_data'], data_dict['valid_ext_input'], example_idxs=example_idxs)\n        if 'valid_truth' in data_dict and data_dict['valid_truth'] is not None:\n            (truth_valid_data_bxtxd, _) = self.get_batch(data_dict['valid_truth'], example_idxs=example_idxs)\n        else:\n            truth_valid_data_bxtxd = None\n        valid_model_values = self.eval_model_runs_batch(data_name, valid_data_bxtxd, valid_ext_input_bxtxi, do_average_batch=False)\n    example_image = plot_lfads(train_bxtxd=train_data_bxtxd, train_model_vals=train_model_values, train_ext_input_bxtxi=train_ext_input_bxtxi, train_truth_bxtxd=truth_train_data_bxtxd, valid_bxtxd=valid_data_bxtxd, valid_model_vals=valid_model_values, valid_ext_input_bxtxi=valid_ext_input_bxtxi, valid_truth_bxtxd=truth_valid_data_bxtxd, bidx=None, cf=cf, output_dist=hps.output_dist)\n    example_image = np.expand_dims(example_image, axis=0)\n    example_summ = session.run(self.merged_examples, feed_dict={self.example_image: example_image})\n    self.writer.add_summary(example_summ)",
        "mutated": [
            "def plot_single_example(self, datasets):\n    if False:\n        i = 10\n    'Plot an image relating to a randomly chosen, specific example.  We use\\n    posterior sample and average by taking one example, and filling a whole\\n    batch with that example, sample from the posterior, and then average the\\n    quantities.\\n\\n    '\n    hps = self.hps\n    all_data_names = datasets.keys()\n    data_name = np.random.permutation(all_data_names)[0]\n    data_dict = datasets[data_name]\n    has_valid_set = True if data_dict['valid_data'] is not None else False\n    cf = 1.0\n    (E, _, _) = data_dict['train_data'].shape\n    eidx = np.random.choice(E)\n    example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n    (train_data_bxtxd, train_ext_input_bxtxi) = self.get_batch(data_dict['train_data'], data_dict['train_ext_input'], example_idxs=example_idxs)\n    truth_train_data_bxtxd = None\n    if 'train_truth' in data_dict and data_dict['train_truth'] is not None:\n        (truth_train_data_bxtxd, _) = self.get_batch(data_dict['train_truth'], example_idxs=example_idxs)\n        cf = data_dict['conversion_factor']\n    train_model_values = self.eval_model_runs_batch(data_name, train_data_bxtxd, train_ext_input_bxtxi, do_average_batch=False)\n    train_step = train_model_values['train_steps']\n    feed_dict = self.build_feed_dict(data_name, train_data_bxtxd, train_ext_input_bxtxi, keep_prob=1.0)\n    session = tf.get_default_session()\n    generic_summ = session.run(self.merged_generic, feed_dict=feed_dict)\n    self.writer.add_summary(generic_summ, train_step)\n    valid_data_bxtxd = valid_model_values = valid_ext_input_bxtxi = None\n    truth_valid_data_bxtxd = None\n    if has_valid_set:\n        (E, _, _) = data_dict['valid_data'].shape\n        eidx = np.random.choice(E)\n        example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n        (valid_data_bxtxd, valid_ext_input_bxtxi) = self.get_batch(data_dict['valid_data'], data_dict['valid_ext_input'], example_idxs=example_idxs)\n        if 'valid_truth' in data_dict and data_dict['valid_truth'] is not None:\n            (truth_valid_data_bxtxd, _) = self.get_batch(data_dict['valid_truth'], example_idxs=example_idxs)\n        else:\n            truth_valid_data_bxtxd = None\n        valid_model_values = self.eval_model_runs_batch(data_name, valid_data_bxtxd, valid_ext_input_bxtxi, do_average_batch=False)\n    example_image = plot_lfads(train_bxtxd=train_data_bxtxd, train_model_vals=train_model_values, train_ext_input_bxtxi=train_ext_input_bxtxi, train_truth_bxtxd=truth_train_data_bxtxd, valid_bxtxd=valid_data_bxtxd, valid_model_vals=valid_model_values, valid_ext_input_bxtxi=valid_ext_input_bxtxi, valid_truth_bxtxd=truth_valid_data_bxtxd, bidx=None, cf=cf, output_dist=hps.output_dist)\n    example_image = np.expand_dims(example_image, axis=0)\n    example_summ = session.run(self.merged_examples, feed_dict={self.example_image: example_image})\n    self.writer.add_summary(example_summ)",
            "def plot_single_example(self, datasets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Plot an image relating to a randomly chosen, specific example.  We use\\n    posterior sample and average by taking one example, and filling a whole\\n    batch with that example, sample from the posterior, and then average the\\n    quantities.\\n\\n    '\n    hps = self.hps\n    all_data_names = datasets.keys()\n    data_name = np.random.permutation(all_data_names)[0]\n    data_dict = datasets[data_name]\n    has_valid_set = True if data_dict['valid_data'] is not None else False\n    cf = 1.0\n    (E, _, _) = data_dict['train_data'].shape\n    eidx = np.random.choice(E)\n    example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n    (train_data_bxtxd, train_ext_input_bxtxi) = self.get_batch(data_dict['train_data'], data_dict['train_ext_input'], example_idxs=example_idxs)\n    truth_train_data_bxtxd = None\n    if 'train_truth' in data_dict and data_dict['train_truth'] is not None:\n        (truth_train_data_bxtxd, _) = self.get_batch(data_dict['train_truth'], example_idxs=example_idxs)\n        cf = data_dict['conversion_factor']\n    train_model_values = self.eval_model_runs_batch(data_name, train_data_bxtxd, train_ext_input_bxtxi, do_average_batch=False)\n    train_step = train_model_values['train_steps']\n    feed_dict = self.build_feed_dict(data_name, train_data_bxtxd, train_ext_input_bxtxi, keep_prob=1.0)\n    session = tf.get_default_session()\n    generic_summ = session.run(self.merged_generic, feed_dict=feed_dict)\n    self.writer.add_summary(generic_summ, train_step)\n    valid_data_bxtxd = valid_model_values = valid_ext_input_bxtxi = None\n    truth_valid_data_bxtxd = None\n    if has_valid_set:\n        (E, _, _) = data_dict['valid_data'].shape\n        eidx = np.random.choice(E)\n        example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n        (valid_data_bxtxd, valid_ext_input_bxtxi) = self.get_batch(data_dict['valid_data'], data_dict['valid_ext_input'], example_idxs=example_idxs)\n        if 'valid_truth' in data_dict and data_dict['valid_truth'] is not None:\n            (truth_valid_data_bxtxd, _) = self.get_batch(data_dict['valid_truth'], example_idxs=example_idxs)\n        else:\n            truth_valid_data_bxtxd = None\n        valid_model_values = self.eval_model_runs_batch(data_name, valid_data_bxtxd, valid_ext_input_bxtxi, do_average_batch=False)\n    example_image = plot_lfads(train_bxtxd=train_data_bxtxd, train_model_vals=train_model_values, train_ext_input_bxtxi=train_ext_input_bxtxi, train_truth_bxtxd=truth_train_data_bxtxd, valid_bxtxd=valid_data_bxtxd, valid_model_vals=valid_model_values, valid_ext_input_bxtxi=valid_ext_input_bxtxi, valid_truth_bxtxd=truth_valid_data_bxtxd, bidx=None, cf=cf, output_dist=hps.output_dist)\n    example_image = np.expand_dims(example_image, axis=0)\n    example_summ = session.run(self.merged_examples, feed_dict={self.example_image: example_image})\n    self.writer.add_summary(example_summ)",
            "def plot_single_example(self, datasets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Plot an image relating to a randomly chosen, specific example.  We use\\n    posterior sample and average by taking one example, and filling a whole\\n    batch with that example, sample from the posterior, and then average the\\n    quantities.\\n\\n    '\n    hps = self.hps\n    all_data_names = datasets.keys()\n    data_name = np.random.permutation(all_data_names)[0]\n    data_dict = datasets[data_name]\n    has_valid_set = True if data_dict['valid_data'] is not None else False\n    cf = 1.0\n    (E, _, _) = data_dict['train_data'].shape\n    eidx = np.random.choice(E)\n    example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n    (train_data_bxtxd, train_ext_input_bxtxi) = self.get_batch(data_dict['train_data'], data_dict['train_ext_input'], example_idxs=example_idxs)\n    truth_train_data_bxtxd = None\n    if 'train_truth' in data_dict and data_dict['train_truth'] is not None:\n        (truth_train_data_bxtxd, _) = self.get_batch(data_dict['train_truth'], example_idxs=example_idxs)\n        cf = data_dict['conversion_factor']\n    train_model_values = self.eval_model_runs_batch(data_name, train_data_bxtxd, train_ext_input_bxtxi, do_average_batch=False)\n    train_step = train_model_values['train_steps']\n    feed_dict = self.build_feed_dict(data_name, train_data_bxtxd, train_ext_input_bxtxi, keep_prob=1.0)\n    session = tf.get_default_session()\n    generic_summ = session.run(self.merged_generic, feed_dict=feed_dict)\n    self.writer.add_summary(generic_summ, train_step)\n    valid_data_bxtxd = valid_model_values = valid_ext_input_bxtxi = None\n    truth_valid_data_bxtxd = None\n    if has_valid_set:\n        (E, _, _) = data_dict['valid_data'].shape\n        eidx = np.random.choice(E)\n        example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n        (valid_data_bxtxd, valid_ext_input_bxtxi) = self.get_batch(data_dict['valid_data'], data_dict['valid_ext_input'], example_idxs=example_idxs)\n        if 'valid_truth' in data_dict and data_dict['valid_truth'] is not None:\n            (truth_valid_data_bxtxd, _) = self.get_batch(data_dict['valid_truth'], example_idxs=example_idxs)\n        else:\n            truth_valid_data_bxtxd = None\n        valid_model_values = self.eval_model_runs_batch(data_name, valid_data_bxtxd, valid_ext_input_bxtxi, do_average_batch=False)\n    example_image = plot_lfads(train_bxtxd=train_data_bxtxd, train_model_vals=train_model_values, train_ext_input_bxtxi=train_ext_input_bxtxi, train_truth_bxtxd=truth_train_data_bxtxd, valid_bxtxd=valid_data_bxtxd, valid_model_vals=valid_model_values, valid_ext_input_bxtxi=valid_ext_input_bxtxi, valid_truth_bxtxd=truth_valid_data_bxtxd, bidx=None, cf=cf, output_dist=hps.output_dist)\n    example_image = np.expand_dims(example_image, axis=0)\n    example_summ = session.run(self.merged_examples, feed_dict={self.example_image: example_image})\n    self.writer.add_summary(example_summ)",
            "def plot_single_example(self, datasets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Plot an image relating to a randomly chosen, specific example.  We use\\n    posterior sample and average by taking one example, and filling a whole\\n    batch with that example, sample from the posterior, and then average the\\n    quantities.\\n\\n    '\n    hps = self.hps\n    all_data_names = datasets.keys()\n    data_name = np.random.permutation(all_data_names)[0]\n    data_dict = datasets[data_name]\n    has_valid_set = True if data_dict['valid_data'] is not None else False\n    cf = 1.0\n    (E, _, _) = data_dict['train_data'].shape\n    eidx = np.random.choice(E)\n    example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n    (train_data_bxtxd, train_ext_input_bxtxi) = self.get_batch(data_dict['train_data'], data_dict['train_ext_input'], example_idxs=example_idxs)\n    truth_train_data_bxtxd = None\n    if 'train_truth' in data_dict and data_dict['train_truth'] is not None:\n        (truth_train_data_bxtxd, _) = self.get_batch(data_dict['train_truth'], example_idxs=example_idxs)\n        cf = data_dict['conversion_factor']\n    train_model_values = self.eval_model_runs_batch(data_name, train_data_bxtxd, train_ext_input_bxtxi, do_average_batch=False)\n    train_step = train_model_values['train_steps']\n    feed_dict = self.build_feed_dict(data_name, train_data_bxtxd, train_ext_input_bxtxi, keep_prob=1.0)\n    session = tf.get_default_session()\n    generic_summ = session.run(self.merged_generic, feed_dict=feed_dict)\n    self.writer.add_summary(generic_summ, train_step)\n    valid_data_bxtxd = valid_model_values = valid_ext_input_bxtxi = None\n    truth_valid_data_bxtxd = None\n    if has_valid_set:\n        (E, _, _) = data_dict['valid_data'].shape\n        eidx = np.random.choice(E)\n        example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n        (valid_data_bxtxd, valid_ext_input_bxtxi) = self.get_batch(data_dict['valid_data'], data_dict['valid_ext_input'], example_idxs=example_idxs)\n        if 'valid_truth' in data_dict and data_dict['valid_truth'] is not None:\n            (truth_valid_data_bxtxd, _) = self.get_batch(data_dict['valid_truth'], example_idxs=example_idxs)\n        else:\n            truth_valid_data_bxtxd = None\n        valid_model_values = self.eval_model_runs_batch(data_name, valid_data_bxtxd, valid_ext_input_bxtxi, do_average_batch=False)\n    example_image = plot_lfads(train_bxtxd=train_data_bxtxd, train_model_vals=train_model_values, train_ext_input_bxtxi=train_ext_input_bxtxi, train_truth_bxtxd=truth_train_data_bxtxd, valid_bxtxd=valid_data_bxtxd, valid_model_vals=valid_model_values, valid_ext_input_bxtxi=valid_ext_input_bxtxi, valid_truth_bxtxd=truth_valid_data_bxtxd, bidx=None, cf=cf, output_dist=hps.output_dist)\n    example_image = np.expand_dims(example_image, axis=0)\n    example_summ = session.run(self.merged_examples, feed_dict={self.example_image: example_image})\n    self.writer.add_summary(example_summ)",
            "def plot_single_example(self, datasets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Plot an image relating to a randomly chosen, specific example.  We use\\n    posterior sample and average by taking one example, and filling a whole\\n    batch with that example, sample from the posterior, and then average the\\n    quantities.\\n\\n    '\n    hps = self.hps\n    all_data_names = datasets.keys()\n    data_name = np.random.permutation(all_data_names)[0]\n    data_dict = datasets[data_name]\n    has_valid_set = True if data_dict['valid_data'] is not None else False\n    cf = 1.0\n    (E, _, _) = data_dict['train_data'].shape\n    eidx = np.random.choice(E)\n    example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n    (train_data_bxtxd, train_ext_input_bxtxi) = self.get_batch(data_dict['train_data'], data_dict['train_ext_input'], example_idxs=example_idxs)\n    truth_train_data_bxtxd = None\n    if 'train_truth' in data_dict and data_dict['train_truth'] is not None:\n        (truth_train_data_bxtxd, _) = self.get_batch(data_dict['train_truth'], example_idxs=example_idxs)\n        cf = data_dict['conversion_factor']\n    train_model_values = self.eval_model_runs_batch(data_name, train_data_bxtxd, train_ext_input_bxtxi, do_average_batch=False)\n    train_step = train_model_values['train_steps']\n    feed_dict = self.build_feed_dict(data_name, train_data_bxtxd, train_ext_input_bxtxi, keep_prob=1.0)\n    session = tf.get_default_session()\n    generic_summ = session.run(self.merged_generic, feed_dict=feed_dict)\n    self.writer.add_summary(generic_summ, train_step)\n    valid_data_bxtxd = valid_model_values = valid_ext_input_bxtxi = None\n    truth_valid_data_bxtxd = None\n    if has_valid_set:\n        (E, _, _) = data_dict['valid_data'].shape\n        eidx = np.random.choice(E)\n        example_idxs = eidx * np.ones(hps.batch_size, dtype=np.int32)\n        (valid_data_bxtxd, valid_ext_input_bxtxi) = self.get_batch(data_dict['valid_data'], data_dict['valid_ext_input'], example_idxs=example_idxs)\n        if 'valid_truth' in data_dict and data_dict['valid_truth'] is not None:\n            (truth_valid_data_bxtxd, _) = self.get_batch(data_dict['valid_truth'], example_idxs=example_idxs)\n        else:\n            truth_valid_data_bxtxd = None\n        valid_model_values = self.eval_model_runs_batch(data_name, valid_data_bxtxd, valid_ext_input_bxtxi, do_average_batch=False)\n    example_image = plot_lfads(train_bxtxd=train_data_bxtxd, train_model_vals=train_model_values, train_ext_input_bxtxi=train_ext_input_bxtxi, train_truth_bxtxd=truth_train_data_bxtxd, valid_bxtxd=valid_data_bxtxd, valid_model_vals=valid_model_values, valid_ext_input_bxtxi=valid_ext_input_bxtxi, valid_truth_bxtxd=truth_valid_data_bxtxd, bidx=None, cf=cf, output_dist=hps.output_dist)\n    example_image = np.expand_dims(example_image, axis=0)\n    example_summ = session.run(self.merged_examples, feed_dict={self.example_image: example_image})\n    self.writer.add_summary(example_summ)"
        ]
    },
    {
        "func_name": "train_model",
        "original": "def train_model(self, datasets):\n    \"\"\"Train the model, print per-epoch information, and save checkpoints.\n\n    Loop over training epochs. The function that actually does the\n    training is train_epoch.  This function iterates over the training\n    data, one epoch at a time.  The learning rate schedule is such\n    that it will stay the same until the cost goes up in comparison to\n    the last few values, then it will drop.\n\n    Args:\n      datasets: A dict of data dicts.  The dataset dict is simply a\n        name(string)-> data dictionary mapping (See top of lfads.py).\n    \"\"\"\n    hps = self.hps\n    has_any_valid_set = False\n    for data_dict in datasets.values():\n        if data_dict['valid_data'] is not None:\n            has_any_valid_set = True\n            break\n    session = tf.get_default_session()\n    lr = session.run(self.learning_rate)\n    lr_stop = hps.learning_rate_stop\n    i = -1\n    train_costs = []\n    valid_costs = []\n    ev_total_cost = ev_recon_cost = ev_kl_cost = 0.0\n    lowest_ev_cost = np.Inf\n    while True:\n        i += 1\n        do_save_ckpt = True if i % 10 == 0 else False\n        (tr_total_cost, tr_recon_cost, tr_kl_cost, kl_weight, l2_cost, l2_weight) = self.train_epoch(datasets, do_save_ckpt=do_save_ckpt)\n        if has_any_valid_set:\n            (ev_total_cost, ev_recon_cost, ev_kl_cost) = self.eval_cost_epoch(datasets, kind='valid')\n            valid_costs.append(ev_total_cost)\n            n_lve = 1\n            run_avg_lve = np.mean(valid_costs[-n_lve:])\n            if kl_weight >= 1.0 and (l2_weight >= 1.0 or (self.hps.l2_gen_scale == 0.0 and self.hps.l2_con_scale == 0.0)) and (len(valid_costs) > n_lve and run_avg_lve < lowest_ev_cost):\n                lowest_ev_cost = run_avg_lve\n                checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '_lve.ckpt')\n                self.lve_saver.save(session, checkpoint_path, global_step=self.train_step, latest_filename='checkpoint_lve')\n        values = {'nepochs': i, 'has_any_valid_set': has_any_valid_set, 'tr_total_cost': tr_total_cost, 'ev_total_cost': ev_total_cost, 'tr_recon_cost': tr_recon_cost, 'ev_recon_cost': ev_recon_cost, 'tr_kl_cost': tr_kl_cost, 'ev_kl_cost': ev_kl_cost, 'l2_weight': l2_weight, 'kl_weight': kl_weight, 'l2_cost': l2_cost}\n        self.summarize_all(datasets, values)\n        self.plot_single_example(datasets)\n        train_res = tr_total_cost\n        n_lr = hps.learning_rate_n_to_compare\n        if len(train_costs) > n_lr and train_res > np.max(train_costs[-n_lr:]):\n            _ = session.run(self.learning_rate_decay_op)\n            lr = session.run(self.learning_rate)\n            print('     Decreasing learning rate to %f.' % lr)\n            train_costs.append(np.inf)\n        else:\n            train_costs.append(train_res)\n        if lr < lr_stop:\n            print('Stopping optimization based on learning rate criteria.')\n            break",
        "mutated": [
            "def train_model(self, datasets):\n    if False:\n        i = 10\n    'Train the model, print per-epoch information, and save checkpoints.\\n\\n    Loop over training epochs. The function that actually does the\\n    training is train_epoch.  This function iterates over the training\\n    data, one epoch at a time.  The learning rate schedule is such\\n    that it will stay the same until the cost goes up in comparison to\\n    the last few values, then it will drop.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n    '\n    hps = self.hps\n    has_any_valid_set = False\n    for data_dict in datasets.values():\n        if data_dict['valid_data'] is not None:\n            has_any_valid_set = True\n            break\n    session = tf.get_default_session()\n    lr = session.run(self.learning_rate)\n    lr_stop = hps.learning_rate_stop\n    i = -1\n    train_costs = []\n    valid_costs = []\n    ev_total_cost = ev_recon_cost = ev_kl_cost = 0.0\n    lowest_ev_cost = np.Inf\n    while True:\n        i += 1\n        do_save_ckpt = True if i % 10 == 0 else False\n        (tr_total_cost, tr_recon_cost, tr_kl_cost, kl_weight, l2_cost, l2_weight) = self.train_epoch(datasets, do_save_ckpt=do_save_ckpt)\n        if has_any_valid_set:\n            (ev_total_cost, ev_recon_cost, ev_kl_cost) = self.eval_cost_epoch(datasets, kind='valid')\n            valid_costs.append(ev_total_cost)\n            n_lve = 1\n            run_avg_lve = np.mean(valid_costs[-n_lve:])\n            if kl_weight >= 1.0 and (l2_weight >= 1.0 or (self.hps.l2_gen_scale == 0.0 and self.hps.l2_con_scale == 0.0)) and (len(valid_costs) > n_lve and run_avg_lve < lowest_ev_cost):\n                lowest_ev_cost = run_avg_lve\n                checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '_lve.ckpt')\n                self.lve_saver.save(session, checkpoint_path, global_step=self.train_step, latest_filename='checkpoint_lve')\n        values = {'nepochs': i, 'has_any_valid_set': has_any_valid_set, 'tr_total_cost': tr_total_cost, 'ev_total_cost': ev_total_cost, 'tr_recon_cost': tr_recon_cost, 'ev_recon_cost': ev_recon_cost, 'tr_kl_cost': tr_kl_cost, 'ev_kl_cost': ev_kl_cost, 'l2_weight': l2_weight, 'kl_weight': kl_weight, 'l2_cost': l2_cost}\n        self.summarize_all(datasets, values)\n        self.plot_single_example(datasets)\n        train_res = tr_total_cost\n        n_lr = hps.learning_rate_n_to_compare\n        if len(train_costs) > n_lr and train_res > np.max(train_costs[-n_lr:]):\n            _ = session.run(self.learning_rate_decay_op)\n            lr = session.run(self.learning_rate)\n            print('     Decreasing learning rate to %f.' % lr)\n            train_costs.append(np.inf)\n        else:\n            train_costs.append(train_res)\n        if lr < lr_stop:\n            print('Stopping optimization based on learning rate criteria.')\n            break",
            "def train_model(self, datasets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Train the model, print per-epoch information, and save checkpoints.\\n\\n    Loop over training epochs. The function that actually does the\\n    training is train_epoch.  This function iterates over the training\\n    data, one epoch at a time.  The learning rate schedule is such\\n    that it will stay the same until the cost goes up in comparison to\\n    the last few values, then it will drop.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n    '\n    hps = self.hps\n    has_any_valid_set = False\n    for data_dict in datasets.values():\n        if data_dict['valid_data'] is not None:\n            has_any_valid_set = True\n            break\n    session = tf.get_default_session()\n    lr = session.run(self.learning_rate)\n    lr_stop = hps.learning_rate_stop\n    i = -1\n    train_costs = []\n    valid_costs = []\n    ev_total_cost = ev_recon_cost = ev_kl_cost = 0.0\n    lowest_ev_cost = np.Inf\n    while True:\n        i += 1\n        do_save_ckpt = True if i % 10 == 0 else False\n        (tr_total_cost, tr_recon_cost, tr_kl_cost, kl_weight, l2_cost, l2_weight) = self.train_epoch(datasets, do_save_ckpt=do_save_ckpt)\n        if has_any_valid_set:\n            (ev_total_cost, ev_recon_cost, ev_kl_cost) = self.eval_cost_epoch(datasets, kind='valid')\n            valid_costs.append(ev_total_cost)\n            n_lve = 1\n            run_avg_lve = np.mean(valid_costs[-n_lve:])\n            if kl_weight >= 1.0 and (l2_weight >= 1.0 or (self.hps.l2_gen_scale == 0.0 and self.hps.l2_con_scale == 0.0)) and (len(valid_costs) > n_lve and run_avg_lve < lowest_ev_cost):\n                lowest_ev_cost = run_avg_lve\n                checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '_lve.ckpt')\n                self.lve_saver.save(session, checkpoint_path, global_step=self.train_step, latest_filename='checkpoint_lve')\n        values = {'nepochs': i, 'has_any_valid_set': has_any_valid_set, 'tr_total_cost': tr_total_cost, 'ev_total_cost': ev_total_cost, 'tr_recon_cost': tr_recon_cost, 'ev_recon_cost': ev_recon_cost, 'tr_kl_cost': tr_kl_cost, 'ev_kl_cost': ev_kl_cost, 'l2_weight': l2_weight, 'kl_weight': kl_weight, 'l2_cost': l2_cost}\n        self.summarize_all(datasets, values)\n        self.plot_single_example(datasets)\n        train_res = tr_total_cost\n        n_lr = hps.learning_rate_n_to_compare\n        if len(train_costs) > n_lr and train_res > np.max(train_costs[-n_lr:]):\n            _ = session.run(self.learning_rate_decay_op)\n            lr = session.run(self.learning_rate)\n            print('     Decreasing learning rate to %f.' % lr)\n            train_costs.append(np.inf)\n        else:\n            train_costs.append(train_res)\n        if lr < lr_stop:\n            print('Stopping optimization based on learning rate criteria.')\n            break",
            "def train_model(self, datasets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Train the model, print per-epoch information, and save checkpoints.\\n\\n    Loop over training epochs. The function that actually does the\\n    training is train_epoch.  This function iterates over the training\\n    data, one epoch at a time.  The learning rate schedule is such\\n    that it will stay the same until the cost goes up in comparison to\\n    the last few values, then it will drop.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n    '\n    hps = self.hps\n    has_any_valid_set = False\n    for data_dict in datasets.values():\n        if data_dict['valid_data'] is not None:\n            has_any_valid_set = True\n            break\n    session = tf.get_default_session()\n    lr = session.run(self.learning_rate)\n    lr_stop = hps.learning_rate_stop\n    i = -1\n    train_costs = []\n    valid_costs = []\n    ev_total_cost = ev_recon_cost = ev_kl_cost = 0.0\n    lowest_ev_cost = np.Inf\n    while True:\n        i += 1\n        do_save_ckpt = True if i % 10 == 0 else False\n        (tr_total_cost, tr_recon_cost, tr_kl_cost, kl_weight, l2_cost, l2_weight) = self.train_epoch(datasets, do_save_ckpt=do_save_ckpt)\n        if has_any_valid_set:\n            (ev_total_cost, ev_recon_cost, ev_kl_cost) = self.eval_cost_epoch(datasets, kind='valid')\n            valid_costs.append(ev_total_cost)\n            n_lve = 1\n            run_avg_lve = np.mean(valid_costs[-n_lve:])\n            if kl_weight >= 1.0 and (l2_weight >= 1.0 or (self.hps.l2_gen_scale == 0.0 and self.hps.l2_con_scale == 0.0)) and (len(valid_costs) > n_lve and run_avg_lve < lowest_ev_cost):\n                lowest_ev_cost = run_avg_lve\n                checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '_lve.ckpt')\n                self.lve_saver.save(session, checkpoint_path, global_step=self.train_step, latest_filename='checkpoint_lve')\n        values = {'nepochs': i, 'has_any_valid_set': has_any_valid_set, 'tr_total_cost': tr_total_cost, 'ev_total_cost': ev_total_cost, 'tr_recon_cost': tr_recon_cost, 'ev_recon_cost': ev_recon_cost, 'tr_kl_cost': tr_kl_cost, 'ev_kl_cost': ev_kl_cost, 'l2_weight': l2_weight, 'kl_weight': kl_weight, 'l2_cost': l2_cost}\n        self.summarize_all(datasets, values)\n        self.plot_single_example(datasets)\n        train_res = tr_total_cost\n        n_lr = hps.learning_rate_n_to_compare\n        if len(train_costs) > n_lr and train_res > np.max(train_costs[-n_lr:]):\n            _ = session.run(self.learning_rate_decay_op)\n            lr = session.run(self.learning_rate)\n            print('     Decreasing learning rate to %f.' % lr)\n            train_costs.append(np.inf)\n        else:\n            train_costs.append(train_res)\n        if lr < lr_stop:\n            print('Stopping optimization based on learning rate criteria.')\n            break",
            "def train_model(self, datasets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Train the model, print per-epoch information, and save checkpoints.\\n\\n    Loop over training epochs. The function that actually does the\\n    training is train_epoch.  This function iterates over the training\\n    data, one epoch at a time.  The learning rate schedule is such\\n    that it will stay the same until the cost goes up in comparison to\\n    the last few values, then it will drop.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n    '\n    hps = self.hps\n    has_any_valid_set = False\n    for data_dict in datasets.values():\n        if data_dict['valid_data'] is not None:\n            has_any_valid_set = True\n            break\n    session = tf.get_default_session()\n    lr = session.run(self.learning_rate)\n    lr_stop = hps.learning_rate_stop\n    i = -1\n    train_costs = []\n    valid_costs = []\n    ev_total_cost = ev_recon_cost = ev_kl_cost = 0.0\n    lowest_ev_cost = np.Inf\n    while True:\n        i += 1\n        do_save_ckpt = True if i % 10 == 0 else False\n        (tr_total_cost, tr_recon_cost, tr_kl_cost, kl_weight, l2_cost, l2_weight) = self.train_epoch(datasets, do_save_ckpt=do_save_ckpt)\n        if has_any_valid_set:\n            (ev_total_cost, ev_recon_cost, ev_kl_cost) = self.eval_cost_epoch(datasets, kind='valid')\n            valid_costs.append(ev_total_cost)\n            n_lve = 1\n            run_avg_lve = np.mean(valid_costs[-n_lve:])\n            if kl_weight >= 1.0 and (l2_weight >= 1.0 or (self.hps.l2_gen_scale == 0.0 and self.hps.l2_con_scale == 0.0)) and (len(valid_costs) > n_lve and run_avg_lve < lowest_ev_cost):\n                lowest_ev_cost = run_avg_lve\n                checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '_lve.ckpt')\n                self.lve_saver.save(session, checkpoint_path, global_step=self.train_step, latest_filename='checkpoint_lve')\n        values = {'nepochs': i, 'has_any_valid_set': has_any_valid_set, 'tr_total_cost': tr_total_cost, 'ev_total_cost': ev_total_cost, 'tr_recon_cost': tr_recon_cost, 'ev_recon_cost': ev_recon_cost, 'tr_kl_cost': tr_kl_cost, 'ev_kl_cost': ev_kl_cost, 'l2_weight': l2_weight, 'kl_weight': kl_weight, 'l2_cost': l2_cost}\n        self.summarize_all(datasets, values)\n        self.plot_single_example(datasets)\n        train_res = tr_total_cost\n        n_lr = hps.learning_rate_n_to_compare\n        if len(train_costs) > n_lr and train_res > np.max(train_costs[-n_lr:]):\n            _ = session.run(self.learning_rate_decay_op)\n            lr = session.run(self.learning_rate)\n            print('     Decreasing learning rate to %f.' % lr)\n            train_costs.append(np.inf)\n        else:\n            train_costs.append(train_res)\n        if lr < lr_stop:\n            print('Stopping optimization based on learning rate criteria.')\n            break",
            "def train_model(self, datasets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Train the model, print per-epoch information, and save checkpoints.\\n\\n    Loop over training epochs. The function that actually does the\\n    training is train_epoch.  This function iterates over the training\\n    data, one epoch at a time.  The learning rate schedule is such\\n    that it will stay the same until the cost goes up in comparison to\\n    the last few values, then it will drop.\\n\\n    Args:\\n      datasets: A dict of data dicts.  The dataset dict is simply a\\n        name(string)-> data dictionary mapping (See top of lfads.py).\\n    '\n    hps = self.hps\n    has_any_valid_set = False\n    for data_dict in datasets.values():\n        if data_dict['valid_data'] is not None:\n            has_any_valid_set = True\n            break\n    session = tf.get_default_session()\n    lr = session.run(self.learning_rate)\n    lr_stop = hps.learning_rate_stop\n    i = -1\n    train_costs = []\n    valid_costs = []\n    ev_total_cost = ev_recon_cost = ev_kl_cost = 0.0\n    lowest_ev_cost = np.Inf\n    while True:\n        i += 1\n        do_save_ckpt = True if i % 10 == 0 else False\n        (tr_total_cost, tr_recon_cost, tr_kl_cost, kl_weight, l2_cost, l2_weight) = self.train_epoch(datasets, do_save_ckpt=do_save_ckpt)\n        if has_any_valid_set:\n            (ev_total_cost, ev_recon_cost, ev_kl_cost) = self.eval_cost_epoch(datasets, kind='valid')\n            valid_costs.append(ev_total_cost)\n            n_lve = 1\n            run_avg_lve = np.mean(valid_costs[-n_lve:])\n            if kl_weight >= 1.0 and (l2_weight >= 1.0 or (self.hps.l2_gen_scale == 0.0 and self.hps.l2_con_scale == 0.0)) and (len(valid_costs) > n_lve and run_avg_lve < lowest_ev_cost):\n                lowest_ev_cost = run_avg_lve\n                checkpoint_path = os.path.join(self.hps.lfads_save_dir, self.hps.checkpoint_name + '_lve.ckpt')\n                self.lve_saver.save(session, checkpoint_path, global_step=self.train_step, latest_filename='checkpoint_lve')\n        values = {'nepochs': i, 'has_any_valid_set': has_any_valid_set, 'tr_total_cost': tr_total_cost, 'ev_total_cost': ev_total_cost, 'tr_recon_cost': tr_recon_cost, 'ev_recon_cost': ev_recon_cost, 'tr_kl_cost': tr_kl_cost, 'ev_kl_cost': ev_kl_cost, 'l2_weight': l2_weight, 'kl_weight': kl_weight, 'l2_cost': l2_cost}\n        self.summarize_all(datasets, values)\n        self.plot_single_example(datasets)\n        train_res = tr_total_cost\n        n_lr = hps.learning_rate_n_to_compare\n        if len(train_costs) > n_lr and train_res > np.max(train_costs[-n_lr:]):\n            _ = session.run(self.learning_rate_decay_op)\n            lr = session.run(self.learning_rate)\n            print('     Decreasing learning rate to %f.' % lr)\n            train_costs.append(np.inf)\n        else:\n            train_costs.append(train_res)\n        if lr < lr_stop:\n            print('Stopping optimization based on learning rate criteria.')\n            break"
        ]
    },
    {
        "func_name": "eval_cost_epoch",
        "original": "def eval_cost_epoch(self, datasets, kind='train', ext_input_extxi=None, batch_size=None):\n    \"\"\"Evaluate the cost of the epoch.\n\n    Args:\n      data_dict: The dictionary of data (training and validation) used for\n        training and evaluation of the model, respectively.\n\n    Returns:\n      a 3 tuple of costs:\n        (epoch total cost, epoch reconstruction cost, epoch KL cost)\n    \"\"\"\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind=kind, keep_prob=1.0)\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost)",
        "mutated": [
            "def eval_cost_epoch(self, datasets, kind='train', ext_input_extxi=None, batch_size=None):\n    if False:\n        i = 10\n    'Evaluate the cost of the epoch.\\n\\n    Args:\\n      data_dict: The dictionary of data (training and validation) used for\\n        training and evaluation of the model, respectively.\\n\\n    Returns:\\n      a 3 tuple of costs:\\n        (epoch total cost, epoch reconstruction cost, epoch KL cost)\\n    '\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind=kind, keep_prob=1.0)\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost)",
            "def eval_cost_epoch(self, datasets, kind='train', ext_input_extxi=None, batch_size=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Evaluate the cost of the epoch.\\n\\n    Args:\\n      data_dict: The dictionary of data (training and validation) used for\\n        training and evaluation of the model, respectively.\\n\\n    Returns:\\n      a 3 tuple of costs:\\n        (epoch total cost, epoch reconstruction cost, epoch KL cost)\\n    '\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind=kind, keep_prob=1.0)\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost)",
            "def eval_cost_epoch(self, datasets, kind='train', ext_input_extxi=None, batch_size=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Evaluate the cost of the epoch.\\n\\n    Args:\\n      data_dict: The dictionary of data (training and validation) used for\\n        training and evaluation of the model, respectively.\\n\\n    Returns:\\n      a 3 tuple of costs:\\n        (epoch total cost, epoch reconstruction cost, epoch KL cost)\\n    '\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind=kind, keep_prob=1.0)\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost)",
            "def eval_cost_epoch(self, datasets, kind='train', ext_input_extxi=None, batch_size=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Evaluate the cost of the epoch.\\n\\n    Args:\\n      data_dict: The dictionary of data (training and validation) used for\\n        training and evaluation of the model, respectively.\\n\\n    Returns:\\n      a 3 tuple of costs:\\n        (epoch total cost, epoch reconstruction cost, epoch KL cost)\\n    '\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind=kind, keep_prob=1.0)\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost)",
            "def eval_cost_epoch(self, datasets, kind='train', ext_input_extxi=None, batch_size=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Evaluate the cost of the epoch.\\n\\n    Args:\\n      data_dict: The dictionary of data (training and validation) used for\\n        training and evaluation of the model, respectively.\\n\\n    Returns:\\n      a 3 tuple of costs:\\n        (epoch total cost, epoch reconstruction cost, epoch KL cost)\\n    '\n    ops_to_eval = [self.cost, self.recon_cost, self.kl_cost]\n    collected_op_values = self.run_epoch(datasets, ops_to_eval, kind=kind, keep_prob=1.0)\n    total_cost = total_recon_cost = total_kl_cost = 0.0\n    epoch_size = len(collected_op_values)\n    for op_values in collected_op_values:\n        total_cost += op_values[0]\n        total_recon_cost += op_values[1]\n        total_kl_cost += op_values[2]\n    epoch_total_cost = total_cost / epoch_size\n    epoch_recon_cost = total_recon_cost / epoch_size\n    epoch_kl_cost = total_kl_cost / epoch_size\n    return (epoch_total_cost, epoch_recon_cost, epoch_kl_cost)"
        ]
    },
    {
        "func_name": "eval_model_runs_batch",
        "original": "def eval_model_runs_batch(self, data_name, data_bxtxd, ext_input_bxtxi=None, do_eval_cost=False, do_average_batch=False):\n    \"\"\"Returns all the goodies for the entire model, per batch.\n\n    If data_bxtxd and ext_input_bxtxi can have fewer than batch_size along dim 1\n    in which case this handles the padding and truncating automatically\n\n    Args:\n      data_name: The name of the data dict, to select which in/out matrices\n        to use.\n      data_bxtxd:  Numpy array training data with shape:\n        batch_size x # time steps x # dimensions\n      ext_input_bxtxi: Numpy array training external input with shape:\n        batch_size x # time steps x # external input dims\n      do_eval_cost (optional): If true, the IWAE (Importance Weighted\n         Autoencoder) log likeihood bound, instead of the VAE version.\n      do_average_batch (optional): average over the batch, useful for getting\n      good IWAE costs, and model outputs for a single data point.\n\n    Returns:\n      A dictionary with the outputs of the model decoder, namely:\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\n        posterior mean, the generator initial conditions, the control inputs (if\n        enabled), the state of the generator, the factors, and the rates.\n    \"\"\"\n    session = tf.get_default_session()\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, _, _) = data_bxtxd.shape\n    if E < hps.batch_size:\n        data_bxtxd = np.pad(data_bxtxd, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n        if ext_input_bxtxi is not None:\n            ext_input_bxtxi = np.pad(ext_input_bxtxi, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n    feed_dict = self.build_feed_dict(data_name, data_bxtxd, ext_input_bxtxi, keep_prob=1.0)\n    tf_vals = [self.gen_ics, self.gen_states, self.factors, self.output_dist_params]\n    tf_vals.append(self.cost)\n    tf_vals.append(self.nll_bound_vae)\n    tf_vals.append(self.nll_bound_iwae)\n    tf_vals.append(self.train_step)\n    if self.hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar, self.posterior_zs_g0.mean, self.posterior_zs_g0.logvar]\n    if self.hps.co_dim > 0:\n        tf_vals.append(self.controller_outputs)\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    out_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_vaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_iwaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    train_steps = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if self.hps.co_dim > 0:\n        controller_outputs = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    nll_bound_vaes = nll_bound_vaes[0]\n    nll_bound_iwaes = nll_bound_iwaes[0]\n    train_steps = train_steps[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    out_dist_params = list_t_bxn_to_tensor_bxtxn(out_dist_params)\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n        post_g0_mean = post_g0_mean[0]\n        post_g0_logvar = post_g0_logvar[0]\n    if self.hps.co_dim > 0:\n        controller_outputs = list_t_bxn_to_tensor_bxtxn(controller_outputs)\n    if E < hps.batch_size:\n        idx = np.arange(E)\n        gen_ics = gen_ics[idx, :]\n        gen_states = gen_states[idx, :]\n        factors = factors[idx, :, :]\n        out_dist_params = out_dist_params[idx, :, :]\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = prior_g0_mean[idx, :]\n            prior_g0_logvar = prior_g0_logvar[idx, :]\n            post_g0_mean = post_g0_mean[idx, :]\n            post_g0_logvar = post_g0_logvar[idx, :]\n        if self.hps.co_dim > 0:\n            controller_outputs = controller_outputs[idx, :, :]\n    if do_average_batch:\n        gen_ics = np.mean(gen_ics, axis=0)\n        gen_states = np.mean(gen_states, axis=0)\n        factors = np.mean(factors, axis=0)\n        out_dist_params = np.mean(out_dist_params, axis=0)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = np.mean(prior_g0_mean, axis=0)\n            prior_g0_logvar = np.mean(prior_g0_logvar, axis=0)\n            post_g0_mean = np.mean(post_g0_mean, axis=0)\n            post_g0_logvar = np.mean(post_g0_logvar, axis=0)\n        if self.hps.co_dim > 0:\n            controller_outputs = np.mean(controller_outputs, axis=0)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = out_dist_params\n    model_vals['costs'] = costs\n    model_vals['nll_bound_vaes'] = nll_bound_vaes\n    model_vals['nll_bound_iwaes'] = nll_bound_iwaes\n    model_vals['train_steps'] = train_steps\n    if self.hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n        model_vals['post_g0_mean'] = post_g0_mean\n        model_vals['post_g0_logvar'] = post_g0_logvar\n    if self.hps.co_dim > 0:\n        model_vals['controller_outputs'] = controller_outputs\n    return model_vals",
        "mutated": [
            "def eval_model_runs_batch(self, data_name, data_bxtxd, ext_input_bxtxi=None, do_eval_cost=False, do_average_batch=False):\n    if False:\n        i = 10\n    'Returns all the goodies for the entire model, per batch.\\n\\n    If data_bxtxd and ext_input_bxtxi can have fewer than batch_size along dim 1\\n    in which case this handles the padding and truncating automatically\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_bxtxd:  Numpy array training data with shape:\\n        batch_size x # time steps x # dimensions\\n      ext_input_bxtxi: Numpy array training external input with shape:\\n        batch_size x # time steps x # external input dims\\n      do_eval_cost (optional): If true, the IWAE (Importance Weighted\\n         Autoencoder) log likeihood bound, instead of the VAE version.\\n      do_average_batch (optional): average over the batch, useful for getting\\n      good IWAE costs, and model outputs for a single data point.\\n\\n    Returns:\\n      A dictionary with the outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the rates.\\n    '\n    session = tf.get_default_session()\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, _, _) = data_bxtxd.shape\n    if E < hps.batch_size:\n        data_bxtxd = np.pad(data_bxtxd, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n        if ext_input_bxtxi is not None:\n            ext_input_bxtxi = np.pad(ext_input_bxtxi, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n    feed_dict = self.build_feed_dict(data_name, data_bxtxd, ext_input_bxtxi, keep_prob=1.0)\n    tf_vals = [self.gen_ics, self.gen_states, self.factors, self.output_dist_params]\n    tf_vals.append(self.cost)\n    tf_vals.append(self.nll_bound_vae)\n    tf_vals.append(self.nll_bound_iwae)\n    tf_vals.append(self.train_step)\n    if self.hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar, self.posterior_zs_g0.mean, self.posterior_zs_g0.logvar]\n    if self.hps.co_dim > 0:\n        tf_vals.append(self.controller_outputs)\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    out_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_vaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_iwaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    train_steps = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if self.hps.co_dim > 0:\n        controller_outputs = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    nll_bound_vaes = nll_bound_vaes[0]\n    nll_bound_iwaes = nll_bound_iwaes[0]\n    train_steps = train_steps[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    out_dist_params = list_t_bxn_to_tensor_bxtxn(out_dist_params)\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n        post_g0_mean = post_g0_mean[0]\n        post_g0_logvar = post_g0_logvar[0]\n    if self.hps.co_dim > 0:\n        controller_outputs = list_t_bxn_to_tensor_bxtxn(controller_outputs)\n    if E < hps.batch_size:\n        idx = np.arange(E)\n        gen_ics = gen_ics[idx, :]\n        gen_states = gen_states[idx, :]\n        factors = factors[idx, :, :]\n        out_dist_params = out_dist_params[idx, :, :]\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = prior_g0_mean[idx, :]\n            prior_g0_logvar = prior_g0_logvar[idx, :]\n            post_g0_mean = post_g0_mean[idx, :]\n            post_g0_logvar = post_g0_logvar[idx, :]\n        if self.hps.co_dim > 0:\n            controller_outputs = controller_outputs[idx, :, :]\n    if do_average_batch:\n        gen_ics = np.mean(gen_ics, axis=0)\n        gen_states = np.mean(gen_states, axis=0)\n        factors = np.mean(factors, axis=0)\n        out_dist_params = np.mean(out_dist_params, axis=0)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = np.mean(prior_g0_mean, axis=0)\n            prior_g0_logvar = np.mean(prior_g0_logvar, axis=0)\n            post_g0_mean = np.mean(post_g0_mean, axis=0)\n            post_g0_logvar = np.mean(post_g0_logvar, axis=0)\n        if self.hps.co_dim > 0:\n            controller_outputs = np.mean(controller_outputs, axis=0)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = out_dist_params\n    model_vals['costs'] = costs\n    model_vals['nll_bound_vaes'] = nll_bound_vaes\n    model_vals['nll_bound_iwaes'] = nll_bound_iwaes\n    model_vals['train_steps'] = train_steps\n    if self.hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n        model_vals['post_g0_mean'] = post_g0_mean\n        model_vals['post_g0_logvar'] = post_g0_logvar\n    if self.hps.co_dim > 0:\n        model_vals['controller_outputs'] = controller_outputs\n    return model_vals",
            "def eval_model_runs_batch(self, data_name, data_bxtxd, ext_input_bxtxi=None, do_eval_cost=False, do_average_batch=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns all the goodies for the entire model, per batch.\\n\\n    If data_bxtxd and ext_input_bxtxi can have fewer than batch_size along dim 1\\n    in which case this handles the padding and truncating automatically\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_bxtxd:  Numpy array training data with shape:\\n        batch_size x # time steps x # dimensions\\n      ext_input_bxtxi: Numpy array training external input with shape:\\n        batch_size x # time steps x # external input dims\\n      do_eval_cost (optional): If true, the IWAE (Importance Weighted\\n         Autoencoder) log likeihood bound, instead of the VAE version.\\n      do_average_batch (optional): average over the batch, useful for getting\\n      good IWAE costs, and model outputs for a single data point.\\n\\n    Returns:\\n      A dictionary with the outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the rates.\\n    '\n    session = tf.get_default_session()\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, _, _) = data_bxtxd.shape\n    if E < hps.batch_size:\n        data_bxtxd = np.pad(data_bxtxd, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n        if ext_input_bxtxi is not None:\n            ext_input_bxtxi = np.pad(ext_input_bxtxi, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n    feed_dict = self.build_feed_dict(data_name, data_bxtxd, ext_input_bxtxi, keep_prob=1.0)\n    tf_vals = [self.gen_ics, self.gen_states, self.factors, self.output_dist_params]\n    tf_vals.append(self.cost)\n    tf_vals.append(self.nll_bound_vae)\n    tf_vals.append(self.nll_bound_iwae)\n    tf_vals.append(self.train_step)\n    if self.hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar, self.posterior_zs_g0.mean, self.posterior_zs_g0.logvar]\n    if self.hps.co_dim > 0:\n        tf_vals.append(self.controller_outputs)\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    out_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_vaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_iwaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    train_steps = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if self.hps.co_dim > 0:\n        controller_outputs = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    nll_bound_vaes = nll_bound_vaes[0]\n    nll_bound_iwaes = nll_bound_iwaes[0]\n    train_steps = train_steps[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    out_dist_params = list_t_bxn_to_tensor_bxtxn(out_dist_params)\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n        post_g0_mean = post_g0_mean[0]\n        post_g0_logvar = post_g0_logvar[0]\n    if self.hps.co_dim > 0:\n        controller_outputs = list_t_bxn_to_tensor_bxtxn(controller_outputs)\n    if E < hps.batch_size:\n        idx = np.arange(E)\n        gen_ics = gen_ics[idx, :]\n        gen_states = gen_states[idx, :]\n        factors = factors[idx, :, :]\n        out_dist_params = out_dist_params[idx, :, :]\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = prior_g0_mean[idx, :]\n            prior_g0_logvar = prior_g0_logvar[idx, :]\n            post_g0_mean = post_g0_mean[idx, :]\n            post_g0_logvar = post_g0_logvar[idx, :]\n        if self.hps.co_dim > 0:\n            controller_outputs = controller_outputs[idx, :, :]\n    if do_average_batch:\n        gen_ics = np.mean(gen_ics, axis=0)\n        gen_states = np.mean(gen_states, axis=0)\n        factors = np.mean(factors, axis=0)\n        out_dist_params = np.mean(out_dist_params, axis=0)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = np.mean(prior_g0_mean, axis=0)\n            prior_g0_logvar = np.mean(prior_g0_logvar, axis=0)\n            post_g0_mean = np.mean(post_g0_mean, axis=0)\n            post_g0_logvar = np.mean(post_g0_logvar, axis=0)\n        if self.hps.co_dim > 0:\n            controller_outputs = np.mean(controller_outputs, axis=0)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = out_dist_params\n    model_vals['costs'] = costs\n    model_vals['nll_bound_vaes'] = nll_bound_vaes\n    model_vals['nll_bound_iwaes'] = nll_bound_iwaes\n    model_vals['train_steps'] = train_steps\n    if self.hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n        model_vals['post_g0_mean'] = post_g0_mean\n        model_vals['post_g0_logvar'] = post_g0_logvar\n    if self.hps.co_dim > 0:\n        model_vals['controller_outputs'] = controller_outputs\n    return model_vals",
            "def eval_model_runs_batch(self, data_name, data_bxtxd, ext_input_bxtxi=None, do_eval_cost=False, do_average_batch=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns all the goodies for the entire model, per batch.\\n\\n    If data_bxtxd and ext_input_bxtxi can have fewer than batch_size along dim 1\\n    in which case this handles the padding and truncating automatically\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_bxtxd:  Numpy array training data with shape:\\n        batch_size x # time steps x # dimensions\\n      ext_input_bxtxi: Numpy array training external input with shape:\\n        batch_size x # time steps x # external input dims\\n      do_eval_cost (optional): If true, the IWAE (Importance Weighted\\n         Autoencoder) log likeihood bound, instead of the VAE version.\\n      do_average_batch (optional): average over the batch, useful for getting\\n      good IWAE costs, and model outputs for a single data point.\\n\\n    Returns:\\n      A dictionary with the outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the rates.\\n    '\n    session = tf.get_default_session()\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, _, _) = data_bxtxd.shape\n    if E < hps.batch_size:\n        data_bxtxd = np.pad(data_bxtxd, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n        if ext_input_bxtxi is not None:\n            ext_input_bxtxi = np.pad(ext_input_bxtxi, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n    feed_dict = self.build_feed_dict(data_name, data_bxtxd, ext_input_bxtxi, keep_prob=1.0)\n    tf_vals = [self.gen_ics, self.gen_states, self.factors, self.output_dist_params]\n    tf_vals.append(self.cost)\n    tf_vals.append(self.nll_bound_vae)\n    tf_vals.append(self.nll_bound_iwae)\n    tf_vals.append(self.train_step)\n    if self.hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar, self.posterior_zs_g0.mean, self.posterior_zs_g0.logvar]\n    if self.hps.co_dim > 0:\n        tf_vals.append(self.controller_outputs)\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    out_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_vaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_iwaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    train_steps = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if self.hps.co_dim > 0:\n        controller_outputs = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    nll_bound_vaes = nll_bound_vaes[0]\n    nll_bound_iwaes = nll_bound_iwaes[0]\n    train_steps = train_steps[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    out_dist_params = list_t_bxn_to_tensor_bxtxn(out_dist_params)\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n        post_g0_mean = post_g0_mean[0]\n        post_g0_logvar = post_g0_logvar[0]\n    if self.hps.co_dim > 0:\n        controller_outputs = list_t_bxn_to_tensor_bxtxn(controller_outputs)\n    if E < hps.batch_size:\n        idx = np.arange(E)\n        gen_ics = gen_ics[idx, :]\n        gen_states = gen_states[idx, :]\n        factors = factors[idx, :, :]\n        out_dist_params = out_dist_params[idx, :, :]\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = prior_g0_mean[idx, :]\n            prior_g0_logvar = prior_g0_logvar[idx, :]\n            post_g0_mean = post_g0_mean[idx, :]\n            post_g0_logvar = post_g0_logvar[idx, :]\n        if self.hps.co_dim > 0:\n            controller_outputs = controller_outputs[idx, :, :]\n    if do_average_batch:\n        gen_ics = np.mean(gen_ics, axis=0)\n        gen_states = np.mean(gen_states, axis=0)\n        factors = np.mean(factors, axis=0)\n        out_dist_params = np.mean(out_dist_params, axis=0)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = np.mean(prior_g0_mean, axis=0)\n            prior_g0_logvar = np.mean(prior_g0_logvar, axis=0)\n            post_g0_mean = np.mean(post_g0_mean, axis=0)\n            post_g0_logvar = np.mean(post_g0_logvar, axis=0)\n        if self.hps.co_dim > 0:\n            controller_outputs = np.mean(controller_outputs, axis=0)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = out_dist_params\n    model_vals['costs'] = costs\n    model_vals['nll_bound_vaes'] = nll_bound_vaes\n    model_vals['nll_bound_iwaes'] = nll_bound_iwaes\n    model_vals['train_steps'] = train_steps\n    if self.hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n        model_vals['post_g0_mean'] = post_g0_mean\n        model_vals['post_g0_logvar'] = post_g0_logvar\n    if self.hps.co_dim > 0:\n        model_vals['controller_outputs'] = controller_outputs\n    return model_vals",
            "def eval_model_runs_batch(self, data_name, data_bxtxd, ext_input_bxtxi=None, do_eval_cost=False, do_average_batch=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns all the goodies for the entire model, per batch.\\n\\n    If data_bxtxd and ext_input_bxtxi can have fewer than batch_size along dim 1\\n    in which case this handles the padding and truncating automatically\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_bxtxd:  Numpy array training data with shape:\\n        batch_size x # time steps x # dimensions\\n      ext_input_bxtxi: Numpy array training external input with shape:\\n        batch_size x # time steps x # external input dims\\n      do_eval_cost (optional): If true, the IWAE (Importance Weighted\\n         Autoencoder) log likeihood bound, instead of the VAE version.\\n      do_average_batch (optional): average over the batch, useful for getting\\n      good IWAE costs, and model outputs for a single data point.\\n\\n    Returns:\\n      A dictionary with the outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the rates.\\n    '\n    session = tf.get_default_session()\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, _, _) = data_bxtxd.shape\n    if E < hps.batch_size:\n        data_bxtxd = np.pad(data_bxtxd, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n        if ext_input_bxtxi is not None:\n            ext_input_bxtxi = np.pad(ext_input_bxtxi, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n    feed_dict = self.build_feed_dict(data_name, data_bxtxd, ext_input_bxtxi, keep_prob=1.0)\n    tf_vals = [self.gen_ics, self.gen_states, self.factors, self.output_dist_params]\n    tf_vals.append(self.cost)\n    tf_vals.append(self.nll_bound_vae)\n    tf_vals.append(self.nll_bound_iwae)\n    tf_vals.append(self.train_step)\n    if self.hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar, self.posterior_zs_g0.mean, self.posterior_zs_g0.logvar]\n    if self.hps.co_dim > 0:\n        tf_vals.append(self.controller_outputs)\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    out_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_vaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_iwaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    train_steps = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if self.hps.co_dim > 0:\n        controller_outputs = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    nll_bound_vaes = nll_bound_vaes[0]\n    nll_bound_iwaes = nll_bound_iwaes[0]\n    train_steps = train_steps[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    out_dist_params = list_t_bxn_to_tensor_bxtxn(out_dist_params)\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n        post_g0_mean = post_g0_mean[0]\n        post_g0_logvar = post_g0_logvar[0]\n    if self.hps.co_dim > 0:\n        controller_outputs = list_t_bxn_to_tensor_bxtxn(controller_outputs)\n    if E < hps.batch_size:\n        idx = np.arange(E)\n        gen_ics = gen_ics[idx, :]\n        gen_states = gen_states[idx, :]\n        factors = factors[idx, :, :]\n        out_dist_params = out_dist_params[idx, :, :]\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = prior_g0_mean[idx, :]\n            prior_g0_logvar = prior_g0_logvar[idx, :]\n            post_g0_mean = post_g0_mean[idx, :]\n            post_g0_logvar = post_g0_logvar[idx, :]\n        if self.hps.co_dim > 0:\n            controller_outputs = controller_outputs[idx, :, :]\n    if do_average_batch:\n        gen_ics = np.mean(gen_ics, axis=0)\n        gen_states = np.mean(gen_states, axis=0)\n        factors = np.mean(factors, axis=0)\n        out_dist_params = np.mean(out_dist_params, axis=0)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = np.mean(prior_g0_mean, axis=0)\n            prior_g0_logvar = np.mean(prior_g0_logvar, axis=0)\n            post_g0_mean = np.mean(post_g0_mean, axis=0)\n            post_g0_logvar = np.mean(post_g0_logvar, axis=0)\n        if self.hps.co_dim > 0:\n            controller_outputs = np.mean(controller_outputs, axis=0)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = out_dist_params\n    model_vals['costs'] = costs\n    model_vals['nll_bound_vaes'] = nll_bound_vaes\n    model_vals['nll_bound_iwaes'] = nll_bound_iwaes\n    model_vals['train_steps'] = train_steps\n    if self.hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n        model_vals['post_g0_mean'] = post_g0_mean\n        model_vals['post_g0_logvar'] = post_g0_logvar\n    if self.hps.co_dim > 0:\n        model_vals['controller_outputs'] = controller_outputs\n    return model_vals",
            "def eval_model_runs_batch(self, data_name, data_bxtxd, ext_input_bxtxi=None, do_eval_cost=False, do_average_batch=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns all the goodies for the entire model, per batch.\\n\\n    If data_bxtxd and ext_input_bxtxi can have fewer than batch_size along dim 1\\n    in which case this handles the padding and truncating automatically\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_bxtxd:  Numpy array training data with shape:\\n        batch_size x # time steps x # dimensions\\n      ext_input_bxtxi: Numpy array training external input with shape:\\n        batch_size x # time steps x # external input dims\\n      do_eval_cost (optional): If true, the IWAE (Importance Weighted\\n         Autoencoder) log likeihood bound, instead of the VAE version.\\n      do_average_batch (optional): average over the batch, useful for getting\\n      good IWAE costs, and model outputs for a single data point.\\n\\n    Returns:\\n      A dictionary with the outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the rates.\\n    '\n    session = tf.get_default_session()\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, _, _) = data_bxtxd.shape\n    if E < hps.batch_size:\n        data_bxtxd = np.pad(data_bxtxd, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n        if ext_input_bxtxi is not None:\n            ext_input_bxtxi = np.pad(ext_input_bxtxi, ((0, hps.batch_size - E), (0, 0), (0, 0)), mode='constant', constant_values=0)\n    feed_dict = self.build_feed_dict(data_name, data_bxtxd, ext_input_bxtxi, keep_prob=1.0)\n    tf_vals = [self.gen_ics, self.gen_states, self.factors, self.output_dist_params]\n    tf_vals.append(self.cost)\n    tf_vals.append(self.nll_bound_vae)\n    tf_vals.append(self.nll_bound_iwae)\n    tf_vals.append(self.train_step)\n    if self.hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar, self.posterior_zs_g0.mean, self.posterior_zs_g0.logvar]\n    if self.hps.co_dim > 0:\n        tf_vals.append(self.controller_outputs)\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    out_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_vaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    nll_bound_iwaes = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    train_steps = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        post_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if self.hps.co_dim > 0:\n        controller_outputs = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    nll_bound_vaes = nll_bound_vaes[0]\n    nll_bound_iwaes = nll_bound_iwaes[0]\n    train_steps = train_steps[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    out_dist_params = list_t_bxn_to_tensor_bxtxn(out_dist_params)\n    if self.hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n        post_g0_mean = post_g0_mean[0]\n        post_g0_logvar = post_g0_logvar[0]\n    if self.hps.co_dim > 0:\n        controller_outputs = list_t_bxn_to_tensor_bxtxn(controller_outputs)\n    if E < hps.batch_size:\n        idx = np.arange(E)\n        gen_ics = gen_ics[idx, :]\n        gen_states = gen_states[idx, :]\n        factors = factors[idx, :, :]\n        out_dist_params = out_dist_params[idx, :, :]\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = prior_g0_mean[idx, :]\n            prior_g0_logvar = prior_g0_logvar[idx, :]\n            post_g0_mean = post_g0_mean[idx, :]\n            post_g0_logvar = post_g0_logvar[idx, :]\n        if self.hps.co_dim > 0:\n            controller_outputs = controller_outputs[idx, :, :]\n    if do_average_batch:\n        gen_ics = np.mean(gen_ics, axis=0)\n        gen_states = np.mean(gen_states, axis=0)\n        factors = np.mean(factors, axis=0)\n        out_dist_params = np.mean(out_dist_params, axis=0)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean = np.mean(prior_g0_mean, axis=0)\n            prior_g0_logvar = np.mean(prior_g0_logvar, axis=0)\n            post_g0_mean = np.mean(post_g0_mean, axis=0)\n            post_g0_logvar = np.mean(post_g0_logvar, axis=0)\n        if self.hps.co_dim > 0:\n            controller_outputs = np.mean(controller_outputs, axis=0)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = out_dist_params\n    model_vals['costs'] = costs\n    model_vals['nll_bound_vaes'] = nll_bound_vaes\n    model_vals['nll_bound_iwaes'] = nll_bound_iwaes\n    model_vals['train_steps'] = train_steps\n    if self.hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n        model_vals['post_g0_mean'] = post_g0_mean\n        model_vals['post_g0_logvar'] = post_g0_logvar\n    if self.hps.co_dim > 0:\n        model_vals['controller_outputs'] = controller_outputs\n    return model_vals"
        ]
    },
    {
        "func_name": "eval_model_runs_avg_epoch",
        "original": "def eval_model_runs_avg_epoch(self, data_name, data_extxd, ext_input_extxi=None):\n    \"\"\"Returns all the expected value for goodies for the entire model.\n\n    The expected value is taken over hidden (z) variables, namely the initial\n    conditions and the control inputs.  The expected value is approximate, and\n    accomplished via sampling (batch_size) samples for every examples.\n\n    Args:\n      data_name: The name of the data dict, to select which in/out matrices\n        to use.\n      data_extxd:  Numpy array training data with shape:\n        # examples x # time steps x # dimensions\n      ext_input_extxi (optional): Numpy array training external input with\n        shape: # examples x # time steps x # external input dims\n\n    Returns:\n      A dictionary with the averaged outputs of the model decoder, namely:\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\n        posterior mean, the generator initial conditions, the control inputs (if\n        enabled), the state of the generator, the factors, and the output\n        distribution parameters, e.g. (rates or mean and variances).\n    \"\"\"\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n    for es_idx in range(E_to_process):\n        print('Running %d of %d.' % (es_idx + 1, E_to_process))\n        example_idxs = es_idx * np.ones(batch_size, dtype=np.int32)\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=example_idxs)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=True)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n        print('bound nll(vae): %.3f, bound nll(iwae): %.3f' % (nll_bound_vaes[es_idx], nll_bound_iwaes[es_idx]))\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs",
        "mutated": [
            "def eval_model_runs_avg_epoch(self, data_name, data_extxd, ext_input_extxi=None):\n    if False:\n        i = 10\n    'Returns all the expected value for goodies for the entire model.\\n\\n    The expected value is taken over hidden (z) variables, namely the initial\\n    conditions and the control inputs.  The expected value is approximate, and\\n    accomplished via sampling (batch_size) samples for every examples.\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_extxd:  Numpy array training data with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): Numpy array training external input with\\n        shape: # examples x # time steps x # external input dims\\n\\n    Returns:\\n      A dictionary with the averaged outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the output\\n        distribution parameters, e.g. (rates or mean and variances).\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n    for es_idx in range(E_to_process):\n        print('Running %d of %d.' % (es_idx + 1, E_to_process))\n        example_idxs = es_idx * np.ones(batch_size, dtype=np.int32)\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=example_idxs)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=True)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n        print('bound nll(vae): %.3f, bound nll(iwae): %.3f' % (nll_bound_vaes[es_idx], nll_bound_iwaes[es_idx]))\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs",
            "def eval_model_runs_avg_epoch(self, data_name, data_extxd, ext_input_extxi=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns all the expected value for goodies for the entire model.\\n\\n    The expected value is taken over hidden (z) variables, namely the initial\\n    conditions and the control inputs.  The expected value is approximate, and\\n    accomplished via sampling (batch_size) samples for every examples.\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_extxd:  Numpy array training data with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): Numpy array training external input with\\n        shape: # examples x # time steps x # external input dims\\n\\n    Returns:\\n      A dictionary with the averaged outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the output\\n        distribution parameters, e.g. (rates or mean and variances).\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n    for es_idx in range(E_to_process):\n        print('Running %d of %d.' % (es_idx + 1, E_to_process))\n        example_idxs = es_idx * np.ones(batch_size, dtype=np.int32)\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=example_idxs)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=True)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n        print('bound nll(vae): %.3f, bound nll(iwae): %.3f' % (nll_bound_vaes[es_idx], nll_bound_iwaes[es_idx]))\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs",
            "def eval_model_runs_avg_epoch(self, data_name, data_extxd, ext_input_extxi=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns all the expected value for goodies for the entire model.\\n\\n    The expected value is taken over hidden (z) variables, namely the initial\\n    conditions and the control inputs.  The expected value is approximate, and\\n    accomplished via sampling (batch_size) samples for every examples.\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_extxd:  Numpy array training data with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): Numpy array training external input with\\n        shape: # examples x # time steps x # external input dims\\n\\n    Returns:\\n      A dictionary with the averaged outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the output\\n        distribution parameters, e.g. (rates or mean and variances).\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n    for es_idx in range(E_to_process):\n        print('Running %d of %d.' % (es_idx + 1, E_to_process))\n        example_idxs = es_idx * np.ones(batch_size, dtype=np.int32)\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=example_idxs)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=True)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n        print('bound nll(vae): %.3f, bound nll(iwae): %.3f' % (nll_bound_vaes[es_idx], nll_bound_iwaes[es_idx]))\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs",
            "def eval_model_runs_avg_epoch(self, data_name, data_extxd, ext_input_extxi=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns all the expected value for goodies for the entire model.\\n\\n    The expected value is taken over hidden (z) variables, namely the initial\\n    conditions and the control inputs.  The expected value is approximate, and\\n    accomplished via sampling (batch_size) samples for every examples.\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_extxd:  Numpy array training data with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): Numpy array training external input with\\n        shape: # examples x # time steps x # external input dims\\n\\n    Returns:\\n      A dictionary with the averaged outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the output\\n        distribution parameters, e.g. (rates or mean and variances).\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n    for es_idx in range(E_to_process):\n        print('Running %d of %d.' % (es_idx + 1, E_to_process))\n        example_idxs = es_idx * np.ones(batch_size, dtype=np.int32)\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=example_idxs)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=True)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n        print('bound nll(vae): %.3f, bound nll(iwae): %.3f' % (nll_bound_vaes[es_idx], nll_bound_iwaes[es_idx]))\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs",
            "def eval_model_runs_avg_epoch(self, data_name, data_extxd, ext_input_extxi=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns all the expected value for goodies for the entire model.\\n\\n    The expected value is taken over hidden (z) variables, namely the initial\\n    conditions and the control inputs.  The expected value is approximate, and\\n    accomplished via sampling (batch_size) samples for every examples.\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_extxd:  Numpy array training data with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): Numpy array training external input with\\n        shape: # examples x # time steps x # external input dims\\n\\n    Returns:\\n      A dictionary with the averaged outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the output\\n        distribution parameters, e.g. (rates or mean and variances).\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n    for es_idx in range(E_to_process):\n        print('Running %d of %d.' % (es_idx + 1, E_to_process))\n        example_idxs = es_idx * np.ones(batch_size, dtype=np.int32)\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=example_idxs)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=True)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n        print('bound nll(vae): %.3f, bound nll(iwae): %.3f' % (nll_bound_vaes[es_idx], nll_bound_iwaes[es_idx]))\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs"
        ]
    },
    {
        "func_name": "trial_batches",
        "original": "def trial_batches(N, per):\n    for i in range(0, N, per):\n        yield np.arange(i, min(i + per, N), dtype=np.int32)",
        "mutated": [
            "def trial_batches(N, per):\n    if False:\n        i = 10\n    for i in range(0, N, per):\n        yield np.arange(i, min(i + per, N), dtype=np.int32)",
            "def trial_batches(N, per):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for i in range(0, N, per):\n        yield np.arange(i, min(i + per, N), dtype=np.int32)",
            "def trial_batches(N, per):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for i in range(0, N, per):\n        yield np.arange(i, min(i + per, N), dtype=np.int32)",
            "def trial_batches(N, per):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for i in range(0, N, per):\n        yield np.arange(i, min(i + per, N), dtype=np.int32)",
            "def trial_batches(N, per):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for i in range(0, N, per):\n        yield np.arange(i, min(i + per, N), dtype=np.int32)"
        ]
    },
    {
        "func_name": "eval_model_runs_push_mean",
        "original": "def eval_model_runs_push_mean(self, data_name, data_extxd, ext_input_extxi=None):\n    \"\"\"Returns values of interest for the  model by pushing the means through\n\n    The mean values for both initial conditions and the control inputs are\n    pushed through the model instead of sampling (as is done in\n    eval_model_runs_avg_epoch).\n    This is a quick and approximate version of estimating these values instead\n    of sampling from the posterior many times and then averaging those values of\n    interest.\n\n    Internally, a total of batch_size trials are run through the model at once.\n\n    Args:\n      data_name: The name of the data dict, to select which in/out matrices\n        to use.\n      data_extxd:  Numpy array training data with shape:\n        # examples x # time steps x # dimensions\n      ext_input_extxi (optional): Numpy array training external input with\n        shape: # examples x # time steps x # external input dims\n\n    Returns:\n      A dictionary with the estimated outputs of the model decoder, namely:\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\n        posterior mean, the generator initial conditions, the control inputs (if\n        enabled), the state of the generator, the factors, and the output\n        distribution parameters, e.g. (rates or mean and variances).\n    \"\"\"\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        print('Setting number of posterior samples to process to : ', E)\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n\n    def trial_batches(N, per):\n        for i in range(0, N, per):\n            yield np.arange(i, min(i + per, N), dtype=np.int32)\n    for (batch_idx, es_idx) in enumerate(trial_batches(E_to_process, hps.batch_size)):\n        print('Running trial batch %d with %d trials' % (batch_idx + 1, len(es_idx)))\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=es_idx)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=False)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs",
        "mutated": [
            "def eval_model_runs_push_mean(self, data_name, data_extxd, ext_input_extxi=None):\n    if False:\n        i = 10\n    'Returns values of interest for the  model by pushing the means through\\n\\n    The mean values for both initial conditions and the control inputs are\\n    pushed through the model instead of sampling (as is done in\\n    eval_model_runs_avg_epoch).\\n    This is a quick and approximate version of estimating these values instead\\n    of sampling from the posterior many times and then averaging those values of\\n    interest.\\n\\n    Internally, a total of batch_size trials are run through the model at once.\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_extxd:  Numpy array training data with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): Numpy array training external input with\\n        shape: # examples x # time steps x # external input dims\\n\\n    Returns:\\n      A dictionary with the estimated outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the output\\n        distribution parameters, e.g. (rates or mean and variances).\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        print('Setting number of posterior samples to process to : ', E)\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n\n    def trial_batches(N, per):\n        for i in range(0, N, per):\n            yield np.arange(i, min(i + per, N), dtype=np.int32)\n    for (batch_idx, es_idx) in enumerate(trial_batches(E_to_process, hps.batch_size)):\n        print('Running trial batch %d with %d trials' % (batch_idx + 1, len(es_idx)))\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=es_idx)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=False)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs",
            "def eval_model_runs_push_mean(self, data_name, data_extxd, ext_input_extxi=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns values of interest for the  model by pushing the means through\\n\\n    The mean values for both initial conditions and the control inputs are\\n    pushed through the model instead of sampling (as is done in\\n    eval_model_runs_avg_epoch).\\n    This is a quick and approximate version of estimating these values instead\\n    of sampling from the posterior many times and then averaging those values of\\n    interest.\\n\\n    Internally, a total of batch_size trials are run through the model at once.\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_extxd:  Numpy array training data with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): Numpy array training external input with\\n        shape: # examples x # time steps x # external input dims\\n\\n    Returns:\\n      A dictionary with the estimated outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the output\\n        distribution parameters, e.g. (rates or mean and variances).\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        print('Setting number of posterior samples to process to : ', E)\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n\n    def trial_batches(N, per):\n        for i in range(0, N, per):\n            yield np.arange(i, min(i + per, N), dtype=np.int32)\n    for (batch_idx, es_idx) in enumerate(trial_batches(E_to_process, hps.batch_size)):\n        print('Running trial batch %d with %d trials' % (batch_idx + 1, len(es_idx)))\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=es_idx)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=False)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs",
            "def eval_model_runs_push_mean(self, data_name, data_extxd, ext_input_extxi=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns values of interest for the  model by pushing the means through\\n\\n    The mean values for both initial conditions and the control inputs are\\n    pushed through the model instead of sampling (as is done in\\n    eval_model_runs_avg_epoch).\\n    This is a quick and approximate version of estimating these values instead\\n    of sampling from the posterior many times and then averaging those values of\\n    interest.\\n\\n    Internally, a total of batch_size trials are run through the model at once.\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_extxd:  Numpy array training data with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): Numpy array training external input with\\n        shape: # examples x # time steps x # external input dims\\n\\n    Returns:\\n      A dictionary with the estimated outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the output\\n        distribution parameters, e.g. (rates or mean and variances).\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        print('Setting number of posterior samples to process to : ', E)\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n\n    def trial_batches(N, per):\n        for i in range(0, N, per):\n            yield np.arange(i, min(i + per, N), dtype=np.int32)\n    for (batch_idx, es_idx) in enumerate(trial_batches(E_to_process, hps.batch_size)):\n        print('Running trial batch %d with %d trials' % (batch_idx + 1, len(es_idx)))\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=es_idx)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=False)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs",
            "def eval_model_runs_push_mean(self, data_name, data_extxd, ext_input_extxi=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns values of interest for the  model by pushing the means through\\n\\n    The mean values for both initial conditions and the control inputs are\\n    pushed through the model instead of sampling (as is done in\\n    eval_model_runs_avg_epoch).\\n    This is a quick and approximate version of estimating these values instead\\n    of sampling from the posterior many times and then averaging those values of\\n    interest.\\n\\n    Internally, a total of batch_size trials are run through the model at once.\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_extxd:  Numpy array training data with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): Numpy array training external input with\\n        shape: # examples x # time steps x # external input dims\\n\\n    Returns:\\n      A dictionary with the estimated outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the output\\n        distribution parameters, e.g. (rates or mean and variances).\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        print('Setting number of posterior samples to process to : ', E)\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n\n    def trial_batches(N, per):\n        for i in range(0, N, per):\n            yield np.arange(i, min(i + per, N), dtype=np.int32)\n    for (batch_idx, es_idx) in enumerate(trial_batches(E_to_process, hps.batch_size)):\n        print('Running trial batch %d with %d trials' % (batch_idx + 1, len(es_idx)))\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=es_idx)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=False)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs",
            "def eval_model_runs_push_mean(self, data_name, data_extxd, ext_input_extxi=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns values of interest for the  model by pushing the means through\\n\\n    The mean values for both initial conditions and the control inputs are\\n    pushed through the model instead of sampling (as is done in\\n    eval_model_runs_avg_epoch).\\n    This is a quick and approximate version of estimating these values instead\\n    of sampling from the posterior many times and then averaging those values of\\n    interest.\\n\\n    Internally, a total of batch_size trials are run through the model at once.\\n\\n    Args:\\n      data_name: The name of the data dict, to select which in/out matrices\\n        to use.\\n      data_extxd:  Numpy array training data with shape:\\n        # examples x # time steps x # dimensions\\n      ext_input_extxi (optional): Numpy array training external input with\\n        shape: # examples x # time steps x # external input dims\\n\\n    Returns:\\n      A dictionary with the estimated outputs of the model decoder, namely:\\n        prior g0 mean, prior g0 variance, approx. posterior mean, approx\\n        posterior mean, the generator initial conditions, the control inputs (if\\n        enabled), the state of the generator, the factors, and the output\\n        distribution parameters, e.g. (rates or mean and variances).\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    (E, T, D) = data_extxd.shape\n    E_to_process = hps.ps_nexamples_to_process\n    if E_to_process > E:\n        print('Setting number of posterior samples to process to : ', E)\n        E_to_process = E\n    if hps.ic_dim > 0:\n        prior_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        prior_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_mean = np.zeros([E_to_process, hps.ic_dim])\n        post_g0_logvar = np.zeros([E_to_process, hps.ic_dim])\n    if hps.co_dim > 0:\n        controller_outputs = np.zeros([E_to_process, T, hps.co_dim])\n    gen_ics = np.zeros([E_to_process, hps.gen_dim])\n    gen_states = np.zeros([E_to_process, T, hps.gen_dim])\n    factors = np.zeros([E_to_process, T, hps.factors_dim])\n    if hps.output_dist == 'poisson':\n        out_dist_params = np.zeros([E_to_process, T, D])\n    elif hps.output_dist == 'gaussian':\n        out_dist_params = np.zeros([E_to_process, T, D + D])\n    else:\n        assert False, 'NIY'\n    costs = np.zeros(E_to_process)\n    nll_bound_vaes = np.zeros(E_to_process)\n    nll_bound_iwaes = np.zeros(E_to_process)\n    train_steps = np.zeros(E_to_process)\n\n    def trial_batches(N, per):\n        for i in range(0, N, per):\n            yield np.arange(i, min(i + per, N), dtype=np.int32)\n    for (batch_idx, es_idx) in enumerate(trial_batches(E_to_process, hps.batch_size)):\n        print('Running trial batch %d with %d trials' % (batch_idx + 1, len(es_idx)))\n        (data_bxtxd, ext_input_bxtxi) = self.get_batch(data_extxd, ext_input_extxi, batch_size=batch_size, example_idxs=es_idx)\n        model_values = self.eval_model_runs_batch(data_name, data_bxtxd, ext_input_bxtxi, do_eval_cost=True, do_average_batch=False)\n        if self.hps.ic_dim > 0:\n            prior_g0_mean[es_idx, :] = model_values['prior_g0_mean']\n            prior_g0_logvar[es_idx, :] = model_values['prior_g0_logvar']\n            post_g0_mean[es_idx, :] = model_values['post_g0_mean']\n            post_g0_logvar[es_idx, :] = model_values['post_g0_logvar']\n        gen_ics[es_idx, :] = model_values['gen_ics']\n        if self.hps.co_dim > 0:\n            controller_outputs[es_idx, :, :] = model_values['controller_outputs']\n        gen_states[es_idx, :, :] = model_values['gen_states']\n        factors[es_idx, :, :] = model_values['factors']\n        out_dist_params[es_idx, :, :] = model_values['output_dist_params']\n        costs[es_idx] = model_values['costs']\n        nll_bound_vaes[es_idx] = model_values['nll_bound_vaes']\n        nll_bound_iwaes[es_idx] = model_values['nll_bound_iwaes']\n        train_steps[es_idx] = model_values['train_steps']\n    model_runs = {}\n    if self.hps.ic_dim > 0:\n        model_runs['prior_g0_mean'] = prior_g0_mean\n        model_runs['prior_g0_logvar'] = prior_g0_logvar\n        model_runs['post_g0_mean'] = post_g0_mean\n        model_runs['post_g0_logvar'] = post_g0_logvar\n    model_runs['gen_ics'] = gen_ics\n    if self.hps.co_dim > 0:\n        model_runs['controller_outputs'] = controller_outputs\n    model_runs['gen_states'] = gen_states\n    model_runs['factors'] = factors\n    model_runs['output_dist_params'] = out_dist_params\n    model_runs['costs'] = costs\n    model_runs['nll_bound_vaes'] = nll_bound_vaes\n    model_runs['nll_bound_iwaes'] = nll_bound_iwaes\n    model_runs['train_steps'] = train_steps\n    return model_runs"
        ]
    },
    {
        "func_name": "write_model_runs",
        "original": "def write_model_runs(self, datasets, output_fname=None, push_mean=False):\n    \"\"\"Run the model on the data in data_dict, and save the computed values.\n\n    LFADS generates a number of outputs for each examples, and these are all\n    saved.  They are:\n      The mean and variance of the prior of g0.\n      The mean and variance of approximate posterior of g0.\n      The control inputs (if enabled)\n      The initial conditions, g0, for all examples.\n      The generator states for all time.\n      The factors for all time.\n      The output distribution parameters (e.g. rates) for all time.\n\n    Args:\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\n      output_fname: a file name stem for the output files.\n      push_mean: if False (default), generates batch_size samples for each trial\n        and averages the results. if True, runs each trial once without noise,\n        pushing the posterior mean initial conditions and control inputs through\n        the trained model. False is used for posterior_sample_and_average, True\n        is used for posterior_push_mean.\n    \"\"\"\n    hps = self.hps\n    kind = hps.kind\n    for (data_name, data_dict) in datasets.items():\n        data_tuple = [('train', data_dict['train_data'], data_dict['train_ext_input']), ('valid', data_dict['valid_data'], data_dict['valid_ext_input'])]\n        for (data_kind, data_extxd, ext_input_extxi) in data_tuple:\n            if not output_fname:\n                fname = 'model_runs_' + data_name + '_' + data_kind + '_' + kind\n            else:\n                fname = output_fname + data_name + '_' + data_kind + '_' + kind\n            print('Writing data for %s data and kind %s.' % (data_name, data_kind))\n            if push_mean:\n                model_runs = self.eval_model_runs_push_mean(data_name, data_extxd, ext_input_extxi)\n            else:\n                model_runs = self.eval_model_runs_avg_epoch(data_name, data_extxd, ext_input_extxi)\n            full_fname = os.path.join(hps.lfads_save_dir, fname)\n            write_data(full_fname, model_runs, compression='gzip')\n            print('Done.')",
        "mutated": [
            "def write_model_runs(self, datasets, output_fname=None, push_mean=False):\n    if False:\n        i = 10\n    'Run the model on the data in data_dict, and save the computed values.\\n\\n    LFADS generates a number of outputs for each examples, and these are all\\n    saved.  They are:\\n      The mean and variance of the prior of g0.\\n      The mean and variance of approximate posterior of g0.\\n      The control inputs (if enabled)\\n      The initial conditions, g0, for all examples.\\n      The generator states for all time.\\n      The factors for all time.\\n      The output distribution parameters (e.g. rates) for all time.\\n\\n    Args:\\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\\n      output_fname: a file name stem for the output files.\\n      push_mean: if False (default), generates batch_size samples for each trial\\n        and averages the results. if True, runs each trial once without noise,\\n        pushing the posterior mean initial conditions and control inputs through\\n        the trained model. False is used for posterior_sample_and_average, True\\n        is used for posterior_push_mean.\\n    '\n    hps = self.hps\n    kind = hps.kind\n    for (data_name, data_dict) in datasets.items():\n        data_tuple = [('train', data_dict['train_data'], data_dict['train_ext_input']), ('valid', data_dict['valid_data'], data_dict['valid_ext_input'])]\n        for (data_kind, data_extxd, ext_input_extxi) in data_tuple:\n            if not output_fname:\n                fname = 'model_runs_' + data_name + '_' + data_kind + '_' + kind\n            else:\n                fname = output_fname + data_name + '_' + data_kind + '_' + kind\n            print('Writing data for %s data and kind %s.' % (data_name, data_kind))\n            if push_mean:\n                model_runs = self.eval_model_runs_push_mean(data_name, data_extxd, ext_input_extxi)\n            else:\n                model_runs = self.eval_model_runs_avg_epoch(data_name, data_extxd, ext_input_extxi)\n            full_fname = os.path.join(hps.lfads_save_dir, fname)\n            write_data(full_fname, model_runs, compression='gzip')\n            print('Done.')",
            "def write_model_runs(self, datasets, output_fname=None, push_mean=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Run the model on the data in data_dict, and save the computed values.\\n\\n    LFADS generates a number of outputs for each examples, and these are all\\n    saved.  They are:\\n      The mean and variance of the prior of g0.\\n      The mean and variance of approximate posterior of g0.\\n      The control inputs (if enabled)\\n      The initial conditions, g0, for all examples.\\n      The generator states for all time.\\n      The factors for all time.\\n      The output distribution parameters (e.g. rates) for all time.\\n\\n    Args:\\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\\n      output_fname: a file name stem for the output files.\\n      push_mean: if False (default), generates batch_size samples for each trial\\n        and averages the results. if True, runs each trial once without noise,\\n        pushing the posterior mean initial conditions and control inputs through\\n        the trained model. False is used for posterior_sample_and_average, True\\n        is used for posterior_push_mean.\\n    '\n    hps = self.hps\n    kind = hps.kind\n    for (data_name, data_dict) in datasets.items():\n        data_tuple = [('train', data_dict['train_data'], data_dict['train_ext_input']), ('valid', data_dict['valid_data'], data_dict['valid_ext_input'])]\n        for (data_kind, data_extxd, ext_input_extxi) in data_tuple:\n            if not output_fname:\n                fname = 'model_runs_' + data_name + '_' + data_kind + '_' + kind\n            else:\n                fname = output_fname + data_name + '_' + data_kind + '_' + kind\n            print('Writing data for %s data and kind %s.' % (data_name, data_kind))\n            if push_mean:\n                model_runs = self.eval_model_runs_push_mean(data_name, data_extxd, ext_input_extxi)\n            else:\n                model_runs = self.eval_model_runs_avg_epoch(data_name, data_extxd, ext_input_extxi)\n            full_fname = os.path.join(hps.lfads_save_dir, fname)\n            write_data(full_fname, model_runs, compression='gzip')\n            print('Done.')",
            "def write_model_runs(self, datasets, output_fname=None, push_mean=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Run the model on the data in data_dict, and save the computed values.\\n\\n    LFADS generates a number of outputs for each examples, and these are all\\n    saved.  They are:\\n      The mean and variance of the prior of g0.\\n      The mean and variance of approximate posterior of g0.\\n      The control inputs (if enabled)\\n      The initial conditions, g0, for all examples.\\n      The generator states for all time.\\n      The factors for all time.\\n      The output distribution parameters (e.g. rates) for all time.\\n\\n    Args:\\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\\n      output_fname: a file name stem for the output files.\\n      push_mean: if False (default), generates batch_size samples for each trial\\n        and averages the results. if True, runs each trial once without noise,\\n        pushing the posterior mean initial conditions and control inputs through\\n        the trained model. False is used for posterior_sample_and_average, True\\n        is used for posterior_push_mean.\\n    '\n    hps = self.hps\n    kind = hps.kind\n    for (data_name, data_dict) in datasets.items():\n        data_tuple = [('train', data_dict['train_data'], data_dict['train_ext_input']), ('valid', data_dict['valid_data'], data_dict['valid_ext_input'])]\n        for (data_kind, data_extxd, ext_input_extxi) in data_tuple:\n            if not output_fname:\n                fname = 'model_runs_' + data_name + '_' + data_kind + '_' + kind\n            else:\n                fname = output_fname + data_name + '_' + data_kind + '_' + kind\n            print('Writing data for %s data and kind %s.' % (data_name, data_kind))\n            if push_mean:\n                model_runs = self.eval_model_runs_push_mean(data_name, data_extxd, ext_input_extxi)\n            else:\n                model_runs = self.eval_model_runs_avg_epoch(data_name, data_extxd, ext_input_extxi)\n            full_fname = os.path.join(hps.lfads_save_dir, fname)\n            write_data(full_fname, model_runs, compression='gzip')\n            print('Done.')",
            "def write_model_runs(self, datasets, output_fname=None, push_mean=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Run the model on the data in data_dict, and save the computed values.\\n\\n    LFADS generates a number of outputs for each examples, and these are all\\n    saved.  They are:\\n      The mean and variance of the prior of g0.\\n      The mean and variance of approximate posterior of g0.\\n      The control inputs (if enabled)\\n      The initial conditions, g0, for all examples.\\n      The generator states for all time.\\n      The factors for all time.\\n      The output distribution parameters (e.g. rates) for all time.\\n\\n    Args:\\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\\n      output_fname: a file name stem for the output files.\\n      push_mean: if False (default), generates batch_size samples for each trial\\n        and averages the results. if True, runs each trial once without noise,\\n        pushing the posterior mean initial conditions and control inputs through\\n        the trained model. False is used for posterior_sample_and_average, True\\n        is used for posterior_push_mean.\\n    '\n    hps = self.hps\n    kind = hps.kind\n    for (data_name, data_dict) in datasets.items():\n        data_tuple = [('train', data_dict['train_data'], data_dict['train_ext_input']), ('valid', data_dict['valid_data'], data_dict['valid_ext_input'])]\n        for (data_kind, data_extxd, ext_input_extxi) in data_tuple:\n            if not output_fname:\n                fname = 'model_runs_' + data_name + '_' + data_kind + '_' + kind\n            else:\n                fname = output_fname + data_name + '_' + data_kind + '_' + kind\n            print('Writing data for %s data and kind %s.' % (data_name, data_kind))\n            if push_mean:\n                model_runs = self.eval_model_runs_push_mean(data_name, data_extxd, ext_input_extxi)\n            else:\n                model_runs = self.eval_model_runs_avg_epoch(data_name, data_extxd, ext_input_extxi)\n            full_fname = os.path.join(hps.lfads_save_dir, fname)\n            write_data(full_fname, model_runs, compression='gzip')\n            print('Done.')",
            "def write_model_runs(self, datasets, output_fname=None, push_mean=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Run the model on the data in data_dict, and save the computed values.\\n\\n    LFADS generates a number of outputs for each examples, and these are all\\n    saved.  They are:\\n      The mean and variance of the prior of g0.\\n      The mean and variance of approximate posterior of g0.\\n      The control inputs (if enabled)\\n      The initial conditions, g0, for all examples.\\n      The generator states for all time.\\n      The factors for all time.\\n      The output distribution parameters (e.g. rates) for all time.\\n\\n    Args:\\n      datasets: a dictionary of named data_dictionaries, see top of lfads.py\\n      output_fname: a file name stem for the output files.\\n      push_mean: if False (default), generates batch_size samples for each trial\\n        and averages the results. if True, runs each trial once without noise,\\n        pushing the posterior mean initial conditions and control inputs through\\n        the trained model. False is used for posterior_sample_and_average, True\\n        is used for posterior_push_mean.\\n    '\n    hps = self.hps\n    kind = hps.kind\n    for (data_name, data_dict) in datasets.items():\n        data_tuple = [('train', data_dict['train_data'], data_dict['train_ext_input']), ('valid', data_dict['valid_data'], data_dict['valid_ext_input'])]\n        for (data_kind, data_extxd, ext_input_extxi) in data_tuple:\n            if not output_fname:\n                fname = 'model_runs_' + data_name + '_' + data_kind + '_' + kind\n            else:\n                fname = output_fname + data_name + '_' + data_kind + '_' + kind\n            print('Writing data for %s data and kind %s.' % (data_name, data_kind))\n            if push_mean:\n                model_runs = self.eval_model_runs_push_mean(data_name, data_extxd, ext_input_extxi)\n            else:\n                model_runs = self.eval_model_runs_avg_epoch(data_name, data_extxd, ext_input_extxi)\n            full_fname = os.path.join(hps.lfads_save_dir, fname)\n            write_data(full_fname, model_runs, compression='gzip')\n            print('Done.')"
        ]
    },
    {
        "func_name": "write_model_samples",
        "original": "def write_model_samples(self, dataset_name, output_fname=None):\n    \"\"\"Use the prior distribution to generate batch_size number of samples\n    from the model.\n\n    LFADS generates a number of outputs for each sample, and these are all\n    saved.  They are:\n      The mean and variance of the prior of g0.\n      The control inputs (if enabled)\n      The initial conditions, g0, for all examples.\n      The generator states for all time.\n      The factors for all time.\n      The output distribution parameters (e.g. rates) for all time.\n\n    Args:\n      dataset_name: The name of the dataset to grab the factors -> rates\n      alignment matrices from.\n      output_fname: The name of the file in which to save the generated\n        samples.\n    \"\"\"\n    hps = self.hps\n    batch_size = hps.batch_size\n    print('Generating %d samples' % batch_size)\n    tf_vals = [self.factors, self.gen_states, self.gen_ics, self.cost, self.output_dist_params]\n    if hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar]\n    if hps.co_dim > 0:\n        tf_vals += [self.prior_zs_ar_con.samples_t]\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    session = tf.get_default_session()\n    feed_dict = {}\n    feed_dict[self.dataName] = dataset_name\n    feed_dict[self.keep_prob] = 1.0\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    output_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if hps.co_dim > 0:\n        prior_zs_ar_con = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    output_dist_params = list_t_bxn_to_tensor_bxtxn(output_dist_params)\n    if hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n    if hps.co_dim > 0:\n        prior_zs_ar_con = list_t_bxn_to_tensor_bxtxn(prior_zs_ar_con)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = output_dist_params\n    model_vals['costs'] = costs.reshape(1)\n    if hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n    if hps.co_dim > 0:\n        model_vals['prior_zs_ar_con'] = prior_zs_ar_con\n    full_fname = os.path.join(hps.lfads_save_dir, output_fname)\n    write_data(full_fname, model_vals, compression='gzip')\n    print('Done.')",
        "mutated": [
            "def write_model_samples(self, dataset_name, output_fname=None):\n    if False:\n        i = 10\n    'Use the prior distribution to generate batch_size number of samples\\n    from the model.\\n\\n    LFADS generates a number of outputs for each sample, and these are all\\n    saved.  They are:\\n      The mean and variance of the prior of g0.\\n      The control inputs (if enabled)\\n      The initial conditions, g0, for all examples.\\n      The generator states for all time.\\n      The factors for all time.\\n      The output distribution parameters (e.g. rates) for all time.\\n\\n    Args:\\n      dataset_name: The name of the dataset to grab the factors -> rates\\n      alignment matrices from.\\n      output_fname: The name of the file in which to save the generated\\n        samples.\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    print('Generating %d samples' % batch_size)\n    tf_vals = [self.factors, self.gen_states, self.gen_ics, self.cost, self.output_dist_params]\n    if hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar]\n    if hps.co_dim > 0:\n        tf_vals += [self.prior_zs_ar_con.samples_t]\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    session = tf.get_default_session()\n    feed_dict = {}\n    feed_dict[self.dataName] = dataset_name\n    feed_dict[self.keep_prob] = 1.0\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    output_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if hps.co_dim > 0:\n        prior_zs_ar_con = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    output_dist_params = list_t_bxn_to_tensor_bxtxn(output_dist_params)\n    if hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n    if hps.co_dim > 0:\n        prior_zs_ar_con = list_t_bxn_to_tensor_bxtxn(prior_zs_ar_con)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = output_dist_params\n    model_vals['costs'] = costs.reshape(1)\n    if hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n    if hps.co_dim > 0:\n        model_vals['prior_zs_ar_con'] = prior_zs_ar_con\n    full_fname = os.path.join(hps.lfads_save_dir, output_fname)\n    write_data(full_fname, model_vals, compression='gzip')\n    print('Done.')",
            "def write_model_samples(self, dataset_name, output_fname=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Use the prior distribution to generate batch_size number of samples\\n    from the model.\\n\\n    LFADS generates a number of outputs for each sample, and these are all\\n    saved.  They are:\\n      The mean and variance of the prior of g0.\\n      The control inputs (if enabled)\\n      The initial conditions, g0, for all examples.\\n      The generator states for all time.\\n      The factors for all time.\\n      The output distribution parameters (e.g. rates) for all time.\\n\\n    Args:\\n      dataset_name: The name of the dataset to grab the factors -> rates\\n      alignment matrices from.\\n      output_fname: The name of the file in which to save the generated\\n        samples.\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    print('Generating %d samples' % batch_size)\n    tf_vals = [self.factors, self.gen_states, self.gen_ics, self.cost, self.output_dist_params]\n    if hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar]\n    if hps.co_dim > 0:\n        tf_vals += [self.prior_zs_ar_con.samples_t]\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    session = tf.get_default_session()\n    feed_dict = {}\n    feed_dict[self.dataName] = dataset_name\n    feed_dict[self.keep_prob] = 1.0\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    output_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if hps.co_dim > 0:\n        prior_zs_ar_con = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    output_dist_params = list_t_bxn_to_tensor_bxtxn(output_dist_params)\n    if hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n    if hps.co_dim > 0:\n        prior_zs_ar_con = list_t_bxn_to_tensor_bxtxn(prior_zs_ar_con)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = output_dist_params\n    model_vals['costs'] = costs.reshape(1)\n    if hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n    if hps.co_dim > 0:\n        model_vals['prior_zs_ar_con'] = prior_zs_ar_con\n    full_fname = os.path.join(hps.lfads_save_dir, output_fname)\n    write_data(full_fname, model_vals, compression='gzip')\n    print('Done.')",
            "def write_model_samples(self, dataset_name, output_fname=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Use the prior distribution to generate batch_size number of samples\\n    from the model.\\n\\n    LFADS generates a number of outputs for each sample, and these are all\\n    saved.  They are:\\n      The mean and variance of the prior of g0.\\n      The control inputs (if enabled)\\n      The initial conditions, g0, for all examples.\\n      The generator states for all time.\\n      The factors for all time.\\n      The output distribution parameters (e.g. rates) for all time.\\n\\n    Args:\\n      dataset_name: The name of the dataset to grab the factors -> rates\\n      alignment matrices from.\\n      output_fname: The name of the file in which to save the generated\\n        samples.\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    print('Generating %d samples' % batch_size)\n    tf_vals = [self.factors, self.gen_states, self.gen_ics, self.cost, self.output_dist_params]\n    if hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar]\n    if hps.co_dim > 0:\n        tf_vals += [self.prior_zs_ar_con.samples_t]\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    session = tf.get_default_session()\n    feed_dict = {}\n    feed_dict[self.dataName] = dataset_name\n    feed_dict[self.keep_prob] = 1.0\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    output_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if hps.co_dim > 0:\n        prior_zs_ar_con = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    output_dist_params = list_t_bxn_to_tensor_bxtxn(output_dist_params)\n    if hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n    if hps.co_dim > 0:\n        prior_zs_ar_con = list_t_bxn_to_tensor_bxtxn(prior_zs_ar_con)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = output_dist_params\n    model_vals['costs'] = costs.reshape(1)\n    if hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n    if hps.co_dim > 0:\n        model_vals['prior_zs_ar_con'] = prior_zs_ar_con\n    full_fname = os.path.join(hps.lfads_save_dir, output_fname)\n    write_data(full_fname, model_vals, compression='gzip')\n    print('Done.')",
            "def write_model_samples(self, dataset_name, output_fname=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Use the prior distribution to generate batch_size number of samples\\n    from the model.\\n\\n    LFADS generates a number of outputs for each sample, and these are all\\n    saved.  They are:\\n      The mean and variance of the prior of g0.\\n      The control inputs (if enabled)\\n      The initial conditions, g0, for all examples.\\n      The generator states for all time.\\n      The factors for all time.\\n      The output distribution parameters (e.g. rates) for all time.\\n\\n    Args:\\n      dataset_name: The name of the dataset to grab the factors -> rates\\n      alignment matrices from.\\n      output_fname: The name of the file in which to save the generated\\n        samples.\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    print('Generating %d samples' % batch_size)\n    tf_vals = [self.factors, self.gen_states, self.gen_ics, self.cost, self.output_dist_params]\n    if hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar]\n    if hps.co_dim > 0:\n        tf_vals += [self.prior_zs_ar_con.samples_t]\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    session = tf.get_default_session()\n    feed_dict = {}\n    feed_dict[self.dataName] = dataset_name\n    feed_dict[self.keep_prob] = 1.0\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    output_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if hps.co_dim > 0:\n        prior_zs_ar_con = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    output_dist_params = list_t_bxn_to_tensor_bxtxn(output_dist_params)\n    if hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n    if hps.co_dim > 0:\n        prior_zs_ar_con = list_t_bxn_to_tensor_bxtxn(prior_zs_ar_con)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = output_dist_params\n    model_vals['costs'] = costs.reshape(1)\n    if hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n    if hps.co_dim > 0:\n        model_vals['prior_zs_ar_con'] = prior_zs_ar_con\n    full_fname = os.path.join(hps.lfads_save_dir, output_fname)\n    write_data(full_fname, model_vals, compression='gzip')\n    print('Done.')",
            "def write_model_samples(self, dataset_name, output_fname=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Use the prior distribution to generate batch_size number of samples\\n    from the model.\\n\\n    LFADS generates a number of outputs for each sample, and these are all\\n    saved.  They are:\\n      The mean and variance of the prior of g0.\\n      The control inputs (if enabled)\\n      The initial conditions, g0, for all examples.\\n      The generator states for all time.\\n      The factors for all time.\\n      The output distribution parameters (e.g. rates) for all time.\\n\\n    Args:\\n      dataset_name: The name of the dataset to grab the factors -> rates\\n      alignment matrices from.\\n      output_fname: The name of the file in which to save the generated\\n        samples.\\n    '\n    hps = self.hps\n    batch_size = hps.batch_size\n    print('Generating %d samples' % batch_size)\n    tf_vals = [self.factors, self.gen_states, self.gen_ics, self.cost, self.output_dist_params]\n    if hps.ic_dim > 0:\n        tf_vals += [self.prior_zs_g0.mean, self.prior_zs_g0.logvar]\n    if hps.co_dim > 0:\n        tf_vals += [self.prior_zs_ar_con.samples_t]\n    (tf_vals_flat, fidxs) = flatten(tf_vals)\n    session = tf.get_default_session()\n    feed_dict = {}\n    feed_dict[self.dataName] = dataset_name\n    feed_dict[self.keep_prob] = 1.0\n    np_vals_flat = session.run(tf_vals_flat, feed_dict=feed_dict)\n    ff = 0\n    factors = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_states = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    gen_ics = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    costs = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    output_dist_params = [np_vals_flat[f] for f in fidxs[ff]]\n    ff += 1\n    if hps.ic_dim > 0:\n        prior_g0_mean = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n        prior_g0_logvar = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    if hps.co_dim > 0:\n        prior_zs_ar_con = [np_vals_flat[f] for f in fidxs[ff]]\n        ff += 1\n    gen_ics = gen_ics[0]\n    costs = costs[0]\n    gen_states = list_t_bxn_to_tensor_bxtxn(gen_states)\n    factors = list_t_bxn_to_tensor_bxtxn(factors)\n    output_dist_params = list_t_bxn_to_tensor_bxtxn(output_dist_params)\n    if hps.ic_dim > 0:\n        prior_g0_mean = prior_g0_mean[0]\n        prior_g0_logvar = prior_g0_logvar[0]\n    if hps.co_dim > 0:\n        prior_zs_ar_con = list_t_bxn_to_tensor_bxtxn(prior_zs_ar_con)\n    model_vals = {}\n    model_vals['gen_ics'] = gen_ics\n    model_vals['gen_states'] = gen_states\n    model_vals['factors'] = factors\n    model_vals['output_dist_params'] = output_dist_params\n    model_vals['costs'] = costs.reshape(1)\n    if hps.ic_dim > 0:\n        model_vals['prior_g0_mean'] = prior_g0_mean\n        model_vals['prior_g0_logvar'] = prior_g0_logvar\n    if hps.co_dim > 0:\n        model_vals['prior_zs_ar_con'] = prior_zs_ar_con\n    full_fname = os.path.join(hps.lfads_save_dir, output_fname)\n    write_data(full_fname, model_vals, compression='gzip')\n    print('Done.')"
        ]
    },
    {
        "func_name": "eval_model_parameters",
        "original": "@staticmethod\ndef eval_model_parameters(use_nested=True, include_strs=None):\n    \"\"\"Evaluate and return all of the TF variables in the model.\n\n    Args:\n    use_nested (optional): For returning values, use a nested dictoinary, based\n      on variable scoping, or return all variables in a flat dictionary.\n    include_strs (optional): A list of strings to use as a filter, to reduce the\n      number of variables returned.  A variable name must contain at least one\n      string in include_strs as a sub-string in order to be returned.\n\n    Returns:\n      The parameters of the model.  This can be in a flat\n      dictionary, or a nested dictionary, where the nesting is by variable\n      scope.\n    \"\"\"\n    all_tf_vars = tf.global_variables()\n    session = tf.get_default_session()\n    all_tf_vars_eval = session.run(all_tf_vars)\n    vars_dict = {}\n    strs = ['LFADS']\n    if include_strs:\n        strs += include_strs\n    for (i, (var, var_eval)) in enumerate(zip(all_tf_vars, all_tf_vars_eval)):\n        if any((s in include_strs for s in var.name)):\n            if not isinstance(var_eval, np.ndarray):\n                print(var.name, ' is not numpy array, saving as numpy array\\n                with value: ', var_eval, type(var_eval))\n                e = np.array(var_eval)\n                print(e, type(e))\n            else:\n                e = var_eval\n            vars_dict[var.name] = e\n    if not use_nested:\n        return vars_dict\n    var_names = vars_dict.keys()\n    nested_vars_dict = {}\n    current_dict = nested_vars_dict\n    for (v, var_name) in enumerate(var_names):\n        var_split_name_list = var_name.split('/')\n        split_name_list_len = len(var_split_name_list)\n        current_dict = nested_vars_dict\n        for (p, part) in enumerate(var_split_name_list):\n            if p < split_name_list_len - 1:\n                if part in current_dict:\n                    current_dict = current_dict[part]\n                else:\n                    current_dict[part] = {}\n                    current_dict = current_dict[part]\n            else:\n                current_dict[part] = vars_dict[var_name]\n    return nested_vars_dict",
        "mutated": [
            "@staticmethod\ndef eval_model_parameters(use_nested=True, include_strs=None):\n    if False:\n        i = 10\n    'Evaluate and return all of the TF variables in the model.\\n\\n    Args:\\n    use_nested (optional): For returning values, use a nested dictoinary, based\\n      on variable scoping, or return all variables in a flat dictionary.\\n    include_strs (optional): A list of strings to use as a filter, to reduce the\\n      number of variables returned.  A variable name must contain at least one\\n      string in include_strs as a sub-string in order to be returned.\\n\\n    Returns:\\n      The parameters of the model.  This can be in a flat\\n      dictionary, or a nested dictionary, where the nesting is by variable\\n      scope.\\n    '\n    all_tf_vars = tf.global_variables()\n    session = tf.get_default_session()\n    all_tf_vars_eval = session.run(all_tf_vars)\n    vars_dict = {}\n    strs = ['LFADS']\n    if include_strs:\n        strs += include_strs\n    for (i, (var, var_eval)) in enumerate(zip(all_tf_vars, all_tf_vars_eval)):\n        if any((s in include_strs for s in var.name)):\n            if not isinstance(var_eval, np.ndarray):\n                print(var.name, ' is not numpy array, saving as numpy array\\n                with value: ', var_eval, type(var_eval))\n                e = np.array(var_eval)\n                print(e, type(e))\n            else:\n                e = var_eval\n            vars_dict[var.name] = e\n    if not use_nested:\n        return vars_dict\n    var_names = vars_dict.keys()\n    nested_vars_dict = {}\n    current_dict = nested_vars_dict\n    for (v, var_name) in enumerate(var_names):\n        var_split_name_list = var_name.split('/')\n        split_name_list_len = len(var_split_name_list)\n        current_dict = nested_vars_dict\n        for (p, part) in enumerate(var_split_name_list):\n            if p < split_name_list_len - 1:\n                if part in current_dict:\n                    current_dict = current_dict[part]\n                else:\n                    current_dict[part] = {}\n                    current_dict = current_dict[part]\n            else:\n                current_dict[part] = vars_dict[var_name]\n    return nested_vars_dict",
            "@staticmethod\ndef eval_model_parameters(use_nested=True, include_strs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Evaluate and return all of the TF variables in the model.\\n\\n    Args:\\n    use_nested (optional): For returning values, use a nested dictoinary, based\\n      on variable scoping, or return all variables in a flat dictionary.\\n    include_strs (optional): A list of strings to use as a filter, to reduce the\\n      number of variables returned.  A variable name must contain at least one\\n      string in include_strs as a sub-string in order to be returned.\\n\\n    Returns:\\n      The parameters of the model.  This can be in a flat\\n      dictionary, or a nested dictionary, where the nesting is by variable\\n      scope.\\n    '\n    all_tf_vars = tf.global_variables()\n    session = tf.get_default_session()\n    all_tf_vars_eval = session.run(all_tf_vars)\n    vars_dict = {}\n    strs = ['LFADS']\n    if include_strs:\n        strs += include_strs\n    for (i, (var, var_eval)) in enumerate(zip(all_tf_vars, all_tf_vars_eval)):\n        if any((s in include_strs for s in var.name)):\n            if not isinstance(var_eval, np.ndarray):\n                print(var.name, ' is not numpy array, saving as numpy array\\n                with value: ', var_eval, type(var_eval))\n                e = np.array(var_eval)\n                print(e, type(e))\n            else:\n                e = var_eval\n            vars_dict[var.name] = e\n    if not use_nested:\n        return vars_dict\n    var_names = vars_dict.keys()\n    nested_vars_dict = {}\n    current_dict = nested_vars_dict\n    for (v, var_name) in enumerate(var_names):\n        var_split_name_list = var_name.split('/')\n        split_name_list_len = len(var_split_name_list)\n        current_dict = nested_vars_dict\n        for (p, part) in enumerate(var_split_name_list):\n            if p < split_name_list_len - 1:\n                if part in current_dict:\n                    current_dict = current_dict[part]\n                else:\n                    current_dict[part] = {}\n                    current_dict = current_dict[part]\n            else:\n                current_dict[part] = vars_dict[var_name]\n    return nested_vars_dict",
            "@staticmethod\ndef eval_model_parameters(use_nested=True, include_strs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Evaluate and return all of the TF variables in the model.\\n\\n    Args:\\n    use_nested (optional): For returning values, use a nested dictoinary, based\\n      on variable scoping, or return all variables in a flat dictionary.\\n    include_strs (optional): A list of strings to use as a filter, to reduce the\\n      number of variables returned.  A variable name must contain at least one\\n      string in include_strs as a sub-string in order to be returned.\\n\\n    Returns:\\n      The parameters of the model.  This can be in a flat\\n      dictionary, or a nested dictionary, where the nesting is by variable\\n      scope.\\n    '\n    all_tf_vars = tf.global_variables()\n    session = tf.get_default_session()\n    all_tf_vars_eval = session.run(all_tf_vars)\n    vars_dict = {}\n    strs = ['LFADS']\n    if include_strs:\n        strs += include_strs\n    for (i, (var, var_eval)) in enumerate(zip(all_tf_vars, all_tf_vars_eval)):\n        if any((s in include_strs for s in var.name)):\n            if not isinstance(var_eval, np.ndarray):\n                print(var.name, ' is not numpy array, saving as numpy array\\n                with value: ', var_eval, type(var_eval))\n                e = np.array(var_eval)\n                print(e, type(e))\n            else:\n                e = var_eval\n            vars_dict[var.name] = e\n    if not use_nested:\n        return vars_dict\n    var_names = vars_dict.keys()\n    nested_vars_dict = {}\n    current_dict = nested_vars_dict\n    for (v, var_name) in enumerate(var_names):\n        var_split_name_list = var_name.split('/')\n        split_name_list_len = len(var_split_name_list)\n        current_dict = nested_vars_dict\n        for (p, part) in enumerate(var_split_name_list):\n            if p < split_name_list_len - 1:\n                if part in current_dict:\n                    current_dict = current_dict[part]\n                else:\n                    current_dict[part] = {}\n                    current_dict = current_dict[part]\n            else:\n                current_dict[part] = vars_dict[var_name]\n    return nested_vars_dict",
            "@staticmethod\ndef eval_model_parameters(use_nested=True, include_strs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Evaluate and return all of the TF variables in the model.\\n\\n    Args:\\n    use_nested (optional): For returning values, use a nested dictoinary, based\\n      on variable scoping, or return all variables in a flat dictionary.\\n    include_strs (optional): A list of strings to use as a filter, to reduce the\\n      number of variables returned.  A variable name must contain at least one\\n      string in include_strs as a sub-string in order to be returned.\\n\\n    Returns:\\n      The parameters of the model.  This can be in a flat\\n      dictionary, or a nested dictionary, where the nesting is by variable\\n      scope.\\n    '\n    all_tf_vars = tf.global_variables()\n    session = tf.get_default_session()\n    all_tf_vars_eval = session.run(all_tf_vars)\n    vars_dict = {}\n    strs = ['LFADS']\n    if include_strs:\n        strs += include_strs\n    for (i, (var, var_eval)) in enumerate(zip(all_tf_vars, all_tf_vars_eval)):\n        if any((s in include_strs for s in var.name)):\n            if not isinstance(var_eval, np.ndarray):\n                print(var.name, ' is not numpy array, saving as numpy array\\n                with value: ', var_eval, type(var_eval))\n                e = np.array(var_eval)\n                print(e, type(e))\n            else:\n                e = var_eval\n            vars_dict[var.name] = e\n    if not use_nested:\n        return vars_dict\n    var_names = vars_dict.keys()\n    nested_vars_dict = {}\n    current_dict = nested_vars_dict\n    for (v, var_name) in enumerate(var_names):\n        var_split_name_list = var_name.split('/')\n        split_name_list_len = len(var_split_name_list)\n        current_dict = nested_vars_dict\n        for (p, part) in enumerate(var_split_name_list):\n            if p < split_name_list_len - 1:\n                if part in current_dict:\n                    current_dict = current_dict[part]\n                else:\n                    current_dict[part] = {}\n                    current_dict = current_dict[part]\n            else:\n                current_dict[part] = vars_dict[var_name]\n    return nested_vars_dict",
            "@staticmethod\ndef eval_model_parameters(use_nested=True, include_strs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Evaluate and return all of the TF variables in the model.\\n\\n    Args:\\n    use_nested (optional): For returning values, use a nested dictoinary, based\\n      on variable scoping, or return all variables in a flat dictionary.\\n    include_strs (optional): A list of strings to use as a filter, to reduce the\\n      number of variables returned.  A variable name must contain at least one\\n      string in include_strs as a sub-string in order to be returned.\\n\\n    Returns:\\n      The parameters of the model.  This can be in a flat\\n      dictionary, or a nested dictionary, where the nesting is by variable\\n      scope.\\n    '\n    all_tf_vars = tf.global_variables()\n    session = tf.get_default_session()\n    all_tf_vars_eval = session.run(all_tf_vars)\n    vars_dict = {}\n    strs = ['LFADS']\n    if include_strs:\n        strs += include_strs\n    for (i, (var, var_eval)) in enumerate(zip(all_tf_vars, all_tf_vars_eval)):\n        if any((s in include_strs for s in var.name)):\n            if not isinstance(var_eval, np.ndarray):\n                print(var.name, ' is not numpy array, saving as numpy array\\n                with value: ', var_eval, type(var_eval))\n                e = np.array(var_eval)\n                print(e, type(e))\n            else:\n                e = var_eval\n            vars_dict[var.name] = e\n    if not use_nested:\n        return vars_dict\n    var_names = vars_dict.keys()\n    nested_vars_dict = {}\n    current_dict = nested_vars_dict\n    for (v, var_name) in enumerate(var_names):\n        var_split_name_list = var_name.split('/')\n        split_name_list_len = len(var_split_name_list)\n        current_dict = nested_vars_dict\n        for (p, part) in enumerate(var_split_name_list):\n            if p < split_name_list_len - 1:\n                if part in current_dict:\n                    current_dict = current_dict[part]\n                else:\n                    current_dict[part] = {}\n                    current_dict = current_dict[part]\n            else:\n                current_dict[part] = vars_dict[var_name]\n    return nested_vars_dict"
        ]
    },
    {
        "func_name": "spikify_rates",
        "original": "@staticmethod\ndef spikify_rates(rates_bxtxd):\n    \"\"\"Randomly spikify underlying rates according a Poisson distribution\n\n    Args:\n      rates_bxtxd: a numpy tensor with shape:\n\n    Returns:\n      A numpy array with the same shape as rates_bxtxd, but with the event\n      counts.\n    \"\"\"\n    (B, T, N) = rates_bxtxd.shape\n    assert all([B > 0, N > 0]), 'problems'\n    spikes_bxtxd = np.zeros([B, T, N], dtype=np.int32)\n    for b in range(B):\n        for t in range(T):\n            for n in range(N):\n                rate = rates_bxtxd[b, t, n]\n                count = np.random.poisson(rate)\n                spikes_bxtxd[b, t, n] = count\n    return spikes_bxtxd",
        "mutated": [
            "@staticmethod\ndef spikify_rates(rates_bxtxd):\n    if False:\n        i = 10\n    'Randomly spikify underlying rates according a Poisson distribution\\n\\n    Args:\\n      rates_bxtxd: a numpy tensor with shape:\\n\\n    Returns:\\n      A numpy array with the same shape as rates_bxtxd, but with the event\\n      counts.\\n    '\n    (B, T, N) = rates_bxtxd.shape\n    assert all([B > 0, N > 0]), 'problems'\n    spikes_bxtxd = np.zeros([B, T, N], dtype=np.int32)\n    for b in range(B):\n        for t in range(T):\n            for n in range(N):\n                rate = rates_bxtxd[b, t, n]\n                count = np.random.poisson(rate)\n                spikes_bxtxd[b, t, n] = count\n    return spikes_bxtxd",
            "@staticmethod\ndef spikify_rates(rates_bxtxd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Randomly spikify underlying rates according a Poisson distribution\\n\\n    Args:\\n      rates_bxtxd: a numpy tensor with shape:\\n\\n    Returns:\\n      A numpy array with the same shape as rates_bxtxd, but with the event\\n      counts.\\n    '\n    (B, T, N) = rates_bxtxd.shape\n    assert all([B > 0, N > 0]), 'problems'\n    spikes_bxtxd = np.zeros([B, T, N], dtype=np.int32)\n    for b in range(B):\n        for t in range(T):\n            for n in range(N):\n                rate = rates_bxtxd[b, t, n]\n                count = np.random.poisson(rate)\n                spikes_bxtxd[b, t, n] = count\n    return spikes_bxtxd",
            "@staticmethod\ndef spikify_rates(rates_bxtxd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Randomly spikify underlying rates according a Poisson distribution\\n\\n    Args:\\n      rates_bxtxd: a numpy tensor with shape:\\n\\n    Returns:\\n      A numpy array with the same shape as rates_bxtxd, but with the event\\n      counts.\\n    '\n    (B, T, N) = rates_bxtxd.shape\n    assert all([B > 0, N > 0]), 'problems'\n    spikes_bxtxd = np.zeros([B, T, N], dtype=np.int32)\n    for b in range(B):\n        for t in range(T):\n            for n in range(N):\n                rate = rates_bxtxd[b, t, n]\n                count = np.random.poisson(rate)\n                spikes_bxtxd[b, t, n] = count\n    return spikes_bxtxd",
            "@staticmethod\ndef spikify_rates(rates_bxtxd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Randomly spikify underlying rates according a Poisson distribution\\n\\n    Args:\\n      rates_bxtxd: a numpy tensor with shape:\\n\\n    Returns:\\n      A numpy array with the same shape as rates_bxtxd, but with the event\\n      counts.\\n    '\n    (B, T, N) = rates_bxtxd.shape\n    assert all([B > 0, N > 0]), 'problems'\n    spikes_bxtxd = np.zeros([B, T, N], dtype=np.int32)\n    for b in range(B):\n        for t in range(T):\n            for n in range(N):\n                rate = rates_bxtxd[b, t, n]\n                count = np.random.poisson(rate)\n                spikes_bxtxd[b, t, n] = count\n    return spikes_bxtxd",
            "@staticmethod\ndef spikify_rates(rates_bxtxd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Randomly spikify underlying rates according a Poisson distribution\\n\\n    Args:\\n      rates_bxtxd: a numpy tensor with shape:\\n\\n    Returns:\\n      A numpy array with the same shape as rates_bxtxd, but with the event\\n      counts.\\n    '\n    (B, T, N) = rates_bxtxd.shape\n    assert all([B > 0, N > 0]), 'problems'\n    spikes_bxtxd = np.zeros([B, T, N], dtype=np.int32)\n    for b in range(B):\n        for t in range(T):\n            for n in range(N):\n                rate = rates_bxtxd[b, t, n]\n                count = np.random.poisson(rate)\n                spikes_bxtxd[b, t, n] = count\n    return spikes_bxtxd"
        ]
    }
]