[
    {
        "func_name": "get_peft_model_state_dict",
        "original": "def get_peft_model_state_dict(model, state_dict=None, adapter_name='default', unwrap_compiled=False):\n    \"\"\"\n    Get the state dict of the Peft model.\n\n    Args:\n        model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,\n            the model should be the underlying model/unwrapped model (i.e. model.module).\n        state_dict (`dict`, *optional*, defaults to `None`):\n            The state dict of the model. If not provided, the state dict of the passed model will be used.\n        adapter_name (`str`, *optional*, defaults to `\"default\"`):\n            The name of the adapter whose state dict should be returned.\n        unwrap_compiled (`bool`, *optional*, defaults to `False`):\n            Whether to unwrap the model if torch.compile was used.\n    \"\"\"\n    if unwrap_compiled:\n        model = getattr(model, '_orig_mod', model)\n    config = model.peft_config[adapter_name]\n    if state_dict is None:\n        state_dict = model.state_dict()\n    if config.peft_type in (PeftType.LORA, PeftType.ADALORA):\n        bias = config.bias\n        if bias == 'none':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k}\n        elif bias == 'all':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k or 'bias' in k}\n        elif bias == 'lora_only':\n            to_return = {}\n            for k in state_dict:\n                if 'lora_' in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split('lora_')[0] + 'bias'\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n        to_return = {k: v for (k, v) in to_return.items() if 'lora_' in k and adapter_name in k or 'bias' in k}\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                rank_pattern = {k.replace(f'.{adapter_name}', ''): v for (k, v) in rank_pattern.items()}\n                config.rank_pattern = rank_pattern\n                to_return = model.resize_state_dict_by_rank_pattern(rank_pattern, to_return, adapter_name)\n    elif config.peft_type == PeftType.LOHA:\n        to_return = {k: state_dict[k] for k in state_dict if 'hada_' in k}\n    elif config.peft_type == PeftType.LOKR:\n        to_return = {k: state_dict[k] for k in state_dict if 'lokr_' in k}\n    elif config.peft_type == PeftType.ADAPTION_PROMPT:\n        to_return = {k: state_dict[k] for k in state_dict if k.split('.')[-1].startswith('adaption_')}\n    elif config.is_prompt_learning:\n        to_return = {}\n        if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n            to_return['prefix_task_cols'] = model.prompt_encoder[adapter_name].prefix_task_cols\n            to_return['prefix_task_rows'] = model.prompt_encoder[adapter_name].prefix_task_rows\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        elif config.inference_mode:\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        else:\n            prompt_embeddings = model.get_prompt_embedding_to_save(adapter_name)\n        to_return['prompt_embeddings'] = prompt_embeddings\n    elif config.peft_type == PeftType.IA3:\n        to_return = {k: state_dict[k] for k in state_dict if 'ia3_' in k}\n    else:\n        raise NotImplementedError\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in state_dict.items():\n            if any((f'{module_name}.modules_to_save.{adapter_name}' in key for module_name in model.modules_to_save)):\n                to_return[key.replace('modules_to_save.', '')] = value\n    to_return = {k.replace(f'.{adapter_name}', ''): v for (k, v) in to_return.items()}\n    return to_return",
        "mutated": [
            "def get_peft_model_state_dict(model, state_dict=None, adapter_name='default', unwrap_compiled=False):\n    if False:\n        i = 10\n    '\\n    Get the state dict of the Peft model.\\n\\n    Args:\\n        model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,\\n            the model should be the underlying model/unwrapped model (i.e. model.module).\\n        state_dict (`dict`, *optional*, defaults to `None`):\\n            The state dict of the model. If not provided, the state dict of the passed model will be used.\\n        adapter_name (`str`, *optional*, defaults to `\"default\"`):\\n            The name of the adapter whose state dict should be returned.\\n        unwrap_compiled (`bool`, *optional*, defaults to `False`):\\n            Whether to unwrap the model if torch.compile was used.\\n    '\n    if unwrap_compiled:\n        model = getattr(model, '_orig_mod', model)\n    config = model.peft_config[adapter_name]\n    if state_dict is None:\n        state_dict = model.state_dict()\n    if config.peft_type in (PeftType.LORA, PeftType.ADALORA):\n        bias = config.bias\n        if bias == 'none':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k}\n        elif bias == 'all':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k or 'bias' in k}\n        elif bias == 'lora_only':\n            to_return = {}\n            for k in state_dict:\n                if 'lora_' in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split('lora_')[0] + 'bias'\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n        to_return = {k: v for (k, v) in to_return.items() if 'lora_' in k and adapter_name in k or 'bias' in k}\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                rank_pattern = {k.replace(f'.{adapter_name}', ''): v for (k, v) in rank_pattern.items()}\n                config.rank_pattern = rank_pattern\n                to_return = model.resize_state_dict_by_rank_pattern(rank_pattern, to_return, adapter_name)\n    elif config.peft_type == PeftType.LOHA:\n        to_return = {k: state_dict[k] for k in state_dict if 'hada_' in k}\n    elif config.peft_type == PeftType.LOKR:\n        to_return = {k: state_dict[k] for k in state_dict if 'lokr_' in k}\n    elif config.peft_type == PeftType.ADAPTION_PROMPT:\n        to_return = {k: state_dict[k] for k in state_dict if k.split('.')[-1].startswith('adaption_')}\n    elif config.is_prompt_learning:\n        to_return = {}\n        if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n            to_return['prefix_task_cols'] = model.prompt_encoder[adapter_name].prefix_task_cols\n            to_return['prefix_task_rows'] = model.prompt_encoder[adapter_name].prefix_task_rows\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        elif config.inference_mode:\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        else:\n            prompt_embeddings = model.get_prompt_embedding_to_save(adapter_name)\n        to_return['prompt_embeddings'] = prompt_embeddings\n    elif config.peft_type == PeftType.IA3:\n        to_return = {k: state_dict[k] for k in state_dict if 'ia3_' in k}\n    else:\n        raise NotImplementedError\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in state_dict.items():\n            if any((f'{module_name}.modules_to_save.{adapter_name}' in key for module_name in model.modules_to_save)):\n                to_return[key.replace('modules_to_save.', '')] = value\n    to_return = {k.replace(f'.{adapter_name}', ''): v for (k, v) in to_return.items()}\n    return to_return",
            "def get_peft_model_state_dict(model, state_dict=None, adapter_name='default', unwrap_compiled=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Get the state dict of the Peft model.\\n\\n    Args:\\n        model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,\\n            the model should be the underlying model/unwrapped model (i.e. model.module).\\n        state_dict (`dict`, *optional*, defaults to `None`):\\n            The state dict of the model. If not provided, the state dict of the passed model will be used.\\n        adapter_name (`str`, *optional*, defaults to `\"default\"`):\\n            The name of the adapter whose state dict should be returned.\\n        unwrap_compiled (`bool`, *optional*, defaults to `False`):\\n            Whether to unwrap the model if torch.compile was used.\\n    '\n    if unwrap_compiled:\n        model = getattr(model, '_orig_mod', model)\n    config = model.peft_config[adapter_name]\n    if state_dict is None:\n        state_dict = model.state_dict()\n    if config.peft_type in (PeftType.LORA, PeftType.ADALORA):\n        bias = config.bias\n        if bias == 'none':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k}\n        elif bias == 'all':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k or 'bias' in k}\n        elif bias == 'lora_only':\n            to_return = {}\n            for k in state_dict:\n                if 'lora_' in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split('lora_')[0] + 'bias'\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n        to_return = {k: v for (k, v) in to_return.items() if 'lora_' in k and adapter_name in k or 'bias' in k}\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                rank_pattern = {k.replace(f'.{adapter_name}', ''): v for (k, v) in rank_pattern.items()}\n                config.rank_pattern = rank_pattern\n                to_return = model.resize_state_dict_by_rank_pattern(rank_pattern, to_return, adapter_name)\n    elif config.peft_type == PeftType.LOHA:\n        to_return = {k: state_dict[k] for k in state_dict if 'hada_' in k}\n    elif config.peft_type == PeftType.LOKR:\n        to_return = {k: state_dict[k] for k in state_dict if 'lokr_' in k}\n    elif config.peft_type == PeftType.ADAPTION_PROMPT:\n        to_return = {k: state_dict[k] for k in state_dict if k.split('.')[-1].startswith('adaption_')}\n    elif config.is_prompt_learning:\n        to_return = {}\n        if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n            to_return['prefix_task_cols'] = model.prompt_encoder[adapter_name].prefix_task_cols\n            to_return['prefix_task_rows'] = model.prompt_encoder[adapter_name].prefix_task_rows\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        elif config.inference_mode:\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        else:\n            prompt_embeddings = model.get_prompt_embedding_to_save(adapter_name)\n        to_return['prompt_embeddings'] = prompt_embeddings\n    elif config.peft_type == PeftType.IA3:\n        to_return = {k: state_dict[k] for k in state_dict if 'ia3_' in k}\n    else:\n        raise NotImplementedError\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in state_dict.items():\n            if any((f'{module_name}.modules_to_save.{adapter_name}' in key for module_name in model.modules_to_save)):\n                to_return[key.replace('modules_to_save.', '')] = value\n    to_return = {k.replace(f'.{adapter_name}', ''): v for (k, v) in to_return.items()}\n    return to_return",
            "def get_peft_model_state_dict(model, state_dict=None, adapter_name='default', unwrap_compiled=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Get the state dict of the Peft model.\\n\\n    Args:\\n        model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,\\n            the model should be the underlying model/unwrapped model (i.e. model.module).\\n        state_dict (`dict`, *optional*, defaults to `None`):\\n            The state dict of the model. If not provided, the state dict of the passed model will be used.\\n        adapter_name (`str`, *optional*, defaults to `\"default\"`):\\n            The name of the adapter whose state dict should be returned.\\n        unwrap_compiled (`bool`, *optional*, defaults to `False`):\\n            Whether to unwrap the model if torch.compile was used.\\n    '\n    if unwrap_compiled:\n        model = getattr(model, '_orig_mod', model)\n    config = model.peft_config[adapter_name]\n    if state_dict is None:\n        state_dict = model.state_dict()\n    if config.peft_type in (PeftType.LORA, PeftType.ADALORA):\n        bias = config.bias\n        if bias == 'none':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k}\n        elif bias == 'all':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k or 'bias' in k}\n        elif bias == 'lora_only':\n            to_return = {}\n            for k in state_dict:\n                if 'lora_' in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split('lora_')[0] + 'bias'\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n        to_return = {k: v for (k, v) in to_return.items() if 'lora_' in k and adapter_name in k or 'bias' in k}\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                rank_pattern = {k.replace(f'.{adapter_name}', ''): v for (k, v) in rank_pattern.items()}\n                config.rank_pattern = rank_pattern\n                to_return = model.resize_state_dict_by_rank_pattern(rank_pattern, to_return, adapter_name)\n    elif config.peft_type == PeftType.LOHA:\n        to_return = {k: state_dict[k] for k in state_dict if 'hada_' in k}\n    elif config.peft_type == PeftType.LOKR:\n        to_return = {k: state_dict[k] for k in state_dict if 'lokr_' in k}\n    elif config.peft_type == PeftType.ADAPTION_PROMPT:\n        to_return = {k: state_dict[k] for k in state_dict if k.split('.')[-1].startswith('adaption_')}\n    elif config.is_prompt_learning:\n        to_return = {}\n        if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n            to_return['prefix_task_cols'] = model.prompt_encoder[adapter_name].prefix_task_cols\n            to_return['prefix_task_rows'] = model.prompt_encoder[adapter_name].prefix_task_rows\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        elif config.inference_mode:\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        else:\n            prompt_embeddings = model.get_prompt_embedding_to_save(adapter_name)\n        to_return['prompt_embeddings'] = prompt_embeddings\n    elif config.peft_type == PeftType.IA3:\n        to_return = {k: state_dict[k] for k in state_dict if 'ia3_' in k}\n    else:\n        raise NotImplementedError\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in state_dict.items():\n            if any((f'{module_name}.modules_to_save.{adapter_name}' in key for module_name in model.modules_to_save)):\n                to_return[key.replace('modules_to_save.', '')] = value\n    to_return = {k.replace(f'.{adapter_name}', ''): v for (k, v) in to_return.items()}\n    return to_return",
            "def get_peft_model_state_dict(model, state_dict=None, adapter_name='default', unwrap_compiled=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Get the state dict of the Peft model.\\n\\n    Args:\\n        model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,\\n            the model should be the underlying model/unwrapped model (i.e. model.module).\\n        state_dict (`dict`, *optional*, defaults to `None`):\\n            The state dict of the model. If not provided, the state dict of the passed model will be used.\\n        adapter_name (`str`, *optional*, defaults to `\"default\"`):\\n            The name of the adapter whose state dict should be returned.\\n        unwrap_compiled (`bool`, *optional*, defaults to `False`):\\n            Whether to unwrap the model if torch.compile was used.\\n    '\n    if unwrap_compiled:\n        model = getattr(model, '_orig_mod', model)\n    config = model.peft_config[adapter_name]\n    if state_dict is None:\n        state_dict = model.state_dict()\n    if config.peft_type in (PeftType.LORA, PeftType.ADALORA):\n        bias = config.bias\n        if bias == 'none':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k}\n        elif bias == 'all':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k or 'bias' in k}\n        elif bias == 'lora_only':\n            to_return = {}\n            for k in state_dict:\n                if 'lora_' in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split('lora_')[0] + 'bias'\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n        to_return = {k: v for (k, v) in to_return.items() if 'lora_' in k and adapter_name in k or 'bias' in k}\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                rank_pattern = {k.replace(f'.{adapter_name}', ''): v for (k, v) in rank_pattern.items()}\n                config.rank_pattern = rank_pattern\n                to_return = model.resize_state_dict_by_rank_pattern(rank_pattern, to_return, adapter_name)\n    elif config.peft_type == PeftType.LOHA:\n        to_return = {k: state_dict[k] for k in state_dict if 'hada_' in k}\n    elif config.peft_type == PeftType.LOKR:\n        to_return = {k: state_dict[k] for k in state_dict if 'lokr_' in k}\n    elif config.peft_type == PeftType.ADAPTION_PROMPT:\n        to_return = {k: state_dict[k] for k in state_dict if k.split('.')[-1].startswith('adaption_')}\n    elif config.is_prompt_learning:\n        to_return = {}\n        if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n            to_return['prefix_task_cols'] = model.prompt_encoder[adapter_name].prefix_task_cols\n            to_return['prefix_task_rows'] = model.prompt_encoder[adapter_name].prefix_task_rows\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        elif config.inference_mode:\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        else:\n            prompt_embeddings = model.get_prompt_embedding_to_save(adapter_name)\n        to_return['prompt_embeddings'] = prompt_embeddings\n    elif config.peft_type == PeftType.IA3:\n        to_return = {k: state_dict[k] for k in state_dict if 'ia3_' in k}\n    else:\n        raise NotImplementedError\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in state_dict.items():\n            if any((f'{module_name}.modules_to_save.{adapter_name}' in key for module_name in model.modules_to_save)):\n                to_return[key.replace('modules_to_save.', '')] = value\n    to_return = {k.replace(f'.{adapter_name}', ''): v for (k, v) in to_return.items()}\n    return to_return",
            "def get_peft_model_state_dict(model, state_dict=None, adapter_name='default', unwrap_compiled=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Get the state dict of the Peft model.\\n\\n    Args:\\n        model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,\\n            the model should be the underlying model/unwrapped model (i.e. model.module).\\n        state_dict (`dict`, *optional*, defaults to `None`):\\n            The state dict of the model. If not provided, the state dict of the passed model will be used.\\n        adapter_name (`str`, *optional*, defaults to `\"default\"`):\\n            The name of the adapter whose state dict should be returned.\\n        unwrap_compiled (`bool`, *optional*, defaults to `False`):\\n            Whether to unwrap the model if torch.compile was used.\\n    '\n    if unwrap_compiled:\n        model = getattr(model, '_orig_mod', model)\n    config = model.peft_config[adapter_name]\n    if state_dict is None:\n        state_dict = model.state_dict()\n    if config.peft_type in (PeftType.LORA, PeftType.ADALORA):\n        bias = config.bias\n        if bias == 'none':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k}\n        elif bias == 'all':\n            to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k or 'bias' in k}\n        elif bias == 'lora_only':\n            to_return = {}\n            for k in state_dict:\n                if 'lora_' in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split('lora_')[0] + 'bias'\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n        to_return = {k: v for (k, v) in to_return.items() if 'lora_' in k and adapter_name in k or 'bias' in k}\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                rank_pattern = {k.replace(f'.{adapter_name}', ''): v for (k, v) in rank_pattern.items()}\n                config.rank_pattern = rank_pattern\n                to_return = model.resize_state_dict_by_rank_pattern(rank_pattern, to_return, adapter_name)\n    elif config.peft_type == PeftType.LOHA:\n        to_return = {k: state_dict[k] for k in state_dict if 'hada_' in k}\n    elif config.peft_type == PeftType.LOKR:\n        to_return = {k: state_dict[k] for k in state_dict if 'lokr_' in k}\n    elif config.peft_type == PeftType.ADAPTION_PROMPT:\n        to_return = {k: state_dict[k] for k in state_dict if k.split('.')[-1].startswith('adaption_')}\n    elif config.is_prompt_learning:\n        to_return = {}\n        if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n            to_return['prefix_task_cols'] = model.prompt_encoder[adapter_name].prefix_task_cols\n            to_return['prefix_task_rows'] = model.prompt_encoder[adapter_name].prefix_task_rows\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        elif config.inference_mode:\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        else:\n            prompt_embeddings = model.get_prompt_embedding_to_save(adapter_name)\n        to_return['prompt_embeddings'] = prompt_embeddings\n    elif config.peft_type == PeftType.IA3:\n        to_return = {k: state_dict[k] for k in state_dict if 'ia3_' in k}\n    else:\n        raise NotImplementedError\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in state_dict.items():\n            if any((f'{module_name}.modules_to_save.{adapter_name}' in key for module_name in model.modules_to_save)):\n                to_return[key.replace('modules_to_save.', '')] = value\n    to_return = {k.replace(f'.{adapter_name}', ''): v for (k, v) in to_return.items()}\n    return to_return"
        ]
    },
    {
        "func_name": "set_peft_model_state_dict",
        "original": "def set_peft_model_state_dict(model, peft_model_state_dict, adapter_name='default'):\n    \"\"\"\n    Set the state dict of the Peft model.\n\n    Args:\n        model ([`PeftModel`]): The Peft model.\n        peft_model_state_dict (`dict`): The state dict of the Peft model.\n    \"\"\"\n    config = model.peft_config[adapter_name]\n    state_dict = {}\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in peft_model_state_dict.items():\n            if any((module_name in key for module_name in model.modules_to_save)):\n                for module_name in model.modules_to_save:\n                    if module_name in key:\n                        key = key.replace(module_name, f'{module_name}.modules_to_save.{adapter_name}')\n                        break\n            state_dict[key] = value\n    else:\n        state_dict = peft_model_state_dict\n    if config.peft_type in (PeftType.LORA, PeftType.LOHA, PeftType.LOKR, PeftType.ADALORA, PeftType.IA3):\n        peft_model_state_dict = {}\n        parameter_prefix = {PeftType.IA3: 'ia3_', PeftType.LORA: 'lora_', PeftType.ADALORA: 'lora_', PeftType.LOHA: 'hada_', PeftType.LOKR: 'lokr_'}[config.peft_type]\n        for (k, v) in state_dict.items():\n            if parameter_prefix in k:\n                suffix = k.split(parameter_prefix)[1]\n                if '.' in suffix:\n                    suffix_to_replace = '.'.join(suffix.split('.')[1:])\n                    k = k.replace(suffix_to_replace, f'{adapter_name}.{suffix_to_replace}')\n                else:\n                    k = f'{k}.{adapter_name}'\n                peft_model_state_dict[k] = v\n            else:\n                peft_model_state_dict[k] = v\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                model.resize_modules_by_rank_pattern(rank_pattern, adapter_name)\n    elif config.is_prompt_learning or config.peft_type == PeftType.ADAPTION_PROMPT:\n        peft_model_state_dict = state_dict\n    else:\n        raise NotImplementedError\n    load_result = model.load_state_dict(peft_model_state_dict, strict=False)\n    if config.is_prompt_learning:\n        model.prompt_encoder[adapter_name].embedding.load_state_dict({'weight': peft_model_state_dict['prompt_embeddings']}, strict=True)\n    if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n        model.prompt_encoder[adapter_name].load_state_dict(peft_model_state_dict, strict=False)\n    return load_result",
        "mutated": [
            "def set_peft_model_state_dict(model, peft_model_state_dict, adapter_name='default'):\n    if False:\n        i = 10\n    '\\n    Set the state dict of the Peft model.\\n\\n    Args:\\n        model ([`PeftModel`]): The Peft model.\\n        peft_model_state_dict (`dict`): The state dict of the Peft model.\\n    '\n    config = model.peft_config[adapter_name]\n    state_dict = {}\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in peft_model_state_dict.items():\n            if any((module_name in key for module_name in model.modules_to_save)):\n                for module_name in model.modules_to_save:\n                    if module_name in key:\n                        key = key.replace(module_name, f'{module_name}.modules_to_save.{adapter_name}')\n                        break\n            state_dict[key] = value\n    else:\n        state_dict = peft_model_state_dict\n    if config.peft_type in (PeftType.LORA, PeftType.LOHA, PeftType.LOKR, PeftType.ADALORA, PeftType.IA3):\n        peft_model_state_dict = {}\n        parameter_prefix = {PeftType.IA3: 'ia3_', PeftType.LORA: 'lora_', PeftType.ADALORA: 'lora_', PeftType.LOHA: 'hada_', PeftType.LOKR: 'lokr_'}[config.peft_type]\n        for (k, v) in state_dict.items():\n            if parameter_prefix in k:\n                suffix = k.split(parameter_prefix)[1]\n                if '.' in suffix:\n                    suffix_to_replace = '.'.join(suffix.split('.')[1:])\n                    k = k.replace(suffix_to_replace, f'{adapter_name}.{suffix_to_replace}')\n                else:\n                    k = f'{k}.{adapter_name}'\n                peft_model_state_dict[k] = v\n            else:\n                peft_model_state_dict[k] = v\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                model.resize_modules_by_rank_pattern(rank_pattern, adapter_name)\n    elif config.is_prompt_learning or config.peft_type == PeftType.ADAPTION_PROMPT:\n        peft_model_state_dict = state_dict\n    else:\n        raise NotImplementedError\n    load_result = model.load_state_dict(peft_model_state_dict, strict=False)\n    if config.is_prompt_learning:\n        model.prompt_encoder[adapter_name].embedding.load_state_dict({'weight': peft_model_state_dict['prompt_embeddings']}, strict=True)\n    if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n        model.prompt_encoder[adapter_name].load_state_dict(peft_model_state_dict, strict=False)\n    return load_result",
            "def set_peft_model_state_dict(model, peft_model_state_dict, adapter_name='default'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Set the state dict of the Peft model.\\n\\n    Args:\\n        model ([`PeftModel`]): The Peft model.\\n        peft_model_state_dict (`dict`): The state dict of the Peft model.\\n    '\n    config = model.peft_config[adapter_name]\n    state_dict = {}\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in peft_model_state_dict.items():\n            if any((module_name in key for module_name in model.modules_to_save)):\n                for module_name in model.modules_to_save:\n                    if module_name in key:\n                        key = key.replace(module_name, f'{module_name}.modules_to_save.{adapter_name}')\n                        break\n            state_dict[key] = value\n    else:\n        state_dict = peft_model_state_dict\n    if config.peft_type in (PeftType.LORA, PeftType.LOHA, PeftType.LOKR, PeftType.ADALORA, PeftType.IA3):\n        peft_model_state_dict = {}\n        parameter_prefix = {PeftType.IA3: 'ia3_', PeftType.LORA: 'lora_', PeftType.ADALORA: 'lora_', PeftType.LOHA: 'hada_', PeftType.LOKR: 'lokr_'}[config.peft_type]\n        for (k, v) in state_dict.items():\n            if parameter_prefix in k:\n                suffix = k.split(parameter_prefix)[1]\n                if '.' in suffix:\n                    suffix_to_replace = '.'.join(suffix.split('.')[1:])\n                    k = k.replace(suffix_to_replace, f'{adapter_name}.{suffix_to_replace}')\n                else:\n                    k = f'{k}.{adapter_name}'\n                peft_model_state_dict[k] = v\n            else:\n                peft_model_state_dict[k] = v\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                model.resize_modules_by_rank_pattern(rank_pattern, adapter_name)\n    elif config.is_prompt_learning or config.peft_type == PeftType.ADAPTION_PROMPT:\n        peft_model_state_dict = state_dict\n    else:\n        raise NotImplementedError\n    load_result = model.load_state_dict(peft_model_state_dict, strict=False)\n    if config.is_prompt_learning:\n        model.prompt_encoder[adapter_name].embedding.load_state_dict({'weight': peft_model_state_dict['prompt_embeddings']}, strict=True)\n    if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n        model.prompt_encoder[adapter_name].load_state_dict(peft_model_state_dict, strict=False)\n    return load_result",
            "def set_peft_model_state_dict(model, peft_model_state_dict, adapter_name='default'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Set the state dict of the Peft model.\\n\\n    Args:\\n        model ([`PeftModel`]): The Peft model.\\n        peft_model_state_dict (`dict`): The state dict of the Peft model.\\n    '\n    config = model.peft_config[adapter_name]\n    state_dict = {}\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in peft_model_state_dict.items():\n            if any((module_name in key for module_name in model.modules_to_save)):\n                for module_name in model.modules_to_save:\n                    if module_name in key:\n                        key = key.replace(module_name, f'{module_name}.modules_to_save.{adapter_name}')\n                        break\n            state_dict[key] = value\n    else:\n        state_dict = peft_model_state_dict\n    if config.peft_type in (PeftType.LORA, PeftType.LOHA, PeftType.LOKR, PeftType.ADALORA, PeftType.IA3):\n        peft_model_state_dict = {}\n        parameter_prefix = {PeftType.IA3: 'ia3_', PeftType.LORA: 'lora_', PeftType.ADALORA: 'lora_', PeftType.LOHA: 'hada_', PeftType.LOKR: 'lokr_'}[config.peft_type]\n        for (k, v) in state_dict.items():\n            if parameter_prefix in k:\n                suffix = k.split(parameter_prefix)[1]\n                if '.' in suffix:\n                    suffix_to_replace = '.'.join(suffix.split('.')[1:])\n                    k = k.replace(suffix_to_replace, f'{adapter_name}.{suffix_to_replace}')\n                else:\n                    k = f'{k}.{adapter_name}'\n                peft_model_state_dict[k] = v\n            else:\n                peft_model_state_dict[k] = v\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                model.resize_modules_by_rank_pattern(rank_pattern, adapter_name)\n    elif config.is_prompt_learning or config.peft_type == PeftType.ADAPTION_PROMPT:\n        peft_model_state_dict = state_dict\n    else:\n        raise NotImplementedError\n    load_result = model.load_state_dict(peft_model_state_dict, strict=False)\n    if config.is_prompt_learning:\n        model.prompt_encoder[adapter_name].embedding.load_state_dict({'weight': peft_model_state_dict['prompt_embeddings']}, strict=True)\n    if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n        model.prompt_encoder[adapter_name].load_state_dict(peft_model_state_dict, strict=False)\n    return load_result",
            "def set_peft_model_state_dict(model, peft_model_state_dict, adapter_name='default'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Set the state dict of the Peft model.\\n\\n    Args:\\n        model ([`PeftModel`]): The Peft model.\\n        peft_model_state_dict (`dict`): The state dict of the Peft model.\\n    '\n    config = model.peft_config[adapter_name]\n    state_dict = {}\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in peft_model_state_dict.items():\n            if any((module_name in key for module_name in model.modules_to_save)):\n                for module_name in model.modules_to_save:\n                    if module_name in key:\n                        key = key.replace(module_name, f'{module_name}.modules_to_save.{adapter_name}')\n                        break\n            state_dict[key] = value\n    else:\n        state_dict = peft_model_state_dict\n    if config.peft_type in (PeftType.LORA, PeftType.LOHA, PeftType.LOKR, PeftType.ADALORA, PeftType.IA3):\n        peft_model_state_dict = {}\n        parameter_prefix = {PeftType.IA3: 'ia3_', PeftType.LORA: 'lora_', PeftType.ADALORA: 'lora_', PeftType.LOHA: 'hada_', PeftType.LOKR: 'lokr_'}[config.peft_type]\n        for (k, v) in state_dict.items():\n            if parameter_prefix in k:\n                suffix = k.split(parameter_prefix)[1]\n                if '.' in suffix:\n                    suffix_to_replace = '.'.join(suffix.split('.')[1:])\n                    k = k.replace(suffix_to_replace, f'{adapter_name}.{suffix_to_replace}')\n                else:\n                    k = f'{k}.{adapter_name}'\n                peft_model_state_dict[k] = v\n            else:\n                peft_model_state_dict[k] = v\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                model.resize_modules_by_rank_pattern(rank_pattern, adapter_name)\n    elif config.is_prompt_learning or config.peft_type == PeftType.ADAPTION_PROMPT:\n        peft_model_state_dict = state_dict\n    else:\n        raise NotImplementedError\n    load_result = model.load_state_dict(peft_model_state_dict, strict=False)\n    if config.is_prompt_learning:\n        model.prompt_encoder[adapter_name].embedding.load_state_dict({'weight': peft_model_state_dict['prompt_embeddings']}, strict=True)\n    if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n        model.prompt_encoder[adapter_name].load_state_dict(peft_model_state_dict, strict=False)\n    return load_result",
            "def set_peft_model_state_dict(model, peft_model_state_dict, adapter_name='default'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Set the state dict of the Peft model.\\n\\n    Args:\\n        model ([`PeftModel`]): The Peft model.\\n        peft_model_state_dict (`dict`): The state dict of the Peft model.\\n    '\n    config = model.peft_config[adapter_name]\n    state_dict = {}\n    if getattr(model, 'modules_to_save', None) is not None:\n        for (key, value) in peft_model_state_dict.items():\n            if any((module_name in key for module_name in model.modules_to_save)):\n                for module_name in model.modules_to_save:\n                    if module_name in key:\n                        key = key.replace(module_name, f'{module_name}.modules_to_save.{adapter_name}')\n                        break\n            state_dict[key] = value\n    else:\n        state_dict = peft_model_state_dict\n    if config.peft_type in (PeftType.LORA, PeftType.LOHA, PeftType.LOKR, PeftType.ADALORA, PeftType.IA3):\n        peft_model_state_dict = {}\n        parameter_prefix = {PeftType.IA3: 'ia3_', PeftType.LORA: 'lora_', PeftType.ADALORA: 'lora_', PeftType.LOHA: 'hada_', PeftType.LOKR: 'lokr_'}[config.peft_type]\n        for (k, v) in state_dict.items():\n            if parameter_prefix in k:\n                suffix = k.split(parameter_prefix)[1]\n                if '.' in suffix:\n                    suffix_to_replace = '.'.join(suffix.split('.')[1:])\n                    k = k.replace(suffix_to_replace, f'{adapter_name}.{suffix_to_replace}')\n                else:\n                    k = f'{k}.{adapter_name}'\n                peft_model_state_dict[k] = v\n            else:\n                peft_model_state_dict[k] = v\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                model.resize_modules_by_rank_pattern(rank_pattern, adapter_name)\n    elif config.is_prompt_learning or config.peft_type == PeftType.ADAPTION_PROMPT:\n        peft_model_state_dict = state_dict\n    else:\n        raise NotImplementedError\n    load_result = model.load_state_dict(peft_model_state_dict, strict=False)\n    if config.is_prompt_learning:\n        model.prompt_encoder[adapter_name].embedding.load_state_dict({'weight': peft_model_state_dict['prompt_embeddings']}, strict=True)\n    if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n        model.prompt_encoder[adapter_name].load_state_dict(peft_model_state_dict, strict=False)\n    return load_result"
        ]
    },
    {
        "func_name": "load_peft_weights",
        "original": "def load_peft_weights(model_id: str, device: Optional[str]=None, **hf_hub_download_kwargs) -> dict:\n    \"\"\"\n    A helper method to load the PEFT weights from the HuggingFace Hub or locally\n\n    Args:\n        model_id (`str`):\n            The local path to the adapter weights or the name of the adapter to load from the HuggingFace Hub.\n        device (`str`):\n            The device to load the weights onto.\n        hf_hub_download_kwargs (`dict`):\n            Additional arguments to pass to the `hf_hub_download` method when loading from the HuggingFace Hub.\n    \"\"\"\n    path = os.path.join(model_id, hf_hub_download_kwargs['subfolder']) if hf_hub_download_kwargs.get('subfolder', None) is not None else model_id\n    if device is None:\n        device = infer_device()\n    if os.path.exists(os.path.join(path, SAFETENSORS_WEIGHTS_NAME)):\n        filename = os.path.join(path, SAFETENSORS_WEIGHTS_NAME)\n        use_safetensors = True\n    elif os.path.exists(os.path.join(path, WEIGHTS_NAME)):\n        filename = os.path.join(path, WEIGHTS_NAME)\n        use_safetensors = False\n    else:\n        has_remote_safetensors_file = hub_file_exists(model_id, SAFETENSORS_WEIGHTS_NAME, revision=hf_hub_download_kwargs.get('revision', None), repo_type=hf_hub_download_kwargs.get('repo_type', None))\n        use_safetensors = has_remote_safetensors_file\n        if has_remote_safetensors_file:\n            filename = hf_hub_download(model_id, SAFETENSORS_WEIGHTS_NAME, **hf_hub_download_kwargs)\n        else:\n            try:\n                filename = hf_hub_download(model_id, WEIGHTS_NAME, **hf_hub_download_kwargs)\n            except EntryNotFoundError:\n                raise ValueError(f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. Please check that the file {WEIGHTS_NAME} or {SAFETENSORS_WEIGHTS_NAME} is present at {model_id}.\")\n    if use_safetensors:\n        adapters_weights = safe_load_file(filename, device=device)\n    else:\n        adapters_weights = torch.load(filename, map_location=torch.device(device))\n    return adapters_weights",
        "mutated": [
            "def load_peft_weights(model_id: str, device: Optional[str]=None, **hf_hub_download_kwargs) -> dict:\n    if False:\n        i = 10\n    '\\n    A helper method to load the PEFT weights from the HuggingFace Hub or locally\\n\\n    Args:\\n        model_id (`str`):\\n            The local path to the adapter weights or the name of the adapter to load from the HuggingFace Hub.\\n        device (`str`):\\n            The device to load the weights onto.\\n        hf_hub_download_kwargs (`dict`):\\n            Additional arguments to pass to the `hf_hub_download` method when loading from the HuggingFace Hub.\\n    '\n    path = os.path.join(model_id, hf_hub_download_kwargs['subfolder']) if hf_hub_download_kwargs.get('subfolder', None) is not None else model_id\n    if device is None:\n        device = infer_device()\n    if os.path.exists(os.path.join(path, SAFETENSORS_WEIGHTS_NAME)):\n        filename = os.path.join(path, SAFETENSORS_WEIGHTS_NAME)\n        use_safetensors = True\n    elif os.path.exists(os.path.join(path, WEIGHTS_NAME)):\n        filename = os.path.join(path, WEIGHTS_NAME)\n        use_safetensors = False\n    else:\n        has_remote_safetensors_file = hub_file_exists(model_id, SAFETENSORS_WEIGHTS_NAME, revision=hf_hub_download_kwargs.get('revision', None), repo_type=hf_hub_download_kwargs.get('repo_type', None))\n        use_safetensors = has_remote_safetensors_file\n        if has_remote_safetensors_file:\n            filename = hf_hub_download(model_id, SAFETENSORS_WEIGHTS_NAME, **hf_hub_download_kwargs)\n        else:\n            try:\n                filename = hf_hub_download(model_id, WEIGHTS_NAME, **hf_hub_download_kwargs)\n            except EntryNotFoundError:\n                raise ValueError(f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. Please check that the file {WEIGHTS_NAME} or {SAFETENSORS_WEIGHTS_NAME} is present at {model_id}.\")\n    if use_safetensors:\n        adapters_weights = safe_load_file(filename, device=device)\n    else:\n        adapters_weights = torch.load(filename, map_location=torch.device(device))\n    return adapters_weights",
            "def load_peft_weights(model_id: str, device: Optional[str]=None, **hf_hub_download_kwargs) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    A helper method to load the PEFT weights from the HuggingFace Hub or locally\\n\\n    Args:\\n        model_id (`str`):\\n            The local path to the adapter weights or the name of the adapter to load from the HuggingFace Hub.\\n        device (`str`):\\n            The device to load the weights onto.\\n        hf_hub_download_kwargs (`dict`):\\n            Additional arguments to pass to the `hf_hub_download` method when loading from the HuggingFace Hub.\\n    '\n    path = os.path.join(model_id, hf_hub_download_kwargs['subfolder']) if hf_hub_download_kwargs.get('subfolder', None) is not None else model_id\n    if device is None:\n        device = infer_device()\n    if os.path.exists(os.path.join(path, SAFETENSORS_WEIGHTS_NAME)):\n        filename = os.path.join(path, SAFETENSORS_WEIGHTS_NAME)\n        use_safetensors = True\n    elif os.path.exists(os.path.join(path, WEIGHTS_NAME)):\n        filename = os.path.join(path, WEIGHTS_NAME)\n        use_safetensors = False\n    else:\n        has_remote_safetensors_file = hub_file_exists(model_id, SAFETENSORS_WEIGHTS_NAME, revision=hf_hub_download_kwargs.get('revision', None), repo_type=hf_hub_download_kwargs.get('repo_type', None))\n        use_safetensors = has_remote_safetensors_file\n        if has_remote_safetensors_file:\n            filename = hf_hub_download(model_id, SAFETENSORS_WEIGHTS_NAME, **hf_hub_download_kwargs)\n        else:\n            try:\n                filename = hf_hub_download(model_id, WEIGHTS_NAME, **hf_hub_download_kwargs)\n            except EntryNotFoundError:\n                raise ValueError(f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. Please check that the file {WEIGHTS_NAME} or {SAFETENSORS_WEIGHTS_NAME} is present at {model_id}.\")\n    if use_safetensors:\n        adapters_weights = safe_load_file(filename, device=device)\n    else:\n        adapters_weights = torch.load(filename, map_location=torch.device(device))\n    return adapters_weights",
            "def load_peft_weights(model_id: str, device: Optional[str]=None, **hf_hub_download_kwargs) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    A helper method to load the PEFT weights from the HuggingFace Hub or locally\\n\\n    Args:\\n        model_id (`str`):\\n            The local path to the adapter weights or the name of the adapter to load from the HuggingFace Hub.\\n        device (`str`):\\n            The device to load the weights onto.\\n        hf_hub_download_kwargs (`dict`):\\n            Additional arguments to pass to the `hf_hub_download` method when loading from the HuggingFace Hub.\\n    '\n    path = os.path.join(model_id, hf_hub_download_kwargs['subfolder']) if hf_hub_download_kwargs.get('subfolder', None) is not None else model_id\n    if device is None:\n        device = infer_device()\n    if os.path.exists(os.path.join(path, SAFETENSORS_WEIGHTS_NAME)):\n        filename = os.path.join(path, SAFETENSORS_WEIGHTS_NAME)\n        use_safetensors = True\n    elif os.path.exists(os.path.join(path, WEIGHTS_NAME)):\n        filename = os.path.join(path, WEIGHTS_NAME)\n        use_safetensors = False\n    else:\n        has_remote_safetensors_file = hub_file_exists(model_id, SAFETENSORS_WEIGHTS_NAME, revision=hf_hub_download_kwargs.get('revision', None), repo_type=hf_hub_download_kwargs.get('repo_type', None))\n        use_safetensors = has_remote_safetensors_file\n        if has_remote_safetensors_file:\n            filename = hf_hub_download(model_id, SAFETENSORS_WEIGHTS_NAME, **hf_hub_download_kwargs)\n        else:\n            try:\n                filename = hf_hub_download(model_id, WEIGHTS_NAME, **hf_hub_download_kwargs)\n            except EntryNotFoundError:\n                raise ValueError(f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. Please check that the file {WEIGHTS_NAME} or {SAFETENSORS_WEIGHTS_NAME} is present at {model_id}.\")\n    if use_safetensors:\n        adapters_weights = safe_load_file(filename, device=device)\n    else:\n        adapters_weights = torch.load(filename, map_location=torch.device(device))\n    return adapters_weights",
            "def load_peft_weights(model_id: str, device: Optional[str]=None, **hf_hub_download_kwargs) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    A helper method to load the PEFT weights from the HuggingFace Hub or locally\\n\\n    Args:\\n        model_id (`str`):\\n            The local path to the adapter weights or the name of the adapter to load from the HuggingFace Hub.\\n        device (`str`):\\n            The device to load the weights onto.\\n        hf_hub_download_kwargs (`dict`):\\n            Additional arguments to pass to the `hf_hub_download` method when loading from the HuggingFace Hub.\\n    '\n    path = os.path.join(model_id, hf_hub_download_kwargs['subfolder']) if hf_hub_download_kwargs.get('subfolder', None) is not None else model_id\n    if device is None:\n        device = infer_device()\n    if os.path.exists(os.path.join(path, SAFETENSORS_WEIGHTS_NAME)):\n        filename = os.path.join(path, SAFETENSORS_WEIGHTS_NAME)\n        use_safetensors = True\n    elif os.path.exists(os.path.join(path, WEIGHTS_NAME)):\n        filename = os.path.join(path, WEIGHTS_NAME)\n        use_safetensors = False\n    else:\n        has_remote_safetensors_file = hub_file_exists(model_id, SAFETENSORS_WEIGHTS_NAME, revision=hf_hub_download_kwargs.get('revision', None), repo_type=hf_hub_download_kwargs.get('repo_type', None))\n        use_safetensors = has_remote_safetensors_file\n        if has_remote_safetensors_file:\n            filename = hf_hub_download(model_id, SAFETENSORS_WEIGHTS_NAME, **hf_hub_download_kwargs)\n        else:\n            try:\n                filename = hf_hub_download(model_id, WEIGHTS_NAME, **hf_hub_download_kwargs)\n            except EntryNotFoundError:\n                raise ValueError(f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. Please check that the file {WEIGHTS_NAME} or {SAFETENSORS_WEIGHTS_NAME} is present at {model_id}.\")\n    if use_safetensors:\n        adapters_weights = safe_load_file(filename, device=device)\n    else:\n        adapters_weights = torch.load(filename, map_location=torch.device(device))\n    return adapters_weights",
            "def load_peft_weights(model_id: str, device: Optional[str]=None, **hf_hub_download_kwargs) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    A helper method to load the PEFT weights from the HuggingFace Hub or locally\\n\\n    Args:\\n        model_id (`str`):\\n            The local path to the adapter weights or the name of the adapter to load from the HuggingFace Hub.\\n        device (`str`):\\n            The device to load the weights onto.\\n        hf_hub_download_kwargs (`dict`):\\n            Additional arguments to pass to the `hf_hub_download` method when loading from the HuggingFace Hub.\\n    '\n    path = os.path.join(model_id, hf_hub_download_kwargs['subfolder']) if hf_hub_download_kwargs.get('subfolder', None) is not None else model_id\n    if device is None:\n        device = infer_device()\n    if os.path.exists(os.path.join(path, SAFETENSORS_WEIGHTS_NAME)):\n        filename = os.path.join(path, SAFETENSORS_WEIGHTS_NAME)\n        use_safetensors = True\n    elif os.path.exists(os.path.join(path, WEIGHTS_NAME)):\n        filename = os.path.join(path, WEIGHTS_NAME)\n        use_safetensors = False\n    else:\n        has_remote_safetensors_file = hub_file_exists(model_id, SAFETENSORS_WEIGHTS_NAME, revision=hf_hub_download_kwargs.get('revision', None), repo_type=hf_hub_download_kwargs.get('repo_type', None))\n        use_safetensors = has_remote_safetensors_file\n        if has_remote_safetensors_file:\n            filename = hf_hub_download(model_id, SAFETENSORS_WEIGHTS_NAME, **hf_hub_download_kwargs)\n        else:\n            try:\n                filename = hf_hub_download(model_id, WEIGHTS_NAME, **hf_hub_download_kwargs)\n            except EntryNotFoundError:\n                raise ValueError(f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. Please check that the file {WEIGHTS_NAME} or {SAFETENSORS_WEIGHTS_NAME} is present at {model_id}.\")\n    if use_safetensors:\n        adapters_weights = safe_load_file(filename, device=device)\n    else:\n        adapters_weights = torch.load(filename, map_location=torch.device(device))\n    return adapters_weights"
        ]
    }
]