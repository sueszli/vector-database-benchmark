[
    {
        "func_name": "gen_op_vjp_str",
        "original": "def gen_op_vjp_str(op_class_name, op_grad_name, op_phi_name, op_info, op_grad_info):\n    bw_input_list = op_grad_info.input_name_list\n    fwd_input_and_mutable_attr_name_list = op_info.input_name_list + op_info.mutable_attribute_name_list\n    backward_input_code = ''\n    build_args_str = ''\n    grad_idx = -1\n    for idx in range(len(bw_input_list)):\n        bw_input_name = bw_input_list[idx]\n        build_args_str += bw_input_name + ', '\n        input_type = input_types_map[op_grad_info.input_type_list[idx]]\n        vjp_param_name = ''\n        index_0 = -1\n        if bw_input_name in fwd_input_and_mutable_attr_name_list:\n            vjp_param_name = 'inputs_'\n            index_0 = fwd_input_and_mutable_attr_name_list.index(bw_input_name)\n        elif bw_input_name in op_info.output_name_list:\n            vjp_param_name = 'outputs'\n            index_0 = op_info.output_name_list.index(bw_input_name)\n        else:\n            vjp_param_name = 'out_grads'\n            grad_idx += 1\n            index_0 = grad_idx\n        if op_grad_info.input_optional_list[idx] == 'true':\n            if input_type == 'Tensor':\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n            else:\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_VECTOR_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n        elif input_type == 'Tensor':\n            backward_input_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_type=input_type, input_name=bw_input_name, input_idx=index_0)\n        else:\n            backward_input_code += OP_VJP_FORWARD_MULTI_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n    op_attribute_list = op_grad_info.attribute_name_list\n    attribute_code = ''\n    build_attr_str = ''\n    array_attr_str = 'pir::ArrayAttribute'\n    for idx in range(len(op_attribute_list)):\n        if op_attribute_list[idx] in op_info.attribute_name_list:\n            if op_attribute_list[idx] in op_info.mutable_attribute_name_list:\n                attribute_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name='inputs_', input_type='Tensor', input_name=op_attribute_list[idx], input_idx=fwd_input_and_mutable_attr_name_list.index(op_attribute_list[idx]))\n                build_args_str += op_attribute_list[idx] + ', '\n            else:\n                func = 'data'\n                attr_type = op_grad_info.attribute_gen_arg_type_list[idx]\n                attr_type = attr_type.replace('const ', '')\n                attr_type = attr_type.replace('&', '')\n                if array_attr_str in op_grad_info.attribute_type_list[idx]:\n                    inner_type = op_grad_info.attribute_type_list[idx][len(array_attr_str) + 1:-1]\n                    func = 'data'\n                    if inner_type == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_ARRAY_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], inner_type=inner_type, func=func)\n                else:\n                    if op_grad_info.attribute_type_list[idx] == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], attr_parse_type=op_grad_info.attribute_type_list[idx], func=func)\n                build_attr_str += op_attribute_list[idx] + ', '\n        else:\n            attribute_code += OP_VJP_ATTRIBUTE_DEFAULT_TEMPLATE.format(attr_type=op_grad_info.attribute_gen_arg_type_list[idx], attr_name=op_attribute_list[idx], default_value=op_grad_info.attribute_default_value_list[idx])\n            build_attr_str += op_attribute_list[idx] + ', '\n    build_args_str += build_attr_str\n    op_phi_name_format = op_info.op_yaml_item['name']\n    call_vjp_code = OP_VJP_CALL_VJP_TEMPLATE.format(op_phi_name=op_phi_name_format, inputs_list=build_args_str)\n    stop_gradient_input_grad_code = OP_VJP_STOPGRADIENT_TEMPLATE\n    check_param = CHECK_INPUT_TEMPLATE.format(op_name=op_phi_name_format, inputs_size=len(fwd_input_and_mutable_attr_name_list), outputs_size=len(op_info.output_name_list), out_grads_size=grad_idx + 1)\n    str = OP_VJP_DEFINE_TEMPLATE.format(check_param=check_param, op_class_name=op_class_name, op_grad_name=op_grad_name, op_phi_name=op_phi_name, backward_input_code=backward_input_code, attribute_code=attribute_code, call_vjp_code=call_vjp_code, stop_gradient_input_grad_code=stop_gradient_input_grad_code)\n    return str",
        "mutated": [
            "def gen_op_vjp_str(op_class_name, op_grad_name, op_phi_name, op_info, op_grad_info):\n    if False:\n        i = 10\n    bw_input_list = op_grad_info.input_name_list\n    fwd_input_and_mutable_attr_name_list = op_info.input_name_list + op_info.mutable_attribute_name_list\n    backward_input_code = ''\n    build_args_str = ''\n    grad_idx = -1\n    for idx in range(len(bw_input_list)):\n        bw_input_name = bw_input_list[idx]\n        build_args_str += bw_input_name + ', '\n        input_type = input_types_map[op_grad_info.input_type_list[idx]]\n        vjp_param_name = ''\n        index_0 = -1\n        if bw_input_name in fwd_input_and_mutable_attr_name_list:\n            vjp_param_name = 'inputs_'\n            index_0 = fwd_input_and_mutable_attr_name_list.index(bw_input_name)\n        elif bw_input_name in op_info.output_name_list:\n            vjp_param_name = 'outputs'\n            index_0 = op_info.output_name_list.index(bw_input_name)\n        else:\n            vjp_param_name = 'out_grads'\n            grad_idx += 1\n            index_0 = grad_idx\n        if op_grad_info.input_optional_list[idx] == 'true':\n            if input_type == 'Tensor':\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n            else:\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_VECTOR_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n        elif input_type == 'Tensor':\n            backward_input_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_type=input_type, input_name=bw_input_name, input_idx=index_0)\n        else:\n            backward_input_code += OP_VJP_FORWARD_MULTI_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n    op_attribute_list = op_grad_info.attribute_name_list\n    attribute_code = ''\n    build_attr_str = ''\n    array_attr_str = 'pir::ArrayAttribute'\n    for idx in range(len(op_attribute_list)):\n        if op_attribute_list[idx] in op_info.attribute_name_list:\n            if op_attribute_list[idx] in op_info.mutable_attribute_name_list:\n                attribute_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name='inputs_', input_type='Tensor', input_name=op_attribute_list[idx], input_idx=fwd_input_and_mutable_attr_name_list.index(op_attribute_list[idx]))\n                build_args_str += op_attribute_list[idx] + ', '\n            else:\n                func = 'data'\n                attr_type = op_grad_info.attribute_gen_arg_type_list[idx]\n                attr_type = attr_type.replace('const ', '')\n                attr_type = attr_type.replace('&', '')\n                if array_attr_str in op_grad_info.attribute_type_list[idx]:\n                    inner_type = op_grad_info.attribute_type_list[idx][len(array_attr_str) + 1:-1]\n                    func = 'data'\n                    if inner_type == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_ARRAY_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], inner_type=inner_type, func=func)\n                else:\n                    if op_grad_info.attribute_type_list[idx] == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], attr_parse_type=op_grad_info.attribute_type_list[idx], func=func)\n                build_attr_str += op_attribute_list[idx] + ', '\n        else:\n            attribute_code += OP_VJP_ATTRIBUTE_DEFAULT_TEMPLATE.format(attr_type=op_grad_info.attribute_gen_arg_type_list[idx], attr_name=op_attribute_list[idx], default_value=op_grad_info.attribute_default_value_list[idx])\n            build_attr_str += op_attribute_list[idx] + ', '\n    build_args_str += build_attr_str\n    op_phi_name_format = op_info.op_yaml_item['name']\n    call_vjp_code = OP_VJP_CALL_VJP_TEMPLATE.format(op_phi_name=op_phi_name_format, inputs_list=build_args_str)\n    stop_gradient_input_grad_code = OP_VJP_STOPGRADIENT_TEMPLATE\n    check_param = CHECK_INPUT_TEMPLATE.format(op_name=op_phi_name_format, inputs_size=len(fwd_input_and_mutable_attr_name_list), outputs_size=len(op_info.output_name_list), out_grads_size=grad_idx + 1)\n    str = OP_VJP_DEFINE_TEMPLATE.format(check_param=check_param, op_class_name=op_class_name, op_grad_name=op_grad_name, op_phi_name=op_phi_name, backward_input_code=backward_input_code, attribute_code=attribute_code, call_vjp_code=call_vjp_code, stop_gradient_input_grad_code=stop_gradient_input_grad_code)\n    return str",
            "def gen_op_vjp_str(op_class_name, op_grad_name, op_phi_name, op_info, op_grad_info):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bw_input_list = op_grad_info.input_name_list\n    fwd_input_and_mutable_attr_name_list = op_info.input_name_list + op_info.mutable_attribute_name_list\n    backward_input_code = ''\n    build_args_str = ''\n    grad_idx = -1\n    for idx in range(len(bw_input_list)):\n        bw_input_name = bw_input_list[idx]\n        build_args_str += bw_input_name + ', '\n        input_type = input_types_map[op_grad_info.input_type_list[idx]]\n        vjp_param_name = ''\n        index_0 = -1\n        if bw_input_name in fwd_input_and_mutable_attr_name_list:\n            vjp_param_name = 'inputs_'\n            index_0 = fwd_input_and_mutable_attr_name_list.index(bw_input_name)\n        elif bw_input_name in op_info.output_name_list:\n            vjp_param_name = 'outputs'\n            index_0 = op_info.output_name_list.index(bw_input_name)\n        else:\n            vjp_param_name = 'out_grads'\n            grad_idx += 1\n            index_0 = grad_idx\n        if op_grad_info.input_optional_list[idx] == 'true':\n            if input_type == 'Tensor':\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n            else:\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_VECTOR_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n        elif input_type == 'Tensor':\n            backward_input_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_type=input_type, input_name=bw_input_name, input_idx=index_0)\n        else:\n            backward_input_code += OP_VJP_FORWARD_MULTI_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n    op_attribute_list = op_grad_info.attribute_name_list\n    attribute_code = ''\n    build_attr_str = ''\n    array_attr_str = 'pir::ArrayAttribute'\n    for idx in range(len(op_attribute_list)):\n        if op_attribute_list[idx] in op_info.attribute_name_list:\n            if op_attribute_list[idx] in op_info.mutable_attribute_name_list:\n                attribute_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name='inputs_', input_type='Tensor', input_name=op_attribute_list[idx], input_idx=fwd_input_and_mutable_attr_name_list.index(op_attribute_list[idx]))\n                build_args_str += op_attribute_list[idx] + ', '\n            else:\n                func = 'data'\n                attr_type = op_grad_info.attribute_gen_arg_type_list[idx]\n                attr_type = attr_type.replace('const ', '')\n                attr_type = attr_type.replace('&', '')\n                if array_attr_str in op_grad_info.attribute_type_list[idx]:\n                    inner_type = op_grad_info.attribute_type_list[idx][len(array_attr_str) + 1:-1]\n                    func = 'data'\n                    if inner_type == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_ARRAY_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], inner_type=inner_type, func=func)\n                else:\n                    if op_grad_info.attribute_type_list[idx] == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], attr_parse_type=op_grad_info.attribute_type_list[idx], func=func)\n                build_attr_str += op_attribute_list[idx] + ', '\n        else:\n            attribute_code += OP_VJP_ATTRIBUTE_DEFAULT_TEMPLATE.format(attr_type=op_grad_info.attribute_gen_arg_type_list[idx], attr_name=op_attribute_list[idx], default_value=op_grad_info.attribute_default_value_list[idx])\n            build_attr_str += op_attribute_list[idx] + ', '\n    build_args_str += build_attr_str\n    op_phi_name_format = op_info.op_yaml_item['name']\n    call_vjp_code = OP_VJP_CALL_VJP_TEMPLATE.format(op_phi_name=op_phi_name_format, inputs_list=build_args_str)\n    stop_gradient_input_grad_code = OP_VJP_STOPGRADIENT_TEMPLATE\n    check_param = CHECK_INPUT_TEMPLATE.format(op_name=op_phi_name_format, inputs_size=len(fwd_input_and_mutable_attr_name_list), outputs_size=len(op_info.output_name_list), out_grads_size=grad_idx + 1)\n    str = OP_VJP_DEFINE_TEMPLATE.format(check_param=check_param, op_class_name=op_class_name, op_grad_name=op_grad_name, op_phi_name=op_phi_name, backward_input_code=backward_input_code, attribute_code=attribute_code, call_vjp_code=call_vjp_code, stop_gradient_input_grad_code=stop_gradient_input_grad_code)\n    return str",
            "def gen_op_vjp_str(op_class_name, op_grad_name, op_phi_name, op_info, op_grad_info):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bw_input_list = op_grad_info.input_name_list\n    fwd_input_and_mutable_attr_name_list = op_info.input_name_list + op_info.mutable_attribute_name_list\n    backward_input_code = ''\n    build_args_str = ''\n    grad_idx = -1\n    for idx in range(len(bw_input_list)):\n        bw_input_name = bw_input_list[idx]\n        build_args_str += bw_input_name + ', '\n        input_type = input_types_map[op_grad_info.input_type_list[idx]]\n        vjp_param_name = ''\n        index_0 = -1\n        if bw_input_name in fwd_input_and_mutable_attr_name_list:\n            vjp_param_name = 'inputs_'\n            index_0 = fwd_input_and_mutable_attr_name_list.index(bw_input_name)\n        elif bw_input_name in op_info.output_name_list:\n            vjp_param_name = 'outputs'\n            index_0 = op_info.output_name_list.index(bw_input_name)\n        else:\n            vjp_param_name = 'out_grads'\n            grad_idx += 1\n            index_0 = grad_idx\n        if op_grad_info.input_optional_list[idx] == 'true':\n            if input_type == 'Tensor':\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n            else:\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_VECTOR_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n        elif input_type == 'Tensor':\n            backward_input_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_type=input_type, input_name=bw_input_name, input_idx=index_0)\n        else:\n            backward_input_code += OP_VJP_FORWARD_MULTI_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n    op_attribute_list = op_grad_info.attribute_name_list\n    attribute_code = ''\n    build_attr_str = ''\n    array_attr_str = 'pir::ArrayAttribute'\n    for idx in range(len(op_attribute_list)):\n        if op_attribute_list[idx] in op_info.attribute_name_list:\n            if op_attribute_list[idx] in op_info.mutable_attribute_name_list:\n                attribute_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name='inputs_', input_type='Tensor', input_name=op_attribute_list[idx], input_idx=fwd_input_and_mutable_attr_name_list.index(op_attribute_list[idx]))\n                build_args_str += op_attribute_list[idx] + ', '\n            else:\n                func = 'data'\n                attr_type = op_grad_info.attribute_gen_arg_type_list[idx]\n                attr_type = attr_type.replace('const ', '')\n                attr_type = attr_type.replace('&', '')\n                if array_attr_str in op_grad_info.attribute_type_list[idx]:\n                    inner_type = op_grad_info.attribute_type_list[idx][len(array_attr_str) + 1:-1]\n                    func = 'data'\n                    if inner_type == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_ARRAY_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], inner_type=inner_type, func=func)\n                else:\n                    if op_grad_info.attribute_type_list[idx] == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], attr_parse_type=op_grad_info.attribute_type_list[idx], func=func)\n                build_attr_str += op_attribute_list[idx] + ', '\n        else:\n            attribute_code += OP_VJP_ATTRIBUTE_DEFAULT_TEMPLATE.format(attr_type=op_grad_info.attribute_gen_arg_type_list[idx], attr_name=op_attribute_list[idx], default_value=op_grad_info.attribute_default_value_list[idx])\n            build_attr_str += op_attribute_list[idx] + ', '\n    build_args_str += build_attr_str\n    op_phi_name_format = op_info.op_yaml_item['name']\n    call_vjp_code = OP_VJP_CALL_VJP_TEMPLATE.format(op_phi_name=op_phi_name_format, inputs_list=build_args_str)\n    stop_gradient_input_grad_code = OP_VJP_STOPGRADIENT_TEMPLATE\n    check_param = CHECK_INPUT_TEMPLATE.format(op_name=op_phi_name_format, inputs_size=len(fwd_input_and_mutable_attr_name_list), outputs_size=len(op_info.output_name_list), out_grads_size=grad_idx + 1)\n    str = OP_VJP_DEFINE_TEMPLATE.format(check_param=check_param, op_class_name=op_class_name, op_grad_name=op_grad_name, op_phi_name=op_phi_name, backward_input_code=backward_input_code, attribute_code=attribute_code, call_vjp_code=call_vjp_code, stop_gradient_input_grad_code=stop_gradient_input_grad_code)\n    return str",
            "def gen_op_vjp_str(op_class_name, op_grad_name, op_phi_name, op_info, op_grad_info):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bw_input_list = op_grad_info.input_name_list\n    fwd_input_and_mutable_attr_name_list = op_info.input_name_list + op_info.mutable_attribute_name_list\n    backward_input_code = ''\n    build_args_str = ''\n    grad_idx = -1\n    for idx in range(len(bw_input_list)):\n        bw_input_name = bw_input_list[idx]\n        build_args_str += bw_input_name + ', '\n        input_type = input_types_map[op_grad_info.input_type_list[idx]]\n        vjp_param_name = ''\n        index_0 = -1\n        if bw_input_name in fwd_input_and_mutable_attr_name_list:\n            vjp_param_name = 'inputs_'\n            index_0 = fwd_input_and_mutable_attr_name_list.index(bw_input_name)\n        elif bw_input_name in op_info.output_name_list:\n            vjp_param_name = 'outputs'\n            index_0 = op_info.output_name_list.index(bw_input_name)\n        else:\n            vjp_param_name = 'out_grads'\n            grad_idx += 1\n            index_0 = grad_idx\n        if op_grad_info.input_optional_list[idx] == 'true':\n            if input_type == 'Tensor':\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n            else:\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_VECTOR_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n        elif input_type == 'Tensor':\n            backward_input_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_type=input_type, input_name=bw_input_name, input_idx=index_0)\n        else:\n            backward_input_code += OP_VJP_FORWARD_MULTI_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n    op_attribute_list = op_grad_info.attribute_name_list\n    attribute_code = ''\n    build_attr_str = ''\n    array_attr_str = 'pir::ArrayAttribute'\n    for idx in range(len(op_attribute_list)):\n        if op_attribute_list[idx] in op_info.attribute_name_list:\n            if op_attribute_list[idx] in op_info.mutable_attribute_name_list:\n                attribute_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name='inputs_', input_type='Tensor', input_name=op_attribute_list[idx], input_idx=fwd_input_and_mutable_attr_name_list.index(op_attribute_list[idx]))\n                build_args_str += op_attribute_list[idx] + ', '\n            else:\n                func = 'data'\n                attr_type = op_grad_info.attribute_gen_arg_type_list[idx]\n                attr_type = attr_type.replace('const ', '')\n                attr_type = attr_type.replace('&', '')\n                if array_attr_str in op_grad_info.attribute_type_list[idx]:\n                    inner_type = op_grad_info.attribute_type_list[idx][len(array_attr_str) + 1:-1]\n                    func = 'data'\n                    if inner_type == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_ARRAY_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], inner_type=inner_type, func=func)\n                else:\n                    if op_grad_info.attribute_type_list[idx] == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], attr_parse_type=op_grad_info.attribute_type_list[idx], func=func)\n                build_attr_str += op_attribute_list[idx] + ', '\n        else:\n            attribute_code += OP_VJP_ATTRIBUTE_DEFAULT_TEMPLATE.format(attr_type=op_grad_info.attribute_gen_arg_type_list[idx], attr_name=op_attribute_list[idx], default_value=op_grad_info.attribute_default_value_list[idx])\n            build_attr_str += op_attribute_list[idx] + ', '\n    build_args_str += build_attr_str\n    op_phi_name_format = op_info.op_yaml_item['name']\n    call_vjp_code = OP_VJP_CALL_VJP_TEMPLATE.format(op_phi_name=op_phi_name_format, inputs_list=build_args_str)\n    stop_gradient_input_grad_code = OP_VJP_STOPGRADIENT_TEMPLATE\n    check_param = CHECK_INPUT_TEMPLATE.format(op_name=op_phi_name_format, inputs_size=len(fwd_input_and_mutable_attr_name_list), outputs_size=len(op_info.output_name_list), out_grads_size=grad_idx + 1)\n    str = OP_VJP_DEFINE_TEMPLATE.format(check_param=check_param, op_class_name=op_class_name, op_grad_name=op_grad_name, op_phi_name=op_phi_name, backward_input_code=backward_input_code, attribute_code=attribute_code, call_vjp_code=call_vjp_code, stop_gradient_input_grad_code=stop_gradient_input_grad_code)\n    return str",
            "def gen_op_vjp_str(op_class_name, op_grad_name, op_phi_name, op_info, op_grad_info):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bw_input_list = op_grad_info.input_name_list\n    fwd_input_and_mutable_attr_name_list = op_info.input_name_list + op_info.mutable_attribute_name_list\n    backward_input_code = ''\n    build_args_str = ''\n    grad_idx = -1\n    for idx in range(len(bw_input_list)):\n        bw_input_name = bw_input_list[idx]\n        build_args_str += bw_input_name + ', '\n        input_type = input_types_map[op_grad_info.input_type_list[idx]]\n        vjp_param_name = ''\n        index_0 = -1\n        if bw_input_name in fwd_input_and_mutable_attr_name_list:\n            vjp_param_name = 'inputs_'\n            index_0 = fwd_input_and_mutable_attr_name_list.index(bw_input_name)\n        elif bw_input_name in op_info.output_name_list:\n            vjp_param_name = 'outputs'\n            index_0 = op_info.output_name_list.index(bw_input_name)\n        else:\n            vjp_param_name = 'out_grads'\n            grad_idx += 1\n            index_0 = grad_idx\n        if op_grad_info.input_optional_list[idx] == 'true':\n            if input_type == 'Tensor':\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n            else:\n                backward_input_code += OP_VJP_FORWARD_OPTIONAL_VECTOR_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n        elif input_type == 'Tensor':\n            backward_input_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_type=input_type, input_name=bw_input_name, input_idx=index_0)\n        else:\n            backward_input_code += OP_VJP_FORWARD_MULTI_INPUT_TEMPLATE.format(vjp_param_name=vjp_param_name, input_name=bw_input_name, input_idx=index_0)\n    op_attribute_list = op_grad_info.attribute_name_list\n    attribute_code = ''\n    build_attr_str = ''\n    array_attr_str = 'pir::ArrayAttribute'\n    for idx in range(len(op_attribute_list)):\n        if op_attribute_list[idx] in op_info.attribute_name_list:\n            if op_attribute_list[idx] in op_info.mutable_attribute_name_list:\n                attribute_code += OP_VJP_FORWARD_INPUT_OR_OUTPUT_TEMPLATE.format(vjp_param_name='inputs_', input_type='Tensor', input_name=op_attribute_list[idx], input_idx=fwd_input_and_mutable_attr_name_list.index(op_attribute_list[idx]))\n                build_args_str += op_attribute_list[idx] + ', '\n            else:\n                func = 'data'\n                attr_type = op_grad_info.attribute_gen_arg_type_list[idx]\n                attr_type = attr_type.replace('const ', '')\n                attr_type = attr_type.replace('&', '')\n                if array_attr_str in op_grad_info.attribute_type_list[idx]:\n                    inner_type = op_grad_info.attribute_type_list[idx][len(array_attr_str) + 1:-1]\n                    func = 'data'\n                    if inner_type == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_ARRAY_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], inner_type=inner_type, func=func)\n                else:\n                    if op_grad_info.attribute_type_list[idx] == 'pir::StrAttribute':\n                        func = 'AsString'\n                    attribute_code += OP_VJP_ATTRIBUTE_TEMPLATE.format(attr_type=attr_type, attr_name=op_attribute_list[idx], attr_parse_type=op_grad_info.attribute_type_list[idx], func=func)\n                build_attr_str += op_attribute_list[idx] + ', '\n        else:\n            attribute_code += OP_VJP_ATTRIBUTE_DEFAULT_TEMPLATE.format(attr_type=op_grad_info.attribute_gen_arg_type_list[idx], attr_name=op_attribute_list[idx], default_value=op_grad_info.attribute_default_value_list[idx])\n            build_attr_str += op_attribute_list[idx] + ', '\n    build_args_str += build_attr_str\n    op_phi_name_format = op_info.op_yaml_item['name']\n    call_vjp_code = OP_VJP_CALL_VJP_TEMPLATE.format(op_phi_name=op_phi_name_format, inputs_list=build_args_str)\n    stop_gradient_input_grad_code = OP_VJP_STOPGRADIENT_TEMPLATE\n    check_param = CHECK_INPUT_TEMPLATE.format(op_name=op_phi_name_format, inputs_size=len(fwd_input_and_mutable_attr_name_list), outputs_size=len(op_info.output_name_list), out_grads_size=grad_idx + 1)\n    str = OP_VJP_DEFINE_TEMPLATE.format(check_param=check_param, op_class_name=op_class_name, op_grad_name=op_grad_name, op_phi_name=op_phi_name, backward_input_code=backward_input_code, attribute_code=attribute_code, call_vjp_code=call_vjp_code, stop_gradient_input_grad_code=stop_gradient_input_grad_code)\n    return str"
        ]
    },
    {
        "func_name": "gen_op_infer_meta_str",
        "original": "def gen_op_infer_meta_str(op_info, op_class_name, op_info_items):\n    op_infer_meta_str = ''\n    if op_info.infer_meta_func:\n        op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info.infer_meta_func)\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info_items[op_info.invoke_map['func']].infer_meta_func)\n    return op_infer_meta_str",
        "mutated": [
            "def gen_op_infer_meta_str(op_info, op_class_name, op_info_items):\n    if False:\n        i = 10\n    op_infer_meta_str = ''\n    if op_info.infer_meta_func:\n        op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info.infer_meta_func)\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info_items[op_info.invoke_map['func']].infer_meta_func)\n    return op_infer_meta_str",
            "def gen_op_infer_meta_str(op_info, op_class_name, op_info_items):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    op_infer_meta_str = ''\n    if op_info.infer_meta_func:\n        op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info.infer_meta_func)\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info_items[op_info.invoke_map['func']].infer_meta_func)\n    return op_infer_meta_str",
            "def gen_op_infer_meta_str(op_info, op_class_name, op_info_items):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    op_infer_meta_str = ''\n    if op_info.infer_meta_func:\n        op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info.infer_meta_func)\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info_items[op_info.invoke_map['func']].infer_meta_func)\n    return op_infer_meta_str",
            "def gen_op_infer_meta_str(op_info, op_class_name, op_info_items):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    op_infer_meta_str = ''\n    if op_info.infer_meta_func:\n        op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info.infer_meta_func)\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info_items[op_info.invoke_map['func']].infer_meta_func)\n    return op_infer_meta_str",
            "def gen_op_infer_meta_str(op_info, op_class_name, op_info_items):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    op_infer_meta_str = ''\n    if op_info.infer_meta_func:\n        op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info.infer_meta_func)\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            op_infer_meta_str = OP_INFER_SHAPE_TEMPLATE.format(op_name=op_class_name, infer_meta_func=op_info_items[op_info.invoke_map['func']].infer_meta_func)\n    return op_infer_meta_str"
        ]
    },
    {
        "func_name": "gen_exclusive_interface_str",
        "original": "def gen_exclusive_interface_str(op_info, op_info_items):\n    exclusive_interface_str = ''\n    if op_info.infer_meta_func:\n        exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    if op_info.op_phi_name[0] not in vjp_interface_black_list:\n        exclusive_interface_str += '\\n  static std::vector<std::vector<pir::OpResult>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::OpResult>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);'\n    return exclusive_interface_str",
        "mutated": [
            "def gen_exclusive_interface_str(op_info, op_info_items):\n    if False:\n        i = 10\n    exclusive_interface_str = ''\n    if op_info.infer_meta_func:\n        exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    if op_info.op_phi_name[0] not in vjp_interface_black_list:\n        exclusive_interface_str += '\\n  static std::vector<std::vector<pir::OpResult>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::OpResult>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);'\n    return exclusive_interface_str",
            "def gen_exclusive_interface_str(op_info, op_info_items):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    exclusive_interface_str = ''\n    if op_info.infer_meta_func:\n        exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    if op_info.op_phi_name[0] not in vjp_interface_black_list:\n        exclusive_interface_str += '\\n  static std::vector<std::vector<pir::OpResult>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::OpResult>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);'\n    return exclusive_interface_str",
            "def gen_exclusive_interface_str(op_info, op_info_items):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    exclusive_interface_str = ''\n    if op_info.infer_meta_func:\n        exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    if op_info.op_phi_name[0] not in vjp_interface_black_list:\n        exclusive_interface_str += '\\n  static std::vector<std::vector<pir::OpResult>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::OpResult>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);'\n    return exclusive_interface_str",
            "def gen_exclusive_interface_str(op_info, op_info_items):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    exclusive_interface_str = ''\n    if op_info.infer_meta_func:\n        exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    if op_info.op_phi_name[0] not in vjp_interface_black_list:\n        exclusive_interface_str += '\\n  static std::vector<std::vector<pir::OpResult>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::OpResult>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);'\n    return exclusive_interface_str",
            "def gen_exclusive_interface_str(op_info, op_info_items):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    exclusive_interface_str = ''\n    if op_info.infer_meta_func:\n        exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    elif op_info.invoke_map and op_info.invoke_map['func'] in op_info_items:\n        if op_info_items[op_info.invoke_map['func']].infer_meta_func:\n            exclusive_interface_str += '  static void InferMeta( phi::InferMetaContext *infer_meta );'\n    if op_info.op_phi_name[0] not in vjp_interface_black_list:\n        exclusive_interface_str += '\\n  static std::vector<std::vector<pir::OpResult>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::OpResult>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);'\n    return exclusive_interface_str"
        ]
    }
]