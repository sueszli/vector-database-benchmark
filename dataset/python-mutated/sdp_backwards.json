[
    {
        "func_name": "__init__",
        "original": "def __init__(self, num_heads, in_proj_weight, in_proj_bias, out_proj):\n    super().__init__()\n    self.in_proj_weight = in_proj_weight\n    self.in_proj_bias = in_proj_bias\n    self.out_proj = out_proj\n    self.num_heads = num_heads",
        "mutated": [
            "def __init__(self, num_heads, in_proj_weight, in_proj_bias, out_proj):\n    if False:\n        i = 10\n    super().__init__()\n    self.in_proj_weight = in_proj_weight\n    self.in_proj_bias = in_proj_bias\n    self.out_proj = out_proj\n    self.num_heads = num_heads",
            "def __init__(self, num_heads, in_proj_weight, in_proj_bias, out_proj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.in_proj_weight = in_proj_weight\n    self.in_proj_bias = in_proj_bias\n    self.out_proj = out_proj\n    self.num_heads = num_heads",
            "def __init__(self, num_heads, in_proj_weight, in_proj_bias, out_proj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.in_proj_weight = in_proj_weight\n    self.in_proj_bias = in_proj_bias\n    self.out_proj = out_proj\n    self.num_heads = num_heads",
            "def __init__(self, num_heads, in_proj_weight, in_proj_bias, out_proj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.in_proj_weight = in_proj_weight\n    self.in_proj_bias = in_proj_bias\n    self.out_proj = out_proj\n    self.num_heads = num_heads",
            "def __init__(self, num_heads, in_proj_weight, in_proj_bias, out_proj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.in_proj_weight = in_proj_weight\n    self.in_proj_bias = in_proj_bias\n    self.out_proj = out_proj\n    self.num_heads = num_heads"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, query, key, value, mask):\n    if not (query is key and key is value):\n        raise NotImplementedError('query, key and value must be the same Tensor for now.')\n    if mask is not None:\n        raise NotImplementedError('mask is currently not supported.')\n    query_projected = torch.nn.functional.linear(query, self.in_proj_weight, self.in_proj_bias)\n    batch_size = query_projected.size(0)\n    embed_dim = query_projected.size(2)\n    head_dim = embed_dim // (self.num_heads * 3)\n    (query, key, value) = query_projected.chunk(3, -1)\n    query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    attn = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    attn = attn.transpose(1, 2).reshape(batch_size, -1, self.num_heads * head_dim)\n    return self.out_proj(attn)",
        "mutated": [
            "def forward(self, query, key, value, mask):\n    if False:\n        i = 10\n    if not (query is key and key is value):\n        raise NotImplementedError('query, key and value must be the same Tensor for now.')\n    if mask is not None:\n        raise NotImplementedError('mask is currently not supported.')\n    query_projected = torch.nn.functional.linear(query, self.in_proj_weight, self.in_proj_bias)\n    batch_size = query_projected.size(0)\n    embed_dim = query_projected.size(2)\n    head_dim = embed_dim // (self.num_heads * 3)\n    (query, key, value) = query_projected.chunk(3, -1)\n    query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    attn = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    attn = attn.transpose(1, 2).reshape(batch_size, -1, self.num_heads * head_dim)\n    return self.out_proj(attn)",
            "def forward(self, query, key, value, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not (query is key and key is value):\n        raise NotImplementedError('query, key and value must be the same Tensor for now.')\n    if mask is not None:\n        raise NotImplementedError('mask is currently not supported.')\n    query_projected = torch.nn.functional.linear(query, self.in_proj_weight, self.in_proj_bias)\n    batch_size = query_projected.size(0)\n    embed_dim = query_projected.size(2)\n    head_dim = embed_dim // (self.num_heads * 3)\n    (query, key, value) = query_projected.chunk(3, -1)\n    query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    attn = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    attn = attn.transpose(1, 2).reshape(batch_size, -1, self.num_heads * head_dim)\n    return self.out_proj(attn)",
            "def forward(self, query, key, value, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not (query is key and key is value):\n        raise NotImplementedError('query, key and value must be the same Tensor for now.')\n    if mask is not None:\n        raise NotImplementedError('mask is currently not supported.')\n    query_projected = torch.nn.functional.linear(query, self.in_proj_weight, self.in_proj_bias)\n    batch_size = query_projected.size(0)\n    embed_dim = query_projected.size(2)\n    head_dim = embed_dim // (self.num_heads * 3)\n    (query, key, value) = query_projected.chunk(3, -1)\n    query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    attn = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    attn = attn.transpose(1, 2).reshape(batch_size, -1, self.num_heads * head_dim)\n    return self.out_proj(attn)",
            "def forward(self, query, key, value, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not (query is key and key is value):\n        raise NotImplementedError('query, key and value must be the same Tensor for now.')\n    if mask is not None:\n        raise NotImplementedError('mask is currently not supported.')\n    query_projected = torch.nn.functional.linear(query, self.in_proj_weight, self.in_proj_bias)\n    batch_size = query_projected.size(0)\n    embed_dim = query_projected.size(2)\n    head_dim = embed_dim // (self.num_heads * 3)\n    (query, key, value) = query_projected.chunk(3, -1)\n    query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    attn = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    attn = attn.transpose(1, 2).reshape(batch_size, -1, self.num_heads * head_dim)\n    return self.out_proj(attn)",
            "def forward(self, query, key, value, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not (query is key and key is value):\n        raise NotImplementedError('query, key and value must be the same Tensor for now.')\n    if mask is not None:\n        raise NotImplementedError('mask is currently not supported.')\n    query_projected = torch.nn.functional.linear(query, self.in_proj_weight, self.in_proj_bias)\n    batch_size = query_projected.size(0)\n    embed_dim = query_projected.size(2)\n    head_dim = embed_dim // (self.num_heads * 3)\n    (query, key, value) = query_projected.chunk(3, -1)\n    query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    attn = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    attn = attn.transpose(1, 2).reshape(batch_size, -1, self.num_heads * head_dim)\n    return self.out_proj(attn)"
        ]
    },
    {
        "func_name": "build_composite_mha_from_nn_mha",
        "original": "def build_composite_mha_from_nn_mha(pt):\n    assert pt._qkv_same_embed_dim\n    in_proj_weight = pt.in_proj_weight\n    assert in_proj_weight is not None\n    assert pt.batch_first\n    return CompositeMHA(pt.num_heads, pt.in_proj_weight, pt.in_proj_bias, pt.out_proj)",
        "mutated": [
            "def build_composite_mha_from_nn_mha(pt):\n    if False:\n        i = 10\n    assert pt._qkv_same_embed_dim\n    in_proj_weight = pt.in_proj_weight\n    assert in_proj_weight is not None\n    assert pt.batch_first\n    return CompositeMHA(pt.num_heads, pt.in_proj_weight, pt.in_proj_bias, pt.out_proj)",
            "def build_composite_mha_from_nn_mha(pt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert pt._qkv_same_embed_dim\n    in_proj_weight = pt.in_proj_weight\n    assert in_proj_weight is not None\n    assert pt.batch_first\n    return CompositeMHA(pt.num_heads, pt.in_proj_weight, pt.in_proj_bias, pt.out_proj)",
            "def build_composite_mha_from_nn_mha(pt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert pt._qkv_same_embed_dim\n    in_proj_weight = pt.in_proj_weight\n    assert in_proj_weight is not None\n    assert pt.batch_first\n    return CompositeMHA(pt.num_heads, pt.in_proj_weight, pt.in_proj_bias, pt.out_proj)",
            "def build_composite_mha_from_nn_mha(pt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert pt._qkv_same_embed_dim\n    in_proj_weight = pt.in_proj_weight\n    assert in_proj_weight is not None\n    assert pt.batch_first\n    return CompositeMHA(pt.num_heads, pt.in_proj_weight, pt.in_proj_bias, pt.out_proj)",
            "def build_composite_mha_from_nn_mha(pt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert pt._qkv_same_embed_dim\n    in_proj_weight = pt.in_proj_weight\n    assert in_proj_weight is not None\n    assert pt.batch_first\n    return CompositeMHA(pt.num_heads, pt.in_proj_weight, pt.in_proj_bias, pt.out_proj)"
        ]
    },
    {
        "func_name": "forw_back",
        "original": "def forw_back(model, input, upward):\n    output = model(*input)\n    output.backward(upward)",
        "mutated": [
            "def forw_back(model, input, upward):\n    if False:\n        i = 10\n    output = model(*input)\n    output.backward(upward)",
            "def forw_back(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    output = model(*input)\n    output.backward(upward)",
            "def forw_back(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    output = model(*input)\n    output.backward(upward)",
            "def forw_back(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    output = model(*input)\n    output.backward(upward)",
            "def forw_back(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    output = model(*input)\n    output.backward(upward)"
        ]
    },
    {
        "func_name": "forw_back_fused",
        "original": "def forw_back_fused(model, input, upward):\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        output = model(*input)\n        output.backward(upward)",
        "mutated": [
            "def forw_back_fused(model, input, upward):\n    if False:\n        i = 10\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        output = model(*input)\n        output.backward(upward)",
            "def forw_back_fused(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        output = model(*input)\n        output.backward(upward)",
            "def forw_back_fused(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        output = model(*input)\n        output.backward(upward)",
            "def forw_back_fused(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        output = model(*input)\n        output.backward(upward)",
            "def forw_back_fused(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        output = model(*input)\n        output.backward(upward)"
        ]
    },
    {
        "func_name": "forw_back_eager",
        "original": "def forw_back_eager(model, input, upward):\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        output = model(*input)\n        output.backward(upward)",
        "mutated": [
            "def forw_back_eager(model, input, upward):\n    if False:\n        i = 10\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        output = model(*input)\n        output.backward(upward)",
            "def forw_back_eager(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        output = model(*input)\n        output.backward(upward)",
            "def forw_back_eager(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        output = model(*input)\n        output.backward(upward)",
            "def forw_back_eager(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        output = model(*input)\n        output.backward(upward)",
            "def forw_back_eager(model, input, upward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        output = model(*input)\n        output.backward(upward)"
        ]
    },
    {
        "func_name": "run_timing",
        "original": "def run_timing(min_run_time, batch_size, embed_dimension, num_heads, max_sequence_len, dtype):\n    dropout_p = 0.0\n    mask = None\n    pt = torch.nn.MultiheadAttention(embed_dim=embed_dimension, num_heads=num_heads, batch_first=True, dropout=dropout_p)\n    npt = pt.cuda().to(dtype)\n    cpt = build_composite_mha_from_nn_mha(npt)\n    x = torch.randn(batch_size, max_sequence_len, embed_dimension, dtype=dtype, device='cuda', requires_grad=True)\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        rand_fused_upward = cpt(x, x, x, mask).clone().detach()\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        rand_eager_upward = cpt(x, x, x, mask).clone().detach()\n    t0 = benchmark.Timer(stmt='forw_back_fused(cpt, (x,x,x,mask), rand_fused_upward)', globals={'forw_back_fused': forw_back_fused, 'cpt': cpt, 'x': x, 'rand_fused_upward': rand_fused_upward, 'mask': mask}, label=f'Fused SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    t1 = benchmark.Timer(stmt='forw_back_eager(cpt, (x,x,x,mask), rand_eager_upward)', globals={'forw_back_eager': forw_back_eager, 'cpt': cpt, 'x': x, 'rand_eager_upward': rand_eager_upward, 'mask': mask}, label=f'Eager SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    m0 = t0.blocked_autorange(min_run_time=min_run_time)\n    m1 = t1.blocked_autorange(min_run_time=min_run_time)\n    print(m0)\n    print(m1)\n    activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n    print('Profile for Fused'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_fused_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))\n    print('Profile for eager'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_eager_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))",
        "mutated": [
            "def run_timing(min_run_time, batch_size, embed_dimension, num_heads, max_sequence_len, dtype):\n    if False:\n        i = 10\n    dropout_p = 0.0\n    mask = None\n    pt = torch.nn.MultiheadAttention(embed_dim=embed_dimension, num_heads=num_heads, batch_first=True, dropout=dropout_p)\n    npt = pt.cuda().to(dtype)\n    cpt = build_composite_mha_from_nn_mha(npt)\n    x = torch.randn(batch_size, max_sequence_len, embed_dimension, dtype=dtype, device='cuda', requires_grad=True)\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        rand_fused_upward = cpt(x, x, x, mask).clone().detach()\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        rand_eager_upward = cpt(x, x, x, mask).clone().detach()\n    t0 = benchmark.Timer(stmt='forw_back_fused(cpt, (x,x,x,mask), rand_fused_upward)', globals={'forw_back_fused': forw_back_fused, 'cpt': cpt, 'x': x, 'rand_fused_upward': rand_fused_upward, 'mask': mask}, label=f'Fused SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    t1 = benchmark.Timer(stmt='forw_back_eager(cpt, (x,x,x,mask), rand_eager_upward)', globals={'forw_back_eager': forw_back_eager, 'cpt': cpt, 'x': x, 'rand_eager_upward': rand_eager_upward, 'mask': mask}, label=f'Eager SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    m0 = t0.blocked_autorange(min_run_time=min_run_time)\n    m1 = t1.blocked_autorange(min_run_time=min_run_time)\n    print(m0)\n    print(m1)\n    activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n    print('Profile for Fused'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_fused_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))\n    print('Profile for eager'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_eager_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))",
            "def run_timing(min_run_time, batch_size, embed_dimension, num_heads, max_sequence_len, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dropout_p = 0.0\n    mask = None\n    pt = torch.nn.MultiheadAttention(embed_dim=embed_dimension, num_heads=num_heads, batch_first=True, dropout=dropout_p)\n    npt = pt.cuda().to(dtype)\n    cpt = build_composite_mha_from_nn_mha(npt)\n    x = torch.randn(batch_size, max_sequence_len, embed_dimension, dtype=dtype, device='cuda', requires_grad=True)\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        rand_fused_upward = cpt(x, x, x, mask).clone().detach()\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        rand_eager_upward = cpt(x, x, x, mask).clone().detach()\n    t0 = benchmark.Timer(stmt='forw_back_fused(cpt, (x,x,x,mask), rand_fused_upward)', globals={'forw_back_fused': forw_back_fused, 'cpt': cpt, 'x': x, 'rand_fused_upward': rand_fused_upward, 'mask': mask}, label=f'Fused SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    t1 = benchmark.Timer(stmt='forw_back_eager(cpt, (x,x,x,mask), rand_eager_upward)', globals={'forw_back_eager': forw_back_eager, 'cpt': cpt, 'x': x, 'rand_eager_upward': rand_eager_upward, 'mask': mask}, label=f'Eager SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    m0 = t0.blocked_autorange(min_run_time=min_run_time)\n    m1 = t1.blocked_autorange(min_run_time=min_run_time)\n    print(m0)\n    print(m1)\n    activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n    print('Profile for Fused'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_fused_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))\n    print('Profile for eager'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_eager_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))",
            "def run_timing(min_run_time, batch_size, embed_dimension, num_heads, max_sequence_len, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dropout_p = 0.0\n    mask = None\n    pt = torch.nn.MultiheadAttention(embed_dim=embed_dimension, num_heads=num_heads, batch_first=True, dropout=dropout_p)\n    npt = pt.cuda().to(dtype)\n    cpt = build_composite_mha_from_nn_mha(npt)\n    x = torch.randn(batch_size, max_sequence_len, embed_dimension, dtype=dtype, device='cuda', requires_grad=True)\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        rand_fused_upward = cpt(x, x, x, mask).clone().detach()\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        rand_eager_upward = cpt(x, x, x, mask).clone().detach()\n    t0 = benchmark.Timer(stmt='forw_back_fused(cpt, (x,x,x,mask), rand_fused_upward)', globals={'forw_back_fused': forw_back_fused, 'cpt': cpt, 'x': x, 'rand_fused_upward': rand_fused_upward, 'mask': mask}, label=f'Fused SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    t1 = benchmark.Timer(stmt='forw_back_eager(cpt, (x,x,x,mask), rand_eager_upward)', globals={'forw_back_eager': forw_back_eager, 'cpt': cpt, 'x': x, 'rand_eager_upward': rand_eager_upward, 'mask': mask}, label=f'Eager SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    m0 = t0.blocked_autorange(min_run_time=min_run_time)\n    m1 = t1.blocked_autorange(min_run_time=min_run_time)\n    print(m0)\n    print(m1)\n    activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n    print('Profile for Fused'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_fused_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))\n    print('Profile for eager'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_eager_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))",
            "def run_timing(min_run_time, batch_size, embed_dimension, num_heads, max_sequence_len, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dropout_p = 0.0\n    mask = None\n    pt = torch.nn.MultiheadAttention(embed_dim=embed_dimension, num_heads=num_heads, batch_first=True, dropout=dropout_p)\n    npt = pt.cuda().to(dtype)\n    cpt = build_composite_mha_from_nn_mha(npt)\n    x = torch.randn(batch_size, max_sequence_len, embed_dimension, dtype=dtype, device='cuda', requires_grad=True)\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        rand_fused_upward = cpt(x, x, x, mask).clone().detach()\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        rand_eager_upward = cpt(x, x, x, mask).clone().detach()\n    t0 = benchmark.Timer(stmt='forw_back_fused(cpt, (x,x,x,mask), rand_fused_upward)', globals={'forw_back_fused': forw_back_fused, 'cpt': cpt, 'x': x, 'rand_fused_upward': rand_fused_upward, 'mask': mask}, label=f'Fused SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    t1 = benchmark.Timer(stmt='forw_back_eager(cpt, (x,x,x,mask), rand_eager_upward)', globals={'forw_back_eager': forw_back_eager, 'cpt': cpt, 'x': x, 'rand_eager_upward': rand_eager_upward, 'mask': mask}, label=f'Eager SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    m0 = t0.blocked_autorange(min_run_time=min_run_time)\n    m1 = t1.blocked_autorange(min_run_time=min_run_time)\n    print(m0)\n    print(m1)\n    activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n    print('Profile for Fused'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_fused_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))\n    print('Profile for eager'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_eager_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))",
            "def run_timing(min_run_time, batch_size, embed_dimension, num_heads, max_sequence_len, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dropout_p = 0.0\n    mask = None\n    pt = torch.nn.MultiheadAttention(embed_dim=embed_dimension, num_heads=num_heads, batch_first=True, dropout=dropout_p)\n    npt = pt.cuda().to(dtype)\n    cpt = build_composite_mha_from_nn_mha(npt)\n    x = torch.randn(batch_size, max_sequence_len, embed_dimension, dtype=dtype, device='cuda', requires_grad=True)\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        rand_fused_upward = cpt(x, x, x, mask).clone().detach()\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        rand_eager_upward = cpt(x, x, x, mask).clone().detach()\n    t0 = benchmark.Timer(stmt='forw_back_fused(cpt, (x,x,x,mask), rand_fused_upward)', globals={'forw_back_fused': forw_back_fused, 'cpt': cpt, 'x': x, 'rand_fused_upward': rand_fused_upward, 'mask': mask}, label=f'Fused SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    t1 = benchmark.Timer(stmt='forw_back_eager(cpt, (x,x,x,mask), rand_eager_upward)', globals={'forw_back_eager': forw_back_eager, 'cpt': cpt, 'x': x, 'rand_eager_upward': rand_eager_upward, 'mask': mask}, label=f'Eager SDP forward and backward batch_size={batch_size} max_sequence_len={max_sequence_len} num_heads={num_heads} embed_dimension={embed_dimension} dtype={dtype}', num_threads=torch.get_num_threads())\n    m0 = t0.blocked_autorange(min_run_time=min_run_time)\n    m1 = t1.blocked_autorange(min_run_time=min_run_time)\n    print(m0)\n    print(m1)\n    activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n    print('Profile for Fused'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_mem_efficient=True):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_fused_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))\n    print('Profile for eager'.center(200, '-'))\n    with torch.backends.cuda.sdp_kernel(enable_math=True, enable_mem_efficient=False):\n        with profile(activities=activities, record_shapes=False, with_stack=True) as prof:\n            with record_function('Fused SDP forward and backward'):\n                for _ in range(20):\n                    forw_back(cpt, (x, x, x, mask), rand_eager_upward)\n    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=20))"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    seed = 123\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    min_run_time = 10\n    batch_size = 64\n    num_heads = 32\n    max_seq_len = 256\n    embed_dim = 1024\n    dtype = torch.bfloat16\n    print(f'Running timing for batch_size={batch_size} max_sequence_len={max_seq_len} num_heads={num_heads} embed_dimension={embed_dim} dtype={dtype}')\n    run_timing(min_run_time, batch_size, embed_dim, num_heads, max_seq_len, dtype)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    seed = 123\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    min_run_time = 10\n    batch_size = 64\n    num_heads = 32\n    max_seq_len = 256\n    embed_dim = 1024\n    dtype = torch.bfloat16\n    print(f'Running timing for batch_size={batch_size} max_sequence_len={max_seq_len} num_heads={num_heads} embed_dimension={embed_dim} dtype={dtype}')\n    run_timing(min_run_time, batch_size, embed_dim, num_heads, max_seq_len, dtype)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    seed = 123\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    min_run_time = 10\n    batch_size = 64\n    num_heads = 32\n    max_seq_len = 256\n    embed_dim = 1024\n    dtype = torch.bfloat16\n    print(f'Running timing for batch_size={batch_size} max_sequence_len={max_seq_len} num_heads={num_heads} embed_dimension={embed_dim} dtype={dtype}')\n    run_timing(min_run_time, batch_size, embed_dim, num_heads, max_seq_len, dtype)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    seed = 123\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    min_run_time = 10\n    batch_size = 64\n    num_heads = 32\n    max_seq_len = 256\n    embed_dim = 1024\n    dtype = torch.bfloat16\n    print(f'Running timing for batch_size={batch_size} max_sequence_len={max_seq_len} num_heads={num_heads} embed_dimension={embed_dim} dtype={dtype}')\n    run_timing(min_run_time, batch_size, embed_dim, num_heads, max_seq_len, dtype)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    seed = 123\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    min_run_time = 10\n    batch_size = 64\n    num_heads = 32\n    max_seq_len = 256\n    embed_dim = 1024\n    dtype = torch.bfloat16\n    print(f'Running timing for batch_size={batch_size} max_sequence_len={max_seq_len} num_heads={num_heads} embed_dimension={embed_dim} dtype={dtype}')\n    run_timing(min_run_time, batch_size, embed_dim, num_heads, max_seq_len, dtype)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    seed = 123\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    min_run_time = 10\n    batch_size = 64\n    num_heads = 32\n    max_seq_len = 256\n    embed_dim = 1024\n    dtype = torch.bfloat16\n    print(f'Running timing for batch_size={batch_size} max_sequence_len={max_seq_len} num_heads={num_heads} embed_dimension={embed_dim} dtype={dtype}')\n    run_timing(min_run_time, batch_size, embed_dim, num_heads, max_seq_len, dtype)"
        ]
    }
]