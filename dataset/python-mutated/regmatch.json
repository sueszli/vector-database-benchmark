[
    {
        "func_name": "__init__",
        "original": "def __init__(self, optimism=True, discount=False, rnd_init=False, seed=None, **kwargs):\n    \"\"\"Ctor.\"\"\"\n    del kwargs\n    self.num_players = None\n    self.lrs = None\n    self.optimism = optimism\n    self.discount = discount\n    self.rnd_init = rnd_init\n    self.has_aux = True\n    self.aux_errors = []\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
        "mutated": [
            "def __init__(self, optimism=True, discount=False, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n    'Ctor.'\n    del kwargs\n    self.num_players = None\n    self.lrs = None\n    self.optimism = optimism\n    self.discount = discount\n    self.rnd_init = rnd_init\n    self.has_aux = True\n    self.aux_errors = []\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, optimism=True, discount=False, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Ctor.'\n    del kwargs\n    self.num_players = None\n    self.lrs = None\n    self.optimism = optimism\n    self.discount = discount\n    self.rnd_init = rnd_init\n    self.has_aux = True\n    self.aux_errors = []\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, optimism=True, discount=False, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Ctor.'\n    del kwargs\n    self.num_players = None\n    self.lrs = None\n    self.optimism = optimism\n    self.discount = discount\n    self.rnd_init = rnd_init\n    self.has_aux = True\n    self.aux_errors = []\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, optimism=True, discount=False, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Ctor.'\n    del kwargs\n    self.num_players = None\n    self.lrs = None\n    self.optimism = optimism\n    self.discount = discount\n    self.rnd_init = rnd_init\n    self.has_aux = True\n    self.aux_errors = []\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, optimism=True, discount=False, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Ctor.'\n    del kwargs\n    self.num_players = None\n    self.lrs = None\n    self.optimism = optimism\n    self.discount = discount\n    self.rnd_init = rnd_init\n    self.has_aux = True\n    self.aux_errors = []\n    self.seed = seed\n    self.random = np.random.RandomState(seed)"
        ]
    },
    {
        "func_name": "init_vars",
        "original": "def init_vars(self, num_strats, num_players):\n    \"\"\"Initialize solver parameters.\"\"\"\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_regret = [np.zeros_like(dist_i) for dist_i in init_dist]\n    return (init_dist, init_regret)",
        "mutated": [
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_regret = [np.zeros_like(dist_i) for dist_i in init_dist]\n    return (init_dist, init_regret)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_regret = [np.zeros_like(dist_i) for dist_i in init_dist]\n    return (init_dist, init_regret)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_regret = [np.zeros_like(dist_i) for dist_i in init_dist]\n    return (init_dist, init_regret)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_regret = [np.zeros_like(dist_i) for dist_i in init_dist]\n    return (init_dist, init_regret)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_regret = [np.zeros_like(dist_i) for dist_i in init_dist]\n    return (init_dist, init_regret)"
        ]
    },
    {
        "func_name": "record_aux_errors",
        "original": "def record_aux_errors(self, grads):\n    \"\"\"Record errors for the auxiliary variables.\"\"\"\n    grad_regret = grads[1]\n    grad_regret_flat = np.concatenate(grad_regret)\n    self.aux_errors.append([np.linalg.norm(grad_regret_flat)])",
        "mutated": [
            "def record_aux_errors(self, grads):\n    if False:\n        i = 10\n    'Record errors for the auxiliary variables.'\n    grad_regret = grads[1]\n    grad_regret_flat = np.concatenate(grad_regret)\n    self.aux_errors.append([np.linalg.norm(grad_regret_flat)])",
            "def record_aux_errors(self, grads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Record errors for the auxiliary variables.'\n    grad_regret = grads[1]\n    grad_regret_flat = np.concatenate(grad_regret)\n    self.aux_errors.append([np.linalg.norm(grad_regret_flat)])",
            "def record_aux_errors(self, grads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Record errors for the auxiliary variables.'\n    grad_regret = grads[1]\n    grad_regret_flat = np.concatenate(grad_regret)\n    self.aux_errors.append([np.linalg.norm(grad_regret_flat)])",
            "def record_aux_errors(self, grads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Record errors for the auxiliary variables.'\n    grad_regret = grads[1]\n    grad_regret_flat = np.concatenate(grad_regret)\n    self.aux_errors.append([np.linalg.norm(grad_regret_flat)])",
            "def record_aux_errors(self, grads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Record errors for the auxiliary variables.'\n    grad_regret = grads[1]\n    grad_regret_flat = np.concatenate(grad_regret)\n    self.aux_errors.append([np.linalg.norm(grad_regret_flat)])"
        ]
    },
    {
        "func_name": "compute_gradients",
        "original": "def compute_gradients(self, params, payoff_matrices):\n    \"\"\"Compute and return gradients (and exploitabilities) for all parameters.\n\n    Args:\n      params: tuple of params (dist, regret), see regmatch.gradients\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\n        are sorted and arrays should be indexed in the same order\n    Returns:\n      tuple of gradients (grad_dist, grad_regret), see ate.gradients\n      unregularized exploitability (stochastic estimate)\n      solver exploitability (stochastic estimate) - NaN\n    \"\"\"\n    return gradients(*params, payoff_matrices, self.num_players)",
        "mutated": [
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, regret), see regmatch.gradients\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    Returns:\\n      tuple of gradients (grad_dist, grad_regret), see ate.gradients\\n      unregularized exploitability (stochastic estimate)\\n      solver exploitability (stochastic estimate) - NaN\\n    '\n    return gradients(*params, payoff_matrices, self.num_players)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, regret), see regmatch.gradients\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    Returns:\\n      tuple of gradients (grad_dist, grad_regret), see ate.gradients\\n      unregularized exploitability (stochastic estimate)\\n      solver exploitability (stochastic estimate) - NaN\\n    '\n    return gradients(*params, payoff_matrices, self.num_players)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, regret), see regmatch.gradients\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    Returns:\\n      tuple of gradients (grad_dist, grad_regret), see ate.gradients\\n      unregularized exploitability (stochastic estimate)\\n      solver exploitability (stochastic estimate) - NaN\\n    '\n    return gradients(*params, payoff_matrices, self.num_players)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, regret), see regmatch.gradients\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    Returns:\\n      tuple of gradients (grad_dist, grad_regret), see ate.gradients\\n      unregularized exploitability (stochastic estimate)\\n      solver exploitability (stochastic estimate) - NaN\\n    '\n    return gradients(*params, payoff_matrices, self.num_players)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, regret), see regmatch.gradients\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    Returns:\\n      tuple of gradients (grad_dist, grad_regret), see ate.gradients\\n      unregularized exploitability (stochastic estimate)\\n      solver exploitability (stochastic estimate) - NaN\\n    '\n    return gradients(*params, payoff_matrices, self.num_players)"
        ]
    },
    {
        "func_name": "exploitability",
        "original": "def exploitability(self, params, payoff_matrices):\n    \"\"\"Regret matching does not minimize any exploitability so return NaN.\n\n    Args:\n      params: tuple of params (dist,)\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\n        are sorted and arrays should be indexed in the same order\n    Returns:\n      np.NaN\n    \"\"\"\n    del params\n    del payoff_matrices\n    return np.NaN",
        "mutated": [
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n    'Regret matching does not minimize any exploitability so return NaN.\\n\\n    Args:\\n      params: tuple of params (dist,)\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    Returns:\\n      np.NaN\\n    '\n    del params\n    del payoff_matrices\n    return np.NaN",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Regret matching does not minimize any exploitability so return NaN.\\n\\n    Args:\\n      params: tuple of params (dist,)\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    Returns:\\n      np.NaN\\n    '\n    del params\n    del payoff_matrices\n    return np.NaN",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Regret matching does not minimize any exploitability so return NaN.\\n\\n    Args:\\n      params: tuple of params (dist,)\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    Returns:\\n      np.NaN\\n    '\n    del params\n    del payoff_matrices\n    return np.NaN",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Regret matching does not minimize any exploitability so return NaN.\\n\\n    Args:\\n      params: tuple of params (dist,)\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    Returns:\\n      np.NaN\\n    '\n    del params\n    del payoff_matrices\n    return np.NaN",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Regret matching does not minimize any exploitability so return NaN.\\n\\n    Args:\\n      params: tuple of params (dist,)\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    Returns:\\n      np.NaN\\n    '\n    del params\n    del payoff_matrices\n    return np.NaN"
        ]
    },
    {
        "func_name": "update",
        "original": "def update(self, params, grads, t):\n    \"\"\"Update cumulative regret and strategy (dist).\n\n    Args:\n      params: tuple of variables to be updated (dist, regret)\n      grads: tuple of variable gradients (grad_dist, grad_regret)\n      t: int, solver iteration (not used)\n    Returns:\n      new_params: tuple of update params (new_dist, new_regret)\n    \"\"\"\n    (dist, regret) = params\n    regret_delta = grads[1]\n    if self.discount:\n        gamma = t / float(t + 1)\n    else:\n        gamma = 1\n    new_dist = []\n    new_regret = []\n    for (dist_i, regret_i, regret_delta_i) in zip(dist, regret, regret_delta):\n        new_regret_i = gamma * regret_i + regret_delta_i\n        new_clipped_regrets_i = np.clip(new_regret_i + self.optimism * regret_delta_i, 0.0, np.inf)\n        if np.sum(new_clipped_regrets_i) > 0:\n            new_dist_i = new_clipped_regrets_i / new_clipped_regrets_i.sum()\n        else:\n            new_dist_i = np.ones_like(dist_i) / dist_i.size\n        new_dist.append(new_dist_i)\n        new_regret.append(new_regret_i)\n    new_params = (new_dist, new_regret)\n    return new_params",
        "mutated": [
            "def update(self, params, grads, t):\n    if False:\n        i = 10\n    'Update cumulative regret and strategy (dist).\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, regret)\\n      grads: tuple of variable gradients (grad_dist, grad_regret)\\n      t: int, solver iteration (not used)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_regret)\\n    '\n    (dist, regret) = params\n    regret_delta = grads[1]\n    if self.discount:\n        gamma = t / float(t + 1)\n    else:\n        gamma = 1\n    new_dist = []\n    new_regret = []\n    for (dist_i, regret_i, regret_delta_i) in zip(dist, regret, regret_delta):\n        new_regret_i = gamma * regret_i + regret_delta_i\n        new_clipped_regrets_i = np.clip(new_regret_i + self.optimism * regret_delta_i, 0.0, np.inf)\n        if np.sum(new_clipped_regrets_i) > 0:\n            new_dist_i = new_clipped_regrets_i / new_clipped_regrets_i.sum()\n        else:\n            new_dist_i = np.ones_like(dist_i) / dist_i.size\n        new_dist.append(new_dist_i)\n        new_regret.append(new_regret_i)\n    new_params = (new_dist, new_regret)\n    return new_params",
            "def update(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Update cumulative regret and strategy (dist).\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, regret)\\n      grads: tuple of variable gradients (grad_dist, grad_regret)\\n      t: int, solver iteration (not used)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_regret)\\n    '\n    (dist, regret) = params\n    regret_delta = grads[1]\n    if self.discount:\n        gamma = t / float(t + 1)\n    else:\n        gamma = 1\n    new_dist = []\n    new_regret = []\n    for (dist_i, regret_i, regret_delta_i) in zip(dist, regret, regret_delta):\n        new_regret_i = gamma * regret_i + regret_delta_i\n        new_clipped_regrets_i = np.clip(new_regret_i + self.optimism * regret_delta_i, 0.0, np.inf)\n        if np.sum(new_clipped_regrets_i) > 0:\n            new_dist_i = new_clipped_regrets_i / new_clipped_regrets_i.sum()\n        else:\n            new_dist_i = np.ones_like(dist_i) / dist_i.size\n        new_dist.append(new_dist_i)\n        new_regret.append(new_regret_i)\n    new_params = (new_dist, new_regret)\n    return new_params",
            "def update(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Update cumulative regret and strategy (dist).\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, regret)\\n      grads: tuple of variable gradients (grad_dist, grad_regret)\\n      t: int, solver iteration (not used)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_regret)\\n    '\n    (dist, regret) = params\n    regret_delta = grads[1]\n    if self.discount:\n        gamma = t / float(t + 1)\n    else:\n        gamma = 1\n    new_dist = []\n    new_regret = []\n    for (dist_i, regret_i, regret_delta_i) in zip(dist, regret, regret_delta):\n        new_regret_i = gamma * regret_i + regret_delta_i\n        new_clipped_regrets_i = np.clip(new_regret_i + self.optimism * regret_delta_i, 0.0, np.inf)\n        if np.sum(new_clipped_regrets_i) > 0:\n            new_dist_i = new_clipped_regrets_i / new_clipped_regrets_i.sum()\n        else:\n            new_dist_i = np.ones_like(dist_i) / dist_i.size\n        new_dist.append(new_dist_i)\n        new_regret.append(new_regret_i)\n    new_params = (new_dist, new_regret)\n    return new_params",
            "def update(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Update cumulative regret and strategy (dist).\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, regret)\\n      grads: tuple of variable gradients (grad_dist, grad_regret)\\n      t: int, solver iteration (not used)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_regret)\\n    '\n    (dist, regret) = params\n    regret_delta = grads[1]\n    if self.discount:\n        gamma = t / float(t + 1)\n    else:\n        gamma = 1\n    new_dist = []\n    new_regret = []\n    for (dist_i, regret_i, regret_delta_i) in zip(dist, regret, regret_delta):\n        new_regret_i = gamma * regret_i + regret_delta_i\n        new_clipped_regrets_i = np.clip(new_regret_i + self.optimism * regret_delta_i, 0.0, np.inf)\n        if np.sum(new_clipped_regrets_i) > 0:\n            new_dist_i = new_clipped_regrets_i / new_clipped_regrets_i.sum()\n        else:\n            new_dist_i = np.ones_like(dist_i) / dist_i.size\n        new_dist.append(new_dist_i)\n        new_regret.append(new_regret_i)\n    new_params = (new_dist, new_regret)\n    return new_params",
            "def update(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Update cumulative regret and strategy (dist).\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, regret)\\n      grads: tuple of variable gradients (grad_dist, grad_regret)\\n      t: int, solver iteration (not used)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_regret)\\n    '\n    (dist, regret) = params\n    regret_delta = grads[1]\n    if self.discount:\n        gamma = t / float(t + 1)\n    else:\n        gamma = 1\n    new_dist = []\n    new_regret = []\n    for (dist_i, regret_i, regret_delta_i) in zip(dist, regret, regret_delta):\n        new_regret_i = gamma * regret_i + regret_delta_i\n        new_clipped_regrets_i = np.clip(new_regret_i + self.optimism * regret_delta_i, 0.0, np.inf)\n        if np.sum(new_clipped_regrets_i) > 0:\n            new_dist_i = new_clipped_regrets_i / new_clipped_regrets_i.sum()\n        else:\n            new_dist_i = np.ones_like(dist_i) / dist_i.size\n        new_dist.append(new_dist_i)\n        new_regret.append(new_regret_i)\n    new_params = (new_dist, new_regret)\n    return new_params"
        ]
    },
    {
        "func_name": "gradients",
        "original": "def gradients(dist, regret, payoff_matrices, num_players):\n    \"\"\"Computes regret delta to be added to regret in update.\n\n  Args:\n    dist: list of 1-d np.arrays, current estimate of nash distribution\n    regret: list of 1-d np.arrays (same as dist), current estimate of regrets\n    payoff_matrices: dictionary with keys as tuples of agents (i, j) and\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\n        are sorted and arrays should be indexed in the same order\n    num_players: int, number of players, in case payoff_matrices is abbreviated\n  Returns:\n    deltas w.r.t. (dist, regret) as tuple\n    unregularized exploitability (stochastic estimate)\n    solver exploitability (stochastic estimate) - NaN\n  \"\"\"\n    del regret\n    grad_dist = []\n    grad_regret = []\n    unreg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_dist_i = np.NaN * np.ones_like(nabla_i)\n        grad_dist.append(grad_dist_i)\n        utility_i = nabla_i.dot(dist[i])\n        grad_regret_i = nabla_i - utility_i\n        grad_regret.append(grad_regret_i)\n        unreg_exp.append(np.max(nabla_i) - nabla_i.dot(dist[i]))\n    return ((grad_dist, grad_regret), np.mean(unreg_exp), np.NaN)",
        "mutated": [
            "def gradients(dist, regret, payoff_matrices, num_players):\n    if False:\n        i = 10\n    'Computes regret delta to be added to regret in update.\\n\\n  Args:\\n    dist: list of 1-d np.arrays, current estimate of nash distribution\\n    regret: list of 1-d np.arrays (same as dist), current estimate of regrets\\n    payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    num_players: int, number of players, in case payoff_matrices is abbreviated\\n  Returns:\\n    deltas w.r.t. (dist, regret) as tuple\\n    unregularized exploitability (stochastic estimate)\\n    solver exploitability (stochastic estimate) - NaN\\n  '\n    del regret\n    grad_dist = []\n    grad_regret = []\n    unreg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_dist_i = np.NaN * np.ones_like(nabla_i)\n        grad_dist.append(grad_dist_i)\n        utility_i = nabla_i.dot(dist[i])\n        grad_regret_i = nabla_i - utility_i\n        grad_regret.append(grad_regret_i)\n        unreg_exp.append(np.max(nabla_i) - nabla_i.dot(dist[i]))\n    return ((grad_dist, grad_regret), np.mean(unreg_exp), np.NaN)",
            "def gradients(dist, regret, payoff_matrices, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes regret delta to be added to regret in update.\\n\\n  Args:\\n    dist: list of 1-d np.arrays, current estimate of nash distribution\\n    regret: list of 1-d np.arrays (same as dist), current estimate of regrets\\n    payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    num_players: int, number of players, in case payoff_matrices is abbreviated\\n  Returns:\\n    deltas w.r.t. (dist, regret) as tuple\\n    unregularized exploitability (stochastic estimate)\\n    solver exploitability (stochastic estimate) - NaN\\n  '\n    del regret\n    grad_dist = []\n    grad_regret = []\n    unreg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_dist_i = np.NaN * np.ones_like(nabla_i)\n        grad_dist.append(grad_dist_i)\n        utility_i = nabla_i.dot(dist[i])\n        grad_regret_i = nabla_i - utility_i\n        grad_regret.append(grad_regret_i)\n        unreg_exp.append(np.max(nabla_i) - nabla_i.dot(dist[i]))\n    return ((grad_dist, grad_regret), np.mean(unreg_exp), np.NaN)",
            "def gradients(dist, regret, payoff_matrices, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes regret delta to be added to regret in update.\\n\\n  Args:\\n    dist: list of 1-d np.arrays, current estimate of nash distribution\\n    regret: list of 1-d np.arrays (same as dist), current estimate of regrets\\n    payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    num_players: int, number of players, in case payoff_matrices is abbreviated\\n  Returns:\\n    deltas w.r.t. (dist, regret) as tuple\\n    unregularized exploitability (stochastic estimate)\\n    solver exploitability (stochastic estimate) - NaN\\n  '\n    del regret\n    grad_dist = []\n    grad_regret = []\n    unreg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_dist_i = np.NaN * np.ones_like(nabla_i)\n        grad_dist.append(grad_dist_i)\n        utility_i = nabla_i.dot(dist[i])\n        grad_regret_i = nabla_i - utility_i\n        grad_regret.append(grad_regret_i)\n        unreg_exp.append(np.max(nabla_i) - nabla_i.dot(dist[i]))\n    return ((grad_dist, grad_regret), np.mean(unreg_exp), np.NaN)",
            "def gradients(dist, regret, payoff_matrices, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes regret delta to be added to regret in update.\\n\\n  Args:\\n    dist: list of 1-d np.arrays, current estimate of nash distribution\\n    regret: list of 1-d np.arrays (same as dist), current estimate of regrets\\n    payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    num_players: int, number of players, in case payoff_matrices is abbreviated\\n  Returns:\\n    deltas w.r.t. (dist, regret) as tuple\\n    unregularized exploitability (stochastic estimate)\\n    solver exploitability (stochastic estimate) - NaN\\n  '\n    del regret\n    grad_dist = []\n    grad_regret = []\n    unreg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_dist_i = np.NaN * np.ones_like(nabla_i)\n        grad_dist.append(grad_dist_i)\n        utility_i = nabla_i.dot(dist[i])\n        grad_regret_i = nabla_i - utility_i\n        grad_regret.append(grad_regret_i)\n        unreg_exp.append(np.max(nabla_i) - nabla_i.dot(dist[i]))\n    return ((grad_dist, grad_regret), np.mean(unreg_exp), np.NaN)",
            "def gradients(dist, regret, payoff_matrices, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes regret delta to be added to regret in update.\\n\\n  Args:\\n    dist: list of 1-d np.arrays, current estimate of nash distribution\\n    regret: list of 1-d np.arrays (same as dist), current estimate of regrets\\n    payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n        values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n        are sorted and arrays should be indexed in the same order\\n    num_players: int, number of players, in case payoff_matrices is abbreviated\\n  Returns:\\n    deltas w.r.t. (dist, regret) as tuple\\n    unregularized exploitability (stochastic estimate)\\n    solver exploitability (stochastic estimate) - NaN\\n  '\n    del regret\n    grad_dist = []\n    grad_regret = []\n    unreg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_dist_i = np.NaN * np.ones_like(nabla_i)\n        grad_dist.append(grad_dist_i)\n        utility_i = nabla_i.dot(dist[i])\n        grad_regret_i = nabla_i - utility_i\n        grad_regret.append(grad_regret_i)\n        unreg_exp.append(np.max(nabla_i) - nabla_i.dot(dist[i]))\n    return ((grad_dist, grad_regret), np.mean(unreg_exp), np.NaN)"
        ]
    }
]