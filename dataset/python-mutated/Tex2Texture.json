[
    {
        "func_name": "__init__",
        "original": "def __init__(self, model_dir, *args, **kwargs):\n    \"\"\"The Tex2Texture is modified based on TEXTure and Text2Tex, publicly available at\n                https://github.com/TEXTurePaper/TEXTurePaper &\n                https://github.com/daveredrum/Text2Tex\n        Args:\n            model_dir: the root directory of the model files\n        \"\"\"\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    if torch.cuda.is_available():\n        self.device = torch.device('cuda')\n        logger.info('Use GPU: {}'.format(self.device))\n    else:\n        print('no gpu avaiable')\n        exit()\n    model_path = model_dir + '/base_model/'\n    controlmodel_path = model_dir + '/control_model/'\n    inpaintmodel_path = model_dir + '/inpaint_model/'\n    torch_dtype = kwargs.get('torch_dtype', torch.float16)\n    self.controlnet = ControlNetModel.from_pretrained(controlmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.inpaintmodel = StableDiffusionInpaintPipeline.from_pretrained(inpaintmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.pipe = StableDiffusionControlinpaintPipeline.from_pretrained(model_path, controlnet=self.controlnet, torch_dtype=torch_dtype).to(self.device)\n    logger.info('model load over')",
        "mutated": [
            "def __init__(self, model_dir, *args, **kwargs):\n    if False:\n        i = 10\n    'The Tex2Texture is modified based on TEXTure and Text2Tex, publicly available at\\n                https://github.com/TEXTurePaper/TEXTurePaper &\\n                https://github.com/daveredrum/Text2Tex\\n        Args:\\n            model_dir: the root directory of the model files\\n        '\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    if torch.cuda.is_available():\n        self.device = torch.device('cuda')\n        logger.info('Use GPU: {}'.format(self.device))\n    else:\n        print('no gpu avaiable')\n        exit()\n    model_path = model_dir + '/base_model/'\n    controlmodel_path = model_dir + '/control_model/'\n    inpaintmodel_path = model_dir + '/inpaint_model/'\n    torch_dtype = kwargs.get('torch_dtype', torch.float16)\n    self.controlnet = ControlNetModel.from_pretrained(controlmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.inpaintmodel = StableDiffusionInpaintPipeline.from_pretrained(inpaintmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.pipe = StableDiffusionControlinpaintPipeline.from_pretrained(model_path, controlnet=self.controlnet, torch_dtype=torch_dtype).to(self.device)\n    logger.info('model load over')",
            "def __init__(self, model_dir, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'The Tex2Texture is modified based on TEXTure and Text2Tex, publicly available at\\n                https://github.com/TEXTurePaper/TEXTurePaper &\\n                https://github.com/daveredrum/Text2Tex\\n        Args:\\n            model_dir: the root directory of the model files\\n        '\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    if torch.cuda.is_available():\n        self.device = torch.device('cuda')\n        logger.info('Use GPU: {}'.format(self.device))\n    else:\n        print('no gpu avaiable')\n        exit()\n    model_path = model_dir + '/base_model/'\n    controlmodel_path = model_dir + '/control_model/'\n    inpaintmodel_path = model_dir + '/inpaint_model/'\n    torch_dtype = kwargs.get('torch_dtype', torch.float16)\n    self.controlnet = ControlNetModel.from_pretrained(controlmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.inpaintmodel = StableDiffusionInpaintPipeline.from_pretrained(inpaintmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.pipe = StableDiffusionControlinpaintPipeline.from_pretrained(model_path, controlnet=self.controlnet, torch_dtype=torch_dtype).to(self.device)\n    logger.info('model load over')",
            "def __init__(self, model_dir, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'The Tex2Texture is modified based on TEXTure and Text2Tex, publicly available at\\n                https://github.com/TEXTurePaper/TEXTurePaper &\\n                https://github.com/daveredrum/Text2Tex\\n        Args:\\n            model_dir: the root directory of the model files\\n        '\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    if torch.cuda.is_available():\n        self.device = torch.device('cuda')\n        logger.info('Use GPU: {}'.format(self.device))\n    else:\n        print('no gpu avaiable')\n        exit()\n    model_path = model_dir + '/base_model/'\n    controlmodel_path = model_dir + '/control_model/'\n    inpaintmodel_path = model_dir + '/inpaint_model/'\n    torch_dtype = kwargs.get('torch_dtype', torch.float16)\n    self.controlnet = ControlNetModel.from_pretrained(controlmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.inpaintmodel = StableDiffusionInpaintPipeline.from_pretrained(inpaintmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.pipe = StableDiffusionControlinpaintPipeline.from_pretrained(model_path, controlnet=self.controlnet, torch_dtype=torch_dtype).to(self.device)\n    logger.info('model load over')",
            "def __init__(self, model_dir, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'The Tex2Texture is modified based on TEXTure and Text2Tex, publicly available at\\n                https://github.com/TEXTurePaper/TEXTurePaper &\\n                https://github.com/daveredrum/Text2Tex\\n        Args:\\n            model_dir: the root directory of the model files\\n        '\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    if torch.cuda.is_available():\n        self.device = torch.device('cuda')\n        logger.info('Use GPU: {}'.format(self.device))\n    else:\n        print('no gpu avaiable')\n        exit()\n    model_path = model_dir + '/base_model/'\n    controlmodel_path = model_dir + '/control_model/'\n    inpaintmodel_path = model_dir + '/inpaint_model/'\n    torch_dtype = kwargs.get('torch_dtype', torch.float16)\n    self.controlnet = ControlNetModel.from_pretrained(controlmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.inpaintmodel = StableDiffusionInpaintPipeline.from_pretrained(inpaintmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.pipe = StableDiffusionControlinpaintPipeline.from_pretrained(model_path, controlnet=self.controlnet, torch_dtype=torch_dtype).to(self.device)\n    logger.info('model load over')",
            "def __init__(self, model_dir, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'The Tex2Texture is modified based on TEXTure and Text2Tex, publicly available at\\n                https://github.com/TEXTurePaper/TEXTurePaper &\\n                https://github.com/daveredrum/Text2Tex\\n        Args:\\n            model_dir: the root directory of the model files\\n        '\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    if torch.cuda.is_available():\n        self.device = torch.device('cuda')\n        logger.info('Use GPU: {}'.format(self.device))\n    else:\n        print('no gpu avaiable')\n        exit()\n    model_path = model_dir + '/base_model/'\n    controlmodel_path = model_dir + '/control_model/'\n    inpaintmodel_path = model_dir + '/inpaint_model/'\n    torch_dtype = kwargs.get('torch_dtype', torch.float16)\n    self.controlnet = ControlNetModel.from_pretrained(controlmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.inpaintmodel = StableDiffusionInpaintPipeline.from_pretrained(inpaintmodel_path, torch_dtype=torch_dtype).to(self.device)\n    self.pipe = StableDiffusionControlinpaintPipeline.from_pretrained(model_path, controlnet=self.controlnet, torch_dtype=torch_dtype).to(self.device)\n    logger.info('model load over')"
        ]
    },
    {
        "func_name": "init_mesh",
        "original": "def init_mesh(self, mesh_path):\n    (verts, faces, aux) = load_obj(mesh_path, device=self.device)\n    mesh = load_objs_as_meshes([mesh_path], device=self.device)\n    return (mesh, verts, faces, aux)",
        "mutated": [
            "def init_mesh(self, mesh_path):\n    if False:\n        i = 10\n    (verts, faces, aux) = load_obj(mesh_path, device=self.device)\n    mesh = load_objs_as_meshes([mesh_path], device=self.device)\n    return (mesh, verts, faces, aux)",
            "def init_mesh(self, mesh_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (verts, faces, aux) = load_obj(mesh_path, device=self.device)\n    mesh = load_objs_as_meshes([mesh_path], device=self.device)\n    return (mesh, verts, faces, aux)",
            "def init_mesh(self, mesh_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (verts, faces, aux) = load_obj(mesh_path, device=self.device)\n    mesh = load_objs_as_meshes([mesh_path], device=self.device)\n    return (mesh, verts, faces, aux)",
            "def init_mesh(self, mesh_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (verts, faces, aux) = load_obj(mesh_path, device=self.device)\n    mesh = load_objs_as_meshes([mesh_path], device=self.device)\n    return (mesh, verts, faces, aux)",
            "def init_mesh(self, mesh_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (verts, faces, aux) = load_obj(mesh_path, device=self.device)\n    mesh = load_objs_as_meshes([mesh_path], device=self.device)\n    return (mesh, verts, faces, aux)"
        ]
    },
    {
        "func_name": "normalize_mesh",
        "original": "def normalize_mesh(self, mesh):\n    bbox = mesh.get_bounding_boxes()\n    num_verts = mesh.verts_packed().shape[0]\n    mesh_center = bbox.mean(dim=2).repeat(num_verts, 1)\n    mesh = mesh.offset_verts(-mesh_center)\n    lens = bbox[0, :, 1] - bbox[0, :, 0]\n    max_len = lens.max()\n    scale = 0.9 / max_len\n    scale = scale.unsqueeze(0).repeat(num_verts)\n    new_mesh = mesh.scale_verts(scale)\n    return (new_mesh.verts_packed(), new_mesh, mesh_center, scale)",
        "mutated": [
            "def normalize_mesh(self, mesh):\n    if False:\n        i = 10\n    bbox = mesh.get_bounding_boxes()\n    num_verts = mesh.verts_packed().shape[0]\n    mesh_center = bbox.mean(dim=2).repeat(num_verts, 1)\n    mesh = mesh.offset_verts(-mesh_center)\n    lens = bbox[0, :, 1] - bbox[0, :, 0]\n    max_len = lens.max()\n    scale = 0.9 / max_len\n    scale = scale.unsqueeze(0).repeat(num_verts)\n    new_mesh = mesh.scale_verts(scale)\n    return (new_mesh.verts_packed(), new_mesh, mesh_center, scale)",
            "def normalize_mesh(self, mesh):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bbox = mesh.get_bounding_boxes()\n    num_verts = mesh.verts_packed().shape[0]\n    mesh_center = bbox.mean(dim=2).repeat(num_verts, 1)\n    mesh = mesh.offset_verts(-mesh_center)\n    lens = bbox[0, :, 1] - bbox[0, :, 0]\n    max_len = lens.max()\n    scale = 0.9 / max_len\n    scale = scale.unsqueeze(0).repeat(num_verts)\n    new_mesh = mesh.scale_verts(scale)\n    return (new_mesh.verts_packed(), new_mesh, mesh_center, scale)",
            "def normalize_mesh(self, mesh):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bbox = mesh.get_bounding_boxes()\n    num_verts = mesh.verts_packed().shape[0]\n    mesh_center = bbox.mean(dim=2).repeat(num_verts, 1)\n    mesh = mesh.offset_verts(-mesh_center)\n    lens = bbox[0, :, 1] - bbox[0, :, 0]\n    max_len = lens.max()\n    scale = 0.9 / max_len\n    scale = scale.unsqueeze(0).repeat(num_verts)\n    new_mesh = mesh.scale_verts(scale)\n    return (new_mesh.verts_packed(), new_mesh, mesh_center, scale)",
            "def normalize_mesh(self, mesh):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bbox = mesh.get_bounding_boxes()\n    num_verts = mesh.verts_packed().shape[0]\n    mesh_center = bbox.mean(dim=2).repeat(num_verts, 1)\n    mesh = mesh.offset_verts(-mesh_center)\n    lens = bbox[0, :, 1] - bbox[0, :, 0]\n    max_len = lens.max()\n    scale = 0.9 / max_len\n    scale = scale.unsqueeze(0).repeat(num_verts)\n    new_mesh = mesh.scale_verts(scale)\n    return (new_mesh.verts_packed(), new_mesh, mesh_center, scale)",
            "def normalize_mesh(self, mesh):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bbox = mesh.get_bounding_boxes()\n    num_verts = mesh.verts_packed().shape[0]\n    mesh_center = bbox.mean(dim=2).repeat(num_verts, 1)\n    mesh = mesh.offset_verts(-mesh_center)\n    lens = bbox[0, :, 1] - bbox[0, :, 0]\n    max_len = lens.max()\n    scale = 0.9 / max_len\n    scale = scale.unsqueeze(0).repeat(num_verts)\n    new_mesh = mesh.scale_verts(scale)\n    return (new_mesh.verts_packed(), new_mesh, mesh_center, scale)"
        ]
    },
    {
        "func_name": "save_normalized_obj",
        "original": "def save_normalized_obj(self, verts, faces, aux, path='normalized.obj'):\n    print('=> saving normalized mesh file...')\n    obj_path = path\n    save_obj(obj_path, verts=verts, faces=faces.verts_idx, decimal_places=5, verts_uvs=aux.verts_uvs, faces_uvs=faces.textures_idx, texture_map=aux.texture_images[list(aux.texture_images.keys())[0]])",
        "mutated": [
            "def save_normalized_obj(self, verts, faces, aux, path='normalized.obj'):\n    if False:\n        i = 10\n    print('=> saving normalized mesh file...')\n    obj_path = path\n    save_obj(obj_path, verts=verts, faces=faces.verts_idx, decimal_places=5, verts_uvs=aux.verts_uvs, faces_uvs=faces.textures_idx, texture_map=aux.texture_images[list(aux.texture_images.keys())[0]])",
            "def save_normalized_obj(self, verts, faces, aux, path='normalized.obj'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    print('=> saving normalized mesh file...')\n    obj_path = path\n    save_obj(obj_path, verts=verts, faces=faces.verts_idx, decimal_places=5, verts_uvs=aux.verts_uvs, faces_uvs=faces.textures_idx, texture_map=aux.texture_images[list(aux.texture_images.keys())[0]])",
            "def save_normalized_obj(self, verts, faces, aux, path='normalized.obj'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    print('=> saving normalized mesh file...')\n    obj_path = path\n    save_obj(obj_path, verts=verts, faces=faces.verts_idx, decimal_places=5, verts_uvs=aux.verts_uvs, faces_uvs=faces.textures_idx, texture_map=aux.texture_images[list(aux.texture_images.keys())[0]])",
            "def save_normalized_obj(self, verts, faces, aux, path='normalized.obj'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    print('=> saving normalized mesh file...')\n    obj_path = path\n    save_obj(obj_path, verts=verts, faces=faces.verts_idx, decimal_places=5, verts_uvs=aux.verts_uvs, faces_uvs=faces.textures_idx, texture_map=aux.texture_images[list(aux.texture_images.keys())[0]])",
            "def save_normalized_obj(self, verts, faces, aux, path='normalized.obj'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    print('=> saving normalized mesh file...')\n    obj_path = path\n    save_obj(obj_path, verts=verts, faces=faces.verts_idx, decimal_places=5, verts_uvs=aux.verts_uvs, faces_uvs=faces.textures_idx, texture_map=aux.texture_images[list(aux.texture_images.keys())[0]])"
        ]
    },
    {
        "func_name": "mesh_normalized",
        "original": "def mesh_normalized(self, mesh_path, save_path='normalized.obj'):\n    (mesh, verts, faces, aux) = self.init_mesh(mesh_path)\n    (verts, mesh, mesh_center, scale) = self.normalize_mesh(mesh)\n    self.save_normalized_obj(verts, faces, aux, save_path)\n    return (mesh, verts, faces, aux, mesh_center, scale)",
        "mutated": [
            "def mesh_normalized(self, mesh_path, save_path='normalized.obj'):\n    if False:\n        i = 10\n    (mesh, verts, faces, aux) = self.init_mesh(mesh_path)\n    (verts, mesh, mesh_center, scale) = self.normalize_mesh(mesh)\n    self.save_normalized_obj(verts, faces, aux, save_path)\n    return (mesh, verts, faces, aux, mesh_center, scale)",
            "def mesh_normalized(self, mesh_path, save_path='normalized.obj'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (mesh, verts, faces, aux) = self.init_mesh(mesh_path)\n    (verts, mesh, mesh_center, scale) = self.normalize_mesh(mesh)\n    self.save_normalized_obj(verts, faces, aux, save_path)\n    return (mesh, verts, faces, aux, mesh_center, scale)",
            "def mesh_normalized(self, mesh_path, save_path='normalized.obj'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (mesh, verts, faces, aux) = self.init_mesh(mesh_path)\n    (verts, mesh, mesh_center, scale) = self.normalize_mesh(mesh)\n    self.save_normalized_obj(verts, faces, aux, save_path)\n    return (mesh, verts, faces, aux, mesh_center, scale)",
            "def mesh_normalized(self, mesh_path, save_path='normalized.obj'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (mesh, verts, faces, aux) = self.init_mesh(mesh_path)\n    (verts, mesh, mesh_center, scale) = self.normalize_mesh(mesh)\n    self.save_normalized_obj(verts, faces, aux, save_path)\n    return (mesh, verts, faces, aux, mesh_center, scale)",
            "def mesh_normalized(self, mesh_path, save_path='normalized.obj'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (mesh, verts, faces, aux) = self.init_mesh(mesh_path)\n    (verts, mesh, mesh_center, scale) = self.normalize_mesh(mesh)\n    self.save_normalized_obj(verts, faces, aux, save_path)\n    return (mesh, verts, faces, aux, mesh_center, scale)"
        ]
    },
    {
        "func_name": "prepare_mask_and_masked_image",
        "original": "def prepare_mask_and_masked_image(image, mask, height, width, return_image=False):\n    if image is None:\n        raise ValueError('`image` input cannot be undefined.')\n    if mask is None:\n        raise ValueError('`mask_image` input cannot be undefined.')\n    if isinstance(image, torch.Tensor):\n        if not isinstance(mask, torch.Tensor):\n            raise TypeError(f'`image` is a torch.Tensor but `mask` (type: {type(mask)} is not')\n        if image.ndim == 3:\n            assert image.shape[0] == 3, 'Image outside a batch should be of shape (3, H, W)'\n            image = image.unsqueeze(0)\n        if mask.ndim == 2:\n            mask = mask.unsqueeze(0).unsqueeze(0)\n        if mask.ndim == 3:\n            if mask.shape[0] == 1:\n                mask = mask.unsqueeze(0)\n            else:\n                mask = mask.unsqueeze(1)\n        assert image.ndim == 4 and mask.ndim == 4, 'Image and Mask must have 4 dimensions'\n        assert image.shape[-2:] == mask.shape[-2:], 'Image and Mask must have the same spatial dimensions'\n        assert image.shape[0] == mask.shape[0], 'Image and Mask must have the same batch size'\n        if image.min() < -1 or image.max() > 1:\n            raise ValueError('Image should be in [-1, 1] range')\n        if mask.min() < 0 or mask.max() > 1:\n            raise ValueError('Mask should be in [0, 1] range')\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        image = image.to(dtype=torch.float32)\n    elif isinstance(mask, torch.Tensor):\n        raise TypeError(f'`mask` is a torch.Tensor but `image` (type: {type(image)} is not')\n    else:\n        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n            image = [image]\n        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n            image = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in image]\n            image = [np.array(i.convert('RGB'))[None, :] for i in image]\n            image = np.concatenate(image, axis=0)\n        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n            image = np.concatenate([i[None, :] for i in image], axis=0)\n        image = image.transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n        if isinstance(mask, (PIL.Image.Image, np.ndarray)):\n            mask = [mask]\n        if isinstance(mask, list) and isinstance(mask[0], PIL.Image.Image):\n            mask = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in mask]\n            mask = np.concatenate([np.array(m.convert('L'))[None, None, :] for m in mask], axis=0)\n            mask = mask.astype(np.float32) / 255.0\n        elif isinstance(mask, list) and isinstance(mask[0], np.ndarray):\n            mask = np.concatenate([m[None, None, :] for m in mask], axis=0)\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        mask = torch.from_numpy(mask)\n    masked_image = image * (mask < 0.5)\n    if return_image:\n        return (mask, masked_image, image)\n    return (mask, masked_image)",
        "mutated": [
            "def prepare_mask_and_masked_image(image, mask, height, width, return_image=False):\n    if False:\n        i = 10\n    if image is None:\n        raise ValueError('`image` input cannot be undefined.')\n    if mask is None:\n        raise ValueError('`mask_image` input cannot be undefined.')\n    if isinstance(image, torch.Tensor):\n        if not isinstance(mask, torch.Tensor):\n            raise TypeError(f'`image` is a torch.Tensor but `mask` (type: {type(mask)} is not')\n        if image.ndim == 3:\n            assert image.shape[0] == 3, 'Image outside a batch should be of shape (3, H, W)'\n            image = image.unsqueeze(0)\n        if mask.ndim == 2:\n            mask = mask.unsqueeze(0).unsqueeze(0)\n        if mask.ndim == 3:\n            if mask.shape[0] == 1:\n                mask = mask.unsqueeze(0)\n            else:\n                mask = mask.unsqueeze(1)\n        assert image.ndim == 4 and mask.ndim == 4, 'Image and Mask must have 4 dimensions'\n        assert image.shape[-2:] == mask.shape[-2:], 'Image and Mask must have the same spatial dimensions'\n        assert image.shape[0] == mask.shape[0], 'Image and Mask must have the same batch size'\n        if image.min() < -1 or image.max() > 1:\n            raise ValueError('Image should be in [-1, 1] range')\n        if mask.min() < 0 or mask.max() > 1:\n            raise ValueError('Mask should be in [0, 1] range')\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        image = image.to(dtype=torch.float32)\n    elif isinstance(mask, torch.Tensor):\n        raise TypeError(f'`mask` is a torch.Tensor but `image` (type: {type(image)} is not')\n    else:\n        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n            image = [image]\n        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n            image = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in image]\n            image = [np.array(i.convert('RGB'))[None, :] for i in image]\n            image = np.concatenate(image, axis=0)\n        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n            image = np.concatenate([i[None, :] for i in image], axis=0)\n        image = image.transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n        if isinstance(mask, (PIL.Image.Image, np.ndarray)):\n            mask = [mask]\n        if isinstance(mask, list) and isinstance(mask[0], PIL.Image.Image):\n            mask = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in mask]\n            mask = np.concatenate([np.array(m.convert('L'))[None, None, :] for m in mask], axis=0)\n            mask = mask.astype(np.float32) / 255.0\n        elif isinstance(mask, list) and isinstance(mask[0], np.ndarray):\n            mask = np.concatenate([m[None, None, :] for m in mask], axis=0)\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        mask = torch.from_numpy(mask)\n    masked_image = image * (mask < 0.5)\n    if return_image:\n        return (mask, masked_image, image)\n    return (mask, masked_image)",
            "def prepare_mask_and_masked_image(image, mask, height, width, return_image=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if image is None:\n        raise ValueError('`image` input cannot be undefined.')\n    if mask is None:\n        raise ValueError('`mask_image` input cannot be undefined.')\n    if isinstance(image, torch.Tensor):\n        if not isinstance(mask, torch.Tensor):\n            raise TypeError(f'`image` is a torch.Tensor but `mask` (type: {type(mask)} is not')\n        if image.ndim == 3:\n            assert image.shape[0] == 3, 'Image outside a batch should be of shape (3, H, W)'\n            image = image.unsqueeze(0)\n        if mask.ndim == 2:\n            mask = mask.unsqueeze(0).unsqueeze(0)\n        if mask.ndim == 3:\n            if mask.shape[0] == 1:\n                mask = mask.unsqueeze(0)\n            else:\n                mask = mask.unsqueeze(1)\n        assert image.ndim == 4 and mask.ndim == 4, 'Image and Mask must have 4 dimensions'\n        assert image.shape[-2:] == mask.shape[-2:], 'Image and Mask must have the same spatial dimensions'\n        assert image.shape[0] == mask.shape[0], 'Image and Mask must have the same batch size'\n        if image.min() < -1 or image.max() > 1:\n            raise ValueError('Image should be in [-1, 1] range')\n        if mask.min() < 0 or mask.max() > 1:\n            raise ValueError('Mask should be in [0, 1] range')\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        image = image.to(dtype=torch.float32)\n    elif isinstance(mask, torch.Tensor):\n        raise TypeError(f'`mask` is a torch.Tensor but `image` (type: {type(image)} is not')\n    else:\n        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n            image = [image]\n        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n            image = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in image]\n            image = [np.array(i.convert('RGB'))[None, :] for i in image]\n            image = np.concatenate(image, axis=0)\n        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n            image = np.concatenate([i[None, :] for i in image], axis=0)\n        image = image.transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n        if isinstance(mask, (PIL.Image.Image, np.ndarray)):\n            mask = [mask]\n        if isinstance(mask, list) and isinstance(mask[0], PIL.Image.Image):\n            mask = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in mask]\n            mask = np.concatenate([np.array(m.convert('L'))[None, None, :] for m in mask], axis=0)\n            mask = mask.astype(np.float32) / 255.0\n        elif isinstance(mask, list) and isinstance(mask[0], np.ndarray):\n            mask = np.concatenate([m[None, None, :] for m in mask], axis=0)\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        mask = torch.from_numpy(mask)\n    masked_image = image * (mask < 0.5)\n    if return_image:\n        return (mask, masked_image, image)\n    return (mask, masked_image)",
            "def prepare_mask_and_masked_image(image, mask, height, width, return_image=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if image is None:\n        raise ValueError('`image` input cannot be undefined.')\n    if mask is None:\n        raise ValueError('`mask_image` input cannot be undefined.')\n    if isinstance(image, torch.Tensor):\n        if not isinstance(mask, torch.Tensor):\n            raise TypeError(f'`image` is a torch.Tensor but `mask` (type: {type(mask)} is not')\n        if image.ndim == 3:\n            assert image.shape[0] == 3, 'Image outside a batch should be of shape (3, H, W)'\n            image = image.unsqueeze(0)\n        if mask.ndim == 2:\n            mask = mask.unsqueeze(0).unsqueeze(0)\n        if mask.ndim == 3:\n            if mask.shape[0] == 1:\n                mask = mask.unsqueeze(0)\n            else:\n                mask = mask.unsqueeze(1)\n        assert image.ndim == 4 and mask.ndim == 4, 'Image and Mask must have 4 dimensions'\n        assert image.shape[-2:] == mask.shape[-2:], 'Image and Mask must have the same spatial dimensions'\n        assert image.shape[0] == mask.shape[0], 'Image and Mask must have the same batch size'\n        if image.min() < -1 or image.max() > 1:\n            raise ValueError('Image should be in [-1, 1] range')\n        if mask.min() < 0 or mask.max() > 1:\n            raise ValueError('Mask should be in [0, 1] range')\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        image = image.to(dtype=torch.float32)\n    elif isinstance(mask, torch.Tensor):\n        raise TypeError(f'`mask` is a torch.Tensor but `image` (type: {type(image)} is not')\n    else:\n        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n            image = [image]\n        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n            image = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in image]\n            image = [np.array(i.convert('RGB'))[None, :] for i in image]\n            image = np.concatenate(image, axis=0)\n        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n            image = np.concatenate([i[None, :] for i in image], axis=0)\n        image = image.transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n        if isinstance(mask, (PIL.Image.Image, np.ndarray)):\n            mask = [mask]\n        if isinstance(mask, list) and isinstance(mask[0], PIL.Image.Image):\n            mask = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in mask]\n            mask = np.concatenate([np.array(m.convert('L'))[None, None, :] for m in mask], axis=0)\n            mask = mask.astype(np.float32) / 255.0\n        elif isinstance(mask, list) and isinstance(mask[0], np.ndarray):\n            mask = np.concatenate([m[None, None, :] for m in mask], axis=0)\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        mask = torch.from_numpy(mask)\n    masked_image = image * (mask < 0.5)\n    if return_image:\n        return (mask, masked_image, image)\n    return (mask, masked_image)",
            "def prepare_mask_and_masked_image(image, mask, height, width, return_image=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if image is None:\n        raise ValueError('`image` input cannot be undefined.')\n    if mask is None:\n        raise ValueError('`mask_image` input cannot be undefined.')\n    if isinstance(image, torch.Tensor):\n        if not isinstance(mask, torch.Tensor):\n            raise TypeError(f'`image` is a torch.Tensor but `mask` (type: {type(mask)} is not')\n        if image.ndim == 3:\n            assert image.shape[0] == 3, 'Image outside a batch should be of shape (3, H, W)'\n            image = image.unsqueeze(0)\n        if mask.ndim == 2:\n            mask = mask.unsqueeze(0).unsqueeze(0)\n        if mask.ndim == 3:\n            if mask.shape[0] == 1:\n                mask = mask.unsqueeze(0)\n            else:\n                mask = mask.unsqueeze(1)\n        assert image.ndim == 4 and mask.ndim == 4, 'Image and Mask must have 4 dimensions'\n        assert image.shape[-2:] == mask.shape[-2:], 'Image and Mask must have the same spatial dimensions'\n        assert image.shape[0] == mask.shape[0], 'Image and Mask must have the same batch size'\n        if image.min() < -1 or image.max() > 1:\n            raise ValueError('Image should be in [-1, 1] range')\n        if mask.min() < 0 or mask.max() > 1:\n            raise ValueError('Mask should be in [0, 1] range')\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        image = image.to(dtype=torch.float32)\n    elif isinstance(mask, torch.Tensor):\n        raise TypeError(f'`mask` is a torch.Tensor but `image` (type: {type(image)} is not')\n    else:\n        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n            image = [image]\n        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n            image = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in image]\n            image = [np.array(i.convert('RGB'))[None, :] for i in image]\n            image = np.concatenate(image, axis=0)\n        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n            image = np.concatenate([i[None, :] for i in image], axis=0)\n        image = image.transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n        if isinstance(mask, (PIL.Image.Image, np.ndarray)):\n            mask = [mask]\n        if isinstance(mask, list) and isinstance(mask[0], PIL.Image.Image):\n            mask = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in mask]\n            mask = np.concatenate([np.array(m.convert('L'))[None, None, :] for m in mask], axis=0)\n            mask = mask.astype(np.float32) / 255.0\n        elif isinstance(mask, list) and isinstance(mask[0], np.ndarray):\n            mask = np.concatenate([m[None, None, :] for m in mask], axis=0)\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        mask = torch.from_numpy(mask)\n    masked_image = image * (mask < 0.5)\n    if return_image:\n        return (mask, masked_image, image)\n    return (mask, masked_image)",
            "def prepare_mask_and_masked_image(image, mask, height, width, return_image=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if image is None:\n        raise ValueError('`image` input cannot be undefined.')\n    if mask is None:\n        raise ValueError('`mask_image` input cannot be undefined.')\n    if isinstance(image, torch.Tensor):\n        if not isinstance(mask, torch.Tensor):\n            raise TypeError(f'`image` is a torch.Tensor but `mask` (type: {type(mask)} is not')\n        if image.ndim == 3:\n            assert image.shape[0] == 3, 'Image outside a batch should be of shape (3, H, W)'\n            image = image.unsqueeze(0)\n        if mask.ndim == 2:\n            mask = mask.unsqueeze(0).unsqueeze(0)\n        if mask.ndim == 3:\n            if mask.shape[0] == 1:\n                mask = mask.unsqueeze(0)\n            else:\n                mask = mask.unsqueeze(1)\n        assert image.ndim == 4 and mask.ndim == 4, 'Image and Mask must have 4 dimensions'\n        assert image.shape[-2:] == mask.shape[-2:], 'Image and Mask must have the same spatial dimensions'\n        assert image.shape[0] == mask.shape[0], 'Image and Mask must have the same batch size'\n        if image.min() < -1 or image.max() > 1:\n            raise ValueError('Image should be in [-1, 1] range')\n        if mask.min() < 0 or mask.max() > 1:\n            raise ValueError('Mask should be in [0, 1] range')\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        image = image.to(dtype=torch.float32)\n    elif isinstance(mask, torch.Tensor):\n        raise TypeError(f'`mask` is a torch.Tensor but `image` (type: {type(image)} is not')\n    else:\n        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n            image = [image]\n        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n            image = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in image]\n            image = [np.array(i.convert('RGB'))[None, :] for i in image]\n            image = np.concatenate(image, axis=0)\n        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n            image = np.concatenate([i[None, :] for i in image], axis=0)\n        image = image.transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n        if isinstance(mask, (PIL.Image.Image, np.ndarray)):\n            mask = [mask]\n        if isinstance(mask, list) and isinstance(mask[0], PIL.Image.Image):\n            mask = [i.resize((width, height), resample=PIL.Image.LANCZOS) for i in mask]\n            mask = np.concatenate([np.array(m.convert('L'))[None, None, :] for m in mask], axis=0)\n            mask = mask.astype(np.float32) / 255.0\n        elif isinstance(mask, list) and isinstance(mask[0], np.ndarray):\n            mask = np.concatenate([m[None, None, :] for m in mask], axis=0)\n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        mask = torch.from_numpy(mask)\n    masked_image = image * (mask < 0.5)\n    if return_image:\n        return (mask, masked_image, image)\n    return (mask, masked_image)"
        ]
    },
    {
        "func_name": "__call__",
        "original": "@torch.no_grad()\n@replace_example_docstring(EXAMPLE_DOC_STRING)\ndef __call__(self, prompt: Union[str, List[str]]=None, image: Union[torch.Tensor, PIL.Image.Image]=None, mask_image: Union[torch.Tensor, PIL.Image.Image]=None, control_image: Union[torch.FloatTensor, PIL.Image.Image, np.ndarray, List[torch.FloatTensor], List[PIL.Image.Image], List[np.ndarray]]=None, height: Optional[int]=None, width: Optional[int]=None, strength: float=1.0, num_inference_steps: int=50, guidance_scale: float=7.5, negative_prompt: Optional[Union[str, List[str]]]=None, num_images_per_prompt: Optional[int]=1, eta: float=0.0, generator: Optional[Union[torch.Generator, List[torch.Generator]]]=None, latents: Optional[torch.FloatTensor]=None, prompt_embeds: Optional[torch.FloatTensor]=None, negative_prompt_embeds: Optional[torch.FloatTensor]=None, output_type: Optional[str]='pil', return_dict: bool=True, callback: Optional[Callable[[int, int, torch.FloatTensor], None]]=None, callback_steps: int=1, cross_attention_kwargs: Optional[Dict[str, Any]]=None, controlnet_conditioning_scale: Union[float, List[float]]=0.5, guess_mode: bool=False):\n    \"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n                instead.\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`,\n                    `List[List[torch.FloatTensor]]`, or `List[List[PIL.Image.Image]]`):\n                The ControlNet input condition. ControlNet uses this input condition to generate guidance to Unet. If\n                the type is specified as `Torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can\n                also be accepted as an image. The dimensions of the output image defaults to `image`'s dimensions. If\n                height and/or width are passed, `image` is resized according to them. If multiple ControlNets are\n                specified in init, images must be passed as a list such that each element of the list can be correctly\n                batched for input to a single controlnet.\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The width in pixels of the generated image.\n            strength (`float`, *optional*, defaults to 1.):\n                Conceptually, indicates how much to transform the masked portion of the reference `image`. Must be\n                between 0 and 1. `image` will be used as a starting point, adding more noise to it the larger the\n                `strength`. The number of denoising steps depends on the amount of noise initially added. When\n                `strength` is 1, added noise will be maximum and the denoising process will run for the full number of\n                iterations specified in `num_inference_steps`. A value of 1, therefore, essentially ignores the masked\n                portion of the reference `image`.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n                less than `1`).\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor will ge generated by sampling using the supplied random `generator`.\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n                argument.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n                `self.processor` in\n                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 0.5):\n                The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added\n                to the residual in the original unet. If multiple ControlNets are specified in init, you can set the\n                corresponding scale as a list. Note that by default, we use a smaller conditioning scale for inpainting\n                than for [`~StableDiffusionControlNetPipeline.__call__`].\n            guess_mode (`bool`, *optional*, defaults to `False`):\n                In this mode, the ControlNet encoder will try best to recognize the content of the input image even if\n                you remove all prompts. The `guidance_scale` between 3.0 and 5.0 is recommended.\n\n        Examples:\n\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n    (height, width) = self._default_height_width(height, width, image)\n    self.check_inputs(prompt, control_image, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds, controlnet_conditioning_scale)\n    if prompt is not None and isinstance(prompt, str):\n        batch_size = 1\n    elif prompt is not None and isinstance(prompt, list):\n        batch_size = len(prompt)\n    else:\n        batch_size = prompt_embeds.shape[0]\n    device = self._execution_device\n    do_classifier_free_guidance = guidance_scale > 1.0\n    controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n    if isinstance(controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n        controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(controlnet.nets)\n    global_pool_conditions = controlnet.config.global_pool_conditions if isinstance(controlnet, ControlNetModel) else controlnet.nets[0].config.global_pool_conditions\n    guess_mode = guess_mode or global_pool_conditions\n    text_encoder_lora_scale = cross_attention_kwargs.get('scale', None) if cross_attention_kwargs is not None else None\n    prompt_embeds = self._encode_prompt(prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_prompt_embeds, lora_scale=text_encoder_lora_scale)\n    if isinstance(controlnet, ControlNetModel):\n        control_image = self.prepare_control_image(image=control_image, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n    elif isinstance(controlnet, MultiControlNetModel):\n        control_images = []\n        for control_image_ in control_image:\n            control_image_ = self.prepare_control_image(image=control_image_, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n            control_images.append(control_image_)\n        control_image = control_images\n    else:\n        assert False\n    (mask, masked_image, init_image) = prepare_mask_and_masked_image(image, mask_image, height, width, return_image=True)\n    self.scheduler.set_timesteps(num_inference_steps, device=device)\n    (timesteps, num_inference_steps) = self.get_timesteps(num_inference_steps=num_inference_steps, strength=strength, device=device)\n    latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n    is_strength_max = strength == 1.0\n    num_channels_latents = self.vae.config.latent_channels\n    num_channels_unet = self.unet.config.in_channels\n    return_image_latents = num_channels_unet == 4\n    latents_outputs = self.prepare_latents(batch_size * num_images_per_prompt, num_channels_latents, height, width, prompt_embeds.dtype, device, generator, latents, image=init_image, timestep=latent_timestep, is_strength_max=is_strength_max, return_noise=True, return_image_latents=return_image_latents)\n    if return_image_latents:\n        (latents, noise, image_latents) = latents_outputs\n    else:\n        (latents, noise) = latents_outputs\n    (mask, masked_image_latents) = self.prepare_mask_latents(mask, masked_image, batch_size * num_images_per_prompt, height, width, prompt_embeds.dtype, device, generator, do_classifier_free_guidance)\n    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for (i, t) in enumerate(timesteps):\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n            if guess_mode and do_classifier_free_guidance:\n                control_model_input = latents\n                control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n                controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n            else:\n                control_model_input = latent_model_input\n                controlnet_prompt_embeds = prompt_embeds\n            (down_block_res_samples, mid_block_res_sample) = self.controlnet(control_model_input, t, encoder_hidden_states=controlnet_prompt_embeds, controlnet_cond=control_image, conditioning_scale=controlnet_conditioning_scale, guess_mode=guess_mode, return_dict=False)\n            if guess_mode and do_classifier_free_guidance:\n                down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n                mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n            if num_channels_unet == 9:\n                latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds, cross_attention_kwargs=cross_attention_kwargs, down_block_additional_residuals=down_block_res_samples, mid_block_additional_residual=mid_block_res_sample, return_dict=False)[0]\n            if do_classifier_free_guidance:\n                (noise_pred_uncond, noise_pred_text) = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n            if num_channels_unet == 4:\n                init_latents_proper = image_latents[:1]\n                init_mask = mask[:1]\n                if i < len(timesteps) - 1:\n                    init_latents_proper = self.scheduler.add_noise(init_latents_proper, noise, torch.tensor([t]))\n                latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n            if i == len(timesteps) - 1 or (i + 1) % self.scheduler.order == 0:\n                progress_bar.update()\n                if callback is not None and i % callback_steps == 0:\n                    callback(i, t, latents)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.unet.to('cpu')\n        self.controlnet.to('cpu')\n        torch.cuda.empty_cache()\n    if not output_type == 'latent':\n        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n        (image, has_nsfw_concept) = self.run_safety_checker(image, device, prompt_embeds.dtype)\n    else:\n        image = latents\n        has_nsfw_concept = None\n    if has_nsfw_concept is None:\n        do_denormalize = [True] * image.shape[0]\n    else:\n        do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n    image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.final_offload_hook.offload()\n    if not return_dict:\n        return (image, has_nsfw_concept)\n    return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)",
        "mutated": [
            "@torch.no_grad()\n@replace_example_docstring(EXAMPLE_DOC_STRING)\ndef __call__(self, prompt: Union[str, List[str]]=None, image: Union[torch.Tensor, PIL.Image.Image]=None, mask_image: Union[torch.Tensor, PIL.Image.Image]=None, control_image: Union[torch.FloatTensor, PIL.Image.Image, np.ndarray, List[torch.FloatTensor], List[PIL.Image.Image], List[np.ndarray]]=None, height: Optional[int]=None, width: Optional[int]=None, strength: float=1.0, num_inference_steps: int=50, guidance_scale: float=7.5, negative_prompt: Optional[Union[str, List[str]]]=None, num_images_per_prompt: Optional[int]=1, eta: float=0.0, generator: Optional[Union[torch.Generator, List[torch.Generator]]]=None, latents: Optional[torch.FloatTensor]=None, prompt_embeds: Optional[torch.FloatTensor]=None, negative_prompt_embeds: Optional[torch.FloatTensor]=None, output_type: Optional[str]='pil', return_dict: bool=True, callback: Optional[Callable[[int, int, torch.FloatTensor], None]]=None, callback_steps: int=1, cross_attention_kwargs: Optional[Dict[str, Any]]=None, controlnet_conditioning_scale: Union[float, List[float]]=0.5, guess_mode: bool=False):\n    if False:\n        i = 10\n    '\\n        Function invoked when calling the pipeline for generation.\\n\\n        Args:\\n            prompt (`str` or `List[str]`, *optional*):\\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\\n                instead.\\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`,\\n                    `List[List[torch.FloatTensor]]`, or `List[List[PIL.Image.Image]]`):\\n                The ControlNet input condition. ControlNet uses this input condition to generate guidance to Unet. If\\n                the type is specified as `Torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can\\n                also be accepted as an image. The dimensions of the output image defaults to `image`\\'s dimensions. If\\n                height and/or width are passed, `image` is resized according to them. If multiple ControlNets are\\n                specified in init, images must be passed as a list such that each element of the list can be correctly\\n                batched for input to a single controlnet.\\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\\n                The height in pixels of the generated image.\\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\\n                The width in pixels of the generated image.\\n            strength (`float`, *optional*, defaults to 1.):\\n                Conceptually, indicates how much to transform the masked portion of the reference `image`. Must be\\n                between 0 and 1. `image` will be used as a starting point, adding more noise to it the larger the\\n                `strength`. The number of denoising steps depends on the amount of noise initially added. When\\n                `strength` is 1, added noise will be maximum and the denoising process will run for the full number of\\n                iterations specified in `num_inference_steps`. A value of 1, therefore, essentially ignores the masked\\n                portion of the reference `image`.\\n            num_inference_steps (`int`, *optional*, defaults to 50):\\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\\n                expense of slower inference.\\n            guidance_scale (`float`, *optional*, defaults to 7.5):\\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\\n                usually at the expense of lower image quality.\\n            negative_prompt (`str` or `List[str]`, *optional*):\\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\\n                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\\n                less than `1`).\\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\\n                The number of images to generate per prompt.\\n            eta (`float`, *optional*, defaults to 0.0):\\n                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\\n                [`schedulers.DDIMScheduler`], will be ignored for others.\\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\\n                to make generation deterministic.\\n            latents (`torch.FloatTensor`, *optional*):\\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\\n                tensor will ge generated by sampling using the supplied random `generator`.\\n            prompt_embeds (`torch.FloatTensor`, *optional*):\\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\\n                provided, text embeddings will be generated from `prompt` input argument.\\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\\n                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\\n                argument.\\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\\n                The output format of the generate image. Choose between\\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\\n            return_dict (`bool`, *optional*, defaults to `True`):\\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\\n                plain tuple.\\n            callback (`Callable`, *optional*):\\n                A function that will be called every `callback_steps` steps during inference. The function will be\\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\\n            callback_steps (`int`, *optional*, defaults to 1):\\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\\n                called at every step.\\n            cross_attention_kwargs (`dict`, *optional*):\\n                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\\n                `self.processor` in\\n                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\\n            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 0.5):\\n                The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added\\n                to the residual in the original unet. If multiple ControlNets are specified in init, you can set the\\n                corresponding scale as a list. Note that by default, we use a smaller conditioning scale for inpainting\\n                than for [`~StableDiffusionControlNetPipeline.__call__`].\\n            guess_mode (`bool`, *optional*, defaults to `False`):\\n                In this mode, the ControlNet encoder will try best to recognize the content of the input image even if\\n                you remove all prompts. The `guidance_scale` between 3.0 and 5.0 is recommended.\\n\\n        Examples:\\n\\n        Returns:\\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\\n            (nsfw) content, according to the `safety_checker`.\\n        '\n    (height, width) = self._default_height_width(height, width, image)\n    self.check_inputs(prompt, control_image, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds, controlnet_conditioning_scale)\n    if prompt is not None and isinstance(prompt, str):\n        batch_size = 1\n    elif prompt is not None and isinstance(prompt, list):\n        batch_size = len(prompt)\n    else:\n        batch_size = prompt_embeds.shape[0]\n    device = self._execution_device\n    do_classifier_free_guidance = guidance_scale > 1.0\n    controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n    if isinstance(controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n        controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(controlnet.nets)\n    global_pool_conditions = controlnet.config.global_pool_conditions if isinstance(controlnet, ControlNetModel) else controlnet.nets[0].config.global_pool_conditions\n    guess_mode = guess_mode or global_pool_conditions\n    text_encoder_lora_scale = cross_attention_kwargs.get('scale', None) if cross_attention_kwargs is not None else None\n    prompt_embeds = self._encode_prompt(prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_prompt_embeds, lora_scale=text_encoder_lora_scale)\n    if isinstance(controlnet, ControlNetModel):\n        control_image = self.prepare_control_image(image=control_image, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n    elif isinstance(controlnet, MultiControlNetModel):\n        control_images = []\n        for control_image_ in control_image:\n            control_image_ = self.prepare_control_image(image=control_image_, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n            control_images.append(control_image_)\n        control_image = control_images\n    else:\n        assert False\n    (mask, masked_image, init_image) = prepare_mask_and_masked_image(image, mask_image, height, width, return_image=True)\n    self.scheduler.set_timesteps(num_inference_steps, device=device)\n    (timesteps, num_inference_steps) = self.get_timesteps(num_inference_steps=num_inference_steps, strength=strength, device=device)\n    latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n    is_strength_max = strength == 1.0\n    num_channels_latents = self.vae.config.latent_channels\n    num_channels_unet = self.unet.config.in_channels\n    return_image_latents = num_channels_unet == 4\n    latents_outputs = self.prepare_latents(batch_size * num_images_per_prompt, num_channels_latents, height, width, prompt_embeds.dtype, device, generator, latents, image=init_image, timestep=latent_timestep, is_strength_max=is_strength_max, return_noise=True, return_image_latents=return_image_latents)\n    if return_image_latents:\n        (latents, noise, image_latents) = latents_outputs\n    else:\n        (latents, noise) = latents_outputs\n    (mask, masked_image_latents) = self.prepare_mask_latents(mask, masked_image, batch_size * num_images_per_prompt, height, width, prompt_embeds.dtype, device, generator, do_classifier_free_guidance)\n    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for (i, t) in enumerate(timesteps):\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n            if guess_mode and do_classifier_free_guidance:\n                control_model_input = latents\n                control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n                controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n            else:\n                control_model_input = latent_model_input\n                controlnet_prompt_embeds = prompt_embeds\n            (down_block_res_samples, mid_block_res_sample) = self.controlnet(control_model_input, t, encoder_hidden_states=controlnet_prompt_embeds, controlnet_cond=control_image, conditioning_scale=controlnet_conditioning_scale, guess_mode=guess_mode, return_dict=False)\n            if guess_mode and do_classifier_free_guidance:\n                down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n                mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n            if num_channels_unet == 9:\n                latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds, cross_attention_kwargs=cross_attention_kwargs, down_block_additional_residuals=down_block_res_samples, mid_block_additional_residual=mid_block_res_sample, return_dict=False)[0]\n            if do_classifier_free_guidance:\n                (noise_pred_uncond, noise_pred_text) = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n            if num_channels_unet == 4:\n                init_latents_proper = image_latents[:1]\n                init_mask = mask[:1]\n                if i < len(timesteps) - 1:\n                    init_latents_proper = self.scheduler.add_noise(init_latents_proper, noise, torch.tensor([t]))\n                latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n            if i == len(timesteps) - 1 or (i + 1) % self.scheduler.order == 0:\n                progress_bar.update()\n                if callback is not None and i % callback_steps == 0:\n                    callback(i, t, latents)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.unet.to('cpu')\n        self.controlnet.to('cpu')\n        torch.cuda.empty_cache()\n    if not output_type == 'latent':\n        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n        (image, has_nsfw_concept) = self.run_safety_checker(image, device, prompt_embeds.dtype)\n    else:\n        image = latents\n        has_nsfw_concept = None\n    if has_nsfw_concept is None:\n        do_denormalize = [True] * image.shape[0]\n    else:\n        do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n    image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.final_offload_hook.offload()\n    if not return_dict:\n        return (image, has_nsfw_concept)\n    return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)",
            "@torch.no_grad()\n@replace_example_docstring(EXAMPLE_DOC_STRING)\ndef __call__(self, prompt: Union[str, List[str]]=None, image: Union[torch.Tensor, PIL.Image.Image]=None, mask_image: Union[torch.Tensor, PIL.Image.Image]=None, control_image: Union[torch.FloatTensor, PIL.Image.Image, np.ndarray, List[torch.FloatTensor], List[PIL.Image.Image], List[np.ndarray]]=None, height: Optional[int]=None, width: Optional[int]=None, strength: float=1.0, num_inference_steps: int=50, guidance_scale: float=7.5, negative_prompt: Optional[Union[str, List[str]]]=None, num_images_per_prompt: Optional[int]=1, eta: float=0.0, generator: Optional[Union[torch.Generator, List[torch.Generator]]]=None, latents: Optional[torch.FloatTensor]=None, prompt_embeds: Optional[torch.FloatTensor]=None, negative_prompt_embeds: Optional[torch.FloatTensor]=None, output_type: Optional[str]='pil', return_dict: bool=True, callback: Optional[Callable[[int, int, torch.FloatTensor], None]]=None, callback_steps: int=1, cross_attention_kwargs: Optional[Dict[str, Any]]=None, controlnet_conditioning_scale: Union[float, List[float]]=0.5, guess_mode: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Function invoked when calling the pipeline for generation.\\n\\n        Args:\\n            prompt (`str` or `List[str]`, *optional*):\\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\\n                instead.\\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`,\\n                    `List[List[torch.FloatTensor]]`, or `List[List[PIL.Image.Image]]`):\\n                The ControlNet input condition. ControlNet uses this input condition to generate guidance to Unet. If\\n                the type is specified as `Torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can\\n                also be accepted as an image. The dimensions of the output image defaults to `image`\\'s dimensions. If\\n                height and/or width are passed, `image` is resized according to them. If multiple ControlNets are\\n                specified in init, images must be passed as a list such that each element of the list can be correctly\\n                batched for input to a single controlnet.\\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\\n                The height in pixels of the generated image.\\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\\n                The width in pixels of the generated image.\\n            strength (`float`, *optional*, defaults to 1.):\\n                Conceptually, indicates how much to transform the masked portion of the reference `image`. Must be\\n                between 0 and 1. `image` will be used as a starting point, adding more noise to it the larger the\\n                `strength`. The number of denoising steps depends on the amount of noise initially added. When\\n                `strength` is 1, added noise will be maximum and the denoising process will run for the full number of\\n                iterations specified in `num_inference_steps`. A value of 1, therefore, essentially ignores the masked\\n                portion of the reference `image`.\\n            num_inference_steps (`int`, *optional*, defaults to 50):\\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\\n                expense of slower inference.\\n            guidance_scale (`float`, *optional*, defaults to 7.5):\\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\\n                usually at the expense of lower image quality.\\n            negative_prompt (`str` or `List[str]`, *optional*):\\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\\n                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\\n                less than `1`).\\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\\n                The number of images to generate per prompt.\\n            eta (`float`, *optional*, defaults to 0.0):\\n                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\\n                [`schedulers.DDIMScheduler`], will be ignored for others.\\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\\n                to make generation deterministic.\\n            latents (`torch.FloatTensor`, *optional*):\\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\\n                tensor will ge generated by sampling using the supplied random `generator`.\\n            prompt_embeds (`torch.FloatTensor`, *optional*):\\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\\n                provided, text embeddings will be generated from `prompt` input argument.\\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\\n                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\\n                argument.\\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\\n                The output format of the generate image. Choose between\\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\\n            return_dict (`bool`, *optional*, defaults to `True`):\\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\\n                plain tuple.\\n            callback (`Callable`, *optional*):\\n                A function that will be called every `callback_steps` steps during inference. The function will be\\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\\n            callback_steps (`int`, *optional*, defaults to 1):\\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\\n                called at every step.\\n            cross_attention_kwargs (`dict`, *optional*):\\n                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\\n                `self.processor` in\\n                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\\n            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 0.5):\\n                The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added\\n                to the residual in the original unet. If multiple ControlNets are specified in init, you can set the\\n                corresponding scale as a list. Note that by default, we use a smaller conditioning scale for inpainting\\n                than for [`~StableDiffusionControlNetPipeline.__call__`].\\n            guess_mode (`bool`, *optional*, defaults to `False`):\\n                In this mode, the ControlNet encoder will try best to recognize the content of the input image even if\\n                you remove all prompts. The `guidance_scale` between 3.0 and 5.0 is recommended.\\n\\n        Examples:\\n\\n        Returns:\\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\\n            (nsfw) content, according to the `safety_checker`.\\n        '\n    (height, width) = self._default_height_width(height, width, image)\n    self.check_inputs(prompt, control_image, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds, controlnet_conditioning_scale)\n    if prompt is not None and isinstance(prompt, str):\n        batch_size = 1\n    elif prompt is not None and isinstance(prompt, list):\n        batch_size = len(prompt)\n    else:\n        batch_size = prompt_embeds.shape[0]\n    device = self._execution_device\n    do_classifier_free_guidance = guidance_scale > 1.0\n    controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n    if isinstance(controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n        controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(controlnet.nets)\n    global_pool_conditions = controlnet.config.global_pool_conditions if isinstance(controlnet, ControlNetModel) else controlnet.nets[0].config.global_pool_conditions\n    guess_mode = guess_mode or global_pool_conditions\n    text_encoder_lora_scale = cross_attention_kwargs.get('scale', None) if cross_attention_kwargs is not None else None\n    prompt_embeds = self._encode_prompt(prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_prompt_embeds, lora_scale=text_encoder_lora_scale)\n    if isinstance(controlnet, ControlNetModel):\n        control_image = self.prepare_control_image(image=control_image, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n    elif isinstance(controlnet, MultiControlNetModel):\n        control_images = []\n        for control_image_ in control_image:\n            control_image_ = self.prepare_control_image(image=control_image_, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n            control_images.append(control_image_)\n        control_image = control_images\n    else:\n        assert False\n    (mask, masked_image, init_image) = prepare_mask_and_masked_image(image, mask_image, height, width, return_image=True)\n    self.scheduler.set_timesteps(num_inference_steps, device=device)\n    (timesteps, num_inference_steps) = self.get_timesteps(num_inference_steps=num_inference_steps, strength=strength, device=device)\n    latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n    is_strength_max = strength == 1.0\n    num_channels_latents = self.vae.config.latent_channels\n    num_channels_unet = self.unet.config.in_channels\n    return_image_latents = num_channels_unet == 4\n    latents_outputs = self.prepare_latents(batch_size * num_images_per_prompt, num_channels_latents, height, width, prompt_embeds.dtype, device, generator, latents, image=init_image, timestep=latent_timestep, is_strength_max=is_strength_max, return_noise=True, return_image_latents=return_image_latents)\n    if return_image_latents:\n        (latents, noise, image_latents) = latents_outputs\n    else:\n        (latents, noise) = latents_outputs\n    (mask, masked_image_latents) = self.prepare_mask_latents(mask, masked_image, batch_size * num_images_per_prompt, height, width, prompt_embeds.dtype, device, generator, do_classifier_free_guidance)\n    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for (i, t) in enumerate(timesteps):\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n            if guess_mode and do_classifier_free_guidance:\n                control_model_input = latents\n                control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n                controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n            else:\n                control_model_input = latent_model_input\n                controlnet_prompt_embeds = prompt_embeds\n            (down_block_res_samples, mid_block_res_sample) = self.controlnet(control_model_input, t, encoder_hidden_states=controlnet_prompt_embeds, controlnet_cond=control_image, conditioning_scale=controlnet_conditioning_scale, guess_mode=guess_mode, return_dict=False)\n            if guess_mode and do_classifier_free_guidance:\n                down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n                mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n            if num_channels_unet == 9:\n                latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds, cross_attention_kwargs=cross_attention_kwargs, down_block_additional_residuals=down_block_res_samples, mid_block_additional_residual=mid_block_res_sample, return_dict=False)[0]\n            if do_classifier_free_guidance:\n                (noise_pred_uncond, noise_pred_text) = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n            if num_channels_unet == 4:\n                init_latents_proper = image_latents[:1]\n                init_mask = mask[:1]\n                if i < len(timesteps) - 1:\n                    init_latents_proper = self.scheduler.add_noise(init_latents_proper, noise, torch.tensor([t]))\n                latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n            if i == len(timesteps) - 1 or (i + 1) % self.scheduler.order == 0:\n                progress_bar.update()\n                if callback is not None and i % callback_steps == 0:\n                    callback(i, t, latents)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.unet.to('cpu')\n        self.controlnet.to('cpu')\n        torch.cuda.empty_cache()\n    if not output_type == 'latent':\n        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n        (image, has_nsfw_concept) = self.run_safety_checker(image, device, prompt_embeds.dtype)\n    else:\n        image = latents\n        has_nsfw_concept = None\n    if has_nsfw_concept is None:\n        do_denormalize = [True] * image.shape[0]\n    else:\n        do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n    image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.final_offload_hook.offload()\n    if not return_dict:\n        return (image, has_nsfw_concept)\n    return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)",
            "@torch.no_grad()\n@replace_example_docstring(EXAMPLE_DOC_STRING)\ndef __call__(self, prompt: Union[str, List[str]]=None, image: Union[torch.Tensor, PIL.Image.Image]=None, mask_image: Union[torch.Tensor, PIL.Image.Image]=None, control_image: Union[torch.FloatTensor, PIL.Image.Image, np.ndarray, List[torch.FloatTensor], List[PIL.Image.Image], List[np.ndarray]]=None, height: Optional[int]=None, width: Optional[int]=None, strength: float=1.0, num_inference_steps: int=50, guidance_scale: float=7.5, negative_prompt: Optional[Union[str, List[str]]]=None, num_images_per_prompt: Optional[int]=1, eta: float=0.0, generator: Optional[Union[torch.Generator, List[torch.Generator]]]=None, latents: Optional[torch.FloatTensor]=None, prompt_embeds: Optional[torch.FloatTensor]=None, negative_prompt_embeds: Optional[torch.FloatTensor]=None, output_type: Optional[str]='pil', return_dict: bool=True, callback: Optional[Callable[[int, int, torch.FloatTensor], None]]=None, callback_steps: int=1, cross_attention_kwargs: Optional[Dict[str, Any]]=None, controlnet_conditioning_scale: Union[float, List[float]]=0.5, guess_mode: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Function invoked when calling the pipeline for generation.\\n\\n        Args:\\n            prompt (`str` or `List[str]`, *optional*):\\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\\n                instead.\\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`,\\n                    `List[List[torch.FloatTensor]]`, or `List[List[PIL.Image.Image]]`):\\n                The ControlNet input condition. ControlNet uses this input condition to generate guidance to Unet. If\\n                the type is specified as `Torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can\\n                also be accepted as an image. The dimensions of the output image defaults to `image`\\'s dimensions. If\\n                height and/or width are passed, `image` is resized according to them. If multiple ControlNets are\\n                specified in init, images must be passed as a list such that each element of the list can be correctly\\n                batched for input to a single controlnet.\\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\\n                The height in pixels of the generated image.\\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\\n                The width in pixels of the generated image.\\n            strength (`float`, *optional*, defaults to 1.):\\n                Conceptually, indicates how much to transform the masked portion of the reference `image`. Must be\\n                between 0 and 1. `image` will be used as a starting point, adding more noise to it the larger the\\n                `strength`. The number of denoising steps depends on the amount of noise initially added. When\\n                `strength` is 1, added noise will be maximum and the denoising process will run for the full number of\\n                iterations specified in `num_inference_steps`. A value of 1, therefore, essentially ignores the masked\\n                portion of the reference `image`.\\n            num_inference_steps (`int`, *optional*, defaults to 50):\\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\\n                expense of slower inference.\\n            guidance_scale (`float`, *optional*, defaults to 7.5):\\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\\n                usually at the expense of lower image quality.\\n            negative_prompt (`str` or `List[str]`, *optional*):\\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\\n                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\\n                less than `1`).\\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\\n                The number of images to generate per prompt.\\n            eta (`float`, *optional*, defaults to 0.0):\\n                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\\n                [`schedulers.DDIMScheduler`], will be ignored for others.\\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\\n                to make generation deterministic.\\n            latents (`torch.FloatTensor`, *optional*):\\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\\n                tensor will ge generated by sampling using the supplied random `generator`.\\n            prompt_embeds (`torch.FloatTensor`, *optional*):\\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\\n                provided, text embeddings will be generated from `prompt` input argument.\\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\\n                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\\n                argument.\\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\\n                The output format of the generate image. Choose between\\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\\n            return_dict (`bool`, *optional*, defaults to `True`):\\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\\n                plain tuple.\\n            callback (`Callable`, *optional*):\\n                A function that will be called every `callback_steps` steps during inference. The function will be\\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\\n            callback_steps (`int`, *optional*, defaults to 1):\\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\\n                called at every step.\\n            cross_attention_kwargs (`dict`, *optional*):\\n                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\\n                `self.processor` in\\n                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\\n            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 0.5):\\n                The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added\\n                to the residual in the original unet. If multiple ControlNets are specified in init, you can set the\\n                corresponding scale as a list. Note that by default, we use a smaller conditioning scale for inpainting\\n                than for [`~StableDiffusionControlNetPipeline.__call__`].\\n            guess_mode (`bool`, *optional*, defaults to `False`):\\n                In this mode, the ControlNet encoder will try best to recognize the content of the input image even if\\n                you remove all prompts. The `guidance_scale` between 3.0 and 5.0 is recommended.\\n\\n        Examples:\\n\\n        Returns:\\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\\n            (nsfw) content, according to the `safety_checker`.\\n        '\n    (height, width) = self._default_height_width(height, width, image)\n    self.check_inputs(prompt, control_image, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds, controlnet_conditioning_scale)\n    if prompt is not None and isinstance(prompt, str):\n        batch_size = 1\n    elif prompt is not None and isinstance(prompt, list):\n        batch_size = len(prompt)\n    else:\n        batch_size = prompt_embeds.shape[0]\n    device = self._execution_device\n    do_classifier_free_guidance = guidance_scale > 1.0\n    controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n    if isinstance(controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n        controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(controlnet.nets)\n    global_pool_conditions = controlnet.config.global_pool_conditions if isinstance(controlnet, ControlNetModel) else controlnet.nets[0].config.global_pool_conditions\n    guess_mode = guess_mode or global_pool_conditions\n    text_encoder_lora_scale = cross_attention_kwargs.get('scale', None) if cross_attention_kwargs is not None else None\n    prompt_embeds = self._encode_prompt(prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_prompt_embeds, lora_scale=text_encoder_lora_scale)\n    if isinstance(controlnet, ControlNetModel):\n        control_image = self.prepare_control_image(image=control_image, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n    elif isinstance(controlnet, MultiControlNetModel):\n        control_images = []\n        for control_image_ in control_image:\n            control_image_ = self.prepare_control_image(image=control_image_, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n            control_images.append(control_image_)\n        control_image = control_images\n    else:\n        assert False\n    (mask, masked_image, init_image) = prepare_mask_and_masked_image(image, mask_image, height, width, return_image=True)\n    self.scheduler.set_timesteps(num_inference_steps, device=device)\n    (timesteps, num_inference_steps) = self.get_timesteps(num_inference_steps=num_inference_steps, strength=strength, device=device)\n    latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n    is_strength_max = strength == 1.0\n    num_channels_latents = self.vae.config.latent_channels\n    num_channels_unet = self.unet.config.in_channels\n    return_image_latents = num_channels_unet == 4\n    latents_outputs = self.prepare_latents(batch_size * num_images_per_prompt, num_channels_latents, height, width, prompt_embeds.dtype, device, generator, latents, image=init_image, timestep=latent_timestep, is_strength_max=is_strength_max, return_noise=True, return_image_latents=return_image_latents)\n    if return_image_latents:\n        (latents, noise, image_latents) = latents_outputs\n    else:\n        (latents, noise) = latents_outputs\n    (mask, masked_image_latents) = self.prepare_mask_latents(mask, masked_image, batch_size * num_images_per_prompt, height, width, prompt_embeds.dtype, device, generator, do_classifier_free_guidance)\n    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for (i, t) in enumerate(timesteps):\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n            if guess_mode and do_classifier_free_guidance:\n                control_model_input = latents\n                control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n                controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n            else:\n                control_model_input = latent_model_input\n                controlnet_prompt_embeds = prompt_embeds\n            (down_block_res_samples, mid_block_res_sample) = self.controlnet(control_model_input, t, encoder_hidden_states=controlnet_prompt_embeds, controlnet_cond=control_image, conditioning_scale=controlnet_conditioning_scale, guess_mode=guess_mode, return_dict=False)\n            if guess_mode and do_classifier_free_guidance:\n                down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n                mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n            if num_channels_unet == 9:\n                latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds, cross_attention_kwargs=cross_attention_kwargs, down_block_additional_residuals=down_block_res_samples, mid_block_additional_residual=mid_block_res_sample, return_dict=False)[0]\n            if do_classifier_free_guidance:\n                (noise_pred_uncond, noise_pred_text) = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n            if num_channels_unet == 4:\n                init_latents_proper = image_latents[:1]\n                init_mask = mask[:1]\n                if i < len(timesteps) - 1:\n                    init_latents_proper = self.scheduler.add_noise(init_latents_proper, noise, torch.tensor([t]))\n                latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n            if i == len(timesteps) - 1 or (i + 1) % self.scheduler.order == 0:\n                progress_bar.update()\n                if callback is not None and i % callback_steps == 0:\n                    callback(i, t, latents)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.unet.to('cpu')\n        self.controlnet.to('cpu')\n        torch.cuda.empty_cache()\n    if not output_type == 'latent':\n        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n        (image, has_nsfw_concept) = self.run_safety_checker(image, device, prompt_embeds.dtype)\n    else:\n        image = latents\n        has_nsfw_concept = None\n    if has_nsfw_concept is None:\n        do_denormalize = [True] * image.shape[0]\n    else:\n        do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n    image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.final_offload_hook.offload()\n    if not return_dict:\n        return (image, has_nsfw_concept)\n    return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)",
            "@torch.no_grad()\n@replace_example_docstring(EXAMPLE_DOC_STRING)\ndef __call__(self, prompt: Union[str, List[str]]=None, image: Union[torch.Tensor, PIL.Image.Image]=None, mask_image: Union[torch.Tensor, PIL.Image.Image]=None, control_image: Union[torch.FloatTensor, PIL.Image.Image, np.ndarray, List[torch.FloatTensor], List[PIL.Image.Image], List[np.ndarray]]=None, height: Optional[int]=None, width: Optional[int]=None, strength: float=1.0, num_inference_steps: int=50, guidance_scale: float=7.5, negative_prompt: Optional[Union[str, List[str]]]=None, num_images_per_prompt: Optional[int]=1, eta: float=0.0, generator: Optional[Union[torch.Generator, List[torch.Generator]]]=None, latents: Optional[torch.FloatTensor]=None, prompt_embeds: Optional[torch.FloatTensor]=None, negative_prompt_embeds: Optional[torch.FloatTensor]=None, output_type: Optional[str]='pil', return_dict: bool=True, callback: Optional[Callable[[int, int, torch.FloatTensor], None]]=None, callback_steps: int=1, cross_attention_kwargs: Optional[Dict[str, Any]]=None, controlnet_conditioning_scale: Union[float, List[float]]=0.5, guess_mode: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Function invoked when calling the pipeline for generation.\\n\\n        Args:\\n            prompt (`str` or `List[str]`, *optional*):\\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\\n                instead.\\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`,\\n                    `List[List[torch.FloatTensor]]`, or `List[List[PIL.Image.Image]]`):\\n                The ControlNet input condition. ControlNet uses this input condition to generate guidance to Unet. If\\n                the type is specified as `Torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can\\n                also be accepted as an image. The dimensions of the output image defaults to `image`\\'s dimensions. If\\n                height and/or width are passed, `image` is resized according to them. If multiple ControlNets are\\n                specified in init, images must be passed as a list such that each element of the list can be correctly\\n                batched for input to a single controlnet.\\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\\n                The height in pixels of the generated image.\\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\\n                The width in pixels of the generated image.\\n            strength (`float`, *optional*, defaults to 1.):\\n                Conceptually, indicates how much to transform the masked portion of the reference `image`. Must be\\n                between 0 and 1. `image` will be used as a starting point, adding more noise to it the larger the\\n                `strength`. The number of denoising steps depends on the amount of noise initially added. When\\n                `strength` is 1, added noise will be maximum and the denoising process will run for the full number of\\n                iterations specified in `num_inference_steps`. A value of 1, therefore, essentially ignores the masked\\n                portion of the reference `image`.\\n            num_inference_steps (`int`, *optional*, defaults to 50):\\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\\n                expense of slower inference.\\n            guidance_scale (`float`, *optional*, defaults to 7.5):\\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\\n                usually at the expense of lower image quality.\\n            negative_prompt (`str` or `List[str]`, *optional*):\\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\\n                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\\n                less than `1`).\\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\\n                The number of images to generate per prompt.\\n            eta (`float`, *optional*, defaults to 0.0):\\n                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\\n                [`schedulers.DDIMScheduler`], will be ignored for others.\\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\\n                to make generation deterministic.\\n            latents (`torch.FloatTensor`, *optional*):\\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\\n                tensor will ge generated by sampling using the supplied random `generator`.\\n            prompt_embeds (`torch.FloatTensor`, *optional*):\\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\\n                provided, text embeddings will be generated from `prompt` input argument.\\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\\n                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\\n                argument.\\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\\n                The output format of the generate image. Choose between\\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\\n            return_dict (`bool`, *optional*, defaults to `True`):\\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\\n                plain tuple.\\n            callback (`Callable`, *optional*):\\n                A function that will be called every `callback_steps` steps during inference. The function will be\\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\\n            callback_steps (`int`, *optional*, defaults to 1):\\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\\n                called at every step.\\n            cross_attention_kwargs (`dict`, *optional*):\\n                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\\n                `self.processor` in\\n                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\\n            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 0.5):\\n                The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added\\n                to the residual in the original unet. If multiple ControlNets are specified in init, you can set the\\n                corresponding scale as a list. Note that by default, we use a smaller conditioning scale for inpainting\\n                than for [`~StableDiffusionControlNetPipeline.__call__`].\\n            guess_mode (`bool`, *optional*, defaults to `False`):\\n                In this mode, the ControlNet encoder will try best to recognize the content of the input image even if\\n                you remove all prompts. The `guidance_scale` between 3.0 and 5.0 is recommended.\\n\\n        Examples:\\n\\n        Returns:\\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\\n            (nsfw) content, according to the `safety_checker`.\\n        '\n    (height, width) = self._default_height_width(height, width, image)\n    self.check_inputs(prompt, control_image, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds, controlnet_conditioning_scale)\n    if prompt is not None and isinstance(prompt, str):\n        batch_size = 1\n    elif prompt is not None and isinstance(prompt, list):\n        batch_size = len(prompt)\n    else:\n        batch_size = prompt_embeds.shape[0]\n    device = self._execution_device\n    do_classifier_free_guidance = guidance_scale > 1.0\n    controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n    if isinstance(controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n        controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(controlnet.nets)\n    global_pool_conditions = controlnet.config.global_pool_conditions if isinstance(controlnet, ControlNetModel) else controlnet.nets[0].config.global_pool_conditions\n    guess_mode = guess_mode or global_pool_conditions\n    text_encoder_lora_scale = cross_attention_kwargs.get('scale', None) if cross_attention_kwargs is not None else None\n    prompt_embeds = self._encode_prompt(prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_prompt_embeds, lora_scale=text_encoder_lora_scale)\n    if isinstance(controlnet, ControlNetModel):\n        control_image = self.prepare_control_image(image=control_image, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n    elif isinstance(controlnet, MultiControlNetModel):\n        control_images = []\n        for control_image_ in control_image:\n            control_image_ = self.prepare_control_image(image=control_image_, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n            control_images.append(control_image_)\n        control_image = control_images\n    else:\n        assert False\n    (mask, masked_image, init_image) = prepare_mask_and_masked_image(image, mask_image, height, width, return_image=True)\n    self.scheduler.set_timesteps(num_inference_steps, device=device)\n    (timesteps, num_inference_steps) = self.get_timesteps(num_inference_steps=num_inference_steps, strength=strength, device=device)\n    latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n    is_strength_max = strength == 1.0\n    num_channels_latents = self.vae.config.latent_channels\n    num_channels_unet = self.unet.config.in_channels\n    return_image_latents = num_channels_unet == 4\n    latents_outputs = self.prepare_latents(batch_size * num_images_per_prompt, num_channels_latents, height, width, prompt_embeds.dtype, device, generator, latents, image=init_image, timestep=latent_timestep, is_strength_max=is_strength_max, return_noise=True, return_image_latents=return_image_latents)\n    if return_image_latents:\n        (latents, noise, image_latents) = latents_outputs\n    else:\n        (latents, noise) = latents_outputs\n    (mask, masked_image_latents) = self.prepare_mask_latents(mask, masked_image, batch_size * num_images_per_prompt, height, width, prompt_embeds.dtype, device, generator, do_classifier_free_guidance)\n    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for (i, t) in enumerate(timesteps):\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n            if guess_mode and do_classifier_free_guidance:\n                control_model_input = latents\n                control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n                controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n            else:\n                control_model_input = latent_model_input\n                controlnet_prompt_embeds = prompt_embeds\n            (down_block_res_samples, mid_block_res_sample) = self.controlnet(control_model_input, t, encoder_hidden_states=controlnet_prompt_embeds, controlnet_cond=control_image, conditioning_scale=controlnet_conditioning_scale, guess_mode=guess_mode, return_dict=False)\n            if guess_mode and do_classifier_free_guidance:\n                down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n                mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n            if num_channels_unet == 9:\n                latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds, cross_attention_kwargs=cross_attention_kwargs, down_block_additional_residuals=down_block_res_samples, mid_block_additional_residual=mid_block_res_sample, return_dict=False)[0]\n            if do_classifier_free_guidance:\n                (noise_pred_uncond, noise_pred_text) = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n            if num_channels_unet == 4:\n                init_latents_proper = image_latents[:1]\n                init_mask = mask[:1]\n                if i < len(timesteps) - 1:\n                    init_latents_proper = self.scheduler.add_noise(init_latents_proper, noise, torch.tensor([t]))\n                latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n            if i == len(timesteps) - 1 or (i + 1) % self.scheduler.order == 0:\n                progress_bar.update()\n                if callback is not None and i % callback_steps == 0:\n                    callback(i, t, latents)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.unet.to('cpu')\n        self.controlnet.to('cpu')\n        torch.cuda.empty_cache()\n    if not output_type == 'latent':\n        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n        (image, has_nsfw_concept) = self.run_safety_checker(image, device, prompt_embeds.dtype)\n    else:\n        image = latents\n        has_nsfw_concept = None\n    if has_nsfw_concept is None:\n        do_denormalize = [True] * image.shape[0]\n    else:\n        do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n    image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.final_offload_hook.offload()\n    if not return_dict:\n        return (image, has_nsfw_concept)\n    return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)",
            "@torch.no_grad()\n@replace_example_docstring(EXAMPLE_DOC_STRING)\ndef __call__(self, prompt: Union[str, List[str]]=None, image: Union[torch.Tensor, PIL.Image.Image]=None, mask_image: Union[torch.Tensor, PIL.Image.Image]=None, control_image: Union[torch.FloatTensor, PIL.Image.Image, np.ndarray, List[torch.FloatTensor], List[PIL.Image.Image], List[np.ndarray]]=None, height: Optional[int]=None, width: Optional[int]=None, strength: float=1.0, num_inference_steps: int=50, guidance_scale: float=7.5, negative_prompt: Optional[Union[str, List[str]]]=None, num_images_per_prompt: Optional[int]=1, eta: float=0.0, generator: Optional[Union[torch.Generator, List[torch.Generator]]]=None, latents: Optional[torch.FloatTensor]=None, prompt_embeds: Optional[torch.FloatTensor]=None, negative_prompt_embeds: Optional[torch.FloatTensor]=None, output_type: Optional[str]='pil', return_dict: bool=True, callback: Optional[Callable[[int, int, torch.FloatTensor], None]]=None, callback_steps: int=1, cross_attention_kwargs: Optional[Dict[str, Any]]=None, controlnet_conditioning_scale: Union[float, List[float]]=0.5, guess_mode: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Function invoked when calling the pipeline for generation.\\n\\n        Args:\\n            prompt (`str` or `List[str]`, *optional*):\\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\\n                instead.\\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`,\\n                    `List[List[torch.FloatTensor]]`, or `List[List[PIL.Image.Image]]`):\\n                The ControlNet input condition. ControlNet uses this input condition to generate guidance to Unet. If\\n                the type is specified as `Torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can\\n                also be accepted as an image. The dimensions of the output image defaults to `image`\\'s dimensions. If\\n                height and/or width are passed, `image` is resized according to them. If multiple ControlNets are\\n                specified in init, images must be passed as a list such that each element of the list can be correctly\\n                batched for input to a single controlnet.\\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\\n                The height in pixels of the generated image.\\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\\n                The width in pixels of the generated image.\\n            strength (`float`, *optional*, defaults to 1.):\\n                Conceptually, indicates how much to transform the masked portion of the reference `image`. Must be\\n                between 0 and 1. `image` will be used as a starting point, adding more noise to it the larger the\\n                `strength`. The number of denoising steps depends on the amount of noise initially added. When\\n                `strength` is 1, added noise will be maximum and the denoising process will run for the full number of\\n                iterations specified in `num_inference_steps`. A value of 1, therefore, essentially ignores the masked\\n                portion of the reference `image`.\\n            num_inference_steps (`int`, *optional*, defaults to 50):\\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\\n                expense of slower inference.\\n            guidance_scale (`float`, *optional*, defaults to 7.5):\\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\\n                usually at the expense of lower image quality.\\n            negative_prompt (`str` or `List[str]`, *optional*):\\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\\n                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\\n                less than `1`).\\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\\n                The number of images to generate per prompt.\\n            eta (`float`, *optional*, defaults to 0.0):\\n                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\\n                [`schedulers.DDIMScheduler`], will be ignored for others.\\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\\n                to make generation deterministic.\\n            latents (`torch.FloatTensor`, *optional*):\\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\\n                tensor will ge generated by sampling using the supplied random `generator`.\\n            prompt_embeds (`torch.FloatTensor`, *optional*):\\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\\n                provided, text embeddings will be generated from `prompt` input argument.\\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\\n                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\\n                argument.\\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\\n                The output format of the generate image. Choose between\\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\\n            return_dict (`bool`, *optional*, defaults to `True`):\\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\\n                plain tuple.\\n            callback (`Callable`, *optional*):\\n                A function that will be called every `callback_steps` steps during inference. The function will be\\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\\n            callback_steps (`int`, *optional*, defaults to 1):\\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\\n                called at every step.\\n            cross_attention_kwargs (`dict`, *optional*):\\n                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\\n                `self.processor` in\\n                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\\n            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 0.5):\\n                The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added\\n                to the residual in the original unet. If multiple ControlNets are specified in init, you can set the\\n                corresponding scale as a list. Note that by default, we use a smaller conditioning scale for inpainting\\n                than for [`~StableDiffusionControlNetPipeline.__call__`].\\n            guess_mode (`bool`, *optional*, defaults to `False`):\\n                In this mode, the ControlNet encoder will try best to recognize the content of the input image even if\\n                you remove all prompts. The `guidance_scale` between 3.0 and 5.0 is recommended.\\n\\n        Examples:\\n\\n        Returns:\\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\\n            (nsfw) content, according to the `safety_checker`.\\n        '\n    (height, width) = self._default_height_width(height, width, image)\n    self.check_inputs(prompt, control_image, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds, controlnet_conditioning_scale)\n    if prompt is not None and isinstance(prompt, str):\n        batch_size = 1\n    elif prompt is not None and isinstance(prompt, list):\n        batch_size = len(prompt)\n    else:\n        batch_size = prompt_embeds.shape[0]\n    device = self._execution_device\n    do_classifier_free_guidance = guidance_scale > 1.0\n    controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n    if isinstance(controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n        controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(controlnet.nets)\n    global_pool_conditions = controlnet.config.global_pool_conditions if isinstance(controlnet, ControlNetModel) else controlnet.nets[0].config.global_pool_conditions\n    guess_mode = guess_mode or global_pool_conditions\n    text_encoder_lora_scale = cross_attention_kwargs.get('scale', None) if cross_attention_kwargs is not None else None\n    prompt_embeds = self._encode_prompt(prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_prompt_embeds, lora_scale=text_encoder_lora_scale)\n    if isinstance(controlnet, ControlNetModel):\n        control_image = self.prepare_control_image(image=control_image, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n    elif isinstance(controlnet, MultiControlNetModel):\n        control_images = []\n        for control_image_ in control_image:\n            control_image_ = self.prepare_control_image(image=control_image_, width=width, height=height, batch_size=batch_size * num_images_per_prompt, num_images_per_prompt=num_images_per_prompt, device=device, dtype=controlnet.dtype, do_classifier_free_guidance=do_classifier_free_guidance, guess_mode=guess_mode)\n            control_images.append(control_image_)\n        control_image = control_images\n    else:\n        assert False\n    (mask, masked_image, init_image) = prepare_mask_and_masked_image(image, mask_image, height, width, return_image=True)\n    self.scheduler.set_timesteps(num_inference_steps, device=device)\n    (timesteps, num_inference_steps) = self.get_timesteps(num_inference_steps=num_inference_steps, strength=strength, device=device)\n    latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n    is_strength_max = strength == 1.0\n    num_channels_latents = self.vae.config.latent_channels\n    num_channels_unet = self.unet.config.in_channels\n    return_image_latents = num_channels_unet == 4\n    latents_outputs = self.prepare_latents(batch_size * num_images_per_prompt, num_channels_latents, height, width, prompt_embeds.dtype, device, generator, latents, image=init_image, timestep=latent_timestep, is_strength_max=is_strength_max, return_noise=True, return_image_latents=return_image_latents)\n    if return_image_latents:\n        (latents, noise, image_latents) = latents_outputs\n    else:\n        (latents, noise) = latents_outputs\n    (mask, masked_image_latents) = self.prepare_mask_latents(mask, masked_image, batch_size * num_images_per_prompt, height, width, prompt_embeds.dtype, device, generator, do_classifier_free_guidance)\n    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for (i, t) in enumerate(timesteps):\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n            if guess_mode and do_classifier_free_guidance:\n                control_model_input = latents\n                control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n                controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n            else:\n                control_model_input = latent_model_input\n                controlnet_prompt_embeds = prompt_embeds\n            (down_block_res_samples, mid_block_res_sample) = self.controlnet(control_model_input, t, encoder_hidden_states=controlnet_prompt_embeds, controlnet_cond=control_image, conditioning_scale=controlnet_conditioning_scale, guess_mode=guess_mode, return_dict=False)\n            if guess_mode and do_classifier_free_guidance:\n                down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n                mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n            if num_channels_unet == 9:\n                latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds, cross_attention_kwargs=cross_attention_kwargs, down_block_additional_residuals=down_block_res_samples, mid_block_additional_residual=mid_block_res_sample, return_dict=False)[0]\n            if do_classifier_free_guidance:\n                (noise_pred_uncond, noise_pred_text) = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n            if num_channels_unet == 4:\n                init_latents_proper = image_latents[:1]\n                init_mask = mask[:1]\n                if i < len(timesteps) - 1:\n                    init_latents_proper = self.scheduler.add_noise(init_latents_proper, noise, torch.tensor([t]))\n                latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n            if i == len(timesteps) - 1 or (i + 1) % self.scheduler.order == 0:\n                progress_bar.update()\n                if callback is not None and i % callback_steps == 0:\n                    callback(i, t, latents)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.unet.to('cpu')\n        self.controlnet.to('cpu')\n        torch.cuda.empty_cache()\n    if not output_type == 'latent':\n        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n        (image, has_nsfw_concept) = self.run_safety_checker(image, device, prompt_embeds.dtype)\n    else:\n        image = latents\n        has_nsfw_concept = None\n    if has_nsfw_concept is None:\n        do_denormalize = [True] * image.shape[0]\n    else:\n        do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n    image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n    if hasattr(self, 'final_offload_hook') and self.final_offload_hook is not None:\n        self.final_offload_hook.offload()\n    if not return_dict:\n        return (image, has_nsfw_concept)\n    return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)"
        ]
    }
]