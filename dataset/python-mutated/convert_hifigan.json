[
    {
        "func_name": "load_weights",
        "original": "def load_weights(checkpoint, hf_model, config):\n    hf_model.apply_weight_norm()\n    hf_model.conv_pre.weight_g.data = checkpoint['input_conv.weight_g']\n    hf_model.conv_pre.weight_v.data = checkpoint['input_conv.weight_v']\n    hf_model.conv_pre.bias.data = checkpoint['input_conv.bias']\n    for i in range(len(config.upsample_rates)):\n        hf_model.upsampler[i].weight_g.data = checkpoint[f'upsamples.{i}.1.weight_g']\n        hf_model.upsampler[i].weight_v.data = checkpoint[f'upsamples.{i}.1.weight_v']\n        hf_model.upsampler[i].bias.data = checkpoint[f'upsamples.{i}.1.bias']\n    for i in range(len(config.upsample_rates) * len(config.resblock_kernel_sizes)):\n        for j in range(len(config.resblock_dilation_sizes)):\n            hf_model.resblocks[i].convs1[j].weight_g.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_g']\n            hf_model.resblocks[i].convs1[j].weight_v.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_v']\n            hf_model.resblocks[i].convs1[j].bias.data = checkpoint[f'blocks.{i}.convs1.{j}.1.bias']\n            hf_model.resblocks[i].convs2[j].weight_g.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_g']\n            hf_model.resblocks[i].convs2[j].weight_v.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_v']\n            hf_model.resblocks[i].convs2[j].bias.data = checkpoint[f'blocks.{i}.convs2.{j}.1.bias']\n    hf_model.conv_post.weight_g.data = checkpoint['output_conv.1.weight_g']\n    hf_model.conv_post.weight_v.data = checkpoint['output_conv.1.weight_v']\n    hf_model.conv_post.bias.data = checkpoint['output_conv.1.bias']\n    hf_model.remove_weight_norm()",
        "mutated": [
            "def load_weights(checkpoint, hf_model, config):\n    if False:\n        i = 10\n    hf_model.apply_weight_norm()\n    hf_model.conv_pre.weight_g.data = checkpoint['input_conv.weight_g']\n    hf_model.conv_pre.weight_v.data = checkpoint['input_conv.weight_v']\n    hf_model.conv_pre.bias.data = checkpoint['input_conv.bias']\n    for i in range(len(config.upsample_rates)):\n        hf_model.upsampler[i].weight_g.data = checkpoint[f'upsamples.{i}.1.weight_g']\n        hf_model.upsampler[i].weight_v.data = checkpoint[f'upsamples.{i}.1.weight_v']\n        hf_model.upsampler[i].bias.data = checkpoint[f'upsamples.{i}.1.bias']\n    for i in range(len(config.upsample_rates) * len(config.resblock_kernel_sizes)):\n        for j in range(len(config.resblock_dilation_sizes)):\n            hf_model.resblocks[i].convs1[j].weight_g.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_g']\n            hf_model.resblocks[i].convs1[j].weight_v.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_v']\n            hf_model.resblocks[i].convs1[j].bias.data = checkpoint[f'blocks.{i}.convs1.{j}.1.bias']\n            hf_model.resblocks[i].convs2[j].weight_g.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_g']\n            hf_model.resblocks[i].convs2[j].weight_v.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_v']\n            hf_model.resblocks[i].convs2[j].bias.data = checkpoint[f'blocks.{i}.convs2.{j}.1.bias']\n    hf_model.conv_post.weight_g.data = checkpoint['output_conv.1.weight_g']\n    hf_model.conv_post.weight_v.data = checkpoint['output_conv.1.weight_v']\n    hf_model.conv_post.bias.data = checkpoint['output_conv.1.bias']\n    hf_model.remove_weight_norm()",
            "def load_weights(checkpoint, hf_model, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    hf_model.apply_weight_norm()\n    hf_model.conv_pre.weight_g.data = checkpoint['input_conv.weight_g']\n    hf_model.conv_pre.weight_v.data = checkpoint['input_conv.weight_v']\n    hf_model.conv_pre.bias.data = checkpoint['input_conv.bias']\n    for i in range(len(config.upsample_rates)):\n        hf_model.upsampler[i].weight_g.data = checkpoint[f'upsamples.{i}.1.weight_g']\n        hf_model.upsampler[i].weight_v.data = checkpoint[f'upsamples.{i}.1.weight_v']\n        hf_model.upsampler[i].bias.data = checkpoint[f'upsamples.{i}.1.bias']\n    for i in range(len(config.upsample_rates) * len(config.resblock_kernel_sizes)):\n        for j in range(len(config.resblock_dilation_sizes)):\n            hf_model.resblocks[i].convs1[j].weight_g.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_g']\n            hf_model.resblocks[i].convs1[j].weight_v.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_v']\n            hf_model.resblocks[i].convs1[j].bias.data = checkpoint[f'blocks.{i}.convs1.{j}.1.bias']\n            hf_model.resblocks[i].convs2[j].weight_g.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_g']\n            hf_model.resblocks[i].convs2[j].weight_v.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_v']\n            hf_model.resblocks[i].convs2[j].bias.data = checkpoint[f'blocks.{i}.convs2.{j}.1.bias']\n    hf_model.conv_post.weight_g.data = checkpoint['output_conv.1.weight_g']\n    hf_model.conv_post.weight_v.data = checkpoint['output_conv.1.weight_v']\n    hf_model.conv_post.bias.data = checkpoint['output_conv.1.bias']\n    hf_model.remove_weight_norm()",
            "def load_weights(checkpoint, hf_model, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    hf_model.apply_weight_norm()\n    hf_model.conv_pre.weight_g.data = checkpoint['input_conv.weight_g']\n    hf_model.conv_pre.weight_v.data = checkpoint['input_conv.weight_v']\n    hf_model.conv_pre.bias.data = checkpoint['input_conv.bias']\n    for i in range(len(config.upsample_rates)):\n        hf_model.upsampler[i].weight_g.data = checkpoint[f'upsamples.{i}.1.weight_g']\n        hf_model.upsampler[i].weight_v.data = checkpoint[f'upsamples.{i}.1.weight_v']\n        hf_model.upsampler[i].bias.data = checkpoint[f'upsamples.{i}.1.bias']\n    for i in range(len(config.upsample_rates) * len(config.resblock_kernel_sizes)):\n        for j in range(len(config.resblock_dilation_sizes)):\n            hf_model.resblocks[i].convs1[j].weight_g.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_g']\n            hf_model.resblocks[i].convs1[j].weight_v.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_v']\n            hf_model.resblocks[i].convs1[j].bias.data = checkpoint[f'blocks.{i}.convs1.{j}.1.bias']\n            hf_model.resblocks[i].convs2[j].weight_g.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_g']\n            hf_model.resblocks[i].convs2[j].weight_v.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_v']\n            hf_model.resblocks[i].convs2[j].bias.data = checkpoint[f'blocks.{i}.convs2.{j}.1.bias']\n    hf_model.conv_post.weight_g.data = checkpoint['output_conv.1.weight_g']\n    hf_model.conv_post.weight_v.data = checkpoint['output_conv.1.weight_v']\n    hf_model.conv_post.bias.data = checkpoint['output_conv.1.bias']\n    hf_model.remove_weight_norm()",
            "def load_weights(checkpoint, hf_model, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    hf_model.apply_weight_norm()\n    hf_model.conv_pre.weight_g.data = checkpoint['input_conv.weight_g']\n    hf_model.conv_pre.weight_v.data = checkpoint['input_conv.weight_v']\n    hf_model.conv_pre.bias.data = checkpoint['input_conv.bias']\n    for i in range(len(config.upsample_rates)):\n        hf_model.upsampler[i].weight_g.data = checkpoint[f'upsamples.{i}.1.weight_g']\n        hf_model.upsampler[i].weight_v.data = checkpoint[f'upsamples.{i}.1.weight_v']\n        hf_model.upsampler[i].bias.data = checkpoint[f'upsamples.{i}.1.bias']\n    for i in range(len(config.upsample_rates) * len(config.resblock_kernel_sizes)):\n        for j in range(len(config.resblock_dilation_sizes)):\n            hf_model.resblocks[i].convs1[j].weight_g.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_g']\n            hf_model.resblocks[i].convs1[j].weight_v.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_v']\n            hf_model.resblocks[i].convs1[j].bias.data = checkpoint[f'blocks.{i}.convs1.{j}.1.bias']\n            hf_model.resblocks[i].convs2[j].weight_g.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_g']\n            hf_model.resblocks[i].convs2[j].weight_v.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_v']\n            hf_model.resblocks[i].convs2[j].bias.data = checkpoint[f'blocks.{i}.convs2.{j}.1.bias']\n    hf_model.conv_post.weight_g.data = checkpoint['output_conv.1.weight_g']\n    hf_model.conv_post.weight_v.data = checkpoint['output_conv.1.weight_v']\n    hf_model.conv_post.bias.data = checkpoint['output_conv.1.bias']\n    hf_model.remove_weight_norm()",
            "def load_weights(checkpoint, hf_model, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    hf_model.apply_weight_norm()\n    hf_model.conv_pre.weight_g.data = checkpoint['input_conv.weight_g']\n    hf_model.conv_pre.weight_v.data = checkpoint['input_conv.weight_v']\n    hf_model.conv_pre.bias.data = checkpoint['input_conv.bias']\n    for i in range(len(config.upsample_rates)):\n        hf_model.upsampler[i].weight_g.data = checkpoint[f'upsamples.{i}.1.weight_g']\n        hf_model.upsampler[i].weight_v.data = checkpoint[f'upsamples.{i}.1.weight_v']\n        hf_model.upsampler[i].bias.data = checkpoint[f'upsamples.{i}.1.bias']\n    for i in range(len(config.upsample_rates) * len(config.resblock_kernel_sizes)):\n        for j in range(len(config.resblock_dilation_sizes)):\n            hf_model.resblocks[i].convs1[j].weight_g.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_g']\n            hf_model.resblocks[i].convs1[j].weight_v.data = checkpoint[f'blocks.{i}.convs1.{j}.1.weight_v']\n            hf_model.resblocks[i].convs1[j].bias.data = checkpoint[f'blocks.{i}.convs1.{j}.1.bias']\n            hf_model.resblocks[i].convs2[j].weight_g.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_g']\n            hf_model.resblocks[i].convs2[j].weight_v.data = checkpoint[f'blocks.{i}.convs2.{j}.1.weight_v']\n            hf_model.resblocks[i].convs2[j].bias.data = checkpoint[f'blocks.{i}.convs2.{j}.1.bias']\n    hf_model.conv_post.weight_g.data = checkpoint['output_conv.1.weight_g']\n    hf_model.conv_post.weight_v.data = checkpoint['output_conv.1.weight_v']\n    hf_model.conv_post.bias.data = checkpoint['output_conv.1.bias']\n    hf_model.remove_weight_norm()"
        ]
    },
    {
        "func_name": "convert_hifigan_checkpoint",
        "original": "@torch.no_grad()\ndef convert_hifigan_checkpoint(checkpoint_path, stats_path, pytorch_dump_folder_path, config_path=None, repo_id=None):\n    if config_path is not None:\n        config = SpeechT5HifiGanConfig.from_pretrained(config_path)\n    else:\n        config = SpeechT5HifiGanConfig()\n    model = SpeechT5HifiGan(config)\n    orig_checkpoint = torch.load(checkpoint_path)\n    load_weights(orig_checkpoint['model']['generator'], model, config)\n    stats = np.load(stats_path)\n    mean = stats[0].reshape(-1)\n    scale = stats[1].reshape(-1)\n    model.mean = torch.from_numpy(mean).float()\n    model.scale = torch.from_numpy(scale).float()\n    model.save_pretrained(pytorch_dump_folder_path)\n    if repo_id:\n        print('Pushing to the hub...')\n        model.push_to_hub(repo_id)",
        "mutated": [
            "@torch.no_grad()\ndef convert_hifigan_checkpoint(checkpoint_path, stats_path, pytorch_dump_folder_path, config_path=None, repo_id=None):\n    if False:\n        i = 10\n    if config_path is not None:\n        config = SpeechT5HifiGanConfig.from_pretrained(config_path)\n    else:\n        config = SpeechT5HifiGanConfig()\n    model = SpeechT5HifiGan(config)\n    orig_checkpoint = torch.load(checkpoint_path)\n    load_weights(orig_checkpoint['model']['generator'], model, config)\n    stats = np.load(stats_path)\n    mean = stats[0].reshape(-1)\n    scale = stats[1].reshape(-1)\n    model.mean = torch.from_numpy(mean).float()\n    model.scale = torch.from_numpy(scale).float()\n    model.save_pretrained(pytorch_dump_folder_path)\n    if repo_id:\n        print('Pushing to the hub...')\n        model.push_to_hub(repo_id)",
            "@torch.no_grad()\ndef convert_hifigan_checkpoint(checkpoint_path, stats_path, pytorch_dump_folder_path, config_path=None, repo_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if config_path is not None:\n        config = SpeechT5HifiGanConfig.from_pretrained(config_path)\n    else:\n        config = SpeechT5HifiGanConfig()\n    model = SpeechT5HifiGan(config)\n    orig_checkpoint = torch.load(checkpoint_path)\n    load_weights(orig_checkpoint['model']['generator'], model, config)\n    stats = np.load(stats_path)\n    mean = stats[0].reshape(-1)\n    scale = stats[1].reshape(-1)\n    model.mean = torch.from_numpy(mean).float()\n    model.scale = torch.from_numpy(scale).float()\n    model.save_pretrained(pytorch_dump_folder_path)\n    if repo_id:\n        print('Pushing to the hub...')\n        model.push_to_hub(repo_id)",
            "@torch.no_grad()\ndef convert_hifigan_checkpoint(checkpoint_path, stats_path, pytorch_dump_folder_path, config_path=None, repo_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if config_path is not None:\n        config = SpeechT5HifiGanConfig.from_pretrained(config_path)\n    else:\n        config = SpeechT5HifiGanConfig()\n    model = SpeechT5HifiGan(config)\n    orig_checkpoint = torch.load(checkpoint_path)\n    load_weights(orig_checkpoint['model']['generator'], model, config)\n    stats = np.load(stats_path)\n    mean = stats[0].reshape(-1)\n    scale = stats[1].reshape(-1)\n    model.mean = torch.from_numpy(mean).float()\n    model.scale = torch.from_numpy(scale).float()\n    model.save_pretrained(pytorch_dump_folder_path)\n    if repo_id:\n        print('Pushing to the hub...')\n        model.push_to_hub(repo_id)",
            "@torch.no_grad()\ndef convert_hifigan_checkpoint(checkpoint_path, stats_path, pytorch_dump_folder_path, config_path=None, repo_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if config_path is not None:\n        config = SpeechT5HifiGanConfig.from_pretrained(config_path)\n    else:\n        config = SpeechT5HifiGanConfig()\n    model = SpeechT5HifiGan(config)\n    orig_checkpoint = torch.load(checkpoint_path)\n    load_weights(orig_checkpoint['model']['generator'], model, config)\n    stats = np.load(stats_path)\n    mean = stats[0].reshape(-1)\n    scale = stats[1].reshape(-1)\n    model.mean = torch.from_numpy(mean).float()\n    model.scale = torch.from_numpy(scale).float()\n    model.save_pretrained(pytorch_dump_folder_path)\n    if repo_id:\n        print('Pushing to the hub...')\n        model.push_to_hub(repo_id)",
            "@torch.no_grad()\ndef convert_hifigan_checkpoint(checkpoint_path, stats_path, pytorch_dump_folder_path, config_path=None, repo_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if config_path is not None:\n        config = SpeechT5HifiGanConfig.from_pretrained(config_path)\n    else:\n        config = SpeechT5HifiGanConfig()\n    model = SpeechT5HifiGan(config)\n    orig_checkpoint = torch.load(checkpoint_path)\n    load_weights(orig_checkpoint['model']['generator'], model, config)\n    stats = np.load(stats_path)\n    mean = stats[0].reshape(-1)\n    scale = stats[1].reshape(-1)\n    model.mean = torch.from_numpy(mean).float()\n    model.scale = torch.from_numpy(scale).float()\n    model.save_pretrained(pytorch_dump_folder_path)\n    if repo_id:\n        print('Pushing to the hub...')\n        model.push_to_hub(repo_id)"
        ]
    }
]