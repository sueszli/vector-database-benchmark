[
    {
        "func_name": "set_random_seed",
        "original": "def set_random_seed(seed):\n    \"\"\"Set random seed for reproducability.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    paddle.seed(seed)\n    fleet.meta_parallel.model_parallel_random_seed(seed)",
        "mutated": [
            "def set_random_seed(seed):\n    if False:\n        i = 10\n    'Set random seed for reproducability.'\n    random.seed(seed)\n    np.random.seed(seed)\n    paddle.seed(seed)\n    fleet.meta_parallel.model_parallel_random_seed(seed)",
            "def set_random_seed(seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Set random seed for reproducability.'\n    random.seed(seed)\n    np.random.seed(seed)\n    paddle.seed(seed)\n    fleet.meta_parallel.model_parallel_random_seed(seed)",
            "def set_random_seed(seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Set random seed for reproducability.'\n    random.seed(seed)\n    np.random.seed(seed)\n    paddle.seed(seed)\n    fleet.meta_parallel.model_parallel_random_seed(seed)",
            "def set_random_seed(seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Set random seed for reproducability.'\n    random.seed(seed)\n    np.random.seed(seed)\n    paddle.seed(seed)\n    fleet.meta_parallel.model_parallel_random_seed(seed)",
            "def set_random_seed(seed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Set random seed for reproducability.'\n    random.seed(seed)\n    np.random.seed(seed)\n    paddle.seed(seed)\n    fleet.meta_parallel.model_parallel_random_seed(seed)"
        ]
    },
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    strategy = fleet.DistributedStrategy()\n    fleet.init(is_collective=True, strategy=strategy)",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    strategy = fleet.DistributedStrategy()\n    fleet.init(is_collective=True, strategy=strategy)",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    strategy = fleet.DistributedStrategy()\n    fleet.init(is_collective=True, strategy=strategy)",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    strategy = fleet.DistributedStrategy()\n    fleet.init(is_collective=True, strategy=strategy)",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    strategy = fleet.DistributedStrategy()\n    fleet.init(is_collective=True, strategy=strategy)",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    strategy = fleet.DistributedStrategy()\n    fleet.init(is_collective=True, strategy=strategy)"
        ]
    },
    {
        "func_name": "test_parallel_margin_softmax_cross_entropy",
        "original": "def test_parallel_margin_softmax_cross_entropy(self):\n    margin1s = [1.0, 1.0, 1.35]\n    margin2s = [0.5, 0.0, 0.0]\n    margin3s = [0.0, 0.35, 0.0]\n    scales = [64.0, 64.0, 64.0]\n    rank_id = dist.get_rank()\n    num_trainer = dist.get_world_size()\n    batch_size = 2\n    feature_length = 4\n    seed = 1025\n    set_random_seed(seed)\n    paddle.seed(rank_id * 10)\n    random.seed(seed)\n    np.random.seed(seed)\n    check_group = dist.new_group(list(range(num_trainer)))\n    for dtype in ('float32', 'float64'):\n        num_class_per_cards = [[4, 8], [2, 2], [4, 2], [3, 9]]\n        for num_class_per_card in num_class_per_cards:\n            num_class = np.sum(num_class_per_card)\n            for (margin1, margin2, margin3, scale) in zip(margin1s, margin2s, margin3s, scales):\n                for _ in range(5):\n                    np_label = np.random.randint(0, num_class, (batch_size,))\n                    label = paddle.to_tensor(np_label, dtype='int64')\n                    input = paddle.randn(shape=[batch_size, feature_length], dtype=dtype)\n                    input.stop_gradient = False\n                    input_l2 = paddle.sqrt(paddle.sum(paddle.square(input), axis=1, keepdim=True))\n                    norm_input = paddle.divide(input, input_l2)\n                    weight = paddle.randn(shape=[feature_length, num_class_per_card[rank_id]], dtype=dtype)\n                    weight.stop_gradient = False\n                    weight_l2 = paddle.sqrt(paddle.sum(paddle.square(weight), axis=0, keepdim=True))\n                    norm_weight = paddle.divide(weight, weight_l2)\n                    data = paddle.matmul(norm_input, norm_weight)\n                    data.retain_grads()\n                    data.stop_gradient = False\n                    sta = np.sum(num_class_per_card[:rank_id]) if rank_id > 0 else 0\n                    end = np.sum(num_class_per_card[:rank_id + 1])\n                    integral_data = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_data[:, sta:end] = data.clone().detach().numpy()\n                    integral_data = paddle.to_tensor(integral_data, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_data, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_data = integral_data.detach().clone()\n                    integral_data.retain_grads()\n                    integral_data.stop_gradient = False\n                    theta = paddle.acos(integral_data)\n                    one_hot_label = paddle.nn.functional.one_hot(label, num_classes=num_class)\n                    one_hot_label.stop_gradient = False\n                    if margin1 != 1.0:\n                        theta = margin1 * theta\n                    if margin2 != 0.0:\n                        theta = theta + margin2\n                    margin_cos = paddle.cos(theta)\n                    if margin3 != 0.0:\n                        margin_cos = margin_cos - margin3\n                    diff = one_hot_label * (margin_cos - integral_data)\n                    arc_data = (integral_data + diff) * scale\n                    (loss_a, softmax_a) = paddle.nn.functional.margin_cross_entropy(data, label, margin1=margin1, margin2=margin2, margin3=margin3, scale=scale, group=check_group, return_softmax=True, reduction=None)\n                    (loss_b, softmax_b) = paddle.nn.functional.softmax_with_cross_entropy(logits=arc_data, label=paddle.reshape(label, (-1, 1)), return_softmax=True)\n                    np.testing.assert_allclose(loss_a.numpy(), loss_b.numpy(), rtol=1e-05, atol=1e-07)\n                    integral_prob = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_prob[:, sta:end] = softmax_a.clone().detach().numpy()\n                    integral_prob = paddle.to_tensor(integral_prob, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_prob, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_prob = integral_prob.detach().clone()\n                    integral_prob.stop_gradient = False\n                    np.testing.assert_allclose(integral_prob.numpy(), softmax_b.numpy(), rtol=1e-05, atol=1e-06)\n                    loss_a = loss_a.sum() / batch_size\n                    loss_b = loss_b.sum() / batch_size\n                    loss_a.backward()\n                    loss_b.backward()\n                    integral_grad = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_grad[:, sta:end] = data.grad.clone().detach()\n                    integral_grad = paddle.to_tensor(integral_grad, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_grad, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    np.testing.assert_allclose(integral_data.grad.numpy(False), integral_grad.numpy(False), rtol=1e-05, atol=1e-07)",
        "mutated": [
            "def test_parallel_margin_softmax_cross_entropy(self):\n    if False:\n        i = 10\n    margin1s = [1.0, 1.0, 1.35]\n    margin2s = [0.5, 0.0, 0.0]\n    margin3s = [0.0, 0.35, 0.0]\n    scales = [64.0, 64.0, 64.0]\n    rank_id = dist.get_rank()\n    num_trainer = dist.get_world_size()\n    batch_size = 2\n    feature_length = 4\n    seed = 1025\n    set_random_seed(seed)\n    paddle.seed(rank_id * 10)\n    random.seed(seed)\n    np.random.seed(seed)\n    check_group = dist.new_group(list(range(num_trainer)))\n    for dtype in ('float32', 'float64'):\n        num_class_per_cards = [[4, 8], [2, 2], [4, 2], [3, 9]]\n        for num_class_per_card in num_class_per_cards:\n            num_class = np.sum(num_class_per_card)\n            for (margin1, margin2, margin3, scale) in zip(margin1s, margin2s, margin3s, scales):\n                for _ in range(5):\n                    np_label = np.random.randint(0, num_class, (batch_size,))\n                    label = paddle.to_tensor(np_label, dtype='int64')\n                    input = paddle.randn(shape=[batch_size, feature_length], dtype=dtype)\n                    input.stop_gradient = False\n                    input_l2 = paddle.sqrt(paddle.sum(paddle.square(input), axis=1, keepdim=True))\n                    norm_input = paddle.divide(input, input_l2)\n                    weight = paddle.randn(shape=[feature_length, num_class_per_card[rank_id]], dtype=dtype)\n                    weight.stop_gradient = False\n                    weight_l2 = paddle.sqrt(paddle.sum(paddle.square(weight), axis=0, keepdim=True))\n                    norm_weight = paddle.divide(weight, weight_l2)\n                    data = paddle.matmul(norm_input, norm_weight)\n                    data.retain_grads()\n                    data.stop_gradient = False\n                    sta = np.sum(num_class_per_card[:rank_id]) if rank_id > 0 else 0\n                    end = np.sum(num_class_per_card[:rank_id + 1])\n                    integral_data = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_data[:, sta:end] = data.clone().detach().numpy()\n                    integral_data = paddle.to_tensor(integral_data, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_data, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_data = integral_data.detach().clone()\n                    integral_data.retain_grads()\n                    integral_data.stop_gradient = False\n                    theta = paddle.acos(integral_data)\n                    one_hot_label = paddle.nn.functional.one_hot(label, num_classes=num_class)\n                    one_hot_label.stop_gradient = False\n                    if margin1 != 1.0:\n                        theta = margin1 * theta\n                    if margin2 != 0.0:\n                        theta = theta + margin2\n                    margin_cos = paddle.cos(theta)\n                    if margin3 != 0.0:\n                        margin_cos = margin_cos - margin3\n                    diff = one_hot_label * (margin_cos - integral_data)\n                    arc_data = (integral_data + diff) * scale\n                    (loss_a, softmax_a) = paddle.nn.functional.margin_cross_entropy(data, label, margin1=margin1, margin2=margin2, margin3=margin3, scale=scale, group=check_group, return_softmax=True, reduction=None)\n                    (loss_b, softmax_b) = paddle.nn.functional.softmax_with_cross_entropy(logits=arc_data, label=paddle.reshape(label, (-1, 1)), return_softmax=True)\n                    np.testing.assert_allclose(loss_a.numpy(), loss_b.numpy(), rtol=1e-05, atol=1e-07)\n                    integral_prob = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_prob[:, sta:end] = softmax_a.clone().detach().numpy()\n                    integral_prob = paddle.to_tensor(integral_prob, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_prob, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_prob = integral_prob.detach().clone()\n                    integral_prob.stop_gradient = False\n                    np.testing.assert_allclose(integral_prob.numpy(), softmax_b.numpy(), rtol=1e-05, atol=1e-06)\n                    loss_a = loss_a.sum() / batch_size\n                    loss_b = loss_b.sum() / batch_size\n                    loss_a.backward()\n                    loss_b.backward()\n                    integral_grad = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_grad[:, sta:end] = data.grad.clone().detach()\n                    integral_grad = paddle.to_tensor(integral_grad, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_grad, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    np.testing.assert_allclose(integral_data.grad.numpy(False), integral_grad.numpy(False), rtol=1e-05, atol=1e-07)",
            "def test_parallel_margin_softmax_cross_entropy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    margin1s = [1.0, 1.0, 1.35]\n    margin2s = [0.5, 0.0, 0.0]\n    margin3s = [0.0, 0.35, 0.0]\n    scales = [64.0, 64.0, 64.0]\n    rank_id = dist.get_rank()\n    num_trainer = dist.get_world_size()\n    batch_size = 2\n    feature_length = 4\n    seed = 1025\n    set_random_seed(seed)\n    paddle.seed(rank_id * 10)\n    random.seed(seed)\n    np.random.seed(seed)\n    check_group = dist.new_group(list(range(num_trainer)))\n    for dtype in ('float32', 'float64'):\n        num_class_per_cards = [[4, 8], [2, 2], [4, 2], [3, 9]]\n        for num_class_per_card in num_class_per_cards:\n            num_class = np.sum(num_class_per_card)\n            for (margin1, margin2, margin3, scale) in zip(margin1s, margin2s, margin3s, scales):\n                for _ in range(5):\n                    np_label = np.random.randint(0, num_class, (batch_size,))\n                    label = paddle.to_tensor(np_label, dtype='int64')\n                    input = paddle.randn(shape=[batch_size, feature_length], dtype=dtype)\n                    input.stop_gradient = False\n                    input_l2 = paddle.sqrt(paddle.sum(paddle.square(input), axis=1, keepdim=True))\n                    norm_input = paddle.divide(input, input_l2)\n                    weight = paddle.randn(shape=[feature_length, num_class_per_card[rank_id]], dtype=dtype)\n                    weight.stop_gradient = False\n                    weight_l2 = paddle.sqrt(paddle.sum(paddle.square(weight), axis=0, keepdim=True))\n                    norm_weight = paddle.divide(weight, weight_l2)\n                    data = paddle.matmul(norm_input, norm_weight)\n                    data.retain_grads()\n                    data.stop_gradient = False\n                    sta = np.sum(num_class_per_card[:rank_id]) if rank_id > 0 else 0\n                    end = np.sum(num_class_per_card[:rank_id + 1])\n                    integral_data = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_data[:, sta:end] = data.clone().detach().numpy()\n                    integral_data = paddle.to_tensor(integral_data, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_data, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_data = integral_data.detach().clone()\n                    integral_data.retain_grads()\n                    integral_data.stop_gradient = False\n                    theta = paddle.acos(integral_data)\n                    one_hot_label = paddle.nn.functional.one_hot(label, num_classes=num_class)\n                    one_hot_label.stop_gradient = False\n                    if margin1 != 1.0:\n                        theta = margin1 * theta\n                    if margin2 != 0.0:\n                        theta = theta + margin2\n                    margin_cos = paddle.cos(theta)\n                    if margin3 != 0.0:\n                        margin_cos = margin_cos - margin3\n                    diff = one_hot_label * (margin_cos - integral_data)\n                    arc_data = (integral_data + diff) * scale\n                    (loss_a, softmax_a) = paddle.nn.functional.margin_cross_entropy(data, label, margin1=margin1, margin2=margin2, margin3=margin3, scale=scale, group=check_group, return_softmax=True, reduction=None)\n                    (loss_b, softmax_b) = paddle.nn.functional.softmax_with_cross_entropy(logits=arc_data, label=paddle.reshape(label, (-1, 1)), return_softmax=True)\n                    np.testing.assert_allclose(loss_a.numpy(), loss_b.numpy(), rtol=1e-05, atol=1e-07)\n                    integral_prob = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_prob[:, sta:end] = softmax_a.clone().detach().numpy()\n                    integral_prob = paddle.to_tensor(integral_prob, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_prob, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_prob = integral_prob.detach().clone()\n                    integral_prob.stop_gradient = False\n                    np.testing.assert_allclose(integral_prob.numpy(), softmax_b.numpy(), rtol=1e-05, atol=1e-06)\n                    loss_a = loss_a.sum() / batch_size\n                    loss_b = loss_b.sum() / batch_size\n                    loss_a.backward()\n                    loss_b.backward()\n                    integral_grad = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_grad[:, sta:end] = data.grad.clone().detach()\n                    integral_grad = paddle.to_tensor(integral_grad, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_grad, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    np.testing.assert_allclose(integral_data.grad.numpy(False), integral_grad.numpy(False), rtol=1e-05, atol=1e-07)",
            "def test_parallel_margin_softmax_cross_entropy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    margin1s = [1.0, 1.0, 1.35]\n    margin2s = [0.5, 0.0, 0.0]\n    margin3s = [0.0, 0.35, 0.0]\n    scales = [64.0, 64.0, 64.0]\n    rank_id = dist.get_rank()\n    num_trainer = dist.get_world_size()\n    batch_size = 2\n    feature_length = 4\n    seed = 1025\n    set_random_seed(seed)\n    paddle.seed(rank_id * 10)\n    random.seed(seed)\n    np.random.seed(seed)\n    check_group = dist.new_group(list(range(num_trainer)))\n    for dtype in ('float32', 'float64'):\n        num_class_per_cards = [[4, 8], [2, 2], [4, 2], [3, 9]]\n        for num_class_per_card in num_class_per_cards:\n            num_class = np.sum(num_class_per_card)\n            for (margin1, margin2, margin3, scale) in zip(margin1s, margin2s, margin3s, scales):\n                for _ in range(5):\n                    np_label = np.random.randint(0, num_class, (batch_size,))\n                    label = paddle.to_tensor(np_label, dtype='int64')\n                    input = paddle.randn(shape=[batch_size, feature_length], dtype=dtype)\n                    input.stop_gradient = False\n                    input_l2 = paddle.sqrt(paddle.sum(paddle.square(input), axis=1, keepdim=True))\n                    norm_input = paddle.divide(input, input_l2)\n                    weight = paddle.randn(shape=[feature_length, num_class_per_card[rank_id]], dtype=dtype)\n                    weight.stop_gradient = False\n                    weight_l2 = paddle.sqrt(paddle.sum(paddle.square(weight), axis=0, keepdim=True))\n                    norm_weight = paddle.divide(weight, weight_l2)\n                    data = paddle.matmul(norm_input, norm_weight)\n                    data.retain_grads()\n                    data.stop_gradient = False\n                    sta = np.sum(num_class_per_card[:rank_id]) if rank_id > 0 else 0\n                    end = np.sum(num_class_per_card[:rank_id + 1])\n                    integral_data = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_data[:, sta:end] = data.clone().detach().numpy()\n                    integral_data = paddle.to_tensor(integral_data, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_data, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_data = integral_data.detach().clone()\n                    integral_data.retain_grads()\n                    integral_data.stop_gradient = False\n                    theta = paddle.acos(integral_data)\n                    one_hot_label = paddle.nn.functional.one_hot(label, num_classes=num_class)\n                    one_hot_label.stop_gradient = False\n                    if margin1 != 1.0:\n                        theta = margin1 * theta\n                    if margin2 != 0.0:\n                        theta = theta + margin2\n                    margin_cos = paddle.cos(theta)\n                    if margin3 != 0.0:\n                        margin_cos = margin_cos - margin3\n                    diff = one_hot_label * (margin_cos - integral_data)\n                    arc_data = (integral_data + diff) * scale\n                    (loss_a, softmax_a) = paddle.nn.functional.margin_cross_entropy(data, label, margin1=margin1, margin2=margin2, margin3=margin3, scale=scale, group=check_group, return_softmax=True, reduction=None)\n                    (loss_b, softmax_b) = paddle.nn.functional.softmax_with_cross_entropy(logits=arc_data, label=paddle.reshape(label, (-1, 1)), return_softmax=True)\n                    np.testing.assert_allclose(loss_a.numpy(), loss_b.numpy(), rtol=1e-05, atol=1e-07)\n                    integral_prob = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_prob[:, sta:end] = softmax_a.clone().detach().numpy()\n                    integral_prob = paddle.to_tensor(integral_prob, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_prob, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_prob = integral_prob.detach().clone()\n                    integral_prob.stop_gradient = False\n                    np.testing.assert_allclose(integral_prob.numpy(), softmax_b.numpy(), rtol=1e-05, atol=1e-06)\n                    loss_a = loss_a.sum() / batch_size\n                    loss_b = loss_b.sum() / batch_size\n                    loss_a.backward()\n                    loss_b.backward()\n                    integral_grad = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_grad[:, sta:end] = data.grad.clone().detach()\n                    integral_grad = paddle.to_tensor(integral_grad, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_grad, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    np.testing.assert_allclose(integral_data.grad.numpy(False), integral_grad.numpy(False), rtol=1e-05, atol=1e-07)",
            "def test_parallel_margin_softmax_cross_entropy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    margin1s = [1.0, 1.0, 1.35]\n    margin2s = [0.5, 0.0, 0.0]\n    margin3s = [0.0, 0.35, 0.0]\n    scales = [64.0, 64.0, 64.0]\n    rank_id = dist.get_rank()\n    num_trainer = dist.get_world_size()\n    batch_size = 2\n    feature_length = 4\n    seed = 1025\n    set_random_seed(seed)\n    paddle.seed(rank_id * 10)\n    random.seed(seed)\n    np.random.seed(seed)\n    check_group = dist.new_group(list(range(num_trainer)))\n    for dtype in ('float32', 'float64'):\n        num_class_per_cards = [[4, 8], [2, 2], [4, 2], [3, 9]]\n        for num_class_per_card in num_class_per_cards:\n            num_class = np.sum(num_class_per_card)\n            for (margin1, margin2, margin3, scale) in zip(margin1s, margin2s, margin3s, scales):\n                for _ in range(5):\n                    np_label = np.random.randint(0, num_class, (batch_size,))\n                    label = paddle.to_tensor(np_label, dtype='int64')\n                    input = paddle.randn(shape=[batch_size, feature_length], dtype=dtype)\n                    input.stop_gradient = False\n                    input_l2 = paddle.sqrt(paddle.sum(paddle.square(input), axis=1, keepdim=True))\n                    norm_input = paddle.divide(input, input_l2)\n                    weight = paddle.randn(shape=[feature_length, num_class_per_card[rank_id]], dtype=dtype)\n                    weight.stop_gradient = False\n                    weight_l2 = paddle.sqrt(paddle.sum(paddle.square(weight), axis=0, keepdim=True))\n                    norm_weight = paddle.divide(weight, weight_l2)\n                    data = paddle.matmul(norm_input, norm_weight)\n                    data.retain_grads()\n                    data.stop_gradient = False\n                    sta = np.sum(num_class_per_card[:rank_id]) if rank_id > 0 else 0\n                    end = np.sum(num_class_per_card[:rank_id + 1])\n                    integral_data = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_data[:, sta:end] = data.clone().detach().numpy()\n                    integral_data = paddle.to_tensor(integral_data, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_data, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_data = integral_data.detach().clone()\n                    integral_data.retain_grads()\n                    integral_data.stop_gradient = False\n                    theta = paddle.acos(integral_data)\n                    one_hot_label = paddle.nn.functional.one_hot(label, num_classes=num_class)\n                    one_hot_label.stop_gradient = False\n                    if margin1 != 1.0:\n                        theta = margin1 * theta\n                    if margin2 != 0.0:\n                        theta = theta + margin2\n                    margin_cos = paddle.cos(theta)\n                    if margin3 != 0.0:\n                        margin_cos = margin_cos - margin3\n                    diff = one_hot_label * (margin_cos - integral_data)\n                    arc_data = (integral_data + diff) * scale\n                    (loss_a, softmax_a) = paddle.nn.functional.margin_cross_entropy(data, label, margin1=margin1, margin2=margin2, margin3=margin3, scale=scale, group=check_group, return_softmax=True, reduction=None)\n                    (loss_b, softmax_b) = paddle.nn.functional.softmax_with_cross_entropy(logits=arc_data, label=paddle.reshape(label, (-1, 1)), return_softmax=True)\n                    np.testing.assert_allclose(loss_a.numpy(), loss_b.numpy(), rtol=1e-05, atol=1e-07)\n                    integral_prob = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_prob[:, sta:end] = softmax_a.clone().detach().numpy()\n                    integral_prob = paddle.to_tensor(integral_prob, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_prob, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_prob = integral_prob.detach().clone()\n                    integral_prob.stop_gradient = False\n                    np.testing.assert_allclose(integral_prob.numpy(), softmax_b.numpy(), rtol=1e-05, atol=1e-06)\n                    loss_a = loss_a.sum() / batch_size\n                    loss_b = loss_b.sum() / batch_size\n                    loss_a.backward()\n                    loss_b.backward()\n                    integral_grad = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_grad[:, sta:end] = data.grad.clone().detach()\n                    integral_grad = paddle.to_tensor(integral_grad, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_grad, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    np.testing.assert_allclose(integral_data.grad.numpy(False), integral_grad.numpy(False), rtol=1e-05, atol=1e-07)",
            "def test_parallel_margin_softmax_cross_entropy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    margin1s = [1.0, 1.0, 1.35]\n    margin2s = [0.5, 0.0, 0.0]\n    margin3s = [0.0, 0.35, 0.0]\n    scales = [64.0, 64.0, 64.0]\n    rank_id = dist.get_rank()\n    num_trainer = dist.get_world_size()\n    batch_size = 2\n    feature_length = 4\n    seed = 1025\n    set_random_seed(seed)\n    paddle.seed(rank_id * 10)\n    random.seed(seed)\n    np.random.seed(seed)\n    check_group = dist.new_group(list(range(num_trainer)))\n    for dtype in ('float32', 'float64'):\n        num_class_per_cards = [[4, 8], [2, 2], [4, 2], [3, 9]]\n        for num_class_per_card in num_class_per_cards:\n            num_class = np.sum(num_class_per_card)\n            for (margin1, margin2, margin3, scale) in zip(margin1s, margin2s, margin3s, scales):\n                for _ in range(5):\n                    np_label = np.random.randint(0, num_class, (batch_size,))\n                    label = paddle.to_tensor(np_label, dtype='int64')\n                    input = paddle.randn(shape=[batch_size, feature_length], dtype=dtype)\n                    input.stop_gradient = False\n                    input_l2 = paddle.sqrt(paddle.sum(paddle.square(input), axis=1, keepdim=True))\n                    norm_input = paddle.divide(input, input_l2)\n                    weight = paddle.randn(shape=[feature_length, num_class_per_card[rank_id]], dtype=dtype)\n                    weight.stop_gradient = False\n                    weight_l2 = paddle.sqrt(paddle.sum(paddle.square(weight), axis=0, keepdim=True))\n                    norm_weight = paddle.divide(weight, weight_l2)\n                    data = paddle.matmul(norm_input, norm_weight)\n                    data.retain_grads()\n                    data.stop_gradient = False\n                    sta = np.sum(num_class_per_card[:rank_id]) if rank_id > 0 else 0\n                    end = np.sum(num_class_per_card[:rank_id + 1])\n                    integral_data = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_data[:, sta:end] = data.clone().detach().numpy()\n                    integral_data = paddle.to_tensor(integral_data, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_data, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_data = integral_data.detach().clone()\n                    integral_data.retain_grads()\n                    integral_data.stop_gradient = False\n                    theta = paddle.acos(integral_data)\n                    one_hot_label = paddle.nn.functional.one_hot(label, num_classes=num_class)\n                    one_hot_label.stop_gradient = False\n                    if margin1 != 1.0:\n                        theta = margin1 * theta\n                    if margin2 != 0.0:\n                        theta = theta + margin2\n                    margin_cos = paddle.cos(theta)\n                    if margin3 != 0.0:\n                        margin_cos = margin_cos - margin3\n                    diff = one_hot_label * (margin_cos - integral_data)\n                    arc_data = (integral_data + diff) * scale\n                    (loss_a, softmax_a) = paddle.nn.functional.margin_cross_entropy(data, label, margin1=margin1, margin2=margin2, margin3=margin3, scale=scale, group=check_group, return_softmax=True, reduction=None)\n                    (loss_b, softmax_b) = paddle.nn.functional.softmax_with_cross_entropy(logits=arc_data, label=paddle.reshape(label, (-1, 1)), return_softmax=True)\n                    np.testing.assert_allclose(loss_a.numpy(), loss_b.numpy(), rtol=1e-05, atol=1e-07)\n                    integral_prob = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_prob[:, sta:end] = softmax_a.clone().detach().numpy()\n                    integral_prob = paddle.to_tensor(integral_prob, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_prob, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    integral_prob = integral_prob.detach().clone()\n                    integral_prob.stop_gradient = False\n                    np.testing.assert_allclose(integral_prob.numpy(), softmax_b.numpy(), rtol=1e-05, atol=1e-06)\n                    loss_a = loss_a.sum() / batch_size\n                    loss_b = loss_b.sum() / batch_size\n                    loss_a.backward()\n                    loss_b.backward()\n                    integral_grad = np.zeros((batch_size, num_class), dtype=dtype)\n                    integral_grad[:, sta:end] = data.grad.clone().detach()\n                    integral_grad = paddle.to_tensor(integral_grad, dtype=dtype)\n                    paddle.distributed.all_reduce(integral_grad, op=paddle.distributed.ReduceOp.SUM, group=check_group)\n                    np.testing.assert_allclose(integral_data.grad.numpy(False), integral_grad.numpy(False), rtol=1e-05, atol=1e-07)"
        ]
    }
]