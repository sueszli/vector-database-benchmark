[
    {
        "func_name": "_is_list_of_strings",
        "original": "def _is_list_of_strings(obj: object) -> TypeGuard[list[str]]:\n    return isinstance(obj, list) and all((isinstance(item, str) for item in obj))",
        "mutated": [
            "def _is_list_of_strings(obj: object) -> TypeGuard[list[str]]:\n    if False:\n        i = 10\n    return isinstance(obj, list) and all((isinstance(item, str) for item in obj))",
            "def _is_list_of_strings(obj: object) -> TypeGuard[list[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return isinstance(obj, list) and all((isinstance(item, str) for item in obj))",
            "def _is_list_of_strings(obj: object) -> TypeGuard[list[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return isinstance(obj, list) and all((isinstance(item, str) for item in obj))",
            "def _is_list_of_strings(obj: object) -> TypeGuard[list[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return isinstance(obj, list) and all((isinstance(item, str) for item in obj))",
            "def _is_list_of_strings(obj: object) -> TypeGuard[list[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return isinstance(obj, list) and all((isinstance(item, str) for item in obj))"
        ]
    },
    {
        "func_name": "_get_oldest_supported_python",
        "original": "@cache\ndef _get_oldest_supported_python() -> str:\n    with open('pyproject.toml', 'rb') as config:\n        val = tomli.load(config)['tool']['typeshed']['oldest_supported_python']\n    assert type(val) is str\n    return val",
        "mutated": [
            "@cache\ndef _get_oldest_supported_python() -> str:\n    if False:\n        i = 10\n    with open('pyproject.toml', 'rb') as config:\n        val = tomli.load(config)['tool']['typeshed']['oldest_supported_python']\n    assert type(val) is str\n    return val",
            "@cache\ndef _get_oldest_supported_python() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with open('pyproject.toml', 'rb') as config:\n        val = tomli.load(config)['tool']['typeshed']['oldest_supported_python']\n    assert type(val) is str\n    return val",
            "@cache\ndef _get_oldest_supported_python() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with open('pyproject.toml', 'rb') as config:\n        val = tomli.load(config)['tool']['typeshed']['oldest_supported_python']\n    assert type(val) is str\n    return val",
            "@cache\ndef _get_oldest_supported_python() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with open('pyproject.toml', 'rb') as config:\n        val = tomli.load(config)['tool']['typeshed']['oldest_supported_python']\n    assert type(val) is str\n    return val",
            "@cache\ndef _get_oldest_supported_python() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with open('pyproject.toml', 'rb') as config:\n        val = tomli.load(config)['tool']['typeshed']['oldest_supported_python']\n    assert type(val) is str\n    return val"
        ]
    },
    {
        "func_name": "system_requirements_for_platform",
        "original": "def system_requirements_for_platform(self, platform: str) -> list[str]:\n    assert platform in _STUBTEST_PLATFORM_MAPPING, f'Unrecognised platform {platform!r}'\n    ret = getattr(self, _STUBTEST_PLATFORM_MAPPING[platform])\n    assert _is_list_of_strings(ret)\n    return ret",
        "mutated": [
            "def system_requirements_for_platform(self, platform: str) -> list[str]:\n    if False:\n        i = 10\n    assert platform in _STUBTEST_PLATFORM_MAPPING, f'Unrecognised platform {platform!r}'\n    ret = getattr(self, _STUBTEST_PLATFORM_MAPPING[platform])\n    assert _is_list_of_strings(ret)\n    return ret",
            "def system_requirements_for_platform(self, platform: str) -> list[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert platform in _STUBTEST_PLATFORM_MAPPING, f'Unrecognised platform {platform!r}'\n    ret = getattr(self, _STUBTEST_PLATFORM_MAPPING[platform])\n    assert _is_list_of_strings(ret)\n    return ret",
            "def system_requirements_for_platform(self, platform: str) -> list[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert platform in _STUBTEST_PLATFORM_MAPPING, f'Unrecognised platform {platform!r}'\n    ret = getattr(self, _STUBTEST_PLATFORM_MAPPING[platform])\n    assert _is_list_of_strings(ret)\n    return ret",
            "def system_requirements_for_platform(self, platform: str) -> list[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert platform in _STUBTEST_PLATFORM_MAPPING, f'Unrecognised platform {platform!r}'\n    ret = getattr(self, _STUBTEST_PLATFORM_MAPPING[platform])\n    assert _is_list_of_strings(ret)\n    return ret",
            "def system_requirements_for_platform(self, platform: str) -> list[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert platform in _STUBTEST_PLATFORM_MAPPING, f'Unrecognised platform {platform!r}'\n    ret = getattr(self, _STUBTEST_PLATFORM_MAPPING[platform])\n    assert _is_list_of_strings(ret)\n    return ret"
        ]
    },
    {
        "func_name": "read_stubtest_settings",
        "original": "@cache\ndef read_stubtest_settings(distribution: str) -> StubtestSettings:\n    \"\"\"Return an object describing the stubtest settings for a single stubs distribution.\"\"\"\n    with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n        data: dict[str, object] = tomli.load(f).get('tool', {}).get('stubtest', {})\n    skipped: object = data.get('skip', False)\n    apt_dependencies: object = data.get('apt_dependencies', [])\n    brew_dependencies: object = data.get('brew_dependencies', [])\n    choco_dependencies: object = data.get('choco_dependencies', [])\n    extras: object = data.get('extras', [])\n    ignore_missing_stub: object = data.get('ignore_missing_stub', False)\n    specified_platforms: object = data.get('platforms', ['linux'])\n    stubtest_requirements: object = data.get('stubtest_requirements', [])\n    assert type(skipped) is bool\n    assert type(ignore_missing_stub) is bool\n    assert _is_list_of_strings(specified_platforms)\n    assert _is_list_of_strings(apt_dependencies)\n    assert _is_list_of_strings(brew_dependencies)\n    assert _is_list_of_strings(choco_dependencies)\n    assert _is_list_of_strings(extras)\n    assert _is_list_of_strings(stubtest_requirements)\n    unrecognised_platforms = set(specified_platforms) - _STUBTEST_PLATFORM_MAPPING.keys()\n    assert not unrecognised_platforms, f'Unrecognised platforms specified for {distribution!r}: {unrecognised_platforms}'\n    for (platform, dep_key) in _STUBTEST_PLATFORM_MAPPING.items():\n        if platform not in specified_platforms:\n            assert dep_key not in data, f'Stubtest is not run on {platform} in CI for {distribution!r}, but {dep_key!r} are specified in METADATA.toml'\n    return StubtestSettings(skipped=skipped, apt_dependencies=apt_dependencies, brew_dependencies=brew_dependencies, choco_dependencies=choco_dependencies, extras=extras, ignore_missing_stub=ignore_missing_stub, platforms=specified_platforms, stubtest_requirements=stubtest_requirements)",
        "mutated": [
            "@cache\ndef read_stubtest_settings(distribution: str) -> StubtestSettings:\n    if False:\n        i = 10\n    'Return an object describing the stubtest settings for a single stubs distribution.'\n    with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n        data: dict[str, object] = tomli.load(f).get('tool', {}).get('stubtest', {})\n    skipped: object = data.get('skip', False)\n    apt_dependencies: object = data.get('apt_dependencies', [])\n    brew_dependencies: object = data.get('brew_dependencies', [])\n    choco_dependencies: object = data.get('choco_dependencies', [])\n    extras: object = data.get('extras', [])\n    ignore_missing_stub: object = data.get('ignore_missing_stub', False)\n    specified_platforms: object = data.get('platforms', ['linux'])\n    stubtest_requirements: object = data.get('stubtest_requirements', [])\n    assert type(skipped) is bool\n    assert type(ignore_missing_stub) is bool\n    assert _is_list_of_strings(specified_platforms)\n    assert _is_list_of_strings(apt_dependencies)\n    assert _is_list_of_strings(brew_dependencies)\n    assert _is_list_of_strings(choco_dependencies)\n    assert _is_list_of_strings(extras)\n    assert _is_list_of_strings(stubtest_requirements)\n    unrecognised_platforms = set(specified_platforms) - _STUBTEST_PLATFORM_MAPPING.keys()\n    assert not unrecognised_platforms, f'Unrecognised platforms specified for {distribution!r}: {unrecognised_platforms}'\n    for (platform, dep_key) in _STUBTEST_PLATFORM_MAPPING.items():\n        if platform not in specified_platforms:\n            assert dep_key not in data, f'Stubtest is not run on {platform} in CI for {distribution!r}, but {dep_key!r} are specified in METADATA.toml'\n    return StubtestSettings(skipped=skipped, apt_dependencies=apt_dependencies, brew_dependencies=brew_dependencies, choco_dependencies=choco_dependencies, extras=extras, ignore_missing_stub=ignore_missing_stub, platforms=specified_platforms, stubtest_requirements=stubtest_requirements)",
            "@cache\ndef read_stubtest_settings(distribution: str) -> StubtestSettings:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return an object describing the stubtest settings for a single stubs distribution.'\n    with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n        data: dict[str, object] = tomli.load(f).get('tool', {}).get('stubtest', {})\n    skipped: object = data.get('skip', False)\n    apt_dependencies: object = data.get('apt_dependencies', [])\n    brew_dependencies: object = data.get('brew_dependencies', [])\n    choco_dependencies: object = data.get('choco_dependencies', [])\n    extras: object = data.get('extras', [])\n    ignore_missing_stub: object = data.get('ignore_missing_stub', False)\n    specified_platforms: object = data.get('platforms', ['linux'])\n    stubtest_requirements: object = data.get('stubtest_requirements', [])\n    assert type(skipped) is bool\n    assert type(ignore_missing_stub) is bool\n    assert _is_list_of_strings(specified_platforms)\n    assert _is_list_of_strings(apt_dependencies)\n    assert _is_list_of_strings(brew_dependencies)\n    assert _is_list_of_strings(choco_dependencies)\n    assert _is_list_of_strings(extras)\n    assert _is_list_of_strings(stubtest_requirements)\n    unrecognised_platforms = set(specified_platforms) - _STUBTEST_PLATFORM_MAPPING.keys()\n    assert not unrecognised_platforms, f'Unrecognised platforms specified for {distribution!r}: {unrecognised_platforms}'\n    for (platform, dep_key) in _STUBTEST_PLATFORM_MAPPING.items():\n        if platform not in specified_platforms:\n            assert dep_key not in data, f'Stubtest is not run on {platform} in CI for {distribution!r}, but {dep_key!r} are specified in METADATA.toml'\n    return StubtestSettings(skipped=skipped, apt_dependencies=apt_dependencies, brew_dependencies=brew_dependencies, choco_dependencies=choco_dependencies, extras=extras, ignore_missing_stub=ignore_missing_stub, platforms=specified_platforms, stubtest_requirements=stubtest_requirements)",
            "@cache\ndef read_stubtest_settings(distribution: str) -> StubtestSettings:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return an object describing the stubtest settings for a single stubs distribution.'\n    with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n        data: dict[str, object] = tomli.load(f).get('tool', {}).get('stubtest', {})\n    skipped: object = data.get('skip', False)\n    apt_dependencies: object = data.get('apt_dependencies', [])\n    brew_dependencies: object = data.get('brew_dependencies', [])\n    choco_dependencies: object = data.get('choco_dependencies', [])\n    extras: object = data.get('extras', [])\n    ignore_missing_stub: object = data.get('ignore_missing_stub', False)\n    specified_platforms: object = data.get('platforms', ['linux'])\n    stubtest_requirements: object = data.get('stubtest_requirements', [])\n    assert type(skipped) is bool\n    assert type(ignore_missing_stub) is bool\n    assert _is_list_of_strings(specified_platforms)\n    assert _is_list_of_strings(apt_dependencies)\n    assert _is_list_of_strings(brew_dependencies)\n    assert _is_list_of_strings(choco_dependencies)\n    assert _is_list_of_strings(extras)\n    assert _is_list_of_strings(stubtest_requirements)\n    unrecognised_platforms = set(specified_platforms) - _STUBTEST_PLATFORM_MAPPING.keys()\n    assert not unrecognised_platforms, f'Unrecognised platforms specified for {distribution!r}: {unrecognised_platforms}'\n    for (platform, dep_key) in _STUBTEST_PLATFORM_MAPPING.items():\n        if platform not in specified_platforms:\n            assert dep_key not in data, f'Stubtest is not run on {platform} in CI for {distribution!r}, but {dep_key!r} are specified in METADATA.toml'\n    return StubtestSettings(skipped=skipped, apt_dependencies=apt_dependencies, brew_dependencies=brew_dependencies, choco_dependencies=choco_dependencies, extras=extras, ignore_missing_stub=ignore_missing_stub, platforms=specified_platforms, stubtest_requirements=stubtest_requirements)",
            "@cache\ndef read_stubtest_settings(distribution: str) -> StubtestSettings:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return an object describing the stubtest settings for a single stubs distribution.'\n    with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n        data: dict[str, object] = tomli.load(f).get('tool', {}).get('stubtest', {})\n    skipped: object = data.get('skip', False)\n    apt_dependencies: object = data.get('apt_dependencies', [])\n    brew_dependencies: object = data.get('brew_dependencies', [])\n    choco_dependencies: object = data.get('choco_dependencies', [])\n    extras: object = data.get('extras', [])\n    ignore_missing_stub: object = data.get('ignore_missing_stub', False)\n    specified_platforms: object = data.get('platforms', ['linux'])\n    stubtest_requirements: object = data.get('stubtest_requirements', [])\n    assert type(skipped) is bool\n    assert type(ignore_missing_stub) is bool\n    assert _is_list_of_strings(specified_platforms)\n    assert _is_list_of_strings(apt_dependencies)\n    assert _is_list_of_strings(brew_dependencies)\n    assert _is_list_of_strings(choco_dependencies)\n    assert _is_list_of_strings(extras)\n    assert _is_list_of_strings(stubtest_requirements)\n    unrecognised_platforms = set(specified_platforms) - _STUBTEST_PLATFORM_MAPPING.keys()\n    assert not unrecognised_platforms, f'Unrecognised platforms specified for {distribution!r}: {unrecognised_platforms}'\n    for (platform, dep_key) in _STUBTEST_PLATFORM_MAPPING.items():\n        if platform not in specified_platforms:\n            assert dep_key not in data, f'Stubtest is not run on {platform} in CI for {distribution!r}, but {dep_key!r} are specified in METADATA.toml'\n    return StubtestSettings(skipped=skipped, apt_dependencies=apt_dependencies, brew_dependencies=brew_dependencies, choco_dependencies=choco_dependencies, extras=extras, ignore_missing_stub=ignore_missing_stub, platforms=specified_platforms, stubtest_requirements=stubtest_requirements)",
            "@cache\ndef read_stubtest_settings(distribution: str) -> StubtestSettings:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return an object describing the stubtest settings for a single stubs distribution.'\n    with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n        data: dict[str, object] = tomli.load(f).get('tool', {}).get('stubtest', {})\n    skipped: object = data.get('skip', False)\n    apt_dependencies: object = data.get('apt_dependencies', [])\n    brew_dependencies: object = data.get('brew_dependencies', [])\n    choco_dependencies: object = data.get('choco_dependencies', [])\n    extras: object = data.get('extras', [])\n    ignore_missing_stub: object = data.get('ignore_missing_stub', False)\n    specified_platforms: object = data.get('platforms', ['linux'])\n    stubtest_requirements: object = data.get('stubtest_requirements', [])\n    assert type(skipped) is bool\n    assert type(ignore_missing_stub) is bool\n    assert _is_list_of_strings(specified_platforms)\n    assert _is_list_of_strings(apt_dependencies)\n    assert _is_list_of_strings(brew_dependencies)\n    assert _is_list_of_strings(choco_dependencies)\n    assert _is_list_of_strings(extras)\n    assert _is_list_of_strings(stubtest_requirements)\n    unrecognised_platforms = set(specified_platforms) - _STUBTEST_PLATFORM_MAPPING.keys()\n    assert not unrecognised_platforms, f'Unrecognised platforms specified for {distribution!r}: {unrecognised_platforms}'\n    for (platform, dep_key) in _STUBTEST_PLATFORM_MAPPING.items():\n        if platform not in specified_platforms:\n            assert dep_key not in data, f'Stubtest is not run on {platform} in CI for {distribution!r}, but {dep_key!r} are specified in METADATA.toml'\n    return StubtestSettings(skipped=skipped, apt_dependencies=apt_dependencies, brew_dependencies=brew_dependencies, choco_dependencies=choco_dependencies, extras=extras, ignore_missing_stub=ignore_missing_stub, platforms=specified_platforms, stubtest_requirements=stubtest_requirements)"
        ]
    },
    {
        "func_name": "read_metadata",
        "original": "@cache\ndef read_metadata(distribution: str) -> StubMetadata:\n    \"\"\"Return an object describing the metadata of a stub as given in the METADATA.toml file.\n\n    This function does some basic validation,\n    but does no parsing, transforming or normalization of the metadata.\n    Use `read_dependencies` if you need to parse the dependencies\n    given in the `requires` field, for example.\n    \"\"\"\n    try:\n        with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n            data: dict[str, object] = tomli.load(f)\n    except FileNotFoundError:\n        raise NoSuchStubError(f'Typeshed has no stubs for {distribution!r}!') from None\n    unknown_metadata_fields = data.keys() - _KNOWN_METADATA_FIELDS\n    assert not unknown_metadata_fields, f'Unexpected keys in METADATA.toml for {distribution!r}: {unknown_metadata_fields}'\n    assert 'version' in data, f\"Missing 'version' field in METADATA.toml for {distribution!r}\"\n    version = data['version']\n    assert isinstance(version, str)\n    Version(version[:-2] if version.endswith('.*') else version)\n    requires: object = data.get('requires', [])\n    assert isinstance(requires, list)\n    for req in requires:\n        assert isinstance(req, str), f'Invalid requirement {req!r} for {distribution!r}'\n        for space in ' \\t\\n':\n            assert space not in req, f'For consistency, requirement should not have whitespace: {req!r}'\n        Requirement(req)\n    extra_description: object = data.get('extra_description')\n    assert isinstance(extra_description, (str, type(None)))\n    if 'stub_distribution' in data:\n        stub_distribution = data['stub_distribution']\n        assert isinstance(stub_distribution, str)\n        assert _DIST_NAME_RE.fullmatch(stub_distribution), f\"Invalid 'stub_distribution' value for {distribution!r}\"\n    else:\n        stub_distribution = f'types-{distribution}'\n    upstream_repository: object = data.get('upstream_repository')\n    assert isinstance(upstream_repository, (str, type(None)))\n    if isinstance(upstream_repository, str):\n        parsed_url = urllib.parse.urlsplit(upstream_repository)\n        assert parsed_url.scheme == 'https', f'{distribution}: URLs in the upstream_repository field should use https'\n        no_www_please = f'{distribution}: `World Wide Web` subdomain (`www.`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.netloc.startswith('www.'), no_www_please\n        no_query_params_please = f'{distribution}: Query params (`?`) should be removed from URLs in the upstream_repository field'\n        assert parsed_url.hostname in _QUERY_URL_ALLOWLIST or not parsed_url.query, no_query_params_please\n        no_fragments_please = f'{distribution}: Fragments (`#`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.fragment, no_fragments_please\n        if parsed_url.netloc == 'github.com':\n            cleaned_url_path = parsed_url.path.strip('/')\n            num_url_path_parts = len(Path(cleaned_url_path).parts)\n            bad_github_url_msg = f'Invalid upstream_repository for {distribution!r}: URLs for GitHub repositories always have two parts in their paths'\n            assert num_url_path_parts == 2, bad_github_url_msg\n    obsolete_since: object = data.get('obsolete_since')\n    assert isinstance(obsolete_since, (str, type(None)))\n    no_longer_updated: object = data.get('no_longer_updated', False)\n    assert type(no_longer_updated) is bool\n    uploaded_to_pypi: object = data.get('upload', True)\n    assert type(uploaded_to_pypi) is bool\n    partial_stub: object = data.get('partial_stub', True)\n    assert type(partial_stub) is bool\n    requires_python_str: object = data.get('requires_python')\n    oldest_supported_python = _get_oldest_supported_python()\n    oldest_supported_python_specifier = Specifier(f'>={oldest_supported_python}')\n    if requires_python_str is None:\n        requires_python = oldest_supported_python_specifier\n    else:\n        assert type(requires_python_str) is str\n        requires_python = Specifier(requires_python_str)\n        assert requires_python != oldest_supported_python_specifier, f'requires_python=\"{requires_python}\" is redundant'\n        assert oldest_supported_python_specifier.contains(requires_python.version), f\"'requires_python' contains versions lower than typeshed's oldest supported Python ({oldest_supported_python})\"\n        assert requires_python.operator == '>=', \"'requires_python' should be a minimum version specifier, use '>=3.x'\"\n    empty_tools: dict[object, object] = {}\n    tools_settings: object = data.get('tool', empty_tools)\n    assert isinstance(tools_settings, dict)\n    assert tools_settings.keys() <= _KNOWN_METADATA_TOOL_FIELDS.keys(), f'Unrecognised tool for {distribution!r}'\n    for (tool, tk) in _KNOWN_METADATA_TOOL_FIELDS.items():\n        settings_for_tool: object = tools_settings.get(tool, {})\n        assert isinstance(settings_for_tool, dict)\n        for key in settings_for_tool:\n            assert key in tk, f'Unrecognised {tool} key {key!r} for {distribution!r}'\n    return StubMetadata(version=version, requires=requires, extra_description=extra_description, stub_distribution=stub_distribution, upstream_repository=upstream_repository, obsolete_since=obsolete_since, no_longer_updated=no_longer_updated, uploaded_to_pypi=uploaded_to_pypi, partial_stub=partial_stub, stubtest_settings=read_stubtest_settings(distribution), requires_python=requires_python)",
        "mutated": [
            "@cache\ndef read_metadata(distribution: str) -> StubMetadata:\n    if False:\n        i = 10\n    'Return an object describing the metadata of a stub as given in the METADATA.toml file.\\n\\n    This function does some basic validation,\\n    but does no parsing, transforming or normalization of the metadata.\\n    Use `read_dependencies` if you need to parse the dependencies\\n    given in the `requires` field, for example.\\n    '\n    try:\n        with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n            data: dict[str, object] = tomli.load(f)\n    except FileNotFoundError:\n        raise NoSuchStubError(f'Typeshed has no stubs for {distribution!r}!') from None\n    unknown_metadata_fields = data.keys() - _KNOWN_METADATA_FIELDS\n    assert not unknown_metadata_fields, f'Unexpected keys in METADATA.toml for {distribution!r}: {unknown_metadata_fields}'\n    assert 'version' in data, f\"Missing 'version' field in METADATA.toml for {distribution!r}\"\n    version = data['version']\n    assert isinstance(version, str)\n    Version(version[:-2] if version.endswith('.*') else version)\n    requires: object = data.get('requires', [])\n    assert isinstance(requires, list)\n    for req in requires:\n        assert isinstance(req, str), f'Invalid requirement {req!r} for {distribution!r}'\n        for space in ' \\t\\n':\n            assert space not in req, f'For consistency, requirement should not have whitespace: {req!r}'\n        Requirement(req)\n    extra_description: object = data.get('extra_description')\n    assert isinstance(extra_description, (str, type(None)))\n    if 'stub_distribution' in data:\n        stub_distribution = data['stub_distribution']\n        assert isinstance(stub_distribution, str)\n        assert _DIST_NAME_RE.fullmatch(stub_distribution), f\"Invalid 'stub_distribution' value for {distribution!r}\"\n    else:\n        stub_distribution = f'types-{distribution}'\n    upstream_repository: object = data.get('upstream_repository')\n    assert isinstance(upstream_repository, (str, type(None)))\n    if isinstance(upstream_repository, str):\n        parsed_url = urllib.parse.urlsplit(upstream_repository)\n        assert parsed_url.scheme == 'https', f'{distribution}: URLs in the upstream_repository field should use https'\n        no_www_please = f'{distribution}: `World Wide Web` subdomain (`www.`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.netloc.startswith('www.'), no_www_please\n        no_query_params_please = f'{distribution}: Query params (`?`) should be removed from URLs in the upstream_repository field'\n        assert parsed_url.hostname in _QUERY_URL_ALLOWLIST or not parsed_url.query, no_query_params_please\n        no_fragments_please = f'{distribution}: Fragments (`#`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.fragment, no_fragments_please\n        if parsed_url.netloc == 'github.com':\n            cleaned_url_path = parsed_url.path.strip('/')\n            num_url_path_parts = len(Path(cleaned_url_path).parts)\n            bad_github_url_msg = f'Invalid upstream_repository for {distribution!r}: URLs for GitHub repositories always have two parts in their paths'\n            assert num_url_path_parts == 2, bad_github_url_msg\n    obsolete_since: object = data.get('obsolete_since')\n    assert isinstance(obsolete_since, (str, type(None)))\n    no_longer_updated: object = data.get('no_longer_updated', False)\n    assert type(no_longer_updated) is bool\n    uploaded_to_pypi: object = data.get('upload', True)\n    assert type(uploaded_to_pypi) is bool\n    partial_stub: object = data.get('partial_stub', True)\n    assert type(partial_stub) is bool\n    requires_python_str: object = data.get('requires_python')\n    oldest_supported_python = _get_oldest_supported_python()\n    oldest_supported_python_specifier = Specifier(f'>={oldest_supported_python}')\n    if requires_python_str is None:\n        requires_python = oldest_supported_python_specifier\n    else:\n        assert type(requires_python_str) is str\n        requires_python = Specifier(requires_python_str)\n        assert requires_python != oldest_supported_python_specifier, f'requires_python=\"{requires_python}\" is redundant'\n        assert oldest_supported_python_specifier.contains(requires_python.version), f\"'requires_python' contains versions lower than typeshed's oldest supported Python ({oldest_supported_python})\"\n        assert requires_python.operator == '>=', \"'requires_python' should be a minimum version specifier, use '>=3.x'\"\n    empty_tools: dict[object, object] = {}\n    tools_settings: object = data.get('tool', empty_tools)\n    assert isinstance(tools_settings, dict)\n    assert tools_settings.keys() <= _KNOWN_METADATA_TOOL_FIELDS.keys(), f'Unrecognised tool for {distribution!r}'\n    for (tool, tk) in _KNOWN_METADATA_TOOL_FIELDS.items():\n        settings_for_tool: object = tools_settings.get(tool, {})\n        assert isinstance(settings_for_tool, dict)\n        for key in settings_for_tool:\n            assert key in tk, f'Unrecognised {tool} key {key!r} for {distribution!r}'\n    return StubMetadata(version=version, requires=requires, extra_description=extra_description, stub_distribution=stub_distribution, upstream_repository=upstream_repository, obsolete_since=obsolete_since, no_longer_updated=no_longer_updated, uploaded_to_pypi=uploaded_to_pypi, partial_stub=partial_stub, stubtest_settings=read_stubtest_settings(distribution), requires_python=requires_python)",
            "@cache\ndef read_metadata(distribution: str) -> StubMetadata:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return an object describing the metadata of a stub as given in the METADATA.toml file.\\n\\n    This function does some basic validation,\\n    but does no parsing, transforming or normalization of the metadata.\\n    Use `read_dependencies` if you need to parse the dependencies\\n    given in the `requires` field, for example.\\n    '\n    try:\n        with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n            data: dict[str, object] = tomli.load(f)\n    except FileNotFoundError:\n        raise NoSuchStubError(f'Typeshed has no stubs for {distribution!r}!') from None\n    unknown_metadata_fields = data.keys() - _KNOWN_METADATA_FIELDS\n    assert not unknown_metadata_fields, f'Unexpected keys in METADATA.toml for {distribution!r}: {unknown_metadata_fields}'\n    assert 'version' in data, f\"Missing 'version' field in METADATA.toml for {distribution!r}\"\n    version = data['version']\n    assert isinstance(version, str)\n    Version(version[:-2] if version.endswith('.*') else version)\n    requires: object = data.get('requires', [])\n    assert isinstance(requires, list)\n    for req in requires:\n        assert isinstance(req, str), f'Invalid requirement {req!r} for {distribution!r}'\n        for space in ' \\t\\n':\n            assert space not in req, f'For consistency, requirement should not have whitespace: {req!r}'\n        Requirement(req)\n    extra_description: object = data.get('extra_description')\n    assert isinstance(extra_description, (str, type(None)))\n    if 'stub_distribution' in data:\n        stub_distribution = data['stub_distribution']\n        assert isinstance(stub_distribution, str)\n        assert _DIST_NAME_RE.fullmatch(stub_distribution), f\"Invalid 'stub_distribution' value for {distribution!r}\"\n    else:\n        stub_distribution = f'types-{distribution}'\n    upstream_repository: object = data.get('upstream_repository')\n    assert isinstance(upstream_repository, (str, type(None)))\n    if isinstance(upstream_repository, str):\n        parsed_url = urllib.parse.urlsplit(upstream_repository)\n        assert parsed_url.scheme == 'https', f'{distribution}: URLs in the upstream_repository field should use https'\n        no_www_please = f'{distribution}: `World Wide Web` subdomain (`www.`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.netloc.startswith('www.'), no_www_please\n        no_query_params_please = f'{distribution}: Query params (`?`) should be removed from URLs in the upstream_repository field'\n        assert parsed_url.hostname in _QUERY_URL_ALLOWLIST or not parsed_url.query, no_query_params_please\n        no_fragments_please = f'{distribution}: Fragments (`#`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.fragment, no_fragments_please\n        if parsed_url.netloc == 'github.com':\n            cleaned_url_path = parsed_url.path.strip('/')\n            num_url_path_parts = len(Path(cleaned_url_path).parts)\n            bad_github_url_msg = f'Invalid upstream_repository for {distribution!r}: URLs for GitHub repositories always have two parts in their paths'\n            assert num_url_path_parts == 2, bad_github_url_msg\n    obsolete_since: object = data.get('obsolete_since')\n    assert isinstance(obsolete_since, (str, type(None)))\n    no_longer_updated: object = data.get('no_longer_updated', False)\n    assert type(no_longer_updated) is bool\n    uploaded_to_pypi: object = data.get('upload', True)\n    assert type(uploaded_to_pypi) is bool\n    partial_stub: object = data.get('partial_stub', True)\n    assert type(partial_stub) is bool\n    requires_python_str: object = data.get('requires_python')\n    oldest_supported_python = _get_oldest_supported_python()\n    oldest_supported_python_specifier = Specifier(f'>={oldest_supported_python}')\n    if requires_python_str is None:\n        requires_python = oldest_supported_python_specifier\n    else:\n        assert type(requires_python_str) is str\n        requires_python = Specifier(requires_python_str)\n        assert requires_python != oldest_supported_python_specifier, f'requires_python=\"{requires_python}\" is redundant'\n        assert oldest_supported_python_specifier.contains(requires_python.version), f\"'requires_python' contains versions lower than typeshed's oldest supported Python ({oldest_supported_python})\"\n        assert requires_python.operator == '>=', \"'requires_python' should be a minimum version specifier, use '>=3.x'\"\n    empty_tools: dict[object, object] = {}\n    tools_settings: object = data.get('tool', empty_tools)\n    assert isinstance(tools_settings, dict)\n    assert tools_settings.keys() <= _KNOWN_METADATA_TOOL_FIELDS.keys(), f'Unrecognised tool for {distribution!r}'\n    for (tool, tk) in _KNOWN_METADATA_TOOL_FIELDS.items():\n        settings_for_tool: object = tools_settings.get(tool, {})\n        assert isinstance(settings_for_tool, dict)\n        for key in settings_for_tool:\n            assert key in tk, f'Unrecognised {tool} key {key!r} for {distribution!r}'\n    return StubMetadata(version=version, requires=requires, extra_description=extra_description, stub_distribution=stub_distribution, upstream_repository=upstream_repository, obsolete_since=obsolete_since, no_longer_updated=no_longer_updated, uploaded_to_pypi=uploaded_to_pypi, partial_stub=partial_stub, stubtest_settings=read_stubtest_settings(distribution), requires_python=requires_python)",
            "@cache\ndef read_metadata(distribution: str) -> StubMetadata:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return an object describing the metadata of a stub as given in the METADATA.toml file.\\n\\n    This function does some basic validation,\\n    but does no parsing, transforming or normalization of the metadata.\\n    Use `read_dependencies` if you need to parse the dependencies\\n    given in the `requires` field, for example.\\n    '\n    try:\n        with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n            data: dict[str, object] = tomli.load(f)\n    except FileNotFoundError:\n        raise NoSuchStubError(f'Typeshed has no stubs for {distribution!r}!') from None\n    unknown_metadata_fields = data.keys() - _KNOWN_METADATA_FIELDS\n    assert not unknown_metadata_fields, f'Unexpected keys in METADATA.toml for {distribution!r}: {unknown_metadata_fields}'\n    assert 'version' in data, f\"Missing 'version' field in METADATA.toml for {distribution!r}\"\n    version = data['version']\n    assert isinstance(version, str)\n    Version(version[:-2] if version.endswith('.*') else version)\n    requires: object = data.get('requires', [])\n    assert isinstance(requires, list)\n    for req in requires:\n        assert isinstance(req, str), f'Invalid requirement {req!r} for {distribution!r}'\n        for space in ' \\t\\n':\n            assert space not in req, f'For consistency, requirement should not have whitespace: {req!r}'\n        Requirement(req)\n    extra_description: object = data.get('extra_description')\n    assert isinstance(extra_description, (str, type(None)))\n    if 'stub_distribution' in data:\n        stub_distribution = data['stub_distribution']\n        assert isinstance(stub_distribution, str)\n        assert _DIST_NAME_RE.fullmatch(stub_distribution), f\"Invalid 'stub_distribution' value for {distribution!r}\"\n    else:\n        stub_distribution = f'types-{distribution}'\n    upstream_repository: object = data.get('upstream_repository')\n    assert isinstance(upstream_repository, (str, type(None)))\n    if isinstance(upstream_repository, str):\n        parsed_url = urllib.parse.urlsplit(upstream_repository)\n        assert parsed_url.scheme == 'https', f'{distribution}: URLs in the upstream_repository field should use https'\n        no_www_please = f'{distribution}: `World Wide Web` subdomain (`www.`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.netloc.startswith('www.'), no_www_please\n        no_query_params_please = f'{distribution}: Query params (`?`) should be removed from URLs in the upstream_repository field'\n        assert parsed_url.hostname in _QUERY_URL_ALLOWLIST or not parsed_url.query, no_query_params_please\n        no_fragments_please = f'{distribution}: Fragments (`#`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.fragment, no_fragments_please\n        if parsed_url.netloc == 'github.com':\n            cleaned_url_path = parsed_url.path.strip('/')\n            num_url_path_parts = len(Path(cleaned_url_path).parts)\n            bad_github_url_msg = f'Invalid upstream_repository for {distribution!r}: URLs for GitHub repositories always have two parts in their paths'\n            assert num_url_path_parts == 2, bad_github_url_msg\n    obsolete_since: object = data.get('obsolete_since')\n    assert isinstance(obsolete_since, (str, type(None)))\n    no_longer_updated: object = data.get('no_longer_updated', False)\n    assert type(no_longer_updated) is bool\n    uploaded_to_pypi: object = data.get('upload', True)\n    assert type(uploaded_to_pypi) is bool\n    partial_stub: object = data.get('partial_stub', True)\n    assert type(partial_stub) is bool\n    requires_python_str: object = data.get('requires_python')\n    oldest_supported_python = _get_oldest_supported_python()\n    oldest_supported_python_specifier = Specifier(f'>={oldest_supported_python}')\n    if requires_python_str is None:\n        requires_python = oldest_supported_python_specifier\n    else:\n        assert type(requires_python_str) is str\n        requires_python = Specifier(requires_python_str)\n        assert requires_python != oldest_supported_python_specifier, f'requires_python=\"{requires_python}\" is redundant'\n        assert oldest_supported_python_specifier.contains(requires_python.version), f\"'requires_python' contains versions lower than typeshed's oldest supported Python ({oldest_supported_python})\"\n        assert requires_python.operator == '>=', \"'requires_python' should be a minimum version specifier, use '>=3.x'\"\n    empty_tools: dict[object, object] = {}\n    tools_settings: object = data.get('tool', empty_tools)\n    assert isinstance(tools_settings, dict)\n    assert tools_settings.keys() <= _KNOWN_METADATA_TOOL_FIELDS.keys(), f'Unrecognised tool for {distribution!r}'\n    for (tool, tk) in _KNOWN_METADATA_TOOL_FIELDS.items():\n        settings_for_tool: object = tools_settings.get(tool, {})\n        assert isinstance(settings_for_tool, dict)\n        for key in settings_for_tool:\n            assert key in tk, f'Unrecognised {tool} key {key!r} for {distribution!r}'\n    return StubMetadata(version=version, requires=requires, extra_description=extra_description, stub_distribution=stub_distribution, upstream_repository=upstream_repository, obsolete_since=obsolete_since, no_longer_updated=no_longer_updated, uploaded_to_pypi=uploaded_to_pypi, partial_stub=partial_stub, stubtest_settings=read_stubtest_settings(distribution), requires_python=requires_python)",
            "@cache\ndef read_metadata(distribution: str) -> StubMetadata:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return an object describing the metadata of a stub as given in the METADATA.toml file.\\n\\n    This function does some basic validation,\\n    but does no parsing, transforming or normalization of the metadata.\\n    Use `read_dependencies` if you need to parse the dependencies\\n    given in the `requires` field, for example.\\n    '\n    try:\n        with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n            data: dict[str, object] = tomli.load(f)\n    except FileNotFoundError:\n        raise NoSuchStubError(f'Typeshed has no stubs for {distribution!r}!') from None\n    unknown_metadata_fields = data.keys() - _KNOWN_METADATA_FIELDS\n    assert not unknown_metadata_fields, f'Unexpected keys in METADATA.toml for {distribution!r}: {unknown_metadata_fields}'\n    assert 'version' in data, f\"Missing 'version' field in METADATA.toml for {distribution!r}\"\n    version = data['version']\n    assert isinstance(version, str)\n    Version(version[:-2] if version.endswith('.*') else version)\n    requires: object = data.get('requires', [])\n    assert isinstance(requires, list)\n    for req in requires:\n        assert isinstance(req, str), f'Invalid requirement {req!r} for {distribution!r}'\n        for space in ' \\t\\n':\n            assert space not in req, f'For consistency, requirement should not have whitespace: {req!r}'\n        Requirement(req)\n    extra_description: object = data.get('extra_description')\n    assert isinstance(extra_description, (str, type(None)))\n    if 'stub_distribution' in data:\n        stub_distribution = data['stub_distribution']\n        assert isinstance(stub_distribution, str)\n        assert _DIST_NAME_RE.fullmatch(stub_distribution), f\"Invalid 'stub_distribution' value for {distribution!r}\"\n    else:\n        stub_distribution = f'types-{distribution}'\n    upstream_repository: object = data.get('upstream_repository')\n    assert isinstance(upstream_repository, (str, type(None)))\n    if isinstance(upstream_repository, str):\n        parsed_url = urllib.parse.urlsplit(upstream_repository)\n        assert parsed_url.scheme == 'https', f'{distribution}: URLs in the upstream_repository field should use https'\n        no_www_please = f'{distribution}: `World Wide Web` subdomain (`www.`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.netloc.startswith('www.'), no_www_please\n        no_query_params_please = f'{distribution}: Query params (`?`) should be removed from URLs in the upstream_repository field'\n        assert parsed_url.hostname in _QUERY_URL_ALLOWLIST or not parsed_url.query, no_query_params_please\n        no_fragments_please = f'{distribution}: Fragments (`#`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.fragment, no_fragments_please\n        if parsed_url.netloc == 'github.com':\n            cleaned_url_path = parsed_url.path.strip('/')\n            num_url_path_parts = len(Path(cleaned_url_path).parts)\n            bad_github_url_msg = f'Invalid upstream_repository for {distribution!r}: URLs for GitHub repositories always have two parts in their paths'\n            assert num_url_path_parts == 2, bad_github_url_msg\n    obsolete_since: object = data.get('obsolete_since')\n    assert isinstance(obsolete_since, (str, type(None)))\n    no_longer_updated: object = data.get('no_longer_updated', False)\n    assert type(no_longer_updated) is bool\n    uploaded_to_pypi: object = data.get('upload', True)\n    assert type(uploaded_to_pypi) is bool\n    partial_stub: object = data.get('partial_stub', True)\n    assert type(partial_stub) is bool\n    requires_python_str: object = data.get('requires_python')\n    oldest_supported_python = _get_oldest_supported_python()\n    oldest_supported_python_specifier = Specifier(f'>={oldest_supported_python}')\n    if requires_python_str is None:\n        requires_python = oldest_supported_python_specifier\n    else:\n        assert type(requires_python_str) is str\n        requires_python = Specifier(requires_python_str)\n        assert requires_python != oldest_supported_python_specifier, f'requires_python=\"{requires_python}\" is redundant'\n        assert oldest_supported_python_specifier.contains(requires_python.version), f\"'requires_python' contains versions lower than typeshed's oldest supported Python ({oldest_supported_python})\"\n        assert requires_python.operator == '>=', \"'requires_python' should be a minimum version specifier, use '>=3.x'\"\n    empty_tools: dict[object, object] = {}\n    tools_settings: object = data.get('tool', empty_tools)\n    assert isinstance(tools_settings, dict)\n    assert tools_settings.keys() <= _KNOWN_METADATA_TOOL_FIELDS.keys(), f'Unrecognised tool for {distribution!r}'\n    for (tool, tk) in _KNOWN_METADATA_TOOL_FIELDS.items():\n        settings_for_tool: object = tools_settings.get(tool, {})\n        assert isinstance(settings_for_tool, dict)\n        for key in settings_for_tool:\n            assert key in tk, f'Unrecognised {tool} key {key!r} for {distribution!r}'\n    return StubMetadata(version=version, requires=requires, extra_description=extra_description, stub_distribution=stub_distribution, upstream_repository=upstream_repository, obsolete_since=obsolete_since, no_longer_updated=no_longer_updated, uploaded_to_pypi=uploaded_to_pypi, partial_stub=partial_stub, stubtest_settings=read_stubtest_settings(distribution), requires_python=requires_python)",
            "@cache\ndef read_metadata(distribution: str) -> StubMetadata:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return an object describing the metadata of a stub as given in the METADATA.toml file.\\n\\n    This function does some basic validation,\\n    but does no parsing, transforming or normalization of the metadata.\\n    Use `read_dependencies` if you need to parse the dependencies\\n    given in the `requires` field, for example.\\n    '\n    try:\n        with Path('stubs', distribution, 'METADATA.toml').open('rb') as f:\n            data: dict[str, object] = tomli.load(f)\n    except FileNotFoundError:\n        raise NoSuchStubError(f'Typeshed has no stubs for {distribution!r}!') from None\n    unknown_metadata_fields = data.keys() - _KNOWN_METADATA_FIELDS\n    assert not unknown_metadata_fields, f'Unexpected keys in METADATA.toml for {distribution!r}: {unknown_metadata_fields}'\n    assert 'version' in data, f\"Missing 'version' field in METADATA.toml for {distribution!r}\"\n    version = data['version']\n    assert isinstance(version, str)\n    Version(version[:-2] if version.endswith('.*') else version)\n    requires: object = data.get('requires', [])\n    assert isinstance(requires, list)\n    for req in requires:\n        assert isinstance(req, str), f'Invalid requirement {req!r} for {distribution!r}'\n        for space in ' \\t\\n':\n            assert space not in req, f'For consistency, requirement should not have whitespace: {req!r}'\n        Requirement(req)\n    extra_description: object = data.get('extra_description')\n    assert isinstance(extra_description, (str, type(None)))\n    if 'stub_distribution' in data:\n        stub_distribution = data['stub_distribution']\n        assert isinstance(stub_distribution, str)\n        assert _DIST_NAME_RE.fullmatch(stub_distribution), f\"Invalid 'stub_distribution' value for {distribution!r}\"\n    else:\n        stub_distribution = f'types-{distribution}'\n    upstream_repository: object = data.get('upstream_repository')\n    assert isinstance(upstream_repository, (str, type(None)))\n    if isinstance(upstream_repository, str):\n        parsed_url = urllib.parse.urlsplit(upstream_repository)\n        assert parsed_url.scheme == 'https', f'{distribution}: URLs in the upstream_repository field should use https'\n        no_www_please = f'{distribution}: `World Wide Web` subdomain (`www.`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.netloc.startswith('www.'), no_www_please\n        no_query_params_please = f'{distribution}: Query params (`?`) should be removed from URLs in the upstream_repository field'\n        assert parsed_url.hostname in _QUERY_URL_ALLOWLIST or not parsed_url.query, no_query_params_please\n        no_fragments_please = f'{distribution}: Fragments (`#`) should be removed from URLs in the upstream_repository field'\n        assert not parsed_url.fragment, no_fragments_please\n        if parsed_url.netloc == 'github.com':\n            cleaned_url_path = parsed_url.path.strip('/')\n            num_url_path_parts = len(Path(cleaned_url_path).parts)\n            bad_github_url_msg = f'Invalid upstream_repository for {distribution!r}: URLs for GitHub repositories always have two parts in their paths'\n            assert num_url_path_parts == 2, bad_github_url_msg\n    obsolete_since: object = data.get('obsolete_since')\n    assert isinstance(obsolete_since, (str, type(None)))\n    no_longer_updated: object = data.get('no_longer_updated', False)\n    assert type(no_longer_updated) is bool\n    uploaded_to_pypi: object = data.get('upload', True)\n    assert type(uploaded_to_pypi) is bool\n    partial_stub: object = data.get('partial_stub', True)\n    assert type(partial_stub) is bool\n    requires_python_str: object = data.get('requires_python')\n    oldest_supported_python = _get_oldest_supported_python()\n    oldest_supported_python_specifier = Specifier(f'>={oldest_supported_python}')\n    if requires_python_str is None:\n        requires_python = oldest_supported_python_specifier\n    else:\n        assert type(requires_python_str) is str\n        requires_python = Specifier(requires_python_str)\n        assert requires_python != oldest_supported_python_specifier, f'requires_python=\"{requires_python}\" is redundant'\n        assert oldest_supported_python_specifier.contains(requires_python.version), f\"'requires_python' contains versions lower than typeshed's oldest supported Python ({oldest_supported_python})\"\n        assert requires_python.operator == '>=', \"'requires_python' should be a minimum version specifier, use '>=3.x'\"\n    empty_tools: dict[object, object] = {}\n    tools_settings: object = data.get('tool', empty_tools)\n    assert isinstance(tools_settings, dict)\n    assert tools_settings.keys() <= _KNOWN_METADATA_TOOL_FIELDS.keys(), f'Unrecognised tool for {distribution!r}'\n    for (tool, tk) in _KNOWN_METADATA_TOOL_FIELDS.items():\n        settings_for_tool: object = tools_settings.get(tool, {})\n        assert isinstance(settings_for_tool, dict)\n        for key in settings_for_tool:\n            assert key in tk, f'Unrecognised {tool} key {key!r} for {distribution!r}'\n    return StubMetadata(version=version, requires=requires, extra_description=extra_description, stub_distribution=stub_distribution, upstream_repository=upstream_repository, obsolete_since=obsolete_since, no_longer_updated=no_longer_updated, uploaded_to_pypi=uploaded_to_pypi, partial_stub=partial_stub, stubtest_settings=read_stubtest_settings(distribution), requires_python=requires_python)"
        ]
    },
    {
        "func_name": "get_pypi_name_to_typeshed_name_mapping",
        "original": "@cache\ndef get_pypi_name_to_typeshed_name_mapping() -> Mapping[str, str]:\n    return {read_metadata(typeshed_name).stub_distribution: typeshed_name for typeshed_name in os.listdir('stubs')}",
        "mutated": [
            "@cache\ndef get_pypi_name_to_typeshed_name_mapping() -> Mapping[str, str]:\n    if False:\n        i = 10\n    return {read_metadata(typeshed_name).stub_distribution: typeshed_name for typeshed_name in os.listdir('stubs')}",
            "@cache\ndef get_pypi_name_to_typeshed_name_mapping() -> Mapping[str, str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {read_metadata(typeshed_name).stub_distribution: typeshed_name for typeshed_name in os.listdir('stubs')}",
            "@cache\ndef get_pypi_name_to_typeshed_name_mapping() -> Mapping[str, str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {read_metadata(typeshed_name).stub_distribution: typeshed_name for typeshed_name in os.listdir('stubs')}",
            "@cache\ndef get_pypi_name_to_typeshed_name_mapping() -> Mapping[str, str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {read_metadata(typeshed_name).stub_distribution: typeshed_name for typeshed_name in os.listdir('stubs')}",
            "@cache\ndef get_pypi_name_to_typeshed_name_mapping() -> Mapping[str, str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {read_metadata(typeshed_name).stub_distribution: typeshed_name for typeshed_name in os.listdir('stubs')}"
        ]
    },
    {
        "func_name": "read_dependencies",
        "original": "@cache\ndef read_dependencies(distribution: str) -> PackageDependencies:\n    \"\"\"Read the dependencies listed in a METADATA.toml file for a stubs package.\n\n    Once the dependencies have been read,\n    determine which dependencies are typeshed-internal dependencies,\n    and which dependencies are external (non-types) dependencies.\n    For typeshed dependencies, translate the \"dependency name\" into the \"package name\";\n    for external dependencies, leave them as they are in the METADATA.toml file.\n\n    Note that this function may consider things to be typeshed stubs\n    even if they haven't yet been uploaded to PyPI.\n    If a typeshed stub is removed, this function will consider it to be an external dependency.\n    \"\"\"\n    pypi_name_to_typeshed_name_mapping = get_pypi_name_to_typeshed_name_mapping()\n    typeshed: list[str] = []\n    external: list[str] = []\n    for dependency in read_metadata(distribution).requires:\n        maybe_typeshed_dependency = Requirement(dependency).name\n        if maybe_typeshed_dependency in pypi_name_to_typeshed_name_mapping:\n            typeshed.append(pypi_name_to_typeshed_name_mapping[maybe_typeshed_dependency])\n        else:\n            external.append(str(Requirement(dependency)))\n    return PackageDependencies(tuple(typeshed), tuple(external))",
        "mutated": [
            "@cache\ndef read_dependencies(distribution: str) -> PackageDependencies:\n    if False:\n        i = 10\n    'Read the dependencies listed in a METADATA.toml file for a stubs package.\\n\\n    Once the dependencies have been read,\\n    determine which dependencies are typeshed-internal dependencies,\\n    and which dependencies are external (non-types) dependencies.\\n    For typeshed dependencies, translate the \"dependency name\" into the \"package name\";\\n    for external dependencies, leave them as they are in the METADATA.toml file.\\n\\n    Note that this function may consider things to be typeshed stubs\\n    even if they haven\\'t yet been uploaded to PyPI.\\n    If a typeshed stub is removed, this function will consider it to be an external dependency.\\n    '\n    pypi_name_to_typeshed_name_mapping = get_pypi_name_to_typeshed_name_mapping()\n    typeshed: list[str] = []\n    external: list[str] = []\n    for dependency in read_metadata(distribution).requires:\n        maybe_typeshed_dependency = Requirement(dependency).name\n        if maybe_typeshed_dependency in pypi_name_to_typeshed_name_mapping:\n            typeshed.append(pypi_name_to_typeshed_name_mapping[maybe_typeshed_dependency])\n        else:\n            external.append(str(Requirement(dependency)))\n    return PackageDependencies(tuple(typeshed), tuple(external))",
            "@cache\ndef read_dependencies(distribution: str) -> PackageDependencies:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Read the dependencies listed in a METADATA.toml file for a stubs package.\\n\\n    Once the dependencies have been read,\\n    determine which dependencies are typeshed-internal dependencies,\\n    and which dependencies are external (non-types) dependencies.\\n    For typeshed dependencies, translate the \"dependency name\" into the \"package name\";\\n    for external dependencies, leave them as they are in the METADATA.toml file.\\n\\n    Note that this function may consider things to be typeshed stubs\\n    even if they haven\\'t yet been uploaded to PyPI.\\n    If a typeshed stub is removed, this function will consider it to be an external dependency.\\n    '\n    pypi_name_to_typeshed_name_mapping = get_pypi_name_to_typeshed_name_mapping()\n    typeshed: list[str] = []\n    external: list[str] = []\n    for dependency in read_metadata(distribution).requires:\n        maybe_typeshed_dependency = Requirement(dependency).name\n        if maybe_typeshed_dependency in pypi_name_to_typeshed_name_mapping:\n            typeshed.append(pypi_name_to_typeshed_name_mapping[maybe_typeshed_dependency])\n        else:\n            external.append(str(Requirement(dependency)))\n    return PackageDependencies(tuple(typeshed), tuple(external))",
            "@cache\ndef read_dependencies(distribution: str) -> PackageDependencies:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Read the dependencies listed in a METADATA.toml file for a stubs package.\\n\\n    Once the dependencies have been read,\\n    determine which dependencies are typeshed-internal dependencies,\\n    and which dependencies are external (non-types) dependencies.\\n    For typeshed dependencies, translate the \"dependency name\" into the \"package name\";\\n    for external dependencies, leave them as they are in the METADATA.toml file.\\n\\n    Note that this function may consider things to be typeshed stubs\\n    even if they haven\\'t yet been uploaded to PyPI.\\n    If a typeshed stub is removed, this function will consider it to be an external dependency.\\n    '\n    pypi_name_to_typeshed_name_mapping = get_pypi_name_to_typeshed_name_mapping()\n    typeshed: list[str] = []\n    external: list[str] = []\n    for dependency in read_metadata(distribution).requires:\n        maybe_typeshed_dependency = Requirement(dependency).name\n        if maybe_typeshed_dependency in pypi_name_to_typeshed_name_mapping:\n            typeshed.append(pypi_name_to_typeshed_name_mapping[maybe_typeshed_dependency])\n        else:\n            external.append(str(Requirement(dependency)))\n    return PackageDependencies(tuple(typeshed), tuple(external))",
            "@cache\ndef read_dependencies(distribution: str) -> PackageDependencies:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Read the dependencies listed in a METADATA.toml file for a stubs package.\\n\\n    Once the dependencies have been read,\\n    determine which dependencies are typeshed-internal dependencies,\\n    and which dependencies are external (non-types) dependencies.\\n    For typeshed dependencies, translate the \"dependency name\" into the \"package name\";\\n    for external dependencies, leave them as they are in the METADATA.toml file.\\n\\n    Note that this function may consider things to be typeshed stubs\\n    even if they haven\\'t yet been uploaded to PyPI.\\n    If a typeshed stub is removed, this function will consider it to be an external dependency.\\n    '\n    pypi_name_to_typeshed_name_mapping = get_pypi_name_to_typeshed_name_mapping()\n    typeshed: list[str] = []\n    external: list[str] = []\n    for dependency in read_metadata(distribution).requires:\n        maybe_typeshed_dependency = Requirement(dependency).name\n        if maybe_typeshed_dependency in pypi_name_to_typeshed_name_mapping:\n            typeshed.append(pypi_name_to_typeshed_name_mapping[maybe_typeshed_dependency])\n        else:\n            external.append(str(Requirement(dependency)))\n    return PackageDependencies(tuple(typeshed), tuple(external))",
            "@cache\ndef read_dependencies(distribution: str) -> PackageDependencies:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Read the dependencies listed in a METADATA.toml file for a stubs package.\\n\\n    Once the dependencies have been read,\\n    determine which dependencies are typeshed-internal dependencies,\\n    and which dependencies are external (non-types) dependencies.\\n    For typeshed dependencies, translate the \"dependency name\" into the \"package name\";\\n    for external dependencies, leave them as they are in the METADATA.toml file.\\n\\n    Note that this function may consider things to be typeshed stubs\\n    even if they haven\\'t yet been uploaded to PyPI.\\n    If a typeshed stub is removed, this function will consider it to be an external dependency.\\n    '\n    pypi_name_to_typeshed_name_mapping = get_pypi_name_to_typeshed_name_mapping()\n    typeshed: list[str] = []\n    external: list[str] = []\n    for dependency in read_metadata(distribution).requires:\n        maybe_typeshed_dependency = Requirement(dependency).name\n        if maybe_typeshed_dependency in pypi_name_to_typeshed_name_mapping:\n            typeshed.append(pypi_name_to_typeshed_name_mapping[maybe_typeshed_dependency])\n        else:\n            external.append(str(Requirement(dependency)))\n    return PackageDependencies(tuple(typeshed), tuple(external))"
        ]
    },
    {
        "func_name": "get_recursive_requirements",
        "original": "@cache\ndef get_recursive_requirements(package_name: str) -> PackageDependencies:\n    \"\"\"Recursively gather dependencies for a single stubs package.\n\n    For example, if the stubs for `caldav`\n    declare a dependency on typeshed's stubs for `requests`,\n    and the stubs for requests declare a dependency on typeshed's stubs for `urllib3`,\n    `get_recursive_requirements(\"caldav\")` will determine that the stubs for `caldav`\n    have both `requests` and `urllib3` as typeshed-internal dependencies.\n    \"\"\"\n    typeshed: set[str] = set()\n    external: set[str] = set()\n    non_recursive_requirements = read_dependencies(package_name)\n    typeshed.update(non_recursive_requirements.typeshed_pkgs)\n    external.update(non_recursive_requirements.external_pkgs)\n    for pkg in non_recursive_requirements.typeshed_pkgs:\n        reqs = get_recursive_requirements(pkg)\n        typeshed.update(reqs.typeshed_pkgs)\n        external.update(reqs.external_pkgs)\n    return PackageDependencies(tuple(sorted(typeshed)), tuple(sorted(external)))",
        "mutated": [
            "@cache\ndef get_recursive_requirements(package_name: str) -> PackageDependencies:\n    if False:\n        i = 10\n    'Recursively gather dependencies for a single stubs package.\\n\\n    For example, if the stubs for `caldav`\\n    declare a dependency on typeshed\\'s stubs for `requests`,\\n    and the stubs for requests declare a dependency on typeshed\\'s stubs for `urllib3`,\\n    `get_recursive_requirements(\"caldav\")` will determine that the stubs for `caldav`\\n    have both `requests` and `urllib3` as typeshed-internal dependencies.\\n    '\n    typeshed: set[str] = set()\n    external: set[str] = set()\n    non_recursive_requirements = read_dependencies(package_name)\n    typeshed.update(non_recursive_requirements.typeshed_pkgs)\n    external.update(non_recursive_requirements.external_pkgs)\n    for pkg in non_recursive_requirements.typeshed_pkgs:\n        reqs = get_recursive_requirements(pkg)\n        typeshed.update(reqs.typeshed_pkgs)\n        external.update(reqs.external_pkgs)\n    return PackageDependencies(tuple(sorted(typeshed)), tuple(sorted(external)))",
            "@cache\ndef get_recursive_requirements(package_name: str) -> PackageDependencies:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Recursively gather dependencies for a single stubs package.\\n\\n    For example, if the stubs for `caldav`\\n    declare a dependency on typeshed\\'s stubs for `requests`,\\n    and the stubs for requests declare a dependency on typeshed\\'s stubs for `urllib3`,\\n    `get_recursive_requirements(\"caldav\")` will determine that the stubs for `caldav`\\n    have both `requests` and `urllib3` as typeshed-internal dependencies.\\n    '\n    typeshed: set[str] = set()\n    external: set[str] = set()\n    non_recursive_requirements = read_dependencies(package_name)\n    typeshed.update(non_recursive_requirements.typeshed_pkgs)\n    external.update(non_recursive_requirements.external_pkgs)\n    for pkg in non_recursive_requirements.typeshed_pkgs:\n        reqs = get_recursive_requirements(pkg)\n        typeshed.update(reqs.typeshed_pkgs)\n        external.update(reqs.external_pkgs)\n    return PackageDependencies(tuple(sorted(typeshed)), tuple(sorted(external)))",
            "@cache\ndef get_recursive_requirements(package_name: str) -> PackageDependencies:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Recursively gather dependencies for a single stubs package.\\n\\n    For example, if the stubs for `caldav`\\n    declare a dependency on typeshed\\'s stubs for `requests`,\\n    and the stubs for requests declare a dependency on typeshed\\'s stubs for `urllib3`,\\n    `get_recursive_requirements(\"caldav\")` will determine that the stubs for `caldav`\\n    have both `requests` and `urllib3` as typeshed-internal dependencies.\\n    '\n    typeshed: set[str] = set()\n    external: set[str] = set()\n    non_recursive_requirements = read_dependencies(package_name)\n    typeshed.update(non_recursive_requirements.typeshed_pkgs)\n    external.update(non_recursive_requirements.external_pkgs)\n    for pkg in non_recursive_requirements.typeshed_pkgs:\n        reqs = get_recursive_requirements(pkg)\n        typeshed.update(reqs.typeshed_pkgs)\n        external.update(reqs.external_pkgs)\n    return PackageDependencies(tuple(sorted(typeshed)), tuple(sorted(external)))",
            "@cache\ndef get_recursive_requirements(package_name: str) -> PackageDependencies:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Recursively gather dependencies for a single stubs package.\\n\\n    For example, if the stubs for `caldav`\\n    declare a dependency on typeshed\\'s stubs for `requests`,\\n    and the stubs for requests declare a dependency on typeshed\\'s stubs for `urllib3`,\\n    `get_recursive_requirements(\"caldav\")` will determine that the stubs for `caldav`\\n    have both `requests` and `urllib3` as typeshed-internal dependencies.\\n    '\n    typeshed: set[str] = set()\n    external: set[str] = set()\n    non_recursive_requirements = read_dependencies(package_name)\n    typeshed.update(non_recursive_requirements.typeshed_pkgs)\n    external.update(non_recursive_requirements.external_pkgs)\n    for pkg in non_recursive_requirements.typeshed_pkgs:\n        reqs = get_recursive_requirements(pkg)\n        typeshed.update(reqs.typeshed_pkgs)\n        external.update(reqs.external_pkgs)\n    return PackageDependencies(tuple(sorted(typeshed)), tuple(sorted(external)))",
            "@cache\ndef get_recursive_requirements(package_name: str) -> PackageDependencies:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Recursively gather dependencies for a single stubs package.\\n\\n    For example, if the stubs for `caldav`\\n    declare a dependency on typeshed\\'s stubs for `requests`,\\n    and the stubs for requests declare a dependency on typeshed\\'s stubs for `urllib3`,\\n    `get_recursive_requirements(\"caldav\")` will determine that the stubs for `caldav`\\n    have both `requests` and `urllib3` as typeshed-internal dependencies.\\n    '\n    typeshed: set[str] = set()\n    external: set[str] = set()\n    non_recursive_requirements = read_dependencies(package_name)\n    typeshed.update(non_recursive_requirements.typeshed_pkgs)\n    external.update(non_recursive_requirements.external_pkgs)\n    for pkg in non_recursive_requirements.typeshed_pkgs:\n        reqs = get_recursive_requirements(pkg)\n        typeshed.update(reqs.typeshed_pkgs)\n        external.update(reqs.external_pkgs)\n    return PackageDependencies(tuple(sorted(typeshed)), tuple(sorted(external)))"
        ]
    }
]