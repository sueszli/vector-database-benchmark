[
    {
        "func_name": "_load_hourly_od",
        "original": "def _load_hourly_od(basename):\n    filename = os.path.join(DATA, basename.replace('.csv.gz', '.pkl'))\n    if os.path.exists(filename):\n        return filename\n    gz_filename = os.path.join(DATA, basename)\n    if not os.path.exists(gz_filename):\n        url = SOURCE_DIR + basename\n        logging.debug('downloading {}'.format(url))\n        urllib.request.urlretrieve(url, gz_filename)\n    csv_filename = gz_filename[:-3]\n    assert csv_filename.endswith('.csv')\n    if not os.path.exists(csv_filename):\n        logging.debug('unzipping {}'.format(gz_filename))\n        subprocess.check_call(['gunzip', '-k', gz_filename])\n    assert os.path.exists(csv_filename)\n    logging.debug('converting {}'.format(csv_filename))\n    start_date = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n    stations = {}\n    num_rows = sum((1 for _ in open(csv_filename)))\n    logging.info('Formatting {} rows'.format(num_rows))\n    rows = torch.empty((num_rows, 4), dtype=torch.long)\n    with open(csv_filename) as f:\n        for (i, (date, hour, origin, destin, trip_count)) in enumerate(csv.reader(f)):\n            date = datetime.datetime.strptime(date, '%Y-%m-%d')\n            date += datetime.timedelta(hours=int(hour))\n            rows[i, 0] = int((date - start_date).total_seconds() / 3600)\n            rows[i, 1] = stations.setdefault(origin, len(stations))\n            rows[i, 2] = stations.setdefault(destin, len(stations))\n            rows[i, 3] = int(trip_count)\n            if i % 10000 == 0:\n                sys.stderr.write('.')\n                sys.stderr.flush()\n    dataset = {'basename': basename, 'start_date': start_date, 'stations': stations, 'rows': rows, 'schema': ['time_hours', 'origin', 'destin', 'trip_count']}\n    dataset['rows']\n    logging.debug('saving {}'.format(filename))\n    torch.save(dataset, filename)\n    return filename",
        "mutated": [
            "def _load_hourly_od(basename):\n    if False:\n        i = 10\n    filename = os.path.join(DATA, basename.replace('.csv.gz', '.pkl'))\n    if os.path.exists(filename):\n        return filename\n    gz_filename = os.path.join(DATA, basename)\n    if not os.path.exists(gz_filename):\n        url = SOURCE_DIR + basename\n        logging.debug('downloading {}'.format(url))\n        urllib.request.urlretrieve(url, gz_filename)\n    csv_filename = gz_filename[:-3]\n    assert csv_filename.endswith('.csv')\n    if not os.path.exists(csv_filename):\n        logging.debug('unzipping {}'.format(gz_filename))\n        subprocess.check_call(['gunzip', '-k', gz_filename])\n    assert os.path.exists(csv_filename)\n    logging.debug('converting {}'.format(csv_filename))\n    start_date = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n    stations = {}\n    num_rows = sum((1 for _ in open(csv_filename)))\n    logging.info('Formatting {} rows'.format(num_rows))\n    rows = torch.empty((num_rows, 4), dtype=torch.long)\n    with open(csv_filename) as f:\n        for (i, (date, hour, origin, destin, trip_count)) in enumerate(csv.reader(f)):\n            date = datetime.datetime.strptime(date, '%Y-%m-%d')\n            date += datetime.timedelta(hours=int(hour))\n            rows[i, 0] = int((date - start_date).total_seconds() / 3600)\n            rows[i, 1] = stations.setdefault(origin, len(stations))\n            rows[i, 2] = stations.setdefault(destin, len(stations))\n            rows[i, 3] = int(trip_count)\n            if i % 10000 == 0:\n                sys.stderr.write('.')\n                sys.stderr.flush()\n    dataset = {'basename': basename, 'start_date': start_date, 'stations': stations, 'rows': rows, 'schema': ['time_hours', 'origin', 'destin', 'trip_count']}\n    dataset['rows']\n    logging.debug('saving {}'.format(filename))\n    torch.save(dataset, filename)\n    return filename",
            "def _load_hourly_od(basename):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    filename = os.path.join(DATA, basename.replace('.csv.gz', '.pkl'))\n    if os.path.exists(filename):\n        return filename\n    gz_filename = os.path.join(DATA, basename)\n    if not os.path.exists(gz_filename):\n        url = SOURCE_DIR + basename\n        logging.debug('downloading {}'.format(url))\n        urllib.request.urlretrieve(url, gz_filename)\n    csv_filename = gz_filename[:-3]\n    assert csv_filename.endswith('.csv')\n    if not os.path.exists(csv_filename):\n        logging.debug('unzipping {}'.format(gz_filename))\n        subprocess.check_call(['gunzip', '-k', gz_filename])\n    assert os.path.exists(csv_filename)\n    logging.debug('converting {}'.format(csv_filename))\n    start_date = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n    stations = {}\n    num_rows = sum((1 for _ in open(csv_filename)))\n    logging.info('Formatting {} rows'.format(num_rows))\n    rows = torch.empty((num_rows, 4), dtype=torch.long)\n    with open(csv_filename) as f:\n        for (i, (date, hour, origin, destin, trip_count)) in enumerate(csv.reader(f)):\n            date = datetime.datetime.strptime(date, '%Y-%m-%d')\n            date += datetime.timedelta(hours=int(hour))\n            rows[i, 0] = int((date - start_date).total_seconds() / 3600)\n            rows[i, 1] = stations.setdefault(origin, len(stations))\n            rows[i, 2] = stations.setdefault(destin, len(stations))\n            rows[i, 3] = int(trip_count)\n            if i % 10000 == 0:\n                sys.stderr.write('.')\n                sys.stderr.flush()\n    dataset = {'basename': basename, 'start_date': start_date, 'stations': stations, 'rows': rows, 'schema': ['time_hours', 'origin', 'destin', 'trip_count']}\n    dataset['rows']\n    logging.debug('saving {}'.format(filename))\n    torch.save(dataset, filename)\n    return filename",
            "def _load_hourly_od(basename):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    filename = os.path.join(DATA, basename.replace('.csv.gz', '.pkl'))\n    if os.path.exists(filename):\n        return filename\n    gz_filename = os.path.join(DATA, basename)\n    if not os.path.exists(gz_filename):\n        url = SOURCE_DIR + basename\n        logging.debug('downloading {}'.format(url))\n        urllib.request.urlretrieve(url, gz_filename)\n    csv_filename = gz_filename[:-3]\n    assert csv_filename.endswith('.csv')\n    if not os.path.exists(csv_filename):\n        logging.debug('unzipping {}'.format(gz_filename))\n        subprocess.check_call(['gunzip', '-k', gz_filename])\n    assert os.path.exists(csv_filename)\n    logging.debug('converting {}'.format(csv_filename))\n    start_date = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n    stations = {}\n    num_rows = sum((1 for _ in open(csv_filename)))\n    logging.info('Formatting {} rows'.format(num_rows))\n    rows = torch.empty((num_rows, 4), dtype=torch.long)\n    with open(csv_filename) as f:\n        for (i, (date, hour, origin, destin, trip_count)) in enumerate(csv.reader(f)):\n            date = datetime.datetime.strptime(date, '%Y-%m-%d')\n            date += datetime.timedelta(hours=int(hour))\n            rows[i, 0] = int((date - start_date).total_seconds() / 3600)\n            rows[i, 1] = stations.setdefault(origin, len(stations))\n            rows[i, 2] = stations.setdefault(destin, len(stations))\n            rows[i, 3] = int(trip_count)\n            if i % 10000 == 0:\n                sys.stderr.write('.')\n                sys.stderr.flush()\n    dataset = {'basename': basename, 'start_date': start_date, 'stations': stations, 'rows': rows, 'schema': ['time_hours', 'origin', 'destin', 'trip_count']}\n    dataset['rows']\n    logging.debug('saving {}'.format(filename))\n    torch.save(dataset, filename)\n    return filename",
            "def _load_hourly_od(basename):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    filename = os.path.join(DATA, basename.replace('.csv.gz', '.pkl'))\n    if os.path.exists(filename):\n        return filename\n    gz_filename = os.path.join(DATA, basename)\n    if not os.path.exists(gz_filename):\n        url = SOURCE_DIR + basename\n        logging.debug('downloading {}'.format(url))\n        urllib.request.urlretrieve(url, gz_filename)\n    csv_filename = gz_filename[:-3]\n    assert csv_filename.endswith('.csv')\n    if not os.path.exists(csv_filename):\n        logging.debug('unzipping {}'.format(gz_filename))\n        subprocess.check_call(['gunzip', '-k', gz_filename])\n    assert os.path.exists(csv_filename)\n    logging.debug('converting {}'.format(csv_filename))\n    start_date = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n    stations = {}\n    num_rows = sum((1 for _ in open(csv_filename)))\n    logging.info('Formatting {} rows'.format(num_rows))\n    rows = torch.empty((num_rows, 4), dtype=torch.long)\n    with open(csv_filename) as f:\n        for (i, (date, hour, origin, destin, trip_count)) in enumerate(csv.reader(f)):\n            date = datetime.datetime.strptime(date, '%Y-%m-%d')\n            date += datetime.timedelta(hours=int(hour))\n            rows[i, 0] = int((date - start_date).total_seconds() / 3600)\n            rows[i, 1] = stations.setdefault(origin, len(stations))\n            rows[i, 2] = stations.setdefault(destin, len(stations))\n            rows[i, 3] = int(trip_count)\n            if i % 10000 == 0:\n                sys.stderr.write('.')\n                sys.stderr.flush()\n    dataset = {'basename': basename, 'start_date': start_date, 'stations': stations, 'rows': rows, 'schema': ['time_hours', 'origin', 'destin', 'trip_count']}\n    dataset['rows']\n    logging.debug('saving {}'.format(filename))\n    torch.save(dataset, filename)\n    return filename",
            "def _load_hourly_od(basename):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    filename = os.path.join(DATA, basename.replace('.csv.gz', '.pkl'))\n    if os.path.exists(filename):\n        return filename\n    gz_filename = os.path.join(DATA, basename)\n    if not os.path.exists(gz_filename):\n        url = SOURCE_DIR + basename\n        logging.debug('downloading {}'.format(url))\n        urllib.request.urlretrieve(url, gz_filename)\n    csv_filename = gz_filename[:-3]\n    assert csv_filename.endswith('.csv')\n    if not os.path.exists(csv_filename):\n        logging.debug('unzipping {}'.format(gz_filename))\n        subprocess.check_call(['gunzip', '-k', gz_filename])\n    assert os.path.exists(csv_filename)\n    logging.debug('converting {}'.format(csv_filename))\n    start_date = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n    stations = {}\n    num_rows = sum((1 for _ in open(csv_filename)))\n    logging.info('Formatting {} rows'.format(num_rows))\n    rows = torch.empty((num_rows, 4), dtype=torch.long)\n    with open(csv_filename) as f:\n        for (i, (date, hour, origin, destin, trip_count)) in enumerate(csv.reader(f)):\n            date = datetime.datetime.strptime(date, '%Y-%m-%d')\n            date += datetime.timedelta(hours=int(hour))\n            rows[i, 0] = int((date - start_date).total_seconds() / 3600)\n            rows[i, 1] = stations.setdefault(origin, len(stations))\n            rows[i, 2] = stations.setdefault(destin, len(stations))\n            rows[i, 3] = int(trip_count)\n            if i % 10000 == 0:\n                sys.stderr.write('.')\n                sys.stderr.flush()\n    dataset = {'basename': basename, 'start_date': start_date, 'stations': stations, 'rows': rows, 'schema': ['time_hours', 'origin', 'destin', 'trip_count']}\n    dataset['rows']\n    logging.debug('saving {}'.format(filename))\n    torch.save(dataset, filename)\n    return filename"
        ]
    },
    {
        "func_name": "load_bart_od",
        "original": "def load_bart_od():\n    \"\"\"\n    Load a dataset of hourly origin-destination ridership counts for every pair\n    of BART stations during the years 2011-2019.\n\n    **Source** https://www.bart.gov/about/reports/ridership\n\n    This downloads the dataset the first time it is called. On subsequent calls\n    this reads from a local cached file ``.pkl.bz2``. This attempts to\n    download a preprocessed compressed cached file maintained by the Pyro team.\n    On cache hit this should be very fast. On cache miss this falls back to\n    downloading the original data source and preprocessing the dataset,\n    requiring about 350MB of file transfer, storing a few GB of temp files, and\n    taking upwards of 30 minutes.\n\n    :returns: a dataset is a dictionary with fields:\n\n        -   \"stations\": a list of strings of station names\n        -   \"start_date\": a :py:class:`datetime.datetime` for the first observaion\n        -   \"counts\": a ``torch.FloatTensor`` of ridership counts, with shape\n            ``(num_hours, len(stations), len(stations))``.\n    \"\"\"\n    _mkdir_p(DATA)\n    filename = os.path.join(DATA, 'bart_full.pkl.bz2')\n    pkl_file = filename.rsplit('.', 1)[0]\n    if not os.path.exists(pkl_file):\n        try:\n            urllib.request.urlretrieve(CACHE_URL, filename)\n            logging.debug('cache hit, uncompressing')\n            with bz2.BZ2File(filename) as src, open(filename[:-4], 'wb') as dst:\n                dst.write(src.read())\n        except urllib.error.HTTPError:\n            logging.debug('cache miss, preprocessing from scratch')\n    if os.path.exists(pkl_file):\n        return torch.load(pkl_file)\n    filenames = multiprocessing.Pool(len(SOURCE_FILES)).map(_load_hourly_od, SOURCE_FILES)\n    datasets = list(map(torch.load, filenames))\n    stations = sorted(set().union(*(d['stations'].keys() for d in datasets)))\n    min_time = min((int(d['rows'][:, 0].min()) for d in datasets))\n    max_time = max((int(d['rows'][:, 0].max()) for d in datasets))\n    num_rows = max_time - min_time + 1\n    start_date = (datasets[0]['start_date'] + datetime.timedelta(hours=min_time),)\n    logging.info('Loaded data from {} stations, {} hours'.format(len(stations), num_rows))\n    result = torch.zeros(num_rows, len(stations), len(stations))\n    for dataset in datasets:\n        part_stations = sorted(dataset['stations'], key=dataset['stations'].__getitem__)\n        part_to_whole = torch.tensor(list(map(stations.index, part_stations)))\n        time = dataset['rows'][:, 0] - min_time\n        origin = part_to_whole[dataset['rows'][:, 1]]\n        destin = part_to_whole[dataset['rows'][:, 2]]\n        count = dataset['rows'][:, 3].float()\n        result[time, origin, destin] = count\n        dataset.clear()\n    logging.info('Loaded {} shaped data of mean {:0.3g}'.format(result.shape, result.mean()))\n    dataset = {'stations': stations, 'start_date': start_date, 'counts': result}\n    torch.save(dataset, pkl_file)\n    subprocess.check_call(['bzip2', '-k', pkl_file])\n    assert os.path.exists(filename)\n    return dataset",
        "mutated": [
            "def load_bart_od():\n    if False:\n        i = 10\n    '\\n    Load a dataset of hourly origin-destination ridership counts for every pair\\n    of BART stations during the years 2011-2019.\\n\\n    **Source** https://www.bart.gov/about/reports/ridership\\n\\n    This downloads the dataset the first time it is called. On subsequent calls\\n    this reads from a local cached file ``.pkl.bz2``. This attempts to\\n    download a preprocessed compressed cached file maintained by the Pyro team.\\n    On cache hit this should be very fast. On cache miss this falls back to\\n    downloading the original data source and preprocessing the dataset,\\n    requiring about 350MB of file transfer, storing a few GB of temp files, and\\n    taking upwards of 30 minutes.\\n\\n    :returns: a dataset is a dictionary with fields:\\n\\n        -   \"stations\": a list of strings of station names\\n        -   \"start_date\": a :py:class:`datetime.datetime` for the first observaion\\n        -   \"counts\": a ``torch.FloatTensor`` of ridership counts, with shape\\n            ``(num_hours, len(stations), len(stations))``.\\n    '\n    _mkdir_p(DATA)\n    filename = os.path.join(DATA, 'bart_full.pkl.bz2')\n    pkl_file = filename.rsplit('.', 1)[0]\n    if not os.path.exists(pkl_file):\n        try:\n            urllib.request.urlretrieve(CACHE_URL, filename)\n            logging.debug('cache hit, uncompressing')\n            with bz2.BZ2File(filename) as src, open(filename[:-4], 'wb') as dst:\n                dst.write(src.read())\n        except urllib.error.HTTPError:\n            logging.debug('cache miss, preprocessing from scratch')\n    if os.path.exists(pkl_file):\n        return torch.load(pkl_file)\n    filenames = multiprocessing.Pool(len(SOURCE_FILES)).map(_load_hourly_od, SOURCE_FILES)\n    datasets = list(map(torch.load, filenames))\n    stations = sorted(set().union(*(d['stations'].keys() for d in datasets)))\n    min_time = min((int(d['rows'][:, 0].min()) for d in datasets))\n    max_time = max((int(d['rows'][:, 0].max()) for d in datasets))\n    num_rows = max_time - min_time + 1\n    start_date = (datasets[0]['start_date'] + datetime.timedelta(hours=min_time),)\n    logging.info('Loaded data from {} stations, {} hours'.format(len(stations), num_rows))\n    result = torch.zeros(num_rows, len(stations), len(stations))\n    for dataset in datasets:\n        part_stations = sorted(dataset['stations'], key=dataset['stations'].__getitem__)\n        part_to_whole = torch.tensor(list(map(stations.index, part_stations)))\n        time = dataset['rows'][:, 0] - min_time\n        origin = part_to_whole[dataset['rows'][:, 1]]\n        destin = part_to_whole[dataset['rows'][:, 2]]\n        count = dataset['rows'][:, 3].float()\n        result[time, origin, destin] = count\n        dataset.clear()\n    logging.info('Loaded {} shaped data of mean {:0.3g}'.format(result.shape, result.mean()))\n    dataset = {'stations': stations, 'start_date': start_date, 'counts': result}\n    torch.save(dataset, pkl_file)\n    subprocess.check_call(['bzip2', '-k', pkl_file])\n    assert os.path.exists(filename)\n    return dataset",
            "def load_bart_od():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Load a dataset of hourly origin-destination ridership counts for every pair\\n    of BART stations during the years 2011-2019.\\n\\n    **Source** https://www.bart.gov/about/reports/ridership\\n\\n    This downloads the dataset the first time it is called. On subsequent calls\\n    this reads from a local cached file ``.pkl.bz2``. This attempts to\\n    download a preprocessed compressed cached file maintained by the Pyro team.\\n    On cache hit this should be very fast. On cache miss this falls back to\\n    downloading the original data source and preprocessing the dataset,\\n    requiring about 350MB of file transfer, storing a few GB of temp files, and\\n    taking upwards of 30 minutes.\\n\\n    :returns: a dataset is a dictionary with fields:\\n\\n        -   \"stations\": a list of strings of station names\\n        -   \"start_date\": a :py:class:`datetime.datetime` for the first observaion\\n        -   \"counts\": a ``torch.FloatTensor`` of ridership counts, with shape\\n            ``(num_hours, len(stations), len(stations))``.\\n    '\n    _mkdir_p(DATA)\n    filename = os.path.join(DATA, 'bart_full.pkl.bz2')\n    pkl_file = filename.rsplit('.', 1)[0]\n    if not os.path.exists(pkl_file):\n        try:\n            urllib.request.urlretrieve(CACHE_URL, filename)\n            logging.debug('cache hit, uncompressing')\n            with bz2.BZ2File(filename) as src, open(filename[:-4], 'wb') as dst:\n                dst.write(src.read())\n        except urllib.error.HTTPError:\n            logging.debug('cache miss, preprocessing from scratch')\n    if os.path.exists(pkl_file):\n        return torch.load(pkl_file)\n    filenames = multiprocessing.Pool(len(SOURCE_FILES)).map(_load_hourly_od, SOURCE_FILES)\n    datasets = list(map(torch.load, filenames))\n    stations = sorted(set().union(*(d['stations'].keys() for d in datasets)))\n    min_time = min((int(d['rows'][:, 0].min()) for d in datasets))\n    max_time = max((int(d['rows'][:, 0].max()) for d in datasets))\n    num_rows = max_time - min_time + 1\n    start_date = (datasets[0]['start_date'] + datetime.timedelta(hours=min_time),)\n    logging.info('Loaded data from {} stations, {} hours'.format(len(stations), num_rows))\n    result = torch.zeros(num_rows, len(stations), len(stations))\n    for dataset in datasets:\n        part_stations = sorted(dataset['stations'], key=dataset['stations'].__getitem__)\n        part_to_whole = torch.tensor(list(map(stations.index, part_stations)))\n        time = dataset['rows'][:, 0] - min_time\n        origin = part_to_whole[dataset['rows'][:, 1]]\n        destin = part_to_whole[dataset['rows'][:, 2]]\n        count = dataset['rows'][:, 3].float()\n        result[time, origin, destin] = count\n        dataset.clear()\n    logging.info('Loaded {} shaped data of mean {:0.3g}'.format(result.shape, result.mean()))\n    dataset = {'stations': stations, 'start_date': start_date, 'counts': result}\n    torch.save(dataset, pkl_file)\n    subprocess.check_call(['bzip2', '-k', pkl_file])\n    assert os.path.exists(filename)\n    return dataset",
            "def load_bart_od():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Load a dataset of hourly origin-destination ridership counts for every pair\\n    of BART stations during the years 2011-2019.\\n\\n    **Source** https://www.bart.gov/about/reports/ridership\\n\\n    This downloads the dataset the first time it is called. On subsequent calls\\n    this reads from a local cached file ``.pkl.bz2``. This attempts to\\n    download a preprocessed compressed cached file maintained by the Pyro team.\\n    On cache hit this should be very fast. On cache miss this falls back to\\n    downloading the original data source and preprocessing the dataset,\\n    requiring about 350MB of file transfer, storing a few GB of temp files, and\\n    taking upwards of 30 minutes.\\n\\n    :returns: a dataset is a dictionary with fields:\\n\\n        -   \"stations\": a list of strings of station names\\n        -   \"start_date\": a :py:class:`datetime.datetime` for the first observaion\\n        -   \"counts\": a ``torch.FloatTensor`` of ridership counts, with shape\\n            ``(num_hours, len(stations), len(stations))``.\\n    '\n    _mkdir_p(DATA)\n    filename = os.path.join(DATA, 'bart_full.pkl.bz2')\n    pkl_file = filename.rsplit('.', 1)[0]\n    if not os.path.exists(pkl_file):\n        try:\n            urllib.request.urlretrieve(CACHE_URL, filename)\n            logging.debug('cache hit, uncompressing')\n            with bz2.BZ2File(filename) as src, open(filename[:-4], 'wb') as dst:\n                dst.write(src.read())\n        except urllib.error.HTTPError:\n            logging.debug('cache miss, preprocessing from scratch')\n    if os.path.exists(pkl_file):\n        return torch.load(pkl_file)\n    filenames = multiprocessing.Pool(len(SOURCE_FILES)).map(_load_hourly_od, SOURCE_FILES)\n    datasets = list(map(torch.load, filenames))\n    stations = sorted(set().union(*(d['stations'].keys() for d in datasets)))\n    min_time = min((int(d['rows'][:, 0].min()) for d in datasets))\n    max_time = max((int(d['rows'][:, 0].max()) for d in datasets))\n    num_rows = max_time - min_time + 1\n    start_date = (datasets[0]['start_date'] + datetime.timedelta(hours=min_time),)\n    logging.info('Loaded data from {} stations, {} hours'.format(len(stations), num_rows))\n    result = torch.zeros(num_rows, len(stations), len(stations))\n    for dataset in datasets:\n        part_stations = sorted(dataset['stations'], key=dataset['stations'].__getitem__)\n        part_to_whole = torch.tensor(list(map(stations.index, part_stations)))\n        time = dataset['rows'][:, 0] - min_time\n        origin = part_to_whole[dataset['rows'][:, 1]]\n        destin = part_to_whole[dataset['rows'][:, 2]]\n        count = dataset['rows'][:, 3].float()\n        result[time, origin, destin] = count\n        dataset.clear()\n    logging.info('Loaded {} shaped data of mean {:0.3g}'.format(result.shape, result.mean()))\n    dataset = {'stations': stations, 'start_date': start_date, 'counts': result}\n    torch.save(dataset, pkl_file)\n    subprocess.check_call(['bzip2', '-k', pkl_file])\n    assert os.path.exists(filename)\n    return dataset",
            "def load_bart_od():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Load a dataset of hourly origin-destination ridership counts for every pair\\n    of BART stations during the years 2011-2019.\\n\\n    **Source** https://www.bart.gov/about/reports/ridership\\n\\n    This downloads the dataset the first time it is called. On subsequent calls\\n    this reads from a local cached file ``.pkl.bz2``. This attempts to\\n    download a preprocessed compressed cached file maintained by the Pyro team.\\n    On cache hit this should be very fast. On cache miss this falls back to\\n    downloading the original data source and preprocessing the dataset,\\n    requiring about 350MB of file transfer, storing a few GB of temp files, and\\n    taking upwards of 30 minutes.\\n\\n    :returns: a dataset is a dictionary with fields:\\n\\n        -   \"stations\": a list of strings of station names\\n        -   \"start_date\": a :py:class:`datetime.datetime` for the first observaion\\n        -   \"counts\": a ``torch.FloatTensor`` of ridership counts, with shape\\n            ``(num_hours, len(stations), len(stations))``.\\n    '\n    _mkdir_p(DATA)\n    filename = os.path.join(DATA, 'bart_full.pkl.bz2')\n    pkl_file = filename.rsplit('.', 1)[0]\n    if not os.path.exists(pkl_file):\n        try:\n            urllib.request.urlretrieve(CACHE_URL, filename)\n            logging.debug('cache hit, uncompressing')\n            with bz2.BZ2File(filename) as src, open(filename[:-4], 'wb') as dst:\n                dst.write(src.read())\n        except urllib.error.HTTPError:\n            logging.debug('cache miss, preprocessing from scratch')\n    if os.path.exists(pkl_file):\n        return torch.load(pkl_file)\n    filenames = multiprocessing.Pool(len(SOURCE_FILES)).map(_load_hourly_od, SOURCE_FILES)\n    datasets = list(map(torch.load, filenames))\n    stations = sorted(set().union(*(d['stations'].keys() for d in datasets)))\n    min_time = min((int(d['rows'][:, 0].min()) for d in datasets))\n    max_time = max((int(d['rows'][:, 0].max()) for d in datasets))\n    num_rows = max_time - min_time + 1\n    start_date = (datasets[0]['start_date'] + datetime.timedelta(hours=min_time),)\n    logging.info('Loaded data from {} stations, {} hours'.format(len(stations), num_rows))\n    result = torch.zeros(num_rows, len(stations), len(stations))\n    for dataset in datasets:\n        part_stations = sorted(dataset['stations'], key=dataset['stations'].__getitem__)\n        part_to_whole = torch.tensor(list(map(stations.index, part_stations)))\n        time = dataset['rows'][:, 0] - min_time\n        origin = part_to_whole[dataset['rows'][:, 1]]\n        destin = part_to_whole[dataset['rows'][:, 2]]\n        count = dataset['rows'][:, 3].float()\n        result[time, origin, destin] = count\n        dataset.clear()\n    logging.info('Loaded {} shaped data of mean {:0.3g}'.format(result.shape, result.mean()))\n    dataset = {'stations': stations, 'start_date': start_date, 'counts': result}\n    torch.save(dataset, pkl_file)\n    subprocess.check_call(['bzip2', '-k', pkl_file])\n    assert os.path.exists(filename)\n    return dataset",
            "def load_bart_od():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Load a dataset of hourly origin-destination ridership counts for every pair\\n    of BART stations during the years 2011-2019.\\n\\n    **Source** https://www.bart.gov/about/reports/ridership\\n\\n    This downloads the dataset the first time it is called. On subsequent calls\\n    this reads from a local cached file ``.pkl.bz2``. This attempts to\\n    download a preprocessed compressed cached file maintained by the Pyro team.\\n    On cache hit this should be very fast. On cache miss this falls back to\\n    downloading the original data source and preprocessing the dataset,\\n    requiring about 350MB of file transfer, storing a few GB of temp files, and\\n    taking upwards of 30 minutes.\\n\\n    :returns: a dataset is a dictionary with fields:\\n\\n        -   \"stations\": a list of strings of station names\\n        -   \"start_date\": a :py:class:`datetime.datetime` for the first observaion\\n        -   \"counts\": a ``torch.FloatTensor`` of ridership counts, with shape\\n            ``(num_hours, len(stations), len(stations))``.\\n    '\n    _mkdir_p(DATA)\n    filename = os.path.join(DATA, 'bart_full.pkl.bz2')\n    pkl_file = filename.rsplit('.', 1)[0]\n    if not os.path.exists(pkl_file):\n        try:\n            urllib.request.urlretrieve(CACHE_URL, filename)\n            logging.debug('cache hit, uncompressing')\n            with bz2.BZ2File(filename) as src, open(filename[:-4], 'wb') as dst:\n                dst.write(src.read())\n        except urllib.error.HTTPError:\n            logging.debug('cache miss, preprocessing from scratch')\n    if os.path.exists(pkl_file):\n        return torch.load(pkl_file)\n    filenames = multiprocessing.Pool(len(SOURCE_FILES)).map(_load_hourly_od, SOURCE_FILES)\n    datasets = list(map(torch.load, filenames))\n    stations = sorted(set().union(*(d['stations'].keys() for d in datasets)))\n    min_time = min((int(d['rows'][:, 0].min()) for d in datasets))\n    max_time = max((int(d['rows'][:, 0].max()) for d in datasets))\n    num_rows = max_time - min_time + 1\n    start_date = (datasets[0]['start_date'] + datetime.timedelta(hours=min_time),)\n    logging.info('Loaded data from {} stations, {} hours'.format(len(stations), num_rows))\n    result = torch.zeros(num_rows, len(stations), len(stations))\n    for dataset in datasets:\n        part_stations = sorted(dataset['stations'], key=dataset['stations'].__getitem__)\n        part_to_whole = torch.tensor(list(map(stations.index, part_stations)))\n        time = dataset['rows'][:, 0] - min_time\n        origin = part_to_whole[dataset['rows'][:, 1]]\n        destin = part_to_whole[dataset['rows'][:, 2]]\n        count = dataset['rows'][:, 3].float()\n        result[time, origin, destin] = count\n        dataset.clear()\n    logging.info('Loaded {} shaped data of mean {:0.3g}'.format(result.shape, result.mean()))\n    dataset = {'stations': stations, 'start_date': start_date, 'counts': result}\n    torch.save(dataset, pkl_file)\n    subprocess.check_call(['bzip2', '-k', pkl_file])\n    assert os.path.exists(filename)\n    return dataset"
        ]
    },
    {
        "func_name": "load_fake_od",
        "original": "def load_fake_od():\n    \"\"\"\n    Create a tiny synthetic dataset for smoke testing.\n    \"\"\"\n    dataset = {'stations': ['12TH', 'EMBR', 'SFIA'], 'start_date': datetime.datetime.strptime('2000-01-01', '%Y-%m-%d'), 'counts': torch.distributions.Poisson(100).sample([24 * 7 * 8, 3, 3])}\n    return dataset",
        "mutated": [
            "def load_fake_od():\n    if False:\n        i = 10\n    '\\n    Create a tiny synthetic dataset for smoke testing.\\n    '\n    dataset = {'stations': ['12TH', 'EMBR', 'SFIA'], 'start_date': datetime.datetime.strptime('2000-01-01', '%Y-%m-%d'), 'counts': torch.distributions.Poisson(100).sample([24 * 7 * 8, 3, 3])}\n    return dataset",
            "def load_fake_od():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Create a tiny synthetic dataset for smoke testing.\\n    '\n    dataset = {'stations': ['12TH', 'EMBR', 'SFIA'], 'start_date': datetime.datetime.strptime('2000-01-01', '%Y-%m-%d'), 'counts': torch.distributions.Poisson(100).sample([24 * 7 * 8, 3, 3])}\n    return dataset",
            "def load_fake_od():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Create a tiny synthetic dataset for smoke testing.\\n    '\n    dataset = {'stations': ['12TH', 'EMBR', 'SFIA'], 'start_date': datetime.datetime.strptime('2000-01-01', '%Y-%m-%d'), 'counts': torch.distributions.Poisson(100).sample([24 * 7 * 8, 3, 3])}\n    return dataset",
            "def load_fake_od():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Create a tiny synthetic dataset for smoke testing.\\n    '\n    dataset = {'stations': ['12TH', 'EMBR', 'SFIA'], 'start_date': datetime.datetime.strptime('2000-01-01', '%Y-%m-%d'), 'counts': torch.distributions.Poisson(100).sample([24 * 7 * 8, 3, 3])}\n    return dataset",
            "def load_fake_od():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Create a tiny synthetic dataset for smoke testing.\\n    '\n    dataset = {'stations': ['12TH', 'EMBR', 'SFIA'], 'start_date': datetime.datetime.strptime('2000-01-01', '%Y-%m-%d'), 'counts': torch.distributions.Poisson(100).sample([24 * 7 * 8, 3, 3])}\n    return dataset"
        ]
    }
]