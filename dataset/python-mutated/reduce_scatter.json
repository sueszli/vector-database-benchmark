[
    {
        "func_name": "reduce_scatter",
        "original": "def reduce_scatter(tensor, tensor_list, op=ReduceOp.SUM, group=None, sync_op=True):\n    \"\"\"\n    Reduces, then scatters a list of tensors to all processes in a group\n\n    Args:\n        tensor (Tensor): The output tensor on each rank. The result will overwrite this tenor after communication. Support\n            float16, float32, float64, int32, int64, int8, uint8 or bool as the input data type.\n        tensor_list (List[Tensor]]): List of tensors to reduce and scatter. Every element in the list must be a Tensor whose data type\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD, optional): The reduction used. If none is given, use ReduceOp.SUM as default.\n        group (Group, optional): Communicate in which group. If none is given, use the global group as default.\n        sync_op (bool, optional): Indicate whether the communication is sync or not. If none is given, use true as default.\n\n    Returns:\n        Return a task object.\n\n    Warning:\n        This API only supports the dygraph mode.\n\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\n            >>> import paddle\n            >>> import paddle.distributed as dist\n\n            >>> dist.init_parallel_env()\n            >>> if dist.get_rank() == 0:\n            ...     data1 = paddle.to_tensor([0, 1])\n            ...     data2 = paddle.to_tensor([2, 3])\n            >>> else:\n            ...     data1 = paddle.to_tensor([4, 5])\n            ...     data2 = paddle.to_tensor([6, 7])\n            >>> dist.reduce_scatter(data1, [data1, data2])\n            >>> print(data1)\n            >>> # [4, 6] (2 GPUs, out for rank 0)\n            >>> # [8, 10] (2 GPUs, out for rank 1)\n\n    \"\"\"\n    return stream.reduce_scatter(tensor, tensor_list, op=op, group=group, sync_op=sync_op, use_calc_stream=False)",
        "mutated": [
            "def reduce_scatter(tensor, tensor_list, op=ReduceOp.SUM, group=None, sync_op=True):\n    if False:\n        i = 10\n    '\\n    Reduces, then scatters a list of tensors to all processes in a group\\n\\n    Args:\\n        tensor (Tensor): The output tensor on each rank. The result will overwrite this tenor after communication. Support\\n            float16, float32, float64, int32, int64, int8, uint8 or bool as the input data type.\\n        tensor_list (List[Tensor]]): List of tensors to reduce and scatter. Every element in the list must be a Tensor whose data type\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD, optional): The reduction used. If none is given, use ReduceOp.SUM as default.\\n        group (Group, optional): Communicate in which group. If none is given, use the global group as default.\\n        sync_op (bool, optional): Indicate whether the communication is sync or not. If none is given, use true as default.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Warning:\\n        This API only supports the dygraph mode.\\n\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> if dist.get_rank() == 0:\\n            ...     data1 = paddle.to_tensor([0, 1])\\n            ...     data2 = paddle.to_tensor([2, 3])\\n            >>> else:\\n            ...     data1 = paddle.to_tensor([4, 5])\\n            ...     data2 = paddle.to_tensor([6, 7])\\n            >>> dist.reduce_scatter(data1, [data1, data2])\\n            >>> print(data1)\\n            >>> # [4, 6] (2 GPUs, out for rank 0)\\n            >>> # [8, 10] (2 GPUs, out for rank 1)\\n\\n    '\n    return stream.reduce_scatter(tensor, tensor_list, op=op, group=group, sync_op=sync_op, use_calc_stream=False)",
            "def reduce_scatter(tensor, tensor_list, op=ReduceOp.SUM, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Reduces, then scatters a list of tensors to all processes in a group\\n\\n    Args:\\n        tensor (Tensor): The output tensor on each rank. The result will overwrite this tenor after communication. Support\\n            float16, float32, float64, int32, int64, int8, uint8 or bool as the input data type.\\n        tensor_list (List[Tensor]]): List of tensors to reduce and scatter. Every element in the list must be a Tensor whose data type\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD, optional): The reduction used. If none is given, use ReduceOp.SUM as default.\\n        group (Group, optional): Communicate in which group. If none is given, use the global group as default.\\n        sync_op (bool, optional): Indicate whether the communication is sync or not. If none is given, use true as default.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Warning:\\n        This API only supports the dygraph mode.\\n\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> if dist.get_rank() == 0:\\n            ...     data1 = paddle.to_tensor([0, 1])\\n            ...     data2 = paddle.to_tensor([2, 3])\\n            >>> else:\\n            ...     data1 = paddle.to_tensor([4, 5])\\n            ...     data2 = paddle.to_tensor([6, 7])\\n            >>> dist.reduce_scatter(data1, [data1, data2])\\n            >>> print(data1)\\n            >>> # [4, 6] (2 GPUs, out for rank 0)\\n            >>> # [8, 10] (2 GPUs, out for rank 1)\\n\\n    '\n    return stream.reduce_scatter(tensor, tensor_list, op=op, group=group, sync_op=sync_op, use_calc_stream=False)",
            "def reduce_scatter(tensor, tensor_list, op=ReduceOp.SUM, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Reduces, then scatters a list of tensors to all processes in a group\\n\\n    Args:\\n        tensor (Tensor): The output tensor on each rank. The result will overwrite this tenor after communication. Support\\n            float16, float32, float64, int32, int64, int8, uint8 or bool as the input data type.\\n        tensor_list (List[Tensor]]): List of tensors to reduce and scatter. Every element in the list must be a Tensor whose data type\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD, optional): The reduction used. If none is given, use ReduceOp.SUM as default.\\n        group (Group, optional): Communicate in which group. If none is given, use the global group as default.\\n        sync_op (bool, optional): Indicate whether the communication is sync or not. If none is given, use true as default.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Warning:\\n        This API only supports the dygraph mode.\\n\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> if dist.get_rank() == 0:\\n            ...     data1 = paddle.to_tensor([0, 1])\\n            ...     data2 = paddle.to_tensor([2, 3])\\n            >>> else:\\n            ...     data1 = paddle.to_tensor([4, 5])\\n            ...     data2 = paddle.to_tensor([6, 7])\\n            >>> dist.reduce_scatter(data1, [data1, data2])\\n            >>> print(data1)\\n            >>> # [4, 6] (2 GPUs, out for rank 0)\\n            >>> # [8, 10] (2 GPUs, out for rank 1)\\n\\n    '\n    return stream.reduce_scatter(tensor, tensor_list, op=op, group=group, sync_op=sync_op, use_calc_stream=False)",
            "def reduce_scatter(tensor, tensor_list, op=ReduceOp.SUM, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Reduces, then scatters a list of tensors to all processes in a group\\n\\n    Args:\\n        tensor (Tensor): The output tensor on each rank. The result will overwrite this tenor after communication. Support\\n            float16, float32, float64, int32, int64, int8, uint8 or bool as the input data type.\\n        tensor_list (List[Tensor]]): List of tensors to reduce and scatter. Every element in the list must be a Tensor whose data type\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD, optional): The reduction used. If none is given, use ReduceOp.SUM as default.\\n        group (Group, optional): Communicate in which group. If none is given, use the global group as default.\\n        sync_op (bool, optional): Indicate whether the communication is sync or not. If none is given, use true as default.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Warning:\\n        This API only supports the dygraph mode.\\n\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> if dist.get_rank() == 0:\\n            ...     data1 = paddle.to_tensor([0, 1])\\n            ...     data2 = paddle.to_tensor([2, 3])\\n            >>> else:\\n            ...     data1 = paddle.to_tensor([4, 5])\\n            ...     data2 = paddle.to_tensor([6, 7])\\n            >>> dist.reduce_scatter(data1, [data1, data2])\\n            >>> print(data1)\\n            >>> # [4, 6] (2 GPUs, out for rank 0)\\n            >>> # [8, 10] (2 GPUs, out for rank 1)\\n\\n    '\n    return stream.reduce_scatter(tensor, tensor_list, op=op, group=group, sync_op=sync_op, use_calc_stream=False)",
            "def reduce_scatter(tensor, tensor_list, op=ReduceOp.SUM, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Reduces, then scatters a list of tensors to all processes in a group\\n\\n    Args:\\n        tensor (Tensor): The output tensor on each rank. The result will overwrite this tenor after communication. Support\\n            float16, float32, float64, int32, int64, int8, uint8 or bool as the input data type.\\n        tensor_list (List[Tensor]]): List of tensors to reduce and scatter. Every element in the list must be a Tensor whose data type\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD, optional): The reduction used. If none is given, use ReduceOp.SUM as default.\\n        group (Group, optional): Communicate in which group. If none is given, use the global group as default.\\n        sync_op (bool, optional): Indicate whether the communication is sync or not. If none is given, use true as default.\\n\\n    Returns:\\n        Return a task object.\\n\\n    Warning:\\n        This API only supports the dygraph mode.\\n\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> if dist.get_rank() == 0:\\n            ...     data1 = paddle.to_tensor([0, 1])\\n            ...     data2 = paddle.to_tensor([2, 3])\\n            >>> else:\\n            ...     data1 = paddle.to_tensor([4, 5])\\n            ...     data2 = paddle.to_tensor([6, 7])\\n            >>> dist.reduce_scatter(data1, [data1, data2])\\n            >>> print(data1)\\n            >>> # [4, 6] (2 GPUs, out for rank 0)\\n            >>> # [8, 10] (2 GPUs, out for rank 1)\\n\\n    '\n    return stream.reduce_scatter(tensor, tensor_list, op=op, group=group, sync_op=sync_op, use_calc_stream=False)"
        ]
    },
    {
        "func_name": "_reduce_scatter_base",
        "original": "def _reduce_scatter_base(output, input, op=ReduceOp.SUM, group=None, sync_op=True):\n    \"\"\"\n    Reduces, then scatters a flattened tensor to all processes in a group.\n\n    Args:\n        output (Tensor): Output tensor. Its data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\n        input (Tensor): Input tensor that is of size output tensor size times world size. Its data type\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD): Optional. The operation used. Default: ReduceOp.SUM.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\n\n    Returns:\n        Async task handle, if sync_op is set to False.\n        None, if sync_op or if not part of the group.\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\n            >>> import paddle\n            >>> import paddle.distributed as dist\n\n            >>> dist.init_parallel_env()\n            >>> rank = dist.get_rank()\n            >>> data = paddle.arange(4) + rank\n            >>> # [0, 1, 2, 3] (2 GPUs, for rank 0)\n            >>> # [1, 2, 3, 4] (2 GPUs, for rank 1)\n            >>> output = paddle.empty(shape=[2], dtype=data.dtype)\n            >>> dist.collective._reduce_scatter_base(output, data)\n            >>> print(output)\n            >>> # [1, 3] (2 GPUs, out for rank 0)\n            >>> # [5, 7] (2 GPUs, out for rank 1)\n\n    \"\"\"\n    return _reduce_scatter_base_stream(output, input, op=op, group=group, sync_op=sync_op, use_calc_stream=False)",
        "mutated": [
            "def _reduce_scatter_base(output, input, op=ReduceOp.SUM, group=None, sync_op=True):\n    if False:\n        i = 10\n    '\\n    Reduces, then scatters a flattened tensor to all processes in a group.\\n\\n    Args:\\n        output (Tensor): Output tensor. Its data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        input (Tensor): Input tensor that is of size output tensor size times world size. Its data type\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD): Optional. The operation used. Default: ReduceOp.SUM.\\n        group (ProcessGroup, optional): The process group to work on. If None,\\n            the default process group will be used.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Async task handle, if sync_op is set to False.\\n        None, if sync_op or if not part of the group.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> rank = dist.get_rank()\\n            >>> data = paddle.arange(4) + rank\\n            >>> # [0, 1, 2, 3] (2 GPUs, for rank 0)\\n            >>> # [1, 2, 3, 4] (2 GPUs, for rank 1)\\n            >>> output = paddle.empty(shape=[2], dtype=data.dtype)\\n            >>> dist.collective._reduce_scatter_base(output, data)\\n            >>> print(output)\\n            >>> # [1, 3] (2 GPUs, out for rank 0)\\n            >>> # [5, 7] (2 GPUs, out for rank 1)\\n\\n    '\n    return _reduce_scatter_base_stream(output, input, op=op, group=group, sync_op=sync_op, use_calc_stream=False)",
            "def _reduce_scatter_base(output, input, op=ReduceOp.SUM, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Reduces, then scatters a flattened tensor to all processes in a group.\\n\\n    Args:\\n        output (Tensor): Output tensor. Its data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        input (Tensor): Input tensor that is of size output tensor size times world size. Its data type\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD): Optional. The operation used. Default: ReduceOp.SUM.\\n        group (ProcessGroup, optional): The process group to work on. If None,\\n            the default process group will be used.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Async task handle, if sync_op is set to False.\\n        None, if sync_op or if not part of the group.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> rank = dist.get_rank()\\n            >>> data = paddle.arange(4) + rank\\n            >>> # [0, 1, 2, 3] (2 GPUs, for rank 0)\\n            >>> # [1, 2, 3, 4] (2 GPUs, for rank 1)\\n            >>> output = paddle.empty(shape=[2], dtype=data.dtype)\\n            >>> dist.collective._reduce_scatter_base(output, data)\\n            >>> print(output)\\n            >>> # [1, 3] (2 GPUs, out for rank 0)\\n            >>> # [5, 7] (2 GPUs, out for rank 1)\\n\\n    '\n    return _reduce_scatter_base_stream(output, input, op=op, group=group, sync_op=sync_op, use_calc_stream=False)",
            "def _reduce_scatter_base(output, input, op=ReduceOp.SUM, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Reduces, then scatters a flattened tensor to all processes in a group.\\n\\n    Args:\\n        output (Tensor): Output tensor. Its data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        input (Tensor): Input tensor that is of size output tensor size times world size. Its data type\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD): Optional. The operation used. Default: ReduceOp.SUM.\\n        group (ProcessGroup, optional): The process group to work on. If None,\\n            the default process group will be used.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Async task handle, if sync_op is set to False.\\n        None, if sync_op or if not part of the group.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> rank = dist.get_rank()\\n            >>> data = paddle.arange(4) + rank\\n            >>> # [0, 1, 2, 3] (2 GPUs, for rank 0)\\n            >>> # [1, 2, 3, 4] (2 GPUs, for rank 1)\\n            >>> output = paddle.empty(shape=[2], dtype=data.dtype)\\n            >>> dist.collective._reduce_scatter_base(output, data)\\n            >>> print(output)\\n            >>> # [1, 3] (2 GPUs, out for rank 0)\\n            >>> # [5, 7] (2 GPUs, out for rank 1)\\n\\n    '\n    return _reduce_scatter_base_stream(output, input, op=op, group=group, sync_op=sync_op, use_calc_stream=False)",
            "def _reduce_scatter_base(output, input, op=ReduceOp.SUM, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Reduces, then scatters a flattened tensor to all processes in a group.\\n\\n    Args:\\n        output (Tensor): Output tensor. Its data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        input (Tensor): Input tensor that is of size output tensor size times world size. Its data type\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD): Optional. The operation used. Default: ReduceOp.SUM.\\n        group (ProcessGroup, optional): The process group to work on. If None,\\n            the default process group will be used.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Async task handle, if sync_op is set to False.\\n        None, if sync_op or if not part of the group.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> rank = dist.get_rank()\\n            >>> data = paddle.arange(4) + rank\\n            >>> # [0, 1, 2, 3] (2 GPUs, for rank 0)\\n            >>> # [1, 2, 3, 4] (2 GPUs, for rank 1)\\n            >>> output = paddle.empty(shape=[2], dtype=data.dtype)\\n            >>> dist.collective._reduce_scatter_base(output, data)\\n            >>> print(output)\\n            >>> # [1, 3] (2 GPUs, out for rank 0)\\n            >>> # [5, 7] (2 GPUs, out for rank 1)\\n\\n    '\n    return _reduce_scatter_base_stream(output, input, op=op, group=group, sync_op=sync_op, use_calc_stream=False)",
            "def _reduce_scatter_base(output, input, op=ReduceOp.SUM, group=None, sync_op=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Reduces, then scatters a flattened tensor to all processes in a group.\\n\\n    Args:\\n        output (Tensor): Output tensor. Its data type should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        input (Tensor): Input tensor that is of size output tensor size times world size. Its data type\\n            should be float16, float32, float64, int32, int64, int8, uint8, bool or bfloat16.\\n        op (ReduceOp.SUM|ReduceOp.MAX|ReduceOp.MIN|ReduceOp.PROD): Optional. The operation used. Default: ReduceOp.SUM.\\n        group (ProcessGroup, optional): The process group to work on. If None,\\n            the default process group will be used.\\n        sync_op (bool, optional): Whether this op is a sync op. The default value is True.\\n\\n    Returns:\\n        Async task handle, if sync_op is set to False.\\n        None, if sync_op or if not part of the group.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env: DISTRIBUTED)\\n            >>> import paddle\\n            >>> import paddle.distributed as dist\\n\\n            >>> dist.init_parallel_env()\\n            >>> rank = dist.get_rank()\\n            >>> data = paddle.arange(4) + rank\\n            >>> # [0, 1, 2, 3] (2 GPUs, for rank 0)\\n            >>> # [1, 2, 3, 4] (2 GPUs, for rank 1)\\n            >>> output = paddle.empty(shape=[2], dtype=data.dtype)\\n            >>> dist.collective._reduce_scatter_base(output, data)\\n            >>> print(output)\\n            >>> # [1, 3] (2 GPUs, out for rank 0)\\n            >>> # [5, 7] (2 GPUs, out for rank 1)\\n\\n    '\n    return _reduce_scatter_base_stream(output, input, op=op, group=group, sync_op=sync_op, use_calc_stream=False)"
        ]
    }
]