[
    {
        "func_name": "_kill_process_if_needed",
        "original": "def _kill_process_if_needed(process: subprocess.Popen, timeout_s: float=10, poll_interval_s: float=1.0):\n    \"\"\"Kills a process if it hasn't finished in `timeout_s` seconds.\n    Polls every `poll_interval_s` seconds to check if the process is still running.\"\"\"\n    kill_timeout = time.monotonic() + timeout_s\n    while process.poll() is None and time.monotonic() < kill_timeout:\n        time.sleep(poll_interval_s)\n    if process.poll() is None:\n        process.terminate()",
        "mutated": [
            "def _kill_process_if_needed(process: subprocess.Popen, timeout_s: float=10, poll_interval_s: float=1.0):\n    if False:\n        i = 10\n    \"Kills a process if it hasn't finished in `timeout_s` seconds.\\n    Polls every `poll_interval_s` seconds to check if the process is still running.\"\n    kill_timeout = time.monotonic() + timeout_s\n    while process.poll() is None and time.monotonic() < kill_timeout:\n        time.sleep(poll_interval_s)\n    if process.poll() is None:\n        process.terminate()",
            "def _kill_process_if_needed(process: subprocess.Popen, timeout_s: float=10, poll_interval_s: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Kills a process if it hasn't finished in `timeout_s` seconds.\\n    Polls every `poll_interval_s` seconds to check if the process is still running.\"\n    kill_timeout = time.monotonic() + timeout_s\n    while process.poll() is None and time.monotonic() < kill_timeout:\n        time.sleep(poll_interval_s)\n    if process.poll() is None:\n        process.terminate()",
            "def _kill_process_if_needed(process: subprocess.Popen, timeout_s: float=10, poll_interval_s: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Kills a process if it hasn't finished in `timeout_s` seconds.\\n    Polls every `poll_interval_s` seconds to check if the process is still running.\"\n    kill_timeout = time.monotonic() + timeout_s\n    while process.poll() is None and time.monotonic() < kill_timeout:\n        time.sleep(poll_interval_s)\n    if process.poll() is None:\n        process.terminate()",
            "def _kill_process_if_needed(process: subprocess.Popen, timeout_s: float=10, poll_interval_s: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Kills a process if it hasn't finished in `timeout_s` seconds.\\n    Polls every `poll_interval_s` seconds to check if the process is still running.\"\n    kill_timeout = time.monotonic() + timeout_s\n    while process.poll() is None and time.monotonic() < kill_timeout:\n        time.sleep(poll_interval_s)\n    if process.poll() is None:\n        process.terminate()",
            "def _kill_process_if_needed(process: subprocess.Popen, timeout_s: float=10, poll_interval_s: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Kills a process if it hasn't finished in `timeout_s` seconds.\\n    Polls every `poll_interval_s` seconds to check if the process is still running.\"\n    kill_timeout = time.monotonic() + timeout_s\n    while process.poll() is None and time.monotonic() < kill_timeout:\n        time.sleep(poll_interval_s)\n    if process.poll() is None:\n        process.terminate()"
        ]
    },
    {
        "func_name": "_print_message",
        "original": "def _print_message(message):\n    sep = '=' * 50\n    print(f'\\n{sep}\\n{message}\\n{sep}\\n')",
        "mutated": [
            "def _print_message(message):\n    if False:\n        i = 10\n    sep = '=' * 50\n    print(f'\\n{sep}\\n{message}\\n{sep}\\n')",
            "def _print_message(message):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    sep = '=' * 50\n    print(f'\\n{sep}\\n{message}\\n{sep}\\n')",
            "def _print_message(message):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    sep = '=' * 50\n    print(f'\\n{sep}\\n{message}\\n{sep}\\n')",
            "def _print_message(message):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    sep = '=' * 50\n    print(f'\\n{sep}\\n{message}\\n{sep}\\n')",
            "def _print_message(message):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    sep = '=' * 50\n    print(f'\\n{sep}\\n{message}\\n{sep}\\n')"
        ]
    },
    {
        "func_name": "test_experiment_restore",
        "original": "@pytest.mark.parametrize('runner_type', ['tuner', 'trainer'])\ndef test_experiment_restore(tmp_path, runner_type):\n    \"\"\"\n    This is an integration stress test for experiment restoration.\n\n\n    Test setup:\n\n    - For Tuner.restore:\n        - 8 trials, with a max of 2 running concurrently (--> 4 rounds of trials)\n        - Each iteration takes 0.5 seconds\n        - Each trial runs for 8 iterations --> 4 seconds\n        - Each round of 2 trials should take 4 seconds\n        - Without any interrupts/restoration:\n            - Minimum runtime: 4 rounds * 4 seconds / round = 16 seconds\n        - The test will stop the script with a SIGINT at a random time between\n        6-10 iterations each restore.\n\n    - For Trainer.restore:\n        - 1 trial with 4 workers\n        - Each iteration takes 0.5 seconds\n        - Runs for 32 iterations --> Minimum runtime = 16 seconds\n        - The test will stop the script with a SIGINT at a random time between\n        6-10 iterations after each restore.\n\n    Requirements:\n    - Req 1: Reasonable runtime\n        - The experiment should finish within 2 * 16 = 32 seconds.\n        - 2x is the passing threshold.\n        - 16 seconds is the minimum runtime.\n    - Req 2: Training progress persisted\n        - The experiment should progress monotonically.\n        (The training iteration shouldn't go backward at any point)\n        - Trials shouldn't start from scratch.\n    - Req 3: Searcher state saved/restored correctly\n    - Req 4: Callback state saved/restored correctly\n    \"\"\"\n    np.random.seed(2023)\n    script_path = Path(__file__).parent / _RUN_SCRIPT_FILENAME\n    exp_name = f'{runner_type}_restore_integration_test'\n    callback_dump_file = tmp_path / f'{runner_type}-callback_dump_file.json'\n    storage_path = tmp_path / 'ray_results'\n    if storage_path.exists():\n        shutil.rmtree(storage_path)\n    csv_file = str(tmp_path / 'dummy_data.csv')\n    dummy_df = pd.DataFrame({'x': np.arange(128), 'y': 2 * np.arange(128)})\n    dummy_df.to_csv(csv_file)\n    run_started_marker = tmp_path / 'run_started_marker'\n    time_per_iter_s = 0.5\n    max_concurrent = 2\n    if runner_type == 'tuner':\n        iters_per_trial = 8\n        num_trials = 8\n    elif runner_type == 'trainer':\n        iters_per_trial = 32\n        num_trials = 1\n    total_iters = iters_per_trial * num_trials\n    env = os.environ.copy()\n    env.update({'RUNNER_TYPE': runner_type, 'STORAGE_PATH': str(storage_path), 'EXP_NAME': exp_name, 'CALLBACK_DUMP_FILE': str(callback_dump_file), 'RUN_STARTED_MARKER': str(run_started_marker), 'TIME_PER_ITER_S': str(time_per_iter_s), 'ITERATIONS_PER_TRIAL': str(iters_per_trial), 'NUM_TRIALS': str(num_trials), 'MAX_CONCURRENT_TRIALS': str(max_concurrent), 'CSV_DATA_FILE': csv_file})\n    no_interrupts_runtime = 16.0\n    passing_factor = 2.5\n    passing_runtime = no_interrupts_runtime * passing_factor\n    _print_message(f'Experiment should finish with a total runtime of\\n<= {passing_runtime} seconds.')\n    return_code = None\n    total_runtime = 0\n    run_iter = 0\n    progress = 0\n    progress_history = []\n    poll_interval_s = 0.1\n    test_start_time = time.monotonic()\n    while total_runtime < passing_runtime:\n        run_started_marker.write_text('', encoding='utf-8')\n        run = subprocess.Popen([sys.executable, script_path], env=env)\n        run_iter += 1\n        _print_message(f'Started run #{run_iter} w/ PID = {run.pid}')\n        while run.poll() is None and run_started_marker.exists():\n            time.sleep(poll_interval_s)\n        if run.poll() is not None:\n            return_code = run.poll()\n            break\n        timeout_s = min(np.random.uniform(6 * time_per_iter_s, 10 * time_per_iter_s), passing_runtime - total_runtime)\n        _print_message(f'Training has started...\\nInterrupting after {timeout_s:.2f} seconds\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n        start_time = time.monotonic()\n        stopping_time = start_time + timeout_s\n        while time.monotonic() < stopping_time:\n            time.sleep(poll_interval_s)\n        total_runtime += time.monotonic() - start_time\n        return_code = run.poll()\n        if return_code is None:\n            _print_message(f'Sending SIGUSR1 to run #{run_iter} w/ PID = {run.pid}')\n            run.send_signal(signal.SIGUSR1)\n            _kill_process_if_needed(run)\n        else:\n            _print_message('Run has already terminated!')\n            break\n        results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n        iters = [result.metrics.get('training_iteration', 0) for result in results]\n        progress = sum(iters) / total_iters\n        progress_history.append(progress)\n        _print_message(f'Number of trials = {len(results)}\\n% completion = {progress} ({sum(iters)} iters / {total_iters})\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n    _print_message(f'Total number of restorations = {run_iter}\\nTotal runtime = {total_runtime:.2f}\\nReturn code = {return_code}')\n    test_end_time = time.monotonic()\n    assert progress == 1.0\n    assert total_runtime <= passing_runtime, f'Expected runtime to be <= {passing_runtime}, but ran for: {total_runtime}. This means the experiment did not finish (iterations still running). Are there any performance regressions or expensive failure recoveries??'\n    assert return_code == 0, f'The script errored with return code: {return_code}.\\nCheck the `{_RUN_SCRIPT_FILENAME}` script for any issues. '\n    assert np.all(np.diff(progress_history) >= 0), 'Expected progress to increase monotonically. Instead, got:\\n{progress_history}'\n    results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n    ids = [result.config.get('id', -1) for result in results]\n    ids = [id for id in ids if id >= 0]\n    if ids:\n        assert sorted(ids) == list(range(1, num_trials + 1)), f'Expected the searcher to assign increasing id for each trial, but got:{ids}'\n    with open(callback_dump_file, 'r') as f:\n        callback_state = json.load(f)\n    trial_iters = callback_state['trial_iters']\n    for iters in trial_iters.values():\n        assert sorted(set(iters)) == list(range(1, iters_per_trial + 1)), f'Expected data from all iterations, but got: {iters}'\n    _print_message(f'Success! Test took {test_end_time - test_start_time:.2f} seconds.')",
        "mutated": [
            "@pytest.mark.parametrize('runner_type', ['tuner', 'trainer'])\ndef test_experiment_restore(tmp_path, runner_type):\n    if False:\n        i = 10\n    \"\\n    This is an integration stress test for experiment restoration.\\n\\n\\n    Test setup:\\n\\n    - For Tuner.restore:\\n        - 8 trials, with a max of 2 running concurrently (--> 4 rounds of trials)\\n        - Each iteration takes 0.5 seconds\\n        - Each trial runs for 8 iterations --> 4 seconds\\n        - Each round of 2 trials should take 4 seconds\\n        - Without any interrupts/restoration:\\n            - Minimum runtime: 4 rounds * 4 seconds / round = 16 seconds\\n        - The test will stop the script with a SIGINT at a random time between\\n        6-10 iterations each restore.\\n\\n    - For Trainer.restore:\\n        - 1 trial with 4 workers\\n        - Each iteration takes 0.5 seconds\\n        - Runs for 32 iterations --> Minimum runtime = 16 seconds\\n        - The test will stop the script with a SIGINT at a random time between\\n        6-10 iterations after each restore.\\n\\n    Requirements:\\n    - Req 1: Reasonable runtime\\n        - The experiment should finish within 2 * 16 = 32 seconds.\\n        - 2x is the passing threshold.\\n        - 16 seconds is the minimum runtime.\\n    - Req 2: Training progress persisted\\n        - The experiment should progress monotonically.\\n        (The training iteration shouldn't go backward at any point)\\n        - Trials shouldn't start from scratch.\\n    - Req 3: Searcher state saved/restored correctly\\n    - Req 4: Callback state saved/restored correctly\\n    \"\n    np.random.seed(2023)\n    script_path = Path(__file__).parent / _RUN_SCRIPT_FILENAME\n    exp_name = f'{runner_type}_restore_integration_test'\n    callback_dump_file = tmp_path / f'{runner_type}-callback_dump_file.json'\n    storage_path = tmp_path / 'ray_results'\n    if storage_path.exists():\n        shutil.rmtree(storage_path)\n    csv_file = str(tmp_path / 'dummy_data.csv')\n    dummy_df = pd.DataFrame({'x': np.arange(128), 'y': 2 * np.arange(128)})\n    dummy_df.to_csv(csv_file)\n    run_started_marker = tmp_path / 'run_started_marker'\n    time_per_iter_s = 0.5\n    max_concurrent = 2\n    if runner_type == 'tuner':\n        iters_per_trial = 8\n        num_trials = 8\n    elif runner_type == 'trainer':\n        iters_per_trial = 32\n        num_trials = 1\n    total_iters = iters_per_trial * num_trials\n    env = os.environ.copy()\n    env.update({'RUNNER_TYPE': runner_type, 'STORAGE_PATH': str(storage_path), 'EXP_NAME': exp_name, 'CALLBACK_DUMP_FILE': str(callback_dump_file), 'RUN_STARTED_MARKER': str(run_started_marker), 'TIME_PER_ITER_S': str(time_per_iter_s), 'ITERATIONS_PER_TRIAL': str(iters_per_trial), 'NUM_TRIALS': str(num_trials), 'MAX_CONCURRENT_TRIALS': str(max_concurrent), 'CSV_DATA_FILE': csv_file})\n    no_interrupts_runtime = 16.0\n    passing_factor = 2.5\n    passing_runtime = no_interrupts_runtime * passing_factor\n    _print_message(f'Experiment should finish with a total runtime of\\n<= {passing_runtime} seconds.')\n    return_code = None\n    total_runtime = 0\n    run_iter = 0\n    progress = 0\n    progress_history = []\n    poll_interval_s = 0.1\n    test_start_time = time.monotonic()\n    while total_runtime < passing_runtime:\n        run_started_marker.write_text('', encoding='utf-8')\n        run = subprocess.Popen([sys.executable, script_path], env=env)\n        run_iter += 1\n        _print_message(f'Started run #{run_iter} w/ PID = {run.pid}')\n        while run.poll() is None and run_started_marker.exists():\n            time.sleep(poll_interval_s)\n        if run.poll() is not None:\n            return_code = run.poll()\n            break\n        timeout_s = min(np.random.uniform(6 * time_per_iter_s, 10 * time_per_iter_s), passing_runtime - total_runtime)\n        _print_message(f'Training has started...\\nInterrupting after {timeout_s:.2f} seconds\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n        start_time = time.monotonic()\n        stopping_time = start_time + timeout_s\n        while time.monotonic() < stopping_time:\n            time.sleep(poll_interval_s)\n        total_runtime += time.monotonic() - start_time\n        return_code = run.poll()\n        if return_code is None:\n            _print_message(f'Sending SIGUSR1 to run #{run_iter} w/ PID = {run.pid}')\n            run.send_signal(signal.SIGUSR1)\n            _kill_process_if_needed(run)\n        else:\n            _print_message('Run has already terminated!')\n            break\n        results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n        iters = [result.metrics.get('training_iteration', 0) for result in results]\n        progress = sum(iters) / total_iters\n        progress_history.append(progress)\n        _print_message(f'Number of trials = {len(results)}\\n% completion = {progress} ({sum(iters)} iters / {total_iters})\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n    _print_message(f'Total number of restorations = {run_iter}\\nTotal runtime = {total_runtime:.2f}\\nReturn code = {return_code}')\n    test_end_time = time.monotonic()\n    assert progress == 1.0\n    assert total_runtime <= passing_runtime, f'Expected runtime to be <= {passing_runtime}, but ran for: {total_runtime}. This means the experiment did not finish (iterations still running). Are there any performance regressions or expensive failure recoveries??'\n    assert return_code == 0, f'The script errored with return code: {return_code}.\\nCheck the `{_RUN_SCRIPT_FILENAME}` script for any issues. '\n    assert np.all(np.diff(progress_history) >= 0), 'Expected progress to increase monotonically. Instead, got:\\n{progress_history}'\n    results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n    ids = [result.config.get('id', -1) for result in results]\n    ids = [id for id in ids if id >= 0]\n    if ids:\n        assert sorted(ids) == list(range(1, num_trials + 1)), f'Expected the searcher to assign increasing id for each trial, but got:{ids}'\n    with open(callback_dump_file, 'r') as f:\n        callback_state = json.load(f)\n    trial_iters = callback_state['trial_iters']\n    for iters in trial_iters.values():\n        assert sorted(set(iters)) == list(range(1, iters_per_trial + 1)), f'Expected data from all iterations, but got: {iters}'\n    _print_message(f'Success! Test took {test_end_time - test_start_time:.2f} seconds.')",
            "@pytest.mark.parametrize('runner_type', ['tuner', 'trainer'])\ndef test_experiment_restore(tmp_path, runner_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    This is an integration stress test for experiment restoration.\\n\\n\\n    Test setup:\\n\\n    - For Tuner.restore:\\n        - 8 trials, with a max of 2 running concurrently (--> 4 rounds of trials)\\n        - Each iteration takes 0.5 seconds\\n        - Each trial runs for 8 iterations --> 4 seconds\\n        - Each round of 2 trials should take 4 seconds\\n        - Without any interrupts/restoration:\\n            - Minimum runtime: 4 rounds * 4 seconds / round = 16 seconds\\n        - The test will stop the script with a SIGINT at a random time between\\n        6-10 iterations each restore.\\n\\n    - For Trainer.restore:\\n        - 1 trial with 4 workers\\n        - Each iteration takes 0.5 seconds\\n        - Runs for 32 iterations --> Minimum runtime = 16 seconds\\n        - The test will stop the script with a SIGINT at a random time between\\n        6-10 iterations after each restore.\\n\\n    Requirements:\\n    - Req 1: Reasonable runtime\\n        - The experiment should finish within 2 * 16 = 32 seconds.\\n        - 2x is the passing threshold.\\n        - 16 seconds is the minimum runtime.\\n    - Req 2: Training progress persisted\\n        - The experiment should progress monotonically.\\n        (The training iteration shouldn't go backward at any point)\\n        - Trials shouldn't start from scratch.\\n    - Req 3: Searcher state saved/restored correctly\\n    - Req 4: Callback state saved/restored correctly\\n    \"\n    np.random.seed(2023)\n    script_path = Path(__file__).parent / _RUN_SCRIPT_FILENAME\n    exp_name = f'{runner_type}_restore_integration_test'\n    callback_dump_file = tmp_path / f'{runner_type}-callback_dump_file.json'\n    storage_path = tmp_path / 'ray_results'\n    if storage_path.exists():\n        shutil.rmtree(storage_path)\n    csv_file = str(tmp_path / 'dummy_data.csv')\n    dummy_df = pd.DataFrame({'x': np.arange(128), 'y': 2 * np.arange(128)})\n    dummy_df.to_csv(csv_file)\n    run_started_marker = tmp_path / 'run_started_marker'\n    time_per_iter_s = 0.5\n    max_concurrent = 2\n    if runner_type == 'tuner':\n        iters_per_trial = 8\n        num_trials = 8\n    elif runner_type == 'trainer':\n        iters_per_trial = 32\n        num_trials = 1\n    total_iters = iters_per_trial * num_trials\n    env = os.environ.copy()\n    env.update({'RUNNER_TYPE': runner_type, 'STORAGE_PATH': str(storage_path), 'EXP_NAME': exp_name, 'CALLBACK_DUMP_FILE': str(callback_dump_file), 'RUN_STARTED_MARKER': str(run_started_marker), 'TIME_PER_ITER_S': str(time_per_iter_s), 'ITERATIONS_PER_TRIAL': str(iters_per_trial), 'NUM_TRIALS': str(num_trials), 'MAX_CONCURRENT_TRIALS': str(max_concurrent), 'CSV_DATA_FILE': csv_file})\n    no_interrupts_runtime = 16.0\n    passing_factor = 2.5\n    passing_runtime = no_interrupts_runtime * passing_factor\n    _print_message(f'Experiment should finish with a total runtime of\\n<= {passing_runtime} seconds.')\n    return_code = None\n    total_runtime = 0\n    run_iter = 0\n    progress = 0\n    progress_history = []\n    poll_interval_s = 0.1\n    test_start_time = time.monotonic()\n    while total_runtime < passing_runtime:\n        run_started_marker.write_text('', encoding='utf-8')\n        run = subprocess.Popen([sys.executable, script_path], env=env)\n        run_iter += 1\n        _print_message(f'Started run #{run_iter} w/ PID = {run.pid}')\n        while run.poll() is None and run_started_marker.exists():\n            time.sleep(poll_interval_s)\n        if run.poll() is not None:\n            return_code = run.poll()\n            break\n        timeout_s = min(np.random.uniform(6 * time_per_iter_s, 10 * time_per_iter_s), passing_runtime - total_runtime)\n        _print_message(f'Training has started...\\nInterrupting after {timeout_s:.2f} seconds\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n        start_time = time.monotonic()\n        stopping_time = start_time + timeout_s\n        while time.monotonic() < stopping_time:\n            time.sleep(poll_interval_s)\n        total_runtime += time.monotonic() - start_time\n        return_code = run.poll()\n        if return_code is None:\n            _print_message(f'Sending SIGUSR1 to run #{run_iter} w/ PID = {run.pid}')\n            run.send_signal(signal.SIGUSR1)\n            _kill_process_if_needed(run)\n        else:\n            _print_message('Run has already terminated!')\n            break\n        results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n        iters = [result.metrics.get('training_iteration', 0) for result in results]\n        progress = sum(iters) / total_iters\n        progress_history.append(progress)\n        _print_message(f'Number of trials = {len(results)}\\n% completion = {progress} ({sum(iters)} iters / {total_iters})\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n    _print_message(f'Total number of restorations = {run_iter}\\nTotal runtime = {total_runtime:.2f}\\nReturn code = {return_code}')\n    test_end_time = time.monotonic()\n    assert progress == 1.0\n    assert total_runtime <= passing_runtime, f'Expected runtime to be <= {passing_runtime}, but ran for: {total_runtime}. This means the experiment did not finish (iterations still running). Are there any performance regressions or expensive failure recoveries??'\n    assert return_code == 0, f'The script errored with return code: {return_code}.\\nCheck the `{_RUN_SCRIPT_FILENAME}` script for any issues. '\n    assert np.all(np.diff(progress_history) >= 0), 'Expected progress to increase monotonically. Instead, got:\\n{progress_history}'\n    results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n    ids = [result.config.get('id', -1) for result in results]\n    ids = [id for id in ids if id >= 0]\n    if ids:\n        assert sorted(ids) == list(range(1, num_trials + 1)), f'Expected the searcher to assign increasing id for each trial, but got:{ids}'\n    with open(callback_dump_file, 'r') as f:\n        callback_state = json.load(f)\n    trial_iters = callback_state['trial_iters']\n    for iters in trial_iters.values():\n        assert sorted(set(iters)) == list(range(1, iters_per_trial + 1)), f'Expected data from all iterations, but got: {iters}'\n    _print_message(f'Success! Test took {test_end_time - test_start_time:.2f} seconds.')",
            "@pytest.mark.parametrize('runner_type', ['tuner', 'trainer'])\ndef test_experiment_restore(tmp_path, runner_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    This is an integration stress test for experiment restoration.\\n\\n\\n    Test setup:\\n\\n    - For Tuner.restore:\\n        - 8 trials, with a max of 2 running concurrently (--> 4 rounds of trials)\\n        - Each iteration takes 0.5 seconds\\n        - Each trial runs for 8 iterations --> 4 seconds\\n        - Each round of 2 trials should take 4 seconds\\n        - Without any interrupts/restoration:\\n            - Minimum runtime: 4 rounds * 4 seconds / round = 16 seconds\\n        - The test will stop the script with a SIGINT at a random time between\\n        6-10 iterations each restore.\\n\\n    - For Trainer.restore:\\n        - 1 trial with 4 workers\\n        - Each iteration takes 0.5 seconds\\n        - Runs for 32 iterations --> Minimum runtime = 16 seconds\\n        - The test will stop the script with a SIGINT at a random time between\\n        6-10 iterations after each restore.\\n\\n    Requirements:\\n    - Req 1: Reasonable runtime\\n        - The experiment should finish within 2 * 16 = 32 seconds.\\n        - 2x is the passing threshold.\\n        - 16 seconds is the minimum runtime.\\n    - Req 2: Training progress persisted\\n        - The experiment should progress monotonically.\\n        (The training iteration shouldn't go backward at any point)\\n        - Trials shouldn't start from scratch.\\n    - Req 3: Searcher state saved/restored correctly\\n    - Req 4: Callback state saved/restored correctly\\n    \"\n    np.random.seed(2023)\n    script_path = Path(__file__).parent / _RUN_SCRIPT_FILENAME\n    exp_name = f'{runner_type}_restore_integration_test'\n    callback_dump_file = tmp_path / f'{runner_type}-callback_dump_file.json'\n    storage_path = tmp_path / 'ray_results'\n    if storage_path.exists():\n        shutil.rmtree(storage_path)\n    csv_file = str(tmp_path / 'dummy_data.csv')\n    dummy_df = pd.DataFrame({'x': np.arange(128), 'y': 2 * np.arange(128)})\n    dummy_df.to_csv(csv_file)\n    run_started_marker = tmp_path / 'run_started_marker'\n    time_per_iter_s = 0.5\n    max_concurrent = 2\n    if runner_type == 'tuner':\n        iters_per_trial = 8\n        num_trials = 8\n    elif runner_type == 'trainer':\n        iters_per_trial = 32\n        num_trials = 1\n    total_iters = iters_per_trial * num_trials\n    env = os.environ.copy()\n    env.update({'RUNNER_TYPE': runner_type, 'STORAGE_PATH': str(storage_path), 'EXP_NAME': exp_name, 'CALLBACK_DUMP_FILE': str(callback_dump_file), 'RUN_STARTED_MARKER': str(run_started_marker), 'TIME_PER_ITER_S': str(time_per_iter_s), 'ITERATIONS_PER_TRIAL': str(iters_per_trial), 'NUM_TRIALS': str(num_trials), 'MAX_CONCURRENT_TRIALS': str(max_concurrent), 'CSV_DATA_FILE': csv_file})\n    no_interrupts_runtime = 16.0\n    passing_factor = 2.5\n    passing_runtime = no_interrupts_runtime * passing_factor\n    _print_message(f'Experiment should finish with a total runtime of\\n<= {passing_runtime} seconds.')\n    return_code = None\n    total_runtime = 0\n    run_iter = 0\n    progress = 0\n    progress_history = []\n    poll_interval_s = 0.1\n    test_start_time = time.monotonic()\n    while total_runtime < passing_runtime:\n        run_started_marker.write_text('', encoding='utf-8')\n        run = subprocess.Popen([sys.executable, script_path], env=env)\n        run_iter += 1\n        _print_message(f'Started run #{run_iter} w/ PID = {run.pid}')\n        while run.poll() is None and run_started_marker.exists():\n            time.sleep(poll_interval_s)\n        if run.poll() is not None:\n            return_code = run.poll()\n            break\n        timeout_s = min(np.random.uniform(6 * time_per_iter_s, 10 * time_per_iter_s), passing_runtime - total_runtime)\n        _print_message(f'Training has started...\\nInterrupting after {timeout_s:.2f} seconds\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n        start_time = time.monotonic()\n        stopping_time = start_time + timeout_s\n        while time.monotonic() < stopping_time:\n            time.sleep(poll_interval_s)\n        total_runtime += time.monotonic() - start_time\n        return_code = run.poll()\n        if return_code is None:\n            _print_message(f'Sending SIGUSR1 to run #{run_iter} w/ PID = {run.pid}')\n            run.send_signal(signal.SIGUSR1)\n            _kill_process_if_needed(run)\n        else:\n            _print_message('Run has already terminated!')\n            break\n        results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n        iters = [result.metrics.get('training_iteration', 0) for result in results]\n        progress = sum(iters) / total_iters\n        progress_history.append(progress)\n        _print_message(f'Number of trials = {len(results)}\\n% completion = {progress} ({sum(iters)} iters / {total_iters})\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n    _print_message(f'Total number of restorations = {run_iter}\\nTotal runtime = {total_runtime:.2f}\\nReturn code = {return_code}')\n    test_end_time = time.monotonic()\n    assert progress == 1.0\n    assert total_runtime <= passing_runtime, f'Expected runtime to be <= {passing_runtime}, but ran for: {total_runtime}. This means the experiment did not finish (iterations still running). Are there any performance regressions or expensive failure recoveries??'\n    assert return_code == 0, f'The script errored with return code: {return_code}.\\nCheck the `{_RUN_SCRIPT_FILENAME}` script for any issues. '\n    assert np.all(np.diff(progress_history) >= 0), 'Expected progress to increase monotonically. Instead, got:\\n{progress_history}'\n    results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n    ids = [result.config.get('id', -1) for result in results]\n    ids = [id for id in ids if id >= 0]\n    if ids:\n        assert sorted(ids) == list(range(1, num_trials + 1)), f'Expected the searcher to assign increasing id for each trial, but got:{ids}'\n    with open(callback_dump_file, 'r') as f:\n        callback_state = json.load(f)\n    trial_iters = callback_state['trial_iters']\n    for iters in trial_iters.values():\n        assert sorted(set(iters)) == list(range(1, iters_per_trial + 1)), f'Expected data from all iterations, but got: {iters}'\n    _print_message(f'Success! Test took {test_end_time - test_start_time:.2f} seconds.')",
            "@pytest.mark.parametrize('runner_type', ['tuner', 'trainer'])\ndef test_experiment_restore(tmp_path, runner_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    This is an integration stress test for experiment restoration.\\n\\n\\n    Test setup:\\n\\n    - For Tuner.restore:\\n        - 8 trials, with a max of 2 running concurrently (--> 4 rounds of trials)\\n        - Each iteration takes 0.5 seconds\\n        - Each trial runs for 8 iterations --> 4 seconds\\n        - Each round of 2 trials should take 4 seconds\\n        - Without any interrupts/restoration:\\n            - Minimum runtime: 4 rounds * 4 seconds / round = 16 seconds\\n        - The test will stop the script with a SIGINT at a random time between\\n        6-10 iterations each restore.\\n\\n    - For Trainer.restore:\\n        - 1 trial with 4 workers\\n        - Each iteration takes 0.5 seconds\\n        - Runs for 32 iterations --> Minimum runtime = 16 seconds\\n        - The test will stop the script with a SIGINT at a random time between\\n        6-10 iterations after each restore.\\n\\n    Requirements:\\n    - Req 1: Reasonable runtime\\n        - The experiment should finish within 2 * 16 = 32 seconds.\\n        - 2x is the passing threshold.\\n        - 16 seconds is the minimum runtime.\\n    - Req 2: Training progress persisted\\n        - The experiment should progress monotonically.\\n        (The training iteration shouldn't go backward at any point)\\n        - Trials shouldn't start from scratch.\\n    - Req 3: Searcher state saved/restored correctly\\n    - Req 4: Callback state saved/restored correctly\\n    \"\n    np.random.seed(2023)\n    script_path = Path(__file__).parent / _RUN_SCRIPT_FILENAME\n    exp_name = f'{runner_type}_restore_integration_test'\n    callback_dump_file = tmp_path / f'{runner_type}-callback_dump_file.json'\n    storage_path = tmp_path / 'ray_results'\n    if storage_path.exists():\n        shutil.rmtree(storage_path)\n    csv_file = str(tmp_path / 'dummy_data.csv')\n    dummy_df = pd.DataFrame({'x': np.arange(128), 'y': 2 * np.arange(128)})\n    dummy_df.to_csv(csv_file)\n    run_started_marker = tmp_path / 'run_started_marker'\n    time_per_iter_s = 0.5\n    max_concurrent = 2\n    if runner_type == 'tuner':\n        iters_per_trial = 8\n        num_trials = 8\n    elif runner_type == 'trainer':\n        iters_per_trial = 32\n        num_trials = 1\n    total_iters = iters_per_trial * num_trials\n    env = os.environ.copy()\n    env.update({'RUNNER_TYPE': runner_type, 'STORAGE_PATH': str(storage_path), 'EXP_NAME': exp_name, 'CALLBACK_DUMP_FILE': str(callback_dump_file), 'RUN_STARTED_MARKER': str(run_started_marker), 'TIME_PER_ITER_S': str(time_per_iter_s), 'ITERATIONS_PER_TRIAL': str(iters_per_trial), 'NUM_TRIALS': str(num_trials), 'MAX_CONCURRENT_TRIALS': str(max_concurrent), 'CSV_DATA_FILE': csv_file})\n    no_interrupts_runtime = 16.0\n    passing_factor = 2.5\n    passing_runtime = no_interrupts_runtime * passing_factor\n    _print_message(f'Experiment should finish with a total runtime of\\n<= {passing_runtime} seconds.')\n    return_code = None\n    total_runtime = 0\n    run_iter = 0\n    progress = 0\n    progress_history = []\n    poll_interval_s = 0.1\n    test_start_time = time.monotonic()\n    while total_runtime < passing_runtime:\n        run_started_marker.write_text('', encoding='utf-8')\n        run = subprocess.Popen([sys.executable, script_path], env=env)\n        run_iter += 1\n        _print_message(f'Started run #{run_iter} w/ PID = {run.pid}')\n        while run.poll() is None and run_started_marker.exists():\n            time.sleep(poll_interval_s)\n        if run.poll() is not None:\n            return_code = run.poll()\n            break\n        timeout_s = min(np.random.uniform(6 * time_per_iter_s, 10 * time_per_iter_s), passing_runtime - total_runtime)\n        _print_message(f'Training has started...\\nInterrupting after {timeout_s:.2f} seconds\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n        start_time = time.monotonic()\n        stopping_time = start_time + timeout_s\n        while time.monotonic() < stopping_time:\n            time.sleep(poll_interval_s)\n        total_runtime += time.monotonic() - start_time\n        return_code = run.poll()\n        if return_code is None:\n            _print_message(f'Sending SIGUSR1 to run #{run_iter} w/ PID = {run.pid}')\n            run.send_signal(signal.SIGUSR1)\n            _kill_process_if_needed(run)\n        else:\n            _print_message('Run has already terminated!')\n            break\n        results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n        iters = [result.metrics.get('training_iteration', 0) for result in results]\n        progress = sum(iters) / total_iters\n        progress_history.append(progress)\n        _print_message(f'Number of trials = {len(results)}\\n% completion = {progress} ({sum(iters)} iters / {total_iters})\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n    _print_message(f'Total number of restorations = {run_iter}\\nTotal runtime = {total_runtime:.2f}\\nReturn code = {return_code}')\n    test_end_time = time.monotonic()\n    assert progress == 1.0\n    assert total_runtime <= passing_runtime, f'Expected runtime to be <= {passing_runtime}, but ran for: {total_runtime}. This means the experiment did not finish (iterations still running). Are there any performance regressions or expensive failure recoveries??'\n    assert return_code == 0, f'The script errored with return code: {return_code}.\\nCheck the `{_RUN_SCRIPT_FILENAME}` script for any issues. '\n    assert np.all(np.diff(progress_history) >= 0), 'Expected progress to increase monotonically. Instead, got:\\n{progress_history}'\n    results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n    ids = [result.config.get('id', -1) for result in results]\n    ids = [id for id in ids if id >= 0]\n    if ids:\n        assert sorted(ids) == list(range(1, num_trials + 1)), f'Expected the searcher to assign increasing id for each trial, but got:{ids}'\n    with open(callback_dump_file, 'r') as f:\n        callback_state = json.load(f)\n    trial_iters = callback_state['trial_iters']\n    for iters in trial_iters.values():\n        assert sorted(set(iters)) == list(range(1, iters_per_trial + 1)), f'Expected data from all iterations, but got: {iters}'\n    _print_message(f'Success! Test took {test_end_time - test_start_time:.2f} seconds.')",
            "@pytest.mark.parametrize('runner_type', ['tuner', 'trainer'])\ndef test_experiment_restore(tmp_path, runner_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    This is an integration stress test for experiment restoration.\\n\\n\\n    Test setup:\\n\\n    - For Tuner.restore:\\n        - 8 trials, with a max of 2 running concurrently (--> 4 rounds of trials)\\n        - Each iteration takes 0.5 seconds\\n        - Each trial runs for 8 iterations --> 4 seconds\\n        - Each round of 2 trials should take 4 seconds\\n        - Without any interrupts/restoration:\\n            - Minimum runtime: 4 rounds * 4 seconds / round = 16 seconds\\n        - The test will stop the script with a SIGINT at a random time between\\n        6-10 iterations each restore.\\n\\n    - For Trainer.restore:\\n        - 1 trial with 4 workers\\n        - Each iteration takes 0.5 seconds\\n        - Runs for 32 iterations --> Minimum runtime = 16 seconds\\n        - The test will stop the script with a SIGINT at a random time between\\n        6-10 iterations after each restore.\\n\\n    Requirements:\\n    - Req 1: Reasonable runtime\\n        - The experiment should finish within 2 * 16 = 32 seconds.\\n        - 2x is the passing threshold.\\n        - 16 seconds is the minimum runtime.\\n    - Req 2: Training progress persisted\\n        - The experiment should progress monotonically.\\n        (The training iteration shouldn't go backward at any point)\\n        - Trials shouldn't start from scratch.\\n    - Req 3: Searcher state saved/restored correctly\\n    - Req 4: Callback state saved/restored correctly\\n    \"\n    np.random.seed(2023)\n    script_path = Path(__file__).parent / _RUN_SCRIPT_FILENAME\n    exp_name = f'{runner_type}_restore_integration_test'\n    callback_dump_file = tmp_path / f'{runner_type}-callback_dump_file.json'\n    storage_path = tmp_path / 'ray_results'\n    if storage_path.exists():\n        shutil.rmtree(storage_path)\n    csv_file = str(tmp_path / 'dummy_data.csv')\n    dummy_df = pd.DataFrame({'x': np.arange(128), 'y': 2 * np.arange(128)})\n    dummy_df.to_csv(csv_file)\n    run_started_marker = tmp_path / 'run_started_marker'\n    time_per_iter_s = 0.5\n    max_concurrent = 2\n    if runner_type == 'tuner':\n        iters_per_trial = 8\n        num_trials = 8\n    elif runner_type == 'trainer':\n        iters_per_trial = 32\n        num_trials = 1\n    total_iters = iters_per_trial * num_trials\n    env = os.environ.copy()\n    env.update({'RUNNER_TYPE': runner_type, 'STORAGE_PATH': str(storage_path), 'EXP_NAME': exp_name, 'CALLBACK_DUMP_FILE': str(callback_dump_file), 'RUN_STARTED_MARKER': str(run_started_marker), 'TIME_PER_ITER_S': str(time_per_iter_s), 'ITERATIONS_PER_TRIAL': str(iters_per_trial), 'NUM_TRIALS': str(num_trials), 'MAX_CONCURRENT_TRIALS': str(max_concurrent), 'CSV_DATA_FILE': csv_file})\n    no_interrupts_runtime = 16.0\n    passing_factor = 2.5\n    passing_runtime = no_interrupts_runtime * passing_factor\n    _print_message(f'Experiment should finish with a total runtime of\\n<= {passing_runtime} seconds.')\n    return_code = None\n    total_runtime = 0\n    run_iter = 0\n    progress = 0\n    progress_history = []\n    poll_interval_s = 0.1\n    test_start_time = time.monotonic()\n    while total_runtime < passing_runtime:\n        run_started_marker.write_text('', encoding='utf-8')\n        run = subprocess.Popen([sys.executable, script_path], env=env)\n        run_iter += 1\n        _print_message(f'Started run #{run_iter} w/ PID = {run.pid}')\n        while run.poll() is None and run_started_marker.exists():\n            time.sleep(poll_interval_s)\n        if run.poll() is not None:\n            return_code = run.poll()\n            break\n        timeout_s = min(np.random.uniform(6 * time_per_iter_s, 10 * time_per_iter_s), passing_runtime - total_runtime)\n        _print_message(f'Training has started...\\nInterrupting after {timeout_s:.2f} seconds\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n        start_time = time.monotonic()\n        stopping_time = start_time + timeout_s\n        while time.monotonic() < stopping_time:\n            time.sleep(poll_interval_s)\n        total_runtime += time.monotonic() - start_time\n        return_code = run.poll()\n        if return_code is None:\n            _print_message(f'Sending SIGUSR1 to run #{run_iter} w/ PID = {run.pid}')\n            run.send_signal(signal.SIGUSR1)\n            _kill_process_if_needed(run)\n        else:\n            _print_message('Run has already terminated!')\n            break\n        results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n        iters = [result.metrics.get('training_iteration', 0) for result in results]\n        progress = sum(iters) / total_iters\n        progress_history.append(progress)\n        _print_message(f'Number of trials = {len(results)}\\n% completion = {progress} ({sum(iters)} iters / {total_iters})\\nCurrently at {total_runtime:.2f}/{passing_runtime} seconds')\n    _print_message(f'Total number of restorations = {run_iter}\\nTotal runtime = {total_runtime:.2f}\\nReturn code = {return_code}')\n    test_end_time = time.monotonic()\n    assert progress == 1.0\n    assert total_runtime <= passing_runtime, f'Expected runtime to be <= {passing_runtime}, but ran for: {total_runtime}. This means the experiment did not finish (iterations still running). Are there any performance regressions or expensive failure recoveries??'\n    assert return_code == 0, f'The script errored with return code: {return_code}.\\nCheck the `{_RUN_SCRIPT_FILENAME}` script for any issues. '\n    assert np.all(np.diff(progress_history) >= 0), 'Expected progress to increase monotonically. Instead, got:\\n{progress_history}'\n    results = ResultGrid(ExperimentAnalysis(str(storage_path / exp_name)))\n    ids = [result.config.get('id', -1) for result in results]\n    ids = [id for id in ids if id >= 0]\n    if ids:\n        assert sorted(ids) == list(range(1, num_trials + 1)), f'Expected the searcher to assign increasing id for each trial, but got:{ids}'\n    with open(callback_dump_file, 'r') as f:\n        callback_state = json.load(f)\n    trial_iters = callback_state['trial_iters']\n    for iters in trial_iters.values():\n        assert sorted(set(iters)) == list(range(1, iters_per_trial + 1)), f'Expected data from all iterations, but got: {iters}'\n    _print_message(f'Success! Test took {test_end_time - test_start_time:.2f} seconds.')"
        ]
    }
]