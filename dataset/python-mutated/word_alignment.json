[
    {
        "func_name": "__init__",
        "original": "def __init__(self, config, **kwargs):\n    super().__init__(config)\n    if config.is_decoder:\n        logger.warning('If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.')\n    config.num_hidden_layers = kwargs.get('encoder_layers', 8)\n    self.bert = BertModel(config, add_pooling_layer=False)\n    self.post_init()",
        "mutated": [
            "def __init__(self, config, **kwargs):\n    if False:\n        i = 10\n    super().__init__(config)\n    if config.is_decoder:\n        logger.warning('If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.')\n    config.num_hidden_layers = kwargs.get('encoder_layers', 8)\n    self.bert = BertModel(config, add_pooling_layer=False)\n    self.post_init()",
            "def __init__(self, config, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(config)\n    if config.is_decoder:\n        logger.warning('If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.')\n    config.num_hidden_layers = kwargs.get('encoder_layers', 8)\n    self.bert = BertModel(config, add_pooling_layer=False)\n    self.post_init()",
            "def __init__(self, config, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(config)\n    if config.is_decoder:\n        logger.warning('If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.')\n    config.num_hidden_layers = kwargs.get('encoder_layers', 8)\n    self.bert = BertModel(config, add_pooling_layer=False)\n    self.post_init()",
            "def __init__(self, config, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(config)\n    if config.is_decoder:\n        logger.warning('If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.')\n    config.num_hidden_layers = kwargs.get('encoder_layers', 8)\n    self.bert = BertModel(config, add_pooling_layer=False)\n    self.post_init()",
            "def __init__(self, config, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(config)\n    if config.is_decoder:\n        logger.warning('If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.')\n    config.num_hidden_layers = kwargs.get('encoder_layers', 8)\n    self.bert = BertModel(config, add_pooling_layer=False)\n    self.post_init()"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, src_input_ids=None, src_attention_mask=None, src_b2w_map=None, tgt_input_ids=None, tgt_attention_mask=None, tgt_b2w_map=None, threshold=0.001, bpe_level=False):\n    \"\"\"\n        Args: src_input_ids:\n            Indices of source input sequence tokens in the vocabulary.\n        src_attention_mask:\n            Source mask to avoid performing attention on padding token indices.\n        src_b2w_map:\n            Word order numner of subword in source sequence.\n        tgt_input_ids:\n            Indices of target input sequence tokens in the vocabulary.\n        tgt_attention_mask:\n            Target mask to avoid performing attention on padding token indices.\n        tgt_b2w_map:\n            Word order numner of subword in target sequence.\n        threshold:\n            The threshold used to extract alignment.\n        bpe_level:\n            Return subword-level alignment or not.\n        Example:\n            {\n            'src_input_ids': LongTensor([[2478,242,24,4]]),\n            'src_attention_mask': BoolTensor([[1,1,1,1]]),\n            'src_b2w_map': LongTensor([[0,1,2,3]]),\n            'tgt_input_ids': LongTensor([[1056,356,934,263,7]]),\n            'tgt_attention_mask': BoolTensor([[1,1,1,1,1]]),\n            'tgt_b2w_map': longtensor([[0,1,1,2,3]]),\n            'threshold': 0.001,\n            'bpe_level': False,\n            }\n        Returns `modelscope.outputs.WordAlignmentOutput`\n        \"\"\"\n    with torch.no_grad():\n        src_encoder_out = self.bert(input_ids=src_input_ids, attention_mask=src_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        tgt_encoder_out = self.bert(input_ids=tgt_input_ids, attention_mask=tgt_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        atten_mask_src = (1 - ((src_input_ids != 101) & (src_input_ids != 102) & src_attention_mask)[:, None, None, :].float()) * -10000\n        atten_mask_tgt = (1 - ((tgt_input_ids != 101) & (tgt_input_ids != 102) & tgt_attention_mask)[:, None, None, :].float()) * -10000\n        src_align_out = src_encoder_out[0]\n        tgt_align_out = tgt_encoder_out[0]\n        bpe_sim = torch.bmm(src_align_out, tgt_align_out.transpose(1, 2))\n    attention_scores_src = bpe_sim.unsqueeze(1) + atten_mask_tgt\n    attention_scores_tgt = bpe_sim.unsqueeze(1) + atten_mask_src.transpose(-1, -2)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src)\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt)\n    align_matrix = (attention_probs_src > threshold) * (attention_probs_tgt > threshold)\n    align_matrix = align_matrix.squeeze(1)\n    len_src = (atten_mask_src == 0).sum(dim=-1).unsqueeze(-1)\n    len_tgt = (atten_mask_tgt == 0).sum(dim=-1).unsqueeze(-1)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src / torch.sqrt(len_src.float()))\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt / torch.sqrt(len_tgt.float()))\n    word_aligns = []\n    for (idx, (line_align, b2w_src, b2w_tgt)) in enumerate(zip(align_matrix, src_b2w_map, tgt_b2w_map)):\n        aligns = dict()\n        non_specials = torch.where(line_align)\n        for (i, j) in zip(*non_specials):\n            if not bpe_level:\n                word_pair = (src_b2w_map[idx][i - 1].item(), tgt_b2w_map[idx][j - 1].item())\n                if word_pair not in aligns:\n                    aligns[word_pair] = bpe_sim[idx][i, j].item()\n                else:\n                    aligns[word_pair] = max(aligns[word_pair], bpe_sim[idx][i, j].item())\n            else:\n                aligns[i.item() - 1, j.item() - 1] = bpe_sim[idx][i, j].item()\n        word_aligns.append(aligns)\n    return WordAlignmentOutput(predictions=word_aligns)",
        "mutated": [
            "def forward(self, src_input_ids=None, src_attention_mask=None, src_b2w_map=None, tgt_input_ids=None, tgt_attention_mask=None, tgt_b2w_map=None, threshold=0.001, bpe_level=False):\n    if False:\n        i = 10\n    \"\\n        Args: src_input_ids:\\n            Indices of source input sequence tokens in the vocabulary.\\n        src_attention_mask:\\n            Source mask to avoid performing attention on padding token indices.\\n        src_b2w_map:\\n            Word order numner of subword in source sequence.\\n        tgt_input_ids:\\n            Indices of target input sequence tokens in the vocabulary.\\n        tgt_attention_mask:\\n            Target mask to avoid performing attention on padding token indices.\\n        tgt_b2w_map:\\n            Word order numner of subword in target sequence.\\n        threshold:\\n            The threshold used to extract alignment.\\n        bpe_level:\\n            Return subword-level alignment or not.\\n        Example:\\n            {\\n            'src_input_ids': LongTensor([[2478,242,24,4]]),\\n            'src_attention_mask': BoolTensor([[1,1,1,1]]),\\n            'src_b2w_map': LongTensor([[0,1,2,3]]),\\n            'tgt_input_ids': LongTensor([[1056,356,934,263,7]]),\\n            'tgt_attention_mask': BoolTensor([[1,1,1,1,1]]),\\n            'tgt_b2w_map': longtensor([[0,1,1,2,3]]),\\n            'threshold': 0.001,\\n            'bpe_level': False,\\n            }\\n        Returns `modelscope.outputs.WordAlignmentOutput`\\n        \"\n    with torch.no_grad():\n        src_encoder_out = self.bert(input_ids=src_input_ids, attention_mask=src_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        tgt_encoder_out = self.bert(input_ids=tgt_input_ids, attention_mask=tgt_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        atten_mask_src = (1 - ((src_input_ids != 101) & (src_input_ids != 102) & src_attention_mask)[:, None, None, :].float()) * -10000\n        atten_mask_tgt = (1 - ((tgt_input_ids != 101) & (tgt_input_ids != 102) & tgt_attention_mask)[:, None, None, :].float()) * -10000\n        src_align_out = src_encoder_out[0]\n        tgt_align_out = tgt_encoder_out[0]\n        bpe_sim = torch.bmm(src_align_out, tgt_align_out.transpose(1, 2))\n    attention_scores_src = bpe_sim.unsqueeze(1) + atten_mask_tgt\n    attention_scores_tgt = bpe_sim.unsqueeze(1) + atten_mask_src.transpose(-1, -2)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src)\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt)\n    align_matrix = (attention_probs_src > threshold) * (attention_probs_tgt > threshold)\n    align_matrix = align_matrix.squeeze(1)\n    len_src = (atten_mask_src == 0).sum(dim=-1).unsqueeze(-1)\n    len_tgt = (atten_mask_tgt == 0).sum(dim=-1).unsqueeze(-1)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src / torch.sqrt(len_src.float()))\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt / torch.sqrt(len_tgt.float()))\n    word_aligns = []\n    for (idx, (line_align, b2w_src, b2w_tgt)) in enumerate(zip(align_matrix, src_b2w_map, tgt_b2w_map)):\n        aligns = dict()\n        non_specials = torch.where(line_align)\n        for (i, j) in zip(*non_specials):\n            if not bpe_level:\n                word_pair = (src_b2w_map[idx][i - 1].item(), tgt_b2w_map[idx][j - 1].item())\n                if word_pair not in aligns:\n                    aligns[word_pair] = bpe_sim[idx][i, j].item()\n                else:\n                    aligns[word_pair] = max(aligns[word_pair], bpe_sim[idx][i, j].item())\n            else:\n                aligns[i.item() - 1, j.item() - 1] = bpe_sim[idx][i, j].item()\n        word_aligns.append(aligns)\n    return WordAlignmentOutput(predictions=word_aligns)",
            "def forward(self, src_input_ids=None, src_attention_mask=None, src_b2w_map=None, tgt_input_ids=None, tgt_attention_mask=None, tgt_b2w_map=None, threshold=0.001, bpe_level=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Args: src_input_ids:\\n            Indices of source input sequence tokens in the vocabulary.\\n        src_attention_mask:\\n            Source mask to avoid performing attention on padding token indices.\\n        src_b2w_map:\\n            Word order numner of subword in source sequence.\\n        tgt_input_ids:\\n            Indices of target input sequence tokens in the vocabulary.\\n        tgt_attention_mask:\\n            Target mask to avoid performing attention on padding token indices.\\n        tgt_b2w_map:\\n            Word order numner of subword in target sequence.\\n        threshold:\\n            The threshold used to extract alignment.\\n        bpe_level:\\n            Return subword-level alignment or not.\\n        Example:\\n            {\\n            'src_input_ids': LongTensor([[2478,242,24,4]]),\\n            'src_attention_mask': BoolTensor([[1,1,1,1]]),\\n            'src_b2w_map': LongTensor([[0,1,2,3]]),\\n            'tgt_input_ids': LongTensor([[1056,356,934,263,7]]),\\n            'tgt_attention_mask': BoolTensor([[1,1,1,1,1]]),\\n            'tgt_b2w_map': longtensor([[0,1,1,2,3]]),\\n            'threshold': 0.001,\\n            'bpe_level': False,\\n            }\\n        Returns `modelscope.outputs.WordAlignmentOutput`\\n        \"\n    with torch.no_grad():\n        src_encoder_out = self.bert(input_ids=src_input_ids, attention_mask=src_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        tgt_encoder_out = self.bert(input_ids=tgt_input_ids, attention_mask=tgt_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        atten_mask_src = (1 - ((src_input_ids != 101) & (src_input_ids != 102) & src_attention_mask)[:, None, None, :].float()) * -10000\n        atten_mask_tgt = (1 - ((tgt_input_ids != 101) & (tgt_input_ids != 102) & tgt_attention_mask)[:, None, None, :].float()) * -10000\n        src_align_out = src_encoder_out[0]\n        tgt_align_out = tgt_encoder_out[0]\n        bpe_sim = torch.bmm(src_align_out, tgt_align_out.transpose(1, 2))\n    attention_scores_src = bpe_sim.unsqueeze(1) + atten_mask_tgt\n    attention_scores_tgt = bpe_sim.unsqueeze(1) + atten_mask_src.transpose(-1, -2)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src)\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt)\n    align_matrix = (attention_probs_src > threshold) * (attention_probs_tgt > threshold)\n    align_matrix = align_matrix.squeeze(1)\n    len_src = (atten_mask_src == 0).sum(dim=-1).unsqueeze(-1)\n    len_tgt = (atten_mask_tgt == 0).sum(dim=-1).unsqueeze(-1)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src / torch.sqrt(len_src.float()))\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt / torch.sqrt(len_tgt.float()))\n    word_aligns = []\n    for (idx, (line_align, b2w_src, b2w_tgt)) in enumerate(zip(align_matrix, src_b2w_map, tgt_b2w_map)):\n        aligns = dict()\n        non_specials = torch.where(line_align)\n        for (i, j) in zip(*non_specials):\n            if not bpe_level:\n                word_pair = (src_b2w_map[idx][i - 1].item(), tgt_b2w_map[idx][j - 1].item())\n                if word_pair not in aligns:\n                    aligns[word_pair] = bpe_sim[idx][i, j].item()\n                else:\n                    aligns[word_pair] = max(aligns[word_pair], bpe_sim[idx][i, j].item())\n            else:\n                aligns[i.item() - 1, j.item() - 1] = bpe_sim[idx][i, j].item()\n        word_aligns.append(aligns)\n    return WordAlignmentOutput(predictions=word_aligns)",
            "def forward(self, src_input_ids=None, src_attention_mask=None, src_b2w_map=None, tgt_input_ids=None, tgt_attention_mask=None, tgt_b2w_map=None, threshold=0.001, bpe_level=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Args: src_input_ids:\\n            Indices of source input sequence tokens in the vocabulary.\\n        src_attention_mask:\\n            Source mask to avoid performing attention on padding token indices.\\n        src_b2w_map:\\n            Word order numner of subword in source sequence.\\n        tgt_input_ids:\\n            Indices of target input sequence tokens in the vocabulary.\\n        tgt_attention_mask:\\n            Target mask to avoid performing attention on padding token indices.\\n        tgt_b2w_map:\\n            Word order numner of subword in target sequence.\\n        threshold:\\n            The threshold used to extract alignment.\\n        bpe_level:\\n            Return subword-level alignment or not.\\n        Example:\\n            {\\n            'src_input_ids': LongTensor([[2478,242,24,4]]),\\n            'src_attention_mask': BoolTensor([[1,1,1,1]]),\\n            'src_b2w_map': LongTensor([[0,1,2,3]]),\\n            'tgt_input_ids': LongTensor([[1056,356,934,263,7]]),\\n            'tgt_attention_mask': BoolTensor([[1,1,1,1,1]]),\\n            'tgt_b2w_map': longtensor([[0,1,1,2,3]]),\\n            'threshold': 0.001,\\n            'bpe_level': False,\\n            }\\n        Returns `modelscope.outputs.WordAlignmentOutput`\\n        \"\n    with torch.no_grad():\n        src_encoder_out = self.bert(input_ids=src_input_ids, attention_mask=src_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        tgt_encoder_out = self.bert(input_ids=tgt_input_ids, attention_mask=tgt_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        atten_mask_src = (1 - ((src_input_ids != 101) & (src_input_ids != 102) & src_attention_mask)[:, None, None, :].float()) * -10000\n        atten_mask_tgt = (1 - ((tgt_input_ids != 101) & (tgt_input_ids != 102) & tgt_attention_mask)[:, None, None, :].float()) * -10000\n        src_align_out = src_encoder_out[0]\n        tgt_align_out = tgt_encoder_out[0]\n        bpe_sim = torch.bmm(src_align_out, tgt_align_out.transpose(1, 2))\n    attention_scores_src = bpe_sim.unsqueeze(1) + atten_mask_tgt\n    attention_scores_tgt = bpe_sim.unsqueeze(1) + atten_mask_src.transpose(-1, -2)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src)\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt)\n    align_matrix = (attention_probs_src > threshold) * (attention_probs_tgt > threshold)\n    align_matrix = align_matrix.squeeze(1)\n    len_src = (atten_mask_src == 0).sum(dim=-1).unsqueeze(-1)\n    len_tgt = (atten_mask_tgt == 0).sum(dim=-1).unsqueeze(-1)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src / torch.sqrt(len_src.float()))\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt / torch.sqrt(len_tgt.float()))\n    word_aligns = []\n    for (idx, (line_align, b2w_src, b2w_tgt)) in enumerate(zip(align_matrix, src_b2w_map, tgt_b2w_map)):\n        aligns = dict()\n        non_specials = torch.where(line_align)\n        for (i, j) in zip(*non_specials):\n            if not bpe_level:\n                word_pair = (src_b2w_map[idx][i - 1].item(), tgt_b2w_map[idx][j - 1].item())\n                if word_pair not in aligns:\n                    aligns[word_pair] = bpe_sim[idx][i, j].item()\n                else:\n                    aligns[word_pair] = max(aligns[word_pair], bpe_sim[idx][i, j].item())\n            else:\n                aligns[i.item() - 1, j.item() - 1] = bpe_sim[idx][i, j].item()\n        word_aligns.append(aligns)\n    return WordAlignmentOutput(predictions=word_aligns)",
            "def forward(self, src_input_ids=None, src_attention_mask=None, src_b2w_map=None, tgt_input_ids=None, tgt_attention_mask=None, tgt_b2w_map=None, threshold=0.001, bpe_level=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Args: src_input_ids:\\n            Indices of source input sequence tokens in the vocabulary.\\n        src_attention_mask:\\n            Source mask to avoid performing attention on padding token indices.\\n        src_b2w_map:\\n            Word order numner of subword in source sequence.\\n        tgt_input_ids:\\n            Indices of target input sequence tokens in the vocabulary.\\n        tgt_attention_mask:\\n            Target mask to avoid performing attention on padding token indices.\\n        tgt_b2w_map:\\n            Word order numner of subword in target sequence.\\n        threshold:\\n            The threshold used to extract alignment.\\n        bpe_level:\\n            Return subword-level alignment or not.\\n        Example:\\n            {\\n            'src_input_ids': LongTensor([[2478,242,24,4]]),\\n            'src_attention_mask': BoolTensor([[1,1,1,1]]),\\n            'src_b2w_map': LongTensor([[0,1,2,3]]),\\n            'tgt_input_ids': LongTensor([[1056,356,934,263,7]]),\\n            'tgt_attention_mask': BoolTensor([[1,1,1,1,1]]),\\n            'tgt_b2w_map': longtensor([[0,1,1,2,3]]),\\n            'threshold': 0.001,\\n            'bpe_level': False,\\n            }\\n        Returns `modelscope.outputs.WordAlignmentOutput`\\n        \"\n    with torch.no_grad():\n        src_encoder_out = self.bert(input_ids=src_input_ids, attention_mask=src_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        tgt_encoder_out = self.bert(input_ids=tgt_input_ids, attention_mask=tgt_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        atten_mask_src = (1 - ((src_input_ids != 101) & (src_input_ids != 102) & src_attention_mask)[:, None, None, :].float()) * -10000\n        atten_mask_tgt = (1 - ((tgt_input_ids != 101) & (tgt_input_ids != 102) & tgt_attention_mask)[:, None, None, :].float()) * -10000\n        src_align_out = src_encoder_out[0]\n        tgt_align_out = tgt_encoder_out[0]\n        bpe_sim = torch.bmm(src_align_out, tgt_align_out.transpose(1, 2))\n    attention_scores_src = bpe_sim.unsqueeze(1) + atten_mask_tgt\n    attention_scores_tgt = bpe_sim.unsqueeze(1) + atten_mask_src.transpose(-1, -2)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src)\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt)\n    align_matrix = (attention_probs_src > threshold) * (attention_probs_tgt > threshold)\n    align_matrix = align_matrix.squeeze(1)\n    len_src = (atten_mask_src == 0).sum(dim=-1).unsqueeze(-1)\n    len_tgt = (atten_mask_tgt == 0).sum(dim=-1).unsqueeze(-1)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src / torch.sqrt(len_src.float()))\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt / torch.sqrt(len_tgt.float()))\n    word_aligns = []\n    for (idx, (line_align, b2w_src, b2w_tgt)) in enumerate(zip(align_matrix, src_b2w_map, tgt_b2w_map)):\n        aligns = dict()\n        non_specials = torch.where(line_align)\n        for (i, j) in zip(*non_specials):\n            if not bpe_level:\n                word_pair = (src_b2w_map[idx][i - 1].item(), tgt_b2w_map[idx][j - 1].item())\n                if word_pair not in aligns:\n                    aligns[word_pair] = bpe_sim[idx][i, j].item()\n                else:\n                    aligns[word_pair] = max(aligns[word_pair], bpe_sim[idx][i, j].item())\n            else:\n                aligns[i.item() - 1, j.item() - 1] = bpe_sim[idx][i, j].item()\n        word_aligns.append(aligns)\n    return WordAlignmentOutput(predictions=word_aligns)",
            "def forward(self, src_input_ids=None, src_attention_mask=None, src_b2w_map=None, tgt_input_ids=None, tgt_attention_mask=None, tgt_b2w_map=None, threshold=0.001, bpe_level=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Args: src_input_ids:\\n            Indices of source input sequence tokens in the vocabulary.\\n        src_attention_mask:\\n            Source mask to avoid performing attention on padding token indices.\\n        src_b2w_map:\\n            Word order numner of subword in source sequence.\\n        tgt_input_ids:\\n            Indices of target input sequence tokens in the vocabulary.\\n        tgt_attention_mask:\\n            Target mask to avoid performing attention on padding token indices.\\n        tgt_b2w_map:\\n            Word order numner of subword in target sequence.\\n        threshold:\\n            The threshold used to extract alignment.\\n        bpe_level:\\n            Return subword-level alignment or not.\\n        Example:\\n            {\\n            'src_input_ids': LongTensor([[2478,242,24,4]]),\\n            'src_attention_mask': BoolTensor([[1,1,1,1]]),\\n            'src_b2w_map': LongTensor([[0,1,2,3]]),\\n            'tgt_input_ids': LongTensor([[1056,356,934,263,7]]),\\n            'tgt_attention_mask': BoolTensor([[1,1,1,1,1]]),\\n            'tgt_b2w_map': longtensor([[0,1,1,2,3]]),\\n            'threshold': 0.001,\\n            'bpe_level': False,\\n            }\\n        Returns `modelscope.outputs.WordAlignmentOutput`\\n        \"\n    with torch.no_grad():\n        src_encoder_out = self.bert(input_ids=src_input_ids, attention_mask=src_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        tgt_encoder_out = self.bert(input_ids=tgt_input_ids, attention_mask=tgt_attention_mask.float(), head_mask=None, inputs_embeds=None, output_hidden_states=True)\n        atten_mask_src = (1 - ((src_input_ids != 101) & (src_input_ids != 102) & src_attention_mask)[:, None, None, :].float()) * -10000\n        atten_mask_tgt = (1 - ((tgt_input_ids != 101) & (tgt_input_ids != 102) & tgt_attention_mask)[:, None, None, :].float()) * -10000\n        src_align_out = src_encoder_out[0]\n        tgt_align_out = tgt_encoder_out[0]\n        bpe_sim = torch.bmm(src_align_out, tgt_align_out.transpose(1, 2))\n    attention_scores_src = bpe_sim.unsqueeze(1) + atten_mask_tgt\n    attention_scores_tgt = bpe_sim.unsqueeze(1) + atten_mask_src.transpose(-1, -2)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src)\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt)\n    align_matrix = (attention_probs_src > threshold) * (attention_probs_tgt > threshold)\n    align_matrix = align_matrix.squeeze(1)\n    len_src = (atten_mask_src == 0).sum(dim=-1).unsqueeze(-1)\n    len_tgt = (atten_mask_tgt == 0).sum(dim=-1).unsqueeze(-1)\n    attention_probs_src = nn.Softmax(dim=-1)(attention_scores_src / torch.sqrt(len_src.float()))\n    attention_probs_tgt = nn.Softmax(dim=-2)(attention_scores_tgt / torch.sqrt(len_tgt.float()))\n    word_aligns = []\n    for (idx, (line_align, b2w_src, b2w_tgt)) in enumerate(zip(align_matrix, src_b2w_map, tgt_b2w_map)):\n        aligns = dict()\n        non_specials = torch.where(line_align)\n        for (i, j) in zip(*non_specials):\n            if not bpe_level:\n                word_pair = (src_b2w_map[idx][i - 1].item(), tgt_b2w_map[idx][j - 1].item())\n                if word_pair not in aligns:\n                    aligns[word_pair] = bpe_sim[idx][i, j].item()\n                else:\n                    aligns[word_pair] = max(aligns[word_pair], bpe_sim[idx][i, j].item())\n            else:\n                aligns[i.item() - 1, j.item() - 1] = bpe_sim[idx][i, j].item()\n        word_aligns.append(aligns)\n    return WordAlignmentOutput(predictions=word_aligns)"
        ]
    }
]