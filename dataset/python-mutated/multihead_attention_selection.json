[
    {
        "func_name": "__init__",
        "original": "def __init__(self, embed_dim, total_num_heads, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8, layer_idx=0, attn_head_selector=None):\n    super().__init__(embed_dim, num_heads, kdim=kdim, vdim=vdim, dropout=dropout, bias=bias, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn, self_attention=self_attention, encoder_decoder_attention=encoder_decoder_attention, q_noise=q_noise, qn_block_size=qn_block_size)\n    self.layer_idx = layer_idx\n    self.attn_head_selector = attn_head_selector\n    self.total_num_heads = total_num_heads\n    self.total_embed_dim = self.head_dim * total_num_heads\n    self.k_proj = quant_noise(nn.Linear(self.kdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.v_proj = quant_noise(nn.Linear(self.vdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.q_proj = quant_noise(nn.Linear(embed_dim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    if add_bias_kv:\n        self.bias_k = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n        self.bias_v = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n    else:\n        self.bias_k = self.bias_v = None\n    self.reset_parameters()",
        "mutated": [
            "def __init__(self, embed_dim, total_num_heads, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8, layer_idx=0, attn_head_selector=None):\n    if False:\n        i = 10\n    super().__init__(embed_dim, num_heads, kdim=kdim, vdim=vdim, dropout=dropout, bias=bias, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn, self_attention=self_attention, encoder_decoder_attention=encoder_decoder_attention, q_noise=q_noise, qn_block_size=qn_block_size)\n    self.layer_idx = layer_idx\n    self.attn_head_selector = attn_head_selector\n    self.total_num_heads = total_num_heads\n    self.total_embed_dim = self.head_dim * total_num_heads\n    self.k_proj = quant_noise(nn.Linear(self.kdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.v_proj = quant_noise(nn.Linear(self.vdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.q_proj = quant_noise(nn.Linear(embed_dim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    if add_bias_kv:\n        self.bias_k = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n        self.bias_v = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n    else:\n        self.bias_k = self.bias_v = None\n    self.reset_parameters()",
            "def __init__(self, embed_dim, total_num_heads, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8, layer_idx=0, attn_head_selector=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(embed_dim, num_heads, kdim=kdim, vdim=vdim, dropout=dropout, bias=bias, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn, self_attention=self_attention, encoder_decoder_attention=encoder_decoder_attention, q_noise=q_noise, qn_block_size=qn_block_size)\n    self.layer_idx = layer_idx\n    self.attn_head_selector = attn_head_selector\n    self.total_num_heads = total_num_heads\n    self.total_embed_dim = self.head_dim * total_num_heads\n    self.k_proj = quant_noise(nn.Linear(self.kdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.v_proj = quant_noise(nn.Linear(self.vdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.q_proj = quant_noise(nn.Linear(embed_dim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    if add_bias_kv:\n        self.bias_k = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n        self.bias_v = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n    else:\n        self.bias_k = self.bias_v = None\n    self.reset_parameters()",
            "def __init__(self, embed_dim, total_num_heads, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8, layer_idx=0, attn_head_selector=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(embed_dim, num_heads, kdim=kdim, vdim=vdim, dropout=dropout, bias=bias, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn, self_attention=self_attention, encoder_decoder_attention=encoder_decoder_attention, q_noise=q_noise, qn_block_size=qn_block_size)\n    self.layer_idx = layer_idx\n    self.attn_head_selector = attn_head_selector\n    self.total_num_heads = total_num_heads\n    self.total_embed_dim = self.head_dim * total_num_heads\n    self.k_proj = quant_noise(nn.Linear(self.kdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.v_proj = quant_noise(nn.Linear(self.vdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.q_proj = quant_noise(nn.Linear(embed_dim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    if add_bias_kv:\n        self.bias_k = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n        self.bias_v = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n    else:\n        self.bias_k = self.bias_v = None\n    self.reset_parameters()",
            "def __init__(self, embed_dim, total_num_heads, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8, layer_idx=0, attn_head_selector=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(embed_dim, num_heads, kdim=kdim, vdim=vdim, dropout=dropout, bias=bias, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn, self_attention=self_attention, encoder_decoder_attention=encoder_decoder_attention, q_noise=q_noise, qn_block_size=qn_block_size)\n    self.layer_idx = layer_idx\n    self.attn_head_selector = attn_head_selector\n    self.total_num_heads = total_num_heads\n    self.total_embed_dim = self.head_dim * total_num_heads\n    self.k_proj = quant_noise(nn.Linear(self.kdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.v_proj = quant_noise(nn.Linear(self.vdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.q_proj = quant_noise(nn.Linear(embed_dim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    if add_bias_kv:\n        self.bias_k = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n        self.bias_v = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n    else:\n        self.bias_k = self.bias_v = None\n    self.reset_parameters()",
            "def __init__(self, embed_dim, total_num_heads, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8, layer_idx=0, attn_head_selector=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(embed_dim, num_heads, kdim=kdim, vdim=vdim, dropout=dropout, bias=bias, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn, self_attention=self_attention, encoder_decoder_attention=encoder_decoder_attention, q_noise=q_noise, qn_block_size=qn_block_size)\n    self.layer_idx = layer_idx\n    self.attn_head_selector = attn_head_selector\n    self.total_num_heads = total_num_heads\n    self.total_embed_dim = self.head_dim * total_num_heads\n    self.k_proj = quant_noise(nn.Linear(self.kdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.v_proj = quant_noise(nn.Linear(self.vdim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    self.q_proj = quant_noise(nn.Linear(embed_dim, self.total_embed_dim, bias=bias), q_noise, qn_block_size)\n    if add_bias_kv:\n        self.bias_k = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n        self.bias_v = Parameter(torch.Tensor(1, 1, self.total_embed_dim))\n    else:\n        self.bias_k = self.bias_v = None\n    self.reset_parameters()"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, query, key: Optional[Tensor], value: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, need_weights: bool=True, static_kv: bool=False, attn_mask: Optional[Tensor]=None, before_softmax: bool=False, need_head_weights: bool=False) -> Tuple[Tensor, Optional[Tensor]]:\n    if need_head_weights:\n        need_weights = True\n    is_tpu = query.device.type == 'xla'\n    (subset_heads, subset_weights) = self.attn_head_selector(self.layer_idx)\n    (tgt_len, bsz, embed_dim) = query.size()\n    src_len = tgt_len\n    assert list(query.size()) == [tgt_len, bsz, self.embed_dim]\n    if key is not None:\n        (src_len, key_bsz, _) = key.size()\n        if not torch.jit.is_scripting():\n            assert key_bsz == bsz\n            assert value is not None\n            assert src_len, bsz == value.shape[:2]\n    if not self.onnx_trace and (not is_tpu) and (incremental_state is None) and (not static_kv) and (not torch.jit.is_scripting()):\n        assert key is not None and value is not None\n        return multi_head_attention_forward(query, key, value, self.embed_dim, self.total_num_heads, self.num_heads, torch.empty([0]), torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)), self.bias_k, self.bias_v, self.add_zero_attn, self.dropout_module.p, self.out_proj.weight, self.out_proj.bias, self.training or self.dropout_module.apply_during_inference, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight=True, q_proj_weight=self.q_proj.weight, k_proj_weight=self.k_proj.weight, v_proj_weight=self.v_proj.weight, subset_heads=subset_heads, subset_weights=subset_weights)\n    if incremental_state is not None:\n        saved_state = self._get_input_buffer(incremental_state)\n        if saved_state is not None and 'prev_key' in saved_state:\n            if static_kv:\n                assert self.encoder_decoder_attention and (not self.self_attention)\n                key = value = None\n    else:\n        saved_state = None\n    if self.self_attention:\n        q = self.q_proj(query)\n        k = self.k_proj(query)\n        v = self.v_proj(query)\n    elif self.encoder_decoder_attention:\n        q = self.q_proj(query)\n        if key is None:\n            assert value is None\n            k = v = None\n        else:\n            k = self.k_proj(key)\n            v = self.v_proj(key)\n    else:\n        assert key is not None and value is not None\n        q = self.q_proj(query)\n        k = self.k_proj(key)\n        v = self.v_proj(value)\n    q *= self.scaling\n    if self.bias_k is not None:\n        assert self.bias_v is not None\n        k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n        v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)], dim=1)\n    q = q.contiguous().view(tgt_len, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if k is not None:\n        k = k.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if v is not None:\n        v = v.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if saved_state is not None:\n        if 'prev_key' in saved_state:\n            _prev_key = saved_state['prev_key']\n            assert _prev_key is not None\n            prev_key = _prev_key.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                k = prev_key\n            else:\n                assert k is not None\n                k = torch.cat([prev_key, k], dim=1)\n            src_len = k.size(1)\n        if 'prev_value' in saved_state:\n            _prev_value = saved_state['prev_value']\n            assert _prev_value is not None\n            prev_value = _prev_value.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                v = prev_value\n            else:\n                assert v is not None\n                v = torch.cat([prev_value, v], dim=1)\n        prev_key_padding_mask: Optional[Tensor] = None\n        if 'prev_key_padding_mask' in saved_state:\n            prev_key_padding_mask = saved_state['prev_key_padding_mask']\n        assert k is not None and v is not None\n        key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=bsz, src_len=k.size(1), static_kv=static_kv)\n        saved_state['prev_key'] = k.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_value'] = v.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_key_padding_mask'] = key_padding_mask\n        assert incremental_state is not None\n        incremental_state = self._set_input_buffer(incremental_state, saved_state)\n    assert k is not None\n    assert k.size(1) == src_len\n    if key_padding_mask is not None and key_padding_mask.dim() == 0:\n        key_padding_mask = None\n    if key_padding_mask is not None:\n        assert key_padding_mask.size(0) == bsz\n        assert key_padding_mask.size(1) == src_len\n    if self.add_zero_attn:\n        assert v is not None\n        src_len += 1\n        k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n        v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, torch.zeros(key_padding_mask.size(0), 1).type_as(key_padding_mask)], dim=1)\n    attn_weights = torch.bmm(q, k.transpose(1, 2))\n    attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n    assert list(attn_weights.size()) == [bsz * self.total_num_heads, tgt_len, src_len]\n    if attn_mask is not None:\n        attn_mask = attn_mask.unsqueeze(0)\n        if self.onnx_trace:\n            attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n        attn_weights += attn_mask\n    if key_padding_mask is not None:\n        attn_weights = attn_weights.view(bsz, self.total_num_heads, tgt_len, src_len)\n        if not is_tpu:\n            attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float('-inf'))\n        else:\n            attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))\n            attn_weights = attn_weights.transpose(0, 2)\n        attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n    if before_softmax:\n        return (attn_weights, v)\n    attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)\n    attn_weights = attn_weights_float.type_as(attn_weights)\n    attn_probs = self.dropout_module(attn_weights)\n    assert v is not None\n    if subset_heads is not None and subset_heads.numel() == 1:\n        subset_heads = subset_heads.repeat(bsz)\n        subset_weights = subset_weights.repeat(bsz)\n    if subset_heads is None:\n        attn = torch.bmm(attn_probs, v)\n    else:\n        mixed_attn = torch.bmm(attn_probs, v).contiguous().view(bsz, self.total_num_heads, tgt_len, self.head_dim)\n        attn = torch.stack([mixed_attn[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1)\n        attn = attn * subset_weights.unsqueeze(2).unsqueeze(3)\n        attn = attn.contiguous().view(bsz * self.num_heads, tgt_len, self.head_dim)\n    assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n    if self.onnx_trace and attn.size(1) == 1:\n        attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n    else:\n        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n    attn = self.out_proj(attn)\n    attn_weights: Optional[Tensor] = None\n    if need_weights:\n        if subset_heads is None:\n            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)\n        else:\n            mixed_attn_weights = attn_weights_float.view(bsz, self.total_num_heads, tgt_len, src_len)\n            attn_weights = torch.stack([mixed_attn_weights[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1).transpose(1, 0)\n        if not need_head_weights:\n            attn_weights = attn_weights.mean(dim=0)\n    return (attn, attn_weights)",
        "mutated": [
            "def forward(self, query, key: Optional[Tensor], value: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, need_weights: bool=True, static_kv: bool=False, attn_mask: Optional[Tensor]=None, before_softmax: bool=False, need_head_weights: bool=False) -> Tuple[Tensor, Optional[Tensor]]:\n    if False:\n        i = 10\n    if need_head_weights:\n        need_weights = True\n    is_tpu = query.device.type == 'xla'\n    (subset_heads, subset_weights) = self.attn_head_selector(self.layer_idx)\n    (tgt_len, bsz, embed_dim) = query.size()\n    src_len = tgt_len\n    assert list(query.size()) == [tgt_len, bsz, self.embed_dim]\n    if key is not None:\n        (src_len, key_bsz, _) = key.size()\n        if not torch.jit.is_scripting():\n            assert key_bsz == bsz\n            assert value is not None\n            assert src_len, bsz == value.shape[:2]\n    if not self.onnx_trace and (not is_tpu) and (incremental_state is None) and (not static_kv) and (not torch.jit.is_scripting()):\n        assert key is not None and value is not None\n        return multi_head_attention_forward(query, key, value, self.embed_dim, self.total_num_heads, self.num_heads, torch.empty([0]), torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)), self.bias_k, self.bias_v, self.add_zero_attn, self.dropout_module.p, self.out_proj.weight, self.out_proj.bias, self.training or self.dropout_module.apply_during_inference, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight=True, q_proj_weight=self.q_proj.weight, k_proj_weight=self.k_proj.weight, v_proj_weight=self.v_proj.weight, subset_heads=subset_heads, subset_weights=subset_weights)\n    if incremental_state is not None:\n        saved_state = self._get_input_buffer(incremental_state)\n        if saved_state is not None and 'prev_key' in saved_state:\n            if static_kv:\n                assert self.encoder_decoder_attention and (not self.self_attention)\n                key = value = None\n    else:\n        saved_state = None\n    if self.self_attention:\n        q = self.q_proj(query)\n        k = self.k_proj(query)\n        v = self.v_proj(query)\n    elif self.encoder_decoder_attention:\n        q = self.q_proj(query)\n        if key is None:\n            assert value is None\n            k = v = None\n        else:\n            k = self.k_proj(key)\n            v = self.v_proj(key)\n    else:\n        assert key is not None and value is not None\n        q = self.q_proj(query)\n        k = self.k_proj(key)\n        v = self.v_proj(value)\n    q *= self.scaling\n    if self.bias_k is not None:\n        assert self.bias_v is not None\n        k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n        v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)], dim=1)\n    q = q.contiguous().view(tgt_len, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if k is not None:\n        k = k.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if v is not None:\n        v = v.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if saved_state is not None:\n        if 'prev_key' in saved_state:\n            _prev_key = saved_state['prev_key']\n            assert _prev_key is not None\n            prev_key = _prev_key.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                k = prev_key\n            else:\n                assert k is not None\n                k = torch.cat([prev_key, k], dim=1)\n            src_len = k.size(1)\n        if 'prev_value' in saved_state:\n            _prev_value = saved_state['prev_value']\n            assert _prev_value is not None\n            prev_value = _prev_value.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                v = prev_value\n            else:\n                assert v is not None\n                v = torch.cat([prev_value, v], dim=1)\n        prev_key_padding_mask: Optional[Tensor] = None\n        if 'prev_key_padding_mask' in saved_state:\n            prev_key_padding_mask = saved_state['prev_key_padding_mask']\n        assert k is not None and v is not None\n        key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=bsz, src_len=k.size(1), static_kv=static_kv)\n        saved_state['prev_key'] = k.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_value'] = v.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_key_padding_mask'] = key_padding_mask\n        assert incremental_state is not None\n        incremental_state = self._set_input_buffer(incremental_state, saved_state)\n    assert k is not None\n    assert k.size(1) == src_len\n    if key_padding_mask is not None and key_padding_mask.dim() == 0:\n        key_padding_mask = None\n    if key_padding_mask is not None:\n        assert key_padding_mask.size(0) == bsz\n        assert key_padding_mask.size(1) == src_len\n    if self.add_zero_attn:\n        assert v is not None\n        src_len += 1\n        k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n        v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, torch.zeros(key_padding_mask.size(0), 1).type_as(key_padding_mask)], dim=1)\n    attn_weights = torch.bmm(q, k.transpose(1, 2))\n    attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n    assert list(attn_weights.size()) == [bsz * self.total_num_heads, tgt_len, src_len]\n    if attn_mask is not None:\n        attn_mask = attn_mask.unsqueeze(0)\n        if self.onnx_trace:\n            attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n        attn_weights += attn_mask\n    if key_padding_mask is not None:\n        attn_weights = attn_weights.view(bsz, self.total_num_heads, tgt_len, src_len)\n        if not is_tpu:\n            attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float('-inf'))\n        else:\n            attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))\n            attn_weights = attn_weights.transpose(0, 2)\n        attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n    if before_softmax:\n        return (attn_weights, v)\n    attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)\n    attn_weights = attn_weights_float.type_as(attn_weights)\n    attn_probs = self.dropout_module(attn_weights)\n    assert v is not None\n    if subset_heads is not None and subset_heads.numel() == 1:\n        subset_heads = subset_heads.repeat(bsz)\n        subset_weights = subset_weights.repeat(bsz)\n    if subset_heads is None:\n        attn = torch.bmm(attn_probs, v)\n    else:\n        mixed_attn = torch.bmm(attn_probs, v).contiguous().view(bsz, self.total_num_heads, tgt_len, self.head_dim)\n        attn = torch.stack([mixed_attn[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1)\n        attn = attn * subset_weights.unsqueeze(2).unsqueeze(3)\n        attn = attn.contiguous().view(bsz * self.num_heads, tgt_len, self.head_dim)\n    assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n    if self.onnx_trace and attn.size(1) == 1:\n        attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n    else:\n        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n    attn = self.out_proj(attn)\n    attn_weights: Optional[Tensor] = None\n    if need_weights:\n        if subset_heads is None:\n            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)\n        else:\n            mixed_attn_weights = attn_weights_float.view(bsz, self.total_num_heads, tgt_len, src_len)\n            attn_weights = torch.stack([mixed_attn_weights[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1).transpose(1, 0)\n        if not need_head_weights:\n            attn_weights = attn_weights.mean(dim=0)\n    return (attn, attn_weights)",
            "def forward(self, query, key: Optional[Tensor], value: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, need_weights: bool=True, static_kv: bool=False, attn_mask: Optional[Tensor]=None, before_softmax: bool=False, need_head_weights: bool=False) -> Tuple[Tensor, Optional[Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if need_head_weights:\n        need_weights = True\n    is_tpu = query.device.type == 'xla'\n    (subset_heads, subset_weights) = self.attn_head_selector(self.layer_idx)\n    (tgt_len, bsz, embed_dim) = query.size()\n    src_len = tgt_len\n    assert list(query.size()) == [tgt_len, bsz, self.embed_dim]\n    if key is not None:\n        (src_len, key_bsz, _) = key.size()\n        if not torch.jit.is_scripting():\n            assert key_bsz == bsz\n            assert value is not None\n            assert src_len, bsz == value.shape[:2]\n    if not self.onnx_trace and (not is_tpu) and (incremental_state is None) and (not static_kv) and (not torch.jit.is_scripting()):\n        assert key is not None and value is not None\n        return multi_head_attention_forward(query, key, value, self.embed_dim, self.total_num_heads, self.num_heads, torch.empty([0]), torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)), self.bias_k, self.bias_v, self.add_zero_attn, self.dropout_module.p, self.out_proj.weight, self.out_proj.bias, self.training or self.dropout_module.apply_during_inference, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight=True, q_proj_weight=self.q_proj.weight, k_proj_weight=self.k_proj.weight, v_proj_weight=self.v_proj.weight, subset_heads=subset_heads, subset_weights=subset_weights)\n    if incremental_state is not None:\n        saved_state = self._get_input_buffer(incremental_state)\n        if saved_state is not None and 'prev_key' in saved_state:\n            if static_kv:\n                assert self.encoder_decoder_attention and (not self.self_attention)\n                key = value = None\n    else:\n        saved_state = None\n    if self.self_attention:\n        q = self.q_proj(query)\n        k = self.k_proj(query)\n        v = self.v_proj(query)\n    elif self.encoder_decoder_attention:\n        q = self.q_proj(query)\n        if key is None:\n            assert value is None\n            k = v = None\n        else:\n            k = self.k_proj(key)\n            v = self.v_proj(key)\n    else:\n        assert key is not None and value is not None\n        q = self.q_proj(query)\n        k = self.k_proj(key)\n        v = self.v_proj(value)\n    q *= self.scaling\n    if self.bias_k is not None:\n        assert self.bias_v is not None\n        k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n        v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)], dim=1)\n    q = q.contiguous().view(tgt_len, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if k is not None:\n        k = k.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if v is not None:\n        v = v.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if saved_state is not None:\n        if 'prev_key' in saved_state:\n            _prev_key = saved_state['prev_key']\n            assert _prev_key is not None\n            prev_key = _prev_key.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                k = prev_key\n            else:\n                assert k is not None\n                k = torch.cat([prev_key, k], dim=1)\n            src_len = k.size(1)\n        if 'prev_value' in saved_state:\n            _prev_value = saved_state['prev_value']\n            assert _prev_value is not None\n            prev_value = _prev_value.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                v = prev_value\n            else:\n                assert v is not None\n                v = torch.cat([prev_value, v], dim=1)\n        prev_key_padding_mask: Optional[Tensor] = None\n        if 'prev_key_padding_mask' in saved_state:\n            prev_key_padding_mask = saved_state['prev_key_padding_mask']\n        assert k is not None and v is not None\n        key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=bsz, src_len=k.size(1), static_kv=static_kv)\n        saved_state['prev_key'] = k.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_value'] = v.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_key_padding_mask'] = key_padding_mask\n        assert incremental_state is not None\n        incremental_state = self._set_input_buffer(incremental_state, saved_state)\n    assert k is not None\n    assert k.size(1) == src_len\n    if key_padding_mask is not None and key_padding_mask.dim() == 0:\n        key_padding_mask = None\n    if key_padding_mask is not None:\n        assert key_padding_mask.size(0) == bsz\n        assert key_padding_mask.size(1) == src_len\n    if self.add_zero_attn:\n        assert v is not None\n        src_len += 1\n        k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n        v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, torch.zeros(key_padding_mask.size(0), 1).type_as(key_padding_mask)], dim=1)\n    attn_weights = torch.bmm(q, k.transpose(1, 2))\n    attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n    assert list(attn_weights.size()) == [bsz * self.total_num_heads, tgt_len, src_len]\n    if attn_mask is not None:\n        attn_mask = attn_mask.unsqueeze(0)\n        if self.onnx_trace:\n            attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n        attn_weights += attn_mask\n    if key_padding_mask is not None:\n        attn_weights = attn_weights.view(bsz, self.total_num_heads, tgt_len, src_len)\n        if not is_tpu:\n            attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float('-inf'))\n        else:\n            attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))\n            attn_weights = attn_weights.transpose(0, 2)\n        attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n    if before_softmax:\n        return (attn_weights, v)\n    attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)\n    attn_weights = attn_weights_float.type_as(attn_weights)\n    attn_probs = self.dropout_module(attn_weights)\n    assert v is not None\n    if subset_heads is not None and subset_heads.numel() == 1:\n        subset_heads = subset_heads.repeat(bsz)\n        subset_weights = subset_weights.repeat(bsz)\n    if subset_heads is None:\n        attn = torch.bmm(attn_probs, v)\n    else:\n        mixed_attn = torch.bmm(attn_probs, v).contiguous().view(bsz, self.total_num_heads, tgt_len, self.head_dim)\n        attn = torch.stack([mixed_attn[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1)\n        attn = attn * subset_weights.unsqueeze(2).unsqueeze(3)\n        attn = attn.contiguous().view(bsz * self.num_heads, tgt_len, self.head_dim)\n    assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n    if self.onnx_trace and attn.size(1) == 1:\n        attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n    else:\n        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n    attn = self.out_proj(attn)\n    attn_weights: Optional[Tensor] = None\n    if need_weights:\n        if subset_heads is None:\n            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)\n        else:\n            mixed_attn_weights = attn_weights_float.view(bsz, self.total_num_heads, tgt_len, src_len)\n            attn_weights = torch.stack([mixed_attn_weights[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1).transpose(1, 0)\n        if not need_head_weights:\n            attn_weights = attn_weights.mean(dim=0)\n    return (attn, attn_weights)",
            "def forward(self, query, key: Optional[Tensor], value: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, need_weights: bool=True, static_kv: bool=False, attn_mask: Optional[Tensor]=None, before_softmax: bool=False, need_head_weights: bool=False) -> Tuple[Tensor, Optional[Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if need_head_weights:\n        need_weights = True\n    is_tpu = query.device.type == 'xla'\n    (subset_heads, subset_weights) = self.attn_head_selector(self.layer_idx)\n    (tgt_len, bsz, embed_dim) = query.size()\n    src_len = tgt_len\n    assert list(query.size()) == [tgt_len, bsz, self.embed_dim]\n    if key is not None:\n        (src_len, key_bsz, _) = key.size()\n        if not torch.jit.is_scripting():\n            assert key_bsz == bsz\n            assert value is not None\n            assert src_len, bsz == value.shape[:2]\n    if not self.onnx_trace and (not is_tpu) and (incremental_state is None) and (not static_kv) and (not torch.jit.is_scripting()):\n        assert key is not None and value is not None\n        return multi_head_attention_forward(query, key, value, self.embed_dim, self.total_num_heads, self.num_heads, torch.empty([0]), torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)), self.bias_k, self.bias_v, self.add_zero_attn, self.dropout_module.p, self.out_proj.weight, self.out_proj.bias, self.training or self.dropout_module.apply_during_inference, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight=True, q_proj_weight=self.q_proj.weight, k_proj_weight=self.k_proj.weight, v_proj_weight=self.v_proj.weight, subset_heads=subset_heads, subset_weights=subset_weights)\n    if incremental_state is not None:\n        saved_state = self._get_input_buffer(incremental_state)\n        if saved_state is not None and 'prev_key' in saved_state:\n            if static_kv:\n                assert self.encoder_decoder_attention and (not self.self_attention)\n                key = value = None\n    else:\n        saved_state = None\n    if self.self_attention:\n        q = self.q_proj(query)\n        k = self.k_proj(query)\n        v = self.v_proj(query)\n    elif self.encoder_decoder_attention:\n        q = self.q_proj(query)\n        if key is None:\n            assert value is None\n            k = v = None\n        else:\n            k = self.k_proj(key)\n            v = self.v_proj(key)\n    else:\n        assert key is not None and value is not None\n        q = self.q_proj(query)\n        k = self.k_proj(key)\n        v = self.v_proj(value)\n    q *= self.scaling\n    if self.bias_k is not None:\n        assert self.bias_v is not None\n        k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n        v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)], dim=1)\n    q = q.contiguous().view(tgt_len, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if k is not None:\n        k = k.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if v is not None:\n        v = v.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if saved_state is not None:\n        if 'prev_key' in saved_state:\n            _prev_key = saved_state['prev_key']\n            assert _prev_key is not None\n            prev_key = _prev_key.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                k = prev_key\n            else:\n                assert k is not None\n                k = torch.cat([prev_key, k], dim=1)\n            src_len = k.size(1)\n        if 'prev_value' in saved_state:\n            _prev_value = saved_state['prev_value']\n            assert _prev_value is not None\n            prev_value = _prev_value.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                v = prev_value\n            else:\n                assert v is not None\n                v = torch.cat([prev_value, v], dim=1)\n        prev_key_padding_mask: Optional[Tensor] = None\n        if 'prev_key_padding_mask' in saved_state:\n            prev_key_padding_mask = saved_state['prev_key_padding_mask']\n        assert k is not None and v is not None\n        key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=bsz, src_len=k.size(1), static_kv=static_kv)\n        saved_state['prev_key'] = k.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_value'] = v.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_key_padding_mask'] = key_padding_mask\n        assert incremental_state is not None\n        incremental_state = self._set_input_buffer(incremental_state, saved_state)\n    assert k is not None\n    assert k.size(1) == src_len\n    if key_padding_mask is not None and key_padding_mask.dim() == 0:\n        key_padding_mask = None\n    if key_padding_mask is not None:\n        assert key_padding_mask.size(0) == bsz\n        assert key_padding_mask.size(1) == src_len\n    if self.add_zero_attn:\n        assert v is not None\n        src_len += 1\n        k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n        v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, torch.zeros(key_padding_mask.size(0), 1).type_as(key_padding_mask)], dim=1)\n    attn_weights = torch.bmm(q, k.transpose(1, 2))\n    attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n    assert list(attn_weights.size()) == [bsz * self.total_num_heads, tgt_len, src_len]\n    if attn_mask is not None:\n        attn_mask = attn_mask.unsqueeze(0)\n        if self.onnx_trace:\n            attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n        attn_weights += attn_mask\n    if key_padding_mask is not None:\n        attn_weights = attn_weights.view(bsz, self.total_num_heads, tgt_len, src_len)\n        if not is_tpu:\n            attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float('-inf'))\n        else:\n            attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))\n            attn_weights = attn_weights.transpose(0, 2)\n        attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n    if before_softmax:\n        return (attn_weights, v)\n    attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)\n    attn_weights = attn_weights_float.type_as(attn_weights)\n    attn_probs = self.dropout_module(attn_weights)\n    assert v is not None\n    if subset_heads is not None and subset_heads.numel() == 1:\n        subset_heads = subset_heads.repeat(bsz)\n        subset_weights = subset_weights.repeat(bsz)\n    if subset_heads is None:\n        attn = torch.bmm(attn_probs, v)\n    else:\n        mixed_attn = torch.bmm(attn_probs, v).contiguous().view(bsz, self.total_num_heads, tgt_len, self.head_dim)\n        attn = torch.stack([mixed_attn[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1)\n        attn = attn * subset_weights.unsqueeze(2).unsqueeze(3)\n        attn = attn.contiguous().view(bsz * self.num_heads, tgt_len, self.head_dim)\n    assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n    if self.onnx_trace and attn.size(1) == 1:\n        attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n    else:\n        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n    attn = self.out_proj(attn)\n    attn_weights: Optional[Tensor] = None\n    if need_weights:\n        if subset_heads is None:\n            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)\n        else:\n            mixed_attn_weights = attn_weights_float.view(bsz, self.total_num_heads, tgt_len, src_len)\n            attn_weights = torch.stack([mixed_attn_weights[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1).transpose(1, 0)\n        if not need_head_weights:\n            attn_weights = attn_weights.mean(dim=0)\n    return (attn, attn_weights)",
            "def forward(self, query, key: Optional[Tensor], value: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, need_weights: bool=True, static_kv: bool=False, attn_mask: Optional[Tensor]=None, before_softmax: bool=False, need_head_weights: bool=False) -> Tuple[Tensor, Optional[Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if need_head_weights:\n        need_weights = True\n    is_tpu = query.device.type == 'xla'\n    (subset_heads, subset_weights) = self.attn_head_selector(self.layer_idx)\n    (tgt_len, bsz, embed_dim) = query.size()\n    src_len = tgt_len\n    assert list(query.size()) == [tgt_len, bsz, self.embed_dim]\n    if key is not None:\n        (src_len, key_bsz, _) = key.size()\n        if not torch.jit.is_scripting():\n            assert key_bsz == bsz\n            assert value is not None\n            assert src_len, bsz == value.shape[:2]\n    if not self.onnx_trace and (not is_tpu) and (incremental_state is None) and (not static_kv) and (not torch.jit.is_scripting()):\n        assert key is not None and value is not None\n        return multi_head_attention_forward(query, key, value, self.embed_dim, self.total_num_heads, self.num_heads, torch.empty([0]), torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)), self.bias_k, self.bias_v, self.add_zero_attn, self.dropout_module.p, self.out_proj.weight, self.out_proj.bias, self.training or self.dropout_module.apply_during_inference, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight=True, q_proj_weight=self.q_proj.weight, k_proj_weight=self.k_proj.weight, v_proj_weight=self.v_proj.weight, subset_heads=subset_heads, subset_weights=subset_weights)\n    if incremental_state is not None:\n        saved_state = self._get_input_buffer(incremental_state)\n        if saved_state is not None and 'prev_key' in saved_state:\n            if static_kv:\n                assert self.encoder_decoder_attention and (not self.self_attention)\n                key = value = None\n    else:\n        saved_state = None\n    if self.self_attention:\n        q = self.q_proj(query)\n        k = self.k_proj(query)\n        v = self.v_proj(query)\n    elif self.encoder_decoder_attention:\n        q = self.q_proj(query)\n        if key is None:\n            assert value is None\n            k = v = None\n        else:\n            k = self.k_proj(key)\n            v = self.v_proj(key)\n    else:\n        assert key is not None and value is not None\n        q = self.q_proj(query)\n        k = self.k_proj(key)\n        v = self.v_proj(value)\n    q *= self.scaling\n    if self.bias_k is not None:\n        assert self.bias_v is not None\n        k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n        v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)], dim=1)\n    q = q.contiguous().view(tgt_len, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if k is not None:\n        k = k.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if v is not None:\n        v = v.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if saved_state is not None:\n        if 'prev_key' in saved_state:\n            _prev_key = saved_state['prev_key']\n            assert _prev_key is not None\n            prev_key = _prev_key.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                k = prev_key\n            else:\n                assert k is not None\n                k = torch.cat([prev_key, k], dim=1)\n            src_len = k.size(1)\n        if 'prev_value' in saved_state:\n            _prev_value = saved_state['prev_value']\n            assert _prev_value is not None\n            prev_value = _prev_value.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                v = prev_value\n            else:\n                assert v is not None\n                v = torch.cat([prev_value, v], dim=1)\n        prev_key_padding_mask: Optional[Tensor] = None\n        if 'prev_key_padding_mask' in saved_state:\n            prev_key_padding_mask = saved_state['prev_key_padding_mask']\n        assert k is not None and v is not None\n        key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=bsz, src_len=k.size(1), static_kv=static_kv)\n        saved_state['prev_key'] = k.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_value'] = v.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_key_padding_mask'] = key_padding_mask\n        assert incremental_state is not None\n        incremental_state = self._set_input_buffer(incremental_state, saved_state)\n    assert k is not None\n    assert k.size(1) == src_len\n    if key_padding_mask is not None and key_padding_mask.dim() == 0:\n        key_padding_mask = None\n    if key_padding_mask is not None:\n        assert key_padding_mask.size(0) == bsz\n        assert key_padding_mask.size(1) == src_len\n    if self.add_zero_attn:\n        assert v is not None\n        src_len += 1\n        k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n        v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, torch.zeros(key_padding_mask.size(0), 1).type_as(key_padding_mask)], dim=1)\n    attn_weights = torch.bmm(q, k.transpose(1, 2))\n    attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n    assert list(attn_weights.size()) == [bsz * self.total_num_heads, tgt_len, src_len]\n    if attn_mask is not None:\n        attn_mask = attn_mask.unsqueeze(0)\n        if self.onnx_trace:\n            attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n        attn_weights += attn_mask\n    if key_padding_mask is not None:\n        attn_weights = attn_weights.view(bsz, self.total_num_heads, tgt_len, src_len)\n        if not is_tpu:\n            attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float('-inf'))\n        else:\n            attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))\n            attn_weights = attn_weights.transpose(0, 2)\n        attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n    if before_softmax:\n        return (attn_weights, v)\n    attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)\n    attn_weights = attn_weights_float.type_as(attn_weights)\n    attn_probs = self.dropout_module(attn_weights)\n    assert v is not None\n    if subset_heads is not None and subset_heads.numel() == 1:\n        subset_heads = subset_heads.repeat(bsz)\n        subset_weights = subset_weights.repeat(bsz)\n    if subset_heads is None:\n        attn = torch.bmm(attn_probs, v)\n    else:\n        mixed_attn = torch.bmm(attn_probs, v).contiguous().view(bsz, self.total_num_heads, tgt_len, self.head_dim)\n        attn = torch.stack([mixed_attn[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1)\n        attn = attn * subset_weights.unsqueeze(2).unsqueeze(3)\n        attn = attn.contiguous().view(bsz * self.num_heads, tgt_len, self.head_dim)\n    assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n    if self.onnx_trace and attn.size(1) == 1:\n        attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n    else:\n        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n    attn = self.out_proj(attn)\n    attn_weights: Optional[Tensor] = None\n    if need_weights:\n        if subset_heads is None:\n            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)\n        else:\n            mixed_attn_weights = attn_weights_float.view(bsz, self.total_num_heads, tgt_len, src_len)\n            attn_weights = torch.stack([mixed_attn_weights[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1).transpose(1, 0)\n        if not need_head_weights:\n            attn_weights = attn_weights.mean(dim=0)\n    return (attn, attn_weights)",
            "def forward(self, query, key: Optional[Tensor], value: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, need_weights: bool=True, static_kv: bool=False, attn_mask: Optional[Tensor]=None, before_softmax: bool=False, need_head_weights: bool=False) -> Tuple[Tensor, Optional[Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if need_head_weights:\n        need_weights = True\n    is_tpu = query.device.type == 'xla'\n    (subset_heads, subset_weights) = self.attn_head_selector(self.layer_idx)\n    (tgt_len, bsz, embed_dim) = query.size()\n    src_len = tgt_len\n    assert list(query.size()) == [tgt_len, bsz, self.embed_dim]\n    if key is not None:\n        (src_len, key_bsz, _) = key.size()\n        if not torch.jit.is_scripting():\n            assert key_bsz == bsz\n            assert value is not None\n            assert src_len, bsz == value.shape[:2]\n    if not self.onnx_trace and (not is_tpu) and (incremental_state is None) and (not static_kv) and (not torch.jit.is_scripting()):\n        assert key is not None and value is not None\n        return multi_head_attention_forward(query, key, value, self.embed_dim, self.total_num_heads, self.num_heads, torch.empty([0]), torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)), self.bias_k, self.bias_v, self.add_zero_attn, self.dropout_module.p, self.out_proj.weight, self.out_proj.bias, self.training or self.dropout_module.apply_during_inference, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight=True, q_proj_weight=self.q_proj.weight, k_proj_weight=self.k_proj.weight, v_proj_weight=self.v_proj.weight, subset_heads=subset_heads, subset_weights=subset_weights)\n    if incremental_state is not None:\n        saved_state = self._get_input_buffer(incremental_state)\n        if saved_state is not None and 'prev_key' in saved_state:\n            if static_kv:\n                assert self.encoder_decoder_attention and (not self.self_attention)\n                key = value = None\n    else:\n        saved_state = None\n    if self.self_attention:\n        q = self.q_proj(query)\n        k = self.k_proj(query)\n        v = self.v_proj(query)\n    elif self.encoder_decoder_attention:\n        q = self.q_proj(query)\n        if key is None:\n            assert value is None\n            k = v = None\n        else:\n            k = self.k_proj(key)\n            v = self.v_proj(key)\n    else:\n        assert key is not None and value is not None\n        q = self.q_proj(query)\n        k = self.k_proj(key)\n        v = self.v_proj(value)\n    q *= self.scaling\n    if self.bias_k is not None:\n        assert self.bias_v is not None\n        k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n        v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)], dim=1)\n    q = q.contiguous().view(tgt_len, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if k is not None:\n        k = k.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if v is not None:\n        v = v.contiguous().view(-1, bsz * self.total_num_heads, self.head_dim).transpose(0, 1)\n    if saved_state is not None:\n        if 'prev_key' in saved_state:\n            _prev_key = saved_state['prev_key']\n            assert _prev_key is not None\n            prev_key = _prev_key.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                k = prev_key\n            else:\n                assert k is not None\n                k = torch.cat([prev_key, k], dim=1)\n            src_len = k.size(1)\n        if 'prev_value' in saved_state:\n            _prev_value = saved_state['prev_value']\n            assert _prev_value is not None\n            prev_value = _prev_value.view(bsz * self.total_num_heads, -1, self.head_dim)\n            if static_kv:\n                v = prev_value\n            else:\n                assert v is not None\n                v = torch.cat([prev_value, v], dim=1)\n        prev_key_padding_mask: Optional[Tensor] = None\n        if 'prev_key_padding_mask' in saved_state:\n            prev_key_padding_mask = saved_state['prev_key_padding_mask']\n        assert k is not None and v is not None\n        key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=bsz, src_len=k.size(1), static_kv=static_kv)\n        saved_state['prev_key'] = k.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_value'] = v.view(bsz, self.total_num_heads, -1, self.head_dim)\n        saved_state['prev_key_padding_mask'] = key_padding_mask\n        assert incremental_state is not None\n        incremental_state = self._set_input_buffer(incremental_state, saved_state)\n    assert k is not None\n    assert k.size(1) == src_len\n    if key_padding_mask is not None and key_padding_mask.dim() == 0:\n        key_padding_mask = None\n    if key_padding_mask is not None:\n        assert key_padding_mask.size(0) == bsz\n        assert key_padding_mask.size(1) == src_len\n    if self.add_zero_attn:\n        assert v is not None\n        src_len += 1\n        k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n        v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat([key_padding_mask, torch.zeros(key_padding_mask.size(0), 1).type_as(key_padding_mask)], dim=1)\n    attn_weights = torch.bmm(q, k.transpose(1, 2))\n    attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n    assert list(attn_weights.size()) == [bsz * self.total_num_heads, tgt_len, src_len]\n    if attn_mask is not None:\n        attn_mask = attn_mask.unsqueeze(0)\n        if self.onnx_trace:\n            attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n        attn_weights += attn_mask\n    if key_padding_mask is not None:\n        attn_weights = attn_weights.view(bsz, self.total_num_heads, tgt_len, src_len)\n        if not is_tpu:\n            attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float('-inf'))\n        else:\n            attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))\n            attn_weights = attn_weights.transpose(0, 2)\n        attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n    if before_softmax:\n        return (attn_weights, v)\n    attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)\n    attn_weights = attn_weights_float.type_as(attn_weights)\n    attn_probs = self.dropout_module(attn_weights)\n    assert v is not None\n    if subset_heads is not None and subset_heads.numel() == 1:\n        subset_heads = subset_heads.repeat(bsz)\n        subset_weights = subset_weights.repeat(bsz)\n    if subset_heads is None:\n        attn = torch.bmm(attn_probs, v)\n    else:\n        mixed_attn = torch.bmm(attn_probs, v).contiguous().view(bsz, self.total_num_heads, tgt_len, self.head_dim)\n        attn = torch.stack([mixed_attn[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1)\n        attn = attn * subset_weights.unsqueeze(2).unsqueeze(3)\n        attn = attn.contiguous().view(bsz * self.num_heads, tgt_len, self.head_dim)\n    assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n    if self.onnx_trace and attn.size(1) == 1:\n        attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n    else:\n        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n    attn = self.out_proj(attn)\n    attn_weights: Optional[Tensor] = None\n    if need_weights:\n        if subset_heads is None:\n            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)\n        else:\n            mixed_attn_weights = attn_weights_float.view(bsz, self.total_num_heads, tgt_len, src_len)\n            attn_weights = torch.stack([mixed_attn_weights[torch.arange(bsz), subset_heads[:, col], :, :] for col in range(subset_heads.size(1))], dim=1).transpose(1, 0)\n        if not need_head_weights:\n            attn_weights = attn_weights.mean(dim=0)\n    return (attn, attn_weights)"
        ]
    }
]