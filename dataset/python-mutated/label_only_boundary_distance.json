[
    {
        "func_name": "__init__",
        "original": "def __init__(self, estimator: 'CLASSIFIER_TYPE', distance_threshold_tau: Optional[float]=None):\n    \"\"\"\n        Create a `LabelOnlyDecisionBoundary` instance for Label-Only Inference Attack based on Decision Boundary.\n\n        :param estimator: A trained classification estimator.\n        :param distance_threshold_tau: Threshold distance for decision boundary. Samples with boundary distances larger\n                                       than threshold are considered members of the training dataset.\n        \"\"\"\n    super().__init__(estimator=estimator)\n    self.distance_threshold_tau = distance_threshold_tau\n    self.threshold_bins: list = []\n    self._check_params()",
        "mutated": [
            "def __init__(self, estimator: 'CLASSIFIER_TYPE', distance_threshold_tau: Optional[float]=None):\n    if False:\n        i = 10\n    '\\n        Create a `LabelOnlyDecisionBoundary` instance for Label-Only Inference Attack based on Decision Boundary.\\n\\n        :param estimator: A trained classification estimator.\\n        :param distance_threshold_tau: Threshold distance for decision boundary. Samples with boundary distances larger\\n                                       than threshold are considered members of the training dataset.\\n        '\n    super().__init__(estimator=estimator)\n    self.distance_threshold_tau = distance_threshold_tau\n    self.threshold_bins: list = []\n    self._check_params()",
            "def __init__(self, estimator: 'CLASSIFIER_TYPE', distance_threshold_tau: Optional[float]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create a `LabelOnlyDecisionBoundary` instance for Label-Only Inference Attack based on Decision Boundary.\\n\\n        :param estimator: A trained classification estimator.\\n        :param distance_threshold_tau: Threshold distance for decision boundary. Samples with boundary distances larger\\n                                       than threshold are considered members of the training dataset.\\n        '\n    super().__init__(estimator=estimator)\n    self.distance_threshold_tau = distance_threshold_tau\n    self.threshold_bins: list = []\n    self._check_params()",
            "def __init__(self, estimator: 'CLASSIFIER_TYPE', distance_threshold_tau: Optional[float]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create a `LabelOnlyDecisionBoundary` instance for Label-Only Inference Attack based on Decision Boundary.\\n\\n        :param estimator: A trained classification estimator.\\n        :param distance_threshold_tau: Threshold distance for decision boundary. Samples with boundary distances larger\\n                                       than threshold are considered members of the training dataset.\\n        '\n    super().__init__(estimator=estimator)\n    self.distance_threshold_tau = distance_threshold_tau\n    self.threshold_bins: list = []\n    self._check_params()",
            "def __init__(self, estimator: 'CLASSIFIER_TYPE', distance_threshold_tau: Optional[float]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create a `LabelOnlyDecisionBoundary` instance for Label-Only Inference Attack based on Decision Boundary.\\n\\n        :param estimator: A trained classification estimator.\\n        :param distance_threshold_tau: Threshold distance for decision boundary. Samples with boundary distances larger\\n                                       than threshold are considered members of the training dataset.\\n        '\n    super().__init__(estimator=estimator)\n    self.distance_threshold_tau = distance_threshold_tau\n    self.threshold_bins: list = []\n    self._check_params()",
            "def __init__(self, estimator: 'CLASSIFIER_TYPE', distance_threshold_tau: Optional[float]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create a `LabelOnlyDecisionBoundary` instance for Label-Only Inference Attack based on Decision Boundary.\\n\\n        :param estimator: A trained classification estimator.\\n        :param distance_threshold_tau: Threshold distance for decision boundary. Samples with boundary distances larger\\n                                       than threshold are considered members of the training dataset.\\n        '\n    super().__init__(estimator=estimator)\n    self.distance_threshold_tau = distance_threshold_tau\n    self.threshold_bins: list = []\n    self._check_params()"
        ]
    },
    {
        "func_name": "infer",
        "original": "def infer(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    \"\"\"\n        Infer membership of input `x` in estimator's training data.\n\n        :param x: Input data.\n        :param y: True labels for `x`.\n        :param probabilities: a boolean indicating whether to return the predicted probabilities per class, or just\n                              the predicted class\n\n        :Keyword Arguments for HopSkipJump:\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\n            * *max_iter*: Maximum number of iterations.\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\n            * *init_eval*: Initial number of evaluations for estimating gradient.\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\n            * *verbose*: Show progress bars.\n\n        :return: An array holding the inferred membership status, 1 indicates a member and 0 indicates non-member,\n                 or class probabilities.\n        \"\"\"\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if y is None:\n        raise ValueError('Argument `y` is None, but this attack requires true labels `y` to be provided.')\n    if self.distance_threshold_tau is None:\n        raise ValueError('No value for distance threshold `distance_threshold_tau` provided. Please set`distance_threshold_tau` or run method `calibrate_distance_threshold` on known training and testdataset.')\n    if 'probabilities' in kwargs:\n        probabilities = kwargs.get('probabilities')\n        del kwargs['probabilities']\n    else:\n        probabilities = False\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_adv = hsj.generate(x=x, y=y)\n    distance = np.linalg.norm((x_adv - x).reshape((x.shape[0], -1)), ord=2, axis=1)\n    y_pred = self.estimator.predict(x=x)\n    distance[np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)] = 0\n    predicted_class = np.where(distance > self.distance_threshold_tau, 1, 0)\n    if probabilities:\n        prob_1 = np.zeros_like(distance)\n        if self.threshold_bins:\n            for t_bin in self.threshold_bins:\n                prob_1[distance > t_bin[0]] = t_bin[1]\n        else:\n            dist_threshold = distance - self.distance_threshold_tau\n            prob_1 = 1 / (1 + np.exp(-dist_threshold))\n        prob_0 = np.ones_like(prob_1) - prob_1\n        return np.stack((prob_0, prob_1), axis=1)\n    return predicted_class",
        "mutated": [
            "def infer(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Infer membership of input `x` in estimator\\'s training data.\\n\\n        :param x: Input data.\\n        :param y: True labels for `x`.\\n        :param probabilities: a boolean indicating whether to return the predicted probabilities per class, or just\\n                              the predicted class\\n\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n\\n        :return: An array holding the inferred membership status, 1 indicates a member and 0 indicates non-member,\\n                 or class probabilities.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if y is None:\n        raise ValueError('Argument `y` is None, but this attack requires true labels `y` to be provided.')\n    if self.distance_threshold_tau is None:\n        raise ValueError('No value for distance threshold `distance_threshold_tau` provided. Please set`distance_threshold_tau` or run method `calibrate_distance_threshold` on known training and testdataset.')\n    if 'probabilities' in kwargs:\n        probabilities = kwargs.get('probabilities')\n        del kwargs['probabilities']\n    else:\n        probabilities = False\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_adv = hsj.generate(x=x, y=y)\n    distance = np.linalg.norm((x_adv - x).reshape((x.shape[0], -1)), ord=2, axis=1)\n    y_pred = self.estimator.predict(x=x)\n    distance[np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)] = 0\n    predicted_class = np.where(distance > self.distance_threshold_tau, 1, 0)\n    if probabilities:\n        prob_1 = np.zeros_like(distance)\n        if self.threshold_bins:\n            for t_bin in self.threshold_bins:\n                prob_1[distance > t_bin[0]] = t_bin[1]\n        else:\n            dist_threshold = distance - self.distance_threshold_tau\n            prob_1 = 1 / (1 + np.exp(-dist_threshold))\n        prob_0 = np.ones_like(prob_1) - prob_1\n        return np.stack((prob_0, prob_1), axis=1)\n    return predicted_class",
            "def infer(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Infer membership of input `x` in estimator\\'s training data.\\n\\n        :param x: Input data.\\n        :param y: True labels for `x`.\\n        :param probabilities: a boolean indicating whether to return the predicted probabilities per class, or just\\n                              the predicted class\\n\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n\\n        :return: An array holding the inferred membership status, 1 indicates a member and 0 indicates non-member,\\n                 or class probabilities.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if y is None:\n        raise ValueError('Argument `y` is None, but this attack requires true labels `y` to be provided.')\n    if self.distance_threshold_tau is None:\n        raise ValueError('No value for distance threshold `distance_threshold_tau` provided. Please set`distance_threshold_tau` or run method `calibrate_distance_threshold` on known training and testdataset.')\n    if 'probabilities' in kwargs:\n        probabilities = kwargs.get('probabilities')\n        del kwargs['probabilities']\n    else:\n        probabilities = False\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_adv = hsj.generate(x=x, y=y)\n    distance = np.linalg.norm((x_adv - x).reshape((x.shape[0], -1)), ord=2, axis=1)\n    y_pred = self.estimator.predict(x=x)\n    distance[np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)] = 0\n    predicted_class = np.where(distance > self.distance_threshold_tau, 1, 0)\n    if probabilities:\n        prob_1 = np.zeros_like(distance)\n        if self.threshold_bins:\n            for t_bin in self.threshold_bins:\n                prob_1[distance > t_bin[0]] = t_bin[1]\n        else:\n            dist_threshold = distance - self.distance_threshold_tau\n            prob_1 = 1 / (1 + np.exp(-dist_threshold))\n        prob_0 = np.ones_like(prob_1) - prob_1\n        return np.stack((prob_0, prob_1), axis=1)\n    return predicted_class",
            "def infer(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Infer membership of input `x` in estimator\\'s training data.\\n\\n        :param x: Input data.\\n        :param y: True labels for `x`.\\n        :param probabilities: a boolean indicating whether to return the predicted probabilities per class, or just\\n                              the predicted class\\n\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n\\n        :return: An array holding the inferred membership status, 1 indicates a member and 0 indicates non-member,\\n                 or class probabilities.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if y is None:\n        raise ValueError('Argument `y` is None, but this attack requires true labels `y` to be provided.')\n    if self.distance_threshold_tau is None:\n        raise ValueError('No value for distance threshold `distance_threshold_tau` provided. Please set`distance_threshold_tau` or run method `calibrate_distance_threshold` on known training and testdataset.')\n    if 'probabilities' in kwargs:\n        probabilities = kwargs.get('probabilities')\n        del kwargs['probabilities']\n    else:\n        probabilities = False\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_adv = hsj.generate(x=x, y=y)\n    distance = np.linalg.norm((x_adv - x).reshape((x.shape[0], -1)), ord=2, axis=1)\n    y_pred = self.estimator.predict(x=x)\n    distance[np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)] = 0\n    predicted_class = np.where(distance > self.distance_threshold_tau, 1, 0)\n    if probabilities:\n        prob_1 = np.zeros_like(distance)\n        if self.threshold_bins:\n            for t_bin in self.threshold_bins:\n                prob_1[distance > t_bin[0]] = t_bin[1]\n        else:\n            dist_threshold = distance - self.distance_threshold_tau\n            prob_1 = 1 / (1 + np.exp(-dist_threshold))\n        prob_0 = np.ones_like(prob_1) - prob_1\n        return np.stack((prob_0, prob_1), axis=1)\n    return predicted_class",
            "def infer(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Infer membership of input `x` in estimator\\'s training data.\\n\\n        :param x: Input data.\\n        :param y: True labels for `x`.\\n        :param probabilities: a boolean indicating whether to return the predicted probabilities per class, or just\\n                              the predicted class\\n\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n\\n        :return: An array holding the inferred membership status, 1 indicates a member and 0 indicates non-member,\\n                 or class probabilities.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if y is None:\n        raise ValueError('Argument `y` is None, but this attack requires true labels `y` to be provided.')\n    if self.distance_threshold_tau is None:\n        raise ValueError('No value for distance threshold `distance_threshold_tau` provided. Please set`distance_threshold_tau` or run method `calibrate_distance_threshold` on known training and testdataset.')\n    if 'probabilities' in kwargs:\n        probabilities = kwargs.get('probabilities')\n        del kwargs['probabilities']\n    else:\n        probabilities = False\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_adv = hsj.generate(x=x, y=y)\n    distance = np.linalg.norm((x_adv - x).reshape((x.shape[0], -1)), ord=2, axis=1)\n    y_pred = self.estimator.predict(x=x)\n    distance[np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)] = 0\n    predicted_class = np.where(distance > self.distance_threshold_tau, 1, 0)\n    if probabilities:\n        prob_1 = np.zeros_like(distance)\n        if self.threshold_bins:\n            for t_bin in self.threshold_bins:\n                prob_1[distance > t_bin[0]] = t_bin[1]\n        else:\n            dist_threshold = distance - self.distance_threshold_tau\n            prob_1 = 1 / (1 + np.exp(-dist_threshold))\n        prob_0 = np.ones_like(prob_1) - prob_1\n        return np.stack((prob_0, prob_1), axis=1)\n    return predicted_class",
            "def infer(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Infer membership of input `x` in estimator\\'s training data.\\n\\n        :param x: Input data.\\n        :param y: True labels for `x`.\\n        :param probabilities: a boolean indicating whether to return the predicted probabilities per class, or just\\n                              the predicted class\\n\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n\\n        :return: An array holding the inferred membership status, 1 indicates a member and 0 indicates non-member,\\n                 or class probabilities.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if y is None:\n        raise ValueError('Argument `y` is None, but this attack requires true labels `y` to be provided.')\n    if self.distance_threshold_tau is None:\n        raise ValueError('No value for distance threshold `distance_threshold_tau` provided. Please set`distance_threshold_tau` or run method `calibrate_distance_threshold` on known training and testdataset.')\n    if 'probabilities' in kwargs:\n        probabilities = kwargs.get('probabilities')\n        del kwargs['probabilities']\n    else:\n        probabilities = False\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_adv = hsj.generate(x=x, y=y)\n    distance = np.linalg.norm((x_adv - x).reshape((x.shape[0], -1)), ord=2, axis=1)\n    y_pred = self.estimator.predict(x=x)\n    distance[np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)] = 0\n    predicted_class = np.where(distance > self.distance_threshold_tau, 1, 0)\n    if probabilities:\n        prob_1 = np.zeros_like(distance)\n        if self.threshold_bins:\n            for t_bin in self.threshold_bins:\n                prob_1[distance > t_bin[0]] = t_bin[1]\n        else:\n            dist_threshold = distance - self.distance_threshold_tau\n            prob_1 = 1 / (1 + np.exp(-dist_threshold))\n        prob_0 = np.ones_like(prob_1) - prob_1\n        return np.stack((prob_0, prob_1), axis=1)\n    return predicted_class"
        ]
    },
    {
        "func_name": "calibrate_distance_threshold",
        "original": "def calibrate_distance_threshold(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray, **kwargs):\n    \"\"\"\n        Calibrate distance threshold maximising the membership inference accuracy on `x_train` and `x_test`.\n\n        | Paper link: https://arxiv.org/abs/2007.14321\n\n        :param x_train: Training data.\n        :param y_train: Labels of training data `x_train`.\n        :param x_test: Test data.\n        :param y_test: Labels of test data `x_test`.\n\n        :Keyword Arguments for HopSkipJump:\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\n            * *max_iter*: Maximum number of iterations.\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\n            * *init_eval*: Initial number of evaluations for estimating gradient.\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\n            * *verbose*: Show progress bars.\n        \"\"\"\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y_train_onehot = check_and_transform_label_format(y_train, nb_classes=self.estimator.nb_classes)\n    if y_train_onehot is None:\n        raise ValueError('None value detected.')\n    y_test_onehot = check_and_transform_label_format(y_test, nb_classes=self.estimator.nb_classes)\n    if y_test_onehot is None:\n        raise ValueError('None value detected.')\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_train_adv = hsj.generate(x=x_train, y=y_train_onehot)\n    x_test_adv = hsj.generate(x=x_test, y=y_test_onehot)\n    distance_train = np.linalg.norm((x_train_adv - x_train).reshape((x_train.shape[0], -1)), ord=2, axis=1)\n    distance_test = np.linalg.norm((x_test_adv - x_test).reshape((x_test.shape[0], -1)), ord=2, axis=1)\n    y_train_pred = self.estimator.predict(x=x_train)\n    y_test_pred = self.estimator.predict(x=x_test)\n    distance_train[np.argmax(y_train_pred, axis=1) != np.argmax(y_train_onehot, axis=1)] = 0\n    distance_test[np.argmax(y_test_pred, axis=1) != np.argmax(y_test_onehot, axis=1)] = 0\n    num_increments = 100\n    tau_increment = np.amax([np.amax(distance_train), np.amax(distance_test)]) / num_increments\n    acc_max = 0.0\n    distance_threshold_tau = 0.0\n    self.threshold_bins = []\n    for i_tau in range(1, num_increments):\n        is_member_train = np.where(distance_train > i_tau * tau_increment, 1, 0)\n        is_member_test = np.where(distance_test > i_tau * tau_increment, 1, 0)\n        acc = (np.sum(is_member_train) + (is_member_test.shape[0] - np.sum(is_member_test))) / (is_member_train.shape[0] + is_member_test.shape[0])\n        new_threshold_tau = i_tau * tau_increment\n        self.threshold_bins.append((new_threshold_tau, acc))\n        if acc > acc_max:\n            distance_threshold_tau = new_threshold_tau\n            acc_max = acc\n    self.distance_threshold_tau = distance_threshold_tau",
        "mutated": [
            "def calibrate_distance_threshold(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray, **kwargs):\n    if False:\n        i = 10\n    '\\n        Calibrate distance threshold maximising the membership inference accuracy on `x_train` and `x_test`.\\n\\n        | Paper link: https://arxiv.org/abs/2007.14321\\n\\n        :param x_train: Training data.\\n        :param y_train: Labels of training data `x_train`.\\n        :param x_test: Test data.\\n        :param y_test: Labels of test data `x_test`.\\n\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y_train_onehot = check_and_transform_label_format(y_train, nb_classes=self.estimator.nb_classes)\n    if y_train_onehot is None:\n        raise ValueError('None value detected.')\n    y_test_onehot = check_and_transform_label_format(y_test, nb_classes=self.estimator.nb_classes)\n    if y_test_onehot is None:\n        raise ValueError('None value detected.')\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_train_adv = hsj.generate(x=x_train, y=y_train_onehot)\n    x_test_adv = hsj.generate(x=x_test, y=y_test_onehot)\n    distance_train = np.linalg.norm((x_train_adv - x_train).reshape((x_train.shape[0], -1)), ord=2, axis=1)\n    distance_test = np.linalg.norm((x_test_adv - x_test).reshape((x_test.shape[0], -1)), ord=2, axis=1)\n    y_train_pred = self.estimator.predict(x=x_train)\n    y_test_pred = self.estimator.predict(x=x_test)\n    distance_train[np.argmax(y_train_pred, axis=1) != np.argmax(y_train_onehot, axis=1)] = 0\n    distance_test[np.argmax(y_test_pred, axis=1) != np.argmax(y_test_onehot, axis=1)] = 0\n    num_increments = 100\n    tau_increment = np.amax([np.amax(distance_train), np.amax(distance_test)]) / num_increments\n    acc_max = 0.0\n    distance_threshold_tau = 0.0\n    self.threshold_bins = []\n    for i_tau in range(1, num_increments):\n        is_member_train = np.where(distance_train > i_tau * tau_increment, 1, 0)\n        is_member_test = np.where(distance_test > i_tau * tau_increment, 1, 0)\n        acc = (np.sum(is_member_train) + (is_member_test.shape[0] - np.sum(is_member_test))) / (is_member_train.shape[0] + is_member_test.shape[0])\n        new_threshold_tau = i_tau * tau_increment\n        self.threshold_bins.append((new_threshold_tau, acc))\n        if acc > acc_max:\n            distance_threshold_tau = new_threshold_tau\n            acc_max = acc\n    self.distance_threshold_tau = distance_threshold_tau",
            "def calibrate_distance_threshold(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Calibrate distance threshold maximising the membership inference accuracy on `x_train` and `x_test`.\\n\\n        | Paper link: https://arxiv.org/abs/2007.14321\\n\\n        :param x_train: Training data.\\n        :param y_train: Labels of training data `x_train`.\\n        :param x_test: Test data.\\n        :param y_test: Labels of test data `x_test`.\\n\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y_train_onehot = check_and_transform_label_format(y_train, nb_classes=self.estimator.nb_classes)\n    if y_train_onehot is None:\n        raise ValueError('None value detected.')\n    y_test_onehot = check_and_transform_label_format(y_test, nb_classes=self.estimator.nb_classes)\n    if y_test_onehot is None:\n        raise ValueError('None value detected.')\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_train_adv = hsj.generate(x=x_train, y=y_train_onehot)\n    x_test_adv = hsj.generate(x=x_test, y=y_test_onehot)\n    distance_train = np.linalg.norm((x_train_adv - x_train).reshape((x_train.shape[0], -1)), ord=2, axis=1)\n    distance_test = np.linalg.norm((x_test_adv - x_test).reshape((x_test.shape[0], -1)), ord=2, axis=1)\n    y_train_pred = self.estimator.predict(x=x_train)\n    y_test_pred = self.estimator.predict(x=x_test)\n    distance_train[np.argmax(y_train_pred, axis=1) != np.argmax(y_train_onehot, axis=1)] = 0\n    distance_test[np.argmax(y_test_pred, axis=1) != np.argmax(y_test_onehot, axis=1)] = 0\n    num_increments = 100\n    tau_increment = np.amax([np.amax(distance_train), np.amax(distance_test)]) / num_increments\n    acc_max = 0.0\n    distance_threshold_tau = 0.0\n    self.threshold_bins = []\n    for i_tau in range(1, num_increments):\n        is_member_train = np.where(distance_train > i_tau * tau_increment, 1, 0)\n        is_member_test = np.where(distance_test > i_tau * tau_increment, 1, 0)\n        acc = (np.sum(is_member_train) + (is_member_test.shape[0] - np.sum(is_member_test))) / (is_member_train.shape[0] + is_member_test.shape[0])\n        new_threshold_tau = i_tau * tau_increment\n        self.threshold_bins.append((new_threshold_tau, acc))\n        if acc > acc_max:\n            distance_threshold_tau = new_threshold_tau\n            acc_max = acc\n    self.distance_threshold_tau = distance_threshold_tau",
            "def calibrate_distance_threshold(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Calibrate distance threshold maximising the membership inference accuracy on `x_train` and `x_test`.\\n\\n        | Paper link: https://arxiv.org/abs/2007.14321\\n\\n        :param x_train: Training data.\\n        :param y_train: Labels of training data `x_train`.\\n        :param x_test: Test data.\\n        :param y_test: Labels of test data `x_test`.\\n\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y_train_onehot = check_and_transform_label_format(y_train, nb_classes=self.estimator.nb_classes)\n    if y_train_onehot is None:\n        raise ValueError('None value detected.')\n    y_test_onehot = check_and_transform_label_format(y_test, nb_classes=self.estimator.nb_classes)\n    if y_test_onehot is None:\n        raise ValueError('None value detected.')\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_train_adv = hsj.generate(x=x_train, y=y_train_onehot)\n    x_test_adv = hsj.generate(x=x_test, y=y_test_onehot)\n    distance_train = np.linalg.norm((x_train_adv - x_train).reshape((x_train.shape[0], -1)), ord=2, axis=1)\n    distance_test = np.linalg.norm((x_test_adv - x_test).reshape((x_test.shape[0], -1)), ord=2, axis=1)\n    y_train_pred = self.estimator.predict(x=x_train)\n    y_test_pred = self.estimator.predict(x=x_test)\n    distance_train[np.argmax(y_train_pred, axis=1) != np.argmax(y_train_onehot, axis=1)] = 0\n    distance_test[np.argmax(y_test_pred, axis=1) != np.argmax(y_test_onehot, axis=1)] = 0\n    num_increments = 100\n    tau_increment = np.amax([np.amax(distance_train), np.amax(distance_test)]) / num_increments\n    acc_max = 0.0\n    distance_threshold_tau = 0.0\n    self.threshold_bins = []\n    for i_tau in range(1, num_increments):\n        is_member_train = np.where(distance_train > i_tau * tau_increment, 1, 0)\n        is_member_test = np.where(distance_test > i_tau * tau_increment, 1, 0)\n        acc = (np.sum(is_member_train) + (is_member_test.shape[0] - np.sum(is_member_test))) / (is_member_train.shape[0] + is_member_test.shape[0])\n        new_threshold_tau = i_tau * tau_increment\n        self.threshold_bins.append((new_threshold_tau, acc))\n        if acc > acc_max:\n            distance_threshold_tau = new_threshold_tau\n            acc_max = acc\n    self.distance_threshold_tau = distance_threshold_tau",
            "def calibrate_distance_threshold(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Calibrate distance threshold maximising the membership inference accuracy on `x_train` and `x_test`.\\n\\n        | Paper link: https://arxiv.org/abs/2007.14321\\n\\n        :param x_train: Training data.\\n        :param y_train: Labels of training data `x_train`.\\n        :param x_test: Test data.\\n        :param y_test: Labels of test data `x_test`.\\n\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y_train_onehot = check_and_transform_label_format(y_train, nb_classes=self.estimator.nb_classes)\n    if y_train_onehot is None:\n        raise ValueError('None value detected.')\n    y_test_onehot = check_and_transform_label_format(y_test, nb_classes=self.estimator.nb_classes)\n    if y_test_onehot is None:\n        raise ValueError('None value detected.')\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_train_adv = hsj.generate(x=x_train, y=y_train_onehot)\n    x_test_adv = hsj.generate(x=x_test, y=y_test_onehot)\n    distance_train = np.linalg.norm((x_train_adv - x_train).reshape((x_train.shape[0], -1)), ord=2, axis=1)\n    distance_test = np.linalg.norm((x_test_adv - x_test).reshape((x_test.shape[0], -1)), ord=2, axis=1)\n    y_train_pred = self.estimator.predict(x=x_train)\n    y_test_pred = self.estimator.predict(x=x_test)\n    distance_train[np.argmax(y_train_pred, axis=1) != np.argmax(y_train_onehot, axis=1)] = 0\n    distance_test[np.argmax(y_test_pred, axis=1) != np.argmax(y_test_onehot, axis=1)] = 0\n    num_increments = 100\n    tau_increment = np.amax([np.amax(distance_train), np.amax(distance_test)]) / num_increments\n    acc_max = 0.0\n    distance_threshold_tau = 0.0\n    self.threshold_bins = []\n    for i_tau in range(1, num_increments):\n        is_member_train = np.where(distance_train > i_tau * tau_increment, 1, 0)\n        is_member_test = np.where(distance_test > i_tau * tau_increment, 1, 0)\n        acc = (np.sum(is_member_train) + (is_member_test.shape[0] - np.sum(is_member_test))) / (is_member_train.shape[0] + is_member_test.shape[0])\n        new_threshold_tau = i_tau * tau_increment\n        self.threshold_bins.append((new_threshold_tau, acc))\n        if acc > acc_max:\n            distance_threshold_tau = new_threshold_tau\n            acc_max = acc\n    self.distance_threshold_tau = distance_threshold_tau",
            "def calibrate_distance_threshold(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Calibrate distance threshold maximising the membership inference accuracy on `x_train` and `x_test`.\\n\\n        | Paper link: https://arxiv.org/abs/2007.14321\\n\\n        :param x_train: Training data.\\n        :param y_train: Labels of training data `x_train`.\\n        :param x_test: Test data.\\n        :param y_test: Labels of test data `x_test`.\\n\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if 'classifier' in kwargs:\n        raise ValueError('Keyword `classifier` in kwargs is not supported.')\n    if 'targeted' in kwargs:\n        raise ValueError('Keyword `targeted` in kwargs is not supported.')\n    y_train_onehot = check_and_transform_label_format(y_train, nb_classes=self.estimator.nb_classes)\n    if y_train_onehot is None:\n        raise ValueError('None value detected.')\n    y_test_onehot = check_and_transform_label_format(y_test, nb_classes=self.estimator.nb_classes)\n    if y_test_onehot is None:\n        raise ValueError('None value detected.')\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    x_train_adv = hsj.generate(x=x_train, y=y_train_onehot)\n    x_test_adv = hsj.generate(x=x_test, y=y_test_onehot)\n    distance_train = np.linalg.norm((x_train_adv - x_train).reshape((x_train.shape[0], -1)), ord=2, axis=1)\n    distance_test = np.linalg.norm((x_test_adv - x_test).reshape((x_test.shape[0], -1)), ord=2, axis=1)\n    y_train_pred = self.estimator.predict(x=x_train)\n    y_test_pred = self.estimator.predict(x=x_test)\n    distance_train[np.argmax(y_train_pred, axis=1) != np.argmax(y_train_onehot, axis=1)] = 0\n    distance_test[np.argmax(y_test_pred, axis=1) != np.argmax(y_test_onehot, axis=1)] = 0\n    num_increments = 100\n    tau_increment = np.amax([np.amax(distance_train), np.amax(distance_test)]) / num_increments\n    acc_max = 0.0\n    distance_threshold_tau = 0.0\n    self.threshold_bins = []\n    for i_tau in range(1, num_increments):\n        is_member_train = np.where(distance_train > i_tau * tau_increment, 1, 0)\n        is_member_test = np.where(distance_test > i_tau * tau_increment, 1, 0)\n        acc = (np.sum(is_member_train) + (is_member_test.shape[0] - np.sum(is_member_test))) / (is_member_train.shape[0] + is_member_test.shape[0])\n        new_threshold_tau = i_tau * tau_increment\n        self.threshold_bins.append((new_threshold_tau, acc))\n        if acc > acc_max:\n            distance_threshold_tau = new_threshold_tau\n            acc_max = acc\n    self.distance_threshold_tau = distance_threshold_tau"
        ]
    },
    {
        "func_name": "calibrate_distance_threshold_unsupervised",
        "original": "def calibrate_distance_threshold_unsupervised(self, top_t: int=50, num_samples: int=100, max_queries: int=1, **kwargs):\n    \"\"\"\n        Calibrate distance threshold on randomly generated samples, choosing the top-t percentile of the noise needed\n        to change the classifier's initial prediction. This method requires the model's clip_values to be set.\n\n        | Paper link: https://arxiv.org/abs/2007.15528\n\n        :param top_t: Top-t percentile.\n        :param num_samples: Number of random samples to generate.\n        :param max_queries: Maximum number of queries. Maximum number of HSJ iterations on a single sample will be\n                            max_queries * max_iter.\n        :Keyword Arguments for HopSkipJump:\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\n            * *max_iter*: Maximum number of iterations.\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\n            * *init_eval*: Initial number of evaluations for estimating gradient.\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\n            * *verbose*: Show progress bars.\n        \"\"\"\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if self.estimator.clip_values is not None:\n        (x_min, x_max) = self.estimator.clip_values\n    else:\n        raise RuntimeError(\"You need to set the estimator's clip_values in order to calibrate the distance threshold.\")\n    x_rand = np.random.rand(*(num_samples,) + self.estimator.input_shape).astype(np.float32)\n    x_rand *= x_max - x_min\n    x_rand += x_min\n    y_rand = self.estimator.predict(x=x_rand)\n    y_rand = check_and_transform_label_format(y_rand, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    distances = []\n    i = 0\n    while len(x_rand) != 0 and i < max_queries:\n        x_adv = hsj.generate(x=x_rand, y=y_rand)\n        distance = np.linalg.norm((x_adv - x_rand).reshape((x_rand.shape[0], -1)), ord=2, axis=1)\n        y_pred = self.estimator.predict(x=x_adv)\n        changed_predictions = np.argmax(y_pred, axis=1) != np.argmax(y_rand, axis=1)\n        distances.extend(distance[changed_predictions])\n        (x_rand, y_rand) = (x_adv[~changed_predictions], y_rand[~changed_predictions])\n        i += 1\n    if len(distances) == 0:\n        raise RuntimeWarning('No successful adversarial examples were generated - no distances were obtained.Distance threshold will not be set.')\n    self.distance_threshold_tau = np.percentile(distances, top_t)",
        "mutated": [
            "def calibrate_distance_threshold_unsupervised(self, top_t: int=50, num_samples: int=100, max_queries: int=1, **kwargs):\n    if False:\n        i = 10\n    '\\n        Calibrate distance threshold on randomly generated samples, choosing the top-t percentile of the noise needed\\n        to change the classifier\\'s initial prediction. This method requires the model\\'s clip_values to be set.\\n\\n        | Paper link: https://arxiv.org/abs/2007.15528\\n\\n        :param top_t: Top-t percentile.\\n        :param num_samples: Number of random samples to generate.\\n        :param max_queries: Maximum number of queries. Maximum number of HSJ iterations on a single sample will be\\n                            max_queries * max_iter.\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if self.estimator.clip_values is not None:\n        (x_min, x_max) = self.estimator.clip_values\n    else:\n        raise RuntimeError(\"You need to set the estimator's clip_values in order to calibrate the distance threshold.\")\n    x_rand = np.random.rand(*(num_samples,) + self.estimator.input_shape).astype(np.float32)\n    x_rand *= x_max - x_min\n    x_rand += x_min\n    y_rand = self.estimator.predict(x=x_rand)\n    y_rand = check_and_transform_label_format(y_rand, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    distances = []\n    i = 0\n    while len(x_rand) != 0 and i < max_queries:\n        x_adv = hsj.generate(x=x_rand, y=y_rand)\n        distance = np.linalg.norm((x_adv - x_rand).reshape((x_rand.shape[0], -1)), ord=2, axis=1)\n        y_pred = self.estimator.predict(x=x_adv)\n        changed_predictions = np.argmax(y_pred, axis=1) != np.argmax(y_rand, axis=1)\n        distances.extend(distance[changed_predictions])\n        (x_rand, y_rand) = (x_adv[~changed_predictions], y_rand[~changed_predictions])\n        i += 1\n    if len(distances) == 0:\n        raise RuntimeWarning('No successful adversarial examples were generated - no distances were obtained.Distance threshold will not be set.')\n    self.distance_threshold_tau = np.percentile(distances, top_t)",
            "def calibrate_distance_threshold_unsupervised(self, top_t: int=50, num_samples: int=100, max_queries: int=1, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Calibrate distance threshold on randomly generated samples, choosing the top-t percentile of the noise needed\\n        to change the classifier\\'s initial prediction. This method requires the model\\'s clip_values to be set.\\n\\n        | Paper link: https://arxiv.org/abs/2007.15528\\n\\n        :param top_t: Top-t percentile.\\n        :param num_samples: Number of random samples to generate.\\n        :param max_queries: Maximum number of queries. Maximum number of HSJ iterations on a single sample will be\\n                            max_queries * max_iter.\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if self.estimator.clip_values is not None:\n        (x_min, x_max) = self.estimator.clip_values\n    else:\n        raise RuntimeError(\"You need to set the estimator's clip_values in order to calibrate the distance threshold.\")\n    x_rand = np.random.rand(*(num_samples,) + self.estimator.input_shape).astype(np.float32)\n    x_rand *= x_max - x_min\n    x_rand += x_min\n    y_rand = self.estimator.predict(x=x_rand)\n    y_rand = check_and_transform_label_format(y_rand, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    distances = []\n    i = 0\n    while len(x_rand) != 0 and i < max_queries:\n        x_adv = hsj.generate(x=x_rand, y=y_rand)\n        distance = np.linalg.norm((x_adv - x_rand).reshape((x_rand.shape[0], -1)), ord=2, axis=1)\n        y_pred = self.estimator.predict(x=x_adv)\n        changed_predictions = np.argmax(y_pred, axis=1) != np.argmax(y_rand, axis=1)\n        distances.extend(distance[changed_predictions])\n        (x_rand, y_rand) = (x_adv[~changed_predictions], y_rand[~changed_predictions])\n        i += 1\n    if len(distances) == 0:\n        raise RuntimeWarning('No successful adversarial examples were generated - no distances were obtained.Distance threshold will not be set.')\n    self.distance_threshold_tau = np.percentile(distances, top_t)",
            "def calibrate_distance_threshold_unsupervised(self, top_t: int=50, num_samples: int=100, max_queries: int=1, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Calibrate distance threshold on randomly generated samples, choosing the top-t percentile of the noise needed\\n        to change the classifier\\'s initial prediction. This method requires the model\\'s clip_values to be set.\\n\\n        | Paper link: https://arxiv.org/abs/2007.15528\\n\\n        :param top_t: Top-t percentile.\\n        :param num_samples: Number of random samples to generate.\\n        :param max_queries: Maximum number of queries. Maximum number of HSJ iterations on a single sample will be\\n                            max_queries * max_iter.\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if self.estimator.clip_values is not None:\n        (x_min, x_max) = self.estimator.clip_values\n    else:\n        raise RuntimeError(\"You need to set the estimator's clip_values in order to calibrate the distance threshold.\")\n    x_rand = np.random.rand(*(num_samples,) + self.estimator.input_shape).astype(np.float32)\n    x_rand *= x_max - x_min\n    x_rand += x_min\n    y_rand = self.estimator.predict(x=x_rand)\n    y_rand = check_and_transform_label_format(y_rand, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    distances = []\n    i = 0\n    while len(x_rand) != 0 and i < max_queries:\n        x_adv = hsj.generate(x=x_rand, y=y_rand)\n        distance = np.linalg.norm((x_adv - x_rand).reshape((x_rand.shape[0], -1)), ord=2, axis=1)\n        y_pred = self.estimator.predict(x=x_adv)\n        changed_predictions = np.argmax(y_pred, axis=1) != np.argmax(y_rand, axis=1)\n        distances.extend(distance[changed_predictions])\n        (x_rand, y_rand) = (x_adv[~changed_predictions], y_rand[~changed_predictions])\n        i += 1\n    if len(distances) == 0:\n        raise RuntimeWarning('No successful adversarial examples were generated - no distances were obtained.Distance threshold will not be set.')\n    self.distance_threshold_tau = np.percentile(distances, top_t)",
            "def calibrate_distance_threshold_unsupervised(self, top_t: int=50, num_samples: int=100, max_queries: int=1, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Calibrate distance threshold on randomly generated samples, choosing the top-t percentile of the noise needed\\n        to change the classifier\\'s initial prediction. This method requires the model\\'s clip_values to be set.\\n\\n        | Paper link: https://arxiv.org/abs/2007.15528\\n\\n        :param top_t: Top-t percentile.\\n        :param num_samples: Number of random samples to generate.\\n        :param max_queries: Maximum number of queries. Maximum number of HSJ iterations on a single sample will be\\n                            max_queries * max_iter.\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if self.estimator.clip_values is not None:\n        (x_min, x_max) = self.estimator.clip_values\n    else:\n        raise RuntimeError(\"You need to set the estimator's clip_values in order to calibrate the distance threshold.\")\n    x_rand = np.random.rand(*(num_samples,) + self.estimator.input_shape).astype(np.float32)\n    x_rand *= x_max - x_min\n    x_rand += x_min\n    y_rand = self.estimator.predict(x=x_rand)\n    y_rand = check_and_transform_label_format(y_rand, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    distances = []\n    i = 0\n    while len(x_rand) != 0 and i < max_queries:\n        x_adv = hsj.generate(x=x_rand, y=y_rand)\n        distance = np.linalg.norm((x_adv - x_rand).reshape((x_rand.shape[0], -1)), ord=2, axis=1)\n        y_pred = self.estimator.predict(x=x_adv)\n        changed_predictions = np.argmax(y_pred, axis=1) != np.argmax(y_rand, axis=1)\n        distances.extend(distance[changed_predictions])\n        (x_rand, y_rand) = (x_adv[~changed_predictions], y_rand[~changed_predictions])\n        i += 1\n    if len(distances) == 0:\n        raise RuntimeWarning('No successful adversarial examples were generated - no distances were obtained.Distance threshold will not be set.')\n    self.distance_threshold_tau = np.percentile(distances, top_t)",
            "def calibrate_distance_threshold_unsupervised(self, top_t: int=50, num_samples: int=100, max_queries: int=1, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Calibrate distance threshold on randomly generated samples, choosing the top-t percentile of the noise needed\\n        to change the classifier\\'s initial prediction. This method requires the model\\'s clip_values to be set.\\n\\n        | Paper link: https://arxiv.org/abs/2007.15528\\n\\n        :param top_t: Top-t percentile.\\n        :param num_samples: Number of random samples to generate.\\n        :param max_queries: Maximum number of queries. Maximum number of HSJ iterations on a single sample will be\\n                            max_queries * max_iter.\\n        :Keyword Arguments for HopSkipJump:\\n            * *norm*: Order of the norm. Possible values: \"inf\", np.inf or 2.\\n            * *max_iter*: Maximum number of iterations.\\n            * *max_eval*: Maximum number of evaluations for estimating gradient.\\n            * *init_eval*: Initial number of evaluations for estimating gradient.\\n            * *init_size*: Maximum number of trials for initial generation of adversarial examples.\\n            * *verbose*: Show progress bars.\\n        '\n    from art.attacks.evasion.hop_skip_jump import HopSkipJump\n    if self.estimator.clip_values is not None:\n        (x_min, x_max) = self.estimator.clip_values\n    else:\n        raise RuntimeError(\"You need to set the estimator's clip_values in order to calibrate the distance threshold.\")\n    x_rand = np.random.rand(*(num_samples,) + self.estimator.input_shape).astype(np.float32)\n    x_rand *= x_max - x_min\n    x_rand += x_min\n    y_rand = self.estimator.predict(x=x_rand)\n    y_rand = check_and_transform_label_format(y_rand, nb_classes=self.estimator.nb_classes)\n    hsj = HopSkipJump(classifier=self.estimator, targeted=False, **kwargs)\n    distances = []\n    i = 0\n    while len(x_rand) != 0 and i < max_queries:\n        x_adv = hsj.generate(x=x_rand, y=y_rand)\n        distance = np.linalg.norm((x_adv - x_rand).reshape((x_rand.shape[0], -1)), ord=2, axis=1)\n        y_pred = self.estimator.predict(x=x_adv)\n        changed_predictions = np.argmax(y_pred, axis=1) != np.argmax(y_rand, axis=1)\n        distances.extend(distance[changed_predictions])\n        (x_rand, y_rand) = (x_adv[~changed_predictions], y_rand[~changed_predictions])\n        i += 1\n    if len(distances) == 0:\n        raise RuntimeWarning('No successful adversarial examples were generated - no distances were obtained.Distance threshold will not be set.')\n    self.distance_threshold_tau = np.percentile(distances, top_t)"
        ]
    },
    {
        "func_name": "_check_params",
        "original": "def _check_params(self) -> None:\n    if self.distance_threshold_tau is not None and (not isinstance(self.distance_threshold_tau, (int, float)) or self.distance_threshold_tau <= 0.0):\n        raise ValueError('The distance threshold `distance_threshold_tau` needs to be a positive float.')",
        "mutated": [
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n    if self.distance_threshold_tau is not None and (not isinstance(self.distance_threshold_tau, (int, float)) or self.distance_threshold_tau <= 0.0):\n        raise ValueError('The distance threshold `distance_threshold_tau` needs to be a positive float.')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.distance_threshold_tau is not None and (not isinstance(self.distance_threshold_tau, (int, float)) or self.distance_threshold_tau <= 0.0):\n        raise ValueError('The distance threshold `distance_threshold_tau` needs to be a positive float.')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.distance_threshold_tau is not None and (not isinstance(self.distance_threshold_tau, (int, float)) or self.distance_threshold_tau <= 0.0):\n        raise ValueError('The distance threshold `distance_threshold_tau` needs to be a positive float.')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.distance_threshold_tau is not None and (not isinstance(self.distance_threshold_tau, (int, float)) or self.distance_threshold_tau <= 0.0):\n        raise ValueError('The distance threshold `distance_threshold_tau` needs to be a positive float.')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.distance_threshold_tau is not None and (not isinstance(self.distance_threshold_tau, (int, float)) or self.distance_threshold_tau <= 0.0):\n        raise ValueError('The distance threshold `distance_threshold_tau` needs to be a positive float.')"
        ]
    }
]