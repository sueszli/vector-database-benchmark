[
    {
        "func_name": "print_debug_output",
        "original": "def print_debug_output(results, dt):\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    coefs = results['est']['Lagged endogenous term']\n    print('coefs:')\n    print(str(type(coefs)) + str(coefs.shape))\n    print(coefs)\n    print('se: ')\n    print(results['se']['Lagged endogenous term'])\n    print('t: ')\n    print(results['t']['Lagged endogenous term'])\n    print('p: ')\n    print(results['p']['Lagged endogenous term'])",
        "mutated": [
            "def print_debug_output(results, dt):\n    if False:\n        i = 10\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    coefs = results['est']['Lagged endogenous term']\n    print('coefs:')\n    print(str(type(coefs)) + str(coefs.shape))\n    print(coefs)\n    print('se: ')\n    print(results['se']['Lagged endogenous term'])\n    print('t: ')\n    print(results['t']['Lagged endogenous term'])\n    print('p: ')\n    print(results['p']['Lagged endogenous term'])",
            "def print_debug_output(results, dt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    coefs = results['est']['Lagged endogenous term']\n    print('coefs:')\n    print(str(type(coefs)) + str(coefs.shape))\n    print(coefs)\n    print('se: ')\n    print(results['se']['Lagged endogenous term'])\n    print('t: ')\n    print(results['t']['Lagged endogenous term'])\n    print('p: ')\n    print(results['p']['Lagged endogenous term'])",
            "def print_debug_output(results, dt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    coefs = results['est']['Lagged endogenous term']\n    print('coefs:')\n    print(str(type(coefs)) + str(coefs.shape))\n    print(coefs)\n    print('se: ')\n    print(results['se']['Lagged endogenous term'])\n    print('t: ')\n    print(results['t']['Lagged endogenous term'])\n    print('p: ')\n    print(results['p']['Lagged endogenous term'])",
            "def print_debug_output(results, dt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    coefs = results['est']['Lagged endogenous term']\n    print('coefs:')\n    print(str(type(coefs)) + str(coefs.shape))\n    print(coefs)\n    print('se: ')\n    print(results['se']['Lagged endogenous term'])\n    print('t: ')\n    print(results['t']['Lagged endogenous term'])\n    print('p: ')\n    print(results['p']['Lagged endogenous term'])",
            "def print_debug_output(results, dt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    coefs = results['est']['Lagged endogenous term']\n    print('coefs:')\n    print(str(type(coefs)) + str(coefs.shape))\n    print(coefs)\n    print('se: ')\n    print(results['se']['Lagged endogenous term'])\n    print('t: ')\n    print(results['t']['Lagged endogenous term'])\n    print('p: ')\n    print(results['p']['Lagged endogenous term'])"
        ]
    },
    {
        "func_name": "dt_s_tup_to_string",
        "original": "def dt_s_tup_to_string(dt_s_tup):\n    \"\"\"\n\n    Parameters\n    ----------\n    dt_s_tup : tuple\n        A tuple of length 2.\n        The first entry is a string specifying the deterministic term without\n        any information about seasonal terms (for example \"nc\" or \"c\").\n        The second entry is an int specifying the number of seasons.\n\n    Returns\n    -------\n    dt_string : str\n        Returns dt_s_tup[0], if dt_s_tup[1] is 0 (i.e. no seasons).\n        If dt_s_tup[1] is > 0 (i.e. there are seasons) add an \"s\" to the string\n        in dt_s_tup[0] like in the following examples:\n        \"nc\" --> \"ncs\"\n        \"c\" --> \"cs\"\n        \"ct\" --> \"cst\"\n    \"\"\"\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if dt_string == 'nc':\n            dt_string = dt_string[:2] + 's'\n        if dt_string == 'c' or dt_string == 'ct':\n            dt_string = dt_string[:1] + 's' + dt_string[1:]\n    return dt_string",
        "mutated": [
            "def dt_s_tup_to_string(dt_s_tup):\n    if False:\n        i = 10\n    '\\n\\n    Parameters\\n    ----------\\n    dt_s_tup : tuple\\n        A tuple of length 2.\\n        The first entry is a string specifying the deterministic term without\\n        any information about seasonal terms (for example \"nc\" or \"c\").\\n        The second entry is an int specifying the number of seasons.\\n\\n    Returns\\n    -------\\n    dt_string : str\\n        Returns dt_s_tup[0], if dt_s_tup[1] is 0 (i.e. no seasons).\\n        If dt_s_tup[1] is > 0 (i.e. there are seasons) add an \"s\" to the string\\n        in dt_s_tup[0] like in the following examples:\\n        \"nc\" --> \"ncs\"\\n        \"c\" --> \"cs\"\\n        \"ct\" --> \"cst\"\\n    '\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if dt_string == 'nc':\n            dt_string = dt_string[:2] + 's'\n        if dt_string == 'c' or dt_string == 'ct':\n            dt_string = dt_string[:1] + 's' + dt_string[1:]\n    return dt_string",
            "def dt_s_tup_to_string(dt_s_tup):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n    Parameters\\n    ----------\\n    dt_s_tup : tuple\\n        A tuple of length 2.\\n        The first entry is a string specifying the deterministic term without\\n        any information about seasonal terms (for example \"nc\" or \"c\").\\n        The second entry is an int specifying the number of seasons.\\n\\n    Returns\\n    -------\\n    dt_string : str\\n        Returns dt_s_tup[0], if dt_s_tup[1] is 0 (i.e. no seasons).\\n        If dt_s_tup[1] is > 0 (i.e. there are seasons) add an \"s\" to the string\\n        in dt_s_tup[0] like in the following examples:\\n        \"nc\" --> \"ncs\"\\n        \"c\" --> \"cs\"\\n        \"ct\" --> \"cst\"\\n    '\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if dt_string == 'nc':\n            dt_string = dt_string[:2] + 's'\n        if dt_string == 'c' or dt_string == 'ct':\n            dt_string = dt_string[:1] + 's' + dt_string[1:]\n    return dt_string",
            "def dt_s_tup_to_string(dt_s_tup):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n    Parameters\\n    ----------\\n    dt_s_tup : tuple\\n        A tuple of length 2.\\n        The first entry is a string specifying the deterministic term without\\n        any information about seasonal terms (for example \"nc\" or \"c\").\\n        The second entry is an int specifying the number of seasons.\\n\\n    Returns\\n    -------\\n    dt_string : str\\n        Returns dt_s_tup[0], if dt_s_tup[1] is 0 (i.e. no seasons).\\n        If dt_s_tup[1] is > 0 (i.e. there are seasons) add an \"s\" to the string\\n        in dt_s_tup[0] like in the following examples:\\n        \"nc\" --> \"ncs\"\\n        \"c\" --> \"cs\"\\n        \"ct\" --> \"cst\"\\n    '\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if dt_string == 'nc':\n            dt_string = dt_string[:2] + 's'\n        if dt_string == 'c' or dt_string == 'ct':\n            dt_string = dt_string[:1] + 's' + dt_string[1:]\n    return dt_string",
            "def dt_s_tup_to_string(dt_s_tup):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n    Parameters\\n    ----------\\n    dt_s_tup : tuple\\n        A tuple of length 2.\\n        The first entry is a string specifying the deterministic term without\\n        any information about seasonal terms (for example \"nc\" or \"c\").\\n        The second entry is an int specifying the number of seasons.\\n\\n    Returns\\n    -------\\n    dt_string : str\\n        Returns dt_s_tup[0], if dt_s_tup[1] is 0 (i.e. no seasons).\\n        If dt_s_tup[1] is > 0 (i.e. there are seasons) add an \"s\" to the string\\n        in dt_s_tup[0] like in the following examples:\\n        \"nc\" --> \"ncs\"\\n        \"c\" --> \"cs\"\\n        \"ct\" --> \"cst\"\\n    '\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if dt_string == 'nc':\n            dt_string = dt_string[:2] + 's'\n        if dt_string == 'c' or dt_string == 'ct':\n            dt_string = dt_string[:1] + 's' + dt_string[1:]\n    return dt_string",
            "def dt_s_tup_to_string(dt_s_tup):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n    Parameters\\n    ----------\\n    dt_s_tup : tuple\\n        A tuple of length 2.\\n        The first entry is a string specifying the deterministic term without\\n        any information about seasonal terms (for example \"nc\" or \"c\").\\n        The second entry is an int specifying the number of seasons.\\n\\n    Returns\\n    -------\\n    dt_string : str\\n        Returns dt_s_tup[0], if dt_s_tup[1] is 0 (i.e. no seasons).\\n        If dt_s_tup[1] is > 0 (i.e. there are seasons) add an \"s\" to the string\\n        in dt_s_tup[0] like in the following examples:\\n        \"nc\" --> \"ncs\"\\n        \"c\" --> \"cs\"\\n        \"ct\" --> \"cst\"\\n    '\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if dt_string == 'nc':\n            dt_string = dt_string[:2] + 's'\n        if dt_string == 'c' or dt_string == 'ct':\n            dt_string = dt_string[:1] + 's' + dt_string[1:]\n    return dt_string"
        ]
    },
    {
        "func_name": "load_results_jmulti",
        "original": "def load_results_jmulti(dataset, dt_s_list):\n    \"\"\"\n\n    Parameters\n    ----------\n    dataset : module\n        A data module in the statsmodels/datasets directory that defines a\n        __str__() method returning the dataset's name.\n    dt_s_list : list\n        A list of strings where each string represents a combination of\n        deterministic terms.\n\n    Returns\n    -------\n    result : dict\n        A dict (keys: tuples of deterministic terms and seasonal terms)\n        of dicts (keys: strings \"est\" (for estimators),\n                              \"se\" (for standard errors),\n                              \"t\" (for t-values),\n                              \"p\" (for p-values))\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\n    \"\"\"\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dt_s_list)\n    for dt_s in dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_headers = ['Lagged endogenous term', 'Deterministic term']\n        if dt_string == 'nc':\n            del section_headers[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(section_headers)\n        results['se'] = dict.fromkeys(section_headers)\n        results['t'] = dict.fromkeys(section_headers)\n        results['p'] = dict.fromkeys(section_headers)\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        section = -1\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_headers[section + 1] not in line:\n                continue\n            if section < len(section_headers) - 1 and section_headers[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][section_headers[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][section_headers[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][section_headers[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][section_headers[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        sigmau_file = dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line[len('Log Likelihood:'):]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n                continue\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{3}\\\\s*)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        neqs = len(results['est']['Sigma_u'])\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], neqs))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], neqs))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], neqs))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        if debug_mode:\n            print('\\n\\n\\n' + dt_string)\n        for causing in var_combs:\n            caused = tuple((name for name in vn if name not in causing))\n            causality_file = dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing, '_') + '.txt'\n            causality_file = os.path.join(here, causality_file)\n            causality_file = open(causality_file, encoding='latin_1')\n            causality_results = []\n            for line in causality_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                causality_results.append(number)\n            causality_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = causality_results[0]\n            results['granger_caus']['p'][causing, caused] = causality_results[1]\n            results['inst_caus']['test_stat'][causing, caused] = causality_results[2]\n            results['inst_caus']['p'][causing, caused] = causality_results[3]\n        ir_file = dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        section_start_marker = 'TESTS FOR NONNORMALITY'\n        section_reached = False\n        subsection_start_marker = 'Introduction to Multiple Time Series A'\n        subsection_reached = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not section_reached:\n                if section_start_marker in line:\n                    section_reached = True\n                continue\n            if not subsection_reached:\n                if subsection_start_marker in line:\n                    subsection_reached = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms",
        "mutated": [
            "def load_results_jmulti(dataset, dt_s_list):\n    if False:\n        i = 10\n    '\\n\\n    Parameters\\n    ----------\\n    dataset : module\\n        A data module in the statsmodels/datasets directory that defines a\\n        __str__() method returning the dataset\\'s name.\\n    dt_s_list : list\\n        A list of strings where each string represents a combination of\\n        deterministic terms.\\n\\n    Returns\\n    -------\\n    result : dict\\n        A dict (keys: tuples of deterministic terms and seasonal terms)\\n        of dicts (keys: strings \"est\" (for estimators),\\n                              \"se\" (for standard errors),\\n                              \"t\" (for t-values),\\n                              \"p\" (for p-values))\\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\\n    '\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dt_s_list)\n    for dt_s in dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_headers = ['Lagged endogenous term', 'Deterministic term']\n        if dt_string == 'nc':\n            del section_headers[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(section_headers)\n        results['se'] = dict.fromkeys(section_headers)\n        results['t'] = dict.fromkeys(section_headers)\n        results['p'] = dict.fromkeys(section_headers)\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        section = -1\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_headers[section + 1] not in line:\n                continue\n            if section < len(section_headers) - 1 and section_headers[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][section_headers[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][section_headers[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][section_headers[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][section_headers[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        sigmau_file = dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line[len('Log Likelihood:'):]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n                continue\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{3}\\\\s*)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        neqs = len(results['est']['Sigma_u'])\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], neqs))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], neqs))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], neqs))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        if debug_mode:\n            print('\\n\\n\\n' + dt_string)\n        for causing in var_combs:\n            caused = tuple((name for name in vn if name not in causing))\n            causality_file = dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing, '_') + '.txt'\n            causality_file = os.path.join(here, causality_file)\n            causality_file = open(causality_file, encoding='latin_1')\n            causality_results = []\n            for line in causality_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                causality_results.append(number)\n            causality_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = causality_results[0]\n            results['granger_caus']['p'][causing, caused] = causality_results[1]\n            results['inst_caus']['test_stat'][causing, caused] = causality_results[2]\n            results['inst_caus']['p'][causing, caused] = causality_results[3]\n        ir_file = dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        section_start_marker = 'TESTS FOR NONNORMALITY'\n        section_reached = False\n        subsection_start_marker = 'Introduction to Multiple Time Series A'\n        subsection_reached = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not section_reached:\n                if section_start_marker in line:\n                    section_reached = True\n                continue\n            if not subsection_reached:\n                if subsection_start_marker in line:\n                    subsection_reached = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms",
            "def load_results_jmulti(dataset, dt_s_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n    Parameters\\n    ----------\\n    dataset : module\\n        A data module in the statsmodels/datasets directory that defines a\\n        __str__() method returning the dataset\\'s name.\\n    dt_s_list : list\\n        A list of strings where each string represents a combination of\\n        deterministic terms.\\n\\n    Returns\\n    -------\\n    result : dict\\n        A dict (keys: tuples of deterministic terms and seasonal terms)\\n        of dicts (keys: strings \"est\" (for estimators),\\n                              \"se\" (for standard errors),\\n                              \"t\" (for t-values),\\n                              \"p\" (for p-values))\\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\\n    '\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dt_s_list)\n    for dt_s in dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_headers = ['Lagged endogenous term', 'Deterministic term']\n        if dt_string == 'nc':\n            del section_headers[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(section_headers)\n        results['se'] = dict.fromkeys(section_headers)\n        results['t'] = dict.fromkeys(section_headers)\n        results['p'] = dict.fromkeys(section_headers)\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        section = -1\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_headers[section + 1] not in line:\n                continue\n            if section < len(section_headers) - 1 and section_headers[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][section_headers[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][section_headers[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][section_headers[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][section_headers[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        sigmau_file = dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line[len('Log Likelihood:'):]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n                continue\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{3}\\\\s*)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        neqs = len(results['est']['Sigma_u'])\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], neqs))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], neqs))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], neqs))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        if debug_mode:\n            print('\\n\\n\\n' + dt_string)\n        for causing in var_combs:\n            caused = tuple((name for name in vn if name not in causing))\n            causality_file = dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing, '_') + '.txt'\n            causality_file = os.path.join(here, causality_file)\n            causality_file = open(causality_file, encoding='latin_1')\n            causality_results = []\n            for line in causality_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                causality_results.append(number)\n            causality_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = causality_results[0]\n            results['granger_caus']['p'][causing, caused] = causality_results[1]\n            results['inst_caus']['test_stat'][causing, caused] = causality_results[2]\n            results['inst_caus']['p'][causing, caused] = causality_results[3]\n        ir_file = dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        section_start_marker = 'TESTS FOR NONNORMALITY'\n        section_reached = False\n        subsection_start_marker = 'Introduction to Multiple Time Series A'\n        subsection_reached = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not section_reached:\n                if section_start_marker in line:\n                    section_reached = True\n                continue\n            if not subsection_reached:\n                if subsection_start_marker in line:\n                    subsection_reached = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms",
            "def load_results_jmulti(dataset, dt_s_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n    Parameters\\n    ----------\\n    dataset : module\\n        A data module in the statsmodels/datasets directory that defines a\\n        __str__() method returning the dataset\\'s name.\\n    dt_s_list : list\\n        A list of strings where each string represents a combination of\\n        deterministic terms.\\n\\n    Returns\\n    -------\\n    result : dict\\n        A dict (keys: tuples of deterministic terms and seasonal terms)\\n        of dicts (keys: strings \"est\" (for estimators),\\n                              \"se\" (for standard errors),\\n                              \"t\" (for t-values),\\n                              \"p\" (for p-values))\\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\\n    '\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dt_s_list)\n    for dt_s in dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_headers = ['Lagged endogenous term', 'Deterministic term']\n        if dt_string == 'nc':\n            del section_headers[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(section_headers)\n        results['se'] = dict.fromkeys(section_headers)\n        results['t'] = dict.fromkeys(section_headers)\n        results['p'] = dict.fromkeys(section_headers)\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        section = -1\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_headers[section + 1] not in line:\n                continue\n            if section < len(section_headers) - 1 and section_headers[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][section_headers[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][section_headers[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][section_headers[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][section_headers[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        sigmau_file = dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line[len('Log Likelihood:'):]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n                continue\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{3}\\\\s*)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        neqs = len(results['est']['Sigma_u'])\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], neqs))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], neqs))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], neqs))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        if debug_mode:\n            print('\\n\\n\\n' + dt_string)\n        for causing in var_combs:\n            caused = tuple((name for name in vn if name not in causing))\n            causality_file = dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing, '_') + '.txt'\n            causality_file = os.path.join(here, causality_file)\n            causality_file = open(causality_file, encoding='latin_1')\n            causality_results = []\n            for line in causality_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                causality_results.append(number)\n            causality_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = causality_results[0]\n            results['granger_caus']['p'][causing, caused] = causality_results[1]\n            results['inst_caus']['test_stat'][causing, caused] = causality_results[2]\n            results['inst_caus']['p'][causing, caused] = causality_results[3]\n        ir_file = dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        section_start_marker = 'TESTS FOR NONNORMALITY'\n        section_reached = False\n        subsection_start_marker = 'Introduction to Multiple Time Series A'\n        subsection_reached = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not section_reached:\n                if section_start_marker in line:\n                    section_reached = True\n                continue\n            if not subsection_reached:\n                if subsection_start_marker in line:\n                    subsection_reached = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms",
            "def load_results_jmulti(dataset, dt_s_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n    Parameters\\n    ----------\\n    dataset : module\\n        A data module in the statsmodels/datasets directory that defines a\\n        __str__() method returning the dataset\\'s name.\\n    dt_s_list : list\\n        A list of strings where each string represents a combination of\\n        deterministic terms.\\n\\n    Returns\\n    -------\\n    result : dict\\n        A dict (keys: tuples of deterministic terms and seasonal terms)\\n        of dicts (keys: strings \"est\" (for estimators),\\n                              \"se\" (for standard errors),\\n                              \"t\" (for t-values),\\n                              \"p\" (for p-values))\\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\\n    '\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dt_s_list)\n    for dt_s in dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_headers = ['Lagged endogenous term', 'Deterministic term']\n        if dt_string == 'nc':\n            del section_headers[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(section_headers)\n        results['se'] = dict.fromkeys(section_headers)\n        results['t'] = dict.fromkeys(section_headers)\n        results['p'] = dict.fromkeys(section_headers)\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        section = -1\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_headers[section + 1] not in line:\n                continue\n            if section < len(section_headers) - 1 and section_headers[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][section_headers[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][section_headers[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][section_headers[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][section_headers[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        sigmau_file = dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line[len('Log Likelihood:'):]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n                continue\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{3}\\\\s*)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        neqs = len(results['est']['Sigma_u'])\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], neqs))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], neqs))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], neqs))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        if debug_mode:\n            print('\\n\\n\\n' + dt_string)\n        for causing in var_combs:\n            caused = tuple((name for name in vn if name not in causing))\n            causality_file = dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing, '_') + '.txt'\n            causality_file = os.path.join(here, causality_file)\n            causality_file = open(causality_file, encoding='latin_1')\n            causality_results = []\n            for line in causality_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                causality_results.append(number)\n            causality_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = causality_results[0]\n            results['granger_caus']['p'][causing, caused] = causality_results[1]\n            results['inst_caus']['test_stat'][causing, caused] = causality_results[2]\n            results['inst_caus']['p'][causing, caused] = causality_results[3]\n        ir_file = dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        section_start_marker = 'TESTS FOR NONNORMALITY'\n        section_reached = False\n        subsection_start_marker = 'Introduction to Multiple Time Series A'\n        subsection_reached = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not section_reached:\n                if section_start_marker in line:\n                    section_reached = True\n                continue\n            if not subsection_reached:\n                if subsection_start_marker in line:\n                    subsection_reached = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms",
            "def load_results_jmulti(dataset, dt_s_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n    Parameters\\n    ----------\\n    dataset : module\\n        A data module in the statsmodels/datasets directory that defines a\\n        __str__() method returning the dataset\\'s name.\\n    dt_s_list : list\\n        A list of strings where each string represents a combination of\\n        deterministic terms.\\n\\n    Returns\\n    -------\\n    result : dict\\n        A dict (keys: tuples of deterministic terms and seasonal terms)\\n        of dicts (keys: strings \"est\" (for estimators),\\n                              \"se\" (for standard errors),\\n                              \"t\" (for t-values),\\n                              \"p\" (for p-values))\\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\\n    '\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dt_s_list)\n    for dt_s in dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_headers = ['Lagged endogenous term', 'Deterministic term']\n        if dt_string == 'nc':\n            del section_headers[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(section_headers)\n        results['se'] = dict.fromkeys(section_headers)\n        results['t'] = dict.fromkeys(section_headers)\n        results['p'] = dict.fromkeys(section_headers)\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        section = -1\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_headers[section + 1] not in line:\n                continue\n            if section < len(section_headers) - 1 and section_headers[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][section_headers[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][section_headers[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][section_headers[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][section_headers[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        sigmau_file = dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line[len('Log Likelihood:'):]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n                continue\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{3}\\\\s*)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        neqs = len(results['est']['Sigma_u'])\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], neqs))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], neqs))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], neqs))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        if debug_mode:\n            print('\\n\\n\\n' + dt_string)\n        for causing in var_combs:\n            caused = tuple((name for name in vn if name not in causing))\n            causality_file = dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing, '_') + '.txt'\n            causality_file = os.path.join(here, causality_file)\n            causality_file = open(causality_file, encoding='latin_1')\n            causality_results = []\n            for line in causality_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                causality_results.append(number)\n            causality_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = causality_results[0]\n            results['granger_caus']['p'][causing, caused] = causality_results[1]\n            results['inst_caus']['test_stat'][causing, caused] = causality_results[2]\n            results['inst_caus']['p'][causing, caused] = causality_results[3]\n        ir_file = dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        section_start_marker = 'TESTS FOR NONNORMALITY'\n        section_reached = False\n        subsection_start_marker = 'Introduction to Multiple Time Series A'\n        subsection_reached = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not section_reached:\n                if section_start_marker in line:\n                    section_reached = True\n                continue\n            if not subsection_reached:\n                if subsection_start_marker in line:\n                    subsection_reached = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms"
        ]
    }
]