[
    {
        "func_name": "__init__",
        "original": "def __init__(self, m, k, dtype=None):\n    super().__init__()\n    self.linear = nn.Linear(k, m)",
        "mutated": [
            "def __init__(self, m, k, dtype=None):\n    if False:\n        i = 10\n    super().__init__()\n    self.linear = nn.Linear(k, m)",
            "def __init__(self, m, k, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.linear = nn.Linear(k, m)",
            "def __init__(self, m, k, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.linear = nn.Linear(k, m)",
            "def __init__(self, m, k, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.linear = nn.Linear(k, m)",
            "def __init__(self, m, k, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.linear = nn.Linear(k, m)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    return self.linear(x)",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    return self.linear(x)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.linear(x)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.linear(x)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.linear(x)",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.linear(x)"
        ]
    },
    {
        "func_name": "rand_sparse_semi_structured_mask",
        "original": "def rand_sparse_semi_structured_mask(r, c, dtype=torch.float16, device='cuda', choice=None):\n    \"\"\"\n    This function returns a 1:2 sparse matrix of size (r, c).\n    Note that this means this matrix will also be 2:4 and 4:8 sparse as well.\n    \"\"\"\n    choices = [[0, 1], [1, 0]]\n    mask_entries = [choice or random.choice(choices) for i in range(r * c // 2)]\n    return torch.tensor(mask_entries, dtype=dtype, device=device).reshape(r, c).contiguous()",
        "mutated": [
            "def rand_sparse_semi_structured_mask(r, c, dtype=torch.float16, device='cuda', choice=None):\n    if False:\n        i = 10\n    '\\n    This function returns a 1:2 sparse matrix of size (r, c).\\n    Note that this means this matrix will also be 2:4 and 4:8 sparse as well.\\n    '\n    choices = [[0, 1], [1, 0]]\n    mask_entries = [choice or random.choice(choices) for i in range(r * c // 2)]\n    return torch.tensor(mask_entries, dtype=dtype, device=device).reshape(r, c).contiguous()",
            "def rand_sparse_semi_structured_mask(r, c, dtype=torch.float16, device='cuda', choice=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This function returns a 1:2 sparse matrix of size (r, c).\\n    Note that this means this matrix will also be 2:4 and 4:8 sparse as well.\\n    '\n    choices = [[0, 1], [1, 0]]\n    mask_entries = [choice or random.choice(choices) for i in range(r * c // 2)]\n    return torch.tensor(mask_entries, dtype=dtype, device=device).reshape(r, c).contiguous()",
            "def rand_sparse_semi_structured_mask(r, c, dtype=torch.float16, device='cuda', choice=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This function returns a 1:2 sparse matrix of size (r, c).\\n    Note that this means this matrix will also be 2:4 and 4:8 sparse as well.\\n    '\n    choices = [[0, 1], [1, 0]]\n    mask_entries = [choice or random.choice(choices) for i in range(r * c // 2)]\n    return torch.tensor(mask_entries, dtype=dtype, device=device).reshape(r, c).contiguous()",
            "def rand_sparse_semi_structured_mask(r, c, dtype=torch.float16, device='cuda', choice=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This function returns a 1:2 sparse matrix of size (r, c).\\n    Note that this means this matrix will also be 2:4 and 4:8 sparse as well.\\n    '\n    choices = [[0, 1], [1, 0]]\n    mask_entries = [choice or random.choice(choices) for i in range(r * c // 2)]\n    return torch.tensor(mask_entries, dtype=dtype, device=device).reshape(r, c).contiguous()",
            "def rand_sparse_semi_structured_mask(r, c, dtype=torch.float16, device='cuda', choice=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This function returns a 1:2 sparse matrix of size (r, c).\\n    Note that this means this matrix will also be 2:4 and 4:8 sparse as well.\\n    '\n    choices = [[0, 1], [1, 0]]\n    mask_entries = [choice or random.choice(choices) for i in range(r * c // 2)]\n    return torch.tensor(mask_entries, dtype=dtype, device=device).reshape(r, c).contiguous()"
        ]
    },
    {
        "func_name": "test_linear",
        "original": "def test_linear(m, k, n, dtype, contiguous, backend):\n    SparseSemiStructuredTensor._FORCE_CUTLASS = backend == 'cutlass'\n    mask = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    sparse_weight = torch.rand(m, k).to(dtype).cuda() * mask\n    input_tensor = torch.zeros(n, k).to(dtype).cuda()\n    model = Model(m, k).to(dtype).cuda().eval()\n    dense_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    dense_output = model(input_tensor)\n    print(dense_output.shape)\n    model.linear.weight = nn.Parameter(to_sparse_semi_structured(sparse_weight))\n    sparse_output = model(input_tensor)\n    print(sparse_output.shape)\n    sparse_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'linear', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}",
        "mutated": [
            "def test_linear(m, k, n, dtype, contiguous, backend):\n    if False:\n        i = 10\n    SparseSemiStructuredTensor._FORCE_CUTLASS = backend == 'cutlass'\n    mask = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    sparse_weight = torch.rand(m, k).to(dtype).cuda() * mask\n    input_tensor = torch.zeros(n, k).to(dtype).cuda()\n    model = Model(m, k).to(dtype).cuda().eval()\n    dense_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    dense_output = model(input_tensor)\n    print(dense_output.shape)\n    model.linear.weight = nn.Parameter(to_sparse_semi_structured(sparse_weight))\n    sparse_output = model(input_tensor)\n    print(sparse_output.shape)\n    sparse_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'linear', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}",
            "def test_linear(m, k, n, dtype, contiguous, backend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    SparseSemiStructuredTensor._FORCE_CUTLASS = backend == 'cutlass'\n    mask = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    sparse_weight = torch.rand(m, k).to(dtype).cuda() * mask\n    input_tensor = torch.zeros(n, k).to(dtype).cuda()\n    model = Model(m, k).to(dtype).cuda().eval()\n    dense_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    dense_output = model(input_tensor)\n    print(dense_output.shape)\n    model.linear.weight = nn.Parameter(to_sparse_semi_structured(sparse_weight))\n    sparse_output = model(input_tensor)\n    print(sparse_output.shape)\n    sparse_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'linear', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}",
            "def test_linear(m, k, n, dtype, contiguous, backend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    SparseSemiStructuredTensor._FORCE_CUTLASS = backend == 'cutlass'\n    mask = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    sparse_weight = torch.rand(m, k).to(dtype).cuda() * mask\n    input_tensor = torch.zeros(n, k).to(dtype).cuda()\n    model = Model(m, k).to(dtype).cuda().eval()\n    dense_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    dense_output = model(input_tensor)\n    print(dense_output.shape)\n    model.linear.weight = nn.Parameter(to_sparse_semi_structured(sparse_weight))\n    sparse_output = model(input_tensor)\n    print(sparse_output.shape)\n    sparse_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'linear', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}",
            "def test_linear(m, k, n, dtype, contiguous, backend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    SparseSemiStructuredTensor._FORCE_CUTLASS = backend == 'cutlass'\n    mask = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    sparse_weight = torch.rand(m, k).to(dtype).cuda() * mask\n    input_tensor = torch.zeros(n, k).to(dtype).cuda()\n    model = Model(m, k).to(dtype).cuda().eval()\n    dense_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    dense_output = model(input_tensor)\n    print(dense_output.shape)\n    model.linear.weight = nn.Parameter(to_sparse_semi_structured(sparse_weight))\n    sparse_output = model(input_tensor)\n    print(sparse_output.shape)\n    sparse_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'linear', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}",
            "def test_linear(m, k, n, dtype, contiguous, backend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    SparseSemiStructuredTensor._FORCE_CUTLASS = backend == 'cutlass'\n    mask = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    sparse_weight = torch.rand(m, k).to(dtype).cuda() * mask\n    input_tensor = torch.zeros(n, k).to(dtype).cuda()\n    model = Model(m, k).to(dtype).cuda().eval()\n    dense_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    dense_output = model(input_tensor)\n    print(dense_output.shape)\n    model.linear.weight = nn.Parameter(to_sparse_semi_structured(sparse_weight))\n    sparse_output = model(input_tensor)\n    print(sparse_output.shape)\n    sparse_measurement = benchmark.Timer(stmt='model(input_tensor)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'linear', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}"
        ]
    },
    {
        "func_name": "test_tensor",
        "original": "def test_tensor(m, k, n, dtype, contiguous, backend):\n    A = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    B = torch.zeros(k, n).to(dtype).cuda()\n    bias = torch.rand(n).to(dtype).cuda()\n    sA = to_sparse_semi_structured(A)\n    if dtype is not torch.int8:\n        dense_output = torch.mm(A, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(A, B)', globals=locals()).blocked_autorange()\n    else:\n        print('int8 baseline not supported')\n        dense_output = torch.mm(sA, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    sparse_output = torch.mm(sA, B)\n    sparse_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'tensor', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}",
        "mutated": [
            "def test_tensor(m, k, n, dtype, contiguous, backend):\n    if False:\n        i = 10\n    A = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    B = torch.zeros(k, n).to(dtype).cuda()\n    bias = torch.rand(n).to(dtype).cuda()\n    sA = to_sparse_semi_structured(A)\n    if dtype is not torch.int8:\n        dense_output = torch.mm(A, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(A, B)', globals=locals()).blocked_autorange()\n    else:\n        print('int8 baseline not supported')\n        dense_output = torch.mm(sA, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    sparse_output = torch.mm(sA, B)\n    sparse_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'tensor', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}",
            "def test_tensor(m, k, n, dtype, contiguous, backend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    A = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    B = torch.zeros(k, n).to(dtype).cuda()\n    bias = torch.rand(n).to(dtype).cuda()\n    sA = to_sparse_semi_structured(A)\n    if dtype is not torch.int8:\n        dense_output = torch.mm(A, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(A, B)', globals=locals()).blocked_autorange()\n    else:\n        print('int8 baseline not supported')\n        dense_output = torch.mm(sA, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    sparse_output = torch.mm(sA, B)\n    sparse_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'tensor', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}",
            "def test_tensor(m, k, n, dtype, contiguous, backend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    A = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    B = torch.zeros(k, n).to(dtype).cuda()\n    bias = torch.rand(n).to(dtype).cuda()\n    sA = to_sparse_semi_structured(A)\n    if dtype is not torch.int8:\n        dense_output = torch.mm(A, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(A, B)', globals=locals()).blocked_autorange()\n    else:\n        print('int8 baseline not supported')\n        dense_output = torch.mm(sA, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    sparse_output = torch.mm(sA, B)\n    sparse_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'tensor', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}",
            "def test_tensor(m, k, n, dtype, contiguous, backend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    A = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    B = torch.zeros(k, n).to(dtype).cuda()\n    bias = torch.rand(n).to(dtype).cuda()\n    sA = to_sparse_semi_structured(A)\n    if dtype is not torch.int8:\n        dense_output = torch.mm(A, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(A, B)', globals=locals()).blocked_autorange()\n    else:\n        print('int8 baseline not supported')\n        dense_output = torch.mm(sA, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    sparse_output = torch.mm(sA, B)\n    sparse_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'tensor', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}",
            "def test_tensor(m, k, n, dtype, contiguous, backend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    A = rand_sparse_semi_structured_mask(m, k, dtype=dtype)\n    B = torch.zeros(k, n).to(dtype).cuda()\n    bias = torch.rand(n).to(dtype).cuda()\n    sA = to_sparse_semi_structured(A)\n    if dtype is not torch.int8:\n        dense_output = torch.mm(A, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(A, B)', globals=locals()).blocked_autorange()\n    else:\n        print('int8 baseline not supported')\n        dense_output = torch.mm(sA, B)\n        dense_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    sparse_output = torch.mm(sA, B)\n    sparse_measurement = benchmark.Timer(stmt='torch.mm(sA, B)', globals=locals()).blocked_autorange()\n    correct = torch.allclose(dense_output, sparse_output, rtol=0.001, atol=0.001)\n    return {'test_function': 'tensor', 'm': m, 'k': k, 'n': n, 'dtype': str(dtype), 'backend': backend, 'sparse_latency (ms)': sparse_measurement.median * 1000, 'dense_latency (ms)': dense_measurement.median * 1000, 'speedup (d/s)': dense_measurement.median / sparse_measurement.median, 'correct': correct, 'contiguous': sparse_output.is_contiguous()}"
        ]
    }
]