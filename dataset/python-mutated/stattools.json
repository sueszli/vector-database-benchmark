[
    {
        "func_name": "durbin_watson",
        "original": "def durbin_watson(resids, axis=0):\n    \"\"\"\n    Calculates the Durbin-Watson statistic.\n\n    Parameters\n    ----------\n    resids : array_like\n        Data for which to compute the Durbin-Watson statistic. Usually\n        regression model residuals.\n    axis : int, optional\n        Axis to use if data has more than 1 dimension. Default is 0.\n\n    Returns\n    -------\n    dw : float, array_like\n        The Durbin-Watson statistic.\n\n    Notes\n    -----\n    The null hypothesis of the test is that there is no serial correlation\n    in the residuals.\n    The Durbin-Watson test statistic is defined as:\n\n    .. math::\n\n       \\\\sum_{t=2}^T((e_t - e_{t-1})^2)/\\\\sum_{t=1}^Te_t^2\n\n    The test statistic is approximately equal to 2*(1-r) where ``r`` is the\n    sample autocorrelation of the residuals. Thus, for r == 0, indicating no\n    serial correlation, the test statistic equals 2. This statistic will\n    always be between 0 and 4. The closer to 0 the statistic, the more\n    evidence for positive serial correlation. The closer to 4, the more\n    evidence for negative serial correlation.\n    \"\"\"\n    resids = np.asarray(resids)\n    diff_resids = np.diff(resids, 1, axis=axis)\n    dw = np.sum(diff_resids ** 2, axis=axis) / np.sum(resids ** 2, axis=axis)\n    return dw",
        "mutated": [
            "def durbin_watson(resids, axis=0):\n    if False:\n        i = 10\n    '\\n    Calculates the Durbin-Watson statistic.\\n\\n    Parameters\\n    ----------\\n    resids : array_like\\n        Data for which to compute the Durbin-Watson statistic. Usually\\n        regression model residuals.\\n    axis : int, optional\\n        Axis to use if data has more than 1 dimension. Default is 0.\\n\\n    Returns\\n    -------\\n    dw : float, array_like\\n        The Durbin-Watson statistic.\\n\\n    Notes\\n    -----\\n    The null hypothesis of the test is that there is no serial correlation\\n    in the residuals.\\n    The Durbin-Watson test statistic is defined as:\\n\\n    .. math::\\n\\n       \\\\sum_{t=2}^T((e_t - e_{t-1})^2)/\\\\sum_{t=1}^Te_t^2\\n\\n    The test statistic is approximately equal to 2*(1-r) where ``r`` is the\\n    sample autocorrelation of the residuals. Thus, for r == 0, indicating no\\n    serial correlation, the test statistic equals 2. This statistic will\\n    always be between 0 and 4. The closer to 0 the statistic, the more\\n    evidence for positive serial correlation. The closer to 4, the more\\n    evidence for negative serial correlation.\\n    '\n    resids = np.asarray(resids)\n    diff_resids = np.diff(resids, 1, axis=axis)\n    dw = np.sum(diff_resids ** 2, axis=axis) / np.sum(resids ** 2, axis=axis)\n    return dw",
            "def durbin_watson(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculates the Durbin-Watson statistic.\\n\\n    Parameters\\n    ----------\\n    resids : array_like\\n        Data for which to compute the Durbin-Watson statistic. Usually\\n        regression model residuals.\\n    axis : int, optional\\n        Axis to use if data has more than 1 dimension. Default is 0.\\n\\n    Returns\\n    -------\\n    dw : float, array_like\\n        The Durbin-Watson statistic.\\n\\n    Notes\\n    -----\\n    The null hypothesis of the test is that there is no serial correlation\\n    in the residuals.\\n    The Durbin-Watson test statistic is defined as:\\n\\n    .. math::\\n\\n       \\\\sum_{t=2}^T((e_t - e_{t-1})^2)/\\\\sum_{t=1}^Te_t^2\\n\\n    The test statistic is approximately equal to 2*(1-r) where ``r`` is the\\n    sample autocorrelation of the residuals. Thus, for r == 0, indicating no\\n    serial correlation, the test statistic equals 2. This statistic will\\n    always be between 0 and 4. The closer to 0 the statistic, the more\\n    evidence for positive serial correlation. The closer to 4, the more\\n    evidence for negative serial correlation.\\n    '\n    resids = np.asarray(resids)\n    diff_resids = np.diff(resids, 1, axis=axis)\n    dw = np.sum(diff_resids ** 2, axis=axis) / np.sum(resids ** 2, axis=axis)\n    return dw",
            "def durbin_watson(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculates the Durbin-Watson statistic.\\n\\n    Parameters\\n    ----------\\n    resids : array_like\\n        Data for which to compute the Durbin-Watson statistic. Usually\\n        regression model residuals.\\n    axis : int, optional\\n        Axis to use if data has more than 1 dimension. Default is 0.\\n\\n    Returns\\n    -------\\n    dw : float, array_like\\n        The Durbin-Watson statistic.\\n\\n    Notes\\n    -----\\n    The null hypothesis of the test is that there is no serial correlation\\n    in the residuals.\\n    The Durbin-Watson test statistic is defined as:\\n\\n    .. math::\\n\\n       \\\\sum_{t=2}^T((e_t - e_{t-1})^2)/\\\\sum_{t=1}^Te_t^2\\n\\n    The test statistic is approximately equal to 2*(1-r) where ``r`` is the\\n    sample autocorrelation of the residuals. Thus, for r == 0, indicating no\\n    serial correlation, the test statistic equals 2. This statistic will\\n    always be between 0 and 4. The closer to 0 the statistic, the more\\n    evidence for positive serial correlation. The closer to 4, the more\\n    evidence for negative serial correlation.\\n    '\n    resids = np.asarray(resids)\n    diff_resids = np.diff(resids, 1, axis=axis)\n    dw = np.sum(diff_resids ** 2, axis=axis) / np.sum(resids ** 2, axis=axis)\n    return dw",
            "def durbin_watson(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculates the Durbin-Watson statistic.\\n\\n    Parameters\\n    ----------\\n    resids : array_like\\n        Data for which to compute the Durbin-Watson statistic. Usually\\n        regression model residuals.\\n    axis : int, optional\\n        Axis to use if data has more than 1 dimension. Default is 0.\\n\\n    Returns\\n    -------\\n    dw : float, array_like\\n        The Durbin-Watson statistic.\\n\\n    Notes\\n    -----\\n    The null hypothesis of the test is that there is no serial correlation\\n    in the residuals.\\n    The Durbin-Watson test statistic is defined as:\\n\\n    .. math::\\n\\n       \\\\sum_{t=2}^T((e_t - e_{t-1})^2)/\\\\sum_{t=1}^Te_t^2\\n\\n    The test statistic is approximately equal to 2*(1-r) where ``r`` is the\\n    sample autocorrelation of the residuals. Thus, for r == 0, indicating no\\n    serial correlation, the test statistic equals 2. This statistic will\\n    always be between 0 and 4. The closer to 0 the statistic, the more\\n    evidence for positive serial correlation. The closer to 4, the more\\n    evidence for negative serial correlation.\\n    '\n    resids = np.asarray(resids)\n    diff_resids = np.diff(resids, 1, axis=axis)\n    dw = np.sum(diff_resids ** 2, axis=axis) / np.sum(resids ** 2, axis=axis)\n    return dw",
            "def durbin_watson(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculates the Durbin-Watson statistic.\\n\\n    Parameters\\n    ----------\\n    resids : array_like\\n        Data for which to compute the Durbin-Watson statistic. Usually\\n        regression model residuals.\\n    axis : int, optional\\n        Axis to use if data has more than 1 dimension. Default is 0.\\n\\n    Returns\\n    -------\\n    dw : float, array_like\\n        The Durbin-Watson statistic.\\n\\n    Notes\\n    -----\\n    The null hypothesis of the test is that there is no serial correlation\\n    in the residuals.\\n    The Durbin-Watson test statistic is defined as:\\n\\n    .. math::\\n\\n       \\\\sum_{t=2}^T((e_t - e_{t-1})^2)/\\\\sum_{t=1}^Te_t^2\\n\\n    The test statistic is approximately equal to 2*(1-r) where ``r`` is the\\n    sample autocorrelation of the residuals. Thus, for r == 0, indicating no\\n    serial correlation, the test statistic equals 2. This statistic will\\n    always be between 0 and 4. The closer to 0 the statistic, the more\\n    evidence for positive serial correlation. The closer to 4, the more\\n    evidence for negative serial correlation.\\n    '\n    resids = np.asarray(resids)\n    diff_resids = np.diff(resids, 1, axis=axis)\n    dw = np.sum(diff_resids ** 2, axis=axis) / np.sum(resids ** 2, axis=axis)\n    return dw"
        ]
    },
    {
        "func_name": "omni_normtest",
        "original": "def omni_normtest(resids, axis=0):\n    \"\"\"\n    Omnibus test for normality\n\n    Parameters\n    ----------\n    resid : array_like\n    axis : int, optional\n        Default is 0\n\n    Returns\n    -------\n    Chi^2 score, two-tail probability\n    \"\"\"\n    resids = np.asarray(resids)\n    n = resids.shape[axis]\n    if n < 8:\n        from warnings import warn\n        warn('omni_normtest is not valid with less than 8 observations; %i samples were given.' % int(n), ValueWarning)\n        return (np.nan, np.nan)\n    return stats.normaltest(resids, axis=axis)",
        "mutated": [
            "def omni_normtest(resids, axis=0):\n    if False:\n        i = 10\n    '\\n    Omnibus test for normality\\n\\n    Parameters\\n    ----------\\n    resid : array_like\\n    axis : int, optional\\n        Default is 0\\n\\n    Returns\\n    -------\\n    Chi^2 score, two-tail probability\\n    '\n    resids = np.asarray(resids)\n    n = resids.shape[axis]\n    if n < 8:\n        from warnings import warn\n        warn('omni_normtest is not valid with less than 8 observations; %i samples were given.' % int(n), ValueWarning)\n        return (np.nan, np.nan)\n    return stats.normaltest(resids, axis=axis)",
            "def omni_normtest(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Omnibus test for normality\\n\\n    Parameters\\n    ----------\\n    resid : array_like\\n    axis : int, optional\\n        Default is 0\\n\\n    Returns\\n    -------\\n    Chi^2 score, two-tail probability\\n    '\n    resids = np.asarray(resids)\n    n = resids.shape[axis]\n    if n < 8:\n        from warnings import warn\n        warn('omni_normtest is not valid with less than 8 observations; %i samples were given.' % int(n), ValueWarning)\n        return (np.nan, np.nan)\n    return stats.normaltest(resids, axis=axis)",
            "def omni_normtest(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Omnibus test for normality\\n\\n    Parameters\\n    ----------\\n    resid : array_like\\n    axis : int, optional\\n        Default is 0\\n\\n    Returns\\n    -------\\n    Chi^2 score, two-tail probability\\n    '\n    resids = np.asarray(resids)\n    n = resids.shape[axis]\n    if n < 8:\n        from warnings import warn\n        warn('omni_normtest is not valid with less than 8 observations; %i samples were given.' % int(n), ValueWarning)\n        return (np.nan, np.nan)\n    return stats.normaltest(resids, axis=axis)",
            "def omni_normtest(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Omnibus test for normality\\n\\n    Parameters\\n    ----------\\n    resid : array_like\\n    axis : int, optional\\n        Default is 0\\n\\n    Returns\\n    -------\\n    Chi^2 score, two-tail probability\\n    '\n    resids = np.asarray(resids)\n    n = resids.shape[axis]\n    if n < 8:\n        from warnings import warn\n        warn('omni_normtest is not valid with less than 8 observations; %i samples were given.' % int(n), ValueWarning)\n        return (np.nan, np.nan)\n    return stats.normaltest(resids, axis=axis)",
            "def omni_normtest(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Omnibus test for normality\\n\\n    Parameters\\n    ----------\\n    resid : array_like\\n    axis : int, optional\\n        Default is 0\\n\\n    Returns\\n    -------\\n    Chi^2 score, two-tail probability\\n    '\n    resids = np.asarray(resids)\n    n = resids.shape[axis]\n    if n < 8:\n        from warnings import warn\n        warn('omni_normtest is not valid with less than 8 observations; %i samples were given.' % int(n), ValueWarning)\n        return (np.nan, np.nan)\n    return stats.normaltest(resids, axis=axis)"
        ]
    },
    {
        "func_name": "jarque_bera",
        "original": "def jarque_bera(resids, axis=0):\n    \"\"\"\n    The Jarque-Bera test of normality.\n\n    Parameters\n    ----------\n    resids : array_like\n        Data to test for normality. Usually regression model residuals that\n        are mean 0.\n    axis : int, optional\n        Axis to use if data has more than 1 dimension. Default is 0.\n\n    Returns\n    -------\n    JB : {float, ndarray}\n        The Jarque-Bera test statistic.\n    JBpv : {float, ndarray}\n        The pvalue of the test statistic.\n    skew : {float, ndarray}\n        Estimated skewness of the data.\n    kurtosis : {float, ndarray}\n        Estimated kurtosis of the data.\n\n    Notes\n    -----\n    Each output returned has 1 dimension fewer than data\n\n    The Jarque-Bera test statistic tests the null that the data is normally\n    distributed against an alternative that the data follow some other\n    distribution. The test statistic is based on two moments of the data,\n    the skewness, and the kurtosis, and has an asymptotic :math:`\\\\chi^2_2`\n    distribution.\n\n    The test statistic is defined\n\n    .. math:: JB = n(S^2/6+(K-3)^2/24)\n\n    where n is the number of data points, S is the sample skewness, and K is\n    the sample kurtosis of the data.\n    \"\"\"\n    resids = np.atleast_1d(np.asarray(resids, dtype=float))\n    if resids.size < 2:\n        raise ValueError('resids must contain at least 2 elements')\n    skew = stats.skew(resids, axis=axis)\n    kurtosis = 3 + stats.kurtosis(resids, axis=axis)\n    n = resids.shape[axis]\n    jb = n / 6.0 * (skew ** 2 + 1 / 4.0 * (kurtosis - 3) ** 2)\n    jb_pv = stats.chi2.sf(jb, 2)\n    return (jb, jb_pv, skew, kurtosis)",
        "mutated": [
            "def jarque_bera(resids, axis=0):\n    if False:\n        i = 10\n    '\\n    The Jarque-Bera test of normality.\\n\\n    Parameters\\n    ----------\\n    resids : array_like\\n        Data to test for normality. Usually regression model residuals that\\n        are mean 0.\\n    axis : int, optional\\n        Axis to use if data has more than 1 dimension. Default is 0.\\n\\n    Returns\\n    -------\\n    JB : {float, ndarray}\\n        The Jarque-Bera test statistic.\\n    JBpv : {float, ndarray}\\n        The pvalue of the test statistic.\\n    skew : {float, ndarray}\\n        Estimated skewness of the data.\\n    kurtosis : {float, ndarray}\\n        Estimated kurtosis of the data.\\n\\n    Notes\\n    -----\\n    Each output returned has 1 dimension fewer than data\\n\\n    The Jarque-Bera test statistic tests the null that the data is normally\\n    distributed against an alternative that the data follow some other\\n    distribution. The test statistic is based on two moments of the data,\\n    the skewness, and the kurtosis, and has an asymptotic :math:`\\\\chi^2_2`\\n    distribution.\\n\\n    The test statistic is defined\\n\\n    .. math:: JB = n(S^2/6+(K-3)^2/24)\\n\\n    where n is the number of data points, S is the sample skewness, and K is\\n    the sample kurtosis of the data.\\n    '\n    resids = np.atleast_1d(np.asarray(resids, dtype=float))\n    if resids.size < 2:\n        raise ValueError('resids must contain at least 2 elements')\n    skew = stats.skew(resids, axis=axis)\n    kurtosis = 3 + stats.kurtosis(resids, axis=axis)\n    n = resids.shape[axis]\n    jb = n / 6.0 * (skew ** 2 + 1 / 4.0 * (kurtosis - 3) ** 2)\n    jb_pv = stats.chi2.sf(jb, 2)\n    return (jb, jb_pv, skew, kurtosis)",
            "def jarque_bera(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    The Jarque-Bera test of normality.\\n\\n    Parameters\\n    ----------\\n    resids : array_like\\n        Data to test for normality. Usually regression model residuals that\\n        are mean 0.\\n    axis : int, optional\\n        Axis to use if data has more than 1 dimension. Default is 0.\\n\\n    Returns\\n    -------\\n    JB : {float, ndarray}\\n        The Jarque-Bera test statistic.\\n    JBpv : {float, ndarray}\\n        The pvalue of the test statistic.\\n    skew : {float, ndarray}\\n        Estimated skewness of the data.\\n    kurtosis : {float, ndarray}\\n        Estimated kurtosis of the data.\\n\\n    Notes\\n    -----\\n    Each output returned has 1 dimension fewer than data\\n\\n    The Jarque-Bera test statistic tests the null that the data is normally\\n    distributed against an alternative that the data follow some other\\n    distribution. The test statistic is based on two moments of the data,\\n    the skewness, and the kurtosis, and has an asymptotic :math:`\\\\chi^2_2`\\n    distribution.\\n\\n    The test statistic is defined\\n\\n    .. math:: JB = n(S^2/6+(K-3)^2/24)\\n\\n    where n is the number of data points, S is the sample skewness, and K is\\n    the sample kurtosis of the data.\\n    '\n    resids = np.atleast_1d(np.asarray(resids, dtype=float))\n    if resids.size < 2:\n        raise ValueError('resids must contain at least 2 elements')\n    skew = stats.skew(resids, axis=axis)\n    kurtosis = 3 + stats.kurtosis(resids, axis=axis)\n    n = resids.shape[axis]\n    jb = n / 6.0 * (skew ** 2 + 1 / 4.0 * (kurtosis - 3) ** 2)\n    jb_pv = stats.chi2.sf(jb, 2)\n    return (jb, jb_pv, skew, kurtosis)",
            "def jarque_bera(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    The Jarque-Bera test of normality.\\n\\n    Parameters\\n    ----------\\n    resids : array_like\\n        Data to test for normality. Usually regression model residuals that\\n        are mean 0.\\n    axis : int, optional\\n        Axis to use if data has more than 1 dimension. Default is 0.\\n\\n    Returns\\n    -------\\n    JB : {float, ndarray}\\n        The Jarque-Bera test statistic.\\n    JBpv : {float, ndarray}\\n        The pvalue of the test statistic.\\n    skew : {float, ndarray}\\n        Estimated skewness of the data.\\n    kurtosis : {float, ndarray}\\n        Estimated kurtosis of the data.\\n\\n    Notes\\n    -----\\n    Each output returned has 1 dimension fewer than data\\n\\n    The Jarque-Bera test statistic tests the null that the data is normally\\n    distributed against an alternative that the data follow some other\\n    distribution. The test statistic is based on two moments of the data,\\n    the skewness, and the kurtosis, and has an asymptotic :math:`\\\\chi^2_2`\\n    distribution.\\n\\n    The test statistic is defined\\n\\n    .. math:: JB = n(S^2/6+(K-3)^2/24)\\n\\n    where n is the number of data points, S is the sample skewness, and K is\\n    the sample kurtosis of the data.\\n    '\n    resids = np.atleast_1d(np.asarray(resids, dtype=float))\n    if resids.size < 2:\n        raise ValueError('resids must contain at least 2 elements')\n    skew = stats.skew(resids, axis=axis)\n    kurtosis = 3 + stats.kurtosis(resids, axis=axis)\n    n = resids.shape[axis]\n    jb = n / 6.0 * (skew ** 2 + 1 / 4.0 * (kurtosis - 3) ** 2)\n    jb_pv = stats.chi2.sf(jb, 2)\n    return (jb, jb_pv, skew, kurtosis)",
            "def jarque_bera(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    The Jarque-Bera test of normality.\\n\\n    Parameters\\n    ----------\\n    resids : array_like\\n        Data to test for normality. Usually regression model residuals that\\n        are mean 0.\\n    axis : int, optional\\n        Axis to use if data has more than 1 dimension. Default is 0.\\n\\n    Returns\\n    -------\\n    JB : {float, ndarray}\\n        The Jarque-Bera test statistic.\\n    JBpv : {float, ndarray}\\n        The pvalue of the test statistic.\\n    skew : {float, ndarray}\\n        Estimated skewness of the data.\\n    kurtosis : {float, ndarray}\\n        Estimated kurtosis of the data.\\n\\n    Notes\\n    -----\\n    Each output returned has 1 dimension fewer than data\\n\\n    The Jarque-Bera test statistic tests the null that the data is normally\\n    distributed against an alternative that the data follow some other\\n    distribution. The test statistic is based on two moments of the data,\\n    the skewness, and the kurtosis, and has an asymptotic :math:`\\\\chi^2_2`\\n    distribution.\\n\\n    The test statistic is defined\\n\\n    .. math:: JB = n(S^2/6+(K-3)^2/24)\\n\\n    where n is the number of data points, S is the sample skewness, and K is\\n    the sample kurtosis of the data.\\n    '\n    resids = np.atleast_1d(np.asarray(resids, dtype=float))\n    if resids.size < 2:\n        raise ValueError('resids must contain at least 2 elements')\n    skew = stats.skew(resids, axis=axis)\n    kurtosis = 3 + stats.kurtosis(resids, axis=axis)\n    n = resids.shape[axis]\n    jb = n / 6.0 * (skew ** 2 + 1 / 4.0 * (kurtosis - 3) ** 2)\n    jb_pv = stats.chi2.sf(jb, 2)\n    return (jb, jb_pv, skew, kurtosis)",
            "def jarque_bera(resids, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    The Jarque-Bera test of normality.\\n\\n    Parameters\\n    ----------\\n    resids : array_like\\n        Data to test for normality. Usually regression model residuals that\\n        are mean 0.\\n    axis : int, optional\\n        Axis to use if data has more than 1 dimension. Default is 0.\\n\\n    Returns\\n    -------\\n    JB : {float, ndarray}\\n        The Jarque-Bera test statistic.\\n    JBpv : {float, ndarray}\\n        The pvalue of the test statistic.\\n    skew : {float, ndarray}\\n        Estimated skewness of the data.\\n    kurtosis : {float, ndarray}\\n        Estimated kurtosis of the data.\\n\\n    Notes\\n    -----\\n    Each output returned has 1 dimension fewer than data\\n\\n    The Jarque-Bera test statistic tests the null that the data is normally\\n    distributed against an alternative that the data follow some other\\n    distribution. The test statistic is based on two moments of the data,\\n    the skewness, and the kurtosis, and has an asymptotic :math:`\\\\chi^2_2`\\n    distribution.\\n\\n    The test statistic is defined\\n\\n    .. math:: JB = n(S^2/6+(K-3)^2/24)\\n\\n    where n is the number of data points, S is the sample skewness, and K is\\n    the sample kurtosis of the data.\\n    '\n    resids = np.atleast_1d(np.asarray(resids, dtype=float))\n    if resids.size < 2:\n        raise ValueError('resids must contain at least 2 elements')\n    skew = stats.skew(resids, axis=axis)\n    kurtosis = 3 + stats.kurtosis(resids, axis=axis)\n    n = resids.shape[axis]\n    jb = n / 6.0 * (skew ** 2 + 1 / 4.0 * (kurtosis - 3) ** 2)\n    jb_pv = stats.chi2.sf(jb, 2)\n    return (jb, jb_pv, skew, kurtosis)"
        ]
    },
    {
        "func_name": "robust_skewness",
        "original": "def robust_skewness(y, axis=0):\n    \"\"\"\n    Calculates the four skewness measures in Kim & White\n\n    Parameters\n    ----------\n    y : array_like\n        Data to compute use in the estimator.\n    axis : int or None, optional\n        Axis along which the skewness measures are computed.  If `None`, the\n        entire array is used.\n\n    Returns\n    -------\n    sk1 : ndarray\n          The standard skewness estimator.\n    sk2 : ndarray\n          Skewness estimator based on quartiles.\n    sk3 : ndarray\n          Skewness estimator based on mean-median difference, standardized by\n          absolute deviation.\n    sk4 : ndarray\n          Skewness estimator based on mean-median difference, standardized by\n          standard deviation.\n\n    Notes\n    -----\n    The robust skewness measures are defined\n\n    .. math::\n\n        SK_{2}=\\\\frac{\\\\left(q_{.75}-q_{.5}\\\\right)\n        -\\\\left(q_{.5}-q_{.25}\\\\right)}{q_{.75}-q_{.25}}\n\n    .. math::\n\n        SK_{3}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}\n        {\\\\hat{E}\\\\left[\\\\left|y-\\\\hat{\\\\mu}\\\\right|\\\\right]}\n\n    .. math::\n\n        SK_{4}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}{\\\\hat{\\\\sigma}}\n\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\n       March 2004.\n    \"\"\"\n    if axis is None:\n        y = y.ravel()\n        axis = 0\n    y = np.sort(y, axis)\n    (q1, q2, q3) = np.percentile(y, [25.0, 50.0, 75.0], axis=axis)\n    mu = y.mean(axis)\n    shape = (y.size,)\n    if axis is not None:\n        shape = list(mu.shape)\n        shape.insert(axis, 1)\n        shape = tuple(shape)\n    mu_b = np.reshape(mu, shape)\n    q2_b = np.reshape(q2, shape)\n    sigma = np.sqrt(np.mean((y - mu_b) ** 2, axis))\n    sk1 = stats.skew(y, axis=axis)\n    sk2 = (q1 + q3 - 2.0 * q2) / (q3 - q1)\n    sk3 = (mu - q2) / np.mean(abs(y - q2_b), axis=axis)\n    sk4 = (mu - q2) / sigma\n    return (sk1, sk2, sk3, sk4)",
        "mutated": [
            "def robust_skewness(y, axis=0):\n    if False:\n        i = 10\n    '\\n    Calculates the four skewness measures in Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : int or None, optional\\n        Axis along which the skewness measures are computed.  If `None`, the\\n        entire array is used.\\n\\n    Returns\\n    -------\\n    sk1 : ndarray\\n          The standard skewness estimator.\\n    sk2 : ndarray\\n          Skewness estimator based on quartiles.\\n    sk3 : ndarray\\n          Skewness estimator based on mean-median difference, standardized by\\n          absolute deviation.\\n    sk4 : ndarray\\n          Skewness estimator based on mean-median difference, standardized by\\n          standard deviation.\\n\\n    Notes\\n    -----\\n    The robust skewness measures are defined\\n\\n    .. math::\\n\\n        SK_{2}=\\\\frac{\\\\left(q_{.75}-q_{.5}\\\\right)\\n        -\\\\left(q_{.5}-q_{.25}\\\\right)}{q_{.75}-q_{.25}}\\n\\n    .. math::\\n\\n        SK_{3}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}\\n        {\\\\hat{E}\\\\left[\\\\left|y-\\\\hat{\\\\mu}\\\\right|\\\\right]}\\n\\n    .. math::\\n\\n        SK_{4}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}{\\\\hat{\\\\sigma}}\\n\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    if axis is None:\n        y = y.ravel()\n        axis = 0\n    y = np.sort(y, axis)\n    (q1, q2, q3) = np.percentile(y, [25.0, 50.0, 75.0], axis=axis)\n    mu = y.mean(axis)\n    shape = (y.size,)\n    if axis is not None:\n        shape = list(mu.shape)\n        shape.insert(axis, 1)\n        shape = tuple(shape)\n    mu_b = np.reshape(mu, shape)\n    q2_b = np.reshape(q2, shape)\n    sigma = np.sqrt(np.mean((y - mu_b) ** 2, axis))\n    sk1 = stats.skew(y, axis=axis)\n    sk2 = (q1 + q3 - 2.0 * q2) / (q3 - q1)\n    sk3 = (mu - q2) / np.mean(abs(y - q2_b), axis=axis)\n    sk4 = (mu - q2) / sigma\n    return (sk1, sk2, sk3, sk4)",
            "def robust_skewness(y, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculates the four skewness measures in Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : int or None, optional\\n        Axis along which the skewness measures are computed.  If `None`, the\\n        entire array is used.\\n\\n    Returns\\n    -------\\n    sk1 : ndarray\\n          The standard skewness estimator.\\n    sk2 : ndarray\\n          Skewness estimator based on quartiles.\\n    sk3 : ndarray\\n          Skewness estimator based on mean-median difference, standardized by\\n          absolute deviation.\\n    sk4 : ndarray\\n          Skewness estimator based on mean-median difference, standardized by\\n          standard deviation.\\n\\n    Notes\\n    -----\\n    The robust skewness measures are defined\\n\\n    .. math::\\n\\n        SK_{2}=\\\\frac{\\\\left(q_{.75}-q_{.5}\\\\right)\\n        -\\\\left(q_{.5}-q_{.25}\\\\right)}{q_{.75}-q_{.25}}\\n\\n    .. math::\\n\\n        SK_{3}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}\\n        {\\\\hat{E}\\\\left[\\\\left|y-\\\\hat{\\\\mu}\\\\right|\\\\right]}\\n\\n    .. math::\\n\\n        SK_{4}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}{\\\\hat{\\\\sigma}}\\n\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    if axis is None:\n        y = y.ravel()\n        axis = 0\n    y = np.sort(y, axis)\n    (q1, q2, q3) = np.percentile(y, [25.0, 50.0, 75.0], axis=axis)\n    mu = y.mean(axis)\n    shape = (y.size,)\n    if axis is not None:\n        shape = list(mu.shape)\n        shape.insert(axis, 1)\n        shape = tuple(shape)\n    mu_b = np.reshape(mu, shape)\n    q2_b = np.reshape(q2, shape)\n    sigma = np.sqrt(np.mean((y - mu_b) ** 2, axis))\n    sk1 = stats.skew(y, axis=axis)\n    sk2 = (q1 + q3 - 2.0 * q2) / (q3 - q1)\n    sk3 = (mu - q2) / np.mean(abs(y - q2_b), axis=axis)\n    sk4 = (mu - q2) / sigma\n    return (sk1, sk2, sk3, sk4)",
            "def robust_skewness(y, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculates the four skewness measures in Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : int or None, optional\\n        Axis along which the skewness measures are computed.  If `None`, the\\n        entire array is used.\\n\\n    Returns\\n    -------\\n    sk1 : ndarray\\n          The standard skewness estimator.\\n    sk2 : ndarray\\n          Skewness estimator based on quartiles.\\n    sk3 : ndarray\\n          Skewness estimator based on mean-median difference, standardized by\\n          absolute deviation.\\n    sk4 : ndarray\\n          Skewness estimator based on mean-median difference, standardized by\\n          standard deviation.\\n\\n    Notes\\n    -----\\n    The robust skewness measures are defined\\n\\n    .. math::\\n\\n        SK_{2}=\\\\frac{\\\\left(q_{.75}-q_{.5}\\\\right)\\n        -\\\\left(q_{.5}-q_{.25}\\\\right)}{q_{.75}-q_{.25}}\\n\\n    .. math::\\n\\n        SK_{3}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}\\n        {\\\\hat{E}\\\\left[\\\\left|y-\\\\hat{\\\\mu}\\\\right|\\\\right]}\\n\\n    .. math::\\n\\n        SK_{4}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}{\\\\hat{\\\\sigma}}\\n\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    if axis is None:\n        y = y.ravel()\n        axis = 0\n    y = np.sort(y, axis)\n    (q1, q2, q3) = np.percentile(y, [25.0, 50.0, 75.0], axis=axis)\n    mu = y.mean(axis)\n    shape = (y.size,)\n    if axis is not None:\n        shape = list(mu.shape)\n        shape.insert(axis, 1)\n        shape = tuple(shape)\n    mu_b = np.reshape(mu, shape)\n    q2_b = np.reshape(q2, shape)\n    sigma = np.sqrt(np.mean((y - mu_b) ** 2, axis))\n    sk1 = stats.skew(y, axis=axis)\n    sk2 = (q1 + q3 - 2.0 * q2) / (q3 - q1)\n    sk3 = (mu - q2) / np.mean(abs(y - q2_b), axis=axis)\n    sk4 = (mu - q2) / sigma\n    return (sk1, sk2, sk3, sk4)",
            "def robust_skewness(y, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculates the four skewness measures in Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : int or None, optional\\n        Axis along which the skewness measures are computed.  If `None`, the\\n        entire array is used.\\n\\n    Returns\\n    -------\\n    sk1 : ndarray\\n          The standard skewness estimator.\\n    sk2 : ndarray\\n          Skewness estimator based on quartiles.\\n    sk3 : ndarray\\n          Skewness estimator based on mean-median difference, standardized by\\n          absolute deviation.\\n    sk4 : ndarray\\n          Skewness estimator based on mean-median difference, standardized by\\n          standard deviation.\\n\\n    Notes\\n    -----\\n    The robust skewness measures are defined\\n\\n    .. math::\\n\\n        SK_{2}=\\\\frac{\\\\left(q_{.75}-q_{.5}\\\\right)\\n        -\\\\left(q_{.5}-q_{.25}\\\\right)}{q_{.75}-q_{.25}}\\n\\n    .. math::\\n\\n        SK_{3}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}\\n        {\\\\hat{E}\\\\left[\\\\left|y-\\\\hat{\\\\mu}\\\\right|\\\\right]}\\n\\n    .. math::\\n\\n        SK_{4}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}{\\\\hat{\\\\sigma}}\\n\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    if axis is None:\n        y = y.ravel()\n        axis = 0\n    y = np.sort(y, axis)\n    (q1, q2, q3) = np.percentile(y, [25.0, 50.0, 75.0], axis=axis)\n    mu = y.mean(axis)\n    shape = (y.size,)\n    if axis is not None:\n        shape = list(mu.shape)\n        shape.insert(axis, 1)\n        shape = tuple(shape)\n    mu_b = np.reshape(mu, shape)\n    q2_b = np.reshape(q2, shape)\n    sigma = np.sqrt(np.mean((y - mu_b) ** 2, axis))\n    sk1 = stats.skew(y, axis=axis)\n    sk2 = (q1 + q3 - 2.0 * q2) / (q3 - q1)\n    sk3 = (mu - q2) / np.mean(abs(y - q2_b), axis=axis)\n    sk4 = (mu - q2) / sigma\n    return (sk1, sk2, sk3, sk4)",
            "def robust_skewness(y, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculates the four skewness measures in Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : int or None, optional\\n        Axis along which the skewness measures are computed.  If `None`, the\\n        entire array is used.\\n\\n    Returns\\n    -------\\n    sk1 : ndarray\\n          The standard skewness estimator.\\n    sk2 : ndarray\\n          Skewness estimator based on quartiles.\\n    sk3 : ndarray\\n          Skewness estimator based on mean-median difference, standardized by\\n          absolute deviation.\\n    sk4 : ndarray\\n          Skewness estimator based on mean-median difference, standardized by\\n          standard deviation.\\n\\n    Notes\\n    -----\\n    The robust skewness measures are defined\\n\\n    .. math::\\n\\n        SK_{2}=\\\\frac{\\\\left(q_{.75}-q_{.5}\\\\right)\\n        -\\\\left(q_{.5}-q_{.25}\\\\right)}{q_{.75}-q_{.25}}\\n\\n    .. math::\\n\\n        SK_{3}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}\\n        {\\\\hat{E}\\\\left[\\\\left|y-\\\\hat{\\\\mu}\\\\right|\\\\right]}\\n\\n    .. math::\\n\\n        SK_{4}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}{\\\\hat{\\\\sigma}}\\n\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    if axis is None:\n        y = y.ravel()\n        axis = 0\n    y = np.sort(y, axis)\n    (q1, q2, q3) = np.percentile(y, [25.0, 50.0, 75.0], axis=axis)\n    mu = y.mean(axis)\n    shape = (y.size,)\n    if axis is not None:\n        shape = list(mu.shape)\n        shape.insert(axis, 1)\n        shape = tuple(shape)\n    mu_b = np.reshape(mu, shape)\n    q2_b = np.reshape(q2, shape)\n    sigma = np.sqrt(np.mean((y - mu_b) ** 2, axis))\n    sk1 = stats.skew(y, axis=axis)\n    sk2 = (q1 + q3 - 2.0 * q2) / (q3 - q1)\n    sk3 = (mu - q2) / np.mean(abs(y - q2_b), axis=axis)\n    sk4 = (mu - q2) / sigma\n    return (sk1, sk2, sk3, sk4)"
        ]
    },
    {
        "func_name": "_kr3",
        "original": "def _kr3(y, alpha=5.0, beta=50.0):\n    \"\"\"\n    KR3 estimator from Kim & White\n\n    Parameters\n    ----------\n    y : array_like, 1-d\n        Data to compute use in the estimator.\n    alpha : float, optional\n        Lower cut-off for measuring expectation in tail.\n    beta :  float, optional\n        Lower cut-off for measuring expectation in center.\n\n    Returns\n    -------\n    kr3 : float\n        Robust kurtosis estimator based on standardized lower- and upper-tail\n        expected values\n\n    Notes\n    -----\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\n       March 2004.\n    \"\"\"\n    perc = (alpha, 100.0 - alpha, beta, 100.0 - beta)\n    (lower_alpha, upper_alpha, lower_beta, upper_beta) = np.percentile(y, perc)\n    l_alpha = np.mean(y[y < lower_alpha])\n    u_alpha = np.mean(y[y > upper_alpha])\n    l_beta = np.mean(y[y < lower_beta])\n    u_beta = np.mean(y[y > upper_beta])\n    return (u_alpha - l_alpha) / (u_beta - l_beta)",
        "mutated": [
            "def _kr3(y, alpha=5.0, beta=50.0):\n    if False:\n        i = 10\n    '\\n    KR3 estimator from Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like, 1-d\\n        Data to compute use in the estimator.\\n    alpha : float, optional\\n        Lower cut-off for measuring expectation in tail.\\n    beta :  float, optional\\n        Lower cut-off for measuring expectation in center.\\n\\n    Returns\\n    -------\\n    kr3 : float\\n        Robust kurtosis estimator based on standardized lower- and upper-tail\\n        expected values\\n\\n    Notes\\n    -----\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    perc = (alpha, 100.0 - alpha, beta, 100.0 - beta)\n    (lower_alpha, upper_alpha, lower_beta, upper_beta) = np.percentile(y, perc)\n    l_alpha = np.mean(y[y < lower_alpha])\n    u_alpha = np.mean(y[y > upper_alpha])\n    l_beta = np.mean(y[y < lower_beta])\n    u_beta = np.mean(y[y > upper_beta])\n    return (u_alpha - l_alpha) / (u_beta - l_beta)",
            "def _kr3(y, alpha=5.0, beta=50.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    KR3 estimator from Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like, 1-d\\n        Data to compute use in the estimator.\\n    alpha : float, optional\\n        Lower cut-off for measuring expectation in tail.\\n    beta :  float, optional\\n        Lower cut-off for measuring expectation in center.\\n\\n    Returns\\n    -------\\n    kr3 : float\\n        Robust kurtosis estimator based on standardized lower- and upper-tail\\n        expected values\\n\\n    Notes\\n    -----\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    perc = (alpha, 100.0 - alpha, beta, 100.0 - beta)\n    (lower_alpha, upper_alpha, lower_beta, upper_beta) = np.percentile(y, perc)\n    l_alpha = np.mean(y[y < lower_alpha])\n    u_alpha = np.mean(y[y > upper_alpha])\n    l_beta = np.mean(y[y < lower_beta])\n    u_beta = np.mean(y[y > upper_beta])\n    return (u_alpha - l_alpha) / (u_beta - l_beta)",
            "def _kr3(y, alpha=5.0, beta=50.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    KR3 estimator from Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like, 1-d\\n        Data to compute use in the estimator.\\n    alpha : float, optional\\n        Lower cut-off for measuring expectation in tail.\\n    beta :  float, optional\\n        Lower cut-off for measuring expectation in center.\\n\\n    Returns\\n    -------\\n    kr3 : float\\n        Robust kurtosis estimator based on standardized lower- and upper-tail\\n        expected values\\n\\n    Notes\\n    -----\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    perc = (alpha, 100.0 - alpha, beta, 100.0 - beta)\n    (lower_alpha, upper_alpha, lower_beta, upper_beta) = np.percentile(y, perc)\n    l_alpha = np.mean(y[y < lower_alpha])\n    u_alpha = np.mean(y[y > upper_alpha])\n    l_beta = np.mean(y[y < lower_beta])\n    u_beta = np.mean(y[y > upper_beta])\n    return (u_alpha - l_alpha) / (u_beta - l_beta)",
            "def _kr3(y, alpha=5.0, beta=50.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    KR3 estimator from Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like, 1-d\\n        Data to compute use in the estimator.\\n    alpha : float, optional\\n        Lower cut-off for measuring expectation in tail.\\n    beta :  float, optional\\n        Lower cut-off for measuring expectation in center.\\n\\n    Returns\\n    -------\\n    kr3 : float\\n        Robust kurtosis estimator based on standardized lower- and upper-tail\\n        expected values\\n\\n    Notes\\n    -----\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    perc = (alpha, 100.0 - alpha, beta, 100.0 - beta)\n    (lower_alpha, upper_alpha, lower_beta, upper_beta) = np.percentile(y, perc)\n    l_alpha = np.mean(y[y < lower_alpha])\n    u_alpha = np.mean(y[y > upper_alpha])\n    l_beta = np.mean(y[y < lower_beta])\n    u_beta = np.mean(y[y > upper_beta])\n    return (u_alpha - l_alpha) / (u_beta - l_beta)",
            "def _kr3(y, alpha=5.0, beta=50.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    KR3 estimator from Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like, 1-d\\n        Data to compute use in the estimator.\\n    alpha : float, optional\\n        Lower cut-off for measuring expectation in tail.\\n    beta :  float, optional\\n        Lower cut-off for measuring expectation in center.\\n\\n    Returns\\n    -------\\n    kr3 : float\\n        Robust kurtosis estimator based on standardized lower- and upper-tail\\n        expected values\\n\\n    Notes\\n    -----\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    perc = (alpha, 100.0 - alpha, beta, 100.0 - beta)\n    (lower_alpha, upper_alpha, lower_beta, upper_beta) = np.percentile(y, perc)\n    l_alpha = np.mean(y[y < lower_alpha])\n    u_alpha = np.mean(y[y > upper_alpha])\n    l_beta = np.mean(y[y < lower_beta])\n    u_beta = np.mean(y[y > upper_beta])\n    return (u_alpha - l_alpha) / (u_beta - l_beta)"
        ]
    },
    {
        "func_name": "expected_robust_kurtosis",
        "original": "def expected_robust_kurtosis(ab=(5.0, 50.0), dg=(2.5, 25.0)):\n    \"\"\"\n    Calculates the expected value of the robust kurtosis measures in Kim and\n    White assuming the data are normally distributed.\n\n    Parameters\n    ----------\n    ab : iterable, optional\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\n        quantile cut-off for measuring the extreme tail and beta is the central\n        quantile cutoff for the standardization of the measure\n    db : iterable, optional\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\n        quantile for measuring extreme values and gamma is the central quantile\n        used in the the standardization of the measure\n\n    Returns\n    -------\n    ekr : ndarray, 4-element\n        Contains the expected values of the 4 robust kurtosis measures\n\n    Notes\n    -----\n    See `robust_kurtosis` for definitions of the robust kurtosis measures\n    \"\"\"\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    expected_value = np.zeros(4)\n    ppf = stats.norm.ppf\n    pdf = stats.norm.pdf\n    (q1, q2, q3, q5, q6, q7) = ppf(np.array((1.0, 2.0, 3.0, 5.0, 6.0, 7.0)) / 8)\n    expected_value[0] = 3\n    expected_value[1] = (q7 - q5 + (q3 - q1)) / (q6 - q2)\n    (q_alpha, q_beta) = ppf(np.array((alpha / 100.0, beta / 100.0)))\n    expected_value[2] = 2 * pdf(q_alpha) / alpha / (2 * pdf(q_beta) / beta)\n    (q_delta, q_gamma) = ppf(np.array((delta / 100.0, gamma / 100.0)))\n    expected_value[3] = -2.0 * q_delta / (-2.0 * q_gamma)\n    return expected_value",
        "mutated": [
            "def expected_robust_kurtosis(ab=(5.0, 50.0), dg=(2.5, 25.0)):\n    if False:\n        i = 10\n    '\\n    Calculates the expected value of the robust kurtosis measures in Kim and\\n    White assuming the data are normally distributed.\\n\\n    Parameters\\n    ----------\\n    ab : iterable, optional\\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\\n        quantile cut-off for measuring the extreme tail and beta is the central\\n        quantile cutoff for the standardization of the measure\\n    db : iterable, optional\\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\\n        quantile for measuring extreme values and gamma is the central quantile\\n        used in the the standardization of the measure\\n\\n    Returns\\n    -------\\n    ekr : ndarray, 4-element\\n        Contains the expected values of the 4 robust kurtosis measures\\n\\n    Notes\\n    -----\\n    See `robust_kurtosis` for definitions of the robust kurtosis measures\\n    '\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    expected_value = np.zeros(4)\n    ppf = stats.norm.ppf\n    pdf = stats.norm.pdf\n    (q1, q2, q3, q5, q6, q7) = ppf(np.array((1.0, 2.0, 3.0, 5.0, 6.0, 7.0)) / 8)\n    expected_value[0] = 3\n    expected_value[1] = (q7 - q5 + (q3 - q1)) / (q6 - q2)\n    (q_alpha, q_beta) = ppf(np.array((alpha / 100.0, beta / 100.0)))\n    expected_value[2] = 2 * pdf(q_alpha) / alpha / (2 * pdf(q_beta) / beta)\n    (q_delta, q_gamma) = ppf(np.array((delta / 100.0, gamma / 100.0)))\n    expected_value[3] = -2.0 * q_delta / (-2.0 * q_gamma)\n    return expected_value",
            "def expected_robust_kurtosis(ab=(5.0, 50.0), dg=(2.5, 25.0)):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculates the expected value of the robust kurtosis measures in Kim and\\n    White assuming the data are normally distributed.\\n\\n    Parameters\\n    ----------\\n    ab : iterable, optional\\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\\n        quantile cut-off for measuring the extreme tail and beta is the central\\n        quantile cutoff for the standardization of the measure\\n    db : iterable, optional\\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\\n        quantile for measuring extreme values and gamma is the central quantile\\n        used in the the standardization of the measure\\n\\n    Returns\\n    -------\\n    ekr : ndarray, 4-element\\n        Contains the expected values of the 4 robust kurtosis measures\\n\\n    Notes\\n    -----\\n    See `robust_kurtosis` for definitions of the robust kurtosis measures\\n    '\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    expected_value = np.zeros(4)\n    ppf = stats.norm.ppf\n    pdf = stats.norm.pdf\n    (q1, q2, q3, q5, q6, q7) = ppf(np.array((1.0, 2.0, 3.0, 5.0, 6.0, 7.0)) / 8)\n    expected_value[0] = 3\n    expected_value[1] = (q7 - q5 + (q3 - q1)) / (q6 - q2)\n    (q_alpha, q_beta) = ppf(np.array((alpha / 100.0, beta / 100.0)))\n    expected_value[2] = 2 * pdf(q_alpha) / alpha / (2 * pdf(q_beta) / beta)\n    (q_delta, q_gamma) = ppf(np.array((delta / 100.0, gamma / 100.0)))\n    expected_value[3] = -2.0 * q_delta / (-2.0 * q_gamma)\n    return expected_value",
            "def expected_robust_kurtosis(ab=(5.0, 50.0), dg=(2.5, 25.0)):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculates the expected value of the robust kurtosis measures in Kim and\\n    White assuming the data are normally distributed.\\n\\n    Parameters\\n    ----------\\n    ab : iterable, optional\\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\\n        quantile cut-off for measuring the extreme tail and beta is the central\\n        quantile cutoff for the standardization of the measure\\n    db : iterable, optional\\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\\n        quantile for measuring extreme values and gamma is the central quantile\\n        used in the the standardization of the measure\\n\\n    Returns\\n    -------\\n    ekr : ndarray, 4-element\\n        Contains the expected values of the 4 robust kurtosis measures\\n\\n    Notes\\n    -----\\n    See `robust_kurtosis` for definitions of the robust kurtosis measures\\n    '\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    expected_value = np.zeros(4)\n    ppf = stats.norm.ppf\n    pdf = stats.norm.pdf\n    (q1, q2, q3, q5, q6, q7) = ppf(np.array((1.0, 2.0, 3.0, 5.0, 6.0, 7.0)) / 8)\n    expected_value[0] = 3\n    expected_value[1] = (q7 - q5 + (q3 - q1)) / (q6 - q2)\n    (q_alpha, q_beta) = ppf(np.array((alpha / 100.0, beta / 100.0)))\n    expected_value[2] = 2 * pdf(q_alpha) / alpha / (2 * pdf(q_beta) / beta)\n    (q_delta, q_gamma) = ppf(np.array((delta / 100.0, gamma / 100.0)))\n    expected_value[3] = -2.0 * q_delta / (-2.0 * q_gamma)\n    return expected_value",
            "def expected_robust_kurtosis(ab=(5.0, 50.0), dg=(2.5, 25.0)):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculates the expected value of the robust kurtosis measures in Kim and\\n    White assuming the data are normally distributed.\\n\\n    Parameters\\n    ----------\\n    ab : iterable, optional\\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\\n        quantile cut-off for measuring the extreme tail and beta is the central\\n        quantile cutoff for the standardization of the measure\\n    db : iterable, optional\\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\\n        quantile for measuring extreme values and gamma is the central quantile\\n        used in the the standardization of the measure\\n\\n    Returns\\n    -------\\n    ekr : ndarray, 4-element\\n        Contains the expected values of the 4 robust kurtosis measures\\n\\n    Notes\\n    -----\\n    See `robust_kurtosis` for definitions of the robust kurtosis measures\\n    '\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    expected_value = np.zeros(4)\n    ppf = stats.norm.ppf\n    pdf = stats.norm.pdf\n    (q1, q2, q3, q5, q6, q7) = ppf(np.array((1.0, 2.0, 3.0, 5.0, 6.0, 7.0)) / 8)\n    expected_value[0] = 3\n    expected_value[1] = (q7 - q5 + (q3 - q1)) / (q6 - q2)\n    (q_alpha, q_beta) = ppf(np.array((alpha / 100.0, beta / 100.0)))\n    expected_value[2] = 2 * pdf(q_alpha) / alpha / (2 * pdf(q_beta) / beta)\n    (q_delta, q_gamma) = ppf(np.array((delta / 100.0, gamma / 100.0)))\n    expected_value[3] = -2.0 * q_delta / (-2.0 * q_gamma)\n    return expected_value",
            "def expected_robust_kurtosis(ab=(5.0, 50.0), dg=(2.5, 25.0)):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculates the expected value of the robust kurtosis measures in Kim and\\n    White assuming the data are normally distributed.\\n\\n    Parameters\\n    ----------\\n    ab : iterable, optional\\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\\n        quantile cut-off for measuring the extreme tail and beta is the central\\n        quantile cutoff for the standardization of the measure\\n    db : iterable, optional\\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\\n        quantile for measuring extreme values and gamma is the central quantile\\n        used in the the standardization of the measure\\n\\n    Returns\\n    -------\\n    ekr : ndarray, 4-element\\n        Contains the expected values of the 4 robust kurtosis measures\\n\\n    Notes\\n    -----\\n    See `robust_kurtosis` for definitions of the robust kurtosis measures\\n    '\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    expected_value = np.zeros(4)\n    ppf = stats.norm.ppf\n    pdf = stats.norm.pdf\n    (q1, q2, q3, q5, q6, q7) = ppf(np.array((1.0, 2.0, 3.0, 5.0, 6.0, 7.0)) / 8)\n    expected_value[0] = 3\n    expected_value[1] = (q7 - q5 + (q3 - q1)) / (q6 - q2)\n    (q_alpha, q_beta) = ppf(np.array((alpha / 100.0, beta / 100.0)))\n    expected_value[2] = 2 * pdf(q_alpha) / alpha / (2 * pdf(q_beta) / beta)\n    (q_delta, q_gamma) = ppf(np.array((delta / 100.0, gamma / 100.0)))\n    expected_value[3] = -2.0 * q_delta / (-2.0 * q_gamma)\n    return expected_value"
        ]
    },
    {
        "func_name": "robust_kurtosis",
        "original": "def robust_kurtosis(y, axis=0, ab=(5.0, 50.0), dg=(2.5, 25.0), excess=True):\n    \"\"\"\n    Calculates the four kurtosis measures in Kim & White\n\n    Parameters\n    ----------\n    y : array_like\n        Data to compute use in the estimator.\n    axis : int or None, optional\n        Axis along which the kurtosis are computed.  If `None`, the\n        entire array is used.\n    a iterable, optional\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\n        quantile cut-off for measuring the extreme tail and beta is the central\n        quantile cutoff for the standardization of the measure\n    db : iterable, optional\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\n        quantile for measuring extreme values and gamma is the central quantile\n        used in the the standardization of the measure\n    excess : bool, optional\n        If true (default), computed values are excess of those for a standard\n        normal distribution.\n\n    Returns\n    -------\n    kr1 : ndarray\n          The standard kurtosis estimator.\n    kr2 : ndarray\n          Kurtosis estimator based on octiles.\n    kr3 : ndarray\n          Kurtosis estimators based on exceedance expectations.\n    kr4 : ndarray\n          Kurtosis measure based on the spread between high and low quantiles.\n\n    Notes\n    -----\n    The robust kurtosis measures are defined\n\n    .. math::\n\n        KR_{2}=\\\\frac{\\\\left(\\\\hat{q}_{.875}-\\\\hat{q}_{.625}\\\\right)\n        +\\\\left(\\\\hat{q}_{.375}-\\\\hat{q}_{.125}\\\\right)}\n        {\\\\hat{q}_{.75}-\\\\hat{q}_{.25}}\n\n    .. math::\n\n        KR_{3}=\\\\frac{\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\alpha}\\\\right)\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\alpha}\\\\right)}\n        {\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\beta}\\\\right)\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\beta}\\\\right)}\n\n    .. math::\n\n        KR_{4}=\\\\frac{\\\\hat{q}_{1-\\\\delta}-\\\\hat{q}_{\\\\delta}}\n        {\\\\hat{q}_{1-\\\\gamma}-\\\\hat{q}_{\\\\gamma}}\n\n    where :math:`\\\\hat{q}_{p}` is the estimated quantile at :math:`p`.\n\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\n       March 2004.\n    \"\"\"\n    if axis is None or (y.squeeze().ndim == 1 and y.ndim != 1):\n        y = y.ravel()\n        axis = 0\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    perc = (12.5, 25.0, 37.5, 62.5, 75.0, 87.5, delta, 100.0 - delta, gamma, 100.0 - gamma)\n    (e1, e2, e3, e5, e6, e7, fd, f1md, fg, f1mg) = np.percentile(y, perc, axis=axis)\n    expected_value = expected_robust_kurtosis(ab, dg) if excess else np.zeros(4)\n    kr1 = stats.kurtosis(y, axis, False) - expected_value[0]\n    kr2 = (e7 - e5 + (e3 - e1)) / (e6 - e2) - expected_value[1]\n    if y.ndim == 1:\n        kr3 = _kr3(y, alpha, beta)\n    else:\n        kr3 = np.apply_along_axis(_kr3, axis, y, alpha, beta)\n    kr3 -= expected_value[2]\n    kr4 = (f1md - fd) / (f1mg - fg) - expected_value[3]\n    return (kr1, kr2, kr3, kr4)",
        "mutated": [
            "def robust_kurtosis(y, axis=0, ab=(5.0, 50.0), dg=(2.5, 25.0), excess=True):\n    if False:\n        i = 10\n    '\\n    Calculates the four kurtosis measures in Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : int or None, optional\\n        Axis along which the kurtosis are computed.  If `None`, the\\n        entire array is used.\\n    a iterable, optional\\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\\n        quantile cut-off for measuring the extreme tail and beta is the central\\n        quantile cutoff for the standardization of the measure\\n    db : iterable, optional\\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\\n        quantile for measuring extreme values and gamma is the central quantile\\n        used in the the standardization of the measure\\n    excess : bool, optional\\n        If true (default), computed values are excess of those for a standard\\n        normal distribution.\\n\\n    Returns\\n    -------\\n    kr1 : ndarray\\n          The standard kurtosis estimator.\\n    kr2 : ndarray\\n          Kurtosis estimator based on octiles.\\n    kr3 : ndarray\\n          Kurtosis estimators based on exceedance expectations.\\n    kr4 : ndarray\\n          Kurtosis measure based on the spread between high and low quantiles.\\n\\n    Notes\\n    -----\\n    The robust kurtosis measures are defined\\n\\n    .. math::\\n\\n        KR_{2}=\\\\frac{\\\\left(\\\\hat{q}_{.875}-\\\\hat{q}_{.625}\\\\right)\\n        +\\\\left(\\\\hat{q}_{.375}-\\\\hat{q}_{.125}\\\\right)}\\n        {\\\\hat{q}_{.75}-\\\\hat{q}_{.25}}\\n\\n    .. math::\\n\\n        KR_{3}=\\\\frac{\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\alpha}\\\\right)\\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\alpha}\\\\right)}\\n        {\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\beta}\\\\right)\\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\beta}\\\\right)}\\n\\n    .. math::\\n\\n        KR_{4}=\\\\frac{\\\\hat{q}_{1-\\\\delta}-\\\\hat{q}_{\\\\delta}}\\n        {\\\\hat{q}_{1-\\\\gamma}-\\\\hat{q}_{\\\\gamma}}\\n\\n    where :math:`\\\\hat{q}_{p}` is the estimated quantile at :math:`p`.\\n\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    if axis is None or (y.squeeze().ndim == 1 and y.ndim != 1):\n        y = y.ravel()\n        axis = 0\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    perc = (12.5, 25.0, 37.5, 62.5, 75.0, 87.5, delta, 100.0 - delta, gamma, 100.0 - gamma)\n    (e1, e2, e3, e5, e6, e7, fd, f1md, fg, f1mg) = np.percentile(y, perc, axis=axis)\n    expected_value = expected_robust_kurtosis(ab, dg) if excess else np.zeros(4)\n    kr1 = stats.kurtosis(y, axis, False) - expected_value[0]\n    kr2 = (e7 - e5 + (e3 - e1)) / (e6 - e2) - expected_value[1]\n    if y.ndim == 1:\n        kr3 = _kr3(y, alpha, beta)\n    else:\n        kr3 = np.apply_along_axis(_kr3, axis, y, alpha, beta)\n    kr3 -= expected_value[2]\n    kr4 = (f1md - fd) / (f1mg - fg) - expected_value[3]\n    return (kr1, kr2, kr3, kr4)",
            "def robust_kurtosis(y, axis=0, ab=(5.0, 50.0), dg=(2.5, 25.0), excess=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculates the four kurtosis measures in Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : int or None, optional\\n        Axis along which the kurtosis are computed.  If `None`, the\\n        entire array is used.\\n    a iterable, optional\\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\\n        quantile cut-off for measuring the extreme tail and beta is the central\\n        quantile cutoff for the standardization of the measure\\n    db : iterable, optional\\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\\n        quantile for measuring extreme values and gamma is the central quantile\\n        used in the the standardization of the measure\\n    excess : bool, optional\\n        If true (default), computed values are excess of those for a standard\\n        normal distribution.\\n\\n    Returns\\n    -------\\n    kr1 : ndarray\\n          The standard kurtosis estimator.\\n    kr2 : ndarray\\n          Kurtosis estimator based on octiles.\\n    kr3 : ndarray\\n          Kurtosis estimators based on exceedance expectations.\\n    kr4 : ndarray\\n          Kurtosis measure based on the spread between high and low quantiles.\\n\\n    Notes\\n    -----\\n    The robust kurtosis measures are defined\\n\\n    .. math::\\n\\n        KR_{2}=\\\\frac{\\\\left(\\\\hat{q}_{.875}-\\\\hat{q}_{.625}\\\\right)\\n        +\\\\left(\\\\hat{q}_{.375}-\\\\hat{q}_{.125}\\\\right)}\\n        {\\\\hat{q}_{.75}-\\\\hat{q}_{.25}}\\n\\n    .. math::\\n\\n        KR_{3}=\\\\frac{\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\alpha}\\\\right)\\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\alpha}\\\\right)}\\n        {\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\beta}\\\\right)\\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\beta}\\\\right)}\\n\\n    .. math::\\n\\n        KR_{4}=\\\\frac{\\\\hat{q}_{1-\\\\delta}-\\\\hat{q}_{\\\\delta}}\\n        {\\\\hat{q}_{1-\\\\gamma}-\\\\hat{q}_{\\\\gamma}}\\n\\n    where :math:`\\\\hat{q}_{p}` is the estimated quantile at :math:`p`.\\n\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    if axis is None or (y.squeeze().ndim == 1 and y.ndim != 1):\n        y = y.ravel()\n        axis = 0\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    perc = (12.5, 25.0, 37.5, 62.5, 75.0, 87.5, delta, 100.0 - delta, gamma, 100.0 - gamma)\n    (e1, e2, e3, e5, e6, e7, fd, f1md, fg, f1mg) = np.percentile(y, perc, axis=axis)\n    expected_value = expected_robust_kurtosis(ab, dg) if excess else np.zeros(4)\n    kr1 = stats.kurtosis(y, axis, False) - expected_value[0]\n    kr2 = (e7 - e5 + (e3 - e1)) / (e6 - e2) - expected_value[1]\n    if y.ndim == 1:\n        kr3 = _kr3(y, alpha, beta)\n    else:\n        kr3 = np.apply_along_axis(_kr3, axis, y, alpha, beta)\n    kr3 -= expected_value[2]\n    kr4 = (f1md - fd) / (f1mg - fg) - expected_value[3]\n    return (kr1, kr2, kr3, kr4)",
            "def robust_kurtosis(y, axis=0, ab=(5.0, 50.0), dg=(2.5, 25.0), excess=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculates the four kurtosis measures in Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : int or None, optional\\n        Axis along which the kurtosis are computed.  If `None`, the\\n        entire array is used.\\n    a iterable, optional\\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\\n        quantile cut-off for measuring the extreme tail and beta is the central\\n        quantile cutoff for the standardization of the measure\\n    db : iterable, optional\\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\\n        quantile for measuring extreme values and gamma is the central quantile\\n        used in the the standardization of the measure\\n    excess : bool, optional\\n        If true (default), computed values are excess of those for a standard\\n        normal distribution.\\n\\n    Returns\\n    -------\\n    kr1 : ndarray\\n          The standard kurtosis estimator.\\n    kr2 : ndarray\\n          Kurtosis estimator based on octiles.\\n    kr3 : ndarray\\n          Kurtosis estimators based on exceedance expectations.\\n    kr4 : ndarray\\n          Kurtosis measure based on the spread between high and low quantiles.\\n\\n    Notes\\n    -----\\n    The robust kurtosis measures are defined\\n\\n    .. math::\\n\\n        KR_{2}=\\\\frac{\\\\left(\\\\hat{q}_{.875}-\\\\hat{q}_{.625}\\\\right)\\n        +\\\\left(\\\\hat{q}_{.375}-\\\\hat{q}_{.125}\\\\right)}\\n        {\\\\hat{q}_{.75}-\\\\hat{q}_{.25}}\\n\\n    .. math::\\n\\n        KR_{3}=\\\\frac{\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\alpha}\\\\right)\\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\alpha}\\\\right)}\\n        {\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\beta}\\\\right)\\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\beta}\\\\right)}\\n\\n    .. math::\\n\\n        KR_{4}=\\\\frac{\\\\hat{q}_{1-\\\\delta}-\\\\hat{q}_{\\\\delta}}\\n        {\\\\hat{q}_{1-\\\\gamma}-\\\\hat{q}_{\\\\gamma}}\\n\\n    where :math:`\\\\hat{q}_{p}` is the estimated quantile at :math:`p`.\\n\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    if axis is None or (y.squeeze().ndim == 1 and y.ndim != 1):\n        y = y.ravel()\n        axis = 0\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    perc = (12.5, 25.0, 37.5, 62.5, 75.0, 87.5, delta, 100.0 - delta, gamma, 100.0 - gamma)\n    (e1, e2, e3, e5, e6, e7, fd, f1md, fg, f1mg) = np.percentile(y, perc, axis=axis)\n    expected_value = expected_robust_kurtosis(ab, dg) if excess else np.zeros(4)\n    kr1 = stats.kurtosis(y, axis, False) - expected_value[0]\n    kr2 = (e7 - e5 + (e3 - e1)) / (e6 - e2) - expected_value[1]\n    if y.ndim == 1:\n        kr3 = _kr3(y, alpha, beta)\n    else:\n        kr3 = np.apply_along_axis(_kr3, axis, y, alpha, beta)\n    kr3 -= expected_value[2]\n    kr4 = (f1md - fd) / (f1mg - fg) - expected_value[3]\n    return (kr1, kr2, kr3, kr4)",
            "def robust_kurtosis(y, axis=0, ab=(5.0, 50.0), dg=(2.5, 25.0), excess=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculates the four kurtosis measures in Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : int or None, optional\\n        Axis along which the kurtosis are computed.  If `None`, the\\n        entire array is used.\\n    a iterable, optional\\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\\n        quantile cut-off for measuring the extreme tail and beta is the central\\n        quantile cutoff for the standardization of the measure\\n    db : iterable, optional\\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\\n        quantile for measuring extreme values and gamma is the central quantile\\n        used in the the standardization of the measure\\n    excess : bool, optional\\n        If true (default), computed values are excess of those for a standard\\n        normal distribution.\\n\\n    Returns\\n    -------\\n    kr1 : ndarray\\n          The standard kurtosis estimator.\\n    kr2 : ndarray\\n          Kurtosis estimator based on octiles.\\n    kr3 : ndarray\\n          Kurtosis estimators based on exceedance expectations.\\n    kr4 : ndarray\\n          Kurtosis measure based on the spread between high and low quantiles.\\n\\n    Notes\\n    -----\\n    The robust kurtosis measures are defined\\n\\n    .. math::\\n\\n        KR_{2}=\\\\frac{\\\\left(\\\\hat{q}_{.875}-\\\\hat{q}_{.625}\\\\right)\\n        +\\\\left(\\\\hat{q}_{.375}-\\\\hat{q}_{.125}\\\\right)}\\n        {\\\\hat{q}_{.75}-\\\\hat{q}_{.25}}\\n\\n    .. math::\\n\\n        KR_{3}=\\\\frac{\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\alpha}\\\\right)\\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\alpha}\\\\right)}\\n        {\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\beta}\\\\right)\\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\beta}\\\\right)}\\n\\n    .. math::\\n\\n        KR_{4}=\\\\frac{\\\\hat{q}_{1-\\\\delta}-\\\\hat{q}_{\\\\delta}}\\n        {\\\\hat{q}_{1-\\\\gamma}-\\\\hat{q}_{\\\\gamma}}\\n\\n    where :math:`\\\\hat{q}_{p}` is the estimated quantile at :math:`p`.\\n\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    if axis is None or (y.squeeze().ndim == 1 and y.ndim != 1):\n        y = y.ravel()\n        axis = 0\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    perc = (12.5, 25.0, 37.5, 62.5, 75.0, 87.5, delta, 100.0 - delta, gamma, 100.0 - gamma)\n    (e1, e2, e3, e5, e6, e7, fd, f1md, fg, f1mg) = np.percentile(y, perc, axis=axis)\n    expected_value = expected_robust_kurtosis(ab, dg) if excess else np.zeros(4)\n    kr1 = stats.kurtosis(y, axis, False) - expected_value[0]\n    kr2 = (e7 - e5 + (e3 - e1)) / (e6 - e2) - expected_value[1]\n    if y.ndim == 1:\n        kr3 = _kr3(y, alpha, beta)\n    else:\n        kr3 = np.apply_along_axis(_kr3, axis, y, alpha, beta)\n    kr3 -= expected_value[2]\n    kr4 = (f1md - fd) / (f1mg - fg) - expected_value[3]\n    return (kr1, kr2, kr3, kr4)",
            "def robust_kurtosis(y, axis=0, ab=(5.0, 50.0), dg=(2.5, 25.0), excess=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculates the four kurtosis measures in Kim & White\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : int or None, optional\\n        Axis along which the kurtosis are computed.  If `None`, the\\n        entire array is used.\\n    a iterable, optional\\n        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail\\n        quantile cut-off for measuring the extreme tail and beta is the central\\n        quantile cutoff for the standardization of the measure\\n    db : iterable, optional\\n        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail\\n        quantile for measuring extreme values and gamma is the central quantile\\n        used in the the standardization of the measure\\n    excess : bool, optional\\n        If true (default), computed values are excess of those for a standard\\n        normal distribution.\\n\\n    Returns\\n    -------\\n    kr1 : ndarray\\n          The standard kurtosis estimator.\\n    kr2 : ndarray\\n          Kurtosis estimator based on octiles.\\n    kr3 : ndarray\\n          Kurtosis estimators based on exceedance expectations.\\n    kr4 : ndarray\\n          Kurtosis measure based on the spread between high and low quantiles.\\n\\n    Notes\\n    -----\\n    The robust kurtosis measures are defined\\n\\n    .. math::\\n\\n        KR_{2}=\\\\frac{\\\\left(\\\\hat{q}_{.875}-\\\\hat{q}_{.625}\\\\right)\\n        +\\\\left(\\\\hat{q}_{.375}-\\\\hat{q}_{.125}\\\\right)}\\n        {\\\\hat{q}_{.75}-\\\\hat{q}_{.25}}\\n\\n    .. math::\\n\\n        KR_{3}=\\\\frac{\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\alpha}\\\\right)\\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\alpha}\\\\right)}\\n        {\\\\hat{E}\\\\left(y|y>\\\\hat{q}_{1-\\\\beta}\\\\right)\\n        -\\\\hat{E}\\\\left(y|y<\\\\hat{q}_{\\\\beta}\\\\right)}\\n\\n    .. math::\\n\\n        KR_{4}=\\\\frac{\\\\hat{q}_{1-\\\\delta}-\\\\hat{q}_{\\\\delta}}\\n        {\\\\hat{q}_{1-\\\\gamma}-\\\\hat{q}_{\\\\gamma}}\\n\\n    where :math:`\\\\hat{q}_{p}` is the estimated quantile at :math:`p`.\\n\\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\\n       March 2004.\\n    '\n    if axis is None or (y.squeeze().ndim == 1 and y.ndim != 1):\n        y = y.ravel()\n        axis = 0\n    (alpha, beta) = ab\n    (delta, gamma) = dg\n    perc = (12.5, 25.0, 37.5, 62.5, 75.0, 87.5, delta, 100.0 - delta, gamma, 100.0 - gamma)\n    (e1, e2, e3, e5, e6, e7, fd, f1md, fg, f1mg) = np.percentile(y, perc, axis=axis)\n    expected_value = expected_robust_kurtosis(ab, dg) if excess else np.zeros(4)\n    kr1 = stats.kurtosis(y, axis, False) - expected_value[0]\n    kr2 = (e7 - e5 + (e3 - e1)) / (e6 - e2) - expected_value[1]\n    if y.ndim == 1:\n        kr3 = _kr3(y, alpha, beta)\n    else:\n        kr3 = np.apply_along_axis(_kr3, axis, y, alpha, beta)\n    kr3 -= expected_value[2]\n    kr4 = (f1md - fd) / (f1mg - fg) - expected_value[3]\n    return (kr1, kr2, kr3, kr4)"
        ]
    },
    {
        "func_name": "_medcouple_1d",
        "original": "def _medcouple_1d(y):\n    \"\"\"\n    Calculates the medcouple robust measure of skew.\n\n    Parameters\n    ----------\n    y : array_like, 1-d\n        Data to compute use in the estimator.\n\n    Returns\n    -------\n    mc : float\n        The medcouple statistic\n\n    Notes\n    -----\n    The current algorithm requires a O(N**2) memory allocations, and so may\n    not work for very large arrays (N>10000).\n\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\n       5186-5201, August 2008.\n    \"\"\"\n    y = np.squeeze(np.asarray(y))\n    if y.ndim != 1:\n        raise ValueError('y must be squeezable to a 1-d array')\n    y = np.sort(y)\n    n = y.shape[0]\n    if n % 2 == 0:\n        mf = (y[n // 2 - 1] + y[n // 2]) / 2\n    else:\n        mf = y[(n - 1) // 2]\n    z = y - mf\n    lower = z[z <= 0.0]\n    upper = z[z >= 0.0]\n    upper = upper[:, None]\n    standardization = upper - lower\n    is_zero = np.logical_and(lower == 0.0, upper == 0.0)\n    standardization[is_zero] = np.inf\n    spread = upper + lower\n    h = spread / standardization\n    num_ties = np.sum(lower == 0.0)\n    if num_ties:\n        replacements = np.ones((num_ties, num_ties)) - np.eye(num_ties)\n        replacements -= 2 * np.triu(replacements)\n        replacements = np.fliplr(replacements)\n        h[:num_ties, -num_ties:] = replacements\n    return np.median(h)",
        "mutated": [
            "def _medcouple_1d(y):\n    if False:\n        i = 10\n    '\\n    Calculates the medcouple robust measure of skew.\\n\\n    Parameters\\n    ----------\\n    y : array_like, 1-d\\n        Data to compute use in the estimator.\\n\\n    Returns\\n    -------\\n    mc : float\\n        The medcouple statistic\\n\\n    Notes\\n    -----\\n    The current algorithm requires a O(N**2) memory allocations, and so may\\n    not work for very large arrays (N>10000).\\n\\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\\n       5186-5201, August 2008.\\n    '\n    y = np.squeeze(np.asarray(y))\n    if y.ndim != 1:\n        raise ValueError('y must be squeezable to a 1-d array')\n    y = np.sort(y)\n    n = y.shape[0]\n    if n % 2 == 0:\n        mf = (y[n // 2 - 1] + y[n // 2]) / 2\n    else:\n        mf = y[(n - 1) // 2]\n    z = y - mf\n    lower = z[z <= 0.0]\n    upper = z[z >= 0.0]\n    upper = upper[:, None]\n    standardization = upper - lower\n    is_zero = np.logical_and(lower == 0.0, upper == 0.0)\n    standardization[is_zero] = np.inf\n    spread = upper + lower\n    h = spread / standardization\n    num_ties = np.sum(lower == 0.0)\n    if num_ties:\n        replacements = np.ones((num_ties, num_ties)) - np.eye(num_ties)\n        replacements -= 2 * np.triu(replacements)\n        replacements = np.fliplr(replacements)\n        h[:num_ties, -num_ties:] = replacements\n    return np.median(h)",
            "def _medcouple_1d(y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculates the medcouple robust measure of skew.\\n\\n    Parameters\\n    ----------\\n    y : array_like, 1-d\\n        Data to compute use in the estimator.\\n\\n    Returns\\n    -------\\n    mc : float\\n        The medcouple statistic\\n\\n    Notes\\n    -----\\n    The current algorithm requires a O(N**2) memory allocations, and so may\\n    not work for very large arrays (N>10000).\\n\\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\\n       5186-5201, August 2008.\\n    '\n    y = np.squeeze(np.asarray(y))\n    if y.ndim != 1:\n        raise ValueError('y must be squeezable to a 1-d array')\n    y = np.sort(y)\n    n = y.shape[0]\n    if n % 2 == 0:\n        mf = (y[n // 2 - 1] + y[n // 2]) / 2\n    else:\n        mf = y[(n - 1) // 2]\n    z = y - mf\n    lower = z[z <= 0.0]\n    upper = z[z >= 0.0]\n    upper = upper[:, None]\n    standardization = upper - lower\n    is_zero = np.logical_and(lower == 0.0, upper == 0.0)\n    standardization[is_zero] = np.inf\n    spread = upper + lower\n    h = spread / standardization\n    num_ties = np.sum(lower == 0.0)\n    if num_ties:\n        replacements = np.ones((num_ties, num_ties)) - np.eye(num_ties)\n        replacements -= 2 * np.triu(replacements)\n        replacements = np.fliplr(replacements)\n        h[:num_ties, -num_ties:] = replacements\n    return np.median(h)",
            "def _medcouple_1d(y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculates the medcouple robust measure of skew.\\n\\n    Parameters\\n    ----------\\n    y : array_like, 1-d\\n        Data to compute use in the estimator.\\n\\n    Returns\\n    -------\\n    mc : float\\n        The medcouple statistic\\n\\n    Notes\\n    -----\\n    The current algorithm requires a O(N**2) memory allocations, and so may\\n    not work for very large arrays (N>10000).\\n\\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\\n       5186-5201, August 2008.\\n    '\n    y = np.squeeze(np.asarray(y))\n    if y.ndim != 1:\n        raise ValueError('y must be squeezable to a 1-d array')\n    y = np.sort(y)\n    n = y.shape[0]\n    if n % 2 == 0:\n        mf = (y[n // 2 - 1] + y[n // 2]) / 2\n    else:\n        mf = y[(n - 1) // 2]\n    z = y - mf\n    lower = z[z <= 0.0]\n    upper = z[z >= 0.0]\n    upper = upper[:, None]\n    standardization = upper - lower\n    is_zero = np.logical_and(lower == 0.0, upper == 0.0)\n    standardization[is_zero] = np.inf\n    spread = upper + lower\n    h = spread / standardization\n    num_ties = np.sum(lower == 0.0)\n    if num_ties:\n        replacements = np.ones((num_ties, num_ties)) - np.eye(num_ties)\n        replacements -= 2 * np.triu(replacements)\n        replacements = np.fliplr(replacements)\n        h[:num_ties, -num_ties:] = replacements\n    return np.median(h)",
            "def _medcouple_1d(y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculates the medcouple robust measure of skew.\\n\\n    Parameters\\n    ----------\\n    y : array_like, 1-d\\n        Data to compute use in the estimator.\\n\\n    Returns\\n    -------\\n    mc : float\\n        The medcouple statistic\\n\\n    Notes\\n    -----\\n    The current algorithm requires a O(N**2) memory allocations, and so may\\n    not work for very large arrays (N>10000).\\n\\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\\n       5186-5201, August 2008.\\n    '\n    y = np.squeeze(np.asarray(y))\n    if y.ndim != 1:\n        raise ValueError('y must be squeezable to a 1-d array')\n    y = np.sort(y)\n    n = y.shape[0]\n    if n % 2 == 0:\n        mf = (y[n // 2 - 1] + y[n // 2]) / 2\n    else:\n        mf = y[(n - 1) // 2]\n    z = y - mf\n    lower = z[z <= 0.0]\n    upper = z[z >= 0.0]\n    upper = upper[:, None]\n    standardization = upper - lower\n    is_zero = np.logical_and(lower == 0.0, upper == 0.0)\n    standardization[is_zero] = np.inf\n    spread = upper + lower\n    h = spread / standardization\n    num_ties = np.sum(lower == 0.0)\n    if num_ties:\n        replacements = np.ones((num_ties, num_ties)) - np.eye(num_ties)\n        replacements -= 2 * np.triu(replacements)\n        replacements = np.fliplr(replacements)\n        h[:num_ties, -num_ties:] = replacements\n    return np.median(h)",
            "def _medcouple_1d(y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculates the medcouple robust measure of skew.\\n\\n    Parameters\\n    ----------\\n    y : array_like, 1-d\\n        Data to compute use in the estimator.\\n\\n    Returns\\n    -------\\n    mc : float\\n        The medcouple statistic\\n\\n    Notes\\n    -----\\n    The current algorithm requires a O(N**2) memory allocations, and so may\\n    not work for very large arrays (N>10000).\\n\\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\\n       5186-5201, August 2008.\\n    '\n    y = np.squeeze(np.asarray(y))\n    if y.ndim != 1:\n        raise ValueError('y must be squeezable to a 1-d array')\n    y = np.sort(y)\n    n = y.shape[0]\n    if n % 2 == 0:\n        mf = (y[n // 2 - 1] + y[n // 2]) / 2\n    else:\n        mf = y[(n - 1) // 2]\n    z = y - mf\n    lower = z[z <= 0.0]\n    upper = z[z >= 0.0]\n    upper = upper[:, None]\n    standardization = upper - lower\n    is_zero = np.logical_and(lower == 0.0, upper == 0.0)\n    standardization[is_zero] = np.inf\n    spread = upper + lower\n    h = spread / standardization\n    num_ties = np.sum(lower == 0.0)\n    if num_ties:\n        replacements = np.ones((num_ties, num_ties)) - np.eye(num_ties)\n        replacements -= 2 * np.triu(replacements)\n        replacements = np.fliplr(replacements)\n        h[:num_ties, -num_ties:] = replacements\n    return np.median(h)"
        ]
    },
    {
        "func_name": "medcouple",
        "original": "def medcouple(y, axis=0):\n    \"\"\"\n    Calculate the medcouple robust measure of skew.\n\n    Parameters\n    ----------\n    y : array_like\n        Data to compute use in the estimator.\n    axis : {int, None}\n        Axis along which the medcouple statistic is computed.  If `None`, the\n        entire array is used.\n\n    Returns\n    -------\n    mc : ndarray\n        The medcouple statistic with the same shape as `y`, with the specified\n        axis removed.\n\n    Notes\n    -----\n    The current algorithm requires a O(N**2) memory allocations, and so may\n    not work for very large arrays (N>10000).\n\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\n       5186-5201, August 2008.\n    \"\"\"\n    y = np.asarray(y, dtype=np.double)\n    if axis is None:\n        return _medcouple_1d(y.ravel())\n    return np.apply_along_axis(_medcouple_1d, axis, y)",
        "mutated": [
            "def medcouple(y, axis=0):\n    if False:\n        i = 10\n    '\\n    Calculate the medcouple robust measure of skew.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : {int, None}\\n        Axis along which the medcouple statistic is computed.  If `None`, the\\n        entire array is used.\\n\\n    Returns\\n    -------\\n    mc : ndarray\\n        The medcouple statistic with the same shape as `y`, with the specified\\n        axis removed.\\n\\n    Notes\\n    -----\\n    The current algorithm requires a O(N**2) memory allocations, and so may\\n    not work for very large arrays (N>10000).\\n\\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\\n       5186-5201, August 2008.\\n    '\n    y = np.asarray(y, dtype=np.double)\n    if axis is None:\n        return _medcouple_1d(y.ravel())\n    return np.apply_along_axis(_medcouple_1d, axis, y)",
            "def medcouple(y, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculate the medcouple robust measure of skew.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : {int, None}\\n        Axis along which the medcouple statistic is computed.  If `None`, the\\n        entire array is used.\\n\\n    Returns\\n    -------\\n    mc : ndarray\\n        The medcouple statistic with the same shape as `y`, with the specified\\n        axis removed.\\n\\n    Notes\\n    -----\\n    The current algorithm requires a O(N**2) memory allocations, and so may\\n    not work for very large arrays (N>10000).\\n\\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\\n       5186-5201, August 2008.\\n    '\n    y = np.asarray(y, dtype=np.double)\n    if axis is None:\n        return _medcouple_1d(y.ravel())\n    return np.apply_along_axis(_medcouple_1d, axis, y)",
            "def medcouple(y, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculate the medcouple robust measure of skew.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : {int, None}\\n        Axis along which the medcouple statistic is computed.  If `None`, the\\n        entire array is used.\\n\\n    Returns\\n    -------\\n    mc : ndarray\\n        The medcouple statistic with the same shape as `y`, with the specified\\n        axis removed.\\n\\n    Notes\\n    -----\\n    The current algorithm requires a O(N**2) memory allocations, and so may\\n    not work for very large arrays (N>10000).\\n\\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\\n       5186-5201, August 2008.\\n    '\n    y = np.asarray(y, dtype=np.double)\n    if axis is None:\n        return _medcouple_1d(y.ravel())\n    return np.apply_along_axis(_medcouple_1d, axis, y)",
            "def medcouple(y, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculate the medcouple robust measure of skew.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : {int, None}\\n        Axis along which the medcouple statistic is computed.  If `None`, the\\n        entire array is used.\\n\\n    Returns\\n    -------\\n    mc : ndarray\\n        The medcouple statistic with the same shape as `y`, with the specified\\n        axis removed.\\n\\n    Notes\\n    -----\\n    The current algorithm requires a O(N**2) memory allocations, and so may\\n    not work for very large arrays (N>10000).\\n\\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\\n       5186-5201, August 2008.\\n    '\n    y = np.asarray(y, dtype=np.double)\n    if axis is None:\n        return _medcouple_1d(y.ravel())\n    return np.apply_along_axis(_medcouple_1d, axis, y)",
            "def medcouple(y, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculate the medcouple robust measure of skew.\\n\\n    Parameters\\n    ----------\\n    y : array_like\\n        Data to compute use in the estimator.\\n    axis : {int, None}\\n        Axis along which the medcouple statistic is computed.  If `None`, the\\n        entire array is used.\\n\\n    Returns\\n    -------\\n    mc : ndarray\\n        The medcouple statistic with the same shape as `y`, with the specified\\n        axis removed.\\n\\n    Notes\\n    -----\\n    The current algorithm requires a O(N**2) memory allocations, and so may\\n    not work for very large arrays (N>10000).\\n\\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\\n       5186-5201, August 2008.\\n    '\n    y = np.asarray(y, dtype=np.double)\n    if axis is None:\n        return _medcouple_1d(y.ravel())\n    return np.apply_along_axis(_medcouple_1d, axis, y)"
        ]
    }
]