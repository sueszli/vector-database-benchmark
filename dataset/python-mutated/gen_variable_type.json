[
    {
        "func_name": "wrapper_registrations",
        "original": "def wrapper_registrations(used_keys: Set[str]) -> str:\n    library_impl_macro_list: List[str] = []\n    for key in sorted(used_keys):\n        dispatch_key = key\n        if key == 'Default':\n            dispatch_key = 'Autograd'\n        library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n        library_impl_macro_list += [library_impl_macro]\n    return '\\n\\n'.join(library_impl_macro_list)",
        "mutated": [
            "def wrapper_registrations(used_keys: Set[str]) -> str:\n    if False:\n        i = 10\n    library_impl_macro_list: List[str] = []\n    for key in sorted(used_keys):\n        dispatch_key = key\n        if key == 'Default':\n            dispatch_key = 'Autograd'\n        library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n        library_impl_macro_list += [library_impl_macro]\n    return '\\n\\n'.join(library_impl_macro_list)",
            "def wrapper_registrations(used_keys: Set[str]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    library_impl_macro_list: List[str] = []\n    for key in sorted(used_keys):\n        dispatch_key = key\n        if key == 'Default':\n            dispatch_key = 'Autograd'\n        library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n        library_impl_macro_list += [library_impl_macro]\n    return '\\n\\n'.join(library_impl_macro_list)",
            "def wrapper_registrations(used_keys: Set[str]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    library_impl_macro_list: List[str] = []\n    for key in sorted(used_keys):\n        dispatch_key = key\n        if key == 'Default':\n            dispatch_key = 'Autograd'\n        library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n        library_impl_macro_list += [library_impl_macro]\n    return '\\n\\n'.join(library_impl_macro_list)",
            "def wrapper_registrations(used_keys: Set[str]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    library_impl_macro_list: List[str] = []\n    for key in sorted(used_keys):\n        dispatch_key = key\n        if key == 'Default':\n            dispatch_key = 'Autograd'\n        library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n        library_impl_macro_list += [library_impl_macro]\n    return '\\n\\n'.join(library_impl_macro_list)",
            "def wrapper_registrations(used_keys: Set[str]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    library_impl_macro_list: List[str] = []\n    for key in sorted(used_keys):\n        dispatch_key = key\n        if key == 'Default':\n            dispatch_key = 'Autograd'\n        library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n        library_impl_macro_list += [library_impl_macro]\n    return '\\n\\n'.join(library_impl_macro_list)"
        ]
    },
    {
        "func_name": "gen_variable_type",
        "original": "def gen_variable_type(out: str, native_yaml_path: str, tags_yaml_path: str, fns_with_diff_infos: List[NativeFunctionWithDifferentiabilityInfo], template_path: str, used_keys: Set[str]) -> None:\n    \"\"\"VariableType.h and VariableType.cpp body\n\n    This is the at::Type subclass for differentiable tensors. The\n    implementation of each function dispatches to the base tensor type to\n    compute the output. The grad_fn is attached to differentiable functions.\n    \"\"\"\n    fm = FileManager(install_dir=out, template_dir=template_path, dry_run=False)\n    fm.write('VariableType.h', lambda : {'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.h'})\n\n    def wrapper_registrations(used_keys: Set[str]) -> str:\n        library_impl_macro_list: List[str] = []\n        for key in sorted(used_keys):\n            dispatch_key = key\n            if key == 'Default':\n                dispatch_key = 'Autograd'\n            library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n            library_impl_macro_list += [library_impl_macro]\n        return '\\n\\n'.join(library_impl_macro_list)\n    fm1 = FileManager(install_dir=out + '/templates', template_dir=template_path, dry_run=False)\n    fm1.write('VariableType.cpp', lambda : {'type_derived_method_definitions': '\\n\\n'.join(['${' + f'type_derived_method_definitions_{key}' + '}' for key in sorted(used_keys)]), 'wrapper_registrations': wrapper_registrations(used_keys)})\n    fm2 = FileManager(install_dir=out, template_dir=out + '/templates', dry_run=False)\n    sharded_keys = set([f'type_derived_method_definitions_{key}' for key in sorted(used_keys)] + [f'wrapper_registrations_{key}' for key in sorted(used_keys)])\n    fm2.write_sharded('VariableType.cpp', [fn for fn in fns_with_diff_infos if use_derived(fn)], key_fn=lambda fn: cpp.name(fn.func.func), base_env={'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.cpp'}, env_callable=gen_variable_type_func, num_shards=5, sharded_keys=sharded_keys)",
        "mutated": [
            "def gen_variable_type(out: str, native_yaml_path: str, tags_yaml_path: str, fns_with_diff_infos: List[NativeFunctionWithDifferentiabilityInfo], template_path: str, used_keys: Set[str]) -> None:\n    if False:\n        i = 10\n    'VariableType.h and VariableType.cpp body\\n\\n    This is the at::Type subclass for differentiable tensors. The\\n    implementation of each function dispatches to the base tensor type to\\n    compute the output. The grad_fn is attached to differentiable functions.\\n    '\n    fm = FileManager(install_dir=out, template_dir=template_path, dry_run=False)\n    fm.write('VariableType.h', lambda : {'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.h'})\n\n    def wrapper_registrations(used_keys: Set[str]) -> str:\n        library_impl_macro_list: List[str] = []\n        for key in sorted(used_keys):\n            dispatch_key = key\n            if key == 'Default':\n                dispatch_key = 'Autograd'\n            library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n            library_impl_macro_list += [library_impl_macro]\n        return '\\n\\n'.join(library_impl_macro_list)\n    fm1 = FileManager(install_dir=out + '/templates', template_dir=template_path, dry_run=False)\n    fm1.write('VariableType.cpp', lambda : {'type_derived_method_definitions': '\\n\\n'.join(['${' + f'type_derived_method_definitions_{key}' + '}' for key in sorted(used_keys)]), 'wrapper_registrations': wrapper_registrations(used_keys)})\n    fm2 = FileManager(install_dir=out, template_dir=out + '/templates', dry_run=False)\n    sharded_keys = set([f'type_derived_method_definitions_{key}' for key in sorted(used_keys)] + [f'wrapper_registrations_{key}' for key in sorted(used_keys)])\n    fm2.write_sharded('VariableType.cpp', [fn for fn in fns_with_diff_infos if use_derived(fn)], key_fn=lambda fn: cpp.name(fn.func.func), base_env={'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.cpp'}, env_callable=gen_variable_type_func, num_shards=5, sharded_keys=sharded_keys)",
            "def gen_variable_type(out: str, native_yaml_path: str, tags_yaml_path: str, fns_with_diff_infos: List[NativeFunctionWithDifferentiabilityInfo], template_path: str, used_keys: Set[str]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'VariableType.h and VariableType.cpp body\\n\\n    This is the at::Type subclass for differentiable tensors. The\\n    implementation of each function dispatches to the base tensor type to\\n    compute the output. The grad_fn is attached to differentiable functions.\\n    '\n    fm = FileManager(install_dir=out, template_dir=template_path, dry_run=False)\n    fm.write('VariableType.h', lambda : {'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.h'})\n\n    def wrapper_registrations(used_keys: Set[str]) -> str:\n        library_impl_macro_list: List[str] = []\n        for key in sorted(used_keys):\n            dispatch_key = key\n            if key == 'Default':\n                dispatch_key = 'Autograd'\n            library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n            library_impl_macro_list += [library_impl_macro]\n        return '\\n\\n'.join(library_impl_macro_list)\n    fm1 = FileManager(install_dir=out + '/templates', template_dir=template_path, dry_run=False)\n    fm1.write('VariableType.cpp', lambda : {'type_derived_method_definitions': '\\n\\n'.join(['${' + f'type_derived_method_definitions_{key}' + '}' for key in sorted(used_keys)]), 'wrapper_registrations': wrapper_registrations(used_keys)})\n    fm2 = FileManager(install_dir=out, template_dir=out + '/templates', dry_run=False)\n    sharded_keys = set([f'type_derived_method_definitions_{key}' for key in sorted(used_keys)] + [f'wrapper_registrations_{key}' for key in sorted(used_keys)])\n    fm2.write_sharded('VariableType.cpp', [fn for fn in fns_with_diff_infos if use_derived(fn)], key_fn=lambda fn: cpp.name(fn.func.func), base_env={'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.cpp'}, env_callable=gen_variable_type_func, num_shards=5, sharded_keys=sharded_keys)",
            "def gen_variable_type(out: str, native_yaml_path: str, tags_yaml_path: str, fns_with_diff_infos: List[NativeFunctionWithDifferentiabilityInfo], template_path: str, used_keys: Set[str]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'VariableType.h and VariableType.cpp body\\n\\n    This is the at::Type subclass for differentiable tensors. The\\n    implementation of each function dispatches to the base tensor type to\\n    compute the output. The grad_fn is attached to differentiable functions.\\n    '\n    fm = FileManager(install_dir=out, template_dir=template_path, dry_run=False)\n    fm.write('VariableType.h', lambda : {'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.h'})\n\n    def wrapper_registrations(used_keys: Set[str]) -> str:\n        library_impl_macro_list: List[str] = []\n        for key in sorted(used_keys):\n            dispatch_key = key\n            if key == 'Default':\n                dispatch_key = 'Autograd'\n            library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n            library_impl_macro_list += [library_impl_macro]\n        return '\\n\\n'.join(library_impl_macro_list)\n    fm1 = FileManager(install_dir=out + '/templates', template_dir=template_path, dry_run=False)\n    fm1.write('VariableType.cpp', lambda : {'type_derived_method_definitions': '\\n\\n'.join(['${' + f'type_derived_method_definitions_{key}' + '}' for key in sorted(used_keys)]), 'wrapper_registrations': wrapper_registrations(used_keys)})\n    fm2 = FileManager(install_dir=out, template_dir=out + '/templates', dry_run=False)\n    sharded_keys = set([f'type_derived_method_definitions_{key}' for key in sorted(used_keys)] + [f'wrapper_registrations_{key}' for key in sorted(used_keys)])\n    fm2.write_sharded('VariableType.cpp', [fn for fn in fns_with_diff_infos if use_derived(fn)], key_fn=lambda fn: cpp.name(fn.func.func), base_env={'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.cpp'}, env_callable=gen_variable_type_func, num_shards=5, sharded_keys=sharded_keys)",
            "def gen_variable_type(out: str, native_yaml_path: str, tags_yaml_path: str, fns_with_diff_infos: List[NativeFunctionWithDifferentiabilityInfo], template_path: str, used_keys: Set[str]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'VariableType.h and VariableType.cpp body\\n\\n    This is the at::Type subclass for differentiable tensors. The\\n    implementation of each function dispatches to the base tensor type to\\n    compute the output. The grad_fn is attached to differentiable functions.\\n    '\n    fm = FileManager(install_dir=out, template_dir=template_path, dry_run=False)\n    fm.write('VariableType.h', lambda : {'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.h'})\n\n    def wrapper_registrations(used_keys: Set[str]) -> str:\n        library_impl_macro_list: List[str] = []\n        for key in sorted(used_keys):\n            dispatch_key = key\n            if key == 'Default':\n                dispatch_key = 'Autograd'\n            library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n            library_impl_macro_list += [library_impl_macro]\n        return '\\n\\n'.join(library_impl_macro_list)\n    fm1 = FileManager(install_dir=out + '/templates', template_dir=template_path, dry_run=False)\n    fm1.write('VariableType.cpp', lambda : {'type_derived_method_definitions': '\\n\\n'.join(['${' + f'type_derived_method_definitions_{key}' + '}' for key in sorted(used_keys)]), 'wrapper_registrations': wrapper_registrations(used_keys)})\n    fm2 = FileManager(install_dir=out, template_dir=out + '/templates', dry_run=False)\n    sharded_keys = set([f'type_derived_method_definitions_{key}' for key in sorted(used_keys)] + [f'wrapper_registrations_{key}' for key in sorted(used_keys)])\n    fm2.write_sharded('VariableType.cpp', [fn for fn in fns_with_diff_infos if use_derived(fn)], key_fn=lambda fn: cpp.name(fn.func.func), base_env={'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.cpp'}, env_callable=gen_variable_type_func, num_shards=5, sharded_keys=sharded_keys)",
            "def gen_variable_type(out: str, native_yaml_path: str, tags_yaml_path: str, fns_with_diff_infos: List[NativeFunctionWithDifferentiabilityInfo], template_path: str, used_keys: Set[str]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'VariableType.h and VariableType.cpp body\\n\\n    This is the at::Type subclass for differentiable tensors. The\\n    implementation of each function dispatches to the base tensor type to\\n    compute the output. The grad_fn is attached to differentiable functions.\\n    '\n    fm = FileManager(install_dir=out, template_dir=template_path, dry_run=False)\n    fm.write('VariableType.h', lambda : {'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.h'})\n\n    def wrapper_registrations(used_keys: Set[str]) -> str:\n        library_impl_macro_list: List[str] = []\n        for key in sorted(used_keys):\n            dispatch_key = key\n            if key == 'Default':\n                dispatch_key = 'Autograd'\n            library_impl_macro = f'TORCH_LIBRARY_IMPL(aten, {dispatch_key}, m) ' + '{\\n' + '${' + f'wrapper_registrations_{key}' + '}\\n}'\n            library_impl_macro_list += [library_impl_macro]\n        return '\\n\\n'.join(library_impl_macro_list)\n    fm1 = FileManager(install_dir=out + '/templates', template_dir=template_path, dry_run=False)\n    fm1.write('VariableType.cpp', lambda : {'type_derived_method_definitions': '\\n\\n'.join(['${' + f'type_derived_method_definitions_{key}' + '}' for key in sorted(used_keys)]), 'wrapper_registrations': wrapper_registrations(used_keys)})\n    fm2 = FileManager(install_dir=out, template_dir=out + '/templates', dry_run=False)\n    sharded_keys = set([f'type_derived_method_definitions_{key}' for key in sorted(used_keys)] + [f'wrapper_registrations_{key}' for key in sorted(used_keys)])\n    fm2.write_sharded('VariableType.cpp', [fn for fn in fns_with_diff_infos if use_derived(fn)], key_fn=lambda fn: cpp.name(fn.func.func), base_env={'generated_comment': '@' + f'generated from {fm.template_dir_for_comments()}/VariableType.cpp'}, env_callable=gen_variable_type_func, num_shards=5, sharded_keys=sharded_keys)"
        ]
    },
    {
        "func_name": "gen_wrapper_registration",
        "original": "@with_native_function_and\ndef gen_wrapper_registration(f: NativeFunction, key: str='Default') -> str:\n    return WRAPPER_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name, type_wrapper_name=type_wrapper_name(f, key), class_type='VariableType')",
        "mutated": [
            "@with_native_function_and\ndef gen_wrapper_registration(f: NativeFunction, key: str='Default') -> str:\n    if False:\n        i = 10\n    return WRAPPER_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name, type_wrapper_name=type_wrapper_name(f, key), class_type='VariableType')",
            "@with_native_function_and\ndef gen_wrapper_registration(f: NativeFunction, key: str='Default') -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return WRAPPER_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name, type_wrapper_name=type_wrapper_name(f, key), class_type='VariableType')",
            "@with_native_function_and\ndef gen_wrapper_registration(f: NativeFunction, key: str='Default') -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return WRAPPER_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name, type_wrapper_name=type_wrapper_name(f, key), class_type='VariableType')",
            "@with_native_function_and\ndef gen_wrapper_registration(f: NativeFunction, key: str='Default') -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return WRAPPER_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name, type_wrapper_name=type_wrapper_name(f, key), class_type='VariableType')",
            "@with_native_function_and\ndef gen_wrapper_registration(f: NativeFunction, key: str='Default') -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return WRAPPER_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name, type_wrapper_name=type_wrapper_name(f, key), class_type='VariableType')"
        ]
    },
    {
        "func_name": "gen_variable_type_func",
        "original": "def gen_variable_type_func(fn: NativeFunctionWithDifferentiabilityInfo) -> Dict[str, List[str]]:\n    f = fn.func\n    result = {}\n    with native_function_manager(f):\n        name = cpp.name(f.func)\n        formals = gen_formals(f)\n        if fn.info is None and str(f.func.name.name) not in RESET_GRAD_ACCUMULATOR and (get_base_name(f) not in DONT_REQUIRE_DERIVATIVE) and (len(gen_differentiable_outputs(fn)) > 0) and (cpp.name(f.func) not in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE) and (type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT) and (type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT):\n            type_definition = ''\n            wrapper_registration = AUTOGRAD_NOT_IMPLEMENTED_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name)\n            result['type_derived_method_definitions_Default'] = [type_definition]\n            result['wrapper_registrations_Default'] = [wrapper_registration]\n        elif not fn.info:\n            key = 'Default'\n            type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n            wrapper_registration = gen_wrapper_registration(f, key)\n            result[f'type_derived_method_definitions_{key}'] = [type_definition]\n            result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n        else:\n            for key in fn.info.keys():\n                type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n                wrapper_registration = gen_wrapper_registration(f, key)\n                result[f'type_derived_method_definitions_{key}'] = [type_definition]\n                result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n    assert (name in MANUAL_BACKEND) == f.manual_kernel_registration\n    if name in MANUAL_AUTOGRAD_AND_TRACER or (fn.info and any((info.has_derivatives for info in fn.info.values()))):\n        msg = f\"There's a formula for {name}(or its functional variant) in derivatives.yaml. It's required to add a dispatch section for it with explicit supported backends e.g CPU/CUDA or CompositeExplicitAutograd in native_functions.yaml. Please see https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword for instructions to choose the right dispatch keyword.\"\n        assert f.is_abstract, msg\n    return result",
        "mutated": [
            "def gen_variable_type_func(fn: NativeFunctionWithDifferentiabilityInfo) -> Dict[str, List[str]]:\n    if False:\n        i = 10\n    f = fn.func\n    result = {}\n    with native_function_manager(f):\n        name = cpp.name(f.func)\n        formals = gen_formals(f)\n        if fn.info is None and str(f.func.name.name) not in RESET_GRAD_ACCUMULATOR and (get_base_name(f) not in DONT_REQUIRE_DERIVATIVE) and (len(gen_differentiable_outputs(fn)) > 0) and (cpp.name(f.func) not in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE) and (type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT) and (type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT):\n            type_definition = ''\n            wrapper_registration = AUTOGRAD_NOT_IMPLEMENTED_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name)\n            result['type_derived_method_definitions_Default'] = [type_definition]\n            result['wrapper_registrations_Default'] = [wrapper_registration]\n        elif not fn.info:\n            key = 'Default'\n            type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n            wrapper_registration = gen_wrapper_registration(f, key)\n            result[f'type_derived_method_definitions_{key}'] = [type_definition]\n            result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n        else:\n            for key in fn.info.keys():\n                type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n                wrapper_registration = gen_wrapper_registration(f, key)\n                result[f'type_derived_method_definitions_{key}'] = [type_definition]\n                result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n    assert (name in MANUAL_BACKEND) == f.manual_kernel_registration\n    if name in MANUAL_AUTOGRAD_AND_TRACER or (fn.info and any((info.has_derivatives for info in fn.info.values()))):\n        msg = f\"There's a formula for {name}(or its functional variant) in derivatives.yaml. It's required to add a dispatch section for it with explicit supported backends e.g CPU/CUDA or CompositeExplicitAutograd in native_functions.yaml. Please see https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword for instructions to choose the right dispatch keyword.\"\n        assert f.is_abstract, msg\n    return result",
            "def gen_variable_type_func(fn: NativeFunctionWithDifferentiabilityInfo) -> Dict[str, List[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    f = fn.func\n    result = {}\n    with native_function_manager(f):\n        name = cpp.name(f.func)\n        formals = gen_formals(f)\n        if fn.info is None and str(f.func.name.name) not in RESET_GRAD_ACCUMULATOR and (get_base_name(f) not in DONT_REQUIRE_DERIVATIVE) and (len(gen_differentiable_outputs(fn)) > 0) and (cpp.name(f.func) not in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE) and (type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT) and (type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT):\n            type_definition = ''\n            wrapper_registration = AUTOGRAD_NOT_IMPLEMENTED_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name)\n            result['type_derived_method_definitions_Default'] = [type_definition]\n            result['wrapper_registrations_Default'] = [wrapper_registration]\n        elif not fn.info:\n            key = 'Default'\n            type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n            wrapper_registration = gen_wrapper_registration(f, key)\n            result[f'type_derived_method_definitions_{key}'] = [type_definition]\n            result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n        else:\n            for key in fn.info.keys():\n                type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n                wrapper_registration = gen_wrapper_registration(f, key)\n                result[f'type_derived_method_definitions_{key}'] = [type_definition]\n                result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n    assert (name in MANUAL_BACKEND) == f.manual_kernel_registration\n    if name in MANUAL_AUTOGRAD_AND_TRACER or (fn.info and any((info.has_derivatives for info in fn.info.values()))):\n        msg = f\"There's a formula for {name}(or its functional variant) in derivatives.yaml. It's required to add a dispatch section for it with explicit supported backends e.g CPU/CUDA or CompositeExplicitAutograd in native_functions.yaml. Please see https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword for instructions to choose the right dispatch keyword.\"\n        assert f.is_abstract, msg\n    return result",
            "def gen_variable_type_func(fn: NativeFunctionWithDifferentiabilityInfo) -> Dict[str, List[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    f = fn.func\n    result = {}\n    with native_function_manager(f):\n        name = cpp.name(f.func)\n        formals = gen_formals(f)\n        if fn.info is None and str(f.func.name.name) not in RESET_GRAD_ACCUMULATOR and (get_base_name(f) not in DONT_REQUIRE_DERIVATIVE) and (len(gen_differentiable_outputs(fn)) > 0) and (cpp.name(f.func) not in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE) and (type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT) and (type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT):\n            type_definition = ''\n            wrapper_registration = AUTOGRAD_NOT_IMPLEMENTED_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name)\n            result['type_derived_method_definitions_Default'] = [type_definition]\n            result['wrapper_registrations_Default'] = [wrapper_registration]\n        elif not fn.info:\n            key = 'Default'\n            type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n            wrapper_registration = gen_wrapper_registration(f, key)\n            result[f'type_derived_method_definitions_{key}'] = [type_definition]\n            result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n        else:\n            for key in fn.info.keys():\n                type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n                wrapper_registration = gen_wrapper_registration(f, key)\n                result[f'type_derived_method_definitions_{key}'] = [type_definition]\n                result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n    assert (name in MANUAL_BACKEND) == f.manual_kernel_registration\n    if name in MANUAL_AUTOGRAD_AND_TRACER or (fn.info and any((info.has_derivatives for info in fn.info.values()))):\n        msg = f\"There's a formula for {name}(or its functional variant) in derivatives.yaml. It's required to add a dispatch section for it with explicit supported backends e.g CPU/CUDA or CompositeExplicitAutograd in native_functions.yaml. Please see https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword for instructions to choose the right dispatch keyword.\"\n        assert f.is_abstract, msg\n    return result",
            "def gen_variable_type_func(fn: NativeFunctionWithDifferentiabilityInfo) -> Dict[str, List[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    f = fn.func\n    result = {}\n    with native_function_manager(f):\n        name = cpp.name(f.func)\n        formals = gen_formals(f)\n        if fn.info is None and str(f.func.name.name) not in RESET_GRAD_ACCUMULATOR and (get_base_name(f) not in DONT_REQUIRE_DERIVATIVE) and (len(gen_differentiable_outputs(fn)) > 0) and (cpp.name(f.func) not in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE) and (type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT) and (type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT):\n            type_definition = ''\n            wrapper_registration = AUTOGRAD_NOT_IMPLEMENTED_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name)\n            result['type_derived_method_definitions_Default'] = [type_definition]\n            result['wrapper_registrations_Default'] = [wrapper_registration]\n        elif not fn.info:\n            key = 'Default'\n            type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n            wrapper_registration = gen_wrapper_registration(f, key)\n            result[f'type_derived_method_definitions_{key}'] = [type_definition]\n            result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n        else:\n            for key in fn.info.keys():\n                type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n                wrapper_registration = gen_wrapper_registration(f, key)\n                result[f'type_derived_method_definitions_{key}'] = [type_definition]\n                result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n    assert (name in MANUAL_BACKEND) == f.manual_kernel_registration\n    if name in MANUAL_AUTOGRAD_AND_TRACER or (fn.info and any((info.has_derivatives for info in fn.info.values()))):\n        msg = f\"There's a formula for {name}(or its functional variant) in derivatives.yaml. It's required to add a dispatch section for it with explicit supported backends e.g CPU/CUDA or CompositeExplicitAutograd in native_functions.yaml. Please see https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword for instructions to choose the right dispatch keyword.\"\n        assert f.is_abstract, msg\n    return result",
            "def gen_variable_type_func(fn: NativeFunctionWithDifferentiabilityInfo) -> Dict[str, List[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    f = fn.func\n    result = {}\n    with native_function_manager(f):\n        name = cpp.name(f.func)\n        formals = gen_formals(f)\n        if fn.info is None and str(f.func.name.name) not in RESET_GRAD_ACCUMULATOR and (get_base_name(f) not in DONT_REQUIRE_DERIVATIVE) and (len(gen_differentiable_outputs(fn)) > 0) and (cpp.name(f.func) not in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE) and (type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT) and (type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT):\n            type_definition = ''\n            wrapper_registration = AUTOGRAD_NOT_IMPLEMENTED_REGISTRATION.substitute(unqual_operator_name_with_overload=f.func.name)\n            result['type_derived_method_definitions_Default'] = [type_definition]\n            result['wrapper_registrations_Default'] = [wrapper_registration]\n        elif not fn.info:\n            key = 'Default'\n            type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n            wrapper_registration = gen_wrapper_registration(f, key)\n            result[f'type_derived_method_definitions_{key}'] = [type_definition]\n            result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n        else:\n            for key in fn.info.keys():\n                type_definition = METHOD_DEFINITION.substitute(return_type=cpp.returns_type(f.func.returns, symint=True).cpp_type(), type_wrapper_name=type_wrapper_name(f, key), type_definition_body=emit_body(fn, key), formals=formals)\n                wrapper_registration = gen_wrapper_registration(f, key)\n                result[f'type_derived_method_definitions_{key}'] = [type_definition]\n                result[f'wrapper_registrations_{key}'] = [wrapper_registration]\n    assert (name in MANUAL_BACKEND) == f.manual_kernel_registration\n    if name in MANUAL_AUTOGRAD_AND_TRACER or (fn.info and any((info.has_derivatives for info in fn.info.values()))):\n        msg = f\"There's a formula for {name}(or its functional variant) in derivatives.yaml. It's required to add a dispatch section for it with explicit supported backends e.g CPU/CUDA or CompositeExplicitAutograd in native_functions.yaml. Please see https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword for instructions to choose the right dispatch keyword.\"\n        assert f.is_abstract, msg\n    return result"
        ]
    },
    {
        "func_name": "gen_differentiable_input",
        "original": "def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n    if isinstance(arg, TensorOptionsArguments):\n        return None\n    a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n    cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n    if not is_differentiable(a.name, a.type, info):\n        return None\n    return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)",
        "mutated": [
            "def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n    if False:\n        i = 10\n    if isinstance(arg, TensorOptionsArguments):\n        return None\n    a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n    cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n    if not is_differentiable(a.name, a.type, info):\n        return None\n    return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)",
            "def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if isinstance(arg, TensorOptionsArguments):\n        return None\n    a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n    cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n    if not is_differentiable(a.name, a.type, info):\n        return None\n    return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)",
            "def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if isinstance(arg, TensorOptionsArguments):\n        return None\n    a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n    cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n    if not is_differentiable(a.name, a.type, info):\n        return None\n    return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)",
            "def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if isinstance(arg, TensorOptionsArguments):\n        return None\n    a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n    cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n    if not is_differentiable(a.name, a.type, info):\n        return None\n    return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)",
            "def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if isinstance(arg, TensorOptionsArguments):\n        return None\n    a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n    cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n    if not is_differentiable(a.name, a.type, info):\n        return None\n    return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)"
        ]
    },
    {
        "func_name": "gen_differentiable_inputs",
        "original": "@with_native_function\ndef gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n    arguments = list(f.func.arguments.non_out)\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n            if arg in inplace_foreacharg2refarg:\n                mapped_arg = inplace_foreacharg2refarg[arg]\n                arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n    return list(mapMaybe(gen_differentiable_input, arguments))",
        "mutated": [
            "@with_native_function\ndef gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n    if False:\n        i = 10\n    arguments = list(f.func.arguments.non_out)\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n            if arg in inplace_foreacharg2refarg:\n                mapped_arg = inplace_foreacharg2refarg[arg]\n                arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n    return list(mapMaybe(gen_differentiable_input, arguments))",
            "@with_native_function\ndef gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    arguments = list(f.func.arguments.non_out)\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n            if arg in inplace_foreacharg2refarg:\n                mapped_arg = inplace_foreacharg2refarg[arg]\n                arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n    return list(mapMaybe(gen_differentiable_input, arguments))",
            "@with_native_function\ndef gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    arguments = list(f.func.arguments.non_out)\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n            if arg in inplace_foreacharg2refarg:\n                mapped_arg = inplace_foreacharg2refarg[arg]\n                arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n    return list(mapMaybe(gen_differentiable_input, arguments))",
            "@with_native_function\ndef gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    arguments = list(f.func.arguments.non_out)\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n            if arg in inplace_foreacharg2refarg:\n                mapped_arg = inplace_foreacharg2refarg[arg]\n                arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n    return list(mapMaybe(gen_differentiable_input, arguments))",
            "@with_native_function\ndef gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    arguments = list(f.func.arguments.non_out)\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n            if arg in inplace_foreacharg2refarg:\n                mapped_arg = inplace_foreacharg2refarg[arg]\n                arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n    return list(mapMaybe(gen_differentiable_input, arguments))"
        ]
    },
    {
        "func_name": "find_args_with_derivatives",
        "original": "def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n    \"\"\"Find arguments that have derivative definitions\"\"\"\n    if info is None or not info.has_derivatives:\n        return differentiable_inputs\n    names = {name for d in info.derivatives for name in d.var_names}\n    differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n    if len(differentiable) != len(names):\n        missing = names - {arg.name for arg in differentiable}\n        raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n    return differentiable",
        "mutated": [
            "def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n    if False:\n        i = 10\n    'Find arguments that have derivative definitions'\n    if info is None or not info.has_derivatives:\n        return differentiable_inputs\n    names = {name for d in info.derivatives for name in d.var_names}\n    differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n    if len(differentiable) != len(names):\n        missing = names - {arg.name for arg in differentiable}\n        raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n    return differentiable",
            "def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Find arguments that have derivative definitions'\n    if info is None or not info.has_derivatives:\n        return differentiable_inputs\n    names = {name for d in info.derivatives for name in d.var_names}\n    differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n    if len(differentiable) != len(names):\n        missing = names - {arg.name for arg in differentiable}\n        raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n    return differentiable",
            "def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Find arguments that have derivative definitions'\n    if info is None or not info.has_derivatives:\n        return differentiable_inputs\n    names = {name for d in info.derivatives for name in d.var_names}\n    differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n    if len(differentiable) != len(names):\n        missing = names - {arg.name for arg in differentiable}\n        raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n    return differentiable",
            "def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Find arguments that have derivative definitions'\n    if info is None or not info.has_derivatives:\n        return differentiable_inputs\n    names = {name for d in info.derivatives for name in d.var_names}\n    differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n    if len(differentiable) != len(names):\n        missing = names - {arg.name for arg in differentiable}\n        raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n    return differentiable",
            "def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Find arguments that have derivative definitions'\n    if info is None or not info.has_derivatives:\n        return differentiable_inputs\n    names = {name for d in info.derivatives for name in d.var_names}\n    differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n    if len(differentiable) != len(names):\n        missing = names - {arg.name for arg in differentiable}\n        raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n    return differentiable"
        ]
    },
    {
        "func_name": "guard_for",
        "original": "def guard_for(arg: SavedAttribute) -> Optional[str]:\n    assert info is not None\n    if has_tensorlist_arg and (not is_inplace_foreach):\n        return None\n    if 'backward' in info.name:\n        return None\n    if len(args_with_derivatives) <= 1:\n        return None\n    if arg.nctype.type != BaseCType(tensorT):\n        return None\n    used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n    assert len(used_in) > 0\n    if len(used_in) != 1:\n        return None\n    derivative = used_in[0]\n    if len(derivative.var_names) != 1:\n        wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n        if wrap_opt_if_start == -1:\n            return None\n        wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n        assert wrap_opt_if_match is not None\n        condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n        wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n        wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n        return f'{wrap_opt_if_condition}'\n    derivative_var_name = derivative.var_names[0]\n    for (edge_off, a) in enumerate(args_with_derivatives):\n        if a.name == derivative_var_name:\n            break\n    else:\n        raise AssertionError()\n    return f'grad_fn->should_compute_output({edge_off})'",
        "mutated": [
            "def guard_for(arg: SavedAttribute) -> Optional[str]:\n    if False:\n        i = 10\n    assert info is not None\n    if has_tensorlist_arg and (not is_inplace_foreach):\n        return None\n    if 'backward' in info.name:\n        return None\n    if len(args_with_derivatives) <= 1:\n        return None\n    if arg.nctype.type != BaseCType(tensorT):\n        return None\n    used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n    assert len(used_in) > 0\n    if len(used_in) != 1:\n        return None\n    derivative = used_in[0]\n    if len(derivative.var_names) != 1:\n        wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n        if wrap_opt_if_start == -1:\n            return None\n        wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n        assert wrap_opt_if_match is not None\n        condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n        wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n        wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n        return f'{wrap_opt_if_condition}'\n    derivative_var_name = derivative.var_names[0]\n    for (edge_off, a) in enumerate(args_with_derivatives):\n        if a.name == derivative_var_name:\n            break\n    else:\n        raise AssertionError()\n    return f'grad_fn->should_compute_output({edge_off})'",
            "def guard_for(arg: SavedAttribute) -> Optional[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert info is not None\n    if has_tensorlist_arg and (not is_inplace_foreach):\n        return None\n    if 'backward' in info.name:\n        return None\n    if len(args_with_derivatives) <= 1:\n        return None\n    if arg.nctype.type != BaseCType(tensorT):\n        return None\n    used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n    assert len(used_in) > 0\n    if len(used_in) != 1:\n        return None\n    derivative = used_in[0]\n    if len(derivative.var_names) != 1:\n        wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n        if wrap_opt_if_start == -1:\n            return None\n        wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n        assert wrap_opt_if_match is not None\n        condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n        wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n        wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n        return f'{wrap_opt_if_condition}'\n    derivative_var_name = derivative.var_names[0]\n    for (edge_off, a) in enumerate(args_with_derivatives):\n        if a.name == derivative_var_name:\n            break\n    else:\n        raise AssertionError()\n    return f'grad_fn->should_compute_output({edge_off})'",
            "def guard_for(arg: SavedAttribute) -> Optional[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert info is not None\n    if has_tensorlist_arg and (not is_inplace_foreach):\n        return None\n    if 'backward' in info.name:\n        return None\n    if len(args_with_derivatives) <= 1:\n        return None\n    if arg.nctype.type != BaseCType(tensorT):\n        return None\n    used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n    assert len(used_in) > 0\n    if len(used_in) != 1:\n        return None\n    derivative = used_in[0]\n    if len(derivative.var_names) != 1:\n        wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n        if wrap_opt_if_start == -1:\n            return None\n        wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n        assert wrap_opt_if_match is not None\n        condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n        wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n        wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n        return f'{wrap_opt_if_condition}'\n    derivative_var_name = derivative.var_names[0]\n    for (edge_off, a) in enumerate(args_with_derivatives):\n        if a.name == derivative_var_name:\n            break\n    else:\n        raise AssertionError()\n    return f'grad_fn->should_compute_output({edge_off})'",
            "def guard_for(arg: SavedAttribute) -> Optional[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert info is not None\n    if has_tensorlist_arg and (not is_inplace_foreach):\n        return None\n    if 'backward' in info.name:\n        return None\n    if len(args_with_derivatives) <= 1:\n        return None\n    if arg.nctype.type != BaseCType(tensorT):\n        return None\n    used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n    assert len(used_in) > 0\n    if len(used_in) != 1:\n        return None\n    derivative = used_in[0]\n    if len(derivative.var_names) != 1:\n        wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n        if wrap_opt_if_start == -1:\n            return None\n        wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n        assert wrap_opt_if_match is not None\n        condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n        wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n        wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n        return f'{wrap_opt_if_condition}'\n    derivative_var_name = derivative.var_names[0]\n    for (edge_off, a) in enumerate(args_with_derivatives):\n        if a.name == derivative_var_name:\n            break\n    else:\n        raise AssertionError()\n    return f'grad_fn->should_compute_output({edge_off})'",
            "def guard_for(arg: SavedAttribute) -> Optional[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert info is not None\n    if has_tensorlist_arg and (not is_inplace_foreach):\n        return None\n    if 'backward' in info.name:\n        return None\n    if len(args_with_derivatives) <= 1:\n        return None\n    if arg.nctype.type != BaseCType(tensorT):\n        return None\n    used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n    assert len(used_in) > 0\n    if len(used_in) != 1:\n        return None\n    derivative = used_in[0]\n    if len(derivative.var_names) != 1:\n        wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n        if wrap_opt_if_start == -1:\n            return None\n        wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n        assert wrap_opt_if_match is not None\n        condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n        wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n        wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n        return f'{wrap_opt_if_condition}'\n    derivative_var_name = derivative.var_names[0]\n    for (edge_off, a) in enumerate(args_with_derivatives):\n        if a.name == derivative_var_name:\n            break\n    else:\n        raise AssertionError()\n    return f'grad_fn->should_compute_output({edge_off})'"
        ]
    },
    {
        "func_name": "emit_save_inputs",
        "original": "def emit_save_inputs() -> List[str]:\n    setup: List[str] = []\n    if info is None or not info.has_derivatives:\n        return setup\n    has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n    def guard_for(arg: SavedAttribute) -> Optional[str]:\n        assert info is not None\n        if has_tensorlist_arg and (not is_inplace_foreach):\n            return None\n        if 'backward' in info.name:\n            return None\n        if len(args_with_derivatives) <= 1:\n            return None\n        if arg.nctype.type != BaseCType(tensorT):\n            return None\n        used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n        assert len(used_in) > 0\n        if len(used_in) != 1:\n            return None\n        derivative = used_in[0]\n        if len(derivative.var_names) != 1:\n            wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n            if wrap_opt_if_start == -1:\n                return None\n            wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n            assert wrap_opt_if_match is not None\n            condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n            wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n            wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n            return f'{wrap_opt_if_condition}'\n        derivative_var_name = derivative.var_names[0]\n        for (edge_off, a) in enumerate(args_with_derivatives):\n            if a.name == derivative_var_name:\n                break\n        else:\n            raise AssertionError()\n        return f'grad_fn->should_compute_output({edge_off})'\n    if is_inplace_foreach:\n        save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n        if save_input_stmts:\n            setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n    else:\n        setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n        for arg in args_with_derivatives:\n            if is_tensor_list_type(arg.type):\n                setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n    return setup",
        "mutated": [
            "def emit_save_inputs() -> List[str]:\n    if False:\n        i = 10\n    setup: List[str] = []\n    if info is None or not info.has_derivatives:\n        return setup\n    has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n    def guard_for(arg: SavedAttribute) -> Optional[str]:\n        assert info is not None\n        if has_tensorlist_arg and (not is_inplace_foreach):\n            return None\n        if 'backward' in info.name:\n            return None\n        if len(args_with_derivatives) <= 1:\n            return None\n        if arg.nctype.type != BaseCType(tensorT):\n            return None\n        used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n        assert len(used_in) > 0\n        if len(used_in) != 1:\n            return None\n        derivative = used_in[0]\n        if len(derivative.var_names) != 1:\n            wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n            if wrap_opt_if_start == -1:\n                return None\n            wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n            assert wrap_opt_if_match is not None\n            condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n            wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n            wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n            return f'{wrap_opt_if_condition}'\n        derivative_var_name = derivative.var_names[0]\n        for (edge_off, a) in enumerate(args_with_derivatives):\n            if a.name == derivative_var_name:\n                break\n        else:\n            raise AssertionError()\n        return f'grad_fn->should_compute_output({edge_off})'\n    if is_inplace_foreach:\n        save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n        if save_input_stmts:\n            setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n    else:\n        setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n        for arg in args_with_derivatives:\n            if is_tensor_list_type(arg.type):\n                setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n    return setup",
            "def emit_save_inputs() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    setup: List[str] = []\n    if info is None or not info.has_derivatives:\n        return setup\n    has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n    def guard_for(arg: SavedAttribute) -> Optional[str]:\n        assert info is not None\n        if has_tensorlist_arg and (not is_inplace_foreach):\n            return None\n        if 'backward' in info.name:\n            return None\n        if len(args_with_derivatives) <= 1:\n            return None\n        if arg.nctype.type != BaseCType(tensorT):\n            return None\n        used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n        assert len(used_in) > 0\n        if len(used_in) != 1:\n            return None\n        derivative = used_in[0]\n        if len(derivative.var_names) != 1:\n            wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n            if wrap_opt_if_start == -1:\n                return None\n            wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n            assert wrap_opt_if_match is not None\n            condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n            wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n            wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n            return f'{wrap_opt_if_condition}'\n        derivative_var_name = derivative.var_names[0]\n        for (edge_off, a) in enumerate(args_with_derivatives):\n            if a.name == derivative_var_name:\n                break\n        else:\n            raise AssertionError()\n        return f'grad_fn->should_compute_output({edge_off})'\n    if is_inplace_foreach:\n        save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n        if save_input_stmts:\n            setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n    else:\n        setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n        for arg in args_with_derivatives:\n            if is_tensor_list_type(arg.type):\n                setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n    return setup",
            "def emit_save_inputs() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    setup: List[str] = []\n    if info is None or not info.has_derivatives:\n        return setup\n    has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n    def guard_for(arg: SavedAttribute) -> Optional[str]:\n        assert info is not None\n        if has_tensorlist_arg and (not is_inplace_foreach):\n            return None\n        if 'backward' in info.name:\n            return None\n        if len(args_with_derivatives) <= 1:\n            return None\n        if arg.nctype.type != BaseCType(tensorT):\n            return None\n        used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n        assert len(used_in) > 0\n        if len(used_in) != 1:\n            return None\n        derivative = used_in[0]\n        if len(derivative.var_names) != 1:\n            wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n            if wrap_opt_if_start == -1:\n                return None\n            wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n            assert wrap_opt_if_match is not None\n            condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n            wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n            wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n            return f'{wrap_opt_if_condition}'\n        derivative_var_name = derivative.var_names[0]\n        for (edge_off, a) in enumerate(args_with_derivatives):\n            if a.name == derivative_var_name:\n                break\n        else:\n            raise AssertionError()\n        return f'grad_fn->should_compute_output({edge_off})'\n    if is_inplace_foreach:\n        save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n        if save_input_stmts:\n            setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n    else:\n        setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n        for arg in args_with_derivatives:\n            if is_tensor_list_type(arg.type):\n                setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n    return setup",
            "def emit_save_inputs() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    setup: List[str] = []\n    if info is None or not info.has_derivatives:\n        return setup\n    has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n    def guard_for(arg: SavedAttribute) -> Optional[str]:\n        assert info is not None\n        if has_tensorlist_arg and (not is_inplace_foreach):\n            return None\n        if 'backward' in info.name:\n            return None\n        if len(args_with_derivatives) <= 1:\n            return None\n        if arg.nctype.type != BaseCType(tensorT):\n            return None\n        used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n        assert len(used_in) > 0\n        if len(used_in) != 1:\n            return None\n        derivative = used_in[0]\n        if len(derivative.var_names) != 1:\n            wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n            if wrap_opt_if_start == -1:\n                return None\n            wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n            assert wrap_opt_if_match is not None\n            condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n            wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n            wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n            return f'{wrap_opt_if_condition}'\n        derivative_var_name = derivative.var_names[0]\n        for (edge_off, a) in enumerate(args_with_derivatives):\n            if a.name == derivative_var_name:\n                break\n        else:\n            raise AssertionError()\n        return f'grad_fn->should_compute_output({edge_off})'\n    if is_inplace_foreach:\n        save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n        if save_input_stmts:\n            setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n    else:\n        setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n        for arg in args_with_derivatives:\n            if is_tensor_list_type(arg.type):\n                setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n    return setup",
            "def emit_save_inputs() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    setup: List[str] = []\n    if info is None or not info.has_derivatives:\n        return setup\n    has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n    def guard_for(arg: SavedAttribute) -> Optional[str]:\n        assert info is not None\n        if has_tensorlist_arg and (not is_inplace_foreach):\n            return None\n        if 'backward' in info.name:\n            return None\n        if len(args_with_derivatives) <= 1:\n            return None\n        if arg.nctype.type != BaseCType(tensorT):\n            return None\n        used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n        assert len(used_in) > 0\n        if len(used_in) != 1:\n            return None\n        derivative = used_in[0]\n        if len(derivative.var_names) != 1:\n            wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n            if wrap_opt_if_start == -1:\n                return None\n            wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n            assert wrap_opt_if_match is not None\n            condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n            wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n            wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n            return f'{wrap_opt_if_condition}'\n        derivative_var_name = derivative.var_names[0]\n        for (edge_off, a) in enumerate(args_with_derivatives):\n            if a.name == derivative_var_name:\n                break\n        else:\n            raise AssertionError()\n        return f'grad_fn->should_compute_output({edge_off})'\n    if is_inplace_foreach:\n        save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n        if save_input_stmts:\n            setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n    else:\n        setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n        for arg in args_with_derivatives:\n            if is_tensor_list_type(arg.type):\n                setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n    return setup"
        ]
    },
    {
        "func_name": "setup_derivative",
        "original": "def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n    body: List[str] = []\n    if is_out_fn:\n        body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n        return body\n    op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n    setup = []\n    if not is_inplace_foreach:\n        setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n    else:\n        list_like_arg = 'self'\n        args = [arg.name for arg in args_with_derivatives]\n        for (i, arg) in enumerate(args):\n            if is_inplace_foreach and info is not None:\n                if arg in refargname2inplace_foreacharg:\n                    foreach_arg = refargname2inplace_foreacharg[arg]\n                    args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n            elif arg == list_like_arg:\n                args[i] = arg + '[i]'\n        setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n    setup.extend(emit_save_inputs())\n    body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n    declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n    body.append(declare_grad_fn_template.substitute(op=op))\n    body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n    return body",
        "mutated": [
            "def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n    if False:\n        i = 10\n    body: List[str] = []\n    if is_out_fn:\n        body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n        return body\n    op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n    setup = []\n    if not is_inplace_foreach:\n        setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n    else:\n        list_like_arg = 'self'\n        args = [arg.name for arg in args_with_derivatives]\n        for (i, arg) in enumerate(args):\n            if is_inplace_foreach and info is not None:\n                if arg in refargname2inplace_foreacharg:\n                    foreach_arg = refargname2inplace_foreacharg[arg]\n                    args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n            elif arg == list_like_arg:\n                args[i] = arg + '[i]'\n        setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n    setup.extend(emit_save_inputs())\n    body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n    declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n    body.append(declare_grad_fn_template.substitute(op=op))\n    body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n    return body",
            "def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    body: List[str] = []\n    if is_out_fn:\n        body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n        return body\n    op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n    setup = []\n    if not is_inplace_foreach:\n        setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n    else:\n        list_like_arg = 'self'\n        args = [arg.name for arg in args_with_derivatives]\n        for (i, arg) in enumerate(args):\n            if is_inplace_foreach and info is not None:\n                if arg in refargname2inplace_foreacharg:\n                    foreach_arg = refargname2inplace_foreacharg[arg]\n                    args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n            elif arg == list_like_arg:\n                args[i] = arg + '[i]'\n        setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n    setup.extend(emit_save_inputs())\n    body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n    declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n    body.append(declare_grad_fn_template.substitute(op=op))\n    body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n    return body",
            "def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    body: List[str] = []\n    if is_out_fn:\n        body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n        return body\n    op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n    setup = []\n    if not is_inplace_foreach:\n        setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n    else:\n        list_like_arg = 'self'\n        args = [arg.name for arg in args_with_derivatives]\n        for (i, arg) in enumerate(args):\n            if is_inplace_foreach and info is not None:\n                if arg in refargname2inplace_foreacharg:\n                    foreach_arg = refargname2inplace_foreacharg[arg]\n                    args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n            elif arg == list_like_arg:\n                args[i] = arg + '[i]'\n        setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n    setup.extend(emit_save_inputs())\n    body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n    declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n    body.append(declare_grad_fn_template.substitute(op=op))\n    body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n    return body",
            "def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    body: List[str] = []\n    if is_out_fn:\n        body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n        return body\n    op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n    setup = []\n    if not is_inplace_foreach:\n        setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n    else:\n        list_like_arg = 'self'\n        args = [arg.name for arg in args_with_derivatives]\n        for (i, arg) in enumerate(args):\n            if is_inplace_foreach and info is not None:\n                if arg in refargname2inplace_foreacharg:\n                    foreach_arg = refargname2inplace_foreacharg[arg]\n                    args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n            elif arg == list_like_arg:\n                args[i] = arg + '[i]'\n        setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n    setup.extend(emit_save_inputs())\n    body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n    declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n    body.append(declare_grad_fn_template.substitute(op=op))\n    body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n    return body",
            "def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    body: List[str] = []\n    if is_out_fn:\n        body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n        body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n        return body\n    op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n    setup = []\n    if not is_inplace_foreach:\n        setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n    else:\n        list_like_arg = 'self'\n        args = [arg.name for arg in args_with_derivatives]\n        for (i, arg) in enumerate(args):\n            if is_inplace_foreach and info is not None:\n                if arg in refargname2inplace_foreacharg:\n                    foreach_arg = refargname2inplace_foreacharg[arg]\n                    args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n            elif arg == list_like_arg:\n                args[i] = arg + '[i]'\n        setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n    setup.extend(emit_save_inputs())\n    body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n    declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n    body.append(declare_grad_fn_template.substitute(op=op))\n    body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n    return body"
        ]
    },
    {
        "func_name": "emit_check_if_in_complex_autograd_allowlist",
        "original": "def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n    body: List[str] = []\n    if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n        return body\n    for arg in differentiable_outputs:\n        name = arg.name\n        if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n            body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n    return body",
        "mutated": [
            "def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n    if False:\n        i = 10\n    body: List[str] = []\n    if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n        return body\n    for arg in differentiable_outputs:\n        name = arg.name\n        if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n            body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n    return body",
            "def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    body: List[str] = []\n    if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n        return body\n    for arg in differentiable_outputs:\n        name = arg.name\n        if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n            body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n    return body",
            "def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    body: List[str] = []\n    if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n        return body\n    for arg in differentiable_outputs:\n        name = arg.name\n        if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n            body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n    return body",
            "def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    body: List[str] = []\n    if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n        return body\n    for arg in differentiable_outputs:\n        name = arg.name\n        if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n            body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n    return body",
            "def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    body: List[str] = []\n    if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n        return body\n    for arg in differentiable_outputs:\n        name = arg.name\n        if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n            body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n    return body"
        ]
    },
    {
        "func_name": "emit_check_no_requires_grad",
        "original": "def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n    \"\"\"Checks that arguments without derivatives don't require grad\"\"\"\n    body: List[str] = []\n    for arg in tensor_args:\n        if arg in args_with_derivatives:\n            continue\n        arg_name = arg.name\n        if info and arg_name in info.non_differentiable_arg_names:\n            continue\n        if arg_name == 'output':\n            continue\n        body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n    return body",
        "mutated": [
            "def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n    if False:\n        i = 10\n    \"Checks that arguments without derivatives don't require grad\"\n    body: List[str] = []\n    for arg in tensor_args:\n        if arg in args_with_derivatives:\n            continue\n        arg_name = arg.name\n        if info and arg_name in info.non_differentiable_arg_names:\n            continue\n        if arg_name == 'output':\n            continue\n        body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n    return body",
            "def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Checks that arguments without derivatives don't require grad\"\n    body: List[str] = []\n    for arg in tensor_args:\n        if arg in args_with_derivatives:\n            continue\n        arg_name = arg.name\n        if info and arg_name in info.non_differentiable_arg_names:\n            continue\n        if arg_name == 'output':\n            continue\n        body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n    return body",
            "def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Checks that arguments without derivatives don't require grad\"\n    body: List[str] = []\n    for arg in tensor_args:\n        if arg in args_with_derivatives:\n            continue\n        arg_name = arg.name\n        if info and arg_name in info.non_differentiable_arg_names:\n            continue\n        if arg_name == 'output':\n            continue\n        body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n    return body",
            "def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Checks that arguments without derivatives don't require grad\"\n    body: List[str] = []\n    for arg in tensor_args:\n        if arg in args_with_derivatives:\n            continue\n        arg_name = arg.name\n        if info and arg_name in info.non_differentiable_arg_names:\n            continue\n        if arg_name == 'output':\n            continue\n        body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n    return body",
            "def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Checks that arguments without derivatives don't require grad\"\n    body: List[str] = []\n    for arg in tensor_args:\n        if arg in args_with_derivatives:\n            continue\n        arg_name = arg.name\n        if info and arg_name in info.non_differentiable_arg_names:\n            continue\n        if arg_name == 'output':\n            continue\n        body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n    return body"
        ]
    },
    {
        "func_name": "emit_original_self_definition",
        "original": "def emit_original_self_definition() -> List[str]:\n    body: List[str] = []\n    if inplace:\n        if is_inplace_foreach:\n            body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n        else:\n            body.append('c10::optional<at::Tensor> original_self;')\n        all_forward_grad_cond = []\n        for derivative in fw_derivatives:\n            if derivative.required_original_self_value:\n                all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n        if all_forward_grad_cond:\n            if not is_inplace_foreach:\n                body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                body.append('  original_self = self.clone();')\n                body.append('}')\n            else:\n                current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                body.append('for (const auto& i : c10::irange(self.size())) {')\n                body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                body.append('    original_selfs[i] = self[i].clone();')\n                body.append('  }')\n                body.append('}')\n    return body",
        "mutated": [
            "def emit_original_self_definition() -> List[str]:\n    if False:\n        i = 10\n    body: List[str] = []\n    if inplace:\n        if is_inplace_foreach:\n            body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n        else:\n            body.append('c10::optional<at::Tensor> original_self;')\n        all_forward_grad_cond = []\n        for derivative in fw_derivatives:\n            if derivative.required_original_self_value:\n                all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n        if all_forward_grad_cond:\n            if not is_inplace_foreach:\n                body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                body.append('  original_self = self.clone();')\n                body.append('}')\n            else:\n                current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                body.append('for (const auto& i : c10::irange(self.size())) {')\n                body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                body.append('    original_selfs[i] = self[i].clone();')\n                body.append('  }')\n                body.append('}')\n    return body",
            "def emit_original_self_definition() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    body: List[str] = []\n    if inplace:\n        if is_inplace_foreach:\n            body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n        else:\n            body.append('c10::optional<at::Tensor> original_self;')\n        all_forward_grad_cond = []\n        for derivative in fw_derivatives:\n            if derivative.required_original_self_value:\n                all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n        if all_forward_grad_cond:\n            if not is_inplace_foreach:\n                body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                body.append('  original_self = self.clone();')\n                body.append('}')\n            else:\n                current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                body.append('for (const auto& i : c10::irange(self.size())) {')\n                body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                body.append('    original_selfs[i] = self[i].clone();')\n                body.append('  }')\n                body.append('}')\n    return body",
            "def emit_original_self_definition() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    body: List[str] = []\n    if inplace:\n        if is_inplace_foreach:\n            body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n        else:\n            body.append('c10::optional<at::Tensor> original_self;')\n        all_forward_grad_cond = []\n        for derivative in fw_derivatives:\n            if derivative.required_original_self_value:\n                all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n        if all_forward_grad_cond:\n            if not is_inplace_foreach:\n                body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                body.append('  original_self = self.clone();')\n                body.append('}')\n            else:\n                current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                body.append('for (const auto& i : c10::irange(self.size())) {')\n                body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                body.append('    original_selfs[i] = self[i].clone();')\n                body.append('  }')\n                body.append('}')\n    return body",
            "def emit_original_self_definition() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    body: List[str] = []\n    if inplace:\n        if is_inplace_foreach:\n            body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n        else:\n            body.append('c10::optional<at::Tensor> original_self;')\n        all_forward_grad_cond = []\n        for derivative in fw_derivatives:\n            if derivative.required_original_self_value:\n                all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n        if all_forward_grad_cond:\n            if not is_inplace_foreach:\n                body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                body.append('  original_self = self.clone();')\n                body.append('}')\n            else:\n                current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                body.append('for (const auto& i : c10::irange(self.size())) {')\n                body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                body.append('    original_selfs[i] = self[i].clone();')\n                body.append('  }')\n                body.append('}')\n    return body",
            "def emit_original_self_definition() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    body: List[str] = []\n    if inplace:\n        if is_inplace_foreach:\n            body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n        else:\n            body.append('c10::optional<at::Tensor> original_self;')\n        all_forward_grad_cond = []\n        for derivative in fw_derivatives:\n            if derivative.required_original_self_value:\n                all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n        if all_forward_grad_cond:\n            if not is_inplace_foreach:\n                body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                body.append('  original_self = self.clone();')\n                body.append('}')\n            else:\n                current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                body.append('for (const auto& i : c10::irange(self.size())) {')\n                body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                body.append('    original_selfs[i] = self[i].clone();')\n                body.append('  }')\n                body.append('}')\n    return body"
        ]
    },
    {
        "func_name": "save_variables",
        "original": "def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n    stmts: List[str] = []\n    for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n        name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n        foreacharg: Optional[Argument] = None\n        is_foreacharg_list_type: bool = False\n        type = arg.nctype.type\n        expr = arg.expr\n        stmts_prepend = None\n        if is_inplace_foreach and info is not None:\n            name_to_query = name.split('_scalar_type')[0]\n            if name_to_query in refargname2inplace_foreacharg:\n                foreacharg = refargname2inplace_foreacharg[name_to_query]\n                is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n            if foreacharg is not None:\n                name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                src_name = name\n                if '_scalar_type' in src_name:\n                    split_src_name = src_name.split('_scalar_type')\n                    assert len(split_src_name) == 2\n                    src_name = split_src_name[0]\n                expr = expr.replace(src_name, name_in_expr)\n        if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n            var = name\n            name += '_'\n            if var == 'self' and inplace:\n                original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                self_var = var if not is_inplace_foreach else var + '[i]'\n                stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                var = f'{original_self_var}.value()'\n                assert not is_output\n            if inplace and is_output:\n                assert name == 'result_'\n                var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                is_inplace_view = f'{var}.is_view()'\n                expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n            else:\n                expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                if foreacharg is not None and 'original_selfs' not in expr:\n                    expr = expr.replace(src_name, name_in_expr)\n        elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n            if type == VectorCType(BaseCType(tensorT)):\n                assert is_foreach and is_output\n            expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n            name += '_'\n        elif type == BaseCType(intArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(symIntArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(stringT):\n            expr = f'std::string({expr})'\n        elif type == OptionalCType(BaseCType(stringT)):\n            expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n        elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n            expr = expr + '.vec()'\n        guard = guard_for(arg)\n        if guard is None:\n            if stmts_prepend:\n                stmts.append(f'{stmts_prepend};')\n            stmts.append(f'grad_fn->{name} = {expr};')\n        else:\n            stmts.append(f'if ({guard}) {{')\n            if stmts_prepend:\n                stmts.append(f'  {stmts_prepend};')\n            stmts.append(f'  grad_fn->{name} = {expr};')\n            stmts.append('}')\n    return stmts",
        "mutated": [
            "def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n    if False:\n        i = 10\n    stmts: List[str] = []\n    for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n        name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n        foreacharg: Optional[Argument] = None\n        is_foreacharg_list_type: bool = False\n        type = arg.nctype.type\n        expr = arg.expr\n        stmts_prepend = None\n        if is_inplace_foreach and info is not None:\n            name_to_query = name.split('_scalar_type')[0]\n            if name_to_query in refargname2inplace_foreacharg:\n                foreacharg = refargname2inplace_foreacharg[name_to_query]\n                is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n            if foreacharg is not None:\n                name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                src_name = name\n                if '_scalar_type' in src_name:\n                    split_src_name = src_name.split('_scalar_type')\n                    assert len(split_src_name) == 2\n                    src_name = split_src_name[0]\n                expr = expr.replace(src_name, name_in_expr)\n        if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n            var = name\n            name += '_'\n            if var == 'self' and inplace:\n                original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                self_var = var if not is_inplace_foreach else var + '[i]'\n                stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                var = f'{original_self_var}.value()'\n                assert not is_output\n            if inplace and is_output:\n                assert name == 'result_'\n                var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                is_inplace_view = f'{var}.is_view()'\n                expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n            else:\n                expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                if foreacharg is not None and 'original_selfs' not in expr:\n                    expr = expr.replace(src_name, name_in_expr)\n        elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n            if type == VectorCType(BaseCType(tensorT)):\n                assert is_foreach and is_output\n            expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n            name += '_'\n        elif type == BaseCType(intArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(symIntArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(stringT):\n            expr = f'std::string({expr})'\n        elif type == OptionalCType(BaseCType(stringT)):\n            expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n        elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n            expr = expr + '.vec()'\n        guard = guard_for(arg)\n        if guard is None:\n            if stmts_prepend:\n                stmts.append(f'{stmts_prepend};')\n            stmts.append(f'grad_fn->{name} = {expr};')\n        else:\n            stmts.append(f'if ({guard}) {{')\n            if stmts_prepend:\n                stmts.append(f'  {stmts_prepend};')\n            stmts.append(f'  grad_fn->{name} = {expr};')\n            stmts.append('}')\n    return stmts",
            "def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    stmts: List[str] = []\n    for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n        name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n        foreacharg: Optional[Argument] = None\n        is_foreacharg_list_type: bool = False\n        type = arg.nctype.type\n        expr = arg.expr\n        stmts_prepend = None\n        if is_inplace_foreach and info is not None:\n            name_to_query = name.split('_scalar_type')[0]\n            if name_to_query in refargname2inplace_foreacharg:\n                foreacharg = refargname2inplace_foreacharg[name_to_query]\n                is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n            if foreacharg is not None:\n                name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                src_name = name\n                if '_scalar_type' in src_name:\n                    split_src_name = src_name.split('_scalar_type')\n                    assert len(split_src_name) == 2\n                    src_name = split_src_name[0]\n                expr = expr.replace(src_name, name_in_expr)\n        if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n            var = name\n            name += '_'\n            if var == 'self' and inplace:\n                original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                self_var = var if not is_inplace_foreach else var + '[i]'\n                stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                var = f'{original_self_var}.value()'\n                assert not is_output\n            if inplace and is_output:\n                assert name == 'result_'\n                var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                is_inplace_view = f'{var}.is_view()'\n                expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n            else:\n                expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                if foreacharg is not None and 'original_selfs' not in expr:\n                    expr = expr.replace(src_name, name_in_expr)\n        elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n            if type == VectorCType(BaseCType(tensorT)):\n                assert is_foreach and is_output\n            expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n            name += '_'\n        elif type == BaseCType(intArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(symIntArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(stringT):\n            expr = f'std::string({expr})'\n        elif type == OptionalCType(BaseCType(stringT)):\n            expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n        elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n            expr = expr + '.vec()'\n        guard = guard_for(arg)\n        if guard is None:\n            if stmts_prepend:\n                stmts.append(f'{stmts_prepend};')\n            stmts.append(f'grad_fn->{name} = {expr};')\n        else:\n            stmts.append(f'if ({guard}) {{')\n            if stmts_prepend:\n                stmts.append(f'  {stmts_prepend};')\n            stmts.append(f'  grad_fn->{name} = {expr};')\n            stmts.append('}')\n    return stmts",
            "def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    stmts: List[str] = []\n    for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n        name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n        foreacharg: Optional[Argument] = None\n        is_foreacharg_list_type: bool = False\n        type = arg.nctype.type\n        expr = arg.expr\n        stmts_prepend = None\n        if is_inplace_foreach and info is not None:\n            name_to_query = name.split('_scalar_type')[0]\n            if name_to_query in refargname2inplace_foreacharg:\n                foreacharg = refargname2inplace_foreacharg[name_to_query]\n                is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n            if foreacharg is not None:\n                name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                src_name = name\n                if '_scalar_type' in src_name:\n                    split_src_name = src_name.split('_scalar_type')\n                    assert len(split_src_name) == 2\n                    src_name = split_src_name[0]\n                expr = expr.replace(src_name, name_in_expr)\n        if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n            var = name\n            name += '_'\n            if var == 'self' and inplace:\n                original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                self_var = var if not is_inplace_foreach else var + '[i]'\n                stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                var = f'{original_self_var}.value()'\n                assert not is_output\n            if inplace and is_output:\n                assert name == 'result_'\n                var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                is_inplace_view = f'{var}.is_view()'\n                expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n            else:\n                expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                if foreacharg is not None and 'original_selfs' not in expr:\n                    expr = expr.replace(src_name, name_in_expr)\n        elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n            if type == VectorCType(BaseCType(tensorT)):\n                assert is_foreach and is_output\n            expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n            name += '_'\n        elif type == BaseCType(intArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(symIntArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(stringT):\n            expr = f'std::string({expr})'\n        elif type == OptionalCType(BaseCType(stringT)):\n            expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n        elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n            expr = expr + '.vec()'\n        guard = guard_for(arg)\n        if guard is None:\n            if stmts_prepend:\n                stmts.append(f'{stmts_prepend};')\n            stmts.append(f'grad_fn->{name} = {expr};')\n        else:\n            stmts.append(f'if ({guard}) {{')\n            if stmts_prepend:\n                stmts.append(f'  {stmts_prepend};')\n            stmts.append(f'  grad_fn->{name} = {expr};')\n            stmts.append('}')\n    return stmts",
            "def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    stmts: List[str] = []\n    for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n        name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n        foreacharg: Optional[Argument] = None\n        is_foreacharg_list_type: bool = False\n        type = arg.nctype.type\n        expr = arg.expr\n        stmts_prepend = None\n        if is_inplace_foreach and info is not None:\n            name_to_query = name.split('_scalar_type')[0]\n            if name_to_query in refargname2inplace_foreacharg:\n                foreacharg = refargname2inplace_foreacharg[name_to_query]\n                is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n            if foreacharg is not None:\n                name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                src_name = name\n                if '_scalar_type' in src_name:\n                    split_src_name = src_name.split('_scalar_type')\n                    assert len(split_src_name) == 2\n                    src_name = split_src_name[0]\n                expr = expr.replace(src_name, name_in_expr)\n        if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n            var = name\n            name += '_'\n            if var == 'self' and inplace:\n                original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                self_var = var if not is_inplace_foreach else var + '[i]'\n                stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                var = f'{original_self_var}.value()'\n                assert not is_output\n            if inplace and is_output:\n                assert name == 'result_'\n                var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                is_inplace_view = f'{var}.is_view()'\n                expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n            else:\n                expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                if foreacharg is not None and 'original_selfs' not in expr:\n                    expr = expr.replace(src_name, name_in_expr)\n        elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n            if type == VectorCType(BaseCType(tensorT)):\n                assert is_foreach and is_output\n            expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n            name += '_'\n        elif type == BaseCType(intArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(symIntArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(stringT):\n            expr = f'std::string({expr})'\n        elif type == OptionalCType(BaseCType(stringT)):\n            expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n        elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n            expr = expr + '.vec()'\n        guard = guard_for(arg)\n        if guard is None:\n            if stmts_prepend:\n                stmts.append(f'{stmts_prepend};')\n            stmts.append(f'grad_fn->{name} = {expr};')\n        else:\n            stmts.append(f'if ({guard}) {{')\n            if stmts_prepend:\n                stmts.append(f'  {stmts_prepend};')\n            stmts.append(f'  grad_fn->{name} = {expr};')\n            stmts.append('}')\n    return stmts",
            "def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    stmts: List[str] = []\n    for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n        name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n        foreacharg: Optional[Argument] = None\n        is_foreacharg_list_type: bool = False\n        type = arg.nctype.type\n        expr = arg.expr\n        stmts_prepend = None\n        if is_inplace_foreach and info is not None:\n            name_to_query = name.split('_scalar_type')[0]\n            if name_to_query in refargname2inplace_foreacharg:\n                foreacharg = refargname2inplace_foreacharg[name_to_query]\n                is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n            if foreacharg is not None:\n                name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                src_name = name\n                if '_scalar_type' in src_name:\n                    split_src_name = src_name.split('_scalar_type')\n                    assert len(split_src_name) == 2\n                    src_name = split_src_name[0]\n                expr = expr.replace(src_name, name_in_expr)\n        if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n            var = name\n            name += '_'\n            if var == 'self' and inplace:\n                original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                self_var = var if not is_inplace_foreach else var + '[i]'\n                stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                var = f'{original_self_var}.value()'\n                assert not is_output\n            if inplace and is_output:\n                assert name == 'result_'\n                var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                is_inplace_view = f'{var}.is_view()'\n                expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n            else:\n                expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                if foreacharg is not None and 'original_selfs' not in expr:\n                    expr = expr.replace(src_name, name_in_expr)\n        elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n            if type == VectorCType(BaseCType(tensorT)):\n                assert is_foreach and is_output\n            expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n            name += '_'\n        elif type == BaseCType(intArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(symIntArrayRefT):\n            expr = expr + '.vec()'\n        elif type == BaseCType(stringT):\n            expr = f'std::string({expr})'\n        elif type == OptionalCType(BaseCType(stringT)):\n            expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n        elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n            expr = expr + '.vec()'\n        guard = guard_for(arg)\n        if guard is None:\n            if stmts_prepend:\n                stmts.append(f'{stmts_prepend};')\n            stmts.append(f'grad_fn->{name} = {expr};')\n        else:\n            stmts.append(f'if ({guard}) {{')\n            if stmts_prepend:\n                stmts.append(f'  {stmts_prepend};')\n            stmts.append(f'  grad_fn->{name} = {expr};')\n            stmts.append('}')\n    return stmts"
        ]
    },
    {
        "func_name": "emit_dispatch_call",
        "original": "def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n    \"\"\"Dispatch call via function in a namespace or method on Tensor.\"\"\"\n    dispatcher_sig = DispatcherSignature.from_schema(f.func)\n    dispatcher_exprs = dispatcher_sig.exprs()\n    dispatch_key_set = 'ks & c10::after_autograd_keyset'\n    call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n    return call",
        "mutated": [
            "def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n    if False:\n        i = 10\n    'Dispatch call via function in a namespace or method on Tensor.'\n    dispatcher_sig = DispatcherSignature.from_schema(f.func)\n    dispatcher_exprs = dispatcher_sig.exprs()\n    dispatch_key_set = 'ks & c10::after_autograd_keyset'\n    call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n    return call",
            "def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Dispatch call via function in a namespace or method on Tensor.'\n    dispatcher_sig = DispatcherSignature.from_schema(f.func)\n    dispatcher_exprs = dispatcher_sig.exprs()\n    dispatch_key_set = 'ks & c10::after_autograd_keyset'\n    call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n    return call",
            "def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Dispatch call via function in a namespace or method on Tensor.'\n    dispatcher_sig = DispatcherSignature.from_schema(f.func)\n    dispatcher_exprs = dispatcher_sig.exprs()\n    dispatch_key_set = 'ks & c10::after_autograd_keyset'\n    call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n    return call",
            "def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Dispatch call via function in a namespace or method on Tensor.'\n    dispatcher_sig = DispatcherSignature.from_schema(f.func)\n    dispatcher_exprs = dispatcher_sig.exprs()\n    dispatch_key_set = 'ks & c10::after_autograd_keyset'\n    call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n    return call",
            "def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Dispatch call via function in a namespace or method on Tensor.'\n    dispatcher_sig = DispatcherSignature.from_schema(f.func)\n    dispatcher_exprs = dispatcher_sig.exprs()\n    dispatch_key_set = 'ks & c10::after_autograd_keyset'\n    call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n    return call"
        ]
    },
    {
        "func_name": "wrap_output",
        "original": "def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n    call = ''\n    rhs_value: Optional[str] = None\n    if not any((r.type.is_tensor_like() for r in f.func.returns)):\n        rhs_value = var\n    else:\n        rhs_value = f'std::move({var})'\n    assert rhs_value is not None\n    call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n    return call",
        "mutated": [
            "def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n    if False:\n        i = 10\n    call = ''\n    rhs_value: Optional[str] = None\n    if not any((r.type.is_tensor_like() for r in f.func.returns)):\n        rhs_value = var\n    else:\n        rhs_value = f'std::move({var})'\n    assert rhs_value is not None\n    call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n    return call",
            "def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    call = ''\n    rhs_value: Optional[str] = None\n    if not any((r.type.is_tensor_like() for r in f.func.returns)):\n        rhs_value = var\n    else:\n        rhs_value = f'std::move({var})'\n    assert rhs_value is not None\n    call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n    return call",
            "def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    call = ''\n    rhs_value: Optional[str] = None\n    if not any((r.type.is_tensor_like() for r in f.func.returns)):\n        rhs_value = var\n    else:\n        rhs_value = f'std::move({var})'\n    assert rhs_value is not None\n    call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n    return call",
            "def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    call = ''\n    rhs_value: Optional[str] = None\n    if not any((r.type.is_tensor_like() for r in f.func.returns)):\n        rhs_value = var\n    else:\n        rhs_value = f'std::move({var})'\n    assert rhs_value is not None\n    call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n    return call",
            "def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    call = ''\n    rhs_value: Optional[str] = None\n    if not any((r.type.is_tensor_like() for r in f.func.returns)):\n        rhs_value = var\n    else:\n        rhs_value = f'std::move({var})'\n    assert rhs_value is not None\n    call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n    return call"
        ]
    },
    {
        "func_name": "check_tensorimpl_and_storage",
        "original": "def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n    stmts_before_call: List[str] = []\n    stmts_after_call: List[str] = []\n    if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n        return call\n    for unpacked_binding in unpacked_bindings:\n        arg = unpacked_binding.name\n        noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n        if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n            stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n            stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == BaseCType(tensorT):\n            stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n    assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n    if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n        base_name = f.func.name.name.base\n        aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n        if aliased_arg_name is not None:\n            aliased_arg_name = unpacked_name(aliased_arg_name)\n        for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n            noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorT):\n                if aliased_arg_name is not None:\n                    assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                    stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n            elif noref_cpp_type == BaseCType(tensorListT):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n    if stmts_before_call and stmts_after_call:\n        call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n    return call",
        "mutated": [
            "def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n    if False:\n        i = 10\n    stmts_before_call: List[str] = []\n    stmts_after_call: List[str] = []\n    if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n        return call\n    for unpacked_binding in unpacked_bindings:\n        arg = unpacked_binding.name\n        noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n        if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n            stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n            stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == BaseCType(tensorT):\n            stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n    assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n    if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n        base_name = f.func.name.name.base\n        aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n        if aliased_arg_name is not None:\n            aliased_arg_name = unpacked_name(aliased_arg_name)\n        for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n            noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorT):\n                if aliased_arg_name is not None:\n                    assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                    stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n            elif noref_cpp_type == BaseCType(tensorListT):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n    if stmts_before_call and stmts_after_call:\n        call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n    return call",
            "def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    stmts_before_call: List[str] = []\n    stmts_after_call: List[str] = []\n    if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n        return call\n    for unpacked_binding in unpacked_bindings:\n        arg = unpacked_binding.name\n        noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n        if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n            stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n            stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == BaseCType(tensorT):\n            stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n    assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n    if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n        base_name = f.func.name.name.base\n        aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n        if aliased_arg_name is not None:\n            aliased_arg_name = unpacked_name(aliased_arg_name)\n        for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n            noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorT):\n                if aliased_arg_name is not None:\n                    assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                    stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n            elif noref_cpp_type == BaseCType(tensorListT):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n    if stmts_before_call and stmts_after_call:\n        call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n    return call",
            "def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    stmts_before_call: List[str] = []\n    stmts_after_call: List[str] = []\n    if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n        return call\n    for unpacked_binding in unpacked_bindings:\n        arg = unpacked_binding.name\n        noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n        if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n            stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n            stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == BaseCType(tensorT):\n            stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n    assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n    if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n        base_name = f.func.name.name.base\n        aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n        if aliased_arg_name is not None:\n            aliased_arg_name = unpacked_name(aliased_arg_name)\n        for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n            noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorT):\n                if aliased_arg_name is not None:\n                    assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                    stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n            elif noref_cpp_type == BaseCType(tensorListT):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n    if stmts_before_call and stmts_after_call:\n        call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n    return call",
            "def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    stmts_before_call: List[str] = []\n    stmts_after_call: List[str] = []\n    if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n        return call\n    for unpacked_binding in unpacked_bindings:\n        arg = unpacked_binding.name\n        noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n        if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n            stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n            stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == BaseCType(tensorT):\n            stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n    assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n    if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n        base_name = f.func.name.name.base\n        aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n        if aliased_arg_name is not None:\n            aliased_arg_name = unpacked_name(aliased_arg_name)\n        for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n            noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorT):\n                if aliased_arg_name is not None:\n                    assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                    stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n            elif noref_cpp_type == BaseCType(tensorListT):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n    if stmts_before_call and stmts_after_call:\n        call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n    return call",
            "def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    stmts_before_call: List[str] = []\n    stmts_after_call: List[str] = []\n    if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n        return call\n    for unpacked_binding in unpacked_bindings:\n        arg = unpacked_binding.name\n        noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n        if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n            stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n            stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n        elif noref_cpp_type == BaseCType(tensorT):\n            stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n            stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n    assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n    if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n        base_name = f.func.name.name.base\n        aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n        if aliased_arg_name is not None:\n            aliased_arg_name = unpacked_name(aliased_arg_name)\n        for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n            noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorT):\n                if aliased_arg_name is not None:\n                    assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                    stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                    stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n            elif noref_cpp_type == BaseCType(tensorListT):\n                raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n    if stmts_before_call and stmts_after_call:\n        call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n    return call"
        ]
    },
    {
        "func_name": "emit_call",
        "original": "def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n    unpacked_args = [b.name for b in unpacked_bindings]\n    base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n    if get_view_info(f) is not None or modifies_arguments(f):\n        guard = 'at::AutoDispatchBelowAutograd guard;'\n    else:\n        guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n    any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n    return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n    if len(f.func.returns) > 1:\n        return_types = f'std::tuple<{return_types}>'\n    arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n    if not modifies_arguments(f) and (not returns_void):\n        if try_jit_decomposition:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n        else:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n        call += wrap_output(f, unpacked_bindings, TMP_VAR)\n    else:\n        assert not try_jit_decomposition\n        call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n    call = check_tensorimpl_and_storage(call, unpacked_bindings)\n    return call",
        "mutated": [
            "def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n    if False:\n        i = 10\n    unpacked_args = [b.name for b in unpacked_bindings]\n    base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n    if get_view_info(f) is not None or modifies_arguments(f):\n        guard = 'at::AutoDispatchBelowAutograd guard;'\n    else:\n        guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n    any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n    return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n    if len(f.func.returns) > 1:\n        return_types = f'std::tuple<{return_types}>'\n    arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n    if not modifies_arguments(f) and (not returns_void):\n        if try_jit_decomposition:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n        else:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n        call += wrap_output(f, unpacked_bindings, TMP_VAR)\n    else:\n        assert not try_jit_decomposition\n        call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n    call = check_tensorimpl_and_storage(call, unpacked_bindings)\n    return call",
            "def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    unpacked_args = [b.name for b in unpacked_bindings]\n    base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n    if get_view_info(f) is not None or modifies_arguments(f):\n        guard = 'at::AutoDispatchBelowAutograd guard;'\n    else:\n        guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n    any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n    return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n    if len(f.func.returns) > 1:\n        return_types = f'std::tuple<{return_types}>'\n    arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n    if not modifies_arguments(f) and (not returns_void):\n        if try_jit_decomposition:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n        else:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n        call += wrap_output(f, unpacked_bindings, TMP_VAR)\n    else:\n        assert not try_jit_decomposition\n        call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n    call = check_tensorimpl_and_storage(call, unpacked_bindings)\n    return call",
            "def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    unpacked_args = [b.name for b in unpacked_bindings]\n    base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n    if get_view_info(f) is not None or modifies_arguments(f):\n        guard = 'at::AutoDispatchBelowAutograd guard;'\n    else:\n        guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n    any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n    return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n    if len(f.func.returns) > 1:\n        return_types = f'std::tuple<{return_types}>'\n    arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n    if not modifies_arguments(f) and (not returns_void):\n        if try_jit_decomposition:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n        else:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n        call += wrap_output(f, unpacked_bindings, TMP_VAR)\n    else:\n        assert not try_jit_decomposition\n        call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n    call = check_tensorimpl_and_storage(call, unpacked_bindings)\n    return call",
            "def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    unpacked_args = [b.name for b in unpacked_bindings]\n    base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n    if get_view_info(f) is not None or modifies_arguments(f):\n        guard = 'at::AutoDispatchBelowAutograd guard;'\n    else:\n        guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n    any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n    return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n    if len(f.func.returns) > 1:\n        return_types = f'std::tuple<{return_types}>'\n    arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n    if not modifies_arguments(f) and (not returns_void):\n        if try_jit_decomposition:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n        else:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n        call += wrap_output(f, unpacked_bindings, TMP_VAR)\n    else:\n        assert not try_jit_decomposition\n        call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n    call = check_tensorimpl_and_storage(call, unpacked_bindings)\n    return call",
            "def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    unpacked_args = [b.name for b in unpacked_bindings]\n    base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n    if get_view_info(f) is not None or modifies_arguments(f):\n        guard = 'at::AutoDispatchBelowAutograd guard;'\n    else:\n        guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n    any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n    return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n    if len(f.func.returns) > 1:\n        return_types = f'std::tuple<{return_types}>'\n    arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n    if not modifies_arguments(f) and (not returns_void):\n        if try_jit_decomposition:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n        else:\n            call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n        call += wrap_output(f, unpacked_bindings, TMP_VAR)\n    else:\n        assert not try_jit_decomposition\n        call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n    call = check_tensorimpl_and_storage(call, unpacked_bindings)\n    return call"
        ]
    },
    {
        "func_name": "emit_history",
        "original": "def emit_history() -> str:\n    fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n    output_names = [r.name for r in differentiable_outputs]\n    outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n    if not is_inplace_foreach:\n        return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n    else:\n        return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')",
        "mutated": [
            "def emit_history() -> str:\n    if False:\n        i = 10\n    fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n    output_names = [r.name for r in differentiable_outputs]\n    outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n    if not is_inplace_foreach:\n        return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n    else:\n        return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')",
            "def emit_history() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n    output_names = [r.name for r in differentiable_outputs]\n    outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n    if not is_inplace_foreach:\n        return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n    else:\n        return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')",
            "def emit_history() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n    output_names = [r.name for r in differentiable_outputs]\n    outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n    if not is_inplace_foreach:\n        return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n    else:\n        return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')",
            "def emit_history() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n    output_names = [r.name for r in differentiable_outputs]\n    outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n    if not is_inplace_foreach:\n        return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n    else:\n        return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')",
            "def emit_history() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n    output_names = [r.name for r in differentiable_outputs]\n    outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n    if not is_inplace_foreach:\n        return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n    else:\n        return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')"
        ]
    },
    {
        "func_name": "emit_save_outputs",
        "original": "def emit_save_outputs() -> str:\n    if is_out_fn:\n        return ''\n    if info is not None and info.has_derivatives:\n        stmts = save_variables(info.all_saved_outputs, True)\n        if len(stmts) == 0:\n            return ''\n        if not is_inplace_foreach:\n            return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n    return ''",
        "mutated": [
            "def emit_save_outputs() -> str:\n    if False:\n        i = 10\n    if is_out_fn:\n        return ''\n    if info is not None and info.has_derivatives:\n        stmts = save_variables(info.all_saved_outputs, True)\n        if len(stmts) == 0:\n            return ''\n        if not is_inplace_foreach:\n            return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n    return ''",
            "def emit_save_outputs() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if is_out_fn:\n        return ''\n    if info is not None and info.has_derivatives:\n        stmts = save_variables(info.all_saved_outputs, True)\n        if len(stmts) == 0:\n            return ''\n        if not is_inplace_foreach:\n            return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n    return ''",
            "def emit_save_outputs() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if is_out_fn:\n        return ''\n    if info is not None and info.has_derivatives:\n        stmts = save_variables(info.all_saved_outputs, True)\n        if len(stmts) == 0:\n            return ''\n        if not is_inplace_foreach:\n            return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n    return ''",
            "def emit_save_outputs() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if is_out_fn:\n        return ''\n    if info is not None and info.has_derivatives:\n        stmts = save_variables(info.all_saved_outputs, True)\n        if len(stmts) == 0:\n            return ''\n        if not is_inplace_foreach:\n            return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n    return ''",
            "def emit_save_outputs() -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if is_out_fn:\n        return ''\n    if info is not None and info.has_derivatives:\n        stmts = save_variables(info.all_saved_outputs, True)\n        if len(stmts) == 0:\n            return ''\n        if not is_inplace_foreach:\n            return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n    return ''"
        ]
    },
    {
        "func_name": "emit_any_requires_grad",
        "original": "def emit_any_requires_grad() -> List[str]:\n    extra_condition = ''\n    if info and info.output_differentiability_conditions:\n        assert len(info.output_differentiability_conditions) == 1\n        extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n    names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(names_of_args_with_derivatives):\n            for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                if arg == r_arg.name:\n                    names_of_args_with_derivatives[i] = f_arg.name\n    return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]",
        "mutated": [
            "def emit_any_requires_grad() -> List[str]:\n    if False:\n        i = 10\n    extra_condition = ''\n    if info and info.output_differentiability_conditions:\n        assert len(info.output_differentiability_conditions) == 1\n        extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n    names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(names_of_args_with_derivatives):\n            for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                if arg == r_arg.name:\n                    names_of_args_with_derivatives[i] = f_arg.name\n    return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]",
            "def emit_any_requires_grad() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    extra_condition = ''\n    if info and info.output_differentiability_conditions:\n        assert len(info.output_differentiability_conditions) == 1\n        extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n    names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(names_of_args_with_derivatives):\n            for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                if arg == r_arg.name:\n                    names_of_args_with_derivatives[i] = f_arg.name\n    return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]",
            "def emit_any_requires_grad() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    extra_condition = ''\n    if info and info.output_differentiability_conditions:\n        assert len(info.output_differentiability_conditions) == 1\n        extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n    names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(names_of_args_with_derivatives):\n            for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                if arg == r_arg.name:\n                    names_of_args_with_derivatives[i] = f_arg.name\n    return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]",
            "def emit_any_requires_grad() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    extra_condition = ''\n    if info and info.output_differentiability_conditions:\n        assert len(info.output_differentiability_conditions) == 1\n        extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n    names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(names_of_args_with_derivatives):\n            for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                if arg == r_arg.name:\n                    names_of_args_with_derivatives[i] = f_arg.name\n    return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]",
            "def emit_any_requires_grad() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    extra_condition = ''\n    if info and info.output_differentiability_conditions:\n        assert len(info.output_differentiability_conditions) == 1\n        extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n    names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n    if is_inplace_foreach and info is not None:\n        for (i, arg) in enumerate(names_of_args_with_derivatives):\n            for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                if arg == r_arg.name:\n                    names_of_args_with_derivatives[i] = f_arg.name\n    return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]"
        ]
    },
    {
        "func_name": "get_any_has_forward_grad_name",
        "original": "def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n    if len(var_names) == 1:\n        return f'_any_has_forward_grad_{var_names[0]}'\n    else:\n        return f\"_any_has_forward_grad_{'_'.join(var_names)}\"",
        "mutated": [
            "def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n    if False:\n        i = 10\n    if len(var_names) == 1:\n        return f'_any_has_forward_grad_{var_names[0]}'\n    else:\n        return f\"_any_has_forward_grad_{'_'.join(var_names)}\"",
            "def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if len(var_names) == 1:\n        return f'_any_has_forward_grad_{var_names[0]}'\n    else:\n        return f\"_any_has_forward_grad_{'_'.join(var_names)}\"",
            "def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if len(var_names) == 1:\n        return f'_any_has_forward_grad_{var_names[0]}'\n    else:\n        return f\"_any_has_forward_grad_{'_'.join(var_names)}\"",
            "def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if len(var_names) == 1:\n        return f'_any_has_forward_grad_{var_names[0]}'\n    else:\n        return f\"_any_has_forward_grad_{'_'.join(var_names)}\"",
            "def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if len(var_names) == 1:\n        return f'_any_has_forward_grad_{var_names[0]}'\n    else:\n        return f\"_any_has_forward_grad_{'_'.join(var_names)}\""
        ]
    },
    {
        "func_name": "emit_any_has_forward_grad",
        "original": "def emit_any_has_forward_grad() -> List[str]:\n    content: List[str] = []\n    if not is_foreach:\n        for derivative in fw_derivatives:\n            requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n            if info and info.output_differentiability_conditions:\n                assert len(info.output_differentiability_conditions) == 1\n                requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n            content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n    else:\n        for derivative in fw_derivatives:\n            bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n            cur_derivative_conditions = []\n            for inp in differentiable_inputs:\n                if derivative.required_inputs_fw_grad is None:\n                    continue\n                if inp.name not in derivative.required_inputs_fw_grad:\n                    continue\n                inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                is_list_type = is_tensor_list_type(inp_type)\n                if is_list_type:\n                    if inp_name != 'self':\n                        content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                else:\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n            content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n            content.append('for (const auto& i : c10::irange(self.size())) {')\n            content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n            content.append('}')\n    return content",
        "mutated": [
            "def emit_any_has_forward_grad() -> List[str]:\n    if False:\n        i = 10\n    content: List[str] = []\n    if not is_foreach:\n        for derivative in fw_derivatives:\n            requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n            if info and info.output_differentiability_conditions:\n                assert len(info.output_differentiability_conditions) == 1\n                requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n            content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n    else:\n        for derivative in fw_derivatives:\n            bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n            cur_derivative_conditions = []\n            for inp in differentiable_inputs:\n                if derivative.required_inputs_fw_grad is None:\n                    continue\n                if inp.name not in derivative.required_inputs_fw_grad:\n                    continue\n                inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                is_list_type = is_tensor_list_type(inp_type)\n                if is_list_type:\n                    if inp_name != 'self':\n                        content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                else:\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n            content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n            content.append('for (const auto& i : c10::irange(self.size())) {')\n            content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n            content.append('}')\n    return content",
            "def emit_any_has_forward_grad() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    content: List[str] = []\n    if not is_foreach:\n        for derivative in fw_derivatives:\n            requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n            if info and info.output_differentiability_conditions:\n                assert len(info.output_differentiability_conditions) == 1\n                requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n            content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n    else:\n        for derivative in fw_derivatives:\n            bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n            cur_derivative_conditions = []\n            for inp in differentiable_inputs:\n                if derivative.required_inputs_fw_grad is None:\n                    continue\n                if inp.name not in derivative.required_inputs_fw_grad:\n                    continue\n                inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                is_list_type = is_tensor_list_type(inp_type)\n                if is_list_type:\n                    if inp_name != 'self':\n                        content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                else:\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n            content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n            content.append('for (const auto& i : c10::irange(self.size())) {')\n            content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n            content.append('}')\n    return content",
            "def emit_any_has_forward_grad() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    content: List[str] = []\n    if not is_foreach:\n        for derivative in fw_derivatives:\n            requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n            if info and info.output_differentiability_conditions:\n                assert len(info.output_differentiability_conditions) == 1\n                requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n            content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n    else:\n        for derivative in fw_derivatives:\n            bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n            cur_derivative_conditions = []\n            for inp in differentiable_inputs:\n                if derivative.required_inputs_fw_grad is None:\n                    continue\n                if inp.name not in derivative.required_inputs_fw_grad:\n                    continue\n                inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                is_list_type = is_tensor_list_type(inp_type)\n                if is_list_type:\n                    if inp_name != 'self':\n                        content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                else:\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n            content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n            content.append('for (const auto& i : c10::irange(self.size())) {')\n            content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n            content.append('}')\n    return content",
            "def emit_any_has_forward_grad() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    content: List[str] = []\n    if not is_foreach:\n        for derivative in fw_derivatives:\n            requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n            if info and info.output_differentiability_conditions:\n                assert len(info.output_differentiability_conditions) == 1\n                requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n            content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n    else:\n        for derivative in fw_derivatives:\n            bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n            cur_derivative_conditions = []\n            for inp in differentiable_inputs:\n                if derivative.required_inputs_fw_grad is None:\n                    continue\n                if inp.name not in derivative.required_inputs_fw_grad:\n                    continue\n                inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                is_list_type = is_tensor_list_type(inp_type)\n                if is_list_type:\n                    if inp_name != 'self':\n                        content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                else:\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n            content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n            content.append('for (const auto& i : c10::irange(self.size())) {')\n            content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n            content.append('}')\n    return content",
            "def emit_any_has_forward_grad() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    content: List[str] = []\n    if not is_foreach:\n        for derivative in fw_derivatives:\n            requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n            if info and info.output_differentiability_conditions:\n                assert len(info.output_differentiability_conditions) == 1\n                requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n            content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n    else:\n        for derivative in fw_derivatives:\n            bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n            cur_derivative_conditions = []\n            for inp in differentiable_inputs:\n                if derivative.required_inputs_fw_grad is None:\n                    continue\n                if inp.name not in derivative.required_inputs_fw_grad:\n                    continue\n                inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                is_list_type = is_tensor_list_type(inp_type)\n                if is_list_type:\n                    if inp_name != 'self':\n                        content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                else:\n                    cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n            content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n            content.append('for (const auto& i : c10::irange(self.size())) {')\n            content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n            content.append('}')\n    return content"
        ]
    },
    {
        "func_name": "emit_check_inplace",
        "original": "def emit_check_inplace() -> List[str]:\n    if not inplace:\n        return []\n    return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]",
        "mutated": [
            "def emit_check_inplace() -> List[str]:\n    if False:\n        i = 10\n    if not inplace:\n        return []\n    return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]",
            "def emit_check_inplace() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not inplace:\n        return []\n    return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]",
            "def emit_check_inplace() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not inplace:\n        return []\n    return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]",
            "def emit_check_inplace() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not inplace:\n        return []\n    return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]",
            "def emit_check_inplace() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not inplace:\n        return []\n    return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]"
        ]
    },
    {
        "func_name": "emit_fw_derivatives",
        "original": "def emit_fw_derivatives() -> List[str]:\n    content: List[str] = []\n    fw_grad_setters: List[str] = []\n    for derivative in fw_derivatives:\n        res = derivative.var_names\n        if f.func.name.name.inplace:\n            assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n            res = ('self',)\n        assert derivative.required_inputs_fw_grad is not None\n        unpacked_arguments = ''\n        for inp in differentiable_inputs:\n            inp_name = inp.name\n            is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n            input_suffix = '[i]' if is_input_tensorlist else ''\n            if is_inplace_foreach:\n                if inp.name in refargname2inplace_foreacharg:\n                    inp_name = refargname2inplace_foreacharg[inp.name].name\n            zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n            if inp.name in derivative.required_inputs_fw_grad:\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n            if inp.name in (derivative.required_inputs_primal or []):\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n        if derivative.required_original_self_value:\n            input_suffix = 's[i]' if is_inplace_foreach else ''\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n        elif inplace and derivative.is_reusing_outplace_formula:\n            unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n        if inplace:\n            is_inplace_str = 'true'\n        else:\n            is_inplace_str = 'false'\n        requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n        if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n            if len(derivative.var_types) == 1:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                if not is_foreach:\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    assert res[0] == ('result' if not inplace else 'self')\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n            else:\n                tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                for (idx, single_res) in enumerate(res):\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n        elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n            assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n            if not is_foreach:\n                opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n        else:\n            raise RuntimeError('Unsupported output type for forward derivative')\n        if not is_foreach:\n            fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n            content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n        else:\n            fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n            foreach_forward_grad_formula = derivative.formula\n            _foreach_arg: Union[Argument, DifferentiableInput]\n            if inplace:\n                for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                    if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                        pattern = _foreach_arg.name\n                        if isinstance(_foreach_arg.type, ListType):\n                            pattern += '[i]'\n                        foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n            elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n            content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n    content.append('\\n'.join(fw_grad_setters))\n    return content",
        "mutated": [
            "def emit_fw_derivatives() -> List[str]:\n    if False:\n        i = 10\n    content: List[str] = []\n    fw_grad_setters: List[str] = []\n    for derivative in fw_derivatives:\n        res = derivative.var_names\n        if f.func.name.name.inplace:\n            assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n            res = ('self',)\n        assert derivative.required_inputs_fw_grad is not None\n        unpacked_arguments = ''\n        for inp in differentiable_inputs:\n            inp_name = inp.name\n            is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n            input_suffix = '[i]' if is_input_tensorlist else ''\n            if is_inplace_foreach:\n                if inp.name in refargname2inplace_foreacharg:\n                    inp_name = refargname2inplace_foreacharg[inp.name].name\n            zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n            if inp.name in derivative.required_inputs_fw_grad:\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n            if inp.name in (derivative.required_inputs_primal or []):\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n        if derivative.required_original_self_value:\n            input_suffix = 's[i]' if is_inplace_foreach else ''\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n        elif inplace and derivative.is_reusing_outplace_formula:\n            unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n        if inplace:\n            is_inplace_str = 'true'\n        else:\n            is_inplace_str = 'false'\n        requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n        if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n            if len(derivative.var_types) == 1:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                if not is_foreach:\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    assert res[0] == ('result' if not inplace else 'self')\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n            else:\n                tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                for (idx, single_res) in enumerate(res):\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n        elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n            assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n            if not is_foreach:\n                opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n        else:\n            raise RuntimeError('Unsupported output type for forward derivative')\n        if not is_foreach:\n            fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n            content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n        else:\n            fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n            foreach_forward_grad_formula = derivative.formula\n            _foreach_arg: Union[Argument, DifferentiableInput]\n            if inplace:\n                for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                    if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                        pattern = _foreach_arg.name\n                        if isinstance(_foreach_arg.type, ListType):\n                            pattern += '[i]'\n                        foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n            elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n            content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n    content.append('\\n'.join(fw_grad_setters))\n    return content",
            "def emit_fw_derivatives() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    content: List[str] = []\n    fw_grad_setters: List[str] = []\n    for derivative in fw_derivatives:\n        res = derivative.var_names\n        if f.func.name.name.inplace:\n            assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n            res = ('self',)\n        assert derivative.required_inputs_fw_grad is not None\n        unpacked_arguments = ''\n        for inp in differentiable_inputs:\n            inp_name = inp.name\n            is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n            input_suffix = '[i]' if is_input_tensorlist else ''\n            if is_inplace_foreach:\n                if inp.name in refargname2inplace_foreacharg:\n                    inp_name = refargname2inplace_foreacharg[inp.name].name\n            zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n            if inp.name in derivative.required_inputs_fw_grad:\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n            if inp.name in (derivative.required_inputs_primal or []):\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n        if derivative.required_original_self_value:\n            input_suffix = 's[i]' if is_inplace_foreach else ''\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n        elif inplace and derivative.is_reusing_outplace_formula:\n            unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n        if inplace:\n            is_inplace_str = 'true'\n        else:\n            is_inplace_str = 'false'\n        requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n        if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n            if len(derivative.var_types) == 1:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                if not is_foreach:\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    assert res[0] == ('result' if not inplace else 'self')\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n            else:\n                tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                for (idx, single_res) in enumerate(res):\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n        elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n            assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n            if not is_foreach:\n                opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n        else:\n            raise RuntimeError('Unsupported output type for forward derivative')\n        if not is_foreach:\n            fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n            content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n        else:\n            fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n            foreach_forward_grad_formula = derivative.formula\n            _foreach_arg: Union[Argument, DifferentiableInput]\n            if inplace:\n                for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                    if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                        pattern = _foreach_arg.name\n                        if isinstance(_foreach_arg.type, ListType):\n                            pattern += '[i]'\n                        foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n            elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n            content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n    content.append('\\n'.join(fw_grad_setters))\n    return content",
            "def emit_fw_derivatives() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    content: List[str] = []\n    fw_grad_setters: List[str] = []\n    for derivative in fw_derivatives:\n        res = derivative.var_names\n        if f.func.name.name.inplace:\n            assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n            res = ('self',)\n        assert derivative.required_inputs_fw_grad is not None\n        unpacked_arguments = ''\n        for inp in differentiable_inputs:\n            inp_name = inp.name\n            is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n            input_suffix = '[i]' if is_input_tensorlist else ''\n            if is_inplace_foreach:\n                if inp.name in refargname2inplace_foreacharg:\n                    inp_name = refargname2inplace_foreacharg[inp.name].name\n            zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n            if inp.name in derivative.required_inputs_fw_grad:\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n            if inp.name in (derivative.required_inputs_primal or []):\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n        if derivative.required_original_self_value:\n            input_suffix = 's[i]' if is_inplace_foreach else ''\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n        elif inplace and derivative.is_reusing_outplace_formula:\n            unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n        if inplace:\n            is_inplace_str = 'true'\n        else:\n            is_inplace_str = 'false'\n        requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n        if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n            if len(derivative.var_types) == 1:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                if not is_foreach:\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    assert res[0] == ('result' if not inplace else 'self')\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n            else:\n                tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                for (idx, single_res) in enumerate(res):\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n        elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n            assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n            if not is_foreach:\n                opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n        else:\n            raise RuntimeError('Unsupported output type for forward derivative')\n        if not is_foreach:\n            fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n            content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n        else:\n            fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n            foreach_forward_grad_formula = derivative.formula\n            _foreach_arg: Union[Argument, DifferentiableInput]\n            if inplace:\n                for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                    if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                        pattern = _foreach_arg.name\n                        if isinstance(_foreach_arg.type, ListType):\n                            pattern += '[i]'\n                        foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n            elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n            content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n    content.append('\\n'.join(fw_grad_setters))\n    return content",
            "def emit_fw_derivatives() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    content: List[str] = []\n    fw_grad_setters: List[str] = []\n    for derivative in fw_derivatives:\n        res = derivative.var_names\n        if f.func.name.name.inplace:\n            assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n            res = ('self',)\n        assert derivative.required_inputs_fw_grad is not None\n        unpacked_arguments = ''\n        for inp in differentiable_inputs:\n            inp_name = inp.name\n            is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n            input_suffix = '[i]' if is_input_tensorlist else ''\n            if is_inplace_foreach:\n                if inp.name in refargname2inplace_foreacharg:\n                    inp_name = refargname2inplace_foreacharg[inp.name].name\n            zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n            if inp.name in derivative.required_inputs_fw_grad:\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n            if inp.name in (derivative.required_inputs_primal or []):\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n        if derivative.required_original_self_value:\n            input_suffix = 's[i]' if is_inplace_foreach else ''\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n        elif inplace and derivative.is_reusing_outplace_formula:\n            unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n        if inplace:\n            is_inplace_str = 'true'\n        else:\n            is_inplace_str = 'false'\n        requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n        if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n            if len(derivative.var_types) == 1:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                if not is_foreach:\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    assert res[0] == ('result' if not inplace else 'self')\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n            else:\n                tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                for (idx, single_res) in enumerate(res):\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n        elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n            assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n            if not is_foreach:\n                opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n        else:\n            raise RuntimeError('Unsupported output type for forward derivative')\n        if not is_foreach:\n            fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n            content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n        else:\n            fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n            foreach_forward_grad_formula = derivative.formula\n            _foreach_arg: Union[Argument, DifferentiableInput]\n            if inplace:\n                for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                    if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                        pattern = _foreach_arg.name\n                        if isinstance(_foreach_arg.type, ListType):\n                            pattern += '[i]'\n                        foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n            elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n            content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n    content.append('\\n'.join(fw_grad_setters))\n    return content",
            "def emit_fw_derivatives() -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    content: List[str] = []\n    fw_grad_setters: List[str] = []\n    for derivative in fw_derivatives:\n        res = derivative.var_names\n        if f.func.name.name.inplace:\n            assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n            res = ('self',)\n        assert derivative.required_inputs_fw_grad is not None\n        unpacked_arguments = ''\n        for inp in differentiable_inputs:\n            inp_name = inp.name\n            is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n            input_suffix = '[i]' if is_input_tensorlist else ''\n            if is_inplace_foreach:\n                if inp.name in refargname2inplace_foreacharg:\n                    inp_name = refargname2inplace_foreacharg[inp.name].name\n            zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n            if inp.name in derivative.required_inputs_fw_grad:\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n            if inp.name in (derivative.required_inputs_primal or []):\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n        if derivative.required_original_self_value:\n            input_suffix = 's[i]' if is_inplace_foreach else ''\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n            unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n        elif inplace and derivative.is_reusing_outplace_formula:\n            unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n        if inplace:\n            is_inplace_str = 'true'\n        else:\n            is_inplace_str = 'false'\n        requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n        if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n            if len(derivative.var_types) == 1:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                if not is_foreach:\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    assert res[0] == ('result' if not inplace else 'self')\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n            else:\n                tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                for (idx, single_res) in enumerate(res):\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n        elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n            assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n            if not is_foreach:\n                opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n        else:\n            raise RuntimeError('Unsupported output type for forward derivative')\n        if not is_foreach:\n            fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n            content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n        else:\n            fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n            foreach_forward_grad_formula = derivative.formula\n            _foreach_arg: Union[Argument, DifferentiableInput]\n            if inplace:\n                for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                    if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                        pattern = _foreach_arg.name\n                        if isinstance(_foreach_arg.type, ListType):\n                            pattern += '[i]'\n                        foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n            elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n            content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n    content.append('\\n'.join(fw_grad_setters))\n    return content"
        ]
    },
    {
        "func_name": "get_any_has_fw_grad_cond",
        "original": "def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n    if derivative is None:\n        to_check: List[str] = []\n        for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n            if is_tensor_type(inp.type):\n                to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            elif is_tensor_list_type(inp.type):\n                to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            else:\n                raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n        return f\"({' || '.join(to_check)})\"\n    else:\n        assert derivative.required_inputs_fw_grad is not None\n        if len(derivative.required_inputs_fw_grad) == 0:\n            if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n            any_has_fw_grad = 'true'\n        else:\n            any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n            any_has_fw_grad = f'({any_has_fw_grad})'\n        return any_has_fw_grad",
        "mutated": [
            "def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n    if False:\n        i = 10\n    if derivative is None:\n        to_check: List[str] = []\n        for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n            if is_tensor_type(inp.type):\n                to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            elif is_tensor_list_type(inp.type):\n                to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            else:\n                raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n        return f\"({' || '.join(to_check)})\"\n    else:\n        assert derivative.required_inputs_fw_grad is not None\n        if len(derivative.required_inputs_fw_grad) == 0:\n            if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n            any_has_fw_grad = 'true'\n        else:\n            any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n            any_has_fw_grad = f'({any_has_fw_grad})'\n        return any_has_fw_grad",
            "def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if derivative is None:\n        to_check: List[str] = []\n        for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n            if is_tensor_type(inp.type):\n                to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            elif is_tensor_list_type(inp.type):\n                to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            else:\n                raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n        return f\"({' || '.join(to_check)})\"\n    else:\n        assert derivative.required_inputs_fw_grad is not None\n        if len(derivative.required_inputs_fw_grad) == 0:\n            if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n            any_has_fw_grad = 'true'\n        else:\n            any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n            any_has_fw_grad = f'({any_has_fw_grad})'\n        return any_has_fw_grad",
            "def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if derivative is None:\n        to_check: List[str] = []\n        for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n            if is_tensor_type(inp.type):\n                to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            elif is_tensor_list_type(inp.type):\n                to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            else:\n                raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n        return f\"({' || '.join(to_check)})\"\n    else:\n        assert derivative.required_inputs_fw_grad is not None\n        if len(derivative.required_inputs_fw_grad) == 0:\n            if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n            any_has_fw_grad = 'true'\n        else:\n            any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n            any_has_fw_grad = f'({any_has_fw_grad})'\n        return any_has_fw_grad",
            "def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if derivative is None:\n        to_check: List[str] = []\n        for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n            if is_tensor_type(inp.type):\n                to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            elif is_tensor_list_type(inp.type):\n                to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            else:\n                raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n        return f\"({' || '.join(to_check)})\"\n    else:\n        assert derivative.required_inputs_fw_grad is not None\n        if len(derivative.required_inputs_fw_grad) == 0:\n            if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n            any_has_fw_grad = 'true'\n        else:\n            any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n            any_has_fw_grad = f'({any_has_fw_grad})'\n        return any_has_fw_grad",
            "def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if derivative is None:\n        to_check: List[str] = []\n        for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n            if is_tensor_type(inp.type):\n                to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            elif is_tensor_list_type(inp.type):\n                to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n            else:\n                raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n        return f\"({' || '.join(to_check)})\"\n    else:\n        assert derivative.required_inputs_fw_grad is not None\n        if len(derivative.required_inputs_fw_grad) == 0:\n            if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n            any_has_fw_grad = 'true'\n        else:\n            any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n            any_has_fw_grad = f'({any_has_fw_grad})'\n        return any_has_fw_grad"
        ]
    },
    {
        "func_name": "emit_forbid_fw_derivatives",
        "original": "def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n    if is_out_fn:\n        msg = 'because it is an out= function'\n    else:\n        msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n    cond = get_any_has_fw_grad_cond(derivative=None)\n    return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''",
        "mutated": [
            "def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n    if False:\n        i = 10\n    if is_out_fn:\n        msg = 'because it is an out= function'\n    else:\n        msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n    cond = get_any_has_fw_grad_cond(derivative=None)\n    return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''",
            "def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if is_out_fn:\n        msg = 'because it is an out= function'\n    else:\n        msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n    cond = get_any_has_fw_grad_cond(derivative=None)\n    return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''",
            "def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if is_out_fn:\n        msg = 'because it is an out= function'\n    else:\n        msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n    cond = get_any_has_fw_grad_cond(derivative=None)\n    return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''",
            "def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if is_out_fn:\n        msg = 'because it is an out= function'\n    else:\n        msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n    cond = get_any_has_fw_grad_cond(derivative=None)\n    return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''",
            "def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if is_out_fn:\n        msg = 'because it is an out= function'\n    else:\n        msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n    cond = get_any_has_fw_grad_cond(derivative=None)\n    return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''"
        ]
    },
    {
        "func_name": "emit_body",
        "original": "@with_native_function_with_differentiability_info_and_key\ndef emit_body(fn: NativeFunctionWithDifferentiabilityInfo, key: str='Default') -> List[str]:\n    assert dispatch_strategy(fn) == 'use_derived'\n    f = fn.func\n    info = fn.info[key] if fn.info else None\n    fw_derivatives = fn.fw_derivatives.get(key, []) if fn.fw_derivatives else []\n    name = cpp.name(f.func)\n    inplace = f.func.kind() == SchemaKind.inplace\n    is_out_fn = f.func.kind() == SchemaKind.out\n    returns_void = len(f.func.returns) == 0\n    base_name = get_base_name(f)\n    view_info = get_view_info(f)\n    is_foreach = name.startswith('_foreach')\n    is_inplace_foreach = is_foreach and inplace\n    if is_inplace_foreach:\n        inplace_foreacharg2refarg: Dict[Argument, Argument] = {}\n        refargname2inplace_foreacharg: Dict[str, Argument] = {}\n        base_name_and_overload_name = (f.func.name.name.base, f.func.name.overload_name)\n        if info is None:\n            assert base_name_and_overload_name in _foreach_ops_without_differentiability_info, f\"{'.'.join(base_name_and_overload_name)} should have a differentiability info\"\n        else:\n            assert len(f.func.arguments.flat_non_out) == len(info.func.func.arguments.flat_non_out) or base_name_and_overload_name in _foreach_ops_with_different_arity, f\"{'.'.join(base_name_and_overload_name)} has {len(f.func.arguments.flat_non_out)} args but the reference has {len(info.func.func.arguments.flat_non_out)}\"\n            for (foreach_arg, ref_arg) in zip(f.func.arguments.flat_non_out, info.func.func.arguments.flat_non_out):\n                foreach_arg_type = foreach_arg.type\n                if isinstance(foreach_arg_type, ListType):\n                    foreach_arg_type = foreach_arg_type.elem\n                assert foreach_arg_type == ref_arg.type\n                inplace_foreacharg2refarg[foreach_arg] = ref_arg\n                refargname2inplace_foreacharg[ref_arg.name] = foreach_arg\n\n    def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n        if isinstance(arg, TensorOptionsArguments):\n            return None\n        a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n        cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n        if not is_differentiable(a.name, a.type, info):\n            return None\n        return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)\n\n    @with_native_function\n    def gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n        arguments = list(f.func.arguments.non_out)\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n                if arg in inplace_foreacharg2refarg:\n                    mapped_arg = inplace_foreacharg2refarg[arg]\n                    arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n        return list(mapMaybe(gen_differentiable_input, arguments))\n\n    def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n        \"\"\"Find arguments that have derivative definitions\"\"\"\n        if info is None or not info.has_derivatives:\n            return differentiable_inputs\n        names = {name for d in info.derivatives for name in d.var_names}\n        differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n        if len(differentiable) != len(names):\n            missing = names - {arg.name for arg in differentiable}\n            raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n        return differentiable\n    differentiable_inputs = gen_differentiable_inputs(f)\n    args_with_derivatives = find_args_with_derivatives(differentiable_inputs)\n    differentiable_outputs = gen_differentiable_outputs(fn, key)\n    undifferentiable = base_name in DONT_REQUIRE_DERIVATIVE or name in DONT_REQUIRE_DERIVATIVE\n    requires_derivative = not undifferentiable and len(differentiable_inputs) > 0 and (len(differentiable_outputs) > 0 or is_inplace_foreach)\n    if info is not None and info.has_derivatives and (not requires_derivative) and (len(f.func.returns) > 0):\n        raise RuntimeError(f'ERROR: derivative ignored for {name} -- specified an autograd function without derivative')\n    if requires_derivative and len(fw_derivatives) > 0 and (not is_inplace_foreach):\n        assert sum((len(derivative.var_names) for derivative in fw_derivatives)) == len(differentiable_outputs), 'Expected the number of forward derivatives implemented to match the number of differentiable outputs. NB: This only applies when at least one forward derivative is implemented. Not implementing any forward derivatives is also okay, and we would require inputs to the op to not have associated tangents in that case.'\n    try_jit_decomposition = requires_derivative and len(fw_derivatives) == 0 and (not modifies_arguments(f)) and (not returns_void)\n\n    def emit_save_inputs() -> List[str]:\n        setup: List[str] = []\n        if info is None or not info.has_derivatives:\n            return setup\n        has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n        def guard_for(arg: SavedAttribute) -> Optional[str]:\n            assert info is not None\n            if has_tensorlist_arg and (not is_inplace_foreach):\n                return None\n            if 'backward' in info.name:\n                return None\n            if len(args_with_derivatives) <= 1:\n                return None\n            if arg.nctype.type != BaseCType(tensorT):\n                return None\n            used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n            assert len(used_in) > 0\n            if len(used_in) != 1:\n                return None\n            derivative = used_in[0]\n            if len(derivative.var_names) != 1:\n                wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n                if wrap_opt_if_start == -1:\n                    return None\n                wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n                assert wrap_opt_if_match is not None\n                condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n                wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n                wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n                return f'{wrap_opt_if_condition}'\n            derivative_var_name = derivative.var_names[0]\n            for (edge_off, a) in enumerate(args_with_derivatives):\n                if a.name == derivative_var_name:\n                    break\n            else:\n                raise AssertionError()\n            return f'grad_fn->should_compute_output({edge_off})'\n        if is_inplace_foreach:\n            save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n            if save_input_stmts:\n                setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n        else:\n            setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n            for arg in args_with_derivatives:\n                if is_tensor_list_type(arg.type):\n                    setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n        return setup\n\n    def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n        body: List[str] = []\n        if is_out_fn:\n            body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n            return body\n        op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n        setup = []\n        if not is_inplace_foreach:\n            setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n        else:\n            list_like_arg = 'self'\n            args = [arg.name for arg in args_with_derivatives]\n            for (i, arg) in enumerate(args):\n                if is_inplace_foreach and info is not None:\n                    if arg in refargname2inplace_foreacharg:\n                        foreach_arg = refargname2inplace_foreacharg[arg]\n                        args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n                elif arg == list_like_arg:\n                    args[i] = arg + '[i]'\n            setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n        setup.extend(emit_save_inputs())\n        body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n        declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n        body.append(declare_grad_fn_template.substitute(op=op))\n        body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n        return body\n\n    def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n        body: List[str] = []\n        if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n            return body\n        for arg in differentiable_outputs:\n            name = arg.name\n            if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n                body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n        return body\n\n    def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n        \"\"\"Checks that arguments without derivatives don't require grad\"\"\"\n        body: List[str] = []\n        for arg in tensor_args:\n            if arg in args_with_derivatives:\n                continue\n            arg_name = arg.name\n            if info and arg_name in info.non_differentiable_arg_names:\n                continue\n            if arg_name == 'output':\n                continue\n            body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n        return body\n\n    def emit_original_self_definition() -> List[str]:\n        body: List[str] = []\n        if inplace:\n            if is_inplace_foreach:\n                body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n            else:\n                body.append('c10::optional<at::Tensor> original_self;')\n            all_forward_grad_cond = []\n            for derivative in fw_derivatives:\n                if derivative.required_original_self_value:\n                    all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n            if all_forward_grad_cond:\n                if not is_inplace_foreach:\n                    body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                    body.append('  original_self = self.clone();')\n                    body.append('}')\n                else:\n                    current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                    body.append('for (const auto& i : c10::irange(self.size())) {')\n                    body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                    body.append('    original_selfs[i] = self[i].clone();')\n                    body.append('  }')\n                    body.append('}')\n        return body\n\n    def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n        stmts: List[str] = []\n        for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n            name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n            foreacharg: Optional[Argument] = None\n            is_foreacharg_list_type: bool = False\n            type = arg.nctype.type\n            expr = arg.expr\n            stmts_prepend = None\n            if is_inplace_foreach and info is not None:\n                name_to_query = name.split('_scalar_type')[0]\n                if name_to_query in refargname2inplace_foreacharg:\n                    foreacharg = refargname2inplace_foreacharg[name_to_query]\n                    is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n                if foreacharg is not None:\n                    name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                    src_name = name\n                    if '_scalar_type' in src_name:\n                        split_src_name = src_name.split('_scalar_type')\n                        assert len(split_src_name) == 2\n                        src_name = split_src_name[0]\n                    expr = expr.replace(src_name, name_in_expr)\n            if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n                var = name\n                name += '_'\n                if var == 'self' and inplace:\n                    original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                    self_var = var if not is_inplace_foreach else var + '[i]'\n                    stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                    var = f'{original_self_var}.value()'\n                    assert not is_output\n                if inplace and is_output:\n                    assert name == 'result_'\n                    var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                    is_inplace_view = f'{var}.is_view()'\n                    expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n                else:\n                    expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                    if foreacharg is not None and 'original_selfs' not in expr:\n                        expr = expr.replace(src_name, name_in_expr)\n            elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n                if type == VectorCType(BaseCType(tensorT)):\n                    assert is_foreach and is_output\n                expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n                name += '_'\n            elif type == BaseCType(intArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(symIntArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(stringT):\n                expr = f'std::string({expr})'\n            elif type == OptionalCType(BaseCType(stringT)):\n                expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n            elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n                expr = expr + '.vec()'\n            guard = guard_for(arg)\n            if guard is None:\n                if stmts_prepend:\n                    stmts.append(f'{stmts_prepend};')\n                stmts.append(f'grad_fn->{name} = {expr};')\n            else:\n                stmts.append(f'if ({guard}) {{')\n                if stmts_prepend:\n                    stmts.append(f'  {stmts_prepend};')\n                stmts.append(f'  grad_fn->{name} = {expr};')\n                stmts.append('}')\n        return stmts\n\n    def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n        \"\"\"Dispatch call via function in a namespace or method on Tensor.\"\"\"\n        dispatcher_sig = DispatcherSignature.from_schema(f.func)\n        dispatcher_exprs = dispatcher_sig.exprs()\n        dispatch_key_set = 'ks & c10::after_autograd_keyset'\n        call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n        return call\n\n    def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n        call = ''\n        rhs_value: Optional[str] = None\n        if not any((r.type.is_tensor_like() for r in f.func.returns)):\n            rhs_value = var\n        else:\n            rhs_value = f'std::move({var})'\n        assert rhs_value is not None\n        call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n        return call\n\n    def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n        stmts_before_call: List[str] = []\n        stmts_after_call: List[str] = []\n        if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n            return call\n        for unpacked_binding in unpacked_bindings:\n            arg = unpacked_binding.name\n            noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n                stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == BaseCType(tensorT):\n                stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n        assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n        if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n            base_name = f.func.name.name.base\n            aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n            if aliased_arg_name is not None:\n                aliased_arg_name = unpacked_name(aliased_arg_name)\n            for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n                noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n                if noref_cpp_type == BaseCType(tensorT):\n                    if aliased_arg_name is not None:\n                        assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                        stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                    elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                    if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n                elif noref_cpp_type == BaseCType(tensorListT):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n        if stmts_before_call and stmts_after_call:\n            call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n        return call\n\n    def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n        unpacked_args = [b.name for b in unpacked_bindings]\n        base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n        if get_view_info(f) is not None or modifies_arguments(f):\n            guard = 'at::AutoDispatchBelowAutograd guard;'\n        else:\n            guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n        any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n        return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n        if len(f.func.returns) > 1:\n            return_types = f'std::tuple<{return_types}>'\n        arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n        if not modifies_arguments(f) and (not returns_void):\n            if try_jit_decomposition:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n            else:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n            call += wrap_output(f, unpacked_bindings, TMP_VAR)\n        else:\n            assert not try_jit_decomposition\n            call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n        call = check_tensorimpl_and_storage(call, unpacked_bindings)\n        return call\n\n    def emit_history() -> str:\n        fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n        output_names = [r.name for r in differentiable_outputs]\n        outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n        if not is_inplace_foreach:\n            return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')\n\n    def emit_save_outputs() -> str:\n        if is_out_fn:\n            return ''\n        if info is not None and info.has_derivatives:\n            stmts = save_variables(info.all_saved_outputs, True)\n            if len(stmts) == 0:\n                return ''\n            if not is_inplace_foreach:\n                return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n            else:\n                return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n        return ''\n\n    def emit_any_requires_grad() -> List[str]:\n        extra_condition = ''\n        if info and info.output_differentiability_conditions:\n            assert len(info.output_differentiability_conditions) == 1\n            extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n        names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(names_of_args_with_derivatives):\n                for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                    if arg == r_arg.name:\n                        names_of_args_with_derivatives[i] = f_arg.name\n        return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]\n\n    def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n        if len(var_names) == 1:\n            return f'_any_has_forward_grad_{var_names[0]}'\n        else:\n            return f\"_any_has_forward_grad_{'_'.join(var_names)}\"\n\n    def emit_any_has_forward_grad() -> List[str]:\n        content: List[str] = []\n        if not is_foreach:\n            for derivative in fw_derivatives:\n                requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n                if info and info.output_differentiability_conditions:\n                    assert len(info.output_differentiability_conditions) == 1\n                    requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n                content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n        else:\n            for derivative in fw_derivatives:\n                bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n                cur_derivative_conditions = []\n                for inp in differentiable_inputs:\n                    if derivative.required_inputs_fw_grad is None:\n                        continue\n                    if inp.name not in derivative.required_inputs_fw_grad:\n                        continue\n                    inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                    inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                    is_list_type = is_tensor_list_type(inp_type)\n                    if is_list_type:\n                        if inp_name != 'self':\n                            content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                    else:\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n                content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n                content.append('for (const auto& i : c10::irange(self.size())) {')\n                content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n                content.append('}')\n        return content\n\n    def emit_check_inplace() -> List[str]:\n        if not inplace:\n            return []\n        return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]\n\n    def emit_fw_derivatives() -> List[str]:\n        content: List[str] = []\n        fw_grad_setters: List[str] = []\n        for derivative in fw_derivatives:\n            res = derivative.var_names\n            if f.func.name.name.inplace:\n                assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n                res = ('self',)\n            assert derivative.required_inputs_fw_grad is not None\n            unpacked_arguments = ''\n            for inp in differentiable_inputs:\n                inp_name = inp.name\n                is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n                input_suffix = '[i]' if is_input_tensorlist else ''\n                if is_inplace_foreach:\n                    if inp.name in refargname2inplace_foreacharg:\n                        inp_name = refargname2inplace_foreacharg[inp.name].name\n                zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n                if inp.name in derivative.required_inputs_fw_grad:\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n                if inp.name in (derivative.required_inputs_primal or []):\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n            if derivative.required_original_self_value:\n                input_suffix = 's[i]' if is_inplace_foreach else ''\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n            elif inplace and derivative.is_reusing_outplace_formula:\n                unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n            if inplace:\n                is_inplace_str = 'true'\n            else:\n                is_inplace_str = 'false'\n            requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n            if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n                if len(derivative.var_types) == 1:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    if not is_foreach:\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    else:\n                        assert res[0] == ('result' if not inplace else 'self')\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n                else:\n                    tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                    opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                    for (idx, single_res) in enumerate(res):\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n            elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n                assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n                if not is_foreach:\n                    opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                raise RuntimeError('Unsupported output type for forward derivative')\n            if not is_foreach:\n                fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n                content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n            else:\n                fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n                foreach_forward_grad_formula = derivative.formula\n                _foreach_arg: Union[Argument, DifferentiableInput]\n                if inplace:\n                    for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                        if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                            pattern = _foreach_arg.name\n                            if isinstance(_foreach_arg.type, ListType):\n                                pattern += '[i]'\n                            foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n                elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                    foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n                content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n        content.append('\\n'.join(fw_grad_setters))\n        return content\n\n    def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n        if derivative is None:\n            to_check: List[str] = []\n            for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n                if is_tensor_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                elif is_tensor_list_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                else:\n                    raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n            return f\"({' || '.join(to_check)})\"\n        else:\n            assert derivative.required_inputs_fw_grad is not None\n            if len(derivative.required_inputs_fw_grad) == 0:\n                if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                    raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n                any_has_fw_grad = 'true'\n            else:\n                any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n                any_has_fw_grad = f'({any_has_fw_grad})'\n            return any_has_fw_grad\n\n    def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n        if is_out_fn:\n            msg = 'because it is an out= function'\n        else:\n            msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n        cond = get_any_has_fw_grad_cond(derivative=None)\n        return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''\n    body: List[str] = []\n    (unpack_args_stats, unpacked_bindings) = unpack_args(f)\n    body.extend(unpack_args_stats)\n    if requires_derivative:\n        body.extend(emit_any_requires_grad())\n        body.extend(emit_any_has_forward_grad())\n        body.extend(emit_check_inplace())\n        body.extend(emit_original_self_definition())\n        body.extend(setup_derivative(differentiable_inputs))\n    body.append(declare_returned_variables(f))\n    body.append(emit_call(f, unpacked_bindings, try_jit_decomposition))\n    if requires_derivative:\n        body.append(emit_history())\n        body.extend(emit_check_if_in_complex_autograd_allowlist())\n    if is_out_fn:\n        body.append(emit_forbid_fw_derivatives(is_out_fn=True))\n    elif requires_derivative and (not try_jit_decomposition):\n        if len(fw_derivatives) > 0:\n            body.extend(emit_fw_derivatives())\n        else:\n            body.append(emit_forbid_fw_derivatives())\n    if requires_derivative:\n        body.append(emit_save_outputs())\n    if str(f.func.name.name) in RESET_GRAD_ACCUMULATOR:\n        assert inplace\n        body.append('reset_grad_accumulator(self);')\n    if not returns_void:\n        body.append(f'return {get_return_value(f)};')\n    return body",
        "mutated": [
            "@with_native_function_with_differentiability_info_and_key\ndef emit_body(fn: NativeFunctionWithDifferentiabilityInfo, key: str='Default') -> List[str]:\n    if False:\n        i = 10\n    assert dispatch_strategy(fn) == 'use_derived'\n    f = fn.func\n    info = fn.info[key] if fn.info else None\n    fw_derivatives = fn.fw_derivatives.get(key, []) if fn.fw_derivatives else []\n    name = cpp.name(f.func)\n    inplace = f.func.kind() == SchemaKind.inplace\n    is_out_fn = f.func.kind() == SchemaKind.out\n    returns_void = len(f.func.returns) == 0\n    base_name = get_base_name(f)\n    view_info = get_view_info(f)\n    is_foreach = name.startswith('_foreach')\n    is_inplace_foreach = is_foreach and inplace\n    if is_inplace_foreach:\n        inplace_foreacharg2refarg: Dict[Argument, Argument] = {}\n        refargname2inplace_foreacharg: Dict[str, Argument] = {}\n        base_name_and_overload_name = (f.func.name.name.base, f.func.name.overload_name)\n        if info is None:\n            assert base_name_and_overload_name in _foreach_ops_without_differentiability_info, f\"{'.'.join(base_name_and_overload_name)} should have a differentiability info\"\n        else:\n            assert len(f.func.arguments.flat_non_out) == len(info.func.func.arguments.flat_non_out) or base_name_and_overload_name in _foreach_ops_with_different_arity, f\"{'.'.join(base_name_and_overload_name)} has {len(f.func.arguments.flat_non_out)} args but the reference has {len(info.func.func.arguments.flat_non_out)}\"\n            for (foreach_arg, ref_arg) in zip(f.func.arguments.flat_non_out, info.func.func.arguments.flat_non_out):\n                foreach_arg_type = foreach_arg.type\n                if isinstance(foreach_arg_type, ListType):\n                    foreach_arg_type = foreach_arg_type.elem\n                assert foreach_arg_type == ref_arg.type\n                inplace_foreacharg2refarg[foreach_arg] = ref_arg\n                refargname2inplace_foreacharg[ref_arg.name] = foreach_arg\n\n    def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n        if isinstance(arg, TensorOptionsArguments):\n            return None\n        a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n        cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n        if not is_differentiable(a.name, a.type, info):\n            return None\n        return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)\n\n    @with_native_function\n    def gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n        arguments = list(f.func.arguments.non_out)\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n                if arg in inplace_foreacharg2refarg:\n                    mapped_arg = inplace_foreacharg2refarg[arg]\n                    arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n        return list(mapMaybe(gen_differentiable_input, arguments))\n\n    def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n        \"\"\"Find arguments that have derivative definitions\"\"\"\n        if info is None or not info.has_derivatives:\n            return differentiable_inputs\n        names = {name for d in info.derivatives for name in d.var_names}\n        differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n        if len(differentiable) != len(names):\n            missing = names - {arg.name for arg in differentiable}\n            raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n        return differentiable\n    differentiable_inputs = gen_differentiable_inputs(f)\n    args_with_derivatives = find_args_with_derivatives(differentiable_inputs)\n    differentiable_outputs = gen_differentiable_outputs(fn, key)\n    undifferentiable = base_name in DONT_REQUIRE_DERIVATIVE or name in DONT_REQUIRE_DERIVATIVE\n    requires_derivative = not undifferentiable and len(differentiable_inputs) > 0 and (len(differentiable_outputs) > 0 or is_inplace_foreach)\n    if info is not None and info.has_derivatives and (not requires_derivative) and (len(f.func.returns) > 0):\n        raise RuntimeError(f'ERROR: derivative ignored for {name} -- specified an autograd function without derivative')\n    if requires_derivative and len(fw_derivatives) > 0 and (not is_inplace_foreach):\n        assert sum((len(derivative.var_names) for derivative in fw_derivatives)) == len(differentiable_outputs), 'Expected the number of forward derivatives implemented to match the number of differentiable outputs. NB: This only applies when at least one forward derivative is implemented. Not implementing any forward derivatives is also okay, and we would require inputs to the op to not have associated tangents in that case.'\n    try_jit_decomposition = requires_derivative and len(fw_derivatives) == 0 and (not modifies_arguments(f)) and (not returns_void)\n\n    def emit_save_inputs() -> List[str]:\n        setup: List[str] = []\n        if info is None or not info.has_derivatives:\n            return setup\n        has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n        def guard_for(arg: SavedAttribute) -> Optional[str]:\n            assert info is not None\n            if has_tensorlist_arg and (not is_inplace_foreach):\n                return None\n            if 'backward' in info.name:\n                return None\n            if len(args_with_derivatives) <= 1:\n                return None\n            if arg.nctype.type != BaseCType(tensorT):\n                return None\n            used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n            assert len(used_in) > 0\n            if len(used_in) != 1:\n                return None\n            derivative = used_in[0]\n            if len(derivative.var_names) != 1:\n                wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n                if wrap_opt_if_start == -1:\n                    return None\n                wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n                assert wrap_opt_if_match is not None\n                condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n                wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n                wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n                return f'{wrap_opt_if_condition}'\n            derivative_var_name = derivative.var_names[0]\n            for (edge_off, a) in enumerate(args_with_derivatives):\n                if a.name == derivative_var_name:\n                    break\n            else:\n                raise AssertionError()\n            return f'grad_fn->should_compute_output({edge_off})'\n        if is_inplace_foreach:\n            save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n            if save_input_stmts:\n                setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n        else:\n            setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n            for arg in args_with_derivatives:\n                if is_tensor_list_type(arg.type):\n                    setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n        return setup\n\n    def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n        body: List[str] = []\n        if is_out_fn:\n            body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n            return body\n        op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n        setup = []\n        if not is_inplace_foreach:\n            setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n        else:\n            list_like_arg = 'self'\n            args = [arg.name for arg in args_with_derivatives]\n            for (i, arg) in enumerate(args):\n                if is_inplace_foreach and info is not None:\n                    if arg in refargname2inplace_foreacharg:\n                        foreach_arg = refargname2inplace_foreacharg[arg]\n                        args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n                elif arg == list_like_arg:\n                    args[i] = arg + '[i]'\n            setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n        setup.extend(emit_save_inputs())\n        body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n        declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n        body.append(declare_grad_fn_template.substitute(op=op))\n        body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n        return body\n\n    def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n        body: List[str] = []\n        if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n            return body\n        for arg in differentiable_outputs:\n            name = arg.name\n            if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n                body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n        return body\n\n    def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n        \"\"\"Checks that arguments without derivatives don't require grad\"\"\"\n        body: List[str] = []\n        for arg in tensor_args:\n            if arg in args_with_derivatives:\n                continue\n            arg_name = arg.name\n            if info and arg_name in info.non_differentiable_arg_names:\n                continue\n            if arg_name == 'output':\n                continue\n            body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n        return body\n\n    def emit_original_self_definition() -> List[str]:\n        body: List[str] = []\n        if inplace:\n            if is_inplace_foreach:\n                body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n            else:\n                body.append('c10::optional<at::Tensor> original_self;')\n            all_forward_grad_cond = []\n            for derivative in fw_derivatives:\n                if derivative.required_original_self_value:\n                    all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n            if all_forward_grad_cond:\n                if not is_inplace_foreach:\n                    body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                    body.append('  original_self = self.clone();')\n                    body.append('}')\n                else:\n                    current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                    body.append('for (const auto& i : c10::irange(self.size())) {')\n                    body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                    body.append('    original_selfs[i] = self[i].clone();')\n                    body.append('  }')\n                    body.append('}')\n        return body\n\n    def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n        stmts: List[str] = []\n        for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n            name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n            foreacharg: Optional[Argument] = None\n            is_foreacharg_list_type: bool = False\n            type = arg.nctype.type\n            expr = arg.expr\n            stmts_prepend = None\n            if is_inplace_foreach and info is not None:\n                name_to_query = name.split('_scalar_type')[0]\n                if name_to_query in refargname2inplace_foreacharg:\n                    foreacharg = refargname2inplace_foreacharg[name_to_query]\n                    is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n                if foreacharg is not None:\n                    name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                    src_name = name\n                    if '_scalar_type' in src_name:\n                        split_src_name = src_name.split('_scalar_type')\n                        assert len(split_src_name) == 2\n                        src_name = split_src_name[0]\n                    expr = expr.replace(src_name, name_in_expr)\n            if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n                var = name\n                name += '_'\n                if var == 'self' and inplace:\n                    original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                    self_var = var if not is_inplace_foreach else var + '[i]'\n                    stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                    var = f'{original_self_var}.value()'\n                    assert not is_output\n                if inplace and is_output:\n                    assert name == 'result_'\n                    var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                    is_inplace_view = f'{var}.is_view()'\n                    expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n                else:\n                    expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                    if foreacharg is not None and 'original_selfs' not in expr:\n                        expr = expr.replace(src_name, name_in_expr)\n            elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n                if type == VectorCType(BaseCType(tensorT)):\n                    assert is_foreach and is_output\n                expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n                name += '_'\n            elif type == BaseCType(intArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(symIntArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(stringT):\n                expr = f'std::string({expr})'\n            elif type == OptionalCType(BaseCType(stringT)):\n                expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n            elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n                expr = expr + '.vec()'\n            guard = guard_for(arg)\n            if guard is None:\n                if stmts_prepend:\n                    stmts.append(f'{stmts_prepend};')\n                stmts.append(f'grad_fn->{name} = {expr};')\n            else:\n                stmts.append(f'if ({guard}) {{')\n                if stmts_prepend:\n                    stmts.append(f'  {stmts_prepend};')\n                stmts.append(f'  grad_fn->{name} = {expr};')\n                stmts.append('}')\n        return stmts\n\n    def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n        \"\"\"Dispatch call via function in a namespace or method on Tensor.\"\"\"\n        dispatcher_sig = DispatcherSignature.from_schema(f.func)\n        dispatcher_exprs = dispatcher_sig.exprs()\n        dispatch_key_set = 'ks & c10::after_autograd_keyset'\n        call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n        return call\n\n    def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n        call = ''\n        rhs_value: Optional[str] = None\n        if not any((r.type.is_tensor_like() for r in f.func.returns)):\n            rhs_value = var\n        else:\n            rhs_value = f'std::move({var})'\n        assert rhs_value is not None\n        call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n        return call\n\n    def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n        stmts_before_call: List[str] = []\n        stmts_after_call: List[str] = []\n        if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n            return call\n        for unpacked_binding in unpacked_bindings:\n            arg = unpacked_binding.name\n            noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n                stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == BaseCType(tensorT):\n                stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n        assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n        if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n            base_name = f.func.name.name.base\n            aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n            if aliased_arg_name is not None:\n                aliased_arg_name = unpacked_name(aliased_arg_name)\n            for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n                noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n                if noref_cpp_type == BaseCType(tensorT):\n                    if aliased_arg_name is not None:\n                        assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                        stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                    elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                    if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n                elif noref_cpp_type == BaseCType(tensorListT):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n        if stmts_before_call and stmts_after_call:\n            call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n        return call\n\n    def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n        unpacked_args = [b.name for b in unpacked_bindings]\n        base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n        if get_view_info(f) is not None or modifies_arguments(f):\n            guard = 'at::AutoDispatchBelowAutograd guard;'\n        else:\n            guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n        any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n        return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n        if len(f.func.returns) > 1:\n            return_types = f'std::tuple<{return_types}>'\n        arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n        if not modifies_arguments(f) and (not returns_void):\n            if try_jit_decomposition:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n            else:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n            call += wrap_output(f, unpacked_bindings, TMP_VAR)\n        else:\n            assert not try_jit_decomposition\n            call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n        call = check_tensorimpl_and_storage(call, unpacked_bindings)\n        return call\n\n    def emit_history() -> str:\n        fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n        output_names = [r.name for r in differentiable_outputs]\n        outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n        if not is_inplace_foreach:\n            return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')\n\n    def emit_save_outputs() -> str:\n        if is_out_fn:\n            return ''\n        if info is not None and info.has_derivatives:\n            stmts = save_variables(info.all_saved_outputs, True)\n            if len(stmts) == 0:\n                return ''\n            if not is_inplace_foreach:\n                return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n            else:\n                return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n        return ''\n\n    def emit_any_requires_grad() -> List[str]:\n        extra_condition = ''\n        if info and info.output_differentiability_conditions:\n            assert len(info.output_differentiability_conditions) == 1\n            extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n        names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(names_of_args_with_derivatives):\n                for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                    if arg == r_arg.name:\n                        names_of_args_with_derivatives[i] = f_arg.name\n        return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]\n\n    def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n        if len(var_names) == 1:\n            return f'_any_has_forward_grad_{var_names[0]}'\n        else:\n            return f\"_any_has_forward_grad_{'_'.join(var_names)}\"\n\n    def emit_any_has_forward_grad() -> List[str]:\n        content: List[str] = []\n        if not is_foreach:\n            for derivative in fw_derivatives:\n                requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n                if info and info.output_differentiability_conditions:\n                    assert len(info.output_differentiability_conditions) == 1\n                    requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n                content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n        else:\n            for derivative in fw_derivatives:\n                bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n                cur_derivative_conditions = []\n                for inp in differentiable_inputs:\n                    if derivative.required_inputs_fw_grad is None:\n                        continue\n                    if inp.name not in derivative.required_inputs_fw_grad:\n                        continue\n                    inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                    inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                    is_list_type = is_tensor_list_type(inp_type)\n                    if is_list_type:\n                        if inp_name != 'self':\n                            content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                    else:\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n                content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n                content.append('for (const auto& i : c10::irange(self.size())) {')\n                content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n                content.append('}')\n        return content\n\n    def emit_check_inplace() -> List[str]:\n        if not inplace:\n            return []\n        return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]\n\n    def emit_fw_derivatives() -> List[str]:\n        content: List[str] = []\n        fw_grad_setters: List[str] = []\n        for derivative in fw_derivatives:\n            res = derivative.var_names\n            if f.func.name.name.inplace:\n                assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n                res = ('self',)\n            assert derivative.required_inputs_fw_grad is not None\n            unpacked_arguments = ''\n            for inp in differentiable_inputs:\n                inp_name = inp.name\n                is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n                input_suffix = '[i]' if is_input_tensorlist else ''\n                if is_inplace_foreach:\n                    if inp.name in refargname2inplace_foreacharg:\n                        inp_name = refargname2inplace_foreacharg[inp.name].name\n                zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n                if inp.name in derivative.required_inputs_fw_grad:\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n                if inp.name in (derivative.required_inputs_primal or []):\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n            if derivative.required_original_self_value:\n                input_suffix = 's[i]' if is_inplace_foreach else ''\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n            elif inplace and derivative.is_reusing_outplace_formula:\n                unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n            if inplace:\n                is_inplace_str = 'true'\n            else:\n                is_inplace_str = 'false'\n            requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n            if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n                if len(derivative.var_types) == 1:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    if not is_foreach:\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    else:\n                        assert res[0] == ('result' if not inplace else 'self')\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n                else:\n                    tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                    opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                    for (idx, single_res) in enumerate(res):\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n            elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n                assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n                if not is_foreach:\n                    opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                raise RuntimeError('Unsupported output type for forward derivative')\n            if not is_foreach:\n                fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n                content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n            else:\n                fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n                foreach_forward_grad_formula = derivative.formula\n                _foreach_arg: Union[Argument, DifferentiableInput]\n                if inplace:\n                    for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                        if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                            pattern = _foreach_arg.name\n                            if isinstance(_foreach_arg.type, ListType):\n                                pattern += '[i]'\n                            foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n                elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                    foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n                content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n        content.append('\\n'.join(fw_grad_setters))\n        return content\n\n    def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n        if derivative is None:\n            to_check: List[str] = []\n            for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n                if is_tensor_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                elif is_tensor_list_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                else:\n                    raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n            return f\"({' || '.join(to_check)})\"\n        else:\n            assert derivative.required_inputs_fw_grad is not None\n            if len(derivative.required_inputs_fw_grad) == 0:\n                if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                    raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n                any_has_fw_grad = 'true'\n            else:\n                any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n                any_has_fw_grad = f'({any_has_fw_grad})'\n            return any_has_fw_grad\n\n    def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n        if is_out_fn:\n            msg = 'because it is an out= function'\n        else:\n            msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n        cond = get_any_has_fw_grad_cond(derivative=None)\n        return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''\n    body: List[str] = []\n    (unpack_args_stats, unpacked_bindings) = unpack_args(f)\n    body.extend(unpack_args_stats)\n    if requires_derivative:\n        body.extend(emit_any_requires_grad())\n        body.extend(emit_any_has_forward_grad())\n        body.extend(emit_check_inplace())\n        body.extend(emit_original_self_definition())\n        body.extend(setup_derivative(differentiable_inputs))\n    body.append(declare_returned_variables(f))\n    body.append(emit_call(f, unpacked_bindings, try_jit_decomposition))\n    if requires_derivative:\n        body.append(emit_history())\n        body.extend(emit_check_if_in_complex_autograd_allowlist())\n    if is_out_fn:\n        body.append(emit_forbid_fw_derivatives(is_out_fn=True))\n    elif requires_derivative and (not try_jit_decomposition):\n        if len(fw_derivatives) > 0:\n            body.extend(emit_fw_derivatives())\n        else:\n            body.append(emit_forbid_fw_derivatives())\n    if requires_derivative:\n        body.append(emit_save_outputs())\n    if str(f.func.name.name) in RESET_GRAD_ACCUMULATOR:\n        assert inplace\n        body.append('reset_grad_accumulator(self);')\n    if not returns_void:\n        body.append(f'return {get_return_value(f)};')\n    return body",
            "@with_native_function_with_differentiability_info_and_key\ndef emit_body(fn: NativeFunctionWithDifferentiabilityInfo, key: str='Default') -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert dispatch_strategy(fn) == 'use_derived'\n    f = fn.func\n    info = fn.info[key] if fn.info else None\n    fw_derivatives = fn.fw_derivatives.get(key, []) if fn.fw_derivatives else []\n    name = cpp.name(f.func)\n    inplace = f.func.kind() == SchemaKind.inplace\n    is_out_fn = f.func.kind() == SchemaKind.out\n    returns_void = len(f.func.returns) == 0\n    base_name = get_base_name(f)\n    view_info = get_view_info(f)\n    is_foreach = name.startswith('_foreach')\n    is_inplace_foreach = is_foreach and inplace\n    if is_inplace_foreach:\n        inplace_foreacharg2refarg: Dict[Argument, Argument] = {}\n        refargname2inplace_foreacharg: Dict[str, Argument] = {}\n        base_name_and_overload_name = (f.func.name.name.base, f.func.name.overload_name)\n        if info is None:\n            assert base_name_and_overload_name in _foreach_ops_without_differentiability_info, f\"{'.'.join(base_name_and_overload_name)} should have a differentiability info\"\n        else:\n            assert len(f.func.arguments.flat_non_out) == len(info.func.func.arguments.flat_non_out) or base_name_and_overload_name in _foreach_ops_with_different_arity, f\"{'.'.join(base_name_and_overload_name)} has {len(f.func.arguments.flat_non_out)} args but the reference has {len(info.func.func.arguments.flat_non_out)}\"\n            for (foreach_arg, ref_arg) in zip(f.func.arguments.flat_non_out, info.func.func.arguments.flat_non_out):\n                foreach_arg_type = foreach_arg.type\n                if isinstance(foreach_arg_type, ListType):\n                    foreach_arg_type = foreach_arg_type.elem\n                assert foreach_arg_type == ref_arg.type\n                inplace_foreacharg2refarg[foreach_arg] = ref_arg\n                refargname2inplace_foreacharg[ref_arg.name] = foreach_arg\n\n    def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n        if isinstance(arg, TensorOptionsArguments):\n            return None\n        a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n        cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n        if not is_differentiable(a.name, a.type, info):\n            return None\n        return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)\n\n    @with_native_function\n    def gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n        arguments = list(f.func.arguments.non_out)\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n                if arg in inplace_foreacharg2refarg:\n                    mapped_arg = inplace_foreacharg2refarg[arg]\n                    arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n        return list(mapMaybe(gen_differentiable_input, arguments))\n\n    def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n        \"\"\"Find arguments that have derivative definitions\"\"\"\n        if info is None or not info.has_derivatives:\n            return differentiable_inputs\n        names = {name for d in info.derivatives for name in d.var_names}\n        differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n        if len(differentiable) != len(names):\n            missing = names - {arg.name for arg in differentiable}\n            raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n        return differentiable\n    differentiable_inputs = gen_differentiable_inputs(f)\n    args_with_derivatives = find_args_with_derivatives(differentiable_inputs)\n    differentiable_outputs = gen_differentiable_outputs(fn, key)\n    undifferentiable = base_name in DONT_REQUIRE_DERIVATIVE or name in DONT_REQUIRE_DERIVATIVE\n    requires_derivative = not undifferentiable and len(differentiable_inputs) > 0 and (len(differentiable_outputs) > 0 or is_inplace_foreach)\n    if info is not None and info.has_derivatives and (not requires_derivative) and (len(f.func.returns) > 0):\n        raise RuntimeError(f'ERROR: derivative ignored for {name} -- specified an autograd function without derivative')\n    if requires_derivative and len(fw_derivatives) > 0 and (not is_inplace_foreach):\n        assert sum((len(derivative.var_names) for derivative in fw_derivatives)) == len(differentiable_outputs), 'Expected the number of forward derivatives implemented to match the number of differentiable outputs. NB: This only applies when at least one forward derivative is implemented. Not implementing any forward derivatives is also okay, and we would require inputs to the op to not have associated tangents in that case.'\n    try_jit_decomposition = requires_derivative and len(fw_derivatives) == 0 and (not modifies_arguments(f)) and (not returns_void)\n\n    def emit_save_inputs() -> List[str]:\n        setup: List[str] = []\n        if info is None or not info.has_derivatives:\n            return setup\n        has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n        def guard_for(arg: SavedAttribute) -> Optional[str]:\n            assert info is not None\n            if has_tensorlist_arg and (not is_inplace_foreach):\n                return None\n            if 'backward' in info.name:\n                return None\n            if len(args_with_derivatives) <= 1:\n                return None\n            if arg.nctype.type != BaseCType(tensorT):\n                return None\n            used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n            assert len(used_in) > 0\n            if len(used_in) != 1:\n                return None\n            derivative = used_in[0]\n            if len(derivative.var_names) != 1:\n                wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n                if wrap_opt_if_start == -1:\n                    return None\n                wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n                assert wrap_opt_if_match is not None\n                condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n                wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n                wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n                return f'{wrap_opt_if_condition}'\n            derivative_var_name = derivative.var_names[0]\n            for (edge_off, a) in enumerate(args_with_derivatives):\n                if a.name == derivative_var_name:\n                    break\n            else:\n                raise AssertionError()\n            return f'grad_fn->should_compute_output({edge_off})'\n        if is_inplace_foreach:\n            save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n            if save_input_stmts:\n                setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n        else:\n            setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n            for arg in args_with_derivatives:\n                if is_tensor_list_type(arg.type):\n                    setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n        return setup\n\n    def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n        body: List[str] = []\n        if is_out_fn:\n            body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n            return body\n        op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n        setup = []\n        if not is_inplace_foreach:\n            setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n        else:\n            list_like_arg = 'self'\n            args = [arg.name for arg in args_with_derivatives]\n            for (i, arg) in enumerate(args):\n                if is_inplace_foreach and info is not None:\n                    if arg in refargname2inplace_foreacharg:\n                        foreach_arg = refargname2inplace_foreacharg[arg]\n                        args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n                elif arg == list_like_arg:\n                    args[i] = arg + '[i]'\n            setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n        setup.extend(emit_save_inputs())\n        body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n        declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n        body.append(declare_grad_fn_template.substitute(op=op))\n        body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n        return body\n\n    def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n        body: List[str] = []\n        if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n            return body\n        for arg in differentiable_outputs:\n            name = arg.name\n            if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n                body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n        return body\n\n    def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n        \"\"\"Checks that arguments without derivatives don't require grad\"\"\"\n        body: List[str] = []\n        for arg in tensor_args:\n            if arg in args_with_derivatives:\n                continue\n            arg_name = arg.name\n            if info and arg_name in info.non_differentiable_arg_names:\n                continue\n            if arg_name == 'output':\n                continue\n            body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n        return body\n\n    def emit_original_self_definition() -> List[str]:\n        body: List[str] = []\n        if inplace:\n            if is_inplace_foreach:\n                body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n            else:\n                body.append('c10::optional<at::Tensor> original_self;')\n            all_forward_grad_cond = []\n            for derivative in fw_derivatives:\n                if derivative.required_original_self_value:\n                    all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n            if all_forward_grad_cond:\n                if not is_inplace_foreach:\n                    body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                    body.append('  original_self = self.clone();')\n                    body.append('}')\n                else:\n                    current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                    body.append('for (const auto& i : c10::irange(self.size())) {')\n                    body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                    body.append('    original_selfs[i] = self[i].clone();')\n                    body.append('  }')\n                    body.append('}')\n        return body\n\n    def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n        stmts: List[str] = []\n        for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n            name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n            foreacharg: Optional[Argument] = None\n            is_foreacharg_list_type: bool = False\n            type = arg.nctype.type\n            expr = arg.expr\n            stmts_prepend = None\n            if is_inplace_foreach and info is not None:\n                name_to_query = name.split('_scalar_type')[0]\n                if name_to_query in refargname2inplace_foreacharg:\n                    foreacharg = refargname2inplace_foreacharg[name_to_query]\n                    is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n                if foreacharg is not None:\n                    name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                    src_name = name\n                    if '_scalar_type' in src_name:\n                        split_src_name = src_name.split('_scalar_type')\n                        assert len(split_src_name) == 2\n                        src_name = split_src_name[0]\n                    expr = expr.replace(src_name, name_in_expr)\n            if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n                var = name\n                name += '_'\n                if var == 'self' and inplace:\n                    original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                    self_var = var if not is_inplace_foreach else var + '[i]'\n                    stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                    var = f'{original_self_var}.value()'\n                    assert not is_output\n                if inplace and is_output:\n                    assert name == 'result_'\n                    var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                    is_inplace_view = f'{var}.is_view()'\n                    expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n                else:\n                    expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                    if foreacharg is not None and 'original_selfs' not in expr:\n                        expr = expr.replace(src_name, name_in_expr)\n            elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n                if type == VectorCType(BaseCType(tensorT)):\n                    assert is_foreach and is_output\n                expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n                name += '_'\n            elif type == BaseCType(intArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(symIntArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(stringT):\n                expr = f'std::string({expr})'\n            elif type == OptionalCType(BaseCType(stringT)):\n                expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n            elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n                expr = expr + '.vec()'\n            guard = guard_for(arg)\n            if guard is None:\n                if stmts_prepend:\n                    stmts.append(f'{stmts_prepend};')\n                stmts.append(f'grad_fn->{name} = {expr};')\n            else:\n                stmts.append(f'if ({guard}) {{')\n                if stmts_prepend:\n                    stmts.append(f'  {stmts_prepend};')\n                stmts.append(f'  grad_fn->{name} = {expr};')\n                stmts.append('}')\n        return stmts\n\n    def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n        \"\"\"Dispatch call via function in a namespace or method on Tensor.\"\"\"\n        dispatcher_sig = DispatcherSignature.from_schema(f.func)\n        dispatcher_exprs = dispatcher_sig.exprs()\n        dispatch_key_set = 'ks & c10::after_autograd_keyset'\n        call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n        return call\n\n    def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n        call = ''\n        rhs_value: Optional[str] = None\n        if not any((r.type.is_tensor_like() for r in f.func.returns)):\n            rhs_value = var\n        else:\n            rhs_value = f'std::move({var})'\n        assert rhs_value is not None\n        call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n        return call\n\n    def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n        stmts_before_call: List[str] = []\n        stmts_after_call: List[str] = []\n        if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n            return call\n        for unpacked_binding in unpacked_bindings:\n            arg = unpacked_binding.name\n            noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n                stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == BaseCType(tensorT):\n                stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n        assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n        if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n            base_name = f.func.name.name.base\n            aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n            if aliased_arg_name is not None:\n                aliased_arg_name = unpacked_name(aliased_arg_name)\n            for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n                noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n                if noref_cpp_type == BaseCType(tensorT):\n                    if aliased_arg_name is not None:\n                        assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                        stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                    elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                    if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n                elif noref_cpp_type == BaseCType(tensorListT):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n        if stmts_before_call and stmts_after_call:\n            call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n        return call\n\n    def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n        unpacked_args = [b.name for b in unpacked_bindings]\n        base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n        if get_view_info(f) is not None or modifies_arguments(f):\n            guard = 'at::AutoDispatchBelowAutograd guard;'\n        else:\n            guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n        any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n        return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n        if len(f.func.returns) > 1:\n            return_types = f'std::tuple<{return_types}>'\n        arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n        if not modifies_arguments(f) and (not returns_void):\n            if try_jit_decomposition:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n            else:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n            call += wrap_output(f, unpacked_bindings, TMP_VAR)\n        else:\n            assert not try_jit_decomposition\n            call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n        call = check_tensorimpl_and_storage(call, unpacked_bindings)\n        return call\n\n    def emit_history() -> str:\n        fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n        output_names = [r.name for r in differentiable_outputs]\n        outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n        if not is_inplace_foreach:\n            return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')\n\n    def emit_save_outputs() -> str:\n        if is_out_fn:\n            return ''\n        if info is not None and info.has_derivatives:\n            stmts = save_variables(info.all_saved_outputs, True)\n            if len(stmts) == 0:\n                return ''\n            if not is_inplace_foreach:\n                return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n            else:\n                return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n        return ''\n\n    def emit_any_requires_grad() -> List[str]:\n        extra_condition = ''\n        if info and info.output_differentiability_conditions:\n            assert len(info.output_differentiability_conditions) == 1\n            extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n        names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(names_of_args_with_derivatives):\n                for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                    if arg == r_arg.name:\n                        names_of_args_with_derivatives[i] = f_arg.name\n        return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]\n\n    def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n        if len(var_names) == 1:\n            return f'_any_has_forward_grad_{var_names[0]}'\n        else:\n            return f\"_any_has_forward_grad_{'_'.join(var_names)}\"\n\n    def emit_any_has_forward_grad() -> List[str]:\n        content: List[str] = []\n        if not is_foreach:\n            for derivative in fw_derivatives:\n                requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n                if info and info.output_differentiability_conditions:\n                    assert len(info.output_differentiability_conditions) == 1\n                    requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n                content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n        else:\n            for derivative in fw_derivatives:\n                bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n                cur_derivative_conditions = []\n                for inp in differentiable_inputs:\n                    if derivative.required_inputs_fw_grad is None:\n                        continue\n                    if inp.name not in derivative.required_inputs_fw_grad:\n                        continue\n                    inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                    inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                    is_list_type = is_tensor_list_type(inp_type)\n                    if is_list_type:\n                        if inp_name != 'self':\n                            content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                    else:\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n                content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n                content.append('for (const auto& i : c10::irange(self.size())) {')\n                content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n                content.append('}')\n        return content\n\n    def emit_check_inplace() -> List[str]:\n        if not inplace:\n            return []\n        return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]\n\n    def emit_fw_derivatives() -> List[str]:\n        content: List[str] = []\n        fw_grad_setters: List[str] = []\n        for derivative in fw_derivatives:\n            res = derivative.var_names\n            if f.func.name.name.inplace:\n                assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n                res = ('self',)\n            assert derivative.required_inputs_fw_grad is not None\n            unpacked_arguments = ''\n            for inp in differentiable_inputs:\n                inp_name = inp.name\n                is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n                input_suffix = '[i]' if is_input_tensorlist else ''\n                if is_inplace_foreach:\n                    if inp.name in refargname2inplace_foreacharg:\n                        inp_name = refargname2inplace_foreacharg[inp.name].name\n                zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n                if inp.name in derivative.required_inputs_fw_grad:\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n                if inp.name in (derivative.required_inputs_primal or []):\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n            if derivative.required_original_self_value:\n                input_suffix = 's[i]' if is_inplace_foreach else ''\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n            elif inplace and derivative.is_reusing_outplace_formula:\n                unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n            if inplace:\n                is_inplace_str = 'true'\n            else:\n                is_inplace_str = 'false'\n            requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n            if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n                if len(derivative.var_types) == 1:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    if not is_foreach:\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    else:\n                        assert res[0] == ('result' if not inplace else 'self')\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n                else:\n                    tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                    opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                    for (idx, single_res) in enumerate(res):\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n            elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n                assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n                if not is_foreach:\n                    opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                raise RuntimeError('Unsupported output type for forward derivative')\n            if not is_foreach:\n                fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n                content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n            else:\n                fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n                foreach_forward_grad_formula = derivative.formula\n                _foreach_arg: Union[Argument, DifferentiableInput]\n                if inplace:\n                    for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                        if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                            pattern = _foreach_arg.name\n                            if isinstance(_foreach_arg.type, ListType):\n                                pattern += '[i]'\n                            foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n                elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                    foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n                content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n        content.append('\\n'.join(fw_grad_setters))\n        return content\n\n    def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n        if derivative is None:\n            to_check: List[str] = []\n            for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n                if is_tensor_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                elif is_tensor_list_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                else:\n                    raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n            return f\"({' || '.join(to_check)})\"\n        else:\n            assert derivative.required_inputs_fw_grad is not None\n            if len(derivative.required_inputs_fw_grad) == 0:\n                if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                    raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n                any_has_fw_grad = 'true'\n            else:\n                any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n                any_has_fw_grad = f'({any_has_fw_grad})'\n            return any_has_fw_grad\n\n    def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n        if is_out_fn:\n            msg = 'because it is an out= function'\n        else:\n            msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n        cond = get_any_has_fw_grad_cond(derivative=None)\n        return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''\n    body: List[str] = []\n    (unpack_args_stats, unpacked_bindings) = unpack_args(f)\n    body.extend(unpack_args_stats)\n    if requires_derivative:\n        body.extend(emit_any_requires_grad())\n        body.extend(emit_any_has_forward_grad())\n        body.extend(emit_check_inplace())\n        body.extend(emit_original_self_definition())\n        body.extend(setup_derivative(differentiable_inputs))\n    body.append(declare_returned_variables(f))\n    body.append(emit_call(f, unpacked_bindings, try_jit_decomposition))\n    if requires_derivative:\n        body.append(emit_history())\n        body.extend(emit_check_if_in_complex_autograd_allowlist())\n    if is_out_fn:\n        body.append(emit_forbid_fw_derivatives(is_out_fn=True))\n    elif requires_derivative and (not try_jit_decomposition):\n        if len(fw_derivatives) > 0:\n            body.extend(emit_fw_derivatives())\n        else:\n            body.append(emit_forbid_fw_derivatives())\n    if requires_derivative:\n        body.append(emit_save_outputs())\n    if str(f.func.name.name) in RESET_GRAD_ACCUMULATOR:\n        assert inplace\n        body.append('reset_grad_accumulator(self);')\n    if not returns_void:\n        body.append(f'return {get_return_value(f)};')\n    return body",
            "@with_native_function_with_differentiability_info_and_key\ndef emit_body(fn: NativeFunctionWithDifferentiabilityInfo, key: str='Default') -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert dispatch_strategy(fn) == 'use_derived'\n    f = fn.func\n    info = fn.info[key] if fn.info else None\n    fw_derivatives = fn.fw_derivatives.get(key, []) if fn.fw_derivatives else []\n    name = cpp.name(f.func)\n    inplace = f.func.kind() == SchemaKind.inplace\n    is_out_fn = f.func.kind() == SchemaKind.out\n    returns_void = len(f.func.returns) == 0\n    base_name = get_base_name(f)\n    view_info = get_view_info(f)\n    is_foreach = name.startswith('_foreach')\n    is_inplace_foreach = is_foreach and inplace\n    if is_inplace_foreach:\n        inplace_foreacharg2refarg: Dict[Argument, Argument] = {}\n        refargname2inplace_foreacharg: Dict[str, Argument] = {}\n        base_name_and_overload_name = (f.func.name.name.base, f.func.name.overload_name)\n        if info is None:\n            assert base_name_and_overload_name in _foreach_ops_without_differentiability_info, f\"{'.'.join(base_name_and_overload_name)} should have a differentiability info\"\n        else:\n            assert len(f.func.arguments.flat_non_out) == len(info.func.func.arguments.flat_non_out) or base_name_and_overload_name in _foreach_ops_with_different_arity, f\"{'.'.join(base_name_and_overload_name)} has {len(f.func.arguments.flat_non_out)} args but the reference has {len(info.func.func.arguments.flat_non_out)}\"\n            for (foreach_arg, ref_arg) in zip(f.func.arguments.flat_non_out, info.func.func.arguments.flat_non_out):\n                foreach_arg_type = foreach_arg.type\n                if isinstance(foreach_arg_type, ListType):\n                    foreach_arg_type = foreach_arg_type.elem\n                assert foreach_arg_type == ref_arg.type\n                inplace_foreacharg2refarg[foreach_arg] = ref_arg\n                refargname2inplace_foreacharg[ref_arg.name] = foreach_arg\n\n    def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n        if isinstance(arg, TensorOptionsArguments):\n            return None\n        a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n        cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n        if not is_differentiable(a.name, a.type, info):\n            return None\n        return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)\n\n    @with_native_function\n    def gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n        arguments = list(f.func.arguments.non_out)\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n                if arg in inplace_foreacharg2refarg:\n                    mapped_arg = inplace_foreacharg2refarg[arg]\n                    arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n        return list(mapMaybe(gen_differentiable_input, arguments))\n\n    def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n        \"\"\"Find arguments that have derivative definitions\"\"\"\n        if info is None or not info.has_derivatives:\n            return differentiable_inputs\n        names = {name for d in info.derivatives for name in d.var_names}\n        differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n        if len(differentiable) != len(names):\n            missing = names - {arg.name for arg in differentiable}\n            raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n        return differentiable\n    differentiable_inputs = gen_differentiable_inputs(f)\n    args_with_derivatives = find_args_with_derivatives(differentiable_inputs)\n    differentiable_outputs = gen_differentiable_outputs(fn, key)\n    undifferentiable = base_name in DONT_REQUIRE_DERIVATIVE or name in DONT_REQUIRE_DERIVATIVE\n    requires_derivative = not undifferentiable and len(differentiable_inputs) > 0 and (len(differentiable_outputs) > 0 or is_inplace_foreach)\n    if info is not None and info.has_derivatives and (not requires_derivative) and (len(f.func.returns) > 0):\n        raise RuntimeError(f'ERROR: derivative ignored for {name} -- specified an autograd function without derivative')\n    if requires_derivative and len(fw_derivatives) > 0 and (not is_inplace_foreach):\n        assert sum((len(derivative.var_names) for derivative in fw_derivatives)) == len(differentiable_outputs), 'Expected the number of forward derivatives implemented to match the number of differentiable outputs. NB: This only applies when at least one forward derivative is implemented. Not implementing any forward derivatives is also okay, and we would require inputs to the op to not have associated tangents in that case.'\n    try_jit_decomposition = requires_derivative and len(fw_derivatives) == 0 and (not modifies_arguments(f)) and (not returns_void)\n\n    def emit_save_inputs() -> List[str]:\n        setup: List[str] = []\n        if info is None or not info.has_derivatives:\n            return setup\n        has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n        def guard_for(arg: SavedAttribute) -> Optional[str]:\n            assert info is not None\n            if has_tensorlist_arg and (not is_inplace_foreach):\n                return None\n            if 'backward' in info.name:\n                return None\n            if len(args_with_derivatives) <= 1:\n                return None\n            if arg.nctype.type != BaseCType(tensorT):\n                return None\n            used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n            assert len(used_in) > 0\n            if len(used_in) != 1:\n                return None\n            derivative = used_in[0]\n            if len(derivative.var_names) != 1:\n                wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n                if wrap_opt_if_start == -1:\n                    return None\n                wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n                assert wrap_opt_if_match is not None\n                condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n                wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n                wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n                return f'{wrap_opt_if_condition}'\n            derivative_var_name = derivative.var_names[0]\n            for (edge_off, a) in enumerate(args_with_derivatives):\n                if a.name == derivative_var_name:\n                    break\n            else:\n                raise AssertionError()\n            return f'grad_fn->should_compute_output({edge_off})'\n        if is_inplace_foreach:\n            save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n            if save_input_stmts:\n                setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n        else:\n            setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n            for arg in args_with_derivatives:\n                if is_tensor_list_type(arg.type):\n                    setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n        return setup\n\n    def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n        body: List[str] = []\n        if is_out_fn:\n            body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n            return body\n        op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n        setup = []\n        if not is_inplace_foreach:\n            setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n        else:\n            list_like_arg = 'self'\n            args = [arg.name for arg in args_with_derivatives]\n            for (i, arg) in enumerate(args):\n                if is_inplace_foreach and info is not None:\n                    if arg in refargname2inplace_foreacharg:\n                        foreach_arg = refargname2inplace_foreacharg[arg]\n                        args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n                elif arg == list_like_arg:\n                    args[i] = arg + '[i]'\n            setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n        setup.extend(emit_save_inputs())\n        body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n        declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n        body.append(declare_grad_fn_template.substitute(op=op))\n        body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n        return body\n\n    def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n        body: List[str] = []\n        if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n            return body\n        for arg in differentiable_outputs:\n            name = arg.name\n            if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n                body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n        return body\n\n    def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n        \"\"\"Checks that arguments without derivatives don't require grad\"\"\"\n        body: List[str] = []\n        for arg in tensor_args:\n            if arg in args_with_derivatives:\n                continue\n            arg_name = arg.name\n            if info and arg_name in info.non_differentiable_arg_names:\n                continue\n            if arg_name == 'output':\n                continue\n            body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n        return body\n\n    def emit_original_self_definition() -> List[str]:\n        body: List[str] = []\n        if inplace:\n            if is_inplace_foreach:\n                body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n            else:\n                body.append('c10::optional<at::Tensor> original_self;')\n            all_forward_grad_cond = []\n            for derivative in fw_derivatives:\n                if derivative.required_original_self_value:\n                    all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n            if all_forward_grad_cond:\n                if not is_inplace_foreach:\n                    body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                    body.append('  original_self = self.clone();')\n                    body.append('}')\n                else:\n                    current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                    body.append('for (const auto& i : c10::irange(self.size())) {')\n                    body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                    body.append('    original_selfs[i] = self[i].clone();')\n                    body.append('  }')\n                    body.append('}')\n        return body\n\n    def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n        stmts: List[str] = []\n        for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n            name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n            foreacharg: Optional[Argument] = None\n            is_foreacharg_list_type: bool = False\n            type = arg.nctype.type\n            expr = arg.expr\n            stmts_prepend = None\n            if is_inplace_foreach and info is not None:\n                name_to_query = name.split('_scalar_type')[0]\n                if name_to_query in refargname2inplace_foreacharg:\n                    foreacharg = refargname2inplace_foreacharg[name_to_query]\n                    is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n                if foreacharg is not None:\n                    name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                    src_name = name\n                    if '_scalar_type' in src_name:\n                        split_src_name = src_name.split('_scalar_type')\n                        assert len(split_src_name) == 2\n                        src_name = split_src_name[0]\n                    expr = expr.replace(src_name, name_in_expr)\n            if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n                var = name\n                name += '_'\n                if var == 'self' and inplace:\n                    original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                    self_var = var if not is_inplace_foreach else var + '[i]'\n                    stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                    var = f'{original_self_var}.value()'\n                    assert not is_output\n                if inplace and is_output:\n                    assert name == 'result_'\n                    var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                    is_inplace_view = f'{var}.is_view()'\n                    expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n                else:\n                    expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                    if foreacharg is not None and 'original_selfs' not in expr:\n                        expr = expr.replace(src_name, name_in_expr)\n            elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n                if type == VectorCType(BaseCType(tensorT)):\n                    assert is_foreach and is_output\n                expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n                name += '_'\n            elif type == BaseCType(intArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(symIntArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(stringT):\n                expr = f'std::string({expr})'\n            elif type == OptionalCType(BaseCType(stringT)):\n                expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n            elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n                expr = expr + '.vec()'\n            guard = guard_for(arg)\n            if guard is None:\n                if stmts_prepend:\n                    stmts.append(f'{stmts_prepend};')\n                stmts.append(f'grad_fn->{name} = {expr};')\n            else:\n                stmts.append(f'if ({guard}) {{')\n                if stmts_prepend:\n                    stmts.append(f'  {stmts_prepend};')\n                stmts.append(f'  grad_fn->{name} = {expr};')\n                stmts.append('}')\n        return stmts\n\n    def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n        \"\"\"Dispatch call via function in a namespace or method on Tensor.\"\"\"\n        dispatcher_sig = DispatcherSignature.from_schema(f.func)\n        dispatcher_exprs = dispatcher_sig.exprs()\n        dispatch_key_set = 'ks & c10::after_autograd_keyset'\n        call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n        return call\n\n    def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n        call = ''\n        rhs_value: Optional[str] = None\n        if not any((r.type.is_tensor_like() for r in f.func.returns)):\n            rhs_value = var\n        else:\n            rhs_value = f'std::move({var})'\n        assert rhs_value is not None\n        call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n        return call\n\n    def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n        stmts_before_call: List[str] = []\n        stmts_after_call: List[str] = []\n        if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n            return call\n        for unpacked_binding in unpacked_bindings:\n            arg = unpacked_binding.name\n            noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n                stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == BaseCType(tensorT):\n                stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n        assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n        if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n            base_name = f.func.name.name.base\n            aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n            if aliased_arg_name is not None:\n                aliased_arg_name = unpacked_name(aliased_arg_name)\n            for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n                noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n                if noref_cpp_type == BaseCType(tensorT):\n                    if aliased_arg_name is not None:\n                        assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                        stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                    elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                    if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n                elif noref_cpp_type == BaseCType(tensorListT):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n        if stmts_before_call and stmts_after_call:\n            call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n        return call\n\n    def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n        unpacked_args = [b.name for b in unpacked_bindings]\n        base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n        if get_view_info(f) is not None or modifies_arguments(f):\n            guard = 'at::AutoDispatchBelowAutograd guard;'\n        else:\n            guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n        any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n        return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n        if len(f.func.returns) > 1:\n            return_types = f'std::tuple<{return_types}>'\n        arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n        if not modifies_arguments(f) and (not returns_void):\n            if try_jit_decomposition:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n            else:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n            call += wrap_output(f, unpacked_bindings, TMP_VAR)\n        else:\n            assert not try_jit_decomposition\n            call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n        call = check_tensorimpl_and_storage(call, unpacked_bindings)\n        return call\n\n    def emit_history() -> str:\n        fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n        output_names = [r.name for r in differentiable_outputs]\n        outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n        if not is_inplace_foreach:\n            return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')\n\n    def emit_save_outputs() -> str:\n        if is_out_fn:\n            return ''\n        if info is not None and info.has_derivatives:\n            stmts = save_variables(info.all_saved_outputs, True)\n            if len(stmts) == 0:\n                return ''\n            if not is_inplace_foreach:\n                return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n            else:\n                return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n        return ''\n\n    def emit_any_requires_grad() -> List[str]:\n        extra_condition = ''\n        if info and info.output_differentiability_conditions:\n            assert len(info.output_differentiability_conditions) == 1\n            extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n        names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(names_of_args_with_derivatives):\n                for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                    if arg == r_arg.name:\n                        names_of_args_with_derivatives[i] = f_arg.name\n        return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]\n\n    def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n        if len(var_names) == 1:\n            return f'_any_has_forward_grad_{var_names[0]}'\n        else:\n            return f\"_any_has_forward_grad_{'_'.join(var_names)}\"\n\n    def emit_any_has_forward_grad() -> List[str]:\n        content: List[str] = []\n        if not is_foreach:\n            for derivative in fw_derivatives:\n                requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n                if info and info.output_differentiability_conditions:\n                    assert len(info.output_differentiability_conditions) == 1\n                    requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n                content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n        else:\n            for derivative in fw_derivatives:\n                bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n                cur_derivative_conditions = []\n                for inp in differentiable_inputs:\n                    if derivative.required_inputs_fw_grad is None:\n                        continue\n                    if inp.name not in derivative.required_inputs_fw_grad:\n                        continue\n                    inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                    inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                    is_list_type = is_tensor_list_type(inp_type)\n                    if is_list_type:\n                        if inp_name != 'self':\n                            content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                    else:\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n                content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n                content.append('for (const auto& i : c10::irange(self.size())) {')\n                content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n                content.append('}')\n        return content\n\n    def emit_check_inplace() -> List[str]:\n        if not inplace:\n            return []\n        return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]\n\n    def emit_fw_derivatives() -> List[str]:\n        content: List[str] = []\n        fw_grad_setters: List[str] = []\n        for derivative in fw_derivatives:\n            res = derivative.var_names\n            if f.func.name.name.inplace:\n                assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n                res = ('self',)\n            assert derivative.required_inputs_fw_grad is not None\n            unpacked_arguments = ''\n            for inp in differentiable_inputs:\n                inp_name = inp.name\n                is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n                input_suffix = '[i]' if is_input_tensorlist else ''\n                if is_inplace_foreach:\n                    if inp.name in refargname2inplace_foreacharg:\n                        inp_name = refargname2inplace_foreacharg[inp.name].name\n                zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n                if inp.name in derivative.required_inputs_fw_grad:\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n                if inp.name in (derivative.required_inputs_primal or []):\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n            if derivative.required_original_self_value:\n                input_suffix = 's[i]' if is_inplace_foreach else ''\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n            elif inplace and derivative.is_reusing_outplace_formula:\n                unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n            if inplace:\n                is_inplace_str = 'true'\n            else:\n                is_inplace_str = 'false'\n            requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n            if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n                if len(derivative.var_types) == 1:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    if not is_foreach:\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    else:\n                        assert res[0] == ('result' if not inplace else 'self')\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n                else:\n                    tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                    opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                    for (idx, single_res) in enumerate(res):\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n            elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n                assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n                if not is_foreach:\n                    opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                raise RuntimeError('Unsupported output type for forward derivative')\n            if not is_foreach:\n                fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n                content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n            else:\n                fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n                foreach_forward_grad_formula = derivative.formula\n                _foreach_arg: Union[Argument, DifferentiableInput]\n                if inplace:\n                    for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                        if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                            pattern = _foreach_arg.name\n                            if isinstance(_foreach_arg.type, ListType):\n                                pattern += '[i]'\n                            foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n                elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                    foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n                content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n        content.append('\\n'.join(fw_grad_setters))\n        return content\n\n    def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n        if derivative is None:\n            to_check: List[str] = []\n            for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n                if is_tensor_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                elif is_tensor_list_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                else:\n                    raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n            return f\"({' || '.join(to_check)})\"\n        else:\n            assert derivative.required_inputs_fw_grad is not None\n            if len(derivative.required_inputs_fw_grad) == 0:\n                if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                    raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n                any_has_fw_grad = 'true'\n            else:\n                any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n                any_has_fw_grad = f'({any_has_fw_grad})'\n            return any_has_fw_grad\n\n    def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n        if is_out_fn:\n            msg = 'because it is an out= function'\n        else:\n            msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n        cond = get_any_has_fw_grad_cond(derivative=None)\n        return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''\n    body: List[str] = []\n    (unpack_args_stats, unpacked_bindings) = unpack_args(f)\n    body.extend(unpack_args_stats)\n    if requires_derivative:\n        body.extend(emit_any_requires_grad())\n        body.extend(emit_any_has_forward_grad())\n        body.extend(emit_check_inplace())\n        body.extend(emit_original_self_definition())\n        body.extend(setup_derivative(differentiable_inputs))\n    body.append(declare_returned_variables(f))\n    body.append(emit_call(f, unpacked_bindings, try_jit_decomposition))\n    if requires_derivative:\n        body.append(emit_history())\n        body.extend(emit_check_if_in_complex_autograd_allowlist())\n    if is_out_fn:\n        body.append(emit_forbid_fw_derivatives(is_out_fn=True))\n    elif requires_derivative and (not try_jit_decomposition):\n        if len(fw_derivatives) > 0:\n            body.extend(emit_fw_derivatives())\n        else:\n            body.append(emit_forbid_fw_derivatives())\n    if requires_derivative:\n        body.append(emit_save_outputs())\n    if str(f.func.name.name) in RESET_GRAD_ACCUMULATOR:\n        assert inplace\n        body.append('reset_grad_accumulator(self);')\n    if not returns_void:\n        body.append(f'return {get_return_value(f)};')\n    return body",
            "@with_native_function_with_differentiability_info_and_key\ndef emit_body(fn: NativeFunctionWithDifferentiabilityInfo, key: str='Default') -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert dispatch_strategy(fn) == 'use_derived'\n    f = fn.func\n    info = fn.info[key] if fn.info else None\n    fw_derivatives = fn.fw_derivatives.get(key, []) if fn.fw_derivatives else []\n    name = cpp.name(f.func)\n    inplace = f.func.kind() == SchemaKind.inplace\n    is_out_fn = f.func.kind() == SchemaKind.out\n    returns_void = len(f.func.returns) == 0\n    base_name = get_base_name(f)\n    view_info = get_view_info(f)\n    is_foreach = name.startswith('_foreach')\n    is_inplace_foreach = is_foreach and inplace\n    if is_inplace_foreach:\n        inplace_foreacharg2refarg: Dict[Argument, Argument] = {}\n        refargname2inplace_foreacharg: Dict[str, Argument] = {}\n        base_name_and_overload_name = (f.func.name.name.base, f.func.name.overload_name)\n        if info is None:\n            assert base_name_and_overload_name in _foreach_ops_without_differentiability_info, f\"{'.'.join(base_name_and_overload_name)} should have a differentiability info\"\n        else:\n            assert len(f.func.arguments.flat_non_out) == len(info.func.func.arguments.flat_non_out) or base_name_and_overload_name in _foreach_ops_with_different_arity, f\"{'.'.join(base_name_and_overload_name)} has {len(f.func.arguments.flat_non_out)} args but the reference has {len(info.func.func.arguments.flat_non_out)}\"\n            for (foreach_arg, ref_arg) in zip(f.func.arguments.flat_non_out, info.func.func.arguments.flat_non_out):\n                foreach_arg_type = foreach_arg.type\n                if isinstance(foreach_arg_type, ListType):\n                    foreach_arg_type = foreach_arg_type.elem\n                assert foreach_arg_type == ref_arg.type\n                inplace_foreacharg2refarg[foreach_arg] = ref_arg\n                refargname2inplace_foreacharg[ref_arg.name] = foreach_arg\n\n    def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n        if isinstance(arg, TensorOptionsArguments):\n            return None\n        a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n        cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n        if not is_differentiable(a.name, a.type, info):\n            return None\n        return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)\n\n    @with_native_function\n    def gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n        arguments = list(f.func.arguments.non_out)\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n                if arg in inplace_foreacharg2refarg:\n                    mapped_arg = inplace_foreacharg2refarg[arg]\n                    arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n        return list(mapMaybe(gen_differentiable_input, arguments))\n\n    def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n        \"\"\"Find arguments that have derivative definitions\"\"\"\n        if info is None or not info.has_derivatives:\n            return differentiable_inputs\n        names = {name for d in info.derivatives for name in d.var_names}\n        differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n        if len(differentiable) != len(names):\n            missing = names - {arg.name for arg in differentiable}\n            raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n        return differentiable\n    differentiable_inputs = gen_differentiable_inputs(f)\n    args_with_derivatives = find_args_with_derivatives(differentiable_inputs)\n    differentiable_outputs = gen_differentiable_outputs(fn, key)\n    undifferentiable = base_name in DONT_REQUIRE_DERIVATIVE or name in DONT_REQUIRE_DERIVATIVE\n    requires_derivative = not undifferentiable and len(differentiable_inputs) > 0 and (len(differentiable_outputs) > 0 or is_inplace_foreach)\n    if info is not None and info.has_derivatives and (not requires_derivative) and (len(f.func.returns) > 0):\n        raise RuntimeError(f'ERROR: derivative ignored for {name} -- specified an autograd function without derivative')\n    if requires_derivative and len(fw_derivatives) > 0 and (not is_inplace_foreach):\n        assert sum((len(derivative.var_names) for derivative in fw_derivatives)) == len(differentiable_outputs), 'Expected the number of forward derivatives implemented to match the number of differentiable outputs. NB: This only applies when at least one forward derivative is implemented. Not implementing any forward derivatives is also okay, and we would require inputs to the op to not have associated tangents in that case.'\n    try_jit_decomposition = requires_derivative and len(fw_derivatives) == 0 and (not modifies_arguments(f)) and (not returns_void)\n\n    def emit_save_inputs() -> List[str]:\n        setup: List[str] = []\n        if info is None or not info.has_derivatives:\n            return setup\n        has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n        def guard_for(arg: SavedAttribute) -> Optional[str]:\n            assert info is not None\n            if has_tensorlist_arg and (not is_inplace_foreach):\n                return None\n            if 'backward' in info.name:\n                return None\n            if len(args_with_derivatives) <= 1:\n                return None\n            if arg.nctype.type != BaseCType(tensorT):\n                return None\n            used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n            assert len(used_in) > 0\n            if len(used_in) != 1:\n                return None\n            derivative = used_in[0]\n            if len(derivative.var_names) != 1:\n                wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n                if wrap_opt_if_start == -1:\n                    return None\n                wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n                assert wrap_opt_if_match is not None\n                condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n                wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n                wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n                return f'{wrap_opt_if_condition}'\n            derivative_var_name = derivative.var_names[0]\n            for (edge_off, a) in enumerate(args_with_derivatives):\n                if a.name == derivative_var_name:\n                    break\n            else:\n                raise AssertionError()\n            return f'grad_fn->should_compute_output({edge_off})'\n        if is_inplace_foreach:\n            save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n            if save_input_stmts:\n                setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n        else:\n            setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n            for arg in args_with_derivatives:\n                if is_tensor_list_type(arg.type):\n                    setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n        return setup\n\n    def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n        body: List[str] = []\n        if is_out_fn:\n            body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n            return body\n        op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n        setup = []\n        if not is_inplace_foreach:\n            setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n        else:\n            list_like_arg = 'self'\n            args = [arg.name for arg in args_with_derivatives]\n            for (i, arg) in enumerate(args):\n                if is_inplace_foreach and info is not None:\n                    if arg in refargname2inplace_foreacharg:\n                        foreach_arg = refargname2inplace_foreacharg[arg]\n                        args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n                elif arg == list_like_arg:\n                    args[i] = arg + '[i]'\n            setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n        setup.extend(emit_save_inputs())\n        body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n        declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n        body.append(declare_grad_fn_template.substitute(op=op))\n        body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n        return body\n\n    def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n        body: List[str] = []\n        if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n            return body\n        for arg in differentiable_outputs:\n            name = arg.name\n            if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n                body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n        return body\n\n    def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n        \"\"\"Checks that arguments without derivatives don't require grad\"\"\"\n        body: List[str] = []\n        for arg in tensor_args:\n            if arg in args_with_derivatives:\n                continue\n            arg_name = arg.name\n            if info and arg_name in info.non_differentiable_arg_names:\n                continue\n            if arg_name == 'output':\n                continue\n            body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n        return body\n\n    def emit_original_self_definition() -> List[str]:\n        body: List[str] = []\n        if inplace:\n            if is_inplace_foreach:\n                body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n            else:\n                body.append('c10::optional<at::Tensor> original_self;')\n            all_forward_grad_cond = []\n            for derivative in fw_derivatives:\n                if derivative.required_original_self_value:\n                    all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n            if all_forward_grad_cond:\n                if not is_inplace_foreach:\n                    body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                    body.append('  original_self = self.clone();')\n                    body.append('}')\n                else:\n                    current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                    body.append('for (const auto& i : c10::irange(self.size())) {')\n                    body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                    body.append('    original_selfs[i] = self[i].clone();')\n                    body.append('  }')\n                    body.append('}')\n        return body\n\n    def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n        stmts: List[str] = []\n        for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n            name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n            foreacharg: Optional[Argument] = None\n            is_foreacharg_list_type: bool = False\n            type = arg.nctype.type\n            expr = arg.expr\n            stmts_prepend = None\n            if is_inplace_foreach and info is not None:\n                name_to_query = name.split('_scalar_type')[0]\n                if name_to_query in refargname2inplace_foreacharg:\n                    foreacharg = refargname2inplace_foreacharg[name_to_query]\n                    is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n                if foreacharg is not None:\n                    name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                    src_name = name\n                    if '_scalar_type' in src_name:\n                        split_src_name = src_name.split('_scalar_type')\n                        assert len(split_src_name) == 2\n                        src_name = split_src_name[0]\n                    expr = expr.replace(src_name, name_in_expr)\n            if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n                var = name\n                name += '_'\n                if var == 'self' and inplace:\n                    original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                    self_var = var if not is_inplace_foreach else var + '[i]'\n                    stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                    var = f'{original_self_var}.value()'\n                    assert not is_output\n                if inplace and is_output:\n                    assert name == 'result_'\n                    var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                    is_inplace_view = f'{var}.is_view()'\n                    expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n                else:\n                    expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                    if foreacharg is not None and 'original_selfs' not in expr:\n                        expr = expr.replace(src_name, name_in_expr)\n            elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n                if type == VectorCType(BaseCType(tensorT)):\n                    assert is_foreach and is_output\n                expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n                name += '_'\n            elif type == BaseCType(intArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(symIntArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(stringT):\n                expr = f'std::string({expr})'\n            elif type == OptionalCType(BaseCType(stringT)):\n                expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n            elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n                expr = expr + '.vec()'\n            guard = guard_for(arg)\n            if guard is None:\n                if stmts_prepend:\n                    stmts.append(f'{stmts_prepend};')\n                stmts.append(f'grad_fn->{name} = {expr};')\n            else:\n                stmts.append(f'if ({guard}) {{')\n                if stmts_prepend:\n                    stmts.append(f'  {stmts_prepend};')\n                stmts.append(f'  grad_fn->{name} = {expr};')\n                stmts.append('}')\n        return stmts\n\n    def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n        \"\"\"Dispatch call via function in a namespace or method on Tensor.\"\"\"\n        dispatcher_sig = DispatcherSignature.from_schema(f.func)\n        dispatcher_exprs = dispatcher_sig.exprs()\n        dispatch_key_set = 'ks & c10::after_autograd_keyset'\n        call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n        return call\n\n    def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n        call = ''\n        rhs_value: Optional[str] = None\n        if not any((r.type.is_tensor_like() for r in f.func.returns)):\n            rhs_value = var\n        else:\n            rhs_value = f'std::move({var})'\n        assert rhs_value is not None\n        call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n        return call\n\n    def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n        stmts_before_call: List[str] = []\n        stmts_after_call: List[str] = []\n        if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n            return call\n        for unpacked_binding in unpacked_bindings:\n            arg = unpacked_binding.name\n            noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n                stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == BaseCType(tensorT):\n                stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n        assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n        if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n            base_name = f.func.name.name.base\n            aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n            if aliased_arg_name is not None:\n                aliased_arg_name = unpacked_name(aliased_arg_name)\n            for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n                noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n                if noref_cpp_type == BaseCType(tensorT):\n                    if aliased_arg_name is not None:\n                        assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                        stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                    elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                    if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n                elif noref_cpp_type == BaseCType(tensorListT):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n        if stmts_before_call and stmts_after_call:\n            call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n        return call\n\n    def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n        unpacked_args = [b.name for b in unpacked_bindings]\n        base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n        if get_view_info(f) is not None or modifies_arguments(f):\n            guard = 'at::AutoDispatchBelowAutograd guard;'\n        else:\n            guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n        any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n        return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n        if len(f.func.returns) > 1:\n            return_types = f'std::tuple<{return_types}>'\n        arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n        if not modifies_arguments(f) and (not returns_void):\n            if try_jit_decomposition:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n            else:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n            call += wrap_output(f, unpacked_bindings, TMP_VAR)\n        else:\n            assert not try_jit_decomposition\n            call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n        call = check_tensorimpl_and_storage(call, unpacked_bindings)\n        return call\n\n    def emit_history() -> str:\n        fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n        output_names = [r.name for r in differentiable_outputs]\n        outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n        if not is_inplace_foreach:\n            return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')\n\n    def emit_save_outputs() -> str:\n        if is_out_fn:\n            return ''\n        if info is not None and info.has_derivatives:\n            stmts = save_variables(info.all_saved_outputs, True)\n            if len(stmts) == 0:\n                return ''\n            if not is_inplace_foreach:\n                return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n            else:\n                return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n        return ''\n\n    def emit_any_requires_grad() -> List[str]:\n        extra_condition = ''\n        if info and info.output_differentiability_conditions:\n            assert len(info.output_differentiability_conditions) == 1\n            extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n        names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(names_of_args_with_derivatives):\n                for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                    if arg == r_arg.name:\n                        names_of_args_with_derivatives[i] = f_arg.name\n        return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]\n\n    def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n        if len(var_names) == 1:\n            return f'_any_has_forward_grad_{var_names[0]}'\n        else:\n            return f\"_any_has_forward_grad_{'_'.join(var_names)}\"\n\n    def emit_any_has_forward_grad() -> List[str]:\n        content: List[str] = []\n        if not is_foreach:\n            for derivative in fw_derivatives:\n                requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n                if info and info.output_differentiability_conditions:\n                    assert len(info.output_differentiability_conditions) == 1\n                    requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n                content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n        else:\n            for derivative in fw_derivatives:\n                bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n                cur_derivative_conditions = []\n                for inp in differentiable_inputs:\n                    if derivative.required_inputs_fw_grad is None:\n                        continue\n                    if inp.name not in derivative.required_inputs_fw_grad:\n                        continue\n                    inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                    inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                    is_list_type = is_tensor_list_type(inp_type)\n                    if is_list_type:\n                        if inp_name != 'self':\n                            content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                    else:\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n                content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n                content.append('for (const auto& i : c10::irange(self.size())) {')\n                content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n                content.append('}')\n        return content\n\n    def emit_check_inplace() -> List[str]:\n        if not inplace:\n            return []\n        return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]\n\n    def emit_fw_derivatives() -> List[str]:\n        content: List[str] = []\n        fw_grad_setters: List[str] = []\n        for derivative in fw_derivatives:\n            res = derivative.var_names\n            if f.func.name.name.inplace:\n                assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n                res = ('self',)\n            assert derivative.required_inputs_fw_grad is not None\n            unpacked_arguments = ''\n            for inp in differentiable_inputs:\n                inp_name = inp.name\n                is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n                input_suffix = '[i]' if is_input_tensorlist else ''\n                if is_inplace_foreach:\n                    if inp.name in refargname2inplace_foreacharg:\n                        inp_name = refargname2inplace_foreacharg[inp.name].name\n                zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n                if inp.name in derivative.required_inputs_fw_grad:\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n                if inp.name in (derivative.required_inputs_primal or []):\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n            if derivative.required_original_self_value:\n                input_suffix = 's[i]' if is_inplace_foreach else ''\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n            elif inplace and derivative.is_reusing_outplace_formula:\n                unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n            if inplace:\n                is_inplace_str = 'true'\n            else:\n                is_inplace_str = 'false'\n            requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n            if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n                if len(derivative.var_types) == 1:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    if not is_foreach:\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    else:\n                        assert res[0] == ('result' if not inplace else 'self')\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n                else:\n                    tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                    opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                    for (idx, single_res) in enumerate(res):\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n            elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n                assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n                if not is_foreach:\n                    opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                raise RuntimeError('Unsupported output type for forward derivative')\n            if not is_foreach:\n                fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n                content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n            else:\n                fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n                foreach_forward_grad_formula = derivative.formula\n                _foreach_arg: Union[Argument, DifferentiableInput]\n                if inplace:\n                    for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                        if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                            pattern = _foreach_arg.name\n                            if isinstance(_foreach_arg.type, ListType):\n                                pattern += '[i]'\n                            foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n                elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                    foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n                content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n        content.append('\\n'.join(fw_grad_setters))\n        return content\n\n    def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n        if derivative is None:\n            to_check: List[str] = []\n            for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n                if is_tensor_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                elif is_tensor_list_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                else:\n                    raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n            return f\"({' || '.join(to_check)})\"\n        else:\n            assert derivative.required_inputs_fw_grad is not None\n            if len(derivative.required_inputs_fw_grad) == 0:\n                if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                    raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n                any_has_fw_grad = 'true'\n            else:\n                any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n                any_has_fw_grad = f'({any_has_fw_grad})'\n            return any_has_fw_grad\n\n    def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n        if is_out_fn:\n            msg = 'because it is an out= function'\n        else:\n            msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n        cond = get_any_has_fw_grad_cond(derivative=None)\n        return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''\n    body: List[str] = []\n    (unpack_args_stats, unpacked_bindings) = unpack_args(f)\n    body.extend(unpack_args_stats)\n    if requires_derivative:\n        body.extend(emit_any_requires_grad())\n        body.extend(emit_any_has_forward_grad())\n        body.extend(emit_check_inplace())\n        body.extend(emit_original_self_definition())\n        body.extend(setup_derivative(differentiable_inputs))\n    body.append(declare_returned_variables(f))\n    body.append(emit_call(f, unpacked_bindings, try_jit_decomposition))\n    if requires_derivative:\n        body.append(emit_history())\n        body.extend(emit_check_if_in_complex_autograd_allowlist())\n    if is_out_fn:\n        body.append(emit_forbid_fw_derivatives(is_out_fn=True))\n    elif requires_derivative and (not try_jit_decomposition):\n        if len(fw_derivatives) > 0:\n            body.extend(emit_fw_derivatives())\n        else:\n            body.append(emit_forbid_fw_derivatives())\n    if requires_derivative:\n        body.append(emit_save_outputs())\n    if str(f.func.name.name) in RESET_GRAD_ACCUMULATOR:\n        assert inplace\n        body.append('reset_grad_accumulator(self);')\n    if not returns_void:\n        body.append(f'return {get_return_value(f)};')\n    return body",
            "@with_native_function_with_differentiability_info_and_key\ndef emit_body(fn: NativeFunctionWithDifferentiabilityInfo, key: str='Default') -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert dispatch_strategy(fn) == 'use_derived'\n    f = fn.func\n    info = fn.info[key] if fn.info else None\n    fw_derivatives = fn.fw_derivatives.get(key, []) if fn.fw_derivatives else []\n    name = cpp.name(f.func)\n    inplace = f.func.kind() == SchemaKind.inplace\n    is_out_fn = f.func.kind() == SchemaKind.out\n    returns_void = len(f.func.returns) == 0\n    base_name = get_base_name(f)\n    view_info = get_view_info(f)\n    is_foreach = name.startswith('_foreach')\n    is_inplace_foreach = is_foreach and inplace\n    if is_inplace_foreach:\n        inplace_foreacharg2refarg: Dict[Argument, Argument] = {}\n        refargname2inplace_foreacharg: Dict[str, Argument] = {}\n        base_name_and_overload_name = (f.func.name.name.base, f.func.name.overload_name)\n        if info is None:\n            assert base_name_and_overload_name in _foreach_ops_without_differentiability_info, f\"{'.'.join(base_name_and_overload_name)} should have a differentiability info\"\n        else:\n            assert len(f.func.arguments.flat_non_out) == len(info.func.func.arguments.flat_non_out) or base_name_and_overload_name in _foreach_ops_with_different_arity, f\"{'.'.join(base_name_and_overload_name)} has {len(f.func.arguments.flat_non_out)} args but the reference has {len(info.func.func.arguments.flat_non_out)}\"\n            for (foreach_arg, ref_arg) in zip(f.func.arguments.flat_non_out, info.func.func.arguments.flat_non_out):\n                foreach_arg_type = foreach_arg.type\n                if isinstance(foreach_arg_type, ListType):\n                    foreach_arg_type = foreach_arg_type.elem\n                assert foreach_arg_type == ref_arg.type\n                inplace_foreacharg2refarg[foreach_arg] = ref_arg\n                refargname2inplace_foreacharg[ref_arg.name] = foreach_arg\n\n    def gen_differentiable_input(arg: Union[Argument, SelfArgument, TensorOptionsArguments]) -> Optional[DifferentiableInput]:\n        if isinstance(arg, TensorOptionsArguments):\n            return None\n        a: Argument = arg.argument if isinstance(arg, SelfArgument) else arg\n        cpp_type = cpp.argument_type(a, binds=a.name, symint=True).cpp_type()\n        if not is_differentiable(a.name, a.type, info):\n            return None\n        return DifferentiableInput(name=a.name, type=a.type, cpp_type=cpp_type)\n\n    @with_native_function\n    def gen_differentiable_inputs(f: NativeFunction) -> List[DifferentiableInput]:\n        arguments = list(f.func.arguments.non_out)\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(f.func.arguments.flat_non_out):\n                if arg in inplace_foreacharg2refarg:\n                    mapped_arg = inplace_foreacharg2refarg[arg]\n                    arguments[i] = Argument(mapped_arg.name, mapped_arg.type, mapped_arg.default, mapped_arg.annotation)\n        return list(mapMaybe(gen_differentiable_input, arguments))\n\n    def find_args_with_derivatives(differentiable_inputs: List[DifferentiableInput]) -> List[DifferentiableInput]:\n        \"\"\"Find arguments that have derivative definitions\"\"\"\n        if info is None or not info.has_derivatives:\n            return differentiable_inputs\n        names = {name for d in info.derivatives for name in d.var_names}\n        differentiable = [arg for arg in differentiable_inputs if arg.name in names]\n        if len(differentiable) != len(names):\n            missing = names - {arg.name for arg in differentiable}\n            raise RuntimeError(f'Missing arguments for derivatives: {missing} in {info.name}')\n        return differentiable\n    differentiable_inputs = gen_differentiable_inputs(f)\n    args_with_derivatives = find_args_with_derivatives(differentiable_inputs)\n    differentiable_outputs = gen_differentiable_outputs(fn, key)\n    undifferentiable = base_name in DONT_REQUIRE_DERIVATIVE or name in DONT_REQUIRE_DERIVATIVE\n    requires_derivative = not undifferentiable and len(differentiable_inputs) > 0 and (len(differentiable_outputs) > 0 or is_inplace_foreach)\n    if info is not None and info.has_derivatives and (not requires_derivative) and (len(f.func.returns) > 0):\n        raise RuntimeError(f'ERROR: derivative ignored for {name} -- specified an autograd function without derivative')\n    if requires_derivative and len(fw_derivatives) > 0 and (not is_inplace_foreach):\n        assert sum((len(derivative.var_names) for derivative in fw_derivatives)) == len(differentiable_outputs), 'Expected the number of forward derivatives implemented to match the number of differentiable outputs. NB: This only applies when at least one forward derivative is implemented. Not implementing any forward derivatives is also okay, and we would require inputs to the op to not have associated tangents in that case.'\n    try_jit_decomposition = requires_derivative and len(fw_derivatives) == 0 and (not modifies_arguments(f)) and (not returns_void)\n\n    def emit_save_inputs() -> List[str]:\n        setup: List[str] = []\n        if info is None or not info.has_derivatives:\n            return setup\n        has_tensorlist_arg = any((is_tensor_list_type(arg.type) for arg in args_with_derivatives))\n\n        def guard_for(arg: SavedAttribute) -> Optional[str]:\n            assert info is not None\n            if has_tensorlist_arg and (not is_inplace_foreach):\n                return None\n            if 'backward' in info.name:\n                return None\n            if len(args_with_derivatives) <= 1:\n                return None\n            if arg.nctype.type != BaseCType(tensorT):\n                return None\n            used_in = [d for d in info.derivatives if arg in d.saved_inputs]\n            assert len(used_in) > 0\n            if len(used_in) != 1:\n                return None\n            derivative = used_in[0]\n            if len(derivative.var_names) != 1:\n                wrap_opt_if_start = derivative.formula.find(f'wrap_opt_if({arg.nctype.name}')\n                if wrap_opt_if_start == -1:\n                    return None\n                wrap_opt_if_match = re.match(f'wrap_opt_if\\\\({arg.nctype.name},(.*?)\\\\)', derivative.formula[wrap_opt_if_start:])\n                assert wrap_opt_if_match is not None\n                condition_slice = slice(len(f'wrap_opt_if\\\\({arg.nctype.name},'), -1)\n                wrap_opt_if_condition = wrap_opt_if_match.group(0)[condition_slice].strip()\n                wrap_opt_if_condition = re.sub('grad_input_mask\\\\[(\\\\d+)\\\\]', 'grad_fn->should_compute_output(\\\\1)', wrap_opt_if_condition)\n                return f'{wrap_opt_if_condition}'\n            derivative_var_name = derivative.var_names[0]\n            for (edge_off, a) in enumerate(args_with_derivatives):\n                if a.name == derivative_var_name:\n                    break\n            else:\n                raise AssertionError()\n            return f'grad_fn->should_compute_output({edge_off})'\n        if is_inplace_foreach:\n            save_input_stmts = save_variables(info.all_saved_inputs, False, guard_for)\n            if save_input_stmts:\n                setup.append(LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=save_input_stmts))\n        else:\n            setup.extend(save_variables(info.all_saved_inputs, False, guard_for))\n            for arg in args_with_derivatives:\n                if is_tensor_list_type(arg.type):\n                    setup.append(f'grad_fn->{arg.name}_size_ = {arg.name}.size();')\n        return setup\n\n    def setup_derivative(differentiable_inputs: List[DifferentiableInput]) -> List[str]:\n        body: List[str] = []\n        if is_out_fn:\n            body.append(DECLARE_GRAD_FN.substitute(op='Node'))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_inputs]))\n            body.append(SETUP_NONE_REQUIRES_GRAD.substitute(base_name=base_name, args_to_check=[arg.name for arg in differentiable_outputs]))\n            return body\n        op = info.op if info is not None and info.has_derivatives else 'NotImplemented'\n        setup = []\n        if not is_inplace_foreach:\n            setup.extend(ASSIGN_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=[arg.name for arg in args_with_derivatives]).split('\\n'))\n        else:\n            list_like_arg = 'self'\n            args = [arg.name for arg in args_with_derivatives]\n            for (i, arg) in enumerate(args):\n                if is_inplace_foreach and info is not None:\n                    if arg in refargname2inplace_foreacharg:\n                        foreach_arg = refargname2inplace_foreacharg[arg]\n                        args[i] = foreach_arg.name + ('[i]' if isinstance(foreach_arg.type, ListType) else '')\n                elif arg == list_like_arg:\n                    args[i] = arg + '[i]'\n            setup.extend(ASSIGN_VECTOR_OF_GRAD_FN.substitute(op=op, op_ctor='' if info is not None and info.has_derivatives else f'\"{cpp.name(f.func)}\"', args_with_derivatives=args, irange=f'{list_like_arg}.size()').split('\\n'))\n        setup.extend(emit_save_inputs())\n        body.extend(emit_check_no_requires_grad(differentiable_inputs, args_with_derivatives))\n        declare_grad_fn_template = DECLARE_GRAD_FN if not is_inplace_foreach else DECLARE_VECTOR_OF_GRAD_FN\n        body.append(declare_grad_fn_template.substitute(op=op))\n        body.append(SETUP_DERIVATIVE.substitute(setup=setup))\n        return body\n\n    def emit_check_if_in_complex_autograd_allowlist() -> List[str]:\n        body: List[str] = []\n        if base_name in GRADIENT_IMPLEMENTED_FOR_COMPLEX:\n            return body\n        for arg in differentiable_outputs:\n            name = arg.name\n            if arg.cpp_type == 'at::Tensor' or arg.cpp_type in TENSOR_LIST_LIKE_CTYPES:\n                body.append(f'throw_error_for_complex_autograd({name}, \"{base_name}\");')\n        return body\n\n    def emit_check_no_requires_grad(tensor_args: List[DifferentiableInput], args_with_derivatives: List[DifferentiableInput]) -> List[str]:\n        \"\"\"Checks that arguments without derivatives don't require grad\"\"\"\n        body: List[str] = []\n        for arg in tensor_args:\n            if arg in args_with_derivatives:\n                continue\n            arg_name = arg.name\n            if info and arg_name in info.non_differentiable_arg_names:\n                continue\n            if arg_name == 'output':\n                continue\n            body.append(f'check_no_requires_grad({arg_name}, \"{arg_name}\", \"{name}\");')\n        return body\n\n    def emit_original_self_definition() -> List[str]:\n        body: List[str] = []\n        if inplace:\n            if is_inplace_foreach:\n                body.append('std::vector<c10::optional<at::Tensor>> original_selfs(self.size());')\n            else:\n                body.append('c10::optional<at::Tensor> original_self;')\n            all_forward_grad_cond = []\n            for derivative in fw_derivatives:\n                if derivative.required_original_self_value:\n                    all_forward_grad_cond.append(get_any_has_forward_grad_name(derivative.var_names))\n            if all_forward_grad_cond:\n                if not is_inplace_foreach:\n                    body.append(f\"if ({' || '.join(all_forward_grad_cond)}) {{\")\n                    body.append('  original_self = self.clone();')\n                    body.append('}')\n                else:\n                    current_all_forward_grad_cond = [f'{cond}[i]' for cond in all_forward_grad_cond]\n                    body.append('for (const auto& i : c10::irange(self.size())) {')\n                    body.append(f\"  if ({' || '.join(current_all_forward_grad_cond)}) {{\")\n                    body.append('    original_selfs[i] = self[i].clone();')\n                    body.append('  }')\n                    body.append('}')\n        return body\n\n    def save_variables(saved_variables: Sequence[SavedAttribute], is_output: bool, guard_for: Callable[[SavedAttribute], Optional[str]]=lambda name: None) -> Sequence[str]:\n        stmts: List[str] = []\n        for arg in sorted(saved_variables, key=lambda sa: str(sa.nctype.name)):\n            name = arg.nctype.name.name if isinstance(arg.nctype.name, SpecialArgName) else arg.nctype.name\n            foreacharg: Optional[Argument] = None\n            is_foreacharg_list_type: bool = False\n            type = arg.nctype.type\n            expr = arg.expr\n            stmts_prepend = None\n            if is_inplace_foreach and info is not None:\n                name_to_query = name.split('_scalar_type')[0]\n                if name_to_query in refargname2inplace_foreacharg:\n                    foreacharg = refargname2inplace_foreacharg[name_to_query]\n                    is_foreacharg_list_type = isinstance(foreacharg.type, ListType)\n                if foreacharg is not None:\n                    name_in_expr = f\"{foreacharg.name}{('[i]' if is_foreacharg_list_type else '')}\"\n                    src_name = name\n                    if '_scalar_type' in src_name:\n                        split_src_name = src_name.split('_scalar_type')\n                        assert len(split_src_name) == 2\n                        src_name = split_src_name[0]\n                    expr = expr.replace(src_name, name_in_expr)\n            if type == BaseCType(tensorT) or type == OptionalCType(BaseCType(tensorT)) or type == MutRefCType(OptionalCType(BaseCType(tensorT))) or (is_output and type == BaseCType(scalarT)):\n                var = name\n                name += '_'\n                if var == 'self' and inplace:\n                    original_self_var = 'original_self' if not is_inplace_foreach else 'original_selfs[i]'\n                    self_var = var if not is_inplace_foreach else var + '[i]'\n                    stmts_prepend = f'if (!{original_self_var}.has_value()) {original_self_var} = {self_var}.clone()'\n                    var = f'{original_self_var}.value()'\n                    assert not is_output\n                if inplace and is_output:\n                    assert name == 'result_'\n                    var = 'self[i]' if is_inplace_foreach or is_foreacharg_list_type else 'self'\n                    is_inplace_view = f'{var}.is_view()'\n                    expr = f'SavedVariable({var}, {str(is_output).lower()}, {is_inplace_view})'\n                else:\n                    expr = f'SavedVariable({var}, {str(is_output).lower()})'\n                    if foreacharg is not None and 'original_selfs' not in expr:\n                        expr = expr.replace(src_name, name_in_expr)\n            elif type == BaseCType(tensorListT) or type == ListCType(OptionalCType(BaseCType(tensorT))) or type == BaseCType(iTensorListRefT) or (type == VectorCType(BaseCType(tensorT))):\n                if type == VectorCType(BaseCType(tensorT)):\n                    assert is_foreach and is_output\n                expr = f'make_saved_variable_list({name}, {str(is_foreach and is_output).lower()})'\n                name += '_'\n            elif type == BaseCType(intArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(symIntArrayRefT):\n                expr = expr + '.vec()'\n            elif type == BaseCType(stringT):\n                expr = f'std::string({expr})'\n            elif type == OptionalCType(BaseCType(stringT)):\n                expr = f'{expr}.has_value() ? c10::optional<std::string>(std::string({expr}.value())) : c10::nullopt'\n            elif type == ArrayRefCType(elem=BaseCType(type=BaseCppType(ns='at', name='Scalar'))):\n                expr = expr + '.vec()'\n            guard = guard_for(arg)\n            if guard is None:\n                if stmts_prepend:\n                    stmts.append(f'{stmts_prepend};')\n                stmts.append(f'grad_fn->{name} = {expr};')\n            else:\n                stmts.append(f'if ({guard}) {{')\n                if stmts_prepend:\n                    stmts.append(f'  {stmts_prepend};')\n                stmts.append(f'  grad_fn->{name} = {expr};')\n                stmts.append('}')\n        return stmts\n\n    def emit_dispatch_call(f: NativeFunction, input_base: str, unpacked_args: Sequence[str]) -> str:\n        \"\"\"Dispatch call via function in a namespace or method on Tensor.\"\"\"\n        dispatcher_sig = DispatcherSignature.from_schema(f.func)\n        dispatcher_exprs = dispatcher_sig.exprs()\n        dispatch_key_set = 'ks & c10::after_autograd_keyset'\n        call = CALL_REDISPATCH.substitute(api_name=cpp.name(f.func, faithful_name_for_out_overloads=True, symint_overload=f.func.has_symint()), unpacked_args=[dispatch_key_set] + list(unpacked_args))\n        return call\n\n    def wrap_output(f: NativeFunction, unpacked_bindings: List[Binding], var: str) -> str:\n        call = ''\n        rhs_value: Optional[str] = None\n        if not any((r.type.is_tensor_like() for r in f.func.returns)):\n            rhs_value = var\n        else:\n            rhs_value = f'std::move({var})'\n        assert rhs_value is not None\n        call += ASSIGN_RETURN_VALUE.substitute(return_values=tie_return_values(f), rhs_value=rhs_value)\n        return call\n\n    def check_tensorimpl_and_storage(call: str, unpacked_bindings: List[Binding]) -> str:\n        stmts_before_call: List[str] = []\n        stmts_after_call: List[str] = []\n        if cpp.name(f.func) in DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE:\n            return call\n        for unpacked_binding in unpacked_bindings:\n            arg = unpacked_binding.name\n            noref_cpp_type = unpacked_binding.nctype.type.remove_const_ref()\n            if noref_cpp_type == BaseCType(tensorListT) or noref_cpp_type == BaseCType(iTensorListRefT):\n                stmts_before_call += [SAVE_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_TENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                stmts_before_call += [SAVE_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), SAVE_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE.substitute(tensorlist_name=arg), ENFORCE_SAME_OPTIONALTENSORLIST_IMPL.substitute(tensorlist_name=arg)]\n            elif noref_cpp_type == BaseCType(tensorT):\n                stmts_before_call += [SAVE_TENSOR_STORAGE.substitute(tensor_name=arg), SAVE_TENSOR_IMPL.substitute(tensor_name=arg)]\n                stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=arg, out_tensor_name=arg), ENFORCE_SAME_TENSOR_IMPL.substitute(tensor_name=arg)]\n        assert stmts_before_call and stmts_after_call or (not stmts_before_call and (not stmts_after_call))\n        if f.func.kind() not in (SchemaKind.inplace, SchemaKind.out):\n            base_name = f.func.name.name.base\n            aliased_arg_name = ALL_VIEW_FUNCTIONS.get(base_name, None)\n            if aliased_arg_name is not None:\n                aliased_arg_name = unpacked_name(aliased_arg_name)\n            for (i, (ret, ret_name)) in enumerate(zip(f.func.returns, cpp.return_names(f))):\n                noref_cpp_type = cpp.return_type(ret, symint=True).remove_const_ref()\n                if noref_cpp_type == BaseCType(tensorT):\n                    if aliased_arg_name is not None:\n                        assert i == 0, 'Expect non-CompositeImplicitAutograd view function {base} to return single output'\n                        stmts_after_call += [ENFORCE_SAME_TENSOR_STORAGE.substitute(tensor_name=aliased_arg_name, out_tensor_name=ret_name)]\n                    elif type_wrapper_name(f) not in DONT_ENFORCE_STORAGE_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                    if type_wrapper_name(f) not in DONT_ENFORCE_TENSOR_IMPL_USE_COUNT:\n                        stmts_after_call += [ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE.substitute(tensor_name=ret_name, fn_name=type_wrapper_name(f))]\n                elif noref_cpp_type == ListCType(OptionalCType(BaseCType(tensorT))):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n                elif noref_cpp_type == BaseCType(tensorListT):\n                    raise AssertionError(f'Please add use_count checks for {noref_cpp_type}')\n        if stmts_before_call and stmts_after_call:\n            call = RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_before_call) + call + RUN_ONLY_IN_DEBUG_MODE.substitute(statements=stmts_after_call)\n        return call\n\n    def emit_call(f: NativeFunction, unpacked_bindings: List[Binding], try_jit_decomposition: bool) -> str:\n        unpacked_args = [b.name for b in unpacked_bindings]\n        base_type_call = emit_dispatch_call(f, 'self_', unpacked_args)\n        if get_view_info(f) is not None or modifies_arguments(f):\n            guard = 'at::AutoDispatchBelowAutograd guard;'\n        else:\n            guard = 'at::AutoDispatchBelowADInplaceOrView guard;'\n        any_has_forward_grad = get_any_has_fw_grad_cond(derivative=None) if requires_derivative else 'false'\n        return_types = ', '.join([cpp.return_type(a, symint=True).cpp_type() for a in f.func.returns])\n        if len(f.func.returns) > 1:\n            return_types = f'std::tuple<{return_types}>'\n        arg_names = [a.name for a in cpp.arguments(f.func.arguments, faithful=True, symint=True, method=False, cpp_no_default_args=set())]\n        if not modifies_arguments(f) and (not returns_void):\n            if try_jit_decomposition:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard, any_has_forward_grad=any_has_forward_grad, op_name=cpp.name(f.func), op_overload=f.func.name.overload_name, return_types=return_types, arg_names=arg_names)\n            else:\n                call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES.substitute(base_type_call=base_type_call, tmp_var=TMP_VAR, guard=guard)\n            call += wrap_output(f, unpacked_bindings, TMP_VAR)\n        else:\n            assert not try_jit_decomposition\n            call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES.substitute(base_type_call=base_type_call, guard=guard)\n        call = check_tensorimpl_and_storage(call, unpacked_bindings)\n        return call\n\n    def emit_history() -> str:\n        fn = 'rebase' if modifies_arguments(f) and view_info is None else 'set'\n        output_names = [r.name for r in differentiable_outputs]\n        outs = CodeTemplate('flatten_tensor_args( ${outs} )').substitute(outs=output_names if not is_inplace_foreach else 'self')\n        if not is_inplace_foreach:\n            return SET_HISTORY.substitute(fn=fn, differentiable_outputs=outs)\n        else:\n            return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble=f'auto differentiable_outputs = {outs};\\nTORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());', statements=f'{fn}_history(differentiable_outputs[i], grad_fns[i]);')\n\n    def emit_save_outputs() -> str:\n        if is_out_fn:\n            return ''\n        if info is not None and info.has_derivatives:\n            stmts = save_variables(info.all_saved_outputs, True)\n            if len(stmts) == 0:\n                return ''\n            if not is_inplace_foreach:\n                return CONDITIONAL.substitute(cond='grad_fn', statements=stmts)\n            else:\n                return LOOP_OVER_VECTOR_OF_GRAD_FNS.substitute(preamble='', statements=stmts)\n        return ''\n\n    def emit_any_requires_grad() -> List[str]:\n        extra_condition = ''\n        if info and info.output_differentiability_conditions:\n            assert len(info.output_differentiability_conditions) == 1\n            extra_condition = f'_any_requires_grad &= ({info.output_differentiability_conditions[0]});'\n        names_of_args_with_derivatives = [arg.name for arg in args_with_derivatives]\n        if is_inplace_foreach and info is not None:\n            for (i, arg) in enumerate(names_of_args_with_derivatives):\n                for (f_arg, r_arg) in inplace_foreacharg2refarg.items():\n                    if arg == r_arg.name:\n                        names_of_args_with_derivatives[i] = f_arg.name\n        return [SETUP_ANY_REQUIRES_GRAD.substitute(args_with_derivatives=names_of_args_with_derivatives, extra_differentiability_conditions=extra_condition)]\n\n    def get_any_has_forward_grad_name(var_names: Tuple[str, ...]) -> str:\n        if len(var_names) == 1:\n            return f'_any_has_forward_grad_{var_names[0]}'\n        else:\n            return f\"_any_has_forward_grad_{'_'.join(var_names)}\"\n\n    def emit_any_has_forward_grad() -> List[str]:\n        content: List[str] = []\n        if not is_foreach:\n            for derivative in fw_derivatives:\n                requires_fw_grad = get_any_has_fw_grad_cond(derivative=derivative)\n                if info and info.output_differentiability_conditions:\n                    assert len(info.output_differentiability_conditions) == 1\n                    requires_fw_grad = f'({info.output_differentiability_conditions[0]}) && {requires_fw_grad}'\n                content.append(f'[[maybe_unused]] auto {get_any_has_forward_grad_name(derivative.var_names)} = {requires_fw_grad};')\n        else:\n            for derivative in fw_derivatives:\n                bool_vector_name = get_any_has_forward_grad_name(derivative.var_names)\n                cur_derivative_conditions = []\n                for inp in differentiable_inputs:\n                    if derivative.required_inputs_fw_grad is None:\n                        continue\n                    if inp.name not in derivative.required_inputs_fw_grad:\n                        continue\n                    inp_name = inp.name if not inplace else refargname2inplace_foreacharg[inp.name].name\n                    inp_type = inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type\n                    is_list_type = is_tensor_list_type(inp_type)\n                    if is_list_type:\n                        if inp_name != 'self':\n                            content.append(FW_DERIVATIVE_SIZE_CHECK_TEMPLATE.substitute(inp_name=inp_name))\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name + '[i]'))\n                    else:\n                        cur_derivative_conditions.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp_name))\n                content.append(f'std::vector<bool> {bool_vector_name}(self.size());')\n                content.append('for (const auto& i : c10::irange(self.size())) {')\n                content.append(f\"  {bool_vector_name}[i] = {' || '.join(cur_derivative_conditions)};\")\n                content.append('}')\n        return content\n\n    def emit_check_inplace() -> List[str]:\n        if not inplace:\n            return []\n        return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]\n\n    def emit_fw_derivatives() -> List[str]:\n        content: List[str] = []\n        fw_grad_setters: List[str] = []\n        for derivative in fw_derivatives:\n            res = derivative.var_names\n            if f.func.name.name.inplace:\n                assert len(res) == 1, 'Expected number of outputs to be 1 if function is inplace'\n                res = ('self',)\n            assert derivative.required_inputs_fw_grad is not None\n            unpacked_arguments = ''\n            for inp in differentiable_inputs:\n                inp_name = inp.name\n                is_input_tensorlist = is_foreach and is_tensor_list_type(inp.type if not inplace else refargname2inplace_foreacharg[inp.name].type)\n                input_suffix = '[i]' if is_input_tensorlist else ''\n                if is_inplace_foreach:\n                    if inp.name in refargname2inplace_foreacharg:\n                        inp_name = refargname2inplace_foreacharg[inp.name].name\n                zeros_fn = 'zeros' if inplace and inp.name == 'self' else '_efficientzerotensor'\n                if inp.name in derivative.required_inputs_fw_grad:\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix, zeros_fn=zeros_fn)\n                if inp.name in (derivative.required_inputs_primal or []):\n                    unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name=inp.name, inp=inp_name + input_suffix)\n            if derivative.required_original_self_value:\n                input_suffix = 's[i]' if is_inplace_foreach else ''\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix, zeros_fn=zeros_fn)\n                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp_name='original_self', inp='original_self' + input_suffix)\n            elif inplace and derivative.is_reusing_outplace_formula:\n                unpacked_arguments += 'self_t = GradMode::is_enabled() ? self_t.clone() : self_t;'\n            if inplace:\n                is_inplace_str = 'true'\n            else:\n                is_inplace_str = 'false'\n            requires_fw_grad = get_any_has_forward_grad_name(derivative.var_names)\n            if all((isinstance(var_type, BaseType) and var_type.is_tensor_like() for var_type in derivative.var_types)):\n                if len(derivative.var_types) == 1:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    if not is_foreach:\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    else:\n                        assert res[0] == ('result' if not inplace else 'self')\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                    requires_fw_grad += f' && ({derivative.var_names[0]}.defined())'\n                else:\n                    tuple_type = TupleCType([BaseCType(tensorT)] * len(derivative.var_types))\n                    opt_res_grad_type = OptionalCType(tuple_type).cpp_type()\n                    for (idx, single_res) in enumerate(res):\n                        fw_grad_setters.append(FW_DERIVATIVE_SETTER_MULTI_OUTPUT.substitute(idx=idx, all_res='_'.join(res), out_arg=single_res))\n            elif isinstance(derivative.var_types[0], ListType) and derivative.var_types[0].is_tensor_like():\n                assert len(derivative.var_types) == 1, 'Expected number of outputs to be 1 if function returns ListType'\n                if not is_foreach:\n                    opt_res_grad_type = OptionalCType(VectorCType(BaseCType(tensorT))).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_LIST.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n                else:\n                    opt_res_grad_type = OptionalCType(BaseCType(tensorT)).cpp_type()\n                    fw_grad_setters.append(FW_DERIVATIVE_SETTER_TENSOR_FOREACH.substitute(out_arg=res[0], is_inplace=is_inplace_str))\n            else:\n                raise RuntimeError('Unsupported output type for forward derivative')\n            if not is_foreach:\n                fw_grad_opt_definition = f\"{opt_res_grad_type} {'_'.join(res)}_new_fw_grad_opt = c10::nullopt;\"\n                content.append(FW_DERIVATIVE_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, requires_fw_grad=requires_fw_grad, formula=derivative.formula, out_arg='_'.join(res), unpacked_arguments=unpacked_arguments))\n            else:\n                fw_grad_opt_definition = f\"std::vector<{opt_res_grad_type}> {'_'.join(res)}_new_fw_grad_opts(self.size(), c10::nullopt);\"\n                foreach_forward_grad_formula = derivative.formula\n                _foreach_arg: Union[Argument, DifferentiableInput]\n                if inplace:\n                    for (_foreach_arg, _ref_arg) in inplace_foreacharg2refarg.items():\n                        if not (is_tensor_type(_foreach_arg.type) or is_tensor_list_type(_foreach_arg.type)):\n                            pattern = _foreach_arg.name\n                            if isinstance(_foreach_arg.type, ListType):\n                                pattern += '[i]'\n                            foreach_forward_grad_formula = foreach_forward_grad_formula.replace(_ref_arg.name, pattern)\n                elif 'result' in foreach_forward_grad_formula and 'result[i]' not in foreach_forward_grad_formula:\n                    foreach_forward_grad_formula = foreach_forward_grad_formula.replace('result', 'result[i]')\n                content.append(FW_DERIVATIVE_FOREACH_TEMPLATE.substitute(fw_grad_opt_definition=fw_grad_opt_definition, vector_of_optional_tensor=f\"{'_'.join(res)}_new_fw_grad_opts\", any_has_forward_grad_for_current_index=' || '.join((get_any_has_forward_grad_name(derivative.var_names) + '[i]' for derivative in fw_derivatives)), formula=foreach_forward_grad_formula, unpacked_arguments=unpacked_arguments))\n        content.append('\\n'.join(fw_grad_setters))\n        return content\n\n    def get_any_has_fw_grad_cond(derivative: Optional[ForwardDerivative]) -> str:\n        if derivative is None:\n            to_check: List[str] = []\n            for inp in list(mapMaybe(gen_differentiable_input, f.func.arguments.non_out + list(f.func.arguments.out))):\n                if is_tensor_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                elif is_tensor_list_type(inp.type):\n                    to_check.append(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE.substitute(req_inp=inp.name))\n                else:\n                    raise RuntimeError(f'Unsupported input type for \"{name}\" when forbidding forward AD usage.')\n            return f\"({' || '.join(to_check)})\"\n        else:\n            assert derivative.required_inputs_fw_grad is not None\n            if len(derivative.required_inputs_fw_grad) == 0:\n                if not (len(differentiable_inputs) == 1 and is_tensor_list_type(differentiable_inputs[0].type)):\n                    raise RuntimeError(f'No differentiable input to \"{name}\" is a differentiable Tensor (as the provided forward AD formula does not use any input tangent) even though a forward gradient formula has been defined for it. This case should only happen for function that take a single TensorList as input. All other cases are not supported right now.')\n                any_has_fw_grad = 'true'\n            else:\n                any_has_fw_grad = ' || '.join([(FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE if is_tensor_list_type(inp.type) else FW_DERIVATIVE_CHECK_TEMPLATE).substitute(req_inp=inp.name) for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])\n                any_has_fw_grad = f'({any_has_fw_grad})'\n            return any_has_fw_grad\n\n    def emit_forbid_fw_derivatives(is_out_fn: bool=False) -> str:\n        if is_out_fn:\n            msg = 'because it is an out= function'\n        else:\n            msg = 'because it has not been implemented yet.\\\\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.'\n        cond = get_any_has_fw_grad_cond(derivative=None)\n        return FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=msg) if cond != '' else ''\n    body: List[str] = []\n    (unpack_args_stats, unpacked_bindings) = unpack_args(f)\n    body.extend(unpack_args_stats)\n    if requires_derivative:\n        body.extend(emit_any_requires_grad())\n        body.extend(emit_any_has_forward_grad())\n        body.extend(emit_check_inplace())\n        body.extend(emit_original_self_definition())\n        body.extend(setup_derivative(differentiable_inputs))\n    body.append(declare_returned_variables(f))\n    body.append(emit_call(f, unpacked_bindings, try_jit_decomposition))\n    if requires_derivative:\n        body.append(emit_history())\n        body.extend(emit_check_if_in_complex_autograd_allowlist())\n    if is_out_fn:\n        body.append(emit_forbid_fw_derivatives(is_out_fn=True))\n    elif requires_derivative and (not try_jit_decomposition):\n        if len(fw_derivatives) > 0:\n            body.extend(emit_fw_derivatives())\n        else:\n            body.append(emit_forbid_fw_derivatives())\n    if requires_derivative:\n        body.append(emit_save_outputs())\n    if str(f.func.name.name) in RESET_GRAD_ACCUMULATOR:\n        assert inplace\n        body.append('reset_grad_accumulator(self);')\n    if not returns_void:\n        body.append(f'return {get_return_value(f)};')\n    return body"
        ]
    }
]