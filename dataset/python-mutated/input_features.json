[
    {
        "func_name": "sample_to_features_text",
        "original": "def sample_to_features_text(sample, tasks, max_seq_len, tokenizer):\n    \"\"\"\n    Generates a dictionary of features for a given input sample that is to be consumed by a text classification model.\n\n    :param sample: Sample object that contains human readable text and label fields from a single text classification data sample\n    :type sample: Sample\n    :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)\n    :type tasks: dict\n    :param max_seq_len: Sequences are truncated after this many tokens\n    :type max_seq_len: int\n    :param tokenizer: A tokenizer object that can turn string sentences into a list of tokens\n    :return: A list with one dictionary containing the keys \"input_ids\", \"padding_mask\" and \"segment_ids\" (also \"label_ids\" if not\n             in inference mode). The values are lists containing those features.\n    :rtype: list\n    \"\"\"\n    if tokenizer.is_fast:\n        text = sample.clear_text['text']\n        inputs = tokenizer(text, return_token_type_ids=True, truncation=True, truncation_strategy='longest_first', max_length=max_seq_len, return_special_tokens_mask=True)\n        if len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1) != len(sample.tokenized['tokens']):\n            logger.error('FastTokenizer encoded sample %s to %s tokens, which differs from number of tokens produced in tokenize_with_metadata(). \\nFurther processing is likely to be wrong.', sample.clear_text['text'], len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1))\n    else:\n        tokens_a = sample.tokenized['tokens']\n        tokens_b = sample.tokenized.get('tokens_b', None)\n        inputs = tokenizer(tokens_a, tokens_b, add_special_tokens=True, truncation=False, return_token_type_ids=True, is_split_into_words=False)\n    (input_ids, segment_ids) = (inputs['input_ids'], inputs['token_type_ids'])\n    padding_mask = [1] * len(input_ids)\n    if tokenizer.__class__.__name__ == 'XLNetTokenizer':\n        pad_on_left = True\n        segment_ids = pad(segment_ids, max_seq_len, 4, pad_on_left=pad_on_left)\n    else:\n        pad_on_left = False\n        segment_ids = pad(segment_ids, max_seq_len, 0, pad_on_left=pad_on_left)\n    input_ids = pad(input_ids, max_seq_len, tokenizer.pad_token_id, pad_on_left=pad_on_left)\n    padding_mask = pad(padding_mask, max_seq_len, 0, pad_on_left=pad_on_left)\n    assert len(input_ids) == max_seq_len\n    assert len(padding_mask) == max_seq_len\n    assert len(segment_ids) == max_seq_len\n    feat_dict = {'input_ids': input_ids, 'padding_mask': padding_mask, 'segment_ids': segment_ids}\n    for (task_name, task) in tasks.items():\n        try:\n            label_name = task['label_name']\n            label_raw = sample.clear_text[label_name]\n            label_list = task['label_list']\n            if task['task_type'] == 'classification':\n                try:\n                    label_ids = [label_list.index(label_raw)]\n                except ValueError:\n                    raise ValueError(f'[Task: {task_name}] Observed label {label_raw} not in defined label_list')\n            elif task['task_type'] == 'multilabel_classification':\n                label_ids = [0] * len(label_list)\n                for l in label_raw.split(','):\n                    if l != '':\n                        label_ids[label_list.index(l)] = 1\n            elif task['task_type'] == 'regression':\n                label_ids = [float(label_raw)]\n            else:\n                raise ValueError(task['task_type'])\n        except KeyError:\n            label_ids = None\n        if label_ids is not None:\n            feat_dict[task['label_tensor_name']] = label_ids\n    return [feat_dict]",
        "mutated": [
            "def sample_to_features_text(sample, tasks, max_seq_len, tokenizer):\n    if False:\n        i = 10\n    '\\n    Generates a dictionary of features for a given input sample that is to be consumed by a text classification model.\\n\\n    :param sample: Sample object that contains human readable text and label fields from a single text classification data sample\\n    :type sample: Sample\\n    :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)\\n    :type tasks: dict\\n    :param max_seq_len: Sequences are truncated after this many tokens\\n    :type max_seq_len: int\\n    :param tokenizer: A tokenizer object that can turn string sentences into a list of tokens\\n    :return: A list with one dictionary containing the keys \"input_ids\", \"padding_mask\" and \"segment_ids\" (also \"label_ids\" if not\\n             in inference mode). The values are lists containing those features.\\n    :rtype: list\\n    '\n    if tokenizer.is_fast:\n        text = sample.clear_text['text']\n        inputs = tokenizer(text, return_token_type_ids=True, truncation=True, truncation_strategy='longest_first', max_length=max_seq_len, return_special_tokens_mask=True)\n        if len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1) != len(sample.tokenized['tokens']):\n            logger.error('FastTokenizer encoded sample %s to %s tokens, which differs from number of tokens produced in tokenize_with_metadata(). \\nFurther processing is likely to be wrong.', sample.clear_text['text'], len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1))\n    else:\n        tokens_a = sample.tokenized['tokens']\n        tokens_b = sample.tokenized.get('tokens_b', None)\n        inputs = tokenizer(tokens_a, tokens_b, add_special_tokens=True, truncation=False, return_token_type_ids=True, is_split_into_words=False)\n    (input_ids, segment_ids) = (inputs['input_ids'], inputs['token_type_ids'])\n    padding_mask = [1] * len(input_ids)\n    if tokenizer.__class__.__name__ == 'XLNetTokenizer':\n        pad_on_left = True\n        segment_ids = pad(segment_ids, max_seq_len, 4, pad_on_left=pad_on_left)\n    else:\n        pad_on_left = False\n        segment_ids = pad(segment_ids, max_seq_len, 0, pad_on_left=pad_on_left)\n    input_ids = pad(input_ids, max_seq_len, tokenizer.pad_token_id, pad_on_left=pad_on_left)\n    padding_mask = pad(padding_mask, max_seq_len, 0, pad_on_left=pad_on_left)\n    assert len(input_ids) == max_seq_len\n    assert len(padding_mask) == max_seq_len\n    assert len(segment_ids) == max_seq_len\n    feat_dict = {'input_ids': input_ids, 'padding_mask': padding_mask, 'segment_ids': segment_ids}\n    for (task_name, task) in tasks.items():\n        try:\n            label_name = task['label_name']\n            label_raw = sample.clear_text[label_name]\n            label_list = task['label_list']\n            if task['task_type'] == 'classification':\n                try:\n                    label_ids = [label_list.index(label_raw)]\n                except ValueError:\n                    raise ValueError(f'[Task: {task_name}] Observed label {label_raw} not in defined label_list')\n            elif task['task_type'] == 'multilabel_classification':\n                label_ids = [0] * len(label_list)\n                for l in label_raw.split(','):\n                    if l != '':\n                        label_ids[label_list.index(l)] = 1\n            elif task['task_type'] == 'regression':\n                label_ids = [float(label_raw)]\n            else:\n                raise ValueError(task['task_type'])\n        except KeyError:\n            label_ids = None\n        if label_ids is not None:\n            feat_dict[task['label_tensor_name']] = label_ids\n    return [feat_dict]",
            "def sample_to_features_text(sample, tasks, max_seq_len, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Generates a dictionary of features for a given input sample that is to be consumed by a text classification model.\\n\\n    :param sample: Sample object that contains human readable text and label fields from a single text classification data sample\\n    :type sample: Sample\\n    :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)\\n    :type tasks: dict\\n    :param max_seq_len: Sequences are truncated after this many tokens\\n    :type max_seq_len: int\\n    :param tokenizer: A tokenizer object that can turn string sentences into a list of tokens\\n    :return: A list with one dictionary containing the keys \"input_ids\", \"padding_mask\" and \"segment_ids\" (also \"label_ids\" if not\\n             in inference mode). The values are lists containing those features.\\n    :rtype: list\\n    '\n    if tokenizer.is_fast:\n        text = sample.clear_text['text']\n        inputs = tokenizer(text, return_token_type_ids=True, truncation=True, truncation_strategy='longest_first', max_length=max_seq_len, return_special_tokens_mask=True)\n        if len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1) != len(sample.tokenized['tokens']):\n            logger.error('FastTokenizer encoded sample %s to %s tokens, which differs from number of tokens produced in tokenize_with_metadata(). \\nFurther processing is likely to be wrong.', sample.clear_text['text'], len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1))\n    else:\n        tokens_a = sample.tokenized['tokens']\n        tokens_b = sample.tokenized.get('tokens_b', None)\n        inputs = tokenizer(tokens_a, tokens_b, add_special_tokens=True, truncation=False, return_token_type_ids=True, is_split_into_words=False)\n    (input_ids, segment_ids) = (inputs['input_ids'], inputs['token_type_ids'])\n    padding_mask = [1] * len(input_ids)\n    if tokenizer.__class__.__name__ == 'XLNetTokenizer':\n        pad_on_left = True\n        segment_ids = pad(segment_ids, max_seq_len, 4, pad_on_left=pad_on_left)\n    else:\n        pad_on_left = False\n        segment_ids = pad(segment_ids, max_seq_len, 0, pad_on_left=pad_on_left)\n    input_ids = pad(input_ids, max_seq_len, tokenizer.pad_token_id, pad_on_left=pad_on_left)\n    padding_mask = pad(padding_mask, max_seq_len, 0, pad_on_left=pad_on_left)\n    assert len(input_ids) == max_seq_len\n    assert len(padding_mask) == max_seq_len\n    assert len(segment_ids) == max_seq_len\n    feat_dict = {'input_ids': input_ids, 'padding_mask': padding_mask, 'segment_ids': segment_ids}\n    for (task_name, task) in tasks.items():\n        try:\n            label_name = task['label_name']\n            label_raw = sample.clear_text[label_name]\n            label_list = task['label_list']\n            if task['task_type'] == 'classification':\n                try:\n                    label_ids = [label_list.index(label_raw)]\n                except ValueError:\n                    raise ValueError(f'[Task: {task_name}] Observed label {label_raw} not in defined label_list')\n            elif task['task_type'] == 'multilabel_classification':\n                label_ids = [0] * len(label_list)\n                for l in label_raw.split(','):\n                    if l != '':\n                        label_ids[label_list.index(l)] = 1\n            elif task['task_type'] == 'regression':\n                label_ids = [float(label_raw)]\n            else:\n                raise ValueError(task['task_type'])\n        except KeyError:\n            label_ids = None\n        if label_ids is not None:\n            feat_dict[task['label_tensor_name']] = label_ids\n    return [feat_dict]",
            "def sample_to_features_text(sample, tasks, max_seq_len, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Generates a dictionary of features for a given input sample that is to be consumed by a text classification model.\\n\\n    :param sample: Sample object that contains human readable text and label fields from a single text classification data sample\\n    :type sample: Sample\\n    :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)\\n    :type tasks: dict\\n    :param max_seq_len: Sequences are truncated after this many tokens\\n    :type max_seq_len: int\\n    :param tokenizer: A tokenizer object that can turn string sentences into a list of tokens\\n    :return: A list with one dictionary containing the keys \"input_ids\", \"padding_mask\" and \"segment_ids\" (also \"label_ids\" if not\\n             in inference mode). The values are lists containing those features.\\n    :rtype: list\\n    '\n    if tokenizer.is_fast:\n        text = sample.clear_text['text']\n        inputs = tokenizer(text, return_token_type_ids=True, truncation=True, truncation_strategy='longest_first', max_length=max_seq_len, return_special_tokens_mask=True)\n        if len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1) != len(sample.tokenized['tokens']):\n            logger.error('FastTokenizer encoded sample %s to %s tokens, which differs from number of tokens produced in tokenize_with_metadata(). \\nFurther processing is likely to be wrong.', sample.clear_text['text'], len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1))\n    else:\n        tokens_a = sample.tokenized['tokens']\n        tokens_b = sample.tokenized.get('tokens_b', None)\n        inputs = tokenizer(tokens_a, tokens_b, add_special_tokens=True, truncation=False, return_token_type_ids=True, is_split_into_words=False)\n    (input_ids, segment_ids) = (inputs['input_ids'], inputs['token_type_ids'])\n    padding_mask = [1] * len(input_ids)\n    if tokenizer.__class__.__name__ == 'XLNetTokenizer':\n        pad_on_left = True\n        segment_ids = pad(segment_ids, max_seq_len, 4, pad_on_left=pad_on_left)\n    else:\n        pad_on_left = False\n        segment_ids = pad(segment_ids, max_seq_len, 0, pad_on_left=pad_on_left)\n    input_ids = pad(input_ids, max_seq_len, tokenizer.pad_token_id, pad_on_left=pad_on_left)\n    padding_mask = pad(padding_mask, max_seq_len, 0, pad_on_left=pad_on_left)\n    assert len(input_ids) == max_seq_len\n    assert len(padding_mask) == max_seq_len\n    assert len(segment_ids) == max_seq_len\n    feat_dict = {'input_ids': input_ids, 'padding_mask': padding_mask, 'segment_ids': segment_ids}\n    for (task_name, task) in tasks.items():\n        try:\n            label_name = task['label_name']\n            label_raw = sample.clear_text[label_name]\n            label_list = task['label_list']\n            if task['task_type'] == 'classification':\n                try:\n                    label_ids = [label_list.index(label_raw)]\n                except ValueError:\n                    raise ValueError(f'[Task: {task_name}] Observed label {label_raw} not in defined label_list')\n            elif task['task_type'] == 'multilabel_classification':\n                label_ids = [0] * len(label_list)\n                for l in label_raw.split(','):\n                    if l != '':\n                        label_ids[label_list.index(l)] = 1\n            elif task['task_type'] == 'regression':\n                label_ids = [float(label_raw)]\n            else:\n                raise ValueError(task['task_type'])\n        except KeyError:\n            label_ids = None\n        if label_ids is not None:\n            feat_dict[task['label_tensor_name']] = label_ids\n    return [feat_dict]",
            "def sample_to_features_text(sample, tasks, max_seq_len, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Generates a dictionary of features for a given input sample that is to be consumed by a text classification model.\\n\\n    :param sample: Sample object that contains human readable text and label fields from a single text classification data sample\\n    :type sample: Sample\\n    :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)\\n    :type tasks: dict\\n    :param max_seq_len: Sequences are truncated after this many tokens\\n    :type max_seq_len: int\\n    :param tokenizer: A tokenizer object that can turn string sentences into a list of tokens\\n    :return: A list with one dictionary containing the keys \"input_ids\", \"padding_mask\" and \"segment_ids\" (also \"label_ids\" if not\\n             in inference mode). The values are lists containing those features.\\n    :rtype: list\\n    '\n    if tokenizer.is_fast:\n        text = sample.clear_text['text']\n        inputs = tokenizer(text, return_token_type_ids=True, truncation=True, truncation_strategy='longest_first', max_length=max_seq_len, return_special_tokens_mask=True)\n        if len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1) != len(sample.tokenized['tokens']):\n            logger.error('FastTokenizer encoded sample %s to %s tokens, which differs from number of tokens produced in tokenize_with_metadata(). \\nFurther processing is likely to be wrong.', sample.clear_text['text'], len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1))\n    else:\n        tokens_a = sample.tokenized['tokens']\n        tokens_b = sample.tokenized.get('tokens_b', None)\n        inputs = tokenizer(tokens_a, tokens_b, add_special_tokens=True, truncation=False, return_token_type_ids=True, is_split_into_words=False)\n    (input_ids, segment_ids) = (inputs['input_ids'], inputs['token_type_ids'])\n    padding_mask = [1] * len(input_ids)\n    if tokenizer.__class__.__name__ == 'XLNetTokenizer':\n        pad_on_left = True\n        segment_ids = pad(segment_ids, max_seq_len, 4, pad_on_left=pad_on_left)\n    else:\n        pad_on_left = False\n        segment_ids = pad(segment_ids, max_seq_len, 0, pad_on_left=pad_on_left)\n    input_ids = pad(input_ids, max_seq_len, tokenizer.pad_token_id, pad_on_left=pad_on_left)\n    padding_mask = pad(padding_mask, max_seq_len, 0, pad_on_left=pad_on_left)\n    assert len(input_ids) == max_seq_len\n    assert len(padding_mask) == max_seq_len\n    assert len(segment_ids) == max_seq_len\n    feat_dict = {'input_ids': input_ids, 'padding_mask': padding_mask, 'segment_ids': segment_ids}\n    for (task_name, task) in tasks.items():\n        try:\n            label_name = task['label_name']\n            label_raw = sample.clear_text[label_name]\n            label_list = task['label_list']\n            if task['task_type'] == 'classification':\n                try:\n                    label_ids = [label_list.index(label_raw)]\n                except ValueError:\n                    raise ValueError(f'[Task: {task_name}] Observed label {label_raw} not in defined label_list')\n            elif task['task_type'] == 'multilabel_classification':\n                label_ids = [0] * len(label_list)\n                for l in label_raw.split(','):\n                    if l != '':\n                        label_ids[label_list.index(l)] = 1\n            elif task['task_type'] == 'regression':\n                label_ids = [float(label_raw)]\n            else:\n                raise ValueError(task['task_type'])\n        except KeyError:\n            label_ids = None\n        if label_ids is not None:\n            feat_dict[task['label_tensor_name']] = label_ids\n    return [feat_dict]",
            "def sample_to_features_text(sample, tasks, max_seq_len, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Generates a dictionary of features for a given input sample that is to be consumed by a text classification model.\\n\\n    :param sample: Sample object that contains human readable text and label fields from a single text classification data sample\\n    :type sample: Sample\\n    :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)\\n    :type tasks: dict\\n    :param max_seq_len: Sequences are truncated after this many tokens\\n    :type max_seq_len: int\\n    :param tokenizer: A tokenizer object that can turn string sentences into a list of tokens\\n    :return: A list with one dictionary containing the keys \"input_ids\", \"padding_mask\" and \"segment_ids\" (also \"label_ids\" if not\\n             in inference mode). The values are lists containing those features.\\n    :rtype: list\\n    '\n    if tokenizer.is_fast:\n        text = sample.clear_text['text']\n        inputs = tokenizer(text, return_token_type_ids=True, truncation=True, truncation_strategy='longest_first', max_length=max_seq_len, return_special_tokens_mask=True)\n        if len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1) != len(sample.tokenized['tokens']):\n            logger.error('FastTokenizer encoded sample %s to %s tokens, which differs from number of tokens produced in tokenize_with_metadata(). \\nFurther processing is likely to be wrong.', sample.clear_text['text'], len(inputs['input_ids']) - inputs['special_tokens_mask'].count(1))\n    else:\n        tokens_a = sample.tokenized['tokens']\n        tokens_b = sample.tokenized.get('tokens_b', None)\n        inputs = tokenizer(tokens_a, tokens_b, add_special_tokens=True, truncation=False, return_token_type_ids=True, is_split_into_words=False)\n    (input_ids, segment_ids) = (inputs['input_ids'], inputs['token_type_ids'])\n    padding_mask = [1] * len(input_ids)\n    if tokenizer.__class__.__name__ == 'XLNetTokenizer':\n        pad_on_left = True\n        segment_ids = pad(segment_ids, max_seq_len, 4, pad_on_left=pad_on_left)\n    else:\n        pad_on_left = False\n        segment_ids = pad(segment_ids, max_seq_len, 0, pad_on_left=pad_on_left)\n    input_ids = pad(input_ids, max_seq_len, tokenizer.pad_token_id, pad_on_left=pad_on_left)\n    padding_mask = pad(padding_mask, max_seq_len, 0, pad_on_left=pad_on_left)\n    assert len(input_ids) == max_seq_len\n    assert len(padding_mask) == max_seq_len\n    assert len(segment_ids) == max_seq_len\n    feat_dict = {'input_ids': input_ids, 'padding_mask': padding_mask, 'segment_ids': segment_ids}\n    for (task_name, task) in tasks.items():\n        try:\n            label_name = task['label_name']\n            label_raw = sample.clear_text[label_name]\n            label_list = task['label_list']\n            if task['task_type'] == 'classification':\n                try:\n                    label_ids = [label_list.index(label_raw)]\n                except ValueError:\n                    raise ValueError(f'[Task: {task_name}] Observed label {label_raw} not in defined label_list')\n            elif task['task_type'] == 'multilabel_classification':\n                label_ids = [0] * len(label_list)\n                for l in label_raw.split(','):\n                    if l != '':\n                        label_ids[label_list.index(l)] = 1\n            elif task['task_type'] == 'regression':\n                label_ids = [float(label_raw)]\n            else:\n                raise ValueError(task['task_type'])\n        except KeyError:\n            label_ids = None\n        if label_ids is not None:\n            feat_dict[task['label_tensor_name']] = label_ids\n    return [feat_dict]"
        ]
    }
]